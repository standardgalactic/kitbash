Welcome to the Deep Dive. Our mission is to take the world's most compelling ideas about
fundamental reality and really distill the crucial insights you need to understand not just what the
cosmos is, but what your role in it actually means. Today, we're undertaking a truly, well,
a truly ambitious project. We're diving into sources that really challenge the traditional
scientific worldview, this stance that's sometimes called hardened materialism. This view often
portrays the cosmos as this vast, complete, autonomous clockwork mechanism just operating
perfectly without any reference to the observer. And the problem, of course, is that the observer
is us. Exactly. We're in it. That's right. We are facing what really amounts to a philosophical
blind spot in the whole scientific project. Science is brilliant at capturing our shared
environment and formalizing it. Right. Into a mathematical structure, a closed vision of the
universe. A closed vision, precisely. Yeah. But in doing that, it so frequently abstracts away the
concrete lived human experience. Things like the feeling of flow, sound, light, thought, the will,
and crucially, action. So you're left with this tension. You've got the beautiful,
abstract, scientific map, you know, relativity, quantum mechanics, all this mathematical structure.
And then you have the territory. The territory, which is our rich, messy, undeniably real lived
experience. And the deep confusion often happens when we mistake the map for the territory itself.
We've seen that happen before. Oh, absolutely. I mean, take the philosophical leap that's often
made from general relativity. Relativity describes space-time as a four-dimensional manifold,
where time is just another spatial-like dimension. And that structure is essential physics.
It's essential physics, yes. But when philosophers or even physicists declare,
based purely on that formal structure, that the flow of time must be an illusion, well,
that confuses the issue entirely. They're prioritizing the model.
They're prioritizing the mathematical model over the subjective phenomenon. The perception of time's
flow, its passage, the openness of the future. All these intrinsic features of our existence get
relegated to a mere psychological artifact. But wait, if these features, the flow of time,
the open future, if they're universally experienced, isn't that a sign that they're connected to something,
you know, deeper than just individual psychology? Yes, exactly. We have to find a way to reconcile
the formal physics with the lived experience without denying the validity of either one.
Oh, how do we do that? We have to look very closely at the relationship between the system we're
studying and the act of studying it. And this brings us right to the core concept of our deep dive today,
interference. Okay, let's unpack this concept of interference because it feels like a familiar term.
But here, I'm guessing it has a very precise, almost ontological weight.
It really does. Interference in this context is the fundamental phenomenon that arises
when a system, and this could be a person making a decision or an instrument taking a measurement.
Anything. Anything that is representing the domain in which it is situated and acting.
Because that representational system is intrinsically part of the world it's trying to describe,
there are certain facts or features that it simply cannot stabilize independently of the act of
representing them. Wow. That is a profound idea because it immediately pulls us out of that passive
observer role we were just talking about. Absolutely.
This isn't just about measurement. It's about the deep ontological reality of the universe being,
well, participatory. It is entirely aligned with John Wheeler's famous idea of observer participancy.
But what we're doing here is grounding it, not just in epistemology, you know, what we can know,
but in a realist ontological mode. So what processes are fundamentally allowed?
Exactly. We're describing the nature of material interactions themselves. You are fundamentally
required to be a participant within the system to interact with it at all. And this framework,
this idea of interference, that's what allows us to distinguish classical physics from quantum
physics without making the quantum world seem, you know, entirely mysterious or dependent on
consciousness or something. Yes. The difference between classical and quantum mechanics isn't whether
interference exists. It absolutely does, even in our macroscopic world. Yeah. But how pervasive or
endemic it is. In classical macroscopic settings, we can usually manage to separate the subject and the
object well enough to stabilize facts. But in the quantum realm, that separation becomes impossible.
And interference just takes over. It affects the most fundamental properties. And we have to
understand this transition if we ever want to move past the traditional measurement problem.
So to really get a handle on what quantum mechanics implies, we first need to stabilize our
definitions. When we talk about interference, we have to distinguish it very clearly from two related
but distinct concepts we use every day, ignorance and disturbance. The distinction is absolutely vital.
It's everything. If I speak of ignorance, there's a definite fact that exists independently of my
knowledge. And I simply don't have the information. Right. Like I don't know the exact position of Mars right
now, but it has one. My not knowing doesn't change Mars. Exactly. That position is fixed by
astronomical laws. Your lack of knowledge doesn't affect the fact itself one bit. Okay, so that's
ignorance. Then there's disturbance. This is when the fact exists, but our act of learning it physically alters it.
Right. The classic example. You use a thermometer to measure the temperature of a small glass of
water. The thermometer have to absorb a tiny bit of heat to work, which in turn slightly lowers the
water's actual temperature. So the initial fact was there, but the interaction necessary to discover
it, well, it changed the system. But interference is the truly radical concept of the three.
Interference means the fact itself cannot be stabilized independently of the act of representing it.
It's not about lacking information or changing a pre-existing state.
It's about the logical impossibility of defining the fact cleanly.
Yes, because the act of defining it is part of the system it's describing. The universe,
in this view, cannot be treated as some pure static object of our epistemic attention.
That sounds almost like a logical or semantic constraint, kind of like a self-referential paradox.
It often manifests precisely that way, as semantic interference. We see this in logical paradoxes
or, you know, computational self-reference all the time.
Okay, give me an example.
Think back to that classic computer example. We have a computer program to display the answer
to any factual question. We ask it, is the answer to this question that's about to be displayed
in the output channel? No.
Okay, I see where this is going. If the computer tries to answer yes, the output is yes, which
contradicts the content of what it just asserted, which was no.
And if it tries to answer no, the output is no, which makes the assertion true. But the
assertion was that the answer is no. It's a contradiction either way.
So, any answer it gives conflicts semantically with the very act of giving it, the fact can't
stabilize itself. The output state, the word yes or no, cannot be fixed independently of
the process of producing the output. The fact is unstable because the act of generating
it contradicts its logical content. And this just goes to show that instability isn't confined
to quantum physics. It's a general property of any self-representing system.
Okay, so that's semantic. What about when we move into the physical world? You mentioned
causal interference. Right, where the link is a physical action propagating through time.
And the most straightforward example of negative interference here is the self-defeating prophecy.
The prophecy defeats itself. Yes. My favorite example is the mother, who is determined to give
her son a genuine surprise for his Christmas present. She knows her son is extremely clever
and will try to guess it. So she has to be strategic.
Exactly. She decides to wait until Christmas Eve to buy the present. And by doing that,
she ensures that no matter how much the son investigates or guesses beforehand,
he can't stabilize his knowledge about what the present is.
So the son's act of prediction, his attempt to stabilize that future fact,
is the very thing that causes the mother to act strategically by waiting. It defeats the prediction.
The prediction prevents the stable fact from existing early enough to be known.
And this is pervasive in all sorts of strategic interactions.
Oh, like polling predictions in an election. They can defeat the predicted outcome because people
use the information to change their votes.
Precisely. Or a market prediction that a stock will drop causes a massive sell-off that preempts
the stable state that the prediction was assuming.
And the opposite, of course, positive interference, is the self-fulfilling prophecy.
Right. A reputable analyst predicts a stock will surge. That favorable prediction drives enough
investment action that the stock actually does surge.
Right.
The prediction caused the predicted event to happen.
The fact stabilizes because an agent represented it.
Exactly.
So we've established that interference is very real, even in the classical world of human
action. But you made a key point earlier that in classical settings, it's somehow controlled
and channeled into the future. How does classical physics actually prevent this inherent instability
from, you know, infecting the past in our spatial environment?
This is a really deep point, and it's rooted in the fundamental structure of classical reality.
In classical physics, because of things like the finite speed of light and the nature of
interaction, we can establish what we call a logic of measurement. When I interact with
an object, say, a billiard ball, my post-interaction state is correlated with the state of the ball
before the interaction.
Meaning the interaction provides information about a stable, pre-existing fact.
Precisely. And this allows us to stabilize the spatial environment and the past as objects
of knowledge. The spatial environment contains systems whose present states hold information
about the past.
Fossils, footprints, magnetic records on a hard drive.
All of it. We can take a purely epistemic attitude toward the past. It's fixed, it's done,
and it's closed. We're just discovering facts that were there already.
But the future remains strategically open, which is why classical interference is mostly
confined to the realm of our choices and our strategic predictions.
Yes. The interaction required to know the future is actually the action required to make the future.
That's why classically, interference is channeled forward. It defines the boundary between the
knowable, stable past and present and the open, unstable future.
And now, we make the quantum jump. Here's the crucial difference. In the quantum world,
you're saying interference is endemic, it's pervasive, it affects the core properties of
systems.
It breaks that crucial classical ability to stabilize facts and separate the subject from
the object.
So the very nature of quantum interactions, what we call measurements, they don't have the logic
of a classical measurement.
Not at all. We simply cannot produce the statistical outcomes we see in the lab if we insist on the
classical assumption that the properties of quantum systems, like position or momentum,
are stable and defined independently of our attempt to know them.
This is where we need to ground the physics a bit. We're talking about the famous no-go theorems,
right? Theorems like Bells and Koch and Speckers. Can you break down simply why these theorems force
us to abandon that idea of they're already there anyway properties?
Let's just focus on the core implication. Imagine a particle has a property called spin.
Classically, we'd assume that particle has a definite spin value along the x-axis, the y-axis,
and the z-axis all at the same time, even if we only ever measure one of them.
And the measurement just reveals the pre-existing value. This is the assumption of non-contextuality,
right? The idea that measuring x shouldn't depend on whether you also could have measured y or z.
Exactly. And what these theorems show, in no uncertain terms, is that if you insist that these
definite pre-existing values exist, and that measurement results simply reveal them in a local
way, you inevitably end up with statistical predictions, specifically inequalities, that
directly conflict with what we actually measure in the lab time and time again.
The math just doesn't work out.
The math doesn't work out. Chronic mechanics demands contextuality. The property that appears,
say, the spin along the x-axis, is dependent on the context of the measurement.
Meaning which axis you chose to measure.
Right. You cannot attribute definite values to all properties simultaneously. The outcome is only
defined at the moment of interaction. The instability is inherent. The quantum world isn't there to be
known in the same fixed way as the classical past. That is just incredibly non-intuitive. If the fact
isn't there already, what is there before the interaction? What's there is the possibility space,
the wave function, a description of potential relationships and tendencies. And the key takeaway
from the endemic nature of quantum interference is that the conditions that allow for classical
knowledge. The clean separation of subject and object, the fixed past, the isolated object.
All of that. Those conditions are special, not generic. They emerge effectively through a process
called decoherence, where the quantum system irreversibly interacts with its environment,
and that effectively suppresses the interference effects and creates the illusion of a stable
classical object for us. So if the properties aren't there already, the entire logic has to
shift. We have to abandon the attitude of the detached observer who is trying to discover a fixed
fact. And adopt the attitude of the participant who's making a choice that interferes with what
happens next. That's a huge philosophical move. It is. The logic becomes one of choice,
of action, rather than measurement or discovery. When you interact with a quantum system,
you are not passively reading off a value from a cosmic spreadsheet. You are strategically choosing
an interaction, say, choosing the orientation of your Stern-Gerlach apparatus, which then participates
in defining what the actual outcome will be. It's like planting seeds in the spring. I know my action
interferes with the universe's evolution to produce a crop in the fall. I'm choosing my actions
strategically to get a desired outcome.
The core analogy. Quantum mechanics is telling us that at the fundamental level, all interactions
operate under this logic of agent of choice and interference, not the passive discovery of
classical physics. The ability to stabilize the world as a pure object of knowledge is a localized,
high-level, emergent phenomenon, not the generic rule of the cosmos.
Okay. So we've established that our embedded participatory role means we really can't trust
common sense or even direct observation to tell us what fundamental reality is like.
The stable, separable objects of our everyday 3D world are, in fact, an emergent phenomenon.
So if we can't use common sense, what guides the physicist in determining the underlying
fundamental reality? I mean, what is the true ontology of the cosmos?
The guiding star, and this is true across every discipline of fundamental physics,
is the principle of simplicity. Simplicity is achieved by minimizing the number of independent,
irreducible elements you need to describe the dynamics of the universe.
And we measure the simplicity by minimizing the number of degrees of freedom.
Exactly.
Can you define degrees of freedom clearly for us? I mean, beyond the basic physics textbook definition,
what does it mean in this context?
In physics, and especially the philosophy of physics, degrees of freedom represent the number
of independently varying components of nature. These include the basic entities, what some call
the Beebles, their internal structure, and all the configurations they can possibly assume.
The goal is to strip away all representation that is merely a mathematical artifact and identify
only the minimal set of entities and properties whose dynamics can organically reproduce the
phenomena we observe.
So this is a kind of anti-maximalist approach. You're always looking for the most economical
representation possible.
Precisely. The model building process for a theoretical physicist is really an exercise in inferring
ontology from data by looking for the most economical set of fundamental movements. The dynamic laws tell us
how the Beebles evolve, and those laws should govern the minimum possible number of independent
variables.
Let's use your example to illustrate this. The complex kinematical system with the weighted
balls. I think we need to spend some time here walking through how this reduction actually works
conceptually.
Okay. So imagine a complicated machine, a system of, say, seven weighted balls, all connected by various
rods and constraints. If you were modeling this system noncommittally, just recording the visible
properties of all seven balls in three dimensions.
You'd have a lot of data points.
A ton. You'd have 21 position coordinates and 21 momentum coordinates. That's potentially 42 separate
independent pieces of data. This represents the capacity of your initial representational space,
your initial phase space.
And that initial high-dimensional space assumes that every single ball can move completely independently
of every other ball.
Right. But then when you plot the actual observed trajectories of this system through this 42-dimensional
space, you realize that the system never visits a vast majority of those phase points. The
trajectories are confined to a much, much smaller constrained subspace.
Why?
Because the underlying system has internal constraints. The rods linking the balls might keep them a
fixed distance apart, or a guide might restrict their movement to a specific plane.
So the theorists look at those unoccupied regions of the phase space, the gaps where the system
physically can't go, and they use that to infer the underlying ontological constraints.
Exactly. They look at those complex observed trajectories, and they try to reproduce them
in a drastically reduced low-dimensional space. The true fundamental degrees of freedom, the beevils,
the independent elements whose states define the whole system, might only be four in number.
For instance, the position of a central pivot, a key rod's angle, and maybe two independent
rotational speeds. Something like that. The physicist isn't trying to describe the 42 messy data points.
They're trying to identify the four fundamental independent things that, when allowed to evolve
according to simple laws, organically generate all 42 of those observed movements.
That's the core dynamic goal then. You're whittling away the excess representational capacity.
That's a great way to put it. And this is why Einstein's maxim remains supreme. The supreme goal
of all theory is to make the irreducible basic elements as simple and as few as possible without
having to surrender the adequate representation of a single datum of experience. We seek organic
explanation where complexity arises from the simple evolution of minimal components rather than complex
external laws restricting otherwise independent components. And if simplicity is the goal, then
symmetry becomes the primary tool. It's the mathematical beacon that guides us toward superfluous
structure. It is. Symmetries are transformations that preserve the laws of a theory. If a theory remains
mathematically unchanged under rotation, for example, it means directionality is not a fundamental feature of
the laws themselves. We use these symmetries to identify mathematical redundancy.
But not all mathematical redundancies point to structures we should just get rid of.
What specific kind of symmetry indicates a structure is physically unreal?
The ones we focus on are symmetries of the laws that are also qualitative structure preserving.
This is a key distinction. It means if you apply the transformation,
it maps one mathematically distinct solution onto another. But these two solutions are observationally
indistinguishable. You cannot even in principle design an experiment to tell them apart.
Not even in principle. So if two mathematically distinct descriptions produce the exact same physical
reality, we should conclude they are redundant. And the theory contains superfluous structure that is
mathematically present, but physically meaningless. This is the driving engine of ontological
reduction in physics. A classic case is the use of gauge transformations or diffeomorphisms.
In general relativity, the whole argument showed that if you identify two mathematically distinct
spacetime manifolds that agree everywhere outside a certain hole but differ slightly inside,
they lead to identical predictions for all physically measurable outcomes.
And the only way to resolve that philosophical puzzle is to conclude that only the invariant
structure, the structure common to both manifolds, is physically real.
You have to. You excise the structure that makes the mathematical distinction.
So the process is, find the mathematical distinction that produces no physical difference,
and declare that distinct structure to be unreal.
Exactly. That is the ideal scenario for achieving simplicity. However, this leads us to a critical warning sign,
the Aharonov-Bohm effect. Right. This effect really seemed to throw a wrench into that tidy assumption
that if a mathematical structure is non-invariant under symmetry transformations,
we can just discard it as unphysical. It did. Historically, in classical electromagnetism,
the vector potential, we call it A, was considered a mere mathematical tool. It was a convenient way to
calculate the observable electric and magnetic fields E and B. And since adding the gradient of
any scalar function to A leaves E and B unchanged, A was considered non-invariant and therefore physically
unreal, just a mathematical convenience. But the Aharonov-Bohm effect showed something remarkable.
If you run a quantum mechanical experiment where an electron passes through a region with zero
electromagnetic and electric field, but the region does contain a vector potential.
The particle's wave function is measurably affected. The vector potential, which was supposed
to be just a mathematical artifact, had a tangible physical consequence on a quantum particle.
Which forces a crucial realization. We cannot simply excise mathematical structures
based on classical intuitions of invariance, often structures that appear superfluous or frame-dependent,
like the vector potential, or concepts like being simultaneous with. They're not merely mathematical.
They are implicitly relational. Relational, meaning they are only measurable or physically meaningful
when coupled to a specific observer, agent, or measuring device. Precisely. They are not absolute
properties of the universe independent of context. They are only physical when relativized to a reference
frame, or coupled to a participant. The warning of the Aharonov-Bohm is that while simplicity demands
we look for minimal structure, we have to be incredibly careful not to prematurely eliminate
structures that encode necessary relational dynamics within the quantum realm. Sometimes the complexity of
the description is required because the universe is just fundamentally intertwined with the measurement process.
Okay, so the drive for simplicity pushes us to strip away non-invariant structure,
leading us to this minimal set of b-bulls defined by their dynamics.
But that same principle of simplicity forces us to confront perhaps the strangest phenomenon in all of physics,
entanglement. Yes.
We have Alice and Bob measuring coordinated but individually unpredictable spins on particles that
shared a common history and are now separated by, you know, vast distances. They are not identical,
but they are absolutely modally connected. That's the key phrase. If Alice measures up,
Bob instantly gets down. And the core puzzle is that correlation. The outcomes are random individually,
yet they are perfectly coordinated. Now if we try to explain this classically, we have two failing
options. First, direct causation. Alice's measurement instantaneously sends some kind of signal to Bob's
particle telling it what to measure. Right, which violates locality and the speed of light. So that's how.
And second, there's a classical common cause. The idea that the outcomes were determined by some event
in the past, a hidden variable that fixed both results beforehand. But Bill's theorem decisively
ruled out the possibility of a local classical common cause. The correlation is not fixed merely by
antecedent facts. It's robust and simultaneous, meaning the influence cannot travel across that
vast distance in time to connect them. So neither direct causation nor classical common cause works.
We must find a third explanation for this modal connection. And here, the drive for simplicity
guides us to the rational default, the common source inference. Okay, what is that? This is a deep
principle of rational default reasoning. If you have distinct events that are modally connected,
they necessarily happen together or in opposition, and neither one causes or grounds the other. The most
rational inference is that they must stem from a common source that is more fundamental than the
distinct events themselves. And in the case of entanglement, that inference leads directly to
quantum holism. It has to. The idea that the entangled whole, the combined system, is more fundamental than
its spatially separated parts. We stop treating Alice's and Bob's measurements as distinct entities that need
to be causally linked. And instead, we treat them as joint manifestations of a single, deeper, more
fundamental underlying phenomenon. They share a common ground in reality that generates their
coordinated behavior simultaneously. And this common ground explanation is superior because it fulfills
that simplicity mandate we've been talking about. We don't have to invent a new fundamental
interaction, like some spooky entanglement force. Exactly. You reduce the number of truly independent
existences. Instead of treating two distant particles and some mysterious link as three fundamental things,
we reduce it to one fundamental, non-separable existence that merely appears to us as two spatially
separated correlated particles. This is organic explanation at its finest. To make this holistic reality more
visualizable, we can turn to analogies where our standard 3D space is treated as an emergent structure,
a kind of low-dimensional projection of a higher reality. Let's start with Bohm's fish tank, but we
need to link it explicitly to the high-dimensional quantum reality. Bohm's analogy is powerful precisely
because it flips our intuition. So imagine the real reality, the ur-space, if you will, is a 3D fish tank with
one single living fish inside. Okay, one fish. Our perceived reality is a 2D screen onto which two
separate cameras project side-by-side images of that single 3D fish. Got it. So the viewer who is
confined to that 2D image space sees two separate objects, two distinct moving fish shapes. Right.
And if the real 3D fish wags its tail, what happens on the screen? Both 2D images wag their tails instantly,
perfectly coordinated. The 2D observer would naturally search for some kind of signal or
causal link passing between image A and image B on the screen. They'd find non-locality. But we,
the 3D viewers, we know the correlation is due to redundancy. There's only one fish. The images are
just two views of it. The 2D image space is an emergent non-autonomous structure. So now let's apply
this to quantum mechanics. Our familiar 3D space is the 2D image space. And the entangled particles,
Alice's and Bob's, are the two images. And the one unified higher dimensional reality, the single
fish, is the high dimensional wave function evolving in what we call configuration space. That is the
crucial connection. It is. The wave function of an entangled system lives in a vastly high dimensional
configuration space, potentially infinite dimensions, not our 3D space. That single
evolving entity is the common ground, the ur space. When we look at that entity projected into the lower
dimensional 3D space of our perception, we see the correlated randomness of entanglement. The
correlations trace back not to signals in our 3D space, but to the single identity in the high
dimensional space. And the Reichenbach's cube analogy, that reinforces this need for a common
external cause, especially when the correlation feels spooky or non-local. Right. In Reichenbach's
scenario, the observers are inside a cube and they see correlated shadows A1 and A2 of some unseen
external birds. They notice that if shadow A1 flaps, shadow A2 flaps instantly. And their hero,
the Copernican, argues that these two correlated events must be the effects of one single outside
object, the bird, which has a unified existence of its own, distinct from the shadows. The shadows are
just manifestations. The conclusion here is just staggering. Our ordinary 3D space, the very thing
that defines our sense of individuality and separation, may itself be an emergent structure.
The non-separable correlations of entanglement are basically the physics screaming at us that the
parts are not truly independent existences. They are scattered reflections of a unified holistic
reality. And this deep explanation fulfills the simplicity mandate by radically reducing the number
of fundamental entities. We move from countless discrete particles interacting locally to one unified,
dynamically evolving whole, like the wave function of the universe, with our observations being mere
projections onto a spatial subspace. This shift toward a holistic reality which is guided by simplicity,
it often results in a profound tension with our subjective experience. The formal physics description
tends toward a Parmenidean block universe, a fixed 4D structure of events, entirely predetermined.
And yet we live a Heraclitian existence, one that's defined by the irreversible passage of time,
the asymmetry between past and future, the feeling of flow, and the genuine openness of the future.
If the physics is fixed, aren't all those lived experiences just mere illusions?
That is the central challenge.
It's a major philosophical cost that has to be addressed. If we say the flow of time is a mere
artifact of perspective, doesn't that fundamentally negate the reality of free will or genuine change? I mean,
if the future is already set in the block, how can my choices be real actions?
That is the challenge. You're right. And the key is showing that the Heraclitian properties flow,
passage, and openness are not fundamental features of the universe as a whole, but are artifacts that
emerge naturally from our perspective as agents embedded within that universe.
So they're real, but they're relational. They're defined relative to the embedded participant.
Exactly. Okay. Let's break down these artifacts using this concept of our perspective as a
temporally embedded perspective. We are not outside the system looking down. We are constantly moving
within the time axis. Let's start with the feeling of flow, the perception that time is passing,
or the world is constantly moving. This arises because the content of experience at any given
instant is not a single point in time, but stands a finite interval of time.
So our consciousness necessarily encompasses events that change and move.
Right. The physical basis of our perception involves a memory buffer and continuous interaction,
so our momentary view is always dynamic. And that gives us the sense that the world itself is flowing.
And then there's passage, that feeling that the future is constantly becoming the past,
that we are moving through time.
Passage arises because our perspective, our constantly evolving viewpoint over our life history,
changes over time. The time parameter that defines our perspective is always progressing.
And this progression is what generates the phenomenon that what was once
future, something we were anticipating, now belongs to our fixed autobiographical past.
It's an indexical feature of our historical record, not an objective,
physical feature of the universe.
Precisely.
And finally, openness, which brings us right back to interference.
The feeling that the future is genuinely indeterminate and not already set.
Well, the future cannot be viewed as fixed and discovered, because as an agent,
your beliefs about the future cannot be stabilized until you stabilize your choices.
Since your decisions are mental performatives that propagate effects into the future,
the very act of predicting or knowing your choice changes what the outcome will be.
The future's indeterminacy is rooted in the inescapable interference associated with strategic
agency. This inability to stabilize future facts creates subjective indeterminacy.
And that issue of subjective indeterminacy becomes acutely important when we discuss the
deterministic physics of the many worlds interpretation ever.
Right. If the universe evolves deterministically via the Schrodinger equation and all quantum
outcomes are realized in different branches, how can the observer inside the system feel genuine
suspense or uncertainty about the result?
This is the critical problem MWI has to solve. If the totality of the cosmos is fixed and all outcomes
exist, why do I, the observer, feel uncertainty waiting for the result of Schrodinger's cat?
This sense of my unique future is so strong it often tempts people to add non-physical elements.
Like a self that somehow transcendentally leaps from one branch to another.
Exactly. A little humunculus. But MWI avoids this by rooting the uncertainty in the physics of self-reference.
Okay, how?
The key is that the uncertainty is indexical. The system is deterministic, but the observer is
situated within the system. Before the measurement, the observer I is correlated with many possible
future states in different branches. The term I fails to pick out a unique future trajectory.
So the born probability, the probability assigned to a quantum result,
it doesn't tell us what will happen in the universe overall since every outcome happens.
No. Instead, it tells me the probability that the result I am waiting for, E, is what I will see.
And more precisely, the born probability tells the post-measurement successor how typical her
situation is among all the possible successors who shared her free measurement history.
That's it. The suspense is real and physical because the act of measurement
creates a correlation between the observer and the system. But since the universe is branching,
the uncertainty is rooted in the indexical failure of my self-reference to locate a single outcome.
So subjective uncertainty is a real emergent feature of being an embedded participant in a
deterministic branching cosmos. It's a genuine form of ignorance, but not of what the universe does.
That of which particular branching trajectory contains me. Wow.
And it accommodates the phenomenology of planning for my future and fearing my death
entirely within the bounds of deterministic physics. It just reinforces the core theme.
Our reality is defined by the perspective of the participant, not the view from outside the cosmos.
This deep dive has been a powerful journey. We started with this philosophical crisis of abstraction,
and realized that our active participation in the world through endemic interference really defines
what knowledge is even possible. Classical reality is the special stable case. Exactly. And we found
that because common sense fails us at the fundamental level, the physicists search for ontology is strictly
led by this principle of simplicity or the reduction of fundamental degrees of freedom. Ontology is now defined by
dynamics, the laws that govern the evolution of the irreducible basic elements. We look for patterns and
phenomena not necessarily as evidence of new laws, but as clues to an underlying identity that lets us simplify
the fundamental components. And this brings us to the individuals who pioneer these radical shifts.
You know, you think of someone like Roger Penrose, celebrated for his powerful geometric insights,
the ability to visualize problems in ways that radically reconfigure existing complexity towards simplicity.
His work is the platonic ideal of this simplicity mandate. I mean, his development of twister theory,
for example, was an attempt to find a framework where the most complex interactions of physics,
space, time, curvature, and quantum particles emerge from a simpler underlying complex geometry.
It's the ultimate pursuit of finding the single lowest dimensional space that generates the highest
dimensional phenomena. It is. Penrose shows us the power of letting dynamics and geometry define
reality, even if that reality, like his conformal cyclic cosmology, strains our ordinary intuition.
But here's the challenge that Penrose himself presents to this simplicity-driven ontology,
specifically when it comes to consciousness. Right. Penrose argues that consciousness,
which allows for these complex, non-algorithmic geometric insights, insights he argues are non-computable
based on results like Goodwill's theorems, cannot be explained by our current simple computable physics.
He suggests consciousness requires a fundamental revolution may be tied to non-local quantum gravity
effects, meaning consciousness itself must involve non-computable physical processes.
Which forces us to ask a really difficult question. If simplicity, the minimizing of degrees of freedom,
is the ultimate guide to reality, how do we treat the undeniable complexity of our lived,
non-computable conscious experience? Do we follow the physics dynamics wherever they lead,
assuming that consciousness must be simpler and ultimately computable or emergent from these Beebles?
Or do we accept the complexity of human genius, that unique non-computational phenomenology,
as a fundamental, non-reducible degree of freedom in the universe that must itself constrain the final
theory of reality? It's the choice between letting the map's simplicity define the territory,
or letting the most intimate feature of the territory consciousness define the map.
That is the ultimate choice faced by those seeking the fundamental ontology. Radical reduction,
or accepting that the human participant is an irreducible component of the cosmos.
What stands out to you? Thank you for taking this deep dive with us.
