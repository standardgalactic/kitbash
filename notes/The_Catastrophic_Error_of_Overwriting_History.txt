it is january 23rd 2026 and just yesterday a researcher or maybe a philosopher we don't really
know who goes by the pseudonym Flyxion dropped two papers not one but two at the exact same time
at the exact same time and if you're the kind of person who you know scrolls through pre-print
servers on a thursday night for fun which let's be honest if you're listening to this show that's
probably you that's probably you yeah you might have seen these pop up and thought your browser
was glitching you'd think they came from two completely different people maybe even from
two different centuries it's a really jarring juxtaposition on the one hand you have something
that looks like it belongs in a a graduate level computer science textbook oh absolutely yeah the
first one is called ecphoric synthesis and event historical algebra and i mean it is dense it's
got appendices on non-euclidean geometry it's throwing around big o notation it's proposing a
completely new fundamental architecture for how computers should store memory it's pure hardcore
theory the kind of thing that you know most people would never ever see exactly and then right next to
it in the upload queue same time stamp is persistence before efficiency and this this is something else
entirely it's a manifesto it's this sweeping philosophical critique of modern civilization
that's a warning it is it's about why supply chains are shattering why our institutions feel so brittle
why we all kind of feel like we're losing our collective minds it reads less like code and more like
i don't know a sermon from the wilderness a very well argued sermon but yeah a sermon so the mission for
today's deep dive isn't just to summarize these two papers separately that's i think missing the point
the real challenge the fun part is to find the invisible thread that connects them because
Flyxian isn't just moonlighting as a philosopher they're making a single unified argument right
they're arguing that the reason your software crashes the the reason an ai can't count the sides
of a triangle and the reason our economy feels like it's built on a house of cards
are all symptoms of the exact same deep structural error and it's an error that's become so fundamental
to how we think that we don't even see it it's like ah the water for the fish we're swimming in it
it just feels like reality okay so let's start there let's pose the question to you listening
why do we treat the current state of things your bank balance a law the location of a package
as the only truth that matters well at the same time we treat the history of how things got that
way as basically garbage as disposable data to be thrown away and that right there is the core idea
connecting both works it's the danger of overwriting that's the word overwriting whether it's a database
deleting an old record to save a few kilobytes or a supply chain manager getting rid of inefficient
backup inventory they're optimizing for the immediate you're prioritizing the fast and the
now over the slow and the persistent and that according to fliction is a recipe for collapse
so to really get this we have to start with the machine we have to understand the technical argument
first let's open up that first paper ecphoric synthesis right we have to eat our vegetables
before we get to the uh civilizational critique so the paper immediately goes on the attack it goes
after the absolute bedrock of modern computing something called mutable state mutable state
it sounds technical but the concept is actually pretty simple yeah help us unpack it i know mutate
just means to change exactly so think about almost any program you use every day a word processor
your contacts app your bank account online they are all built on this idea of variables okay like in
algebra class x equals five precisely think of a variable as a tiny box in the computer's memory
with a label on it so you have a box labeled current address and inside that box is the value 123 main
street now you move you go online you update your profile to 456 oak lane what does the computer do
it finds that specific physical box in its memory it takes the value 123 main street and it erases it
it overwrites it with 456 oak lane the old address is just gosh it's gone as far as the system is
concerned it never existed the state of your address has mutated it's been changed in place and this is
how everything works almost everything and there's a good historical reason for it back in the you know
the 70s and 80s memory and storage were astronomically expensive a single megabyte could cost thousands of
dollars so you couldn't afford to keep a history of every little change you couldn't it was a massive
waste of a precious resource so the whole paradigm was built around this idea out of keep only the
most recent most correct value the current state is the only thing that matters optimize for space
efficiency there's that word again there it is but Flyxion argues that this design choice which made
sense in 1975 is now the source of two catastrophic problems in our modern complex systems okay which
first one the first is what the paper calls loss of provenance provenance like for a piece of art where
it came from exactly when you look at the value 456 oak lane in that memory box you know what it is
but you have no idea how it got there you've lost the story i don't know who changed it or when or why
did you type it in did a customer service rep do it did a buggy software update from another system
accidentally override it was it a hacker you have no context and without context you can't establish
trust and you can't debug the system when things go wrong you just have the answer with no way to
check the work you got it and that leads to the second problem brittleness this is where systems shatter
imagine you have two different programs or two different people trying to update your address
at the exact same millisecond ah the classic race condition the classic race condition program a wants
to change it to oak lane program b wants to change it to pine street in a mutable system they are literally
racing to write to that one single box in memory one of them will get there a nanosecond later than the
other and it wins it wins it overwrites the other change the system doesn't even know there was a
conflict it just silently accepts the last right as the one and only truth the other change is completely
lost poof gone and this is how you get packages sent to the wrong address or worse medical records
getting scrambled or financial trades being executed based on stale data it makes the system fragile
unpredictable and prone to these silent undetectable errors the paper has this great analogy for it
it says modern computers are like the main character for the movie memento it's a perfect and frankly
terrifying analogy for anyone who hasn't seen it the main character leonard has a condition where he
can't form new long-term memories he forgets everything after a few minutes he's living in a permanent
now exactly so to keep track of reality he has to tattoo important facts onto his body
fine john g don't believe her lies these tattoos are his mutable state but here's a horrifying part
he wakes up every morning looks at a tattoo and he has to trust it implicitly he has no memory of
getting it he doesn't know the context was he lied to right before i got it did he get it right did he
maybe cross out a previous tattoo that said the opposite he is a slave to the current state with
no providence that is our software our databases are leonard every time they read a value from the
disk they are like leonard waking up they have no memory of how that value came to be they are operating
in a state of constant induced amnesia and we trust these amnesiac systems with i mean the entire global
economy that's unsettling it is deeply unsettling infliction contrasts this with how biological
memory works which thankfully is nothing like a computer hard drive right if my brain worked like
that every time i learned your name i'd forget my own or learning calculus would overwrite your memory
of how to ride a bike it would be chaotic biology doesn't overwrite the paper calls it an irreversible
history meaning the old memories are still there somewhere the traces are physically encoded in
your neural architecture you don't delete the child to become the adult the child is part of the adult
the memories are layered integrated and that brings us to the very strange word in the title
ekphory i had looked this one up i thought Flyxion just made it up because it sounds cool
but it's a real term it's a real and very specific term from a german biologist named richard seaman
from way back in the early 1900s and the distinction is critical so in computers we think of memory as
retrieval yes like going into a library finding the right book on the shelf call number 7a and pulling
it out the book is static it's exactly as it was when it was put there but ekphory is different ekphory is a
process of reconstruction seaman's idea was that when you remember something say your childhood bedroom
you're not playing back a video file from a folder in your brain it doesn't feel like that it feels
easier it's because you are actively synthesizing that memory in the present moment you're pulling
together millions of fragmented traces the way the light hit the wall the smell of the old wood the
feeling of the carpet the emotional tone of that time in your life and you're weaving them together
into a coherent image so remembering is an act of creation it's an act of synthesis you are
reconstructing the past to serve the needs of the present and that is what Flyxion proposes we build
computers to do okay so that's the diagnosis mutable state is amnesia the solution is this ekphoric
reconstructive memory how on earth do you build that in silicon the paper lays out a new foundation it
calls it an event historical substrate and the core rule is incredibly simple but it changes everything
what's the rule the rule is append only you never change data you never delete data you only ever add
new information never not even if it's wrong what if i make a typo especially if it's wrong the typo is
part of the story in this new architecture we stop thinking about storing state we only store events
an event so like something happened at this specific time exactly an event is an atomic immutable fact
locked in time at 10.01.32 am user alice typed hello that fact is written to the record it is now there
forever it cannot be changed okay so the typo is immortalized what happens when i fix it that's just
two more events at 10.01.35 am user alice press the backspace key a new event is recorded at 10.01.36 am
user alice type oh another new event so the record doesn't contain the word hello it contains the
entire narrative of how the word hello came to be mistakes and all you've got it the history is the data
this sounds huge i mean my hard drive would fill up in five minutes just from writing one email this
is the first objection everyone has the thing about it storage capacity has been doubling every what 18
months for 50 years storage is practically free today it's our most abundant resource in computing
we are still operating under the mental constraints of 1975 when it was our most scarce resource
we're solving for the wrong problem we're solving for the wrong problem we're trading away truth and
context and safety just to save a few pennies on disk space we have an abundance okay so we're keeping
every event but how does the system make sense of it this is where the algebra part comes in right with
relations yes this is the real meat of the proposal in a normal database a relation is just a line
it's a pointer this customer id is linked to this order id it's just a dumb link a line connecting
two dots on a whiteboard right but infliction system a relation is what they call a first
class entity it's not just a line it's a rich heavy object it has its own properties like what
well a relation has an author who is making this claim it has a time stamp when was this claim made
and critically it has a validity interval a validity interval so it knows when it's supposed to be true
precisely so let's take a medical record in the old mutable world the database has a field
patient diagnosis flu simple okay in the event historical system you'd have a rich relation
object that says dr smith asserts a relationship between patient 123 and diagnosis influenza
this assertion is valid starting from january 22nd at 10 a.m the evidence is lab result hashtag 405
the confidence level is high wow so the link itself contains its own justification its own provenance
the provenance is baked right into the structure of the data now here's the magic a week later new
tests come in dr smith realizes he made a mistake it wasn't the flu it was a rare bacterial infection
in the old system he just goes in deletes influenza and types in bacterial infection the original diagnosis
vanishes and if the patient later has an adverse reaction to the flu medication he was given a future
doctor looking at the record would be completely baffled why was he on this medicine the chart says he
has a bacterial infection the history is gone and that loss of history creates danger so how does the new
system handle it dr smith doesn't delete anything he can't instead he creates a new kind of relation
a meta relation a relation that is about another relation okay my head's starting to spin a little
bit it's like an editor's note this new meta relation says the relation asserting bacterial infection
supersedes the relation asserting influenza this supersedence is effective as of january 29th at 3 pm
ah so the old diagnosis is still there in the database it's just been flagged as outdated it's still there as
a historical fact the fact is on january 22nd we believe the patient had the flu that is a true
statement about the past and it's a critical piece of the patient's story you don't erase it you annotate
it you add a new layer of knowledge this reminds me of how the law is supposed to work you don't just go
into the original constitution with an eraser and start rubbing things out you can't you add an amendment
the old text is still there the amendment acts as a meta relation it says from this day forward we
will interpret that old text in this new way that's a perfect analogy law is an append only system or it's
meant to be this brings up the big obvious question though right if my database is this enormous ever
growing pile of events typos mistakes old addresses superseded diagnoses how do i get a simple answer to a
simple question how do i find the now exactly if i ask the system what is this patient's current
diagnosis i don't want a 50 page history lesson i want one answer and for that we need the synthesis
operator in the paper it's represented by the greek letter sigma this is the engine that makes the whole
thing usable this is the ekvary part the reconstruction this is it sigma is a function you give it two inputs
the entire massive messy history of all events let's call it dollars and a specific point in time
that you care about two dollars so now or last tuesday or ten years from now and it gives you back
a clean view of the world at that moment a present view as the paper calls it it acts like a powerful
intelligent lens it looks at the entire mountain of historical data and filters it according to a set of
rules to construct a coherent consistent snapshot for that specific time two dollars what are the
rules how does it know what to show me and what to hide it runs three main tests the first is simple
temporal admissibility is this fact even valid at the time i'm asking about the relation for your
address at 123 main street had a validity interval that ended in 2022 if i'm asking for your address today
that fact is inadmissible it gets filtered out okay that makes sense it's like a time filter
the second test is for meta-relational constraints it looks for those superseding relations we talked
about it sees the relation for influenza but then it also sees the meta relation that says bacterial
infection superseded it on january 29th if i'm asking for the diagnosis today the meta relation wins the
old diagnosis gets filtered out of the view so it respects the chain of command the amendments exactly
but the third one is the most powerful and the most radical departure from traditional computing it's
contradiction resolution this is the big one because normally if a computer sees a is true
and a is false in its database at the same time it well it breaks it panics it's a logical paradox it's a
short circuit the whole foundation of classical logic that computing is built on just collapses
if you can prove a contradiction you can prove anything and the system becomes useless the blue screen of
death right but the synthesis operator is designed to handle contradictions gracefully it assumes the
world is messy and that conflicting data is normal so when it finds two valid competing facts it doesn't
crash it consults a set of explicit programmable rules to decide which one to prioritize for that
specific view to a place judge the place judge and you the system designer get to write the law book for
the judge the rule might be if two temperature sensors disagree trust the one that was more recently
calibrated or if two editors submit a change trust the one from the senior editor so the conflicting
data the wrong sensor reading is still saved in the history yes and this is so important it's preserved
because maybe the recently calibrated sensor was actually faulty by keeping the conflict in the
historical record you can go back later and rerun the synthesis with a different rule okay let's see
what the world would have looked like if we trusted the other sensor you can ask what if questions
about the past you can audit reality this flips the whole idea of correctness on its head in the old
world correctness means the data stored on the disk is a perfect mirror of reality an impossible standard
right in the new world correctness isn't a property of the stored data the stored data is just a messy
contradictory history correctness is a property of the synthesis it's a view you construct on demand
i love that it feels so much more honest about how knowledge actually works it's not a static thing
it's a process of interpretation it's a derivation not an object and the paper is quick to point out this
isn't pure science fiction there are systems that think this way already software developers who use
version control systems like git live in this world every day and get you never really delete
anything you just add new commits the change thing exactly it's an append only log of events and the
paper gives a shout out to something called merge based systems and it mentions a project called
sphere pop sphere pop yeah i admit i had to look that one up it seems pretty niche it's very niche
but it's part of a broader family of ideas in distributed systems research things like crdts
these conflict-free replicated data types the core problem they're trying to solve is what happens if
you and i are both editing the same document on our laptops on an airplane with no internet we both have
our own version of the truth our histories have diverged right when the plane lands and our laptops
reconnect a normal system like say google docs would have a meltdown it would say conflict you have to
manually resolve this it would force one version to overwrite the other and someone loses their work
it's a pain but in emerge based system there's no conflict it just says great two new histories
it unions all the events together into one big pool and lets the synthesis algebra figure out how to weave
them together into a coherent view later it embraces divergence instead of fearing it it's a much more resilient
more grain up way of dealing with information it accepts that there isn't one single divine timeline
there are multiple perspectives and we need a system that can hold them all at once okay i think i'm
starting to get the mechanics of this but let's make it concrete let's go to that appendix on ai and geometry
this was the part that really made the whole ai hallucination problem click for me yeah this is a
fantastic example it moves us out of the realm of databases and into computer vision
but the core principle is identical so the paper poses this simple question why can't a multi-trillion
parameter ai model an llm that's been trained on literally all the text and images on the internet
reliably count the size of a polygon it's a famous and frankly embarrassing
failure mode you show it a clear image of say a heptagon a seven-sided shape and it will confidently
tell you that is a hexagon or an octagon it just guesses just doing a vibe check on the shape that's
a great way to put it Flyxion's argument is that the ai fails because it is designed to see texture
but it is blind to relations okay break that down for us what's the difference between texture and
relations here when the ai looks at the image of the heptagon it doesn't see a shape it sees a grid of
pixels a static two-dimensional array of color values it's all just texture to the model a blob a blob
it then tries to match the statistical properties of that blob against all the blobs it saw during
training it says well this blob is 83 percent similar to the cluster of blobs that humans labeled
hexagon so it outputs hexagon it's just doing a massive sophisticated pattern matching job
on a static state but that's not what a human does at all if i show you a weird complex shape and ask
you to count the sides what's the first thing you do you don't just look at it as a whole you perform
an action you trace it your eye or your finger starts at one vertex one corner and then it travels along an
edge to the next vertex you count one it's a journey it's a journey you are generating a sequence of events
in time i'm at vertex a now i am traversing edge one now i have arrived at vertex b a polygon to a human
brain isn't a static image it's a history it's a closed loop of successor relations edge two comes
after edge one edge three comes after edge two and so on it's an algorithm a procedure and the ai can't
do that it can't because it's fundamental architecture the transformer is built to process a whole batch of
data at once it's all about parallel processing of a static state it doesn't have innate concept of
a sequential historical traversal so it can't derive the number seven it can only guess at the label
seven-sided shape and a paper points out this fascinating trick if you want to help the ai
you can draw little red dots on all the corners and suddenly it gets it right most of the time why
because you have manually injected the events into the texture you've made the vertices the crucial
historical moments visually salient you've done the first step of the traversal for the ai it's a
damning critique we're building these incredible pattern matching machines that are fundamentally
a historical they live in a flat timeless world of texture so they can write a sonnet in the style
of shakespeare but they can't count to seven because the sonnet is about statistical texture
but counting is about sequential history okay this is a huge idea and it scales up right this isn't just
about ai how does this apply to human systems to a government or a hospital that's where the paper
really broadens the scope let's go back to the medical records example a diagnosis is an event it's a
snapshot of belief at a point in time if that diagnosis turns out to be wrong the mutable state
approach deleting it is an act of historical vandalism you're destroying the clinical history
you're destroying the story the append-only ephoric approach preserves that story the superseded diagnosis
becomes part of the context explaining why certain treatments were tried it makes the entire system
more transparent and more auditable and you can apply the same logic to law absolutely laws shouldn't
be simple text replacements in a file they should be a clear unbroken chain of amendments and supersed
jurors meta relations so you can trace exactly how a legal concept has evolved why a certain clause was
added what it was intended to fix you preserve the legislative intent without that history you're just
left with a bunch of rules you don't understand the origin of which is you know a pretty good
description of many modern bureaucracies okay this feels like the perfect pivot point we've established
the technical argument from the first paper overwriting history whether in a database or an ai's brain
leads to brittle amnesiac and often dangerous systems the solution is to build systems that remember
that treat history as a first-class citizen so now let's pick up that second paper persistence before
efficiency because it feels like fliction takes this exact same logical razor and just applies it to
everything else that's exactly what it is it's a fractal scaling of the same core argument the first paper
says computers fail because they overwrite their own history the second paper says civilizations fail
because they overwrite their own foundations the structural error is identical it's identical the paper
frames it as a fundamental conflict between two opposing values persistence and efficiency and we need to be
really clear on these definitions because our culture especially our business culture basis worships
efficiency it's the highest possible good it is but fliction defines it very specifically
efficiency in this context is about maximizing throughput or speed within a narrow local scope
how many widgets can i produce per hour how quickly can i get this package from a to b how low can i get
my cost per unit it's optimization it's local optimization yes now persistence is a different axis entirely
persistence is the ability of a system to continue its essential functions over a long potentially infinite
time frame especially in the face of shocks and surprises it's not about how fast it can go it's about whether
you can keep going at all can it survive can it regenerate can it last for a hundred years or a
thousand and the central brutal thesis of the paper is that for the last century we have been aggressively
trading away persistence to buy little bits of efficiency there's a quote here that just floored me
civilizations fail not by lacking efficiency but by allowing efficiency to destroy the conditions that
make continued operation possible it's the whole essay in one sentence it's like we're sawing off the
branch we're sitting on because it's a very efficient way to get firewood for tonight that's a great way to
put it and to make this concrete the paper introduces a crucial vocabulary the distinction between fast
variables and slow variables this was the framework that made it all click for me yeah fast variables are
what you see on the dashboard of society right they're the things that change quickly the things that are
easy to measure the things our systems are designed to see stock prices quarterly profits website
engagement metrics delivery times crop yields for this season they are legible to use the term from
political science you can put them on a chart and show them to your boss and because they're legible and
fast our entire incentive structure bonuses promotions elections news cycles is built around optimizing them
make the line go up but the paper argues these fast variables are totally dependent on a hidden
foundation of slow variables yes the slow variables are the substrate the bedrock they regenerate or
degenerate over very long time scales years decades centuries they are incredibly difficult to measure things like
soil fertility the stability of the climate the level of trust in public institutions
the biodiversity of an ecosystem the average attention span of a population the accumulated skill of a
master craftsman you can't put societal trust on a stock ticker you can't and because you can't measure
them easily you tend to ignore them you treat them as if they are constant as if they are free
and that is where the architecture of failure comes in the failure mode is that we let the fast variables
cannibalize the slow ones exactly we achieve an illusion of efficiency in the short term by secretly draining the
long-term battery of the slow variables you can boost this quarter's profits a fast variable by dumping
industrial waste into a river destroying the ecosystem a slow variable the profit is immediate and visible the cost is
delayed and hidden the paper calls this cost displacement we're not actually becoming more efficient we're
just shoving the bill onto future generations or other parts of the world we are let's look at the
wooden chair example from the paper it is so simple and so completely absurd yeah walk us through the global
journey of a simple wooden chair okay so we start by cutting down high quality timber in say the temperate
rainforests of north america that timber is then loaded onto a massive cargo ship powered by the dirtiest
bunker fuel imaginable and shipped 6 000 miles across the pacific ocean to a factory in southeast asia
why to save money on labor costs exactly the fast variable of dollars per hour of labor is being
aggressively optimized okay so in asia the wood is processed cut into parts for the chair then those parts
are put back on another ship and sent 8 000 miles to a port in europe why europe for final assembly
often to take advantage of some specific tax loophole or trade agreement it gets a made in the eu stamp
then the fully assembled chair is put back on another ship and sent 4 000 miles back across
the atlantic to a distribution center in north america where it's eventually sold in a big box store for
49.99 and on the spreadsheet in the quarterly report this looks like a masterpiece of efficiency
the cost per unit is as low as it can possibly be financially it's a triumph but physically energetically
ecologically it is an act of profound insanity we have moved dead trees around the entire circumference of
the planet to make a cheap chair the paper has this killer line distance has been misclassified as a
cheap resource we treat it as if it's free we ignore the energetic cost the geopolitical risk the
environmental cost we are burning the slow variable of a stable atmosphere and depending on the slow
variable of peaceful global shipping lanes all to shave a few cents off the fast variable of production
costs so it's not actually an efficient system it's an incredibly brittle one it's maximally efficient
and minimally resilient the moment one of those slow variable snaps a pandemic shuts down a port a war
closes a shipping lane fuel prices spike the entire chain doesn't just slow down it shatters instantly
because there's no slack no redundancy there is no local capacity to build a chair anymore we have
optimized it away in the name of efficiency this leads perfectly into the case studies in the second
half of the paper the first one for me was the most chilling case study a authorization versus reality
this one gets right to the heart of how our symbolic systems have detached from the physical world
so the scenario affliction paints is a modern supermarket the shelves are overflowing with food there is a
material abundance of calories a parent is at the checkout with a cart full of essentials milk bread
vegetables they swipe their payment card and the machine says declined the machine says declined and
the result is what the result is the parent cannot take the food a security guard might even step in to ensure
the food remains on the premises the food is there the hunger is there but the transaction is forbidden
we see this and we think well that's just how things work they didn't have the money but fliction forces
us to see how profoundly bizarre this situation is it's an inversion of priorities the physical reality is
abundance the symbolic reality the zero or one in a bank's database ledger is scarcity and we have built
a society that has decided that the symbol is more real more primary than the physical object the paper
calls this procedural scarcity we create scarcity not through a lack of stuff but through a failure
of permission we've made the accounting layer the base layer of reality and the biological survival
layer is secondary to it in any persistent sane system it would be the other way around and the
connection to the first paper is so clear the mutable state of that bank account ledger is treated as more
important than the irreversible historical event of a child needing to eat exactly and this creates a
terrifyingly brittle system if the symbolic layer fails if there's a banking crisis a power outage that
takes down the payment network a cyber attack the link between people and the resources they need to
survive is severed instantly even if the food is sitting right there in front of them people can starve in the
midst of plenty not because of a famine but because of a database error that is the ultimate fragility
let's move to the next case study because this is one that affects every single person listening
case study b the mining of attention yeah this one feels very personal the paper frames human attention
deep focused attention as a critical slow variable it's the soil out of which all creativity all problem
solving all skill development grows think about how you learn anything difficult a new language a
musical instrument how to understand these papers it requires long uninterrupted blocks of slow attention
that's how we build human capital that's how a society gets smarter and more competent but what are
the fast variables that our modern digital economy is built to optimize engagement clicks likes shares
swirl velocity time on site tiny measurable immediate bursts of interaction the entire multi-trillion dollar
digital economy is a finely tuned engine designed to do one thing fracture your slow attention into the
smallest possible monetizable units they are strip mining the slow resource of my cognitive capacity
to boost the fast variable of their quarterly ad revenue they're extracting your attention at a rate far
faster than it can naturally regenerate the paper calls this cognitive erosion and it leads to a very
dangerous outcome that fliction calls the generality problem what's the generality problem as our
collective attention becomes more fragmented our ability to think deeply to connect disparate ideas
to solve complex novel problems our general intelligence decays we get really good at the specific narrow task
of using the app we become expert level content consumers but we lose the ability to think outside
the feed we become cognitively dependent on the very tools that are diminishing our capacity we're trading
wisdom for information and information for raw data we are overwriting our own minds yeah we are becoming
to use the language from the first paper state-based humans we just react to the current stimulus the latest
notification with no deep historical context and no ability to project a long-term future we're becoming
amnesiacs just like our databases okay there's one more case study we have to touch on case study c logistics
and redundancy this brings us back to the wooden chair but it's more about the philosophy of waste it's
the battle between two different ideologies just in time versus just in case i went to business school and
just in time was the gospel the idea that inventory is waste a box sitting in a warehouse is liability
it's capital that's not being productive that's the efficiency argument maximum capital efficiency
zero slack in the system you want the component to arrive at the factory at the exact minute it's
needed on the assembly line but fliction makes this incredible counter argument the paper just says it
flatly slack is insurance i love that line it's so good that warehouse full of wasteful inventory is
actually a buffer it's resilience if a blizzard shuts down the highway for three days or a supplier's
factory has a fire you can keep operating you have persistence you paid a small price and efficiency
to buy a large amount of robustness the slack is the system's immune system and the just-in-time system
has no immune system it has a t-cell count of zero the tiniest disruption the smallest shock doesn't
just cause a delay it causes an immediate cascading system-wide failure the whole thing grinds to a
halt we've designed a world that is incredibly fine-tuned to operate perfectly in perfect conditions
and catastrophically fails in any other condition we had stripped away all the fat but in biological
systems fat is the energy reserve it's the survival mechanism for lean times we've optimized for the
feast and in doing so we've guaranteed we won't survive the famine so this is a pretty bleak diagnosis
of our current predicament computers with amnesia an economy that eats its own seed corn and a culture
that's destroying its own ability to think does fliction offer any way out it does and this is really
important the solution presented is not about degrowth or you know going back to being hunter
gatherers it's not anti-technology or anti-progress it's about changing the rules of the game it's about
fundamentally re-architecting the rules of our systems the paper calls this approach constraint
first design constraint first design what does that mean it means that before you even start thinking
about optimizing for speed or efficiency you first establish a set of hard non-negotiable constraints
designed to protect the critical slow variables so you build the guardrails before you build the highway
perfect analogy you define what you are not willing to sacrifice you protect the substrate first give me
some examples from the text what would these constraints look like okay one of the most provocative
is the idea of upper bounds on accumulation so a hard limit on how much wealth or resources one person
or entity can hoard yes or more subtly the paper suggests building decay functions into our symbolic
systems of value decay functions like how food rots exactly like how food rots nature has built in decay
if you hoard a million apples they spoil the resource naturally recycles itself back into the system
but our primary symbol of value money doesn't rot land titles don't expire this allows for effectively
infinite accumulation which leads to extreme imbalances that destabilize the entire system so a decay
function on say massive bank accounts would act like a tax that forces that capital to be reinvested
or spent rather than just sitting there it forces circulation it mimics a natural healthy ecosystem
it prevents the system from seizing up and on the other end the paper talks about lower bounds on access
right this is the flip side this is the constraint that addresses the authorization versus reality problem
it's a hard rule that says symbolic systems like payment networks cannot fully block a person's access to the
basic physical requirements for survival so your bank account being zero can't stop you from getting
a basic provision of water or food if it's physically available survival must precede accounting that is
the constraint you simply cannot optimize your profit and loss statement in a way that violates this lower
bound the system forbids it what about the problem of the wooden chair the insane global supply chains for that the paper
proposes designing for local regeneration loops meaning you have to produce things closer to where
you consume them close enough that the feedback loop between your actions and their consequences
is short and unavoidable if you deplete your local soil to grow your food you are the one who goes hungry
next season you can't just displace the cost to some other continent the feedback keeps the system honest it
forces you to maintain the health of the slow variable the soil because your own persistence depends on it
directly and what about for our world for software for institutions what's the takeaway for the people
building these systems the paper suggests the idea of persistence contracts i love that phrase it sounds
so solid it is it's a promise a commitment to stability it means you don't change your software's
user interface every six months just because the design team has a new theory it means you prioritize
backward compatibility because that allows people to build mastery yes if the tool is constantly
changing under your feet you can never become a true master of it you remain a perpetual novice
constantly relearning the basics a persistence contract is a promise from the builder to the user
we will not overwrite your muscle memory we will not devalue the skills you have invested time in
building that feels like a radical idea in today's tech climate it is it's a commitment to persistence
over the efficiency of constant disruptive change there was one last little gem in the appendix of the
second paper from the world of control theory it was called state aware control yeah that's the engineer's
summary of the entire philosophy in a badly designed control system like say a simple thermostat you
only look at the output is the room at the target temperature yes or no but a sophisticated system is
state aware it looks at the internal state of the system itself it doesn't just ask about the output it
asks how hard is the furnace working how much fuel is left is any component close to overheating and if the
engine's about to explode you stop optimizing for the output you slow down you ease off you prioritize
the long-term health of the machine over achieving the short-term goal and the argument is that our
entire civilization is being run by a control system that is only looking at the speedometer of gdp growth
while completely ignoring the flashing red lights on the engine temperature gauge okay so let's bring it
all home we've been through two dense powerful papers one about database theory the other about
the collapse of civilization what is the final grand synthesis what's the one big aha moment i think the
aha moment is realizing it's all about one problem we have an erasure problem we are addicted to
overwriting in computing we erase data we built our digital world on mutable state overwriting the past
to save a little space and gain a little speed and the solution proposed is ekphoric synthesis an append
only world where we preserve history and construct the present as an intelligent view of that past and
in our civilization we erase our foundations we overwrite the slow regenerative variables of our planet
and our own minds to maximize the fast variables of profit and efficiency and the solution there is
constraint first design building systems that protect their own foundations that prioritize
long-term survival over short-term optimization it's funny all our language is about speed
we're always asking how can we make this faster how can we make this more efficient those are the
questions of the fast variable world but what fliction is really telling us is that we've been asking the
wrong question all along the question isn't how fast can the system go the question is how long can
the system last and when you start asking that question about your software your company your
economy your own life the answers you get are very very different if you're building something you
want to endure how long is the only question that matters the conclusion the second paper leaves us with
a truly chilling thought it says if we don't build systems that remember kphoric systems and systems that
persist constraint first systems then we are by definition building engines of our own erasure we are
building machines perfectly designed to delete ourselves from the record that is a heavy idea to end
on but it's also a direct challenge to you listening look around at your own work at your own life
where are you choosing the illusion of efficiency over the reality of persistence where are you overwriting your
your own slow variables your health your focus your relationships for a short-term gain maybe it's
time we all stopped overwriting and started appending thanks for joining us on the deep dive we'll see you next
we'll see you next time
