

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank


Published by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office:  27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office:  57 Shelton Street, Covent Garden, London WC2H 9HE
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
LINEAR ALGEBRA AND OPTIMIZATION WITH APPLICATIONS TO 
MACHINE LEARNING
Volume I: Linear Algebra for Computer Vision, Robotics, and Machine Learning
Copyright © 2020 by World Scientific Publishing Co. Pte. Ltd. 
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means, 
electronic or mechanical, including photocopying, recording or any information storage and retrieval 
system now known or to be invented, without written permission from the publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance 
Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy 
is not required from the publisher.
ISBN	 978-981-120-639-9	
ISBN	 978-981-120-771-6 (pbk)
For any available supplementary material, please visit 
https://www.worldscientific.com/worldscibooks/10.1142/11446#t=suppl
Desk Editor: Liu Yumeng 
Printed in Singapore

Preface
In recent years, computer vision, robotics, machine learning, and data sci-
ence have been some of the key areas that have contributed to major ad-
vances in technology. Anyone who looks at papers or books in the above
areas will be baﬄed by a strange jargon involving exotic terms such as
kernel PCA, ridge regression, lasso regression, support vector machines
(SVM), Lagrange multipliers, KKT conditions, etc. Do support vector ma-
chines chase cattle to catch them with some kind of super lasso? No! But
one will quickly discover that behind the jargon which always comes with
a new ﬁeld (perhaps to keep the outsiders out of the club), lies a lot of
"classical" linear algebra and techniques from optimization theory. And
there comes the main challenge: in order to understand and use tools from
machine learning, computer vision, and so on, one needs to have a ﬁrm
background in linear algebra and optimization theory. To be honest, some
probability theory and statistics should also be included, but we already
have enough to contend with.
Many books on machine learning struggle with the above problem. How
can one understand what are the dual variables of a ridge regression problem
if one doesn't know about the Lagrangian duality framework? Similarly,
how is it possible to discuss the dual formulation of SVM without a ﬁrm
understanding of the Lagrangian framework?
The easy way out is to sweep these diﬃculties under the rug. If one
is just a consumer of the techniques we mentioned above, the cookbook
recipe approach is probably adequate.
But this approach doesn't work
for someone who really wants to do serious research and make signiﬁcant
contributions. To do so, we believe that one must have a solid background
in linear algebra and optimization theory.
v

vi
Preface
This is a problem because it means investing a great deal of time and
energy studying these ﬁelds, but we believe that perseverance will be amply
rewarded.
Our main goal is to present fundamentals of linear algebra and optimiza-
tion theory, keeping in mind applications to machine learning, robotics, and
computer vision. This work consists of two volumes, the ﬁrst one being lin-
ear algebra, the second one optimization theory and applications, especially
to machine learning.
This ﬁrst volume covers "classical" linear algebra, up to and including
the primary decomposition and the Jordan form.
Besides covering the
standard topics, we discuss a few topics that are important for applications.
These include:
(1) Haar bases and the corresponding Haar wavelets.
(2) Hadamard matrices.
(3) Aﬃne maps (see Section 5.4).
(4) Norms and matrix norms (Chapter 8).
(5) Convergence of sequences and series in a normed vector space. The
matrix exponential eA and its basic properties (see Section 8.8).
(6) The group of unit quaternions, SU(2), and the representation of rota-
tions in SO(3) by unit quaternions (Chapter 15).
(7) An introduction to algebraic and spectral graph theory.
(8) Applications of SVD and pseudo-inverses, in particular, principal com-
ponent analysis, for short PCA (Chapter 21).
(9) Methods for computing eigenvalues and eigenvectors, with a main focus
on the QR algorithm (Chapter 17).
Four topics are covered in more detail than usual. These are
(1) Duality (Chapter 10).
(2) Dual norms (Section 13.7).
(3) The geometry of the orthogonal groups O(n) and SO(n), and of the
unitary groups U(n) and SU(n).
(4) The spectral theorems (Chapter 16).
Except for a few exceptions, we provide complete proofs. We did so
to make this book self-contained, but also because we believe that no
deep knowledge of this material can be acquired without working out some
proofs. However, our advice is to skip some of the proofs upon ﬁrst reading,
especially if they are long and intricate.

Preface
vii
The chapters or sections marked with the symbol ⊛contain material
that is typically more specialized or more advanced, and they can be omit-
ted upon ﬁrst (or second) reading.
Acknowledgement:
We would like to thank Christine Allen-Blanchette,
Kostas Daniilidis, Carlos Esteves, Spyridon Leonardos, Stephen Phillips,
Jo˜ao Sedoc, Stephen Shatz, Jianbo Shi, Marcelo Siqueira, and C.J. Taylor
for reporting typos and for helpful comments. Special thanks to Gilbert
Strang. We learned much from his books which have been a major source
of inspiration. Thanks to Steven Boyd and James Demmel whose books
have been an invaluable source of information. The ﬁrst author also wishes
to express his deepest gratitude to Philippe G. Ciarlet who was his teacher
and mentor in 1970-1972 while he was a student at ENPC in Paris. Pro-
fessor Ciarlet was by far his best teacher. He also knew how to instill in
his students the importance of intellectual rigor, honesty, and modesty. He
still has his typewritten notes on measure theory and integration, and on
numerical linear algebra. The latter became his wonderful book Ciarlet
[Ciarlet (1989)], from which we have borrowed heavily.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Contents
Preface
v
1.
Introduction
1
2.
Vector Spaces, Bases, Linear Maps
5
2.1
Motivations: Linear Combinations, Linear Independence
and Rank
. . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.3
Indexed Families; the Sum Notation P
i∈I ai . . . . . . . .
26
2.4
Linear Independence, Subspaces . . . . . . . . . . . . . . .
32
2.5
Bases of a Vector Space
. . . . . . . . . . . . . . . . . . .
39
2.6
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.7
Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.8
Linear Forms and the Dual Space . . . . . . . . . . . . . .
61
2.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.10
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3.
Matrices and Linear Maps
75
3.1
Representation of Linear Maps by Matrices
. . . . . . . .
75
3.2
Composition of Linear Maps and Matrix Multiplication
.
80
3.3
Change of Basis Matrix
. . . . . . . . . . . . . . . . . . .
86
3.4
The Eﬀect of a Change of Bases on Matrices . . . . . . . .
90
3.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.6
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
94
ix

x
Contents
4.
Haar Bases, Haar Wavelets, Hadamard Matrices
101
4.1
Introduction to Signal Compression Using Haar Wavelets
101
4.2
Haar Bases and Haar Matrices, Scaling Properties of Haar
Wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.3
Kronecker Product Construction of Haar Matrices
. . . .
108
4.4
Multiresolution Signal Analysis with Haar Bases
. . . . .
111
4.5
Haar Transform for Digital Images . . . . . . . . . . . . .
113
4.6
Hadamard Matrices . . . . . . . . . . . . . . . . . . . . . .
122
4.7
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
125
4.8
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
125
5.
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
129
5.1
Direct Products . . . . . . . . . . . . . . . . . . . . . . . .
129
5.2
Sums and Direct Sums . . . . . . . . . . . . . . . . . . . .
130
5.3
The Rank-Nullity Theorem; Grassmann's Relation
. . . .
136
5.4
Aﬃne Maps . . . . . . . . . . . . . . . . . . . . . . . . . .
143
5.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
150
5.6
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
150
6.
Determinants
159
6.1
Permutations, Signature of a Permutation . . . . . . . . .
159
6.2
Alternating Multilinear Maps . . . . . . . . . . . . . . . .
164
6.3
Deﬁnition of a Determinant . . . . . . . . . . . . . . . . .
168
6.4
Inverse Matrices and Determinants . . . . . . . . . . . . .
177
6.5
Systems of Linear Equations and Determinants . . . . . .
180
6.6
Determinant of a Linear Map . . . . . . . . . . . . . . . .
183
6.7
The Cayley-Hamilton Theorem . . . . . . . . . . . . . . .
183
6.8
Permanents . . . . . . . . . . . . . . . . . . . . . . . . . .
189
6.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
192
6.10
Further Readings . . . . . . . . . . . . . . . . . . . . . . .
193
6.11
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
193
7.
Gaussian Elimination, LU-Factorization, Cholesky
Factorization, Reduced Row Echelon Form
199
7.1
Motivating Example: Curve Interpolation . . . . . . . . .
199
7.2
Gaussian Elimination . . . . . . . . . . . . . . . . . . . . .
203
7.3
Elementary Matrices and Row Operations . . . . . . . . .
208
7.4
LU-Factorization . . . . . . . . . . . . . . . . . . . . . . .
212

Contents
xi
7.5
PA = LU Factorization
. . . . . . . . . . . . . . . . . . .
218
7.6
Proof of Theorem 7.2 ⊛
. . . . . . . . . . . . . . . . . . .
227
7.7
Dealing with RoundoﬀErrors; Pivoting Strategies . . . . .
233
7.8
Gaussian Elimination of Tridiagonal Matrices . . . . . . .
234
7.9
SPD Matrices and the Cholesky Decomposition . . . . . .
237
7.10
Reduced Row Echelon Form (RREF) . . . . . . . . . . . .
247
7.11
RREF, Free Variables, and Homogenous Linear Systems .
253
7.12
Uniqueness of RREF Form . . . . . . . . . . . . . . . . . .
257
7.13
Solving Linear Systems Using RREF . . . . . . . . . . . .
259
7.14
Elementary Matrices and Columns Operations . . . . . . .
266
7.15
Transvections and Dilatations ⊛
. . . . . . . . . . . . . .
267
7.16
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
274
7.17
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
275
8.
Vector Norms and Matrix Norms
287
8.1
Normed Vector Spaces . . . . . . . . . . . . . . . . . . . .
287
8.2
Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . .
299
8.3
Subordinate Norms . . . . . . . . . . . . . . . . . . . . . .
304
8.4
Inequalities Involving Subordinate Norms
. . . . . . . . .
312
8.5
Condition Numbers of Matrices . . . . . . . . . . . . . . .
314
8.6
An Application of Norms: Solving Inconsistent
Linear Systems . . . . . . . . . . . . . . . . . . . . . . . .
323
8.7
Limits of Sequences and Series
. . . . . . . . . . . . . . .
325
8.8
The Matrix Exponential . . . . . . . . . . . . . . . . . . .
328
8.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
331
8.10
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
333
9.
Iterative Methods for Solving Linear Systems
339
9.1
Convergence of Sequences of Vectors and Matrices
. . . .
339
9.2
Convergence of Iterative Methods . . . . . . . . . . . . . .
342
9.3
Description of the Methods of Jacobi, Gauss-Seidel, and
Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . .
344
9.4
Convergence of the Methods of Gauss-Seidel and
Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
9.5
Convergence of the Methods of Jacobi, Gauss-Seidel, and
Relaxation for Tridiagonal Matrices . . . . . . . . . . . . .
356
9.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
362
9.7
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
363

xii
Contents
10. The Dual Space and Duality
367
10.1
The Dual Space E∗and Linear Forms
. . . . . . . . . . .
367
10.2
Pairing and Duality Between E and E∗
. . . . . . . . . .
375
10.3
The Duality Theorem and Some Consequences
. . . . . .
380
10.4
The Bidual and Canonical Pairings . . . . . . . . . . . . .
386
10.5
Hyperplanes and Linear Forms
. . . . . . . . . . . . . . .
388
10.6
Transpose of a Linear Map and of a Matrix
. . . . . . . .
389
10.7
Properties of the Double Transpose . . . . . . . . . . . . .
394
10.8
The Four Fundamental Subspaces . . . . . . . . . . . . . .
397
10.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
399
10.10 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
400
11. Euclidean Spaces
405
11.1
Inner Products, Euclidean Spaces . . . . . . . . . . . . . .
405
11.2
Orthogonality and Duality in Euclidean Spaces . . . . . .
415
11.3
Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . .
422
11.4
Existence and Construction of Orthonormal Bases
. . . .
425
11.5
Linear Isometries (Orthogonal Transformations) . . . . . .
433
11.6
The Orthogonal Group, Orthogonal Matrices
. . . . . . .
436
11.7
The Rodrigues Formula
. . . . . . . . . . . . . . . . . . .
438
11.8
QR-Decomposition for Invertible Matrices . . . . . . . . .
441
11.9
Some Applications of Euclidean Geometry . . . . . . . . .
447
11.10 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
448
11.11 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
449
12. QR-Decomposition for Arbitrary Matrices
463
12.1
Orthogonal Reﬂections . . . . . . . . . . . . . . . . . . . .
463
12.2
QR-Decomposition Using Householder Matrices . . . . . .
469
12.3
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
479
12.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
480
13. Hermitian Spaces
487
13.1
Sesquilinear and Hermitian Forms, Pre-Hilbert Spaces and
Hermitian Spaces . . . . . . . . . . . . . . . . . . . . . . .
487
13.2
Orthogonality, Duality, Adjoint of a Linear Map . . . . . .
497
13.3
Linear Isometries (Also Called Unitary Transformations) .
504
13.4
The Unitary Group, Unitary Matrices
. . . . . . . . . . .
505
13.5
Hermitian Reﬂections and QR-Decomposition . . . . . . .
509

Contents
xiii
13.6
Orthogonal Projections and Involutions
. . . . . . . . . .
514
13.7
Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . .
517
13.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
524
13.9
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
525
14. Eigenvectors and Eigenvalues
531
14.1
Eigenvectors and Eigenvalues of a Linear Map . . . . . . .
531
14.2
Reduction to Upper Triangular Form . . . . . . . . . . . .
540
14.3
Location of Eigenvalues
. . . . . . . . . . . . . . . . . . .
545
14.4
Conditioning of Eigenvalue Problems . . . . . . . . . . . .
549
14.5
Eigenvalues of the Matrix Exponential . . . . . . . . . . .
551
14.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
554
14.7
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
554
15. Unit Quaternions and Rotations in SO(3)
565
15.1
The Group SU(2) of Unit Quaternions and the Skew Field
H of Quaternions . . . . . . . . . . . . . . . . . . . . . . .
566
15.2
Representation of Rotations in SO(3) by Quaternions in
SU(2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
567
15.3
Matrix Representation of the Rotation rq
. . . . . . . . .
572
15.4
An Algorithm to Find a Quaternion Representing a
Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
574
15.5
The Exponential Map exp: su(2) →SU(2) . . . . . . . . .
578
15.6
Quaternion Interpolation ⊛. . . . . . . . . . . . . . . . .
580
15.7
Nonexistence of a "Nice" Section from SO(3) to SU(2)
.
583
15.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
585
15.9
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
585
16. Spectral Theorems in Euclidean and Hermitian Spaces
589
16.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
589
16.2
Normal Linear Maps: Eigenvalues and Eigenvectors . . . .
589
16.3
Spectral Theorem for Normal Linear Maps . . . . . . . . .
595
16.4
Self-Adjoint, Skew-Self-Adjoint, and Orthogonal Linear
Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
601
16.5
Normal and Other Special Matrices . . . . . . . . . . . . .
607
16.6
Rayleigh-Ritz Theorems and Eigenvalue Interlacing . . . .
611
16.7
The Courant-Fischer Theorem; Perturbation Results . . .
615
16.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
620

xiv
Contents
16.9
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
620
17. Computing Eigenvalues and Eigenvectors
625
17.1
The Basic QR Algorithm
. . . . . . . . . . . . . . . . . .
627
17.2
Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . .
635
17.3
Making the QR Method More Eﬃcient Using Shifts
. . .
641
17.4
Krylov Subspaces; Arnoldi Iteration
. . . . . . . . . . . .
647
17.5
GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . .
651
17.6
The Hermitian Case; Lanczos Iteration . . . . . . . . . . .
652
17.7
Power Methods . . . . . . . . . . . . . . . . . . . . . . . .
653
17.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
656
17.9
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
657
18. Graphs and Graph Laplacians; Basic Facts
659
18.1
Directed Graphs, Undirected Graphs, Incidence
Matrices, Adjacency Matrices, Weighted Graphs
. . . . .
662
18.2
Laplacian Matrices of Graphs . . . . . . . . . . . . . . . .
670
18.3
Normalized Laplacian Matrices of Graphs . . . . . . . . .
675
18.4
Graph Clustering Using Normalized Cuts
. . . . . . . . .
679
18.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
682
18.6
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
683
19. Spectral Graph Drawing
687
19.1
Graph Drawing and Energy Minimization . . . . . . . . .
687
19.2
Examples of Graph Drawings . . . . . . . . . . . . . . . .
691
19.3
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
696
20. Singular Value Decomposition and Polar Form
699
20.1
Properties of f ∗◦f . . . . . . . . . . . . . . . . . . . . . .
699
20.2
Singular Value Decomposition for Square Matrices
. . . .
703
20.3
Polar Form for Square Matrices . . . . . . . . . . . . . . .
706
20.4
Singular Value Decomposition for Rectangular Matrices
.
709
20.5
Ky Fan Norms and Schatten Norms
. . . . . . . . . . . .
714
20.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
715
20.7
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
715
21. Applications of SVD and Pseudo-Inverses
719
21.1
Least Squares Problems and the Pseudo-Inverse . . . . . .
719

Contents
xv
21.2
Properties of the Pseudo-Inverse
. . . . . . . . . . . . . .
726
21.3
Data Compression and SVD . . . . . . . . . . . . . . . . .
733
21.4
Principal Components Analysis (PCA) . . . . . . . . . . .
735
21.5
Best Aﬃne Approximation . . . . . . . . . . . . . . . . . .
745
21.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
752
21.7
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
752
22. Annihilating Polynomials and the Primary Decomposition
755
22.1
Basic Properties of Polynomials; Ideals, GCD's . . . . . .
757
22.2
Annihilating Polynomials and the Minimal Polynomial . .
762
22.3
Minimal Polynomials of Diagonalizable Linear Maps
. . .
764
22.4
Commuting Families of Diagonalizable and
Triangulable Maps . . . . . . . . . . . . . . . . . . . . . .
768
22.5
The Primary Decomposition Theorem
. . . . . . . . . . .
771
22.6
Jordan Decomposition . . . . . . . . . . . . . . . . . . . .
777
22.7
Nilpotent Linear Maps and Jordan Form . . . . . . . . . .
780
22.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
788
22.9
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
789
Bibliography
791
Index
795

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 1
Introduction
As we explained in the preface, this ﬁrst volume covers "classical" linear
algebra, up to and including the primary decomposition and the Jordan
form. Besides covering the standard topics, we discuss a few topics that
are important for applications. These include:
(1) Haar bases and the corresponding Haar wavelets, a fundamental tool
in signal processing and computer graphics.
(2) Hadamard matrices which have applications in error correcting codes,
signal processing, and low rank approximation.
(3) Aﬃne maps (see Section 5.4). These are usually ignored or treated in a
somewhat obscure fashion. Yet they play an important role in computer
vision and robotics. There is a clean and elegant way to deﬁne aﬃne
maps.
One simply has to deﬁne aﬃne combinations.
Linear maps
preserve linear combinations, and similarly aﬃne maps preserve aﬃne
combinations.
(4) Norms and matrix norms (Chapter 8). These are used extensively in
optimization theory.
(5) Convergence of sequences and series in a normed vector space. Ba-
nach spaces (see Section 8.7). The matrix exponential eA and its basic
properties (see Section 8.8).
In particular, we prove the Rodrigues
formula for rotations in SO(3) and discuss the surjectivity of the expo-
nential map exp: so(3) →SO(3), where so(3) is the real vector space
of 3×3 skew symmetric matrices (see Section 11.7). We also show that
det(eA) = etr(A) (see Section 14.5).
(6) The group of unit quaternions, SU(2), and the representation of rota-
tions in SO(3) by unit quaternions (Chapter 15). We deﬁne a homo-
morphism r: SU(2) →SO(3) and prove that it is surjective and that
its kernel is {−I, I}. We compute the rotation matrix Rq associated
1

2
Introduction
with a unit quaternion q, and give an algorithm to construct a quater-
nion from a rotation matrix. We also show that the exponential map
exp: su(2) →SU(2) is surjective, where su(2) is the real vector space of
skew-Hermitian 2 × 2 matrices with zero trace. We discuss quaternion
interpolation and prove the famous slerp interpolation formula due to
Ken Shoemake.
(7) An introduction to algebraic and spectral graph theory. We deﬁne the
graph Laplacian and prove some of its basic properties (see Chapter 18).
In Chapter 19, we explain how the eigenvectors of the graph Laplacian
can be used for graph drawing.
(8) Applications of SVD and pseudo-inverses, in particular, principal com-
ponent analysis, for short PCA (Chapter 21).
(9) Methods for computing eigenvalues and eigenvectors are discussed in
Chapter 17. We ﬁrst focus on the QR algorithm due to Rutishauser,
Francis, and Kublanovskaya.
See Sections 17.1 and 17.3.
We then
discuss how to use an Arnoldi iteration, in combination with the QR
algorithm, to approximate eigenvalues for a matrix A of large dimen-
sion. See Section 17.4. The special case where A is a symmetric (or
Hermitian) tridiagonal matrix, involves a Lanczos iteration, and is dis-
cussed in Section 17.6. In Section 17.7, we present power iterations and
inverse (power) iterations.
Five topics are covered in more detail than usual. These are
(1) Matrix factorizations such as LU, PA = LU, Cholesky, and reduced
row echelon form (rref).
Deciding the solvablity of a linear system
Ax = b, and describing the space of solutions when a solution exists.
See Chapter 7.
(2) Duality (Chapter 10).
(3) Dual norms (Section 13.7).
(4) The geometry of the orthogonal groups O(n) and SO(n), and of the
unitary groups U(n) and SU(n).
(5) The spectral theorems (Chapter 16).
Most texts omit the proof that the PA = LU factorization can be
obtained by a simple modiﬁcation of Gaussian elimination.
We give a
complete proof of Theorem 7.2 in Section 7.6. We also prove the uniqueness
of the rref of a matrix; see Proposition 7.13.
At the most basic level, duality corresponds to transposition.
But
duality is really the bijection between subspaces of a vector space E (say

Introduction
3
ﬁnite-dimensional) and subspaces of linear forms (subspaces of the dual
space E∗) established by two maps: the ﬁrst map assigns to a subspace V of
E the subspace V 0 of linear forms that vanish on V ; the second map assigns
to a subspace U of linear forms the subspace U 0 consisting of the vectors
in E on which all linear forms in U vanish. The above maps deﬁne a bijec-
tion such that dim(V ) + dim(V 0) = dim(E), dim(U) + dim(U 0) = dim(E),
V 00 = V , and U 00 = U.
Another important fact is that if E is a ﬁnite-dimensional space with
an inner product u, v 7→⟨u, v⟩(or a Hermitian inner product if E is a
complex vector space), then there is a canonical isomorphism between E
and its dual E∗. This means that every linear form f ∈E∗is uniquely
represented by some vector u ∈E, in the sense that f(v) = ⟨v, u⟩for all
v ∈E. As a consequence, every linear map f has an adjoint f ∗such that
⟨f(u), v⟩= ⟨u, f ∗(v)⟩for all u, v ∈E.
Dual norms show up in convex optimization; see Boyd and Vanden-
berghe [Boyd and Vandenberghe (2004)].
Because of their importance in robotics and computer vision, we discuss
in some detail the groups of isometries O(E) and SO(E) of a vector space
with an inner product. The isometries in O(E) are the linear maps such
that f ◦f ∗= f ∗◦f = id, and the direct isometries in SO(E), also called
rotations, are the isometries in O(E) whose determinant is equal to +1.
We also discuss the hermitian counterparts U(E) and SU(E).
We prove the spectral theorems not only for real symmetric matrices,
but also for real and complex normal matrices.
We stress the importance of linear maps. Matrices are of course invalu-
able for computing and one needs to develop skills for manipulating them.
But matrices are used to represent a linear map over a basis (or two bases),
and the same linear map has diﬀerent matrix representations. In fact, we
can view the various normal forms of a matrix (Schur, SVD, Jordan) as a
suitably convenient choice of bases.
We have listed most of the Matlab functions relevant to numerical lin-
ear algebra and have included Matlab programs implementing most of the
algorithms discussed in this book.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 2
Vector Spaces, Bases, Linear Maps
2.1
Motivations: Linear Combinations, Linear
Independence and Rank
In linear optimization problems, we often encounter systems of linear equa-
tions. For example, consider the problem of solving the following system of
three linear equations in the three variables x1, x2, x3 ∈R:
x1 + 2x2 −x3 = 1
2x1 + x2 + x3 = 2
x1 −2x2 −2x3 = 3.
One way to approach this problem is introduce the "vectors" u, v, w,
and b, given by
u =


1
2
1


v =


2
1
−2


w =


−1
1
−2


b =


1
2
3


and to write our linear system as
x1u + x2v + x3w = b.
In the above equation, we used implicitly the fact that a vector z can be
multiplied by a scalar λ ∈R, where
λz = λ


z1
z2
z3

=


λz1
λz2
λz3

,
and two vectors y and and z can be added, where
y + z =


y1
y2
y3

+


z1
z2
z3

=


y1 + z1
y2 + z2
y3 + z3

.
5

6
Vector Spaces, Bases, Linear Maps
Also, given a vector
x =


x1
x2
x3

,
we deﬁne the additive inverse −x of x (pronounced minus x) as
−x =


−x1
−x2
−x3

.
Observe that −x = (−1)x, the scalar multiplication of x by −1.
The set of all vectors with three components is denoted by R3×1. The
reason for using the notation R3×1 rather than the more conventional no-
tation R3 is that the elements of R3×1 are column vectors; they consist of
three rows and a single column, which explains the superscript 3 × 1. On
the other hand, R3 = R×R×R consists of all triples of the form (x1, x2, x3),
with x1, x2, x3 ∈R, and these are row vectors. However, there is an ob-
vious bijection between R3×1 and R3 and they are usually identiﬁed. For
the sake of clarity, in this introduction, we will denote the set of column
vectors with n components by Rn×1.
An expression such as
x1u + x2v + x3w
where u, v, w are vectors and the xis are scalars (in R) is called a linear
combination. Using this notion, the problem of solving our linear system
x1u + x2v + x3w = b
is equivalent to determining whether b can be expressed as a linear combi-
nation of u, v, w.
Now if the vectors u, v, w are linearly independent, which means that
there is no triple (x1, x2, x3) ̸= (0, 0, 0) such that
x1u + x2v + x3w = 03,
it can be shown that every vector in R3×1 can be written as a linear com-
bination of u, v, w. Here, 03 is the zero vector
03 =


0
0
0

.
It is customary to abuse notation and to write 0 instead of 03. This rarely
causes a problem because in most cases, whether 0 denotes the scalar zero
or the zero vector can be inferred from the context.

2.1. Motivations: Linear Combinations, Linear Independence and Rank
7
In fact, every vector z ∈R3×1 can be written in a unique way as a linear
combination
z = x1u + x2v + x3w.
This is because if
z = x1u + x2v + x3w = y1u + y2v + y3w,
then by using our (linear!) operations on vectors, we get
(y1 −x1)u + (y2 −x2)v + (y3 −x3)w = 0,
which implies that
y1 −x1 = y2 −x2 = y3 −x3 = 0,
by linear independence. Thus,
y1 = x1,
y2 = x2,
y3 = x3,
which shows that z has a unique expression as a linear combination, as
claimed. Then our equation
x1u + x2v + x3w = b
has a unique solution, and indeed, we can check that
x1 = 1.4
x2 = −0.4
x3 = −0.4
is the solution.
But then, how do we determine that some vectors are linearly indepen-
dent?
One answer is to compute a numerical quantity det(u, v, w), called the
determinant of (u, v, w), and to check that it is nonzero. In our case, it
turns out that
det(u, v, w) =

1 2 −1
2 1
1
1 −2 −2

= 15,
which conﬁrms that u, v, w are linearly independent.
Other methods, which are much better for systems with a large num-
ber of variables, consist of computing an LU-decomposition or a QR-
decomposition, or an SVD of the matrix consisting of the three columns
u, v, w,
A =
 u v w

=


1 2 −1
2 1
1
1 −2 −2

.

8
Vector Spaces, Bases, Linear Maps
If we form the vector of unknowns
x =


x1
x2
x3

,
then our linear combination x1u+x2v +x3w can be written in matrix form
as
x1u + x2v + x3w =


1 2 −1
2 1
1
1 −2 −2




x1
x2
x3

,
so our linear system is expressed by


1 2 −1
2 1
1
1 −2 −2




x1
x2
x3

=


1
2
3

,
or more concisely as
Ax = b.
Now what if the vectors u, v, w are linearly dependent? For example, if
we consider the vectors
u =


1
2
1


v =


2
1
−1


w =


−1
1
2

,
we see that
u −v = w,
a nontrivial linear dependence. It can be veriﬁed that u and v are still
linearly independent. Now for our problem
x1u + x2v + x3w = b
it must be the case that b can be expressed as linear combination of u and
v. However, it turns out that u, v, b are linearly independent (one way to
see this is to compute the determinant det(u, v, b) = −6), so b cannot be
expressed as a linear combination of u and v and thus, our system has no
solution.
If we change the vector b to
b =


3
3
0

,

2.1. Motivations: Linear Combinations, Linear Independence and Rank
9
then
b = u + v,
and so the system
x1u + x2v + x3w = b
has the solution
x1 = 1,
x2 = 1,
x3 = 0.
Actually, since w = u −v, the above system is equivalent to
(x1 + x3)u + (x2 −x3)v = b,
and because u and v are linearly independent, the unique solution in x1+x3
and x2 −x3 is
x1 + x3 = 1
x2 −x3 = 1,
which yields an inﬁnite number of solutions parameterized by x3, namely
x1 = 1 −x3
x2 = 1 + x3.
In summary, a 3 × 3 linear system may have a unique solution, no
solution, or an inﬁnite number of solutions, depending on the linear inde-
pendence (and dependence) or the vectors u, v, w, b. This situation can be
generalized to any n×n system, and even to any n×m system (n equations
in m variables), as we will see later.
The point of view where our linear system is expressed in matrix form
as Ax = b stresses the fact that the map x 7→Ax is a linear transformation.
This means that
A(λx) = λ(Ax)
for all x ∈R3×1 and all λ ∈R and that
A(u + v) = Au + Av,
for all u, v ∈R3×1. We can view the matrix A as a way of expressing a
linear map from R3×1 to R3×1 and solving the system Ax = b amounts to
determining whether b belongs to the image of this linear map.
Given a 3 × 3 matrix
A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

,

10
Vector Spaces, Bases, Linear Maps
whose columns are three vectors denoted A1, A2, A3, and given any vector
x = (x1, x2, x3), we deﬁned the product Ax as the linear combination
Ax = x1A1 + x2A2 + x3A3 =


a11x1 + a12x2 + a13x3
a21x1 + a22x2 + a23x3
a31x1 + a32x2 + a33x3

.
The common pattern is that the ith coordinate of Ax is given by a certain
kind of product called an inner product, of a row vector, the ith row of A,
times the column vector x:
 ai1 ai2 ai3

·


x1
x2
x3

= ai1x1 + ai2x2 + ai3x3.
More generally, given any two vectors x = (x1, . . . , xn) and y =
(y1, . . . , yn) ∈Rn, their inner product denoted x·y, or ⟨x, y⟩, is the number
x · y =
 x1 x2 · · · xn

·





y1
y2
...
yn




=
n
X
i=1
xiyi.
Inner products play a very important role. First, we quantity
∥x∥2 = √x · x = (x2
1 + · · · + x2
n)1/2
is a generalization of the length of a vector, called the Euclidean norm, or
ℓ2-norm. Second, it can be shown that we have the inequality
|x · y| ≤∥x∥∥y∥,
so if x, y ̸= 0, the ratio (x · y)/(∥x∥∥y∥) can be viewed as the cosine of
an angle, the angle between x and y. In particular, if x · y = 0 then the
vectors x and y make the angle π/2, that is, they are orthogonal. The
(square) matrices Q that preserve the inner product, in the sense that
⟨Qx, Qy⟩= ⟨x, y⟩for all x, y ∈Rn, also play a very important role. They
can be thought of as generalized rotations.
Returning to matrices, if A is an m × n matrix consisting of n columns
A1, . . . , An (in Rm), and B is a n × p matrix consisting of p columns
B1, . . . , Bp (in Rn) we can form the p vectors (in Rm)
AB1, . . . , ABp.

2.1. Motivations: Linear Combinations, Linear Independence and Rank
11
These p vectors constitute the m×p matrix denoted AB, whose jth column
is ABj. But we know that the ith coordinate of ABj is the inner product
of the ith row of A by the jth column of B,
 ai1 ai2 · · · ain

·





b1j
b2j
...
bnj




=
n
X
k=1
aikbkj.
Thus we have deﬁned a multiplication operation on matrices, namely if
A = (aik) is a m × n matrix and if B = (bjk) if n × p matrix, then their
product AB is the m × n matrix whose entry on the ith row and the jth
column is given by the inner product of the ith row of A by the jth column
of B,
(AB)ij =
n
X
k=1
aikbkj.
Beware that unlike the multiplication of real (or complex) numbers, if A
and B are two n × n matrices, in general, AB ̸= BA.
Suppose that A is an n × n matrix and that we are trying to solve the
linear system
Ax = b,
with b ∈Rn. Suppose we can ﬁnd an n × n matrix B such that
BAi = ei,
i = 1, . . . , n,
with ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith
slot. If we form the n × n matrix
In =










1 0 0 · · · 0 0
0 1 0 · · · 0 0
0 0 1 · · · 0 0
... ... ... ... ... ...
0 0 0 · · · 1 0
0 0 0 · · · 0 1










,
called the identity matrix, whose ith column is ei, then the above is equiv-
alent to
BA = In.
If Ax = b, then multiplying both sides on the left by B, we get
B(Ax) = Bb.

12
Vector Spaces, Bases, Linear Maps
But is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have
x = Bb.
We can verify that x = Bb is indeed a solution, because it can be shown
that
A(Bb) = (AB)b = Inb = b.
What is not obvious is that BA = In implies AB = In, but this is indeed
provable. The matrix B is usually denoted A−1 and called the inverse of
A. It can be shown that it is the unique matrix such that
AA−1 = A−1A = In.
If a square matrix A has an inverse, then we say that it is invertible or
nonsingular, otherwise we say that it is singular. We will show later that
a square matrix is invertible iﬀits columns are linearly independent iﬀits
determinant is nonzero.
In summary, if A is a square invertible matrix, then the linear system
Ax = b has the unique solution x = A−1b. In practice, this is not a good
way to solve a linear system because computing A−1 is too expensive. A
practical method for solving a linear system is Gaussian elimination, dis-
cussed in Chapter 7. Other practical methods for solving a linear system
Ax = b make use of a factorization of A (QR decomposition, SVD decom-
position), using orthogonal matrices deﬁned next.
Given an m × n matrix A = (akl), the n × m matrix A⊤= (a⊤
ij) whose
ith row is the ith column of A, which means that a⊤
ij = aji for i = 1, . . . , n
and j = 1, . . . , m, is called the transpose of A. An n × n matrix Q such
that
QQ⊤= Q⊤Q = In
is called an orthogonal matrix. Equivalently, the inverse Q−1 of an orthog-
onal matrix Q is equal to its transpose Q⊤. Orthogonal matrices play an
important role. Geometrically, they correspond to linear transformation
that preserve length. A major result of linear algebra states that every
m × n matrix A can be written as
A = V ΣU ⊤,
where V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and
Σ is an m × n matrix whose only nonzero entries are nonnegative diagonal
entries σ1 ≥σ2 ≥· · · ≥σp, where p = min(m, n), called the singular values

2.1. Motivations: Linear Combinations, Linear Independence and Rank
13
of A. The factorization A = V ΣU ⊤is called a singular decomposition of
A, or SVD.
The SVD can be used to "solve" a linear system Ax = b where A is an
m × n matrix, even when this system has no solution. This may happen
when there are more equations that variables (m > n), in which case the
system is overdetermined.
Of course, there is no miracle, an unsolvable system has no solution.
But we can look for a good approximate solution, namely a vector x that
minimizes some measure of the error Ax −b. Legendre and Gauss used
∥Ax −b∥2
2, which is the squared Euclidean norm of the error. This quan-
tity is diﬀerentiable, and it turns out that there is a unique vector x+ of
minimum Euclidean norm that minimizes ∥Ax −b∥2
2. Furthermore, x+ is
given by the expression x+ = A+b, where A+ is the pseudo-inverse of
A, and A+ can be computed from an SVD A = V ΣU ⊤of A.
Indeed,
A+ = UΣ+V ⊤, where Σ+ is the matrix obtained from Σ by replacing every
positive singular value σi by its inverse σ−1
i
, leaving all zero entries intact,
and transposing.
Instead of searching for the vector of least Euclidean norm minimizing
∥Ax −b∥2
2, we can add the penalty term K ∥x∥2
2 (for some positive K > 0)
to ∥Ax −b∥2
2 and minimize the quantity ∥Ax −b∥2
2+K ∥x∥2
2. This approach
is called ridge regression. It turns out that there is a unique minimizer x+
given by x+ = (A⊤A + KIn)−1A⊤b, as shown in the second volume.
Another approach is to replace the penalty term K ∥x∥2
2 by K ∥x∥1,
where ∥x∥1 = |x1| + · · · + |xn| (the ℓ1-norm of x). The remarkable fact
is that the minimizers x of ∥Ax −b∥2
2 + K ∥x∥1 tend to be sparse, which
means that many components of x are equal to zero. This approach known
as lasso is popular in machine learning and will be discussed in the second
volume.
Another important application of the SVD is principal component anal-
ysis (or PCA), an important tool in data analysis.
Yet another fruitful way of interpreting the resolution of the system
Ax = b is to view this problem as an intersection problem. Indeed, each of
the equations
x1 + 2x2 −x3 = 1
2x1 + x2 + x3 = 2
x1 −2x2 −2x3 = 3
deﬁnes a subset of R3 which is actually a plane. The ﬁrst equation
x1 + 2x2 −x3 = 1

14
Vector Spaces, Bases, Linear Maps
deﬁnes the plane H1 passing through the three points (1, 0, 0), (0, 1/2, 0),
(0, 0, −1), on the coordinate axes, the second equation
2x1 + x2 + x3 = 2
deﬁnes the plane H2 passing through the three points (1, 0, 0), (0, 2, 0),
(0, 0, 2), on the coordinate axes, and the third equation
x1 −2x2 −2x3 = 3
deﬁnes the plane H3 passing through the three points (3, 0, 0), (0, −3/2, 0),
(0, 0, −3/2), on the coordinate axes. See Figure 2.1.
2x + 2x - x = 1
1
2
3
2x + x + x = 2
1
2
3
x -2x -2x = 3
1
2
3
Fig. 2.1
The planes deﬁned by the preceding linear equations.
The intersection Hi ∩Hj of any two distinct planes Hi and Hj is a line,
and the intersection H1 ∩H2 ∩H3 of the three planes consists of the single
point (1.4, −0.4, −0.4), as illustrated in Figure 2.2.
The planes corresponding to the system
x1 + 2x2 −x3 = 1
2x1 + x2 + x3 = 2
x1 −x2 + 2x3 = 3,
are illustrated in Figure 2.3. This system has no solution since there is no
point simultaneously contained in all three planes; see Figure 2.4.

2.1. Motivations: Linear Combinations, Linear Independence and Rank
15
x -2x -2x = 3
1
2
3
2x + x + x = 2
1
2
3
2x + 2x - x = 1
1
2
3
(1 . 4 ,  -0 . 4 ,  -0 . 4 )
Fig. 2.2
The solution of the system is the point in common with each of the three
planes.
2x + 2x - x = 1
1
2
3
2x + x + x = 2
1
2
3
1
2
3
1
2
3
x - x +2x = 3
1
2
3
Fig. 2.3
The planes deﬁned by the equations x1 + 2x2 −x3 = 1, 2x1 + x2 + x3 = 2,
and x1 −x2 + 2x3 = 3.
Finally, the planes corresponding to the system
x1 + 2x2 −x3 = 3
2x1 + x2 + x3 = 3
x1 −x2 + 2x3 = 0,

16
Vector Spaces, Bases, Linear Maps
2x + 2x - x = 1
1
2
3
x - x +2x = 3
1
2
3
2x + x + x = 2
1
2
3
2x + x + x = 2
1
2
3
Fig. 2.4
The linear system x1 + 2x2 −x3 = 1, 2x1 + x2 + x3 = 2, x1 −x2 + 2x3 = 3
has no solution.
2x + 2x -  x = 3
1
1
1
2
2
3
3
1
2x + x + x = 3
1
2
3
2
3
x - x + 2x = 0
1
2
3
1
Fig. 2.5
The planes deﬁned by the equations x1 + 2x2 −x3 = 3, 2x1 + x2 + x3 = 3,
and x1 −x2 + 2x3 = 0.
are illustrated in Figure 2.5.
This system has inﬁnitely many solutions, given parametrically by (1 −
x3, 1 + x3, x3). Geometrically, this is a line common to all three planes; see
Figure 2.6.
Under the above interpretation, observe that we are focusing on the
rows of the matrix A, rather than on its columns, as in the previous inter-
pretations.

2.1. Motivations: Linear Combinations, Linear Independence and Rank
17
2x + 2x -  x = 3
1
2
3
x - x + 2x = 0
1
2
3
1
2x + x + x = 3
2
3
Fig. 2.6
The linear system x1 + 2x2 −x3 = 3, 2x1 + x2 + x3 = 3, x1 −x2 + 2x3 = 0
has the red line common to all three planes.
Another great example of a real-world problem where linear algebra
proves to be very eﬀective is the problem of data compression, that is, of
representing a very large data set using a much smaller amount of storage.
Typically the data set is represented as an m × n matrix A where each
row corresponds to an n-dimensional data point and typically, m ≥n. In
most applications, the data are not independent so the rank of A is a lot
smaller than min{m, n}, and the the goal of low-rank decomposition is to
factor A as the product of two matrices B and C, where B is a m × k
matrix and C is a k × n matrix, with k ≪min{m, n} (here, ≪means
"much smaller than"):











A
m × n











=











B
m × k













C
k × n

.
Now it is generally too costly to ﬁnd an exact factorization as above,
so we look for a low-rank matrix A′ which is a "good" approximation of
A. In order to make this statement precise, we need to deﬁne a mechanism
to determine how close two matrices are. This can be done using matrix
norms, a notion discussed in Chapter 8.
The norm of a matrix A is a
nonnegative real number ∥A∥which behaves a lot like the absolute value

18
Vector Spaces, Bases, Linear Maps
|x| of a real number x. Then our goal is to ﬁnd some low-rank matrix A′
that minimizes the norm
∥A −A′∥2 ,
over all matrices A′ of rank at most k, for some given k ≪min{m, n}.
Some advantages of a low-rank approximation are:
(1) Fewer elements are required to represent A; namely, k(m+n) instead of
mn. Thus less storage and fewer operations are needed to reconstruct
A.
(2) Often, the process for obtaining the decomposition exposes the under-
lying structure of the data. Thus, it may turn out that "most" of the
signiﬁcant data are concentrated along some directions called principal
directions.
Low-rank decompositions of a set of data have a multitude of applica-
tions in engineering, including computer science (especially computer vi-
sion), statistics, and machine learning. As we will see later in Chapter 21,
the singular value decomposition (SVD) provides a very satisfactory so-
lution to the low-rank approximation problem. Still, in many cases, the
data sets are so large that another ingredient is needed: randomization.
However, as a ﬁrst step, linear algebra often yields a good initial solution.
We will now be more precise as to what kinds of operations are allowed
on vectors. In the early 1900, the notion of a vector space emerged as a
convenient and unifying framework for working with "linear" objects and
we will discuss this notion in the next few sections.
2.2
Vector Spaces
A (real) vector space is a set E together with two operations, +: E ×
E →E and ·: R × E →E, called addition and scalar multiplication, that
satisfy some simple properties. First of all, E under addition has to be a
commutative (or abelian) group, a notion that we review next.
However, keep in mind that vector spaces
are not just algebraic objects; they are also
geometric objects.
Deﬁnition 2.1. A group is a set G equipped with a binary operation ·: G×
G →G that associates an element a·b ∈G to every pair of elements a, b ∈G,

2.2. Vector Spaces
19
and having the following properties: · is associative, has an identity element
e ∈G, and every element in G is invertible (w.r.t. ·). More explicitly, this
means that the following equations hold for all a, b, c ∈G:
(G1) a · (b · c) = (a · b) · c.
(associativity);
(G2) a · e = e · a = a.
(identity);
(G3) For every a ∈G, there is some a−1 ∈G such that
a · a−1 = a−1 · a = e.
(inverse).
A group G is abelian (or commutative) if
a · b = b · a
for all a, b ∈G.
A set M together with an operation ·: M × M →M and an element e
satisfying only Conditions (G1) and (G2) is called a monoid. For example,
the set N = {0, 1, . . . , n, . . .} of natural numbers is a (commutative) monoid
under addition with identity element 0. However, it is not a group.
Some examples of groups are given below.
Example 2.1.
(1) The set Z = {. . . , −n, . . . , −1, 0, 1, . . . , n, . . .} of integers is an abelian
group under addition, with identity element 0. However, Z∗= Z−{0}
is not a group under multiplication; it is a commutative monoid with
identity element 1.
(2) The set Q of rational numbers (fractions p/q with p, q ∈Z and q ̸= 0)
is an abelian group under addition, with identity element 0. The set
Q∗= Q −{0} is also an abelian group under multiplication, with
identity element 1.
(3) Similarly, the sets R of real numbers and C of complex numbers are
abelian groups under addition (with identity element 0), and R∗=
R −{0} and C∗= C −{0} are abelian groups under multiplication
(with identity element 1).
(4) The sets Rn and Cn of n-tuples of real or complex numbers are abelian
groups under componentwise addition:
(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn),
with identity element (0, . . . , 0).
(5) Given any nonempty set S, the set of bijections f : S →S, also called
permutations of S, is a group under function composition (i.e., the
multiplication of f and g is the composition g ◦f), with identity ele-
ment the identity function idS. This group is not abelian as soon as
S has more than two elements.

20
Vector Spaces, Bases, Linear Maps
(6) The set of n × n matrices with real (or complex) coeﬃcients is an
abelian group under addition of matrices, with identity element the
null matrix. It is denoted by Mn(R) (or Mn(C)).
(7) The set R[X] of all polynomials in one variable X with real coeﬃcients,
P(X) = anXn + an−1Xn−1 + · · · + a1X + a0,
(with ai ∈R), is an abelian group under addition of polynomials. The
identity element is the zero polynomial.
(8) The set of n×n invertible matrices with real (or complex) coeﬃcients is
a group under matrix multiplication, with identity element the identity
matrix In. This group is called the general linear group and is usually
denoted by GL(n, R) (or GL(n, C)).
(9) The set of n×n invertible matrices with real (or complex) coeﬃcients
and determinant +1 is a group under matrix multiplication, with iden-
tity element the identity matrix In. This group is called the special
linear group and is usually denoted by SL(n, R) (or SL(n, C)).
(10) The set of n × n invertible matrices with real coeﬃcients such that
RR⊤= R⊤R = In and of determinant +1 is a group (under matrix
multiplication) called the special orthogonal group and is usually de-
noted by SO(n) (where R⊤is the transpose of the matrix R, i.e., the
rows of R⊤are the columns of R). It corresponds to the rotations in
Rn.
(11) Given an open interval (a, b), the set C(a, b) of continuous functions
f : (a, b) →R is an abelian group under the operation f + g deﬁned
such that
(f + g)(x) = f(x) + g(x)
for all x ∈(a, b).
It is customary to denote the operation of an abelian group G by +, in
which case the inverse a−1 of an element a ∈G is denoted by −a.
The identity element of a group is unique. In fact, we can prove a more
general fact:
Proposition 2.1. If a binary operation ·: M × M →M is associative and
if e′ ∈M is a left identity and e′′ ∈M is a right identity, which means that
e′ · a = a
for all
a ∈M
(2.1)
and
a · e′′ = a
for all
a ∈M,
(2.2)
then e′ = e′′.

2.2. Vector Spaces
21
Proof. If we let a = e′′ in equation (2.1), we get
e′ · e′′ = e′′,
and if we let a = e′ in equation (2.2), we get
e′ · e′′ = e′,
and thus
e′ = e′ · e′′ = e′′,
as claimed.
Proposition 2.1 implies that the identity element of a monoid is unique,
and since every group is a monoid, the identity element of a group is unique.
Furthermore, every element in a group has a unique inverse.
This is a
consequence of a slightly more general fact:
Proposition 2.2. In a monoid M with identity element e, if some element
a ∈M has some left inverse a′ ∈M and some right inverse a′′ ∈M, which
means that
a′ · a = e
(2.3)
and
a · a′′ = e,
(2.4)
then a′ = a′′.
Proof. Using (2.3) and the fact that e is an identity element, we have
(a′ · a) · a′′ = e · a′′ = a′′.
Similarly, using (2.4) and the fact that e is an identity element, we have
a′ · (a · a′′) = a′ · e = a′.
However, since M is monoid, the operation · is associative, so
a′ = a′ · (a · a′′) = (a′ · a) · a′′ = a′′,
as claimed.
Remark: Axioms (G2) and (G3) can be weakened a bit by requiring only
(2.2) (the existence of a right identity) and (2.4) (the existence of a right
inverse for every element) (or (2.1) and (2.3)). It is a good exercise to prove
that the group axioms (G2) and (G3) follow from (2.2) and (2.4).

22
Vector Spaces, Bases, Linear Maps
A vector space is an abelian group E with an additional operation ·: K×
E →E called scalar multiplication that allows rescaling a vector in E by
an element in K. The set K itself is an algebraic structure called a ﬁeld. A
ﬁeld is a special kind of structure called a ring. These notions are deﬁned
below. We begin with rings.
Deﬁnition 2.2. A ring is a set A equipped with two operations +: A×A →
A (called addition) and ∗: A × A →A (called multiplication) having the
following properties:
(R1) A is an abelian group w.r.t. +;
(R2) ∗is associative and has an identity element 1 ∈A;
(R3) ∗is distributive w.r.t. +.
The identity element for addition is denoted 0, and the additive inverse
of a ∈A is denoted by −a. More explicitly, the axioms of a ring are the
following equations which hold for all a, b, c ∈A:
a + (b + c) = (a + b) + c
(associativity of +)
(2.5)
a + b = b + a
(commutativity of +)
(2.6)
a + 0 = 0 + a = a
(zero)
(2.7)
a + (−a) = (−a) + a = 0
(additive inverse)
(2.8)
a ∗(b ∗c) = (a ∗b) ∗c
(associativity of ∗)
(2.9)
a ∗1 = 1 ∗a = a
(identity for ∗)
(2.10)
(a + b) ∗c = (a ∗c) + (b ∗c)
(distributivity)
(2.11)
a ∗(b + c) = (a ∗b) + (a ∗c)
(distributivity)
(2.12)
The ring A is commutative if
a ∗b = b ∗a
for all a, b ∈A.
From (2.11) and (2.12), we easily obtain
a ∗0 = 0 ∗a = 0
(2.13)
a ∗(−b) = (−a) ∗b = −(a ∗b).
(2.14)
Note that (2.13) implies that if 1 = 0, then a = 0 for all a ∈A, and
thus, A = {0}. The ring A = {0} is called the trivial ring. A ring for which
1 ̸= 0 is called nontrivial. The multiplication a ∗b of two elements a, b ∈A
is often denoted by ab.
The abelian group Z is a commutative ring (with unit 1), and for any
ﬁeld K, the abelian group K[X] of polynomials is also a commutative ring

2.2. Vector Spaces
23
(also with unit 1).
The set Z/mZ of residues modulo m where m is a
positive integer is a commutative ring.
A ﬁeld is a commutative ring K for which K −{0} is a group under
multiplication.
Deﬁnition 2.3. A set K is a ﬁeld if it is a ring and the following properties
hold:
(F1) 0 ̸= 1;
(F2) K∗= K −{0} is a group w.r.t. ∗(i.e., every a ̸= 0 has an inverse
w.r.t. ∗);
(F3) ∗is commutative.
If ∗is not commutative but (F1) and (F2) hold, we say that we have a
skew ﬁeld (or noncommutative ﬁeld).
Note that we are assuming that the operation ∗of a ﬁeld is commutative.
This convention is not universally adopted, but since ∗will be commutative
for most ﬁelds we will encounter, we may as well include this condition in
the deﬁnition.
Example 2.2.
(1) The rings Q, R, and C are ﬁelds.
(2) The set Z/pZ of residues modulo p where p is a prime number is ﬁeld.
(3) The set of (formal) fractions f(X)/g(X) of polynomials f(X), g(X) ∈
R[X], where g(X) is not the zero polynomial, is a ﬁeld.
Vector spaces are deﬁned as follows.
Deﬁnition 2.4. A real vector space is a set E (of vectors) together with two
operations +: E×E →E (called vector addition)1 and ·: R×E →E (called
scalar multiplication) satisfying the following conditions for all α, β ∈R and
all u, v ∈E;
(V0) E is an abelian group w.r.t. +,with identity element 0;2
(V1) α · (u + v) = (α · u) + (α · v);
(V2) (α + β) · u = (α · u) + (β · u);
1The symbol + is overloaded, since it denotes both addition in the ﬁeld R and addition
of vectors in E. It is usually clear from the context which + is intended.
2The symbol 0 is also overloaded, since it represents both the zero in R (a scalar) and
the identity element of E (the zero vector). Confusion rarely arises, but one may prefer
using 0 for the zero vector.

24
Vector Spaces, Bases, Linear Maps
(V3) (α ∗β) · u = α · (β · u);
(V4) 1 · u = u.
In (V3), ∗denotes multiplication in R.
Given α ∈R and v ∈E, the element α · v is also denoted by αv. The
ﬁeld R is often called the ﬁeld of scalars.
In Deﬁnition 2.4, the ﬁeld R may be replaced by the ﬁeld of complex
numbers C, in which case we have a complex vector space. It is even possible
to replace R by the ﬁeld of rational numbers Q or by any arbitrary ﬁeld K
(for example Z/pZ, where p is a prime number), in which case we have a
K-vector space (in (V3), ∗denotes multiplication in the ﬁeld K). In most
cases, the ﬁeld K will be the ﬁeld R of reals, but all results in this chapter
hold for vector spaces over an arbitrary ﬁeld.
From (V0), a vector space always contains the null vector 0, and thus
is nonempty. From (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From
(V2), we get 0 · v = 0, and (−α) · v = −(α · v).
Another important consequence of the axioms is the following fact:
Proposition 2.3. For any u ∈E and any λ ∈R, if λ ̸= 0 and λ · u = 0,
then u = 0.
Proof. Indeed, since λ ̸= 0, it has a multiplicative inverse λ−1, so from
λ · u = 0, we get
λ−1 · (λ · u) = λ−1 · 0.
However, we just observed that λ−1 · 0 = 0, and from (V3) and (V4), we
have
λ−1 · (λ · u) = (λ−1λ) · u = 1 · u = u,
and we deduce that u = 0.
Remark: One may wonder whether axiom (V4) is really needed. Could
it be derived from the other axioms? The answer is no. For example, one
can take E = Rn and deﬁne ·: R × Rn →Rn by
λ · (x1, . . . , xn) = (0, . . . , 0)
for all (x1, . . . , xn) ∈Rn and all λ ∈R. Axioms (V0)-(V3) are all satisﬁed,
but (V4) fails. Less trivial examples can be given using the notion of a
basis, which has not been deﬁned yet.

2.2. Vector Spaces
25
The ﬁeld R itself can be viewed as a vector space over itself, addition
of vectors being addition in the ﬁeld, and multiplication by a scalar being
multiplication in the ﬁeld.
Example 2.3.
(1) The ﬁelds R and C are vector spaces over R.
(2) The groups Rn and Cn are vector spaces over R, with scalar multipli-
cation given by
λ(x1, . . . , xn) = (λx1, . . . , λxn),
for any λ ∈R and with (x1, . . . , xn) ∈Rn or (x1, . . . , xn) ∈Cn, and Cn
is a vector space over C with scalar multiplication as above, but with
λ ∈C.
(3) The ring R[X]n of polynomials of degree at most n with real coeﬃcients
is a vector space over R, and the ring C[X]n of polynomials of degree at
most n with complex coeﬃcients is a vector space over C, with scalar
multiplication λ · P(X) of a polynomial
P(X) = amXm + am−1Xm−1 + · · · + a1X + a0
(with ai ∈R or ai ∈C) by the scalar λ (in R or C), with m ≤n, given
by
λ · P(X) = λamXm + λam−1Xm−1 + · · · + λa1X + λa0.
(4) The ring R[X] of all polynomials with real coeﬃcients is a vector space
over R, and the ring C[X] of all polynomials with complex coeﬃcients
is a vector space over C, with the same scalar multiplication as above.
(5) The ring of n × n matrices Mn(R) is a vector space over R.
(6) The ring of m × n matrices Mm,n(R) is a vector space over R.
(7) The ring C(a, b) of continuous functions f : (a, b) →R is a vector space
over R, with the scalar multiplication λf of a function f : (a, b) →R
by a scalar λ ∈R given by
(λf)(x) = λf(x),
for all x ∈(a, b).
(8) A very important example of vector space is the set of linear maps
between two vector spaces to be deﬁned in Section 2.7.
Here is an
example that will prepare us for the vector space of linear maps. Let
X be any nonempty set and let E be a vector space. The set of all
functions f : X →E can be made into a vector space as follows: Given

26
Vector Spaces, Bases, Linear Maps
any two functions f : X →E and g: X →E, let (f + g): X →E be
deﬁned such that
(f + g)(x) = f(x) + g(x)
for all x ∈X, and for every λ ∈R, let λf : X →E be deﬁned such that
(λf)(x) = λf(x)
for all x ∈X. The axioms of a vector space are easily veriﬁed.
Let E be a vector space. We would like to deﬁne the important notions
of linear combination and linear independence.
Before deﬁning these notions, we need to discuss a strategic choice
which, depending how it is settled, may reduce or increase headaches in
dealing with notions such as linear combinations and linear dependence
(or independence). The issue has to do with using sets of vectors versus
sequences of vectors.
2.3
Indexed Families; the Sum Notation P
i∈I ai
Our experience tells us that it is preferable to use sequences of vectors;
even better, indexed families of vectors. (We are not alone in having opted
for sequences over sets, and we are in good company; for example, Artin
[Artin (1991)], Axler [Axler (2004)], and Lang [Lang (1993)] use sequences.
Nevertheless, some prominent authors such as Lax [Lax (2007)] use sets.
We leave it to the reader to conduct a survey on this issue.)
Given a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈
An of elements from A, for some natural number n. The elements of a se-
quence need not be distinct and the order is important.
For example,
(a1, a2, a1) and (a2, a1, a1) are two distinct sequences in A3. Their under-
lying set is {a1, a2}.
What we just deﬁned are ﬁnite sequences, which can also be viewed as
functions from {1, 2, . . . , n} to the set A; the ith element of the sequence
(a1, . . . , an) is the image of i under the function. This viewpoint is fruitful,
because it allows us to deﬁne (countably) inﬁnite sequences as functions
s: N →A. But then, why limit ourselves to ordered sets such as {1, . . . , n}
or N as index sets?
The main role of the index set is to tag each element uniquely, and the
order of the tags is not crucial, although convenient. Thus, it is natural to
deﬁne the notion of indexed family.
Deﬁnition 2.5. Given a set A, an I-indexed family of elements of A, for
short a family, is a function a: I →A where I is any set viewed as an index

2.3. Indexed Families; the Sum Notation P
i∈I ai
27
set. Since the function a is determined by its graph
{(i, a(i)) | i ∈I},
the family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈I}. For
notational simplicity, we write ai instead of a(i), and denote the family
a = {(i, a(i)) | i ∈I} by (ai)i∈I.
For example, if I = {r, g, b, y} and A = N, the set of pairs
a = {(r, 2), (g, 3), (b, 2), (y, 11)}
is an indexed family. The element 2 appears twice in the family with the
two distinct tags r and b.
When the indexed set I is totally ordered, a family (ai)i∈I is often called
an I-sequence. Interestingly, sets can be viewed as special cases of families.
Indeed, a set A can be viewed as the A-indexed family {(a, a) | a ∈I}
corresponding to the identity function.
Remark: An indexed family should not be confused with a multiset. Given
any set A, a multiset is a similar to a set, except that elements of A may oc-
cur more than once. For example, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d}
is a multiset. Each element appears with a certain multiplicity, but the or-
der of the elements does not matter. For example, a has multiplicity 3.
Formally, a multiset is a function s: A →N, or equivalently a set of pairs
{(a, i) | a ∈A}. Thus, a multiset is an A-indexed family of elements from
N, but not a N-indexed family, since distinct elements may have the same
multiplicity (such as c an d in the example above). An indexed family is a
generalization of a sequence, but a multiset is a generalization of a set.
We also need to take care of an annoying technicality, which is to deﬁne
sums of the form P
i∈I ai, where I is any ﬁnite index set and (ai)i∈I is a
family of elements in some set A equiped with a binary operation +: A ×
A →A which is associative (Axiom (G1)) and commutative.
This will
come up when we deﬁne linear combinations.
The issue is that the binary operation + only tells us how to compute
a1 + a2 for two elements of A, but it does not tell us what is the sum of
three of more elements. For example, how should a1 + a2 + a3 be deﬁned?
What we have to do is to deﬁne a1 +a2 +a3 by using a sequence of steps
each involving two elements, and there are two possible ways to do this:
a1 +(a2 +a3) and (a1 +a2)+a3. If our operation + is not associative, these
are diﬀerent values. If it associative, then a1 + (a2 + a3) = (a1 + a2) + a3,
but then there are still six possible permutations of the indices 1, 2, 3, and if

28
Vector Spaces, Bases, Linear Maps
+ is not commutative, these values are generally diﬀerent. If our operation
is commutative, then all six permutations have the same value. Thus, if
+ is associative and commutative, it seems intuitively clear that a sum of
the form P
i∈I ai does not depend on the order of the operations used to
compute it.
This is indeed the case, but a rigorous proof requires induction, and
such a proof is surprisingly involved. Readers may accept without proof
the fact that sums of the form P
i∈I ai are indeed well deﬁned, and jump
directly to Deﬁnition 2.6. For those who want to see the gory details, here
we go.
First, we deﬁne sums P
i∈I ai, where I is a ﬁnite sequence of distinct
natural numbers, say I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥2, we
denote the sequence (i2, . . . , im) by I −{i1}. We proceed by induction on
the size m of I. Let
X
i∈I
ai = ai1,
if m = 1,
X
i∈I
ai = ai1 +

X
i∈I−{i1}
ai

,
if m > 1.
For example, if I = (1, 2, 3, 4), we have
X
i∈I
ai = a1 + (a2 + (a3 + a4)).
If the operation + is not associative, the grouping of the terms matters.
For instance, in general
a1 + (a2 + (a3 + a4)) ̸= (a1 + a2) + (a3 + a4).
However, if the operation + is associative, the sum P
i∈I ai should not
depend on the grouping of the elements in I, as long as their order is
preserved. For example, if I = (1, 2, 3, 4, 5), J1 = (1, 2), and J2 = (3, 4, 5),
we expect that
X
i∈I
ai =
X
j∈J1
aj

+
X
j∈J2
aj

.
This indeed the case, as we have the following proposition.
Proposition 2.4. Given any nonempty set A equipped with an associative
binary operation +: A×A →A, for any nonempty ﬁnite sequence I of dis-
tinct natural numbers and for any partition of I into p nonempty sequences

2.3. Indexed Families; the Sum Notation P
i∈I ai
29
Ik1, . . . , Ikp, for some nonempty sequence K = (k1, . . . , kp) of distinct nat-
ural numbers such that ki < kj implies that α < β for all α ∈Iki and all
β ∈Ikj, for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
k∈K
 X
α∈Ik
aα

.
Proof. We proceed by induction on the size n of I.
If n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds
trivially.
Next, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial,
so assume that p ≥2 and write J = (k2, . . . , kp). There are two cases.
Case 1. The sequence Ik1 has a single element, say β, which is the ﬁrst
element of I. In this case, write C for the sequence obtained from I by
deleting its ﬁrst element β. By deﬁnition,
X
α∈I
aα = aβ +
X
α∈C
aα

,
and
X
k∈K
 X
α∈Ik
aα

= aβ +
X
j∈J
 X
α∈Ij
aα

.
Since |C| = n −1, by the induction hypothesis, we have
X
α∈C
aα

=
X
j∈J
 X
α∈Ij
aα

,
which yields our identity.
Case 2. The sequence Ik1 has at least two elements. In this case, let β
be the ﬁrst element of I (and thus of Ik1), let I′ be the sequence obtained
from I by deleting its ﬁrst element β, let I′
k1 be the sequence obtained from
Ik1 by deleting its ﬁrst element β, and let I′
ki = Iki for i = 2, . . . , p. Recall
that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I′ has n −1
elements, so by the induction hypothesis applied to I′ and the I′
ki, we get
X
α∈I′
aα =
X
k∈K
 X
α∈I′
k
aα

=
 X
α∈I′
k1
aα

+
X
j∈J
 X
α∈Ij
aα

.
If we add the left-hand side to aβ, by deﬁnition we get
X
α∈I
aα.

30
Vector Spaces, Bases, Linear Maps
If we add the right-hand side to aβ, using associativity and the deﬁnition
of an indexed sum, we get
aβ +
 X
α∈I′
k1
aα

+
X
j∈J
 X
α∈Ij
aα

=

aβ +
 X
α∈I′
k1
aα

+
X
j∈J
 X
α∈Ij
aα

=
 X
α∈Ik1
aα

+
X
j∈J
 X
α∈Ij
aα

=
X
k∈K
 X
α∈Ik
aα

,
as claimed.
If I = (1, . . . , n), we also write Pn
i=1 ai instead of P
i∈I ai. Since + is
associative, Proposition 2.4 shows that the sum Pn
i=1 ai is independent of
the grouping of its elements, which justiﬁes the use the notation a1+· · ·+an
(without any parentheses).
If we also assume that our associative binary operation on A is com-
mutative, then we can show that the sum P
i∈I ai does not depend on the
ordering of the index set I.
Proposition 2.5. Given any nonempty set A equipped with an associative
and commutative binary operation +: A × A →A, for any two nonempty
ﬁnite sequences I and J of distinct natural numbers such that J is a per-
mutation of I (in other words, the underlying sets of I and J are identical),
for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
α∈J
aα.
Proof. We proceed by induction on the number p of elements in I.
If
p = 1, we have I = J and the proposition holds trivially.
If p > 1, to simplify notation, assume that I = (1, . . . , p) and that
J is a permutation (i1, . . . , ip) of I. First, assume that 2 ≤i1 ≤p −1,
let J′ be the sequence obtained from J by deleting i1, I′ be the sequence
obtained from I by deleting i1, and let P = (1, 2, . . . , i1 −1) and Q =
(i1 + 1, . . . , p −1, p). Observe that the sequence I′ is the concatenation of
the sequences P and Q. By the induction hypothesis applied to J′ and I′,
and then by Proposition 2.4 applied to I′ and its partition (P, Q), we have
X
α∈J′
aα =
X
α∈I′
aα =
i1−1
X
i=1
ai

+

p
X
i=i1+1
ai

.

2.3. Indexed Families; the Sum Notation P
i∈I ai
31
If we add the left-hand side to ai1, by deﬁnition we get
X
α∈J
aα.
If we add the right-hand side to ai1, we get
ai1 +
i1−1
X
i=1
ai

+

p
X
i=i1+1
ai

.
Using associativity, we get
ai1 +
i1−1
X
i=1
ai

+

p
X
i=i1+1
ai

=

ai1 +
i1−1
X
i=1
ai

+

p
X
i=i1+1
ai

,
then using associativity and commutativity several times (more rigorously,
using induction on i1 −1), we get

ai1 +
i1−1
X
i=1
ai

+

p
X
i=i1+1
ai

=
i1−1
X
i=1
ai

+ ai1 +

p
X
i=i1+1
ai

=
p
X
i=1
ai,
as claimed.
The cases where i1 = 1 or i1 = p are treated similarly, but in a sim-
pler manner since either P = () or Q = () (where () denotes the empty
sequence).
Having done all this, we can now make sense of sums of the form P
i∈I ai,
for any ﬁnite indexed set I and any family a = (ai)i∈I of elements in A,
where A is a set equipped with a binary operation + which is associative
and commutative.
Indeed, since I is ﬁnite, it is in bijection with the set {1, . . . , n} for some
n ∈N, and any total ordering ⪯on I corresponds to a permutation I⪯of
{1, . . . , n} (where we identify a permutation with its image). For any total
ordering ⪯on I, we deﬁne P
i∈I,⪯ai as
X
i∈I,⪯
ai =
X
j∈I⪯
aj.
Then for any other total ordering ⪯′ on I, we have
X
i∈I,⪯′
ai =
X
j∈I⪯′
aj,

32
Vector Spaces, Bases, Linear Maps
and since I⪯and I⪯′ are diﬀerent permutations of {1, . . . , n}, by Proposi-
tion 2.5, we have
X
j∈I⪯
aj =
X
j∈I⪯′
aj.
Therefore, the sum P
i∈I,⪯ai does not depend on the total ordering on I.
We deﬁne the sum P
i∈I ai as the common value P
i∈I,⪯ai for all total
orderings ⪯of I.
Here are some examples with A = R:
(1) If I = {1, 2, 3}, a = {(1, 2), (2, −3), (3,
√
2)}, then P
i∈I ai = 2 −3 +
√
2 = −1 +
√
2.
(2) If I = {2, 5, 7}, a = {(2, 2), (5, −3), (7,
√
2)}, then P
i∈I ai = 2 −3 +
√
2 = −1 +
√
2.
(3) If I = {r, g, b}, a = {(r, 2), (g, −3), (b, 1)}, then P
i∈I ai = 2−3+1 = 0.
2.4
Linear Independence, Subspaces
One of the most useful properties of vector spaces is that they possess
bases. What this means is that in every vector space E, there is some set
of vectors, {e1, . . . , en}, such that every vector v ∈E can be written as a
linear combination,
v = λ1e1 + · · · + λnen,
of the ei, for some scalars, λ1, . . . , λn ∈R.
Furthermore, the n-tuple,
(λ1, . . . , λn), as above is unique.
This description is ﬁne when E has a ﬁnite basis, {e1, . . . , en}, but this
is not always the case! For example, the vector space of real polynomials,
R[X], does not have a ﬁnite basis but instead it has an inﬁnite basis, namely
1, X, X2, . . . , Xn, . . .
Given a set A, recall that an I-indexed family (ai)i∈I of elements of A
(for short, a family) is a function a: I →A, or equivalently a set of pairs
{(i, ai) | i ∈I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is
ﬁnite if I is ﬁnite.
Remark: When considering a family (ai)i∈I, there is no reason to assume
that I is ordered. The crucial point is that every element of the family is
uniquely indexed by an element of I. Thus, unless speciﬁed otherwise, we
do not assume that the elements of an index set are ordered.

2.4. Linear Independence, Subspaces
33
Given two disjoint sets I and J, the union of two families (ui)i∈I and
(vj)j∈J, denoted as (ui)i∈I ∪(vj)j∈J, is the family (wk)k∈(I∪J) deﬁned such
that wk = uk if k ∈I, and wk = vk if k ∈J. Given a family (ui)i∈I and
any element v, we denote by (ui)i∈I ∪k (v) the family (wi)i∈I∪{k} deﬁned
such that, wi = ui if i ∈I, and wk = v, where k is any index such that
k /∈I. Given a family (ui)i∈I, a subfamily of (ui)i∈I is a family (uj)j∈J
where J is any subset of I.
In this chapter, unless speciﬁed otherwise, it is assumed that all families
of scalars are ﬁnite (i.e., their index set is ﬁnite).
Deﬁnition 2.6. Let E be a vector space.
A vector v ∈E is a linear
combination of a family (ui)i∈I of elements of E iﬀthere is a family (λi)i∈I
of scalars in R such that
v =
X
i∈I
λiui.
When I = ∅, we stipulate that v = 0. (By Proposition 2.5, sums of the
form P
i∈I λiui are well deﬁned.) We say that a family (ui)i∈I is linearly
independent iﬀfor every family (λi)i∈I of scalars in R,
X
i∈I
λiui = 0
implies that
λi = 0 for all i ∈I.
Equivalently, a family (ui)i∈I is linearly dependent iﬀthere is some family
(λi)i∈I of scalars in R such that
X
i∈I
λiui = 0
and
λj ̸= 0 for some j ∈I.
We agree that when I = ∅, the family ∅is linearly independent.
Observe that deﬁning linear combinations for families of vectors rather
than for sets of vectors has the advantage that the vectors being combined
need not be distinct. For example, for I = {1, 2, 3} and the families (u, v, u)
and (λ1, λ2, λ1), the linear combination
X
i∈I
λiui = λ1u + λ2v + λ1u
makes sense. Using sets of vectors in the deﬁnition of a linear combination
does not allow such linear combinations; this is too restrictive.
Unravelling Deﬁnition 2.6, a family (ui)i∈I is linearly dependent iﬀei-
ther I consists of a single element, say i, and ui = 0, or |I| ≥2 and some uj
in the family can be expressed as a linear combination of the other vectors

34
Vector Spaces, Bases, Linear Maps
in the family. Indeed, in the second case, there is some family (λi)i∈I of
scalars in R such that
X
i∈I
λiui = 0
and
λj ̸= 0 for some j ∈I,
and since |I| ≥2, the set I −{j} is nonempty and we get
uj =
X
i∈(I−{j})
−λ−1
j λiui.
Observe that one of the reasons for deﬁning linear dependence for fam-
ilies of vectors rather than for sets of vectors is that our deﬁnition allows
multiple occurrences of a vector. This is important because a matrix may
contain identical columns, and we would like to say that these columns are
linearly dependent. The deﬁnition of linear dependence for sets does not
allow us to do that.
The above also shows that a family (ui)i∈I is linearly independent iﬀ
either I = ∅, or I consists of a single element i and ui ̸= 0, or |I| ≥2 and
no vector uj in the family can be expressed as a linear combination of the
other vectors in the family.
When I is nonempty, if the family (ui)i∈I is linearly independent, note
that ui ̸= 0 for all i ∈I. Otherwise, if ui = 0 for some i ∈I, then we
get a nontrivial linear dependence P
i∈I λiui = 0 by picking any nonzero
λi and letting λk = 0 for all k ∈I with k ̸= i, since λi0 = 0. If |I| ≥2,
we must also have ui ̸= uj for all i, j ∈I with i ̸= j, since otherwise we
get a nontrivial linear dependence by picking λi = λ and λj = −λ for any
nonzero λ, and letting λk = 0 for all k ∈I with k ̸= i, j.
Thus, the deﬁnition of linear independence implies that a nontrivial
linearly independent family is actually a set. This explains why certain au-
thors choose to deﬁne linear independence for sets of vectors. The problem
with this approach is that linear dependence, which is the logical negation
of linear independence, is then only deﬁned for sets of vectors. However, as
we pointed out earlier, it is really desirable to deﬁne linear dependence for
families allowing multiple occurrences of the same vector.
Example 2.4.
(1) Any two distinct scalars λ, µ ̸= 0 in R are linearly dependent.
(2) In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly indepen-
dent. See Figure 2.7.
(3) In R4, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are
linearly independent.

2.4. Linear Independence, Subspaces
35
Fig. 2.7
A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),
and the blue vector (0, 0, 1) in R3.
(4) In R2, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly
dependent, since
w = 2u + v.
See Figure 2.8.
(2 , 3)
2u
v
w
Fig. 2.8
A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector
v = (0, 1), and the vector sum w = 2u + v.
When I is ﬁnite, we often assume that it is the set I = {1, 2, . . . , n}. In
this case, we denote the family (ui)i∈I as (u1, . . . , un).

36
Vector Spaces, Bases, Linear Maps
The notion of a subspace of a vector space is deﬁned as follows.
Deﬁnition 2.7. Given a vector space E, a subset F of E is a linear subspace
(or subspace) of E iﬀF is nonempty and λu + µv ∈F for all u, v ∈F, and
all λ, µ ∈R.
It is easy to see that a subspace F of E is indeed a vector space, since
the restriction of +: E ×E →E to F ×F is indeed a function +: F ×F →
F, and the restriction of ·: R × E →E to R × F is indeed a function
·: R × F →F.
Since a subspace F is nonempty, if we pick any vector u ∈F and if we
let λ = µ = 0, then λu + µu = 0u + 0u = 0, so every subspace contains the
vector 0.
The following facts also hold. The proof is left as an exercise.
Proposition 2.6.
(1) The intersection of any family (even inﬁnite) of subspaces of a vector
space E is a subspace.
(2) Let F be any subspace of a vector space E. For any nonempty ﬁnite
index set I, if (ui)i∈I is any family of vectors ui ∈F and (λi)i∈I is
any family of scalars, then P
i∈I λiui ∈F.
The subspace {0} will be denoted by (0), or even 0 (with a mild abuse
of notation).
Example 2.5.
(1) In R2, the set of vectors u = (x, y) such that
x + y = 0
is the subspace illustrated by Figure 2.9.
(2) In R3, the set of vectors u = (x, y, z) such that
x + y + z = 0
is the subspace illustrated by Figure 2.10.
(3) For any n ≥0, the set of polynomials f(X) ∈R[X] of degree at most
n is a subspace of R[X].
(4) The set of upper triangular n × n matrices is a subspace of the space
of n × n matrices.
Proposition 2.7. Given any vector space E, if S is any nonempty subset
of E, then the smallest subspace ⟨S⟩(or Span(S)) of E containing S is the
set of all (ﬁnite) linear combinations of elements from S.

2.4. Linear Independence, Subspaces
37
Fig. 2.9
The subspace x+y = 0 is the line through the origin with slope −1. It consists
of all vectors of the form λ(−1, 1).
Fig. 2.10
The subspace x + y + z = 0 is the plane through the origin with normal
(1, 1, 1).
Proof. We prove that the set Span(S) of all linear combinations of elements
of S is a subspace of E, leaving as an exercise the veriﬁcation that every
subspace containing S also contains Span(S).
First, Span(S) is nonempty since it contains S (which is nonempty).
If u = 
i∈I λiui and v = 
j∈J µjvj are any two linear combinations in

38
Vector Spaces, Bases, Linear Maps
Span(S), for any two scalars λ, µ ∈R,
λu + µv = λ
X
i∈I
λiui + µ
X
j∈J
µjvj
=
X
i∈I
λλiui +
X
j∈J
µµjvj
=
X
i∈I−J
λλiui +
X
i∈I∩J
(λλi + µµi)ui +
X
j∈J−I
µµjvj,
which is a linear combination with index set I ∪J, and thus λu + µv ∈
Span(S), which proves that Span(S) is a subspace.
One might wonder what happens if we add extra conditions to the
coeﬃcients involved in forming linear combinations. Here are three natural
restrictions which turn out to be important (as usual, we assume that our
index sets are ﬁnite):
(1) Consider combinations P
i∈I λiui for which
X
i∈I
λi = 1.
These are called aﬃne combinations. One should realize that every
linear combination P
i∈I λiui can be viewed as an aﬃne combination.
For example, if k is an index not in I, if we let J = I ∪{k}, uk = 0,
and λk = 1 −P
i∈I λi, then P
j∈J λjuj is an aﬃne combination and
X
i∈I
λiui =
X
j∈J
λjuj.
However, we get new spaces. For example, in R3, the set of all aﬃne
combinations of the three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 =
(0, 0, 1), is the plane passing through these three points. Since it does
not contain 0 = (0, 0, 0), it is not a linear subspace.
(2) Consider combinations P
i∈I λiui for which
λi ≥0,
for all i ∈I.
These are called positive (or conic) combinations. It turns out that
positive combinations of families of vectors are cones. They show up
naturally in convex optimization.
(3) Consider combinations P
i∈I λiui for which we require (1) and (2), that
is
X
i∈I
λi = 1,
and
λi ≥0
for all i ∈I.

2.5. Bases of a Vector Space
39
These are called convex combinations. Given any ﬁnite family of vec-
tors, the set of all convex combinations of these vectors is a convex
polyhedron. Convex polyhedra play a very important role in convex
optimization.
Remark: The notion of linear combination can also be deﬁned for inﬁnite
index sets I. To ensure that a sum P
i∈I λiui makes sense, we restrict our
attention to families of ﬁnite support.
Deﬁnition 2.8. Given any ﬁeld K, a family of scalars (λi)i∈I has ﬁnite
support if λi = 0 for all i ∈I −J, for some ﬁnite subset J of I.
If (λi)i∈I is a family of scalars of ﬁnite support, for any vector space
E over K, for any (possibly inﬁnite) family (ui)i∈I of vectors ui ∈E, we
deﬁne the linear combination P
i∈I λiui as the ﬁnite linear combination
P
j∈J λjuj, where J is any ﬁnite subset of I such that λi = 0 for all
i ∈I −J. In general, results stated for ﬁnite families also hold for families
of ﬁnite support.
2.5
Bases of a Vector Space
Given a vector space E, given a family (vi)i∈I, the subset V of E consisting
of the null vector 0 and of all linear combinations of (vi)i∈I is easily seen to
be a subspace of E. The family (vi)i∈I is an economical way of representing
the entire subspace V , but such a family would be even nicer if it was not
redundant. Subspaces having such an "eﬃcient" generating family (called
a basis) play an important role and motivate the following deﬁnition.
Deﬁnition 2.9. Given a vector space E and a subspace V of E, a family
(vi)i∈I of vectors vi ∈V spans V or generates V iﬀfor every v ∈V , there
is some family (λi)i∈I of scalars in R such that
v =
X
i∈I
λivi.
We also say that the elements of (vi)i∈I are generators of V and that V
is spanned by (vi)i∈I, or generated by (vi)i∈I.
If a subspace V of E is
generated by a ﬁnite family (vi)i∈I, we say that V is ﬁnitely generated. A
family (ui)i∈I that spans V and is linearly independent is called a basis
of V .

40
Vector Spaces, Bases, Linear Maps
Example 2.6.
(1) In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Fig-
ure 2.9, form a basis.
(2) The vectors (1, 1, 1, 1), (1, 1, −1, −1), (1, −1, 0, 0), (0, 0, 1, −1) form a ba-
sis of R4 known as the Haar basis. This basis and its generalization to
dimension 2n are crucial in wavelet theory.
(3) In the subspace of polynomials in R[X] of degree at most n, the poly-
nomials 1, X, X2, . . . , Xn form a basis.
(4) The Bernstein polynomials
n
k

(1 −X)n−kXk for k = 0, . . . , n, also
form a basis of that space. These polynomials play a major role in the
theory of spline curves.
The ﬁrst key result of linear algebra is that every vector space E has a
basis. We begin with a crucial lemma which formalizes the mechanism for
building a basis incrementally.
Lemma 2.1. Given a linearly independent family (ui)i∈I of elements of a
vector space E, if v ∈E is not a linear combination of (ui)i∈I, then the
family (ui)i∈I ∪k (v) obtained by adding v to the family (ui)i∈I is linearly
independent (where k /∈I).
Proof. Assume that µv + P
i∈I λiui = 0, for any family (λi)i∈I of scalars
in R.
If µ ̸= 0, then µ has an inverse (because R is a ﬁeld), and thus
we have v = −P
i∈I(µ−1λi)ui, showing that v is a linear combination of
(ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But then, we have
P
i∈I λiui = 0, and since the family (ui)i∈I is linearly independent, we have
λi = 0 for all i ∈I.
The next theorem holds in general, but the proof is more sophisticated
for vector spaces that do not have a ﬁnite set of generators. Thus, in this
chapter, we only prove the theorem for ﬁnitely generated vector spaces.
Theorem 2.1. Given any ﬁnite family S = (ui)i∈I generating a vector
space E and any linearly independent subfamily L = (uj)j∈J of S (where
J ⊆I), there is a basis B of E such that L ⊆B ⊆S.
Proof. Consider the set of linearly independent families B such that
L ⊆B ⊆S. Since this set is nonempty and ﬁnite, it has some maximal
element (that is, a subfamily B = (uh)h∈H of S with H ⊆I of maximum
cardinality), say B = (uh)h∈H. We claim that B generates E. Indeed,
if B does not generate E, then there is some up ∈S that is not a linear

2.5. Bases of a Vector Space
41
combination of vectors in B (since S generates E), with p /∈H. Then by
Lemma 2.1, the family B′ = (uh)h∈H∪{p} is linearly independent, and since
L ⊆B ⊂B′ ⊆S, this contradicts the maximality of B. Thus, B is a basis
of E such that L ⊆B ⊆S.
Remark: Theorem 2.1 also holds for vector spaces that are not ﬁnitely
generated. In this case, the problem is to guarantee the existence of a max-
imal linearly independent family B such that L ⊆B ⊆S. The existence of
such a maximal family can be shown using Zorn's lemma; see Lang [Lang
(1993)] (Theorem 5.1).
A situation where the full generality of Theorem 2.1 is needed is the
case of the vector space R over the ﬁeld of coeﬃcients Q. The numbers
1 and
√
2 are linearly independent over Q, so according to Theorem 2.1,
the linearly independent family L = (1,
√
2) can be extended to a basis B
of R. Since R is uncountable and Q is countable, such a basis must be
uncountable!
The notion of a basis can also be deﬁned in terms of the notion of
maximal linearly independent family and minimal generating family.
Deﬁnition 2.10. Let (vi)i∈I be a family of vectors in a vector space E. We
say that (vi)i∈I a maximal linearly independent family of E if it is linearly
independent, and if for any vector w ∈E, the family (vi)i∈I ∪k{w} obtained
by adding w to the family (vi)i∈I is linearly dependent. We say that (vi)i∈I
a minimal generating family of E if it spans E, and if for any index p ∈I,
the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does
not span E.
The following proposition giving useful properties characterizing a basis
is an immediate consequence of Lemma 2.1.
Proposition 2.8. Given a vector space E, for any family B = (vi)i∈I of
vectors of E, the following properties are equivalent:
(1) B is a basis of E.
(2) B is a maximal linearly independent family of E.
(3) B is a minimal generating family of E.
Proof. We will ﬁrst prove the equivalence of (1) and (2).
Assume (1).
Since B is a basis, it is a linearly independent family. We claim that B
is a maximal linearly independent family. If B is not a maximal linearly

42
Vector Spaces, Bases, Linear Maps
independent family, then there is some vector w ∈E such that the family
B′ obtained by adding w to B is linearly independent. However, since B
is a basis of E, the vector w can be expressed as a linear combination of
vectors in B, contradicting the fact that B′ is linearly independent.
Conversely, assume (2). We claim that B spans E. If B does not span
E, then there is some vector w ∈E which is not a linear combination of
vectors in B. By Lemma 2.1, the family B′ obtained by adding w to B is
linearly independent. Since B is a proper subfamily of B′, this contradicts
the assumption that B is a maximal linearly independent family. Therefore,
B must span E, and since B is also linearly independent, it is a basis of E.
Now we will prove the equivalence of (1) and (3). Again, assume (1).
Since B is a basis, it is a generating family of E. We claim that B is a
minimal generating family. If B is not a minimal generating family, then
there is a proper subfamily B′ of B that spans E. Then, every w ∈B −B′
can be expressed as a linear combination of vectors from B′, contradicting
the fact that B is linearly independent.
Conversely, assume (3). We claim that B is linearly independent. If B
is not linearly independent, then some vector w ∈B can be expressed as a
linear combination of vectors in B′ = B −{w}. Since B generates E, the
family B′ also generates E, but B′ is a proper subfamily of B, contradicting
the minimality of B. Since B spans E and is linearly independent, it is a
basis of E.
The second key result of linear algebra is that for any two bases (ui)i∈I
and (vj)j∈J of a vector space E, the index sets I and J have the same
cardinality. In particular, if E has a ﬁnite basis of n elements, every basis
of E has n elements, and the integer n is called the dimension of the vector
space E.
To prove the second key result, we can use the following replacement
lemma due to Steinitz. This result shows the relationship between ﬁnite
linearly independent families and ﬁnite families of generators of a vector
space. We begin with a version of the lemma which is a bit informal, but
easier to understand than the precise and more formal formulation given in
Proposition 2.10. The technical diﬃculty has to do with the fact that some
of the indices need to be renamed.
Proposition 2.9. (Replacement lemma, version 1) Given a vector space
E, let (u1, . . . , um) be any ﬁnite linearly independent family in E, and let
(v1, . . . , vn) be any ﬁnite family such that every ui is a linear combination
of (v1, . . . , vn). Then we must have m ≤n, and there is a replacement of

2.5. Bases of a Vector Space
43
m of the vectors vj by (u1, . . . , um), such that after renaming some of the
indices of the vjs, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn)
generate the same subspace of E.
Proof. We proceed by induction on m.
When m = 0, the family
(u1, . . . , um) is empty, and the proposition holds trivially.
For the in-
duction step, we have a linearly independent family (u1, . . . , um, um+1).
Consider the linearly independent family (u1, . . . , um). By the induction
hypothesis, m ≤n, and there is a replacement of m of the vectors vj by
(u1, . . . , um), such that after renaming some of the indices of the vs, the
families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same sub-
space of E. The vector um+1 can also be expressed as a linear combination
of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) gener-
ate the same subspace, um+1 can be expressed as a linear combination of
(u1, . . . , um, vm+1, . . ., vn), say
um+1 =
m
X
i=1
λiui +
n
X
j=m+1
λjvj.
We claim that λj ̸= 0 for some j with m + 1 ≤j ≤n, which implies
that m + 1 ≤n.
Otherwise, we would have
um+1 =
m
X
i=1
λiui,
a nontrivial linear dependence of the ui, which is impossible since
(u1, . . . , um+1) are linearly independent.
Therefore, m + 1 ≤n, and after renaming indices if necessary, we may
assume that λm+1 ̸= 0, so we get
vm+1 = −
m
X
i=1
(λ−1
m+1λi)ui −λ−1
m+1um+1 −
n
X
j=m+2
(λ−1
m+1λj)vj.
Observe that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1,
vm+2, . . . , vn) generate the same subspace, since um+1 is a linear com-
bination of (u1, . . . , um, vm+1, . . . , vn) and vm+1 is a linear combination
of (u1, . . . , um+1, vm+2, . . . , vn).
Since (u1, . . . , um, vm+1, . . . , vn) and
(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1,
vm+2, . . . , vn) and and (v1, . . . , vn) generate the same subspace, which con-
cludes the induction hypothesis.

44
Vector Spaces, Bases, Linear Maps
Here is an example illustrating the replacement lemma. Consider se-
quences (u1, u2, u3) and (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly
independent family and with the uis expressed in terms of the vjs as fol-
lows:
u1 = v4 + v5
u2 = v3 + v4 −v5
u3 = v1 + v2 + v3.
From the ﬁrst equation we get
v4 = u1 −v5,
and by substituting in the second equation we have
u2 = v3 + v4 −v5 = v3 + u1 −v5 −v5 = u1 + v3 −2v5.
From the above equation we get
v3 = −u1 + u2 + 2v5,
and so
u3 = v1 + v2 + v3 = v1 + v2 −u1 + u2 + 2v5.
Finally, we get
v1 = u1 −u2 + u3 −v2 −2v5.
Therefore we have
v1 = u1 −u2 + u3 −v2 −2v5
v3 = −u1 + u2 + 2v5
v4 = u1 −v5,
which shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3,
v4, v5). The vectors (v1, v3, v4) have been replaced by (u1, u2, u3), and the
vectors left over are (v2, v5). We can rename them (v4, v5).
For the sake of completeness, here is a more formal statement of the
replacement lemma (and its proof).
Proposition 2.10. (Replacement lemma, version 2) Given a vector space
E, let (ui)i∈I be any ﬁnite linearly independent family in E, where |I| = m,
and let (vj)j∈J be any ﬁnite family such that every ui is a linear combination
of (vj)j∈J, where |J| = n.
Then there exists a set L and an injection
ρ: L →J (a relabeling function) such that L ∩I = ∅, |L| = n −m, and the
families (ui)i∈I ∪(vρ(l))l∈L and (vj)j∈J generate the same subspace of E.
In particular, m ≤n.

2.5. Bases of a Vector Space
45
Proof. We proceed by induction on |I| = m. When m = 0, the family
(ui)i∈I is empty, and the proposition holds trivially with L = J (ρ is the
identity). Assume |I| = m + 1. Consider the linearly independent family
(ui)i∈(I−{p}), where p is any member of I. By the induction hypothesis,
there exists a set L and an injection ρ: L →J such that L ∩(I −{p}) = ∅,
|L| = n−m, and the families (ui)i∈(I−{p}) ∪(vρ(l))l∈L and (vj)j∈J generate
the same subspace of E. If p ∈L, we can replace L by (L −{p}) ∪{p′}
where p′ does not belong to I ∪L, and replace ρ by the injection ρ′ which
agrees with ρ on L −{p} and such that ρ′(p′) = ρ(p). Thus, we can always
assume that L ∩I = ∅. Since up is a linear combination of (vj)j∈J and the
families (ui)i∈(I−{p}) ∪(vρ(l))l∈L and (vj)j∈J generate the same subspace
of E, up is a linear combination of (ui)i∈(I−{p}) ∪(vρ(l))l∈L. Let
up =
X
i∈(I−{p})
λiui +
X
l∈L
λlvρ(l).
(2.15)
If λl = 0 for all l ∈L, we have
X
i∈(I−{p})
λiui −up = 0,
contradicting the fact that (ui)i∈I is linearly independent. Thus, λl ̸= 0 for
some l ∈L, say l = q. Since λq ̸= 0, we have
vρ(q) =
X
i∈(I−{p})
(−λ−1
q λi)ui + λ−1
q up +
X
l∈(L−{q})
(−λ−1
q λl)vρ(l).
(2.16)
We claim that the families (ui)i∈(I−{p}) ∪(vρ(l))l∈L
and (ui)i∈I ∪
(vρ(l))l∈(L−{q}) generate the same subset of E. Indeed, the second fam-
ily is obtained from the ﬁrst by replacing vρ(q) by up, and vice-versa, and
up is a linear combination of (ui)i∈(I−{p}) ∪(vρ(l))l∈L, by (2.15), and vρ(q)
is a linear combination of (ui)i∈I ∪(vρ(l))l∈(L−{q}), by (2.16). Thus, the
families (ui)i∈I ∪(vρ(l))l∈(L−{q}) and (vj)j∈J generate the same subspace
of E, and the proposition holds for L −{q} and the restriction of the in-
jection ρ: L →J to L −{q}, since L ∩I = ∅and |L| = n −m imply that
(L −{q}) ∩I = ∅and |L −{q}| = n −(m + 1).
The idea is that m of the vectors vj can be replaced by the linearly
independent uis in such a way that the same subspace is still generated.
The purpose of the function ρ: L →J is to pick n−m elements j1, . . . , jn−m
of J and to relabel them l1, . . . , ln−m in such a way that these new indices
do not clash with the indices in I; this way, the vectors vj1, . . . , vjn−m who
"survive" (i.e. are not replaced) are relabeled vl1, . . . , vln−m, and the other

46
Vector Spaces, Bases, Linear Maps
m vectors vj with j ∈J −{j1, . . . , jn−m} are replaced by the ui. The index
set of this new family is I ∪L.
Actually, one can prove that Proposition 2.10 implies Theorem 2.1 when
the vector space is ﬁnitely generated. Putting Theorem 2.1 and Proposi-
tion 2.10 together, we obtain the following fundamental theorem.
Theorem 2.2. Let E be a ﬁnitely generated vector space.
Any family
(ui)i∈I generating E contains a subfamily (uj)j∈J which is a basis of E.
Any linearly independent family (ui)i∈I can be extended to a family (uj)j∈J
which is a basis of E (with I ⊆J). Furthermore, for every two bases (ui)i∈I
and (vj)j∈J of E, we have |I| = |J| = n for some ﬁxed integer n ≥0.
Proof. The ﬁrst part follows immediately by applying Theorem 2.1 with
L = ∅and S = (ui)i∈I. For the second part, consider the family S′ =
(ui)i∈I ∪(vh)h∈H, where (vh)h∈H is any ﬁnitely generated family generating
E, and with I ∩H = ∅. Then apply Theorem 2.1 to L = (ui)i∈I and to
S′. For the last statement, assume that (ui)i∈I and (vj)j∈J are bases of E.
Since (ui)i∈I is linearly independent and (vj)j∈J spans E, Proposition 2.10
implies that |I| ≤|J|. A symmetric argument yields |J| ≤|I|.
Remark: Theorem 2.2 also holds for vector spaces that are not ﬁnitely
generated.
Deﬁnition 2.11. When a vector space E is not ﬁnitely generated, we say
that E is of inﬁnite dimension. The dimension of a ﬁnitely generated vector
space E is the common dimension n of all of its bases and is denoted by
dim(E).
Clearly, if the ﬁeld R itself is viewed as a vector space, then every
family (a) where a ∈R and a ̸= 0 is a basis. Thus dim(R) = 1. Note that
dim({0}) = 0.
Deﬁnition 2.12. If E is a vector space of dimension n ≥1, for any sub-
space U of E, if dim(U) = 1, then U is called a line; if dim(U) = 2, then
U is called a plane; if dim(U) = n −1, then U is called a hyperplane. If
dim(U) = k, then U is sometimes called a k-plane.
Let (ui)i∈I be a basis of a vector space E. For any vector v ∈E, since
the family (ui)i∈I generates E, there is a family (λi)i∈I of scalars in R, such
that
v =
X
i∈I
λiui.

2.6. Matrices
47
A very important fact is that the family (λi)i∈I is unique.
Proposition 2.11. Given a vector space E, let (ui)i∈I be a family of vec-
tors in E. Let v ∈E, and assume that v = P
i∈I λiui. Then the family
(λi)i∈I of scalars such that v = P
i∈I λiui is unique iﬀ(ui)i∈I is linearly
independent.
Proof. First, assume that (ui)i∈I is linearly independent.
If (µi)i∈I is
another family of scalars in R such that v = P
i∈I µiui, then we have
X
i∈I
(λi −µi)ui = 0,
and since (ui)i∈I is linearly independent, we must have λi −µi = 0 for all
i ∈I, that is, λi = µi for all i ∈I. The converse is shown by contradiction.
If (ui)i∈I was linearly dependent, there would be a family (µi)i∈I of scalars
not all null such that
X
i∈I
µiui = 0
and µj ̸= 0 for some j ∈I. But then,
v =
X
i∈I
λiui + 0 =
X
i∈I
λiui +
X
i∈I
µiui =
X
i∈I
(λi + µi)ui,
with λj ̸= λj + µj since µj ̸= 0, contradicting the assumption that (λi)i∈I
is the unique family such that v = P
i∈I λiui.
Deﬁnition 2.13. If (ui)i∈I is a basis of a vector space E, for any vector
v ∈E, if (xi)i∈I is the unique family of scalars in R such that
v =
X
i∈I
xiui,
each xi is called the component (or coordinate) of index i of v with respect
to the basis (ui)i∈I.
2.6
Matrices
In Section 2.1 we introduced informally the notion of a matrix. In this
section we deﬁne matrices precisely, and also introduce some operations on
matrices. It turns out that matrices form a vector space equipped with a
multiplication operation which is associative, but noncommutative. We will

48
Vector Spaces, Bases, Linear Maps
explain in Section 3.1 how matrices can be used to represent linear maps,
deﬁned in the next section.
Deﬁnition 2.14. If K = R or K = C, an m × n-matrix over K is a family
(ai j)1≤i≤m, 1≤j≤n of scalars in K, represented by an array





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n




.
In the special case where m = 1, we have a row vector, represented by
(a1 1 · · · a1 n)
and in the special case where n = 1, we have a column vector, represented
by



a1 1
...
am 1


.
In these last two cases, we usually omit the constant index 1 (ﬁrst index
in case of a row, second index in case of a column). The set of all m × n-
matrices is denoted by Mm,n(K) or Mm,n. An n × n-matrix is called a
square matrix of dimension n. The set of all square matrices of dimension
n is denoted by Mn(K), or Mn.
Remark: As deﬁned, a matrix A = (ai j)1≤i≤m, 1≤j≤n is a family, that
is, a function from {1, 2, . . . , m} × {1, 2, . . . , n} to K.
As such, there is
no reason to assume an ordering on the indices. Thus, the matrix A can
be represented in many diﬀerent ways as an array, by adopting diﬀerent
orders for the rows or the columns. However, it is customary (and usually
convenient) to assume the natural ordering on the sets {1, 2, . . . , m} and
{1, 2, . . . , n}, and to represent A as an array according to this ordering of
the rows and columns.
We deﬁne some operations on matrices as follows.
Deﬁnition 2.15. Given two m × n matrices A = (ai j) and B = (bi j), we
deﬁne their sum A + B as the matrix C = (ci j) such that ci j = ai j + bi j;

2.6. Matrices
49
that is,





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n




+





b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
...
...
...
...
bm 1 bm 2 . . . bm n





=





a1 1 + b1 1
a1 2 + b1 2 . . . a1 n + b1 n
a2 1 + b2 1
a2 2 + b2 2 . . . a2 n + b2 n
...
...
...
...
am 1 + bm 1 am 2 + bm 2 . . . am n + bm n




.
For any matrix A = (ai j), we let −A be the matrix (−ai j).
Given a
scalar λ ∈K, we deﬁne the matrix λA as the matrix C = (ci j) such that
ci j = λai j; that is
λ





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n




=





λa1 1 λa1 2 . . . λa1 n
λa2 1 λa2 2 . . . λa2 n
...
...
...
...
λam 1 λam 2 . . . λam n




.
Given an m × n matrices A = (ai k) and an n × p matrices B = (bk j), we
deﬁne their product AB as the m × p matrix C = (ci j) such that
ci j =
n
X
k=1
ai kbk j,
for 1 ≤i ≤m, and 1 ≤j ≤p. In the product AB = C shown below





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n










b1 1 b1 2 . . . b1 p
b2 1 b2 2 . . . b2 p
...
...
...
...
bn 1 bn 2 . . . bn p




=





c1 1 c1 2 . . . c1 p
c2 1 c2 2 . . . c2 p
...
...
...
...
cm 1 cm 2 . . . cm p




,
note that the entry of index i and j of the matrix AB obtained by mul-
tiplying the matrices A and B can be identiﬁed with the product of the
row matrix corresponding to the i-th row of A with the column matrix
corresponding to the j-column of B:
(ai 1 · · · ai n)



b1 j
...
bn j


=
n
X
k=1
ai kbk j.

50
Vector Spaces, Bases, Linear Maps
Deﬁnition 2.16. The square matrix In of dimension n containing 1 on the
diagonal and 0 everywhere else is called the identity matrix. It is denoted
by
In =





1 0 . . . 0
0 1 . . . 0
... ... ... ...
0 0 . . . 1




.
Deﬁnition 2.17. Given an m × n matrix A = (ai j), its transpose A⊤=
(a⊤
j i), is the n × m-matrix such that a⊤
j i = ai j, for all i, 1 ≤i ≤m, and all
j, 1 ≤j ≤n.
The transpose of a matrix A is sometimes denoted by At, or even by tA.
Note that the transpose A⊤of a matrix A has the property that the j-th
row of A⊤is the j-th column of A. In other words, transposition exchanges
the rows and the columns of a matrix. Here is an example. If A is the 5×6
matrix
A =






1 2 3 4 5 6
7 1 2 3 4 5
8 7 1 2 3 4
9 8 7 1 2 3
10 9 8 7 1 2






,
then A⊤is the 6 × 5 matrix
A⊤=









1 7 8 9 10
2 1 7 8 9
3 2 1 7 8
4 3 2 1 7
5 4 3 2 1
6 5 4 3 2









.
The following observation will be useful later on when we discuss the
SVD. Given any m × n matrix A and any n × p matrix B, if we denote the
columns of A by A1, . . . , An and the rows of B by B1, . . . , Bn, then we have
AB = A1B1 + · · · + AnBn.
For every square matrix A of dimension n, it is immediately veriﬁed that
AIn = InA = A.
Deﬁnition 2.18. For any square matrix A of dimension n, if a matrix B
such that AB = BA = In exists, then it is unique, and it is called the

2.6. Matrices
51
inverse of A. The matrix B is also denoted by A−1. An invertible matrix
is also called a nonsingular matrix, and a matrix that is not invertible is
called a singular matrix.
Using Proposition 2.16 and the fact that matrices represent linear maps,
it can be shown that if a square matrix A has a left inverse, that is a matrix
B such that BA = I, or a right inverse, that is a matrix C such that AC = I,
then A is actually invertible; so B = A−1 and C = A−1. These facts also
follow from Proposition 5.10.
It is immediately veriﬁed that the set Mm,n(K) of m × n matrices is a
vector space under addition of matrices and multiplication of a matrix by
a scalar.
Deﬁnition 2.19. The m × n-matrices Eij = (eh k), are deﬁned such that
ei j = 1, and eh k = 0, if h ̸= i or k ̸= j; in other words, the (i, j)-entry is
equal to 1 and all other entries are 0.
Here are the Eij matrices for m = 2 and n = 3:
E11 =
1 0 0
0 0 0

,
E12 =
0 1 0
0 0 0

,
E13 =
0 0 1
0 0 0

E21 =
0 0 0
1 0 0

,
E22 =
0 0 0
0 1 0

,
E23 =
0 0 0
0 0 1

.
It is clear that every matrix A = (ai j) ∈Mm,n(K) can be written in a
unique way as
A =
m
X
i=1
n
X
j=1
ai jEij.
Thus, the family (Eij)1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K),
which has dimension mn.
Remark: Deﬁnition 2.14 and Deﬁnition 2.15 also make perfect sense when
K is a (commutative) ring rather than a ﬁeld. In this more general setting,
the framework of vector spaces is too narrow, but we can consider struc-
tures over a commutative ring A satisfying all the axioms of Deﬁnition 2.4.
Such structures are called modules. The theory of modules is (much) more
complicated than that of vector spaces. For example, modules do not al-
ways have a basis, and other properties holding for vector spaces usually
fail for modules. When a module has a basis, it is called a free module. For
example, when A is a commutative ring, the structure An is a module such

52
Vector Spaces, Bases, Linear Maps
that the vectors ei, with (ei)i = 1 and (ei)j = 0 for j ̸= i, form a basis of
An. Many properties of vector spaces still hold for An. Thus, An is a free
module. As another example, when A is a commutative ring, Mm,n(A) is a
free module with basis (Ei,j)1≤i≤m,1≤j≤n. Polynomials over a commutative
ring also form a free module of inﬁnite dimension.
The properties listed in Proposition 2.12 are easily veriﬁed, although
some of the computations are a bit tedious. A more conceptual proof is
given in Proposition 3.1.
Proposition 2.12. (1) Given any matrices A ∈Mm,n(K), B ∈Mn,p(K),
and C ∈Mp,q(K), we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈Mm,n(K), and C, D ∈Mn,p(K), for all
λ ∈K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K)×Mn,p(K) →Mm,p(K) is bilinear.
The properties of Proposition 2.12 together with the fact that AIn =
InA = A for all square n × n matrices show that Mn(K) is a ring with unit
In (in fact, an associative algebra). This is a noncommutative ring with
zero divisors, as shown by the following example.
Example 2.7. For example, letting A, B be the 2 × 2-matrices
A =
1 0
0 0

,
B =
0 0
1 0

,
then
AB =
1 0
0 0
 0 0
1 0

=
0 0
0 0

,
and
BA =
0 0
1 0
 1 0
0 0

=
0 0
1 0

.
Thus AB ̸= BA, and AB = 0, even though both A, B ̸= 0.

2.7. Linear Maps
53
2.7
Linear Maps
Now that we understand vector spaces and how to generate them, we would
like to be able to transform one vector space E into another vector space
F. A function between two vector spaces that preserves the vector space
structure is called a homomorphism of vector spaces, or linear map. Linear
maps formalize the concept of linearity of a function.
Keep in mind that linear maps, which are
transformations of space, are usually far
more important than the spaces themselves.
In the rest of this section, we assume that all vector spaces are real
vector spaces, but all results hold for vector spaces over an arbitrary ﬁeld.
Deﬁnition 2.20. Given two vector spaces E and F, a linear map between
E and F is a function f : E →F satisfying the following two conditions:
f(x + y) = f(x) + f(y)
for all x, y ∈E;
f(λx) = λf(x)
for all λ ∈R, x ∈E.
Setting x = y = 0 in the ﬁrst identity, we get f(0) = 0. The basic prop-
erty of linear maps is that they transform linear combinations into linear
combinations. Given any ﬁnite family (ui)i∈I of vectors in E, given any
family (λi)i∈I of scalars in R, we have
f
 X
i∈I
λiui
!
=
X
i∈I
λif(ui).
The above identity is shown by induction on |I| using the properties of
Deﬁnition 2.20.
Example 2.8.
(1) The map f : R2 →R2 deﬁned such that
x′ = x −y
y′ = x + y
is a linear map. The reader should check that it is the composition of
a rotation by π/4 with a magniﬁcation of ratio
√
2.
(2) For any vector space E, the identity map id: E →E given by
id(u) = u
for all u ∈E
is a linear map. When we want to be more precise, we write idE instead
of id.

54
Vector Spaces, Bases, Linear Maps
(3) The map D: R[X] →R[X] deﬁned such that
D(f(X)) = f ′(X),
where f ′(X) is the derivative of the polynomial f(X), is a linear map.
(4) The map Φ: C([a, b]) →R given by
Φ(f) =
Z b
a
f(t)dt,
where C([a, b]) is the set of continuous functions deﬁned on the interval
[a, b], is a linear map.
(5) The function ⟨−, −⟩: C([a, b]) × C([a, b]) →R given by
⟨f, g⟩=
Z b
a
f(t)g(t)dt,
is linear in each of the variable f, g. It also satisﬁes the properties
⟨f, g⟩= ⟨g, f⟩and ⟨f, f⟩= 0 iﬀf = 0. It is an example of an inner
product.
Deﬁnition 2.21. Given a linear map f : E →F, we deﬁne its image (or
range) Im f = f(E), as the set
Im f = {y ∈F | (∃x ∈E)(y = f(x))},
and its Kernel (or nullspace) Ker f = f −1(0), as the set
Ker f = {x ∈E | f(x) = 0}.
The derivative map D: R[X] →R[X] from Example 2.8(3) has ker-
nel the constant polynomials, so Ker D = R. If we consider the second
derivative D ◦D: R[X] →R[X], then the kernel of D ◦D consists of all
polynomials of degree ≤1. The image of D: R[X] →R[X] is actually R[X]
itself, because every polynomial P(X) = a0Xn +· · ·+an−1X +an of degree
n is the derivative of the polynomial Q(X) of degree n + 1 given by
Q(X) = a0
Xn+1
n + 1 + · · · + an−1
X2
2 + anX.
On the other hand, if we consider the restriction of D to the vector space
R[X]n of polynomials of degree ≤n, then the kernel of D is still R, but
the image of D is the R[X]n−1, the vector space of polynomials of degree
≤n −1.
Proposition 2.13. Given a linear map f : E →F, the set Im f is a sub-
space of F and the set Ker f is a subspace of E. The linear map f : E →F
is injective iﬀKer f = (0) (where (0) is the trivial subspace {0}).

2.7. Linear Maps
55
Proof. Given any x, y ∈Im f, there are some u, v ∈E such that x = f(u)
and y = f(v), and for all λ, µ ∈R, we have
f(λu + µv) = λf(u) + µf(v) = λx + µy,
and thus, λx + µy ∈Im f, showing that Im f is a subspace of F.
Given any x, y ∈Ker f, we have f(x) = 0 and f(y) = 0, and thus,
f(λx + µy) = λf(x) + µf(y) = 0,
that is, λx + µy ∈Ker f, showing that Ker f is a subspace of E.
First, assume that Ker f = (0). We need to prove that f(x) = f(y)
implies that x = y. However, if f(x) = f(y), then f(x) −f(y) = 0, and
by linearity of f we get f(x −y) = 0. Because Ker f = (0), we must have
x −y = 0, that is x = y, so f is injective. Conversely, assume that f is
injective. If x ∈Ker f, that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),
and by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is
injective iﬀKer f = (0).
Since by Proposition 2.13, the image Im f of a linear map f is a subspace
of F, we can deﬁne the rank rk(f) of f as the dimension of Im f.
Deﬁnition 2.22. Given a linear map f : E →F, the rank rk(f) of f is the
dimension of the image Im f of f.
A fundamental property of bases in a vector space is that they allow
the deﬁnition of linear maps as unique homomorphic extensions, as shown
in the following proposition.
Proposition 2.14. Given any two vector spaces E and F, given any basis
(ui)i∈I of E, given any other family of vectors (vi)i∈I in F, there is a unique
linear map f : E →F such that f(ui) = vi for all i ∈I. Furthermore, f
is injective iﬀ(vi)i∈I is linearly independent, and f is surjective iﬀ(vi)i∈I
generates F.
Proof. If such a linear map f : E →F exists, since (ui)i∈I is a basis of E,
every vector x ∈E can written uniquely as a linear combination
x =
X
i∈I
xiui,
and by linearity, we must have
f(x) =
X
i∈I
xif(ui) =
X
i∈I
xivi.

56
Vector Spaces, Bases, Linear Maps
Deﬁne the function f : E →F, by letting
f(x) =
X
i∈I
xivi
for every x = P
i∈I xiui. It is easy to verify that f is indeed linear, it is
unique by the previous reasoning, and obviously, f(ui) = vi.
Now assume that f is injective. Let (λi)i∈I be any family of scalars,
and assume that
X
i∈I
λivi = 0.
Since vi = f(ui) for every i ∈I, we have
f
 X
i∈I
λiui
!
=
X
i∈I
λif(ui) =
X
i∈I
λivi = 0.
Since f is injective iﬀKer f = (0), we have
X
i∈I
λiui = 0,
and since (ui)i∈I is a basis, we have λi = 0 for all i ∈I, which shows that
(vi)i∈I is linearly independent. Conversely, assume that (vi)i∈I is linearly
independent. Since (ui)i∈I is a basis of E, every vector x ∈E is a linear
combination x = P
i∈I λiui of (ui)i∈I. If
f(x) = f
 X
i∈I
λiui
!
= 0,
then
X
i∈I
λivi =
X
i∈I
λif(ui) = f
 X
i∈I
λiui
!
= 0,
and λi = 0 for all i ∈I because (vi)i∈I is linearly independent, which means
that x = 0. Therefore, Ker f = (0), which implies that f is injective. The
part where f is surjective is left as a simple exercise.
Figure 2.11 provides an illustration of Proposition 2.14 when E = R3
and V = R2.
By the second part of Proposition 2.14, an injective linear map f : E →
F sends a basis (ui)i∈I to a linearly independent family (f(ui))i∈I of F,
which is also a basis when f is bijective. Also, when E and F have the
same ﬁnite dimension n, (ui)i∈I is a basis of E, and f : E →F is injective,
then (f(ui))i∈I is a basis of F (by Proposition 2.8).
The following simple proposition is also useful.
Proposition 2.15. Given any two vector spaces E and F, with F non-
trivial, given any family (ui)i∈I of vectors in E, the following properties
hold:

2.7. Linear Maps
57
u  = (1,0,0)
1
u = (0,1,0)
2
u = (0,0,1)
3
v = (1,1)
1
v = (-1,1)
2
v = (1,0)
3
f(u )1
f(u )
2
-
2f(u  )
3
E = 
f
F =
R
R2
3
f is not injective
deﬁning f
Fig. 2.11
Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),
v3 = (1, 0), deﬁne the unique linear map f : R3 →R2 by f(u1) = v1, f(u2) = v2, and
f(u3) = v3. This map is surjective but not injective since f(u1 −u2) = f(u1) −f(u2) =
(1, 1) −(−1, 1) = (2, 0) = 2f(u3) = f(2u3).
(1) The family (ui)i∈I generates E iﬀfor every family of vectors (vi)i∈I in
F, there is at most one linear map f : E →F such that f(ui) = vi for
all i ∈I.
(2) The family (ui)i∈I is linearly independent iﬀfor every family of vectors
(vi)i∈I in F, there is some linear map f : E →F such that f(ui) = vi
for all i ∈I.
Proof. (1) If there is any linear map f : E →F such that f(ui) = vi for
all i ∈I, since (ui)i∈I generates E, every vector x ∈E can be written as
some linear combination
x =
X
i∈I
xiui,
and by linearity, we must have
f(x) =
X
i∈I
xif(ui) =
X
i∈I
xivi.
This shows that f is unique if it exists. Conversely, assume that (ui)i∈I does
not generate E. Since F is nontrivial, there is some some vector y ∈F such
that y ̸= 0. Since (ui)i∈I does not generate E, there is some vector w ∈E
that is not in the subspace generated by (ui)i∈I. By Theorem 2.2, there

58
Vector Spaces, Bases, Linear Maps
is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the same
subspace. Since by hypothesis, w ∈E is not in the subspace generated
by (ui)i∈I0, by Lemma 2.1 and by Theorem 2.2 again, there is a basis
(ej)j∈I0∪J of E, such that ei = ui for all i ∈I0, and w = ej0 for some
j0 ∈J. Letting (vi)i∈I be the family in F such that vi = 0 for all i ∈I,
deﬁning f : E →F to be the constant linear map with value 0, we have a
linear map such that f(ui) = 0 for all i ∈I. By Proposition 2.14, there
is a unique linear map g: E →F such that g(w) = y, and g(ej) = 0 for
all j ∈(I0 ∪J) −{j0}. By deﬁnition of the basis (ej)j∈I0∪J of E, we have
g(ui) = 0 for all i ∈I, and since f ̸= g, this contradicts the fact that there
is at most one such map. See Figure 2.12.
u  = (1,0,0)
1
u = (0,1,0)
2
E = 
F =
!
!
2
3
u  = (1,0,0)
1 1 
u = (0,1,0)
2u 2u 
u  = (1,0,0)
1
u = (0,1,0)
2
E = 
F =
!
!
2
3
u  = (1,0,0)
1 1 
u = (0,1,0)
2u 2u 
w = (0,0,1)
w = (0,0,1)
deﬁning f as the zero
deﬁning g
y = (1,0)
g(w) = y
Fig. 2.12
Let E = R3 and F = R2. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not
generate R3 since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the
peach xy-plane to the origin.
(2) If the family (ui)i∈I is linearly independent, then by Theorem 2.2,
(ui)i∈I can be extended to a basis of E, and the conclusion follows by
Proposition 2.14. Conversely, assume that (ui)i∈I is linearly dependent.

2.7. Linear Maps
59
Then there is some family (λi)i∈I of scalars (not all zero) such that
X
i∈I
λiui = 0.
By the assumption, for any nonzero vector y ∈F, for every i ∈I, there
is some linear map fi : E →F, such that fi(ui) = y, and fi(uj) = 0, for
j ∈I −{i}. Then we would get
0 = fi(
X
i∈I
λiui) =
X
i∈I
λifi(ui) = λiy,
and since y ̸= 0, this implies λi = 0 for every i ∈I. Thus, (ui)i∈I is linearly
independent.
Given vector spaces E, F, and G, and linear maps f : E →F and
g: F →G, it is easily veriﬁed that the composition g ◦f : E →G of f and
g is a linear map.
Deﬁnition 2.23. A linear map f : E →F is an isomorphism iﬀthere is a
linear map g: F →E, such that
g ◦f = idE
and
f ◦g = idF .
(2.17)
The map g in Deﬁnition 2.23 is unique. This is because if g and h both
satisfy g ◦f = idE, f ◦g = idF , h ◦f = idE, and f ◦h = idF , then
g = g ◦idF = g ◦(f ◦h) = (g ◦f) ◦h = idE ◦h = h.
The map g satisfying (2.17) above is called the inverse of f and it is also
denoted by f −1.
Observe that Proposition 2.14 shows that if F = Rn, then we get an
isomorphism between any vector space E of dimension |J| = n and Rn.
Proposition 2.14 also implies that if E and F are two vector spaces, (ui)i∈I
is a basis of E, and f : E →F is a linear map which is an isomorphism,
then the family (f(ui))i∈I is a basis of F.
One can verify that if f : E →F is a bijective linear map, then its
inverse f −1 : F →E, as a function, is also a linear map, and thus f is an
isomorphism.
Another useful corollary of Proposition 2.14 is this:
Proposition 2.16. Let E be a vector space of ﬁnite dimension n ≥1 and
let f : E →E be any linear map. The following properties hold:
(1) If f has a left inverse g, that is, if g is a linear map such that g◦f = id,
then f is an isomorphism and f −1 = g.

60
Vector Spaces, Bases, Linear Maps
(2) If f has a right inverse h, that is, if h is a linear map such that f ◦h =
id, then f is an isomorphism and f −1 = h.
Proof. (1) The equation g ◦f = id implies that f is injective; this is a
standard result about functions (if f(x) = f(y), then g(f(x)) = g(f(y)),
which implies that x = y since g ◦f = id). Let (u1, . . . , un) be any ba-
sis of E. By Proposition 2.14, since f is injective, (f(u1), . . . , f(un)) is
linearly independent, and since E has dimension n, it is a basis of E (if
(f(u1), . . . , f(un)) doesn't span E, then it can be extended to a basis of
dimension strictly greater than n, contradicting Theorem 2.2). Then f is
bijective, and by a previous observation its inverse is a linear map. We also
have
g = g ◦id = g ◦(f ◦f −1) = (g ◦f) ◦f −1 = id ◦f −1 = f −1.
(2) The equation f ◦h = id implies that f is surjective; this is a stan-
dard result about functions (for any y ∈E, we have f(h(y)) = y). Let
(u1, . . . , un) be any basis of E. By Proposition 2.14, since f is surjective,
(f(u1), . . . , f(un)) spans E, and since E has dimension n, it is a basis of
E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans
E, it contains a basis of dimension strictly smaller than n, contradicting
Theorem 2.2). Then f is bijective, and by a previous observation its inverse
is a linear map. We also have
h = id ◦h = (f −1 ◦f) ◦h = f −1 ◦(f ◦h) = f −1 ◦id = f −1.
This completes the proof.
Deﬁnition 2.24. The set of all linear maps between two vector spaces
E and F is denoted by Hom(E, F) or by L(E; F) (the notation L(E; F)
is usually reserved to the set of continuous linear maps, where E and F
are normed vector spaces). When we wish to be more precise and specify
the ﬁeld K over which the vector spaces E and F are deﬁned we write
HomK(E, F).
The set Hom(E, F) is a vector space under the operations deﬁned in
Example 2.3, namely
(f + g)(x) = f(x) + g(x)
for all x ∈E, and
(λf)(x) = λf(x)

2.8. Linear Forms and the Dual Space
61
for all x ∈E. The point worth checking carefully is that λf is indeed a
linear map, which uses the commutativity of ∗in the ﬁeld K (typically,
K = R or K = C). Indeed, we have
(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).
When E and F have ﬁnite dimensions, the vector space Hom(E, F) also
has ﬁnite dimension, as we shall see shortly.
Deﬁnition 2.25. When E = F, a linear map f : E →E is also called an
endomorphism. The space Hom(E, E) is also denoted by End(E).
It is also important to note that composition confers to Hom(E, E)
a ring structure.
Indeed, composition is an operation ◦: Hom(E, E) ×
Hom(E, E) →Hom(E, E), which is associative and has an identity idE,
and the distributivity properties hold:
(g1 + g2) ◦f = g1 ◦f + g2 ◦f;
g ◦(f1 + f2) = g ◦f1 + g ◦f2.
The ring Hom(E, E) is an example of a noncommutative ring.
It is easily seen that the set of bijective linear maps f : E →E is a
group under composition.
Deﬁnition 2.26. Bijective linear maps f : E →E are also called automor-
phisms. The group of automorphisms of E is called the general linear group
(of E), and it is denoted by GL(E), or by Aut(E), or when E = Rn, by
GL(n, R), or even by GL(n).
2.8
Linear Forms and the Dual Space
We already observed that the ﬁeld K itself (K = R or K = C) is a vector
space (over itself). The vector space Hom(E, K) of linear maps from E to
the ﬁeld K, the linear forms, plays a particular role. In this section, we only
deﬁne linear forms and show that every ﬁnite-dimensional vector space has
a dual basis. A more advanced presentation of dual spaces and duality is
given in Chapter 10.
Deﬁnition 2.27. Given a vector space E, the vector space Hom(E, K) of
linear maps from E to the ﬁeld K is called the dual space (or dual) of E.
The space Hom(E, K) is also denoted by E∗, and the linear maps in E∗
are called the linear forms, or covectors. The dual space E∗∗of the space
E∗is called the bidual of E.

62
Vector Spaces, Bases, Linear Maps
As a matter of notation, linear forms f : E →K will also be denoted
by starred symbol, such as u∗, x∗, etc.
If E is a vector space of ﬁnite dimension n and (u1, . . . , un) is a basis
of E, for any linear form f ∗∈E∗, for every x = x1u1 + · · · + xnun ∈E, by
linearity we have
f ∗(x) = f ∗(u1)x1 + · · · + f ∗(un)xn
= λ1x1 + · · · + λnxn,
with λi = f ∗(ui) ∈K for every i, 1 ≤i ≤n. Thus, with respect to the
basis (u1, . . . , un), the linear form f ∗is represented by the row vector
(λ1 · · · λn),
we have
f ∗(x) =
 λ1 · · · λn




x1
...
xn


,
a linear combination of the coordinates of x, and we can view the linear form
f ∗as a linear equation. If we decide to use a column vector of coeﬃcients
c =



c1
...
cn



instead of a row vector, then the linear form f ∗is deﬁned by
f ∗(x) = c⊤x.
The above notation is often used in machine learning.
Example 2.9. Given any diﬀerentiable function f : Rn →R, by deﬁnition,
for any x ∈Rn, the total derivative dfx of f at x is the linear form dfx : Rn →
R deﬁned so that for all u = (u1, . . . , un) ∈Rn,
dfx(u) =
 ∂f
∂x1
(x) · · ·
∂f
∂xn
(x)




u1
...
un


=
n
X
i=1
∂f
∂xi
(x) ui.
Example 2.10. Let C([0, 1]) be the vector space of continuous functions
f : [0, 1] →R. The map I : C([0, 1]) →R given by
I(f) =
Z 1
0
f(x)dx
for any f ∈C([0, 1])
is a linear form (integration).

2.8. Linear Forms and the Dual Space
63
Example 2.11. Consider the vector space Mn(R) of real n × n matrices.
Let tr: Mn(R) →R be the function given by
tr(A) = a11 + a22 + · · · + ann,
called the trace of A. It is a linear form. Let s: Mn(R) →R be the function
given by
s(A) =
n
X
i,j=1
aij,
where A = (aij). It is immediately veriﬁed that s is a linear form.
Given a vector space E and any basis (ui)i∈I for E, we can associate to
each ui a linear form u∗
i ∈E∗, and the u∗
i have some remarkable properties.
Deﬁnition 2.28. Given a vector space E and any basis (ui)i∈I for E, by
Proposition 2.14, for every i ∈I, there is a unique linear form u∗
i such that
u∗
i (uj) =
 1
if i = j
0
if i ̸= j,
for every j ∈I. The linear form u∗
i is called the coordinate form of index i
w.r.t. the basis (ui)i∈I.
Remark: Given an index set I, authors often deﬁne the so called "Kro-
necker symbol" δi j such that
δi j =
 1
if i = j
0
if i ̸= j,
for all i, j ∈I. Then, u∗
i (uj) = δi j.
The reason for the terminology coordinate form is as follows: If E has
ﬁnite dimension and if (u1, . . . , un) is a basis of E, for any vector
v = λ1u1 + · · · + λnun,
we have
u∗
i (v) = u∗
i (λ1u1 + · · · + λnun)
= λ1u∗
i (u1) + · · · + λiu∗
i (ui) + · · · + λnu∗
i (un)
= λi,
since u∗
i (uj) = δi j. Therefore, u∗
i is the linear function that returns the ith
coordinate of a vector expressed over the basis (u1, . . . , un).

64
Vector Spaces, Bases, Linear Maps
The following theorem shows that in ﬁnite-dimension, every basis
(u1, . . . , un) of a vector space E yields a basis (u∗
1, . . . , u∗
n) of the dual
space E∗, called a dual basis.
Theorem 2.3. (Existence of dual bases) Let E be a vector space of dimen-
sion n. The following properties hold: For every basis (u1, . . . , un) of E,
the family of coordinate forms (u∗
1, . . . , u∗
n) is a basis of E∗(called the dual
basis of (u1, . . . , un)).
Proof. (a) If v∗∈E∗is any linear form, consider the linear form
f ∗= v∗(u1)u∗
1 + · · · + v∗(un)u∗
n.
Observe that because u∗
i (uj) = δi j,
f ∗(ui) = (v∗(u1)u∗
1 + · · · + v∗(un)u∗
n)(ui)
= v∗(u1)u∗
1(ui) + · · · + v∗(ui)u∗
i (ui) + · · · + v∗(un)u∗
n(ui)
= v∗(ui),
and so f ∗and v∗agree on the basis (u1, . . . , un), which implies that
v∗= f ∗= v∗(u1)u∗
1 + · · · + v∗(un)u∗
n.
Therefore, (u∗
1, . . . , u∗
n) spans E∗. We claim that the covectors u∗
1, . . . , u∗
n
are linearly independent. If not, we have a nontrivial linear dependence
λ1u∗
1 + · · · + λnu∗
n = 0,
and if we apply the above linear form to each ui, using a familiar compu-
tation, we get
0 = λiu∗
i (ui) = λi,
proving that u∗
1, . . . , u∗
n are indeed linearly independent.
Therefore,
(u∗
1, . . . , u∗
n) is a basis of E∗.
In particular, Theorem 2.3 shows a ﬁnite-dimensional vector space and
its dual E∗have the same dimension.
2.9
Summary
The main concepts and results of this chapter are listed below:
• The notion of a vector space.
• Families of vectors.

2.9. Summary
65
• Linear combinations of vectors; linear dependence and linear indepen-
dence of a family of vectors.
• Linear subspaces.
• Spanning (or generating) family; generators, ﬁnitely generated sub-
space; basis of a subspace.
• Every linearly independent family can be extended to a basis (Theo-
rem 2.1).
• A family B of vectors is a basis iﬀit is a maximal linearly independent
family iﬀit is a minimal generating family (Proposition 2.8).
• The replacement lemma (Proposition 2.10).
• Any two bases in a ﬁnitely generated vector space E have the same
number of elements; this is the dimension of E (Theorem 2.2).
• Hyperplanes.
• Every vector has a unique representation over a basis (in terms of its
coordinates).
• Matrices
• Column vectors, row vectors.
• Matrix operations: addition, scalar multiplication, multiplication.
• The vector space Mm,n(K) of m × n matrices over the ﬁeld K; The
ring Mn(K) of n × n matrices over the ﬁeld K.
• The notion of a linear map.
• The image Im f (or range) of a linear map f.
• The kernel Ker f (or nullspace) of a linear map f.
• The rank rk(f) of a linear map f.
• The image and the kernel of a linear map are subspaces. A linear map
is injective iﬀits kernel is the trivial space (0) (Proposition 2.13).
• The unique homomorphic extension property of linear maps with re-
spect to bases (Proposition 2.14).
• The vector space of linear maps HomK(E, F).
• Linear forms (covectors) and the dual space E∗.
• Coordinate forms.
• The existence of dual bases (in ﬁnite dimension).

66
Vector Spaces, Bases, Linear Maps
2.10
Problems
Problem 2.1. Let H be the set of 3 × 3 upper triangular matrices given
by
H =





1 a b
0 1 c
0 0 1

| a, b, c ∈R


.
(1) Prove that H with the binary operation of matrix multiplication
is a group; ﬁnd explicitly the inverse of every matrix in H. Is H abelian
(commutative)?
(2) Given two groups G1 and G2, recall that a homomorphism if a
function ϕ: G1 →G2 such that
ϕ(ab) = ϕ(a)ϕ(b),
a, b ∈G1.
Prove that ϕ(e1) = e2 (where ei is the identity element of Gi) and that
ϕ(a−1) = (ϕ(a))−1,
a ∈G1.
(3) Let S1 be the unit circle, that is
S1 = {eiθ = cos θ + i sin θ | 0 ≤θ < 2π},
and let ϕ be the function given by
ϕ


1 a b
0 1 c
0 0 1

= (a, c, eib).
Prove that ϕ is a surjective function onto G = R × R × S1, and that if
we deﬁne multiplication on this set by
(x1, y1, u1) · (x2, y2, u2) = (x1 + x2, y1 + y2, eix1y2u1u2),
then G is a group and ϕ is a group homomorphism from H onto G.
(4) The kernel of a homomorphism ϕ: G1 →G2 is deﬁned as
Ker (ϕ) = {a ∈G1 | ϕ(a) = e2}.
Find explicitly the kernel of ϕ and show that it is a subgroup of H.
Problem 2.2. For any m ∈Z with m > 0, the subset mZ = {mk | k ∈Z}
is an abelian subgroup of Z. Check this.
(1) Give a group isomorphism (an invertible homomorphism) from mZ
to Z.

2.10. Problems
67
(2) Check that the inclusion map i: mZ →Z given by i(mk) = mk
is a group homomorphism. Prove that if m ≥2 then there is no group
homomorphism p: Z →mZ such that p ◦i = id.
Remark: The above shows that abelian groups fail to have some of the
properties of vector spaces. We will show later that a linear map satisfying
the condition p ◦i = id always exists.
Problem 2.3. Let E = R × R, and deﬁne the addition operation
(x1, y1) + (x2, y2) = (x1 + x2, y1, +y2),
x1, x2, y1, y2 ∈R,
and the multiplication operation ·: R × E →E by
λ · (x, y) = (λx, y),
λ, x, y ∈R.
Show that E with the above operations + and · is not a vector space.
Which of the axioms is violated?
Problem 2.4. (1) Prove that the axioms of vector spaces imply that
α · 0 = 0
0 · v = 0
α · (−v) = −(α · v)
(−α) · v = −(α · v),
for all v ∈E and all α ∈K, where E is a vector space over K.
(2) For every λ ∈R and every x = (x1, . . . , xn) ∈Rn, deﬁne λx by
λx = λ(x1, . . . , xn) = (λx1, . . . , λxn).
Recall that every vector x = (x1, . . . , xn) ∈Rn can be written uniquely as
x = x1e1 + · · · + xnen,
where ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i.
For any
operation ·: R × Rn →Rn, if · satisﬁes the Axiom (V1) of a vector space,
then prove that for any α ∈R, we have
α · x = α · (x1e1 + · · · + xnen) = α · (x1e1) + · · · + α · (xnen).
Conclude that · is completely determined by its action on the one-
dimensional subspaces of Rn spanned by e1, . . . , en.
(3) Use (2) to deﬁne operations ·: R×Rn →Rn that satisfy the Axioms
(V1-V3), but for which Axiom V4 fails.

68
Vector Spaces, Bases, Linear Maps
(4) For any operation ·: R × Rn →Rn, prove that if · satisﬁes the
Axioms (V2-V3), then for every rational number r ∈Q and every vector
x ∈Rn, we have
r · x = r(1 · x).
In the above equation, 1·x is some vector (y1, . . . , yn) ∈Rn not necessarily
equal to x = (x1, . . . , xn), and
r(1 · x) = (ry1, . . . , ryn),
as in Part (2).
Use (4) to conclude that any operation ·: Q × Rn →Rn that satisﬁes
the Axioms (V1-V3) is completely determined by the action of 1 on the
one-dimensional subspaces of Rn spanned by e1, . . . , en.
Problem 2.5. Let A1 be the following matrix:
A1 =


2
3
1
1
2 −1
−3 −5 1

.
Prove that the columns of A1 are linearly independent. Find the coordinates
of the vector x = (6, 2, −7) over the basis consisting of the column vectors
of A1.
Problem 2.6. Let A2 be the following matrix:
A2 =




1
2 1 1
2
3 2 3
−1 0 1 −1
−2 −1 3 0



.
Express the fourth column of A2 as a linear combination of the ﬁrst three
columns of A2. Is the vector x = (7, 14, −1, 2) a linear combination of the
columns of A2?
Problem 2.7. Let A3 be the following matrix:
A3 =


1 1 1
1 1 2
1 2 3

.
Prove that the columns of A1 are linearly independent. Find the coordinates
of the vector x = (6, 9, 14) over the basis consisting of the column vectors
of A3.

2.10. Problems
69
Problem 2.8. Let A4 be the following matrix:
A4 =




1
2 1 1
2
3 2 3
−1 0 1 −1
−2 −1 4 0



.
Prove that the columns of A4 are linearly independent. Find the coordinates
of the vector x = (7, 14, −1, 2) over the basis consisting of the column
vectors of A4.
Problem 2.9. Consider the following Haar matrix
H =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1



.
Prove that the columns of H are linearly independent.
Hint. Compute the product H⊤H.
Problem 2.10. Consider the following Hadamard matrix
H4 =




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1



.
Prove that the columns of H4 are linearly independent.
Hint. Compute the product H⊤
4 H4.
Problem 2.11. In solving this problem, do not use determinants.
(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some
vector space E. Assume that each vi is a linear combination of the ujs, so
that
vi = ai 1u1 + · · · + ai mum,
1 ≤i ≤m,
and that the matrix A = (ai j) is an upper-triangular matrix, which means
that if 1 ≤j < i ≤m, then ai j = 0.
Prove that if (u1, . . . , um) are
linearly independent and if all the diagonal entries of A are nonzero, then
(v1, . . . , vm) are also linearly independent.
Hint. Use induction on m.
(2) Let A = (ai j) be an upper-triangular matrix. Prove that if all the
diagonal entries of A are nonzero, then A is invertible and the inverse A−1
of A is also upper-triangular.

70
Vector Spaces, Bases, Linear Maps
Hint. Use induction on m.
Prove that if A is invertible, then all the diagonal entries of A are
nonzero.
(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related
as in (1), then (u1, . . . , um) are linearly independent iﬀ(v1, . . . , vm) are
linearly independent.
Problem 2.12. In solving this problem, do not use determinants. Con-
sider the n × n matrix
A =












1 2 0
0 . . . 0 0
0 1 2
0 . . . 0 0
0 0 1
2 . . . 0 0
... ... ... ... ... ... ...
0 0 . . . 0
1 2 0
0 0 . . . 0
0 1 2
0 0 . . . 0
0 0 1












.
(1) Find the solution x = (x1, . . . , xn) of the linear system
Ax = b,
for
b =





b1
b2
...
bn




.
(2) Prove that the matrix A is invertible and ﬁnd its inverse A−1. Given
that the number of atoms in the universe is estimated to be ≤1082, compare
the size of the coeﬃcients the inverse of A to 1082, if n ≥300.
(3) Assume b is perturbed by a small amount ∆b (note that ∆b is a
vector). Find the new solution of the system
A(x + ∆x) = b + ∆b,
where ∆x is also a vector. In the case where b = (0, . . . , 0, 1), and ∆b =
(0, . . . , 0, ϵ), show that
|(∆x)1| = 2n−1|ϵ|
(where (∆x)1 is the ﬁrst component of ∆x).
(4) Prove that (A −I)n = 0.

2.10. Problems
71
Problem 2.13. An n × n matrix N is nilpotent if there is some integer
r ≥1 such that N r = 0.
(1) Prove that if N is a nilpotent matrix, then the matrix I −N is
invertible and
(I −N)−1 = I + N + N 2 + · · · + N r−1.
(2) Compute the inverse of the following matrix A using (1):
A =






1 2 3 4 5
0 1 2 3 4
0 0 1 2 3
0 0 0 1 2
0 0 0 0 1






.
Problem 2.14. (1) Let A be an n×n matrix. If A is invertible, prove that
for any x ∈Rn, if Ax = 0, then x = 0.
(2) Let A be an m × n matrix and let B be an n × m matrix. Prove
that Im −AB is invertible iﬀIn −BA is invertible.
Hint. If for all x ∈Rn, Mx = 0 implies that x = 0, then M is invertible.
Problem 2.15. Consider the following n × n matrix, for n ≥3:
B =












1 −1 −1 −1 · · · −1 −1
1 −1 1
1 · · ·
1
1
1 1 −1 1 · · ·
1
1
1 1
1 −1 · · ·
1
1
...
...
...
...
...
...
...
1 1
1
1 · · · −1 1
1 1
1
1 · · ·
1 −1












.
(1) If we denote the columns of B by b1, . . . , bn, prove that
(n −3)b1 −(b2 + · · · + bn) = 2(n −2)e1
b1 −b2 = 2(e1 + e2)
b1 −b3 = 2(e1 + e3)
...
...
b1 −bn = 2(e1 + en),
where e1, . . . , en are the canonical basis vectors of Rn.
(2) Prove that B is invertible and that its inverse A = (aij) is given by
a11 = (n −3)
2(n −2),
ai1 = −
1
2(n −2)
2 ≤i ≤n

72
Vector Spaces, Bases, Linear Maps
and
aii = −(n −3)
2(n −2),
2 ≤i ≤n
aji =
1
2(n −2),
2 ≤i ≤n, j ̸= i.
(3) Show that the n diagonal n × n matrices Di deﬁned such that the
diagonal entries of Di are equal the entries (from top down) of the ith
column of B form a basis of the space of n × n diagonal matrices (matrices
with zeros everywhere except possibly on the diagonal). For example, when
n = 4, we have
D1 =




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1




D2 =




−1 0 0 0
0 −1 0 0
0
0 1 0
0
0 0 1



,
D3 =




−1 0 0 0
0 1 0 0
0 0 −1 0
0 0 0 1



,
D4 =




−1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 −1



.
Problem 2.16. Given any m × n matrix A and any n × p matrix B, if we
denote the columns of A by A1, . . . , An and the rows of B by B1, . . . , Bn,
prove that
AB = A1B1 + · · · + AnBn.
Problem 2.17. Let f : E →F be a linear map which is also a bijection (it
is injective and surjective). Prove that the inverse function f −1 : F →E is
linear.
Problem 2.18. Given two vectors spaces E and F, let (ui)i∈I be any basis
of E and let (vi)i∈I be any family of vectors in F. Prove that the unique
linear map f : E →F such that f(ui) = vi for all i ∈I is surjective iﬀ
(vi)i∈I spans F.
Problem 2.19. Let f : E →F be a linear map with dim(E) = n and
dim(F) = m. Prove that f has rank 1 iﬀf is represented by an m × n
matrix of the form
A = uv⊤
with u a nonzero column vector of dimension m and v a nonzero column
vector of dimension n.

2.10. Problems
73
Problem 2.20. Find a nontrivial linear dependence among the linear forms
ϕ1(x, y, z) = 2x−y+3z,
ϕ2(x, y, z) = 3x−5y+z,
ϕ3(x, y, z) = 4x−7y+z.
Problem 2.21. Prove that the linear forms
ϕ1(x, y, z) = x+2y+z,
ϕ2(x, y, z) = 2x+3y+3z,
ϕ3(x, y, z) = 3x+7y+z
are linearly independent. Express the linear form ϕ(x, y, z) = x + y + z as
a linear combination of ϕ1, ϕ2, ϕ3.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 3
Matrices and Linear Maps
In this chapter, all vector spaces are deﬁned over an arbitrary ﬁeld K. For
the sake of concreteness, the reader may safely assume that K = R.
3.1
Representation of Linear Maps by Matrices
Proposition 2.14 shows that given two vector spaces E and F and a basis
(uj)j∈J of E, every linear map f : E →F is uniquely determined by the
family (f(uj))j∈J of the images under f of the vectors in the basis (uj)j∈J.
If we also have a basis (vi)i∈I of F, then every vector f(uj) can be
written in a unique way as
f(uj) =
X
i∈I
ai jvi,
where j ∈J, for a family of scalars (ai j)i∈I. Thus, with respect to the
two bases (uj)j∈J of E and (vi)i∈I of F, the linear map f is completely
determined by a "I × J-matrix" M(f) = (ai j)i∈I, j∈J.
Remark: Note that we intentionally assigned the index set J to the basis
(uj)j∈J of E, and the index set I to the basis (vi)i∈I of F, so that the
rows of the matrix M(f) associated with f : E →F are indexed by I, and
the columns of the matrix M(f) are indexed by J. Obviously, this causes
a mildly unpleasant reversal. If we had considered the bases (ui)i∈I of E
and (vj)j∈J of F, we would obtain a J × I-matrix M(f) = (aj i)j∈J, i∈I.
No matter what we do, there will be a reversal! We decided to stick to the
bases (uj)j∈J of E and (vi)i∈I of F, so that we get an I × J-matrix M(f),
knowing that we may occasionally suﬀer from this decision!
When I and J are ﬁnite, and say, when |I| = m and |J| = n, the linear
map f is determined by the matrix M(f) whose entries in the j-th column
75

76
Matrices and Linear Maps
are the components of the vector f(uj) over the basis (v1, . . . , vm), that is,
the matrix
M(f) =





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n





whose entry on Row i and Column j is ai j (1 ≤i ≤m, 1 ≤j ≤n).
We will now show that when E and F have ﬁnite dimension, linear maps
can be very conveniently represented by matrices, and that composition of
linear maps corresponds to matrix multiplication.
We will follow rather
closely an elegant presentation method due to Emil Artin.
Let E and F be two vector spaces, and assume that E has a ﬁnite basis
(u1, . . . , un) and that F has a ﬁnite basis (v1, . . . , vm). Recall that we have
shown that every vector x ∈E can be written in a unique way as
x = x1u1 + · · · + xnun,
and similarly every vector y ∈F can be written in a unique way as
y = y1v1 + · · · + ymvm.
Let f : E →F be a linear map between E and F. Then for every x =
x1u1 + · · · + xnun in E, by linearity, we have
f(x) = x1f(u1) + · · · + xnf(un).
Let
f(uj) = a1 jv1 + · · · + am jvm,
or more concisely,
f(uj) =
m
X
i=1
ai jvi,
for every j, 1 ≤j ≤n. This can be expressed by writing the coeﬃcients
a1j, a2j, . . . , amj of f(uj) over the basis (v1, . . . , vm), as the jth column of
a matrix, as shown below:
f(u1) f(u2) . . . f(un)
v1
v2
...
vm





a11
a12
. . . a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2 . . . amn




.

3.1. Representation of Linear Maps by Matrices
77
Then substituting the right-hand side of each f(uj) into the expression
for f(x), we get
f(x) = x1
 m
X
i=1
ai 1vi
!
+ · · · + xn
 m
X
i=1
ai nvi
!
,
which, by regrouping terms to obtain a linear combination of the vi, yields
f(x) =


n
X
j=1
a1 jxj

v1 + · · · +


n
X
j=1
am jxj

vm.
Thus, letting f(x) = y = y1v1 + · · · + ymvm, we have
yi =
n
X
j=1
ai jxj
(3.1)
for all i, 1 ≤i ≤m.
To make things more concrete, let us treat the case where n = 3 and
m = 2. In this case,
f(u1) = a11v1 + a21v2
f(u2) = a12v1 + a22v2
f(u3) = a13v1 + a23v2,
which in matrix form is expressed by
f(u1) f(u2) f(u3)
v1
v2
 a11
a12
a13
a21
a22
a23

,
and for any x = x1u1 + x2u2 + x3u3, we have
f(x) = f(x1u1 + x2u2 + x3u3)
= x1f(u1) + x2f(u2) + x3f(u3)
= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)
= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.
Consequently, since
y = y1v1 + y2v2,
we have
y1 = a11x1 + a12x2 + a13x3
y2 = a21x1 + a22x2 + a23x3.

78
Matrices and Linear Maps
This agrees with the matrix equation
y1
y2

=
a11 a12 a13
a21 a22 a23
 

x1
x2
x3

.
We now formalize the representation of linear maps by matrices.
Deﬁnition 3.1. Let E and F be two vector spaces, and let (u1, . . . , un)
be a basis for E, and (v1, . . . , vm) be a basis for F. Each vector x ∈E
expressed in the basis (u1, . . . , un) as x = x1u1 + · · · + xnun is represented
by the column matrix
M(x) =



x1
...
xn



and similarly for each vector y ∈F expressed in the basis (v1, . . . , vm).
Every linear map f : E →F is represented by the matrix M(f) =
(ai j), where ai j is the i-th component of the vector f(uj) over the basis
(v1, . . . , vm), i.e., where
f(uj) =
m
X
i=1
ai jvi,
for every j, 1 ≤j ≤n.
The coeﬃcients a1j, a2j, . . . , amj of f(uj) over the basis (v1, . . . , vm) form
the jth column of the matrix M(f) shown below:
f(u1) f(u2) . . . f(un)
v1
v2
...
vm





a11
a12
. . . a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2 . . . amn




.
The matrix M(f) associated with the linear map f : E →F is called the
matrix of f with respect to the bases (u1, . . . , un) and (v1, . . . , vm). When
E = F and the basis (v1, . . . , vm) is identical to the basis (u1, . . . , un) of E,
the matrix M(f) associated with f : E →E (as above) is called the matrix
of f with respect to the basis (u1, . . . , un).
Remark: As in the remark after Deﬁnition 2.14, there is no reason to
assume that the vectors in the bases (u1, . . . , un) and (v1, . . . , vm) are or-
dered in any particular way. However, it is often convenient to assume the

3.1. Representation of Linear Maps by Matrices
79
natural ordering. When this is so, authors sometimes refer to the matrix
M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un) and
(v1, . . . , vm).
Let us illustrate the representation of a linear map by a matrix in a
concrete situation.
Let E be the vector space R[X]4 of polynomials of
degree at most 4, let F be the vector space R[X]3 of polynomials of degree
at most 3, and let the linear map be the derivative map d: that is,
d(P + Q) = dP + dQ
d(λP) = λdP,
with λ ∈R. We choose (1, x, x2, x3, x4) as a basis of E and (1, x, x2, x3) as
a basis of F. Then the 4 × 5 matrix D associated with d is obtained by
expressing the derivative dxi of each basis vector xi for i = 0, 1, 2, 3, 4 over
the basis (1, x, x2, x3). We ﬁnd
D =




0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4



.
If P denotes the polynomial
P = 3x4 −5x3 + x2 −7x + 5,
we have
dP = 12x3 −15x2 + 2x −7.
The polynomial P is represented by the vector (5, −7, 1, −5, 3), the poly-
nomial dP is represented by the vector (−7, 2, −15, 12), and we have




0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4










5
−7
1
−5
3






=




−7
2
−15
12



,
as expected!
The kernel (nullspace) of d consists of the polynomials of
degree 0, that is, the constant polynomials. Therefore dim(Ker d) = 1, and
from
dim(E) = dim(Ker d) + dim(Im d)
(see Theorem 5.1), we get dim(Im d) = 4 (since dim(E) = 5).

80
Matrices and Linear Maps
For fun, let us ﬁgure out the linear map from the vector space R[X]3
to the vector space R[X]4 given by integration (ﬁnding the primitive, or
anti-derivative) of xi, for i = 0, 1, 2, 3). The 5 × 4 matrix S representing
R
with respect to the same bases as before is
S =






0 0
0
0
1 0
0
0
0 1/2 0
0
0 0 1/3
0
0 0
0 1/4






.
We verify that DS = I4,




0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4










0 0
0
0
1 0
0
0
0 1/2 0
0
0 0 1/3
0
0 0
0 1/4






=




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



.
This is to be expected by the fundamental theorem of calculus since
the derivative of an integral returns the function. As we will shortly see,
the above matrix product corresponds to this functional composition. The
equation DS = I4 shows that S is injective and has D as a left inverse.
However, SD ̸= I5, and instead






0
0
0
0
1
0
0
0
0 1/2 0
0
0
0 1/3
0
0
0
0 1/4










0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4



=






0 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1






,
because constant polynomials (polynomials of degree 0) belong to the kernel
of D.
3.2
Composition of Linear Maps and Matrix Multiplication
Let us now consider how the composition of linear maps is expressed in
terms of bases.
Let E, F, and G, be three vectors spaces with respective bases
(u1, . . . , up) for E, (v1, . . . , vn) for F, and (w1, . . . , wm) for G. Let g: E →
F and f : F →G be linear maps. As explained earlier, g: E →F is deter-
mined by the images of the basis vectors uj, and f : F →G is determined

3.2. Composition of Linear Maps and Matrix Multiplication
81
by the images of the basis vectors vk. We would like to understand how
f ◦g: E →G is determined by the images of the basis vectors uj.
Remark: Note that we are considering linear maps g: E →F and f : F →
G, instead of f : E →F and g: F →G, which yields the composition
f ◦g: E →G instead of g ◦f : E →G. Our perhaps unusual choice is
motivated by the fact that if f is represented by a matrix M(f) = (ai k) and
g is represented by a matrix M(g) = (bk j), then f ◦g: E →G is represented
by the product AB of the matrices A and B. If we had adopted the other
choice where f : E →F and g: F →G, then g ◦f : E →G would be
represented by the product BA. Personally, we ﬁnd it easier to remember
the formula for the entry in Row i and Column j of the product of two
matrices when this product is written by AB, rather than BA. Obviously,
this is a matter of taste! We will have to live with our perhaps unorthodox
choice.
Thus, let
f(vk) =
m
X
i=1
ai kwi,
for every k, 1 ≤k ≤n, and let
g(uj) =
n
X
k=1
bk jvk,
for every j, 1 ≤j ≤p; in matrix form, we have
f(v1) f(v2) . . . f(vn)
w1
w2
...
wm





a11
a12
. . . a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2 . . . amn





and
g(u1) g(u2) . . . g(up)
v1
v2
...
vn





b11
b12
. . . b1p
b21
b22
. . .
b2p
...
...
...
...
bn1
bn2
. . .
bnp




.
By previous considerations, for every
x = x1u1 + · · · + xpup,

82
Matrices and Linear Maps
letting g(x) = y = y1v1 + · · · + ynvn, we have
yk =
p
X
j=1
bk jxj
(3.2)
for all k, 1 ≤k ≤n, and for every
y = y1v1 + · · · + ynvn,
letting f(y) = z = z1w1 + · · · + zmwm, we have
zi =
n
X
k=1
ai kyk
(3.3)
for all i, 1 ≤i ≤m. Then if y = g(x) and z = f(y), we have z = f(g(x)),
and in view of (3.2) and (3.3), we have
zi =
n
X
k=1
ai k


p
X
j=1
bk jxj


=
n
X
k=1
p
X
j=1
ai kbk jxj
=
p
X
j=1
n
X
k=1
ai kbk jxj
=
p
X
j=1
 n
X
k=1
ai kbk j
!
xj.
Thus, deﬁning ci j such that
ci j =
n
X
k=1
ai kbk j,
for 1 ≤i ≤m, and 1 ≤j ≤p, we have
zi =
p
X
j=1
ci jxj.
(3.4)
Identity (3.4) shows that the composition of linear maps corresponds to
the product of matrices.
Then given a linear map f : E →F represented by the matrix M(f) =
(ai j) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm), by equation (3.1),
namely
yi =
n
X
j=1
ai jxj
1 ≤i ≤m,

3.2. Composition of Linear Maps and Matrix Multiplication
83
and the deﬁnition of matrix multiplication, the equation y = f(x) corre-
sponds to the matrix equation M(y) = M(f)M(x), that is,



y1
...
ym


=



a1 1 . . . a1 n
...
...
...
am 1 . . . am n






x1
...
xn


.
Recall that





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
am 1 am 2 . . . am n










x1
x2
...
xn




= x1





a1 1
a2 1
...
am 1




+x2





a1 2
a2 2
...
am 2




+· · ·+xn





a1 n
a2 n
...
am n




.
Sometimes, it is necessary to incorporate the bases (u1, . . . , un) and
(v1, . . . , vm) in the notation for the matrix M(f) expressing f with respect
to these bases. This turns out to be a messy enterprise!
We propose the following course of action:
Deﬁnition 3.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases
of E and F, and denote by MU,V(f) the matrix of f with respect to the bases
U and V. Furthermore, write xU for the coordinates M(x) = (x1, . . . , xn)
of x ∈E w.r.t. the basis U and write yV for the coordinates M(y) =
(y1, . . . , ym) of y ∈F w.r.t. the basis V. Then
y = f(x)
is expressed in matrix form by
yV = MU,V(f) xU.
When U = V, we abbreviate MU,V(f) as MU(f).
The above notation seems reasonable, but it has the slight disadvantage
that in the expression MU,V(f)xU, the input argument xU which is fed to
the matrix MU,V(f) does not appear next to the subscript U in MU,V(f).
We could have used the notation MV,U(f), and some people do that. But
then, we ﬁnd a bit confusing that V comes before U when f maps from the
space E with the basis U to the space F with the basis V. So, we prefer to
use the notation MU,V(f).
Be aware that other authors such as Meyer [Meyer (2000)] use the no-
tation [f]U,V, and others such as Dummit and Foote [Dummit and Foote
(1999)] use the notation M V
U (f), instead of MU,V(f). This gets worse! You

84
Matrices and Linear Maps
may ﬁnd the notation M U
V (f) (as in Lang [Lang (1993)]), or U[f]V, or other
strange notations.
Deﬁnition 3.2 shows that the function which associates to a linear map
f : E →F the matrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm)
has the property that matrix multiplication corresponds to composition of
linear maps. This allows us to transfer properties of linear maps to matrices.
Here is an illustration of this technique:
Proposition 3.1. (1) Given any matrices A ∈Mm,n(K), B ∈Mn,p(K),
and C ∈Mp,q(K), we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈Mm,n(K), and C, D ∈Mn,p(K), for all
λ ∈K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K)×Mn,p(K) →Mm,p(K) is bilinear.
Proof. (1) Every m × n matrix A = (ai j) deﬁnes the function fA : Kn →
Km given by
fA(x) = Ax,
for all x ∈Kn. It is immediately veriﬁed that fA is linear and that the
matrix M(fA) representing fA over the canonical bases in Kn and Km is
equal to A. Then Formula (4) proves that
M(fA ◦fB) = M(fA)M(fB) = AB,
so we get
M((fA ◦fB) ◦fC) = M(fA ◦fB)M(fC) = (AB)C
and
M(fA ◦(fB ◦fC)) = M(fA)M(fB ◦fC) = A(BC),
and since composition of functions is associative, we have (fA ◦fB) ◦fC =
fA ◦(fB ◦fC), which implies that
(AB)C = A(BC).

3.2. Composition of Linear Maps and Matrix Multiplication
85
(2) It is immediately veriﬁed that if f1, f2
∈HomK(E, F), A, B
∈
Mm,n(K), (u1, . . . , un) is any basis of E, and (v1, . . . , vm) is any basis of
F, then
M(f1 + f2) = M(f1) + M(f2)
fA+B = fA + fB.
Then we have
(A + B)C = M(fA+B)M(fC)
= M(fA+B ◦fC)
= M((fA + fB) ◦fC))
= M((fA ◦fC) + (fB ◦fC))
= M(fA ◦fC) + M(fB ◦fC)
= M(fA)M(fC) + M(fB)M(fC)
= AC + BC.
The equation A(C + D) = AC + AD is proven in a similar fashion, and the
last two equations are easily veriﬁed. We could also have veriﬁed all the
identities by making matrix computations.
Note that Proposition 3.1 implies that the vector space Mn(K) of square
matrices is a (noncommutative) ring with unit In.
(It even shows that
Mn(K) is an associative algebra.)
The following proposition states the main properties of the mapping
f 7→M(f) between Hom(E, F) and Mm,n. In short, it is an isomorphism
of vector spaces.
Proposition 3.2. Given three vector spaces E, F, G, with respec-
tive bases (u1, . . . , up),
(v1, . . . , vn),
and (w1, . . . , wm),
the mapping
M : Hom(E, F) →Mn,p that associates the matrix M(g) to a linear map
g: E →F satisﬁes the following properties for all x ∈E, all g, h: E →F,
and all f : F →G:
M(g(x)) = M(g)M(x)
M(g + h) = M(g) + M(h)
M(λg) = λM(g)
M(f ◦g) = M(f)M(g),
where M(x) is the column vector associated with the vector x and M(g(x))
is the column vector associated with g(x), as explained in Deﬁnition 3.1.

86
Matrices and Linear Maps
Thus, M : Hom(E, F) →Mn,p is an isomorphism of vector spaces, and
when p = n and the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up),
M : Hom(E, E) →Mn is an isomorphism of rings.
Proof. That M(g(x)) = M(g)M(x) was shown by Deﬁnition 3.2 or equiv-
alently by Formula (1).
The identities M(g + h) = M(g) + M(h) and
M(λg) = λM(g) are straightforward, and M(f ◦g) = M(f)M(g) follows
from Identity (4) and the deﬁnition of matrix multiplication. The mapping
M : Hom(E, F) →Mn,p is clearly injective, and since every matrix deﬁnes
a linear map (see Proposition 3.1), it is also surjective, and thus bijective.
In view of the above identities, it is an isomorphism (and similarly for
M : Hom(E, E) →Mn, where Proposition 3.1 is used to show that Mn is a
ring).
In view of Proposition 3.2, it seems preferable to represent vectors from
a vector space of ﬁnite dimension as column vectors rather than row vectors.
Thus, from now on, we will denote vectors of Rn (or more generally, of Kn)
as column vectors.
3.3
Change of Basis Matrix
It is important to observe that the isomorphism M : Hom(E, F) →Mn,p
given by Proposition 3.2 depends on the choice of the bases (u1, . . . , up)
and (v1, . . . , vn), and similarly for the isomorphism M : Hom(E, E) →Mn,
which depends on the choice of the basis (u1, . . . , un). Thus, it would be
useful to know how a change of basis aﬀects the representation of a linear
map f : E →F as a matrix. The following simple proposition is needed.
Proposition 3.3. Let E be a vector space, and let (u1, . . . , un) be a basis
of E. For every family (v1, . . . , vn), let P = (ai j) be the matrix deﬁned
such that vj = Pn
i=1 ai jui. The matrix P is invertible iﬀ(v1, . . . , vn) is a
basis of E.
Proof. Note that we have P = M(f), the matrix associated with the
unique linear map f : E →E such that f(ui) = vi. By Proposition 2.14, f
is bijective iﬀ(v1, . . . , vn) is a basis of E. Furthermore, it is obvious that
the identity matrix In is the matrix associated with the identity id: E →E
w.r.t. any basis. If f is an isomorphism, then f ◦f −1 = f −1 ◦f = id, and
by Proposition 3.2, we get M(f)M(f −1) = M(f −1)M(f) = In, showing
that P is invertible and that M(f −1) = P −1.

3.3. Change of Basis Matrix
87
Proposition 3.3 suggests the following deﬁnition.
Deﬁnition 3.3. Given a vector space E of dimension n, for any two bases
(u1, . . . , un) and (v1, . . . , vn) of E, let P = (ai j) be the invertible matrix
deﬁned such that
vj =
n
X
i=1
ai jui,
which is also the matrix of the identity id: E →E with respect to the
bases (v1, . . . , vn) and (u1, . . . , un), in that order. Indeed, we express each
id(vj) = vj over the basis (u1, . . . , un). The coeﬃcients a1j, a2j, . . . , anj of
vj over the basis (u1, . . . , un) form the jth column of the matrix P shown
below:
v1 v2 . . . vn
u1
u2
...
un





a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
...
an1 an2 . . . ann




.
The matrix P is called the change of basis matrix from (u1, . . . , un) to
(v1, . . . , vn).
Clearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un)
is P −1. Since P = (ai j) is the matrix of the identity id: E →E with
respect to the bases (v1, . . . , vn) and (u1, . . . , un), given any vector x ∈E,
if x = x1u1+· · ·+xnun over the basis (u1, . . . , un) and x = x′
1v1+· · ·+x′
nvn
over the basis (v1, . . . , vn), from Proposition 3.2, we have



x1
...
xn


=



a1 1 . . . a1 n
...
...
...
an 1 . . . an n






x′
1
...
x′
n


,
showing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed
in terms of the new coordinates (x′
i) of x (over (v1, . . . , vn)).
Now we face the painful task of assigning a "good" notation incorpo-
rating the bases U = (u1, . . . , un) and V = (v1, . . . , vn) into the notation
for the change of basis matrix from U to V. Because the change of basis
matrix from U to V is the matrix of the identity map idE with respect to
the bases V and U in that order, we could denote it by MV,U(id) (Meyer

88
Matrices and Linear Maps
[Meyer (2000)] uses the notation [I]V,U). We prefer to use an abbreviation
for MV,U(id).
Deﬁnition 3.4. The change of basis matrix from U to V is denoted
PV,U.
Note that
PU,V = P −1
V,U.
Then, if we write xU = (x1, . . . , xn) for the old coordinates of x with
respect to the basis U and xV = (x′
1, . . . , x′
n) for the new coordinates of x
with respect to the basis V, we have
xU = PV,U xV,
xV = P −1
V,U xU.
The above may look backward, but remember that the matrix MU,V(f)
takes input expressed over the basis U to output expressed over the basis
V. Consequently, PV,U takes input expressed over the basis V to output
expressed over the basis U, and xU = PV,U xV matches this point of view!

Beware that some authors (such as Artin [Artin (1991)]) deﬁne
the change of basis matrix from U to V as PU,V = P −1
V,U. Under
this point of view, the old basis U is expressed in terms of the new basis
V. We ﬁnd this a bit unnatural. Also, in practice, it seems that the new
basis is often expressed in terms of the old basis, rather than the other way
around.
Since the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms
of the old basis (u1, . . ., un), we observe that the coordinates (xi) of a vector
x vary in the opposite direction of the change of basis. For this reason,
vectors are sometimes said to be contravariant. However, this expression
does not make sense! Indeed, a vector in an intrinsic quantity that does
not depend on a speciﬁc basis. What makes sense is that the coordinates
of a vector vary in a contravariant fashion.
Let us consider some concrete examples of change of bases.
Example 3.1. Let E = F = R2, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1)
and v2 = (−1, 1). The change of basis matrix P from the basis U = (u1, u2)
to the basis V = (v1, v2) is
P =
1 −1
1 1


3.3. Change of Basis Matrix
89
and its inverse is
P −1 =
 1/2 1/2
−1/2 1/2

.
The old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms
of the new coordinates (x′
1, x′
2) with respect to (v1, v2) by
x1
x2

=
1 −1
1 1
 x′
1
x′
2

,
and the new coordinates (x′
1, x′
2) with respect to (v1, v2) are expressed in
terms of the old coordinates (x1, x2) with respect to (u1, u2) by
x′
1
x′
2

=
 1/2 1/2
−1/2 1/2
 x1
x2

.
Example 3.2. Let E = F = R[X]3 be the set of polynomials of degree at
most 3, and consider the bases U = (1, x, x2, x3) and V = (B3
0(x), B3
1(x),
B3
2(x), B3
3(x)), where B3
0(x), B3
1(x), B3
2(x), B3
3(x) are the Bernstein poly-
nomials of degree 3, given by
B3
0(x) = (1 −x)3
B3
1(x) = 3(1 −x)2x
B3
2(x) = 3(1 −x)x2
B3
3(x) = x3.
By expanding the Bernstein polynomials, we ﬁnd that the change of basis
matrix PV,U is given by
PV,U =




1
0
0 0
−3 3
0 0
3 −6 3 0
−1 3 −3 1



.
We also ﬁnd that the inverse of PV,U is
P −1
V,U =




1 0
0 0
1 1/3 0
0
1 2/3 1/3 0
1 1
1 1



.
Therefore, the coordinates of the polynomial 2x3 −x + 1 over the basis V
are




1
2/3
1/3
2



=




1 0
0 0
1 1/3 0
0
1 2/3 1/3 0
1 1
1 1








1
−1
0
2



,
and so
2x3 −x + 1 = B3
0(x) + 2
3B3
1(x) + 1
3B3
2(x) + 2B3
3(x).

90
Matrices and Linear Maps
3.4
The Eﬀect of a Change of Bases on Matrices
The eﬀect of a change of bases on the representation of a linear map is
described in the following proposition.
Proposition 3.4. Let E and F be vector spaces, let U = (u1, . . . , un) and
U′ = (u′
1, . . . , u′
n) be two bases of E, and let V = (v1, . . . , vm) and V′ =
(v′
1, . . . , v′
m) be two bases of F. Let P = PU′,U be the change of basis matrix
from U to U′, and let Q = PV′,V be the change of basis matrix from V to
V′. For any linear map f : E →F, let M(f) = MU,V(f) be the matrix
associated to f w.r.t. the bases U and V, and let M ′(f) = MU′,V′(f) be the
matrix associated to f w.r.t. the bases U′ and V′. We have
M ′(f) = Q−1M(f)P,
or more explicitly
MU′,V′(f) = P −1
V′,VMU,V(f)PU′,U = PV,V′MU,V(f)PU′,U.
Proof. Since f : E →F can be written as f = idF ◦f ◦idE, since P
is the matrix of idE w.r.t. the bases (u′
1, . . . , u′
n) and (u1, . . . , un), and
Q−1 is the matrix of idF w.r.t. the bases (v1, . . . , vm) and (v′
1, . . . , v′
m), by
Proposition 3.2, we have M ′(f) = Q−1M(f)P.
As a corollary, we get the following result.
Corollary 3.1. Let E be a vector space, and let U = (u1, . . . , un) and
U′ = (u′
1, . . . , u′
n) be two bases of E. Let P = PU′,U be the change of basis
matrix from U to U′. For any linear map f : E →E, let M(f) = MU(f)
be the matrix associated to f w.r.t. the basis U, and let M ′(f) = MU′(f)
be the matrix associated to f w.r.t. the basis U′. We have
M ′(f) = P −1M(f)P,
or more explicitly,
MU′(f) = P −1
U′,UMU(f)PU′,U = PU,U′MU(f)PU′,U.
Example 3.3. Let E = R2, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1)
are the canonical basis vectors, let V = (v1, v2) = (e1, e1 −e2), and let
A =
2 1
0 1

.
The change of basis matrix P = PV,U from U to V is
P =
1 1
0 −1

,

3.4. The Eﬀect of a Change of Bases on Matrices
91
and we check that
P −1 = P.
Therefore, in the basis V, the matrix representing the linear map f deﬁned
by A is
A′ = P −1AP = PAP =
1 1
0 −1
 2 1
0 1
 1 1
0 −1

=
2 0
0 1

= D,
a diagonal matrix. In the basis V, it is clear what the action of f is: it is
a stretch by a factor of 2 in the v1 direction and it is the identity in the v2
direction. Observe that v1 and v2 are not orthogonal.
What happened is that we diagonalized the matrix A. The diagonal
entries 2 and 1 are the eigenvalues of A (and f), and v1 and v2 are corre-
sponding eigenvectors. We will come back to eigenvalues and eigenvectors
later on.
The above example showed that the same linear map can be represented
by diﬀerent matrices. This suggests making the following deﬁnition:
Deﬁnition 3.5. Two n × n matrices A and B are said to be similar iﬀ
there is some invertible matrix P such that
B = P −1AP.
It is easily checked that similarity is an equivalence relation. From our
previous considerations, two n × n matrices A and B are similar iﬀthey
represent the same linear map with respect to two diﬀerent bases.
The
following surprising fact can be shown: Every square matrix A is similar to
its transpose A⊤. The proof requires advanced concepts (the Jordan form
or similarity invariants).
If U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change
of basis matrix
P = PV,U =





a11 a12 · · · a1n
a21 a22 · · · a2n
...
...
...
...
an1 an2 · · · ann





from (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists
of the coordinates of vj over the basis (u1, . . . , un), which means that
vj =
n
X
i=1
aijui.

92
Matrices and Linear Maps
It is natural to extend the matrix notation and to express the vector



v1
...
vn



in En as the product of a matrix times the vector



u1
...
un


in En, namely as





v1
v2
...
vn




=





a11 a21 · · · an1
a12 a22 · · · an2
...
...
...
...
a1n a2n · · · ann










u1
u2
...
un




,
but notice that the matrix involved is not P, but its transpose P ⊤.
This observation has the following consequence: if U = (u1, . . . , un) and
V = (v1, . . . , vn) are two bases of E and if



v1
...
vn


= A



u1
...
un


,
that is,
vi =
n
X
j=1
aijuj,
for any vector w ∈E, if
w =
n
X
i=1
xiui =
n
X
k=1
ykvk,
then



x1
...
xn


= A⊤



y1
...
yn


,
and so



y1
...
yn


= (A⊤)−1



x1
...
xn


.
It is easy to see that (A⊤)−1 = (A−1)⊤. Also, if U = (u1, . . . , un), V =
(v1, . . . , vn), and W = (w1, . . . , wn) are three bases of E, and if the change

3.4. The Eﬀect of a Change of Bases on Matrices
93
of basis matrix from U to V is P = PV,U and the change of basis matrix
from V to W is Q = PW,V, then



v1
...
vn


= P ⊤



u1
...
un


,



w1
...
wn


= Q⊤



v1
...
vn


,
so



w1
...
wn


= Q⊤P ⊤



u1
...
un


= (PQ)⊤



u1
...
un


,
which means that the change of basis matrix PW,U from U to W is PQ.
This proves that
PW,U = PV,UPW,V.
Even though matrices are indispensable since they are the major tool
in applications of linear algebra, one should not lose track of the fact that
linear maps are more fundamental because
they are intrinsic objects that do not depend
on the choice of bases. Consequently, we
advise the reader to try to think in terms of
linear maps rather than reduce everything
to matrices.
In our experience, this is particularly eﬀective when it comes to proving
results about linear maps and matrices, where proofs involving linear maps
are often more "conceptual." These proofs are usually more general because
they do not depend on the fact that the dimension is ﬁnite. Also, instead
of thinking of a matrix decomposition as a purely algebraic operation, it is
often illuminating to view it as a geometric decomposition. This is the case
of the SVD, which in geometric terms says that every linear map can be
factored as a rotation, followed by a rescaling along orthogonal axes and
then another rotation.
After all,
a matrix is a representation of a linear
map,

94
Matrices and Linear Maps
and most decompositions of a matrix reﬂect the fact that with a suitable
choice of a basis (or bases), the linear map is a represented by a matrix
having a special shape. The problem is then to ﬁnd such bases.
Still, for the beginner, matrices have a certain irresistible appeal, and we
confess that it takes a certain amount of practice to reach the point where
it becomes more natural to deal with linear maps. We still recommend it!
For example, try to translate a result stated in terms of matrices into a
result stated in terms of linear maps. Whenever we tried this exercise, we
learned something.
Also, always try to keep in mind that
linear maps are geometric in nature; they
act on space.
3.5
Summary
The main concepts and results of this chapter are listed below:
• The representation of linear maps by matrices.
• The matrix representation mapping M : Hom(E, F) →Mn,p and the
representation isomorphism (Proposition 3.2).
• Change of basis matrix and Proposition 3.4.
3.6
Problems
Problem 3.1. Prove that the column vectors of the matrix A1 given by
A1 =


1 2 3
2 3 7
1 3 1


are linearly independent.
Prove that the coordinates of the column vectors of the matrix B1 over
the basis consisting of the column vectors of A1 given by
B1 =


3 5 1
1 2 1
4 3 −6


are the columns of the matrix P1 given by
P1 =


−27 −61 −41
9
18
9
4
10
8

.

3.6. Problems
95
Give a nontrivial linear dependence of the columns of P1. Check that
B1 = A1P1. Is the matrix B1 invertible?
Problem 3.2. Prove that the column vectors of the matrix A2 given by
A2 =




1 1 1 1
1 2 1 3
1 1 2 2
1 1 1 3




are linearly independent.
Prove that the column vectors of the matrix B2 given by
B2 =




1 −2 2 −2
0 −3 2 −3
3 −5 5 −4
3 −4 4 −4




are linearly independent.
Prove that the coordinates of the column vectors of the matrix B2 over
the basis consisting of the column vectors of A2 are the columns of the
matrix P2 given by
P2 =




2
0
1 −1
−3 1 −2 1
1 −2 2 −1
1 −1 1 −1



.
Check that A2P2 = B2. Prove that
P −1
2
=




−1 −1 −1 1
2
1
1 −2
2
1
2 −3
−1 −1 0 −1



.
What are the coordinates over the basis consisting of the column vectors of
B2 of the vector whose coordinates over the basis consisting of the column
vectors of A1 are (2, −3, 0, 0)?
Problem 3.3. Consider the polynomials
B2
0(t) = (1 −t)2
B2
1(t) = 2(1 −t)t
B2
2(t) = t2
B3
0(t) = (1 −t)3
B3
1(t) = 3(1 −t)2t
B3
2(t) = 3(1 −t)t2
B3
3(t) = t3,
known as the Bernstein polynomials of degree 2 and 3.

96
Matrices and Linear Maps
(1) Show that the Bernstein polynomials B2
0(t), B2
1(t), B2
2(t) are ex-
pressed as linear combinations of the basis (1, t, t2) of the vector space
of polynomials of degree at most 2 as follows:


B2
0(t)
B2
1(t)
B2
2(t)

=


1 −2 1
0 2 −2
0 0
1




1
t
t2

.
Prove that
B2
0(t) + B2
1(t) + B2
2(t) = 1.
(2) Show that the Bernstein polynomials B3
0(t), B3
1(t), B3
2(t), B3
3(t) are
expressed as linear combinations of the basis (1, t, t2, t3) of the vector space
of polynomials of degree at most 3 as follows:




B3
0(t)
B3
1(t)
B3
2(t)
B3
3(t)



=




1 −3 3 −1
0 3 −6 3
0 0
3 −3
0 0
0
1








1
t
t2
t3



.
Prove that
B3
0(t) + B3
1(t) + B3
2(t) + B3
3(t) = 1.
(3) Prove that the Bernstein polynomials of degree 2 are linearly in-
dependent, and that the Bernstein polynomials of degree 3 are linearly
independent.
Problem 3.4. Recall that the binomial coeﬃcient
 m
k

is given by
m
k

=
m!
k!(m −k)!,
with 0 ≤k ≤m.
For any m ≥1, we have the m + 1 Bernstein polynomials of degree m
given by
Bm
k (t) =
m
k

(1 −t)m−ktk,
0 ≤k ≤m.
(1) Prove that
Bm
k (t) =
m
X
j=k
(−1)j−k
m
j
j
k

tj.
(3.5)
Use the above to prove that Bm
0 (t), . . . , Bm
m(t) are linearly independent.

3.6. Problems
97
(2) Prove that
Bm
0 (t) + · · · + Bm
m(t) = 1.
(3) What can you say about the symmetries of the (m + 1) × (m + 1)
matrix expressing Bm
0 , . . . , Bm
m in terms of the basis 1, t, . . . , tm?
Prove your claim (beware that in equation (3.5) the coeﬃcient of tj
in Bm
k
is the entry on the (k + 1)th row of the (j + 1)th column, since
0 ≤k, j ≤m. Make appropriate modiﬁcations to the indices).
What can you say about the sum of the entries on each row of the above
matrix? What about the sum of the entries on each column?
(4) The purpose of this question is to express the ti in terms of the
Bernstein polynomials Bm
0 (t), . . . , Bm
m(t), with 0 ≤i ≤m.
First, prove that
ti =
m−i
X
j=0
tiBm−i
j
(t),
0 ≤i ≤m.
Then prove that
m
i
m −i
j

=
 m
i + j
i + j
i

.
Use the above facts to prove that
ti =
m−i
X
j=0
 i+j
i

 m
i
 Bm
i+j(t).
Conclude that the Bernstein polynomials Bm
0 (t), . . . , Bm
m(t) form a basis
of the vector space of polynomials of degree ≤m.
Compute the matrix expressing 1, t, t2 in terms of B2
0(t), B2
1(t), B2
2(t),
and the matrix expressing 1, t, t2, t3 in terms of B3
0(t), B3
1(t), B3
2(t), B3
3(t).
You should ﬁnd


1 1 1
0 1/2 1
0 0 1


and




1 1
1 1
0 1/3 2/3 1
0 0 1/3 1
0 0
0 1



.

98
Matrices and Linear Maps
(5) A polynomial curve C(t) of degree m in the plane is the set of points
C(t) =
x(t)
y(t)

given by two polynomials of degree ≤m,
x(t) = α0tm1 + α1tm1−1 + · · · + αm1
y(t) = β0tm2 + β1tm2−1 + · · · + βm2,
with 1 ≤m1, m2 ≤m and α0, β0 ̸= 0.
Prove that there exist m + 1 points b0, . . . , bm ∈R2 so that
C(t) =
x(t)
y(t)

= Bm
0 (t)b0 + Bm
1 (t)b1 + · · · + Bm
m(t)bm
for all t ∈R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1
generally on the curve?
We say that the curve C is a B´ezier curve and (b0, . . . , bm) is the list of
control points of the curve (control points need not be distinct).
Remark: Because Bm
0 (t) + · · · + Bm
m(t) = 1 and Bm
i (t) ≥0 when t ∈
[0, 1], the curve segment C[0, 1] corresponding to t ∈[0, 1] belongs to the
convex hull of the control points. This is an important property of B´ezier
curves which is used in geometric modeling to ﬁnd the intersection of curve
segments. B´ezier curves play an important role in computer graphics and
geometric modeling, but also in robotics because they can be used to model
the trajectories of moving objects.
Problem 3.5. Consider the n × n matrix
A =











0 0
0 · · · 0
−an
1 0
0 · · · 0 −an−1
0 1
0 · · · 0 −an−2
... ... ... ... ...
...
0 0
0 ... 0
−a2
0 0
0 · · · 1
−a1











,
with an ̸= 0.
(1) Find a matrix P such that
A⊤= P −1AP.
What happens when an = 0?
Hint. First, try n = 3, 4, 5.
Such a matrix must have zeros above the
"antidiagonal," and identical entries pij for all i, j ≥0 such that i+j = n+k,
where k = 1, . . . , n.
(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be
chosen so that the entries in P −1 are also integers.

3.6. Problems
99
Problem 3.6. For any matrix A ∈Mn(C), let RA and LA be the maps
from Mn(C) to itself deﬁned so that
LA(B) = AB,
RA(B) = BA,
for all B ∈Mn(C).
(1) Check that LA and RA are linear, and that LA and RB commute
for all A, B.
Let adA : Mn(C) →Mn(C) be the linear map given by
adA(B) = LA(B) −RA(B) = AB −BA = [A, B],
for all B ∈Mn(C).
Note that [A, B] is the Lie bracket.
(2) Prove that if A is invertible, then LA and RA are invertible; in fact,
(LA)−1 = LA−1 and (RA)−1 = RA−1. Prove that if A = PBP −1 for some
invertible matrix P, then
LA = LP ◦LB ◦L−1
P ,
RA = R−1
P ◦RB ◦RP .
(3) Recall that the n2 matrices Eij deﬁned such that all entries in Eij
are zero except the (i, j)th entry, which is equal to 1, form a basis of the
vector space Mn(C). Consider the partial ordering of the Eij deﬁned such
that for i = 1, . . . , n, if n ≥j > k ≥1, then then Eij precedes Eik, and for
j = 1, . . . , n, if 1 ≤i < h ≤n, then Eij precedes Ehj.
Draw the Hasse diagram of the partial order deﬁned above when n = 3.
There are total orderings extending this partial ordering. How would
you ﬁnd them algorithmically? Check that the following is such a total
order:
(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).
(4) Let the total order of the basis (Eij) extending the partial ordering
deﬁned in (2) be given by
(i, j) < (h, k)
iﬀ
i = h and j > k
or i < h.
Let R be the n × n permutation matrix given by
R =







0 0 . . . 0 1
0 0 . . . 1 0
... ... ... ... ...
0 1 . . . 0 0
1 0 . . . 0 0







.
Observe that R−1 = R. Prove that for any n ≥1, the matrix of LA is given
by A ⊗In, and the matrix of RA is given by In ⊗RA⊤R (over the basis

100
Matrices and Linear Maps
(Eij) ordered as speciﬁed above), where ⊗is the Kronecker product (also
called tensor product) of matrices deﬁned in Deﬁnition 4.4.
Hint. Figure out what are RB(Eij) = EijB and LB(Eij) = BEij.
(5) Prove that if A is upper triangular, then the matrices representing
LA and RA are also upper triangular.
Note that if instead of the ordering
E1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,
that I proposed you use the standard lexicographic ordering
E11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,
then the matrix representing LA is still A⊗In, but the matrix representing
RA is In ⊗A⊤. In this case, if A is upper-triangular, then the matrix of RA
is lower triangular. This is the motivation for using the ﬁrst basis (avoid
upper becoming lower).

Chapter 4
Haar Bases, Haar Wavelets,
Hadamard Matrices
In this chapter, we discuss two types of matrices that have applications in
computer science and engineering:
(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool
in signal processing and computer graphics.
(2) Hadamard matrices which have applications in error correcting codes,
signal processing, and low rank approximation.
4.1
Introduction to Signal Compression Using Haar
Wavelets
We begin by considering Haar wavelets in R4. Wavelets play an important
role in audio and video signal processing, especially for compressing long
signals into much smaller ones that still retain enough information so that
when they are played, we can't see or hear any diﬀerence.
Consider the four vectors w1, w2, w3, w4 given by
w1 =




1
1
1
1




w2 =




1
1
−1
−1




w3 =




1
−1
0
0




w4 =




0
0
1
−1



.
Note that these vectors are pairwise orthogonal, so they are indeed linearly
independent (we will see this in a later chapter). Let W = {w1, w2, w3, w4}
be the Haar basis, and let U = {e1, e2, e3, e4} be the canonical basis of R4.
The change of basis matrix W = PW,U from U to W is given by
W =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1



,
101

102
Haar Bases, Haar Wavelets, Hadamard Matrices
and we easily ﬁnd that the inverse of W is given by
W −1 =




1/4 0
0
0
0
1/4 0
0
0
0 1/2
0
0
0
0 1/2








1 1
1
1
1 1 −1 −1
1 −1 0
0
0 0
1 −1



.
So the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4)
over the Haar basis W, with




c1
c2
c3
c4



=




1/4 0
0
0
0 1/4 0
0
0
0 1/2
0
0
0
0 1/2








1 1
1
1
1 1 −1 −1
1 −1 0
0
0 0
1 −1








6
4
5
1



=




4
1
1
2



.
Given a signal v = (v1, v2, v3, v4), we ﬁrst transform v into its coeﬃcients
c = (c1, c2, c3, c4) over the Haar basis by computing c = W −1v. Observe
that
c1 = v1 + v2 + v3 + v4
4
is the overall average value of the signal v. The coeﬃcient c1 corresponds
to the background of the image (or of the sound). Then, c2 gives the coarse
details of v, whereas, c3 gives the details in the ﬁrst part of v, and c4 gives
the details in the second half of v.
Reconstruction of the signal consists in computing v = Wc. The trick
for good compression is to throw away some of the coeﬃcients of c (set
them to zero), obtaining a compressed signal bc, and still retain enough
crucial information so that the reconstructed signal bv = Wbc looks almost
as good as the original signal v. Thus, the steps are:
input v −→coeﬃcients c
= W −1v −→compressed bc −→compressed bv = Wbc.
This kind of compression scheme makes modern video conferencing pos-
sible.
It turns out that there is a faster way to ﬁnd c = W −1v, without actually
using W −1. This has to do with the multiscale nature of Haar wavelets.
Given the original signal v = (6, 4, 5, 1) shown in Figure 4.1, we compute
averages and half diﬀerences obtaining Figure 4.2. We get the coeﬃcients
c3 = 1 and c4 = 2. Then again we compute averages and half diﬀerences
obtaining Figure 4.3. We get the coeﬃcients c1 = 4 and c2 = 1. Note that

4.2. Haar Bases and Haar Matrices, Scaling Properties of Haar Wavelets
103
v  = 6
1
v  = 4
2
v  = 5
3
v  = 1
4
Fig. 4.1
The original signal v.
v   +  v   
1
5
2
5
2
3
3
v   +  v   
2
3
4
c  =  v   - v  
3
1
2
2
1
2
2
- v
- v
1 =
2
- v
- v
3
4
2
v
- v
3
4
2 = c =
4
Fig. 4.2
First averages and ﬁrst half diﬀerences.
the original signal v can be reconstructed from the two signals in Figure 4.2,
and the signal on the left of Figure 4.2 can be reconstructed from the two
signals in Figure 4.3. In particular, the data from Figure 4.2 gives us
5 + 1 = v1 + v2
2
+ v1 −v2
2
= v1
5 −1 = v1 + v2
2
−v1 −v2
2
= v2
3 + 2 = v3 + v4
2
+ v3 −v4
2
= v3
3 −2 = v3 + v4
2
−v3 −v4
2
= v4.
4.2
Haar Bases and Haar Matrices, Scaling Properties of
Haar Wavelets
The method discussed in Section 4.2 can be generalized to signals of any
length 2n. The previous case corresponds to n = 2. Let us consider the

104
Haar Bases, Haar Wavelets, Hadamard Matrices
4
4
4
4
1
2
3
4
4
v  + v  + v  + v
c =
1
1
1
−1
−1
v  + v  - v  - v
1
2
3
4
4
c = 
2
Fig. 4.3
Second averages and second half diﬀerences.
case n = 3. The Haar basis (w1, w2, w3, w4, w5, w6, w7, w8) is given by the
matrix
W =













1 1
1
0
1
0
0
0
1 1
1
0 −1 0
0
0
1 1 −1 0
0
1
0
0
1 1 −1 0
0 −1 0
0
1 −1 0
1
0
0
1
0
1 −1 0
1
0
0 −1 0
1 −1 0 −1 0
0
0
1
1 −1 0 −1 0
0
0 −1













.
The columns of this matrix are orthogonal, and it is easy to see that
W −1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W ⊤.
A pattern is beginning to emerge. It looks like the second Haar basis vector
w2 is the "mother" of all the other basis vectors, except the ﬁrst, whose
purpose is to perform averaging. Indeed, in general, given
w2 = (1, . . . , 1, −1, . . . , −1)



2n
,
the other Haar basis vectors are obtained by a "scaling and shifting pro-
cess." Starting from w2, the scaling process generates the vectors
w3, w5, w9, . . . , w2j+1, . . . , w2n−1+1,
such that w2j+1+1 is obtained from w2j+1 by forming two consecutive blocks
of 1 and −1 of half the size of the blocks in w2j+1, and setting all other

4.2. Haar Bases and Haar Matrices, Scaling Properties of Haar Wavelets
105
entries to zero. Observe that w2j+1 has 2j blocks of 2n−j elements. The
shifting process consists in shifting the blocks of 1 and −1 in w2j+1 to
the right by inserting a block of (k −1)2n−j zeros from the left, with
0 ≤j ≤n −1 and 1 ≤k ≤2j.
Note that our convention is to use
j as the scaling index and k as the shifting index. Thus, we obtain the
following formula for w2j+k:
w2j+k(i) =











0
1 ≤i ≤(k −1)2n−j
1
(k −1)2n−j + 1 ≤i ≤(k −1)2n−j + 2n−j−1
−1
(k −1)2n−j + 2n−j−1 + 1 ≤i ≤k2n−j
0
k2n−j + 1 ≤i ≤2n,
with 0 ≤j ≤n −1 and 1 ≤k ≤2j. Of course
w1 = (1, . . . , 1)
|
{z
}
2n
.
The above formulae look a little better if we change our indexing slightly
by letting k vary from 0 to 2j −1, and using the index j instead of 2j.
Deﬁnition 4.1. The vectors of the Haar basis of dimension 2n are denoted
by
w1, h0
0, h1
0, h1
1, h2
0, h2
1, h2
2, h2
3, . . . , hj
k, . . . , hn−1
2n−1−1,
where
hj
k(i) =











0
1 ≤i ≤k2n−j
1
k2n−j + 1 ≤i ≤k2n−j + 2n−j−1
−1
k2n−j + 2n−j−1 + 1 ≤i ≤(k + 1)2n−j
0
(k + 1)2n−j + 1 ≤i ≤2n,
with 0 ≤j ≤n −1 and 0 ≤k ≤2j −1. The 2n × 2n matrix whose columns
are the vectors
w1, h0
0, h1
0, h1
1, h2
0, h2
1, h2
2, h2
3, . . . , hj
k, . . . , hn−1
2n−1−1,
(in that order), is called the Haar matrix of dimension 2n, and is denoted
by Wn.
It turns out that there is a way to understand these formulae better if
we interpret a vector u = (u1, . . . , um) as a piecewise linear function over
the interval [0, 1).
Deﬁnition 4.2. Given a vector u = (u1, . . . , um), the piecewise linear func-
tion plf(u) is deﬁned such that
plf(u)(x) = ui,
i −1
m
≤x < i
m, 1 ≤i ≤m.

106
Haar Bases, Haar Wavelets, Hadamard Matrices
In words, the function plf(u) has the value u1 on the interval [0, 1/m),
the value u2 on [1/m, 2/m), etc., and the value um on the interval [(m −
1)/m, 1).
For example, the piecewise linear function associated with the vector
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3)
is shown in Figure 4.4.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−2
−1
0
1
2
3
4
5
6
7
Fig. 4.4
The piecewise linear function plf(u).
Then each basis vector hj
k corresponds to the function
ψj
k = plf(hj
k).
In particular, for all n, the Haar basis vectors
h0
0 = w2 = (1, . . . , 1, −1, . . . , −1)
|
{z
}
2n
yield the same piecewise linear function ψ given by
ψ(x) =







1
if
0 ≤x < 1/2
−1
if
1/2 ≤x < 1
0
otherwise,
whose graph is shown in Figure 4.5. It is easy to see that ψj
k is given by
the simple expression
ψj
k(x) = ψ(2jx −k),
0 ≤j ≤n −1, 0 ≤k ≤2j −1.
The above formula makes it clear that ψj
k is obtained from ψ by scaling
and shifting.
Deﬁnition 4.3. The function φ0
0 = plf(w1) is the piecewise linear function
with the constant value 1 on [0, 1), and the functions ψj
k = plf(hj
k) together
with φ0
0 are known as the Haar wavelets.

4.2. Haar Bases and Haar Matrices, Scaling Properties of Haar Wavelets
107
1
1
−1
0
Fig. 4.5
The Haar wavelet ψ.
Rather than using W −1 to convert a vector u to a vector c of coeﬃcients
over the Haar basis, and the matrix W to reconstruct the vector u from
its Haar coeﬃcients c, we can use faster algorithms that use averaging and
diﬀerencing.
If c is a vector of Haar coeﬃcients of dimension 2n, we compute the
sequence of vectors u0, u1, . . ., un as follows:
u0 = c
uj+1 = uj
uj+1(2i −1) = uj(i) + uj(2j + i)
uj+1(2i) = uj(i) −uj(2j + i),
for j = 0, . . . , n −1 and i = 1, . . . , 2j. The reconstructed vector (signal) is
u = un.
If u is a vector of dimension 2n, we compute the sequence of vectors
cn, cn−1, . . . , c0 as follows:
cn = u
cj = cj+1
cj(i) = (cj+1(2i −1) + cj+1(2i))/2
cj(2j + i) = (cj+1(2i −1) −cj+1(2i))/2,
for j = n −1, . . . , 0 and i = 1, . . . , 2j. The vector over the Haar basis is
c = c0.
We leave it as an exercise to implement the above programs in Matlab
using two variables u and c, and by building iteratively 2j.
Here is an
example of the conversion of a vector to its Haar coeﬃcients for n = 3.

108
Haar Bases, Haar Wavelets, Hadamard Matrices
Given the sequence u = (31, 29, 23, 17, −6, −8, −2, −4), we get the se-
quence
c3 = (31, 29, 23, 17, −6, −8, 2, −4)
c2 =
31 + 29
2
, 23 + 17
2
, −6 −8
2
, −2 −4
2
, 31 −29
2
, 23 −17
2
, −6 −(−8)
2
,
−2 −(−4)
2

= (30, 20, −7, −3, 1, 3, 1, 1)
c1 =
30 + 20
2
, −7 −3
2
, 30 −20
2
, −7 −(−3)
2
, 1, 3, 1, 1

= (25, −5, 5, −2, 1, 3, 1, 1)
c0 =
25 −5
2
, 25 −(−5)
2
, 5, −2, 1, 3, 1, 1

= (10, 15, 5, −2, 1, 3, 1, 1)
so c = (10, 15, 5, −2, 1, 3, 1, 1).
Conversely, given c = (10, 15, 5, −2,
1, 3, 1, 1), we get the sequence
u0 = (10, 15, 5, −2, 1, 3, 1, 1)
u1 = (10 + 15, 10 −15, 5, −2, 1, 3, 1, 1) = (25, −5, 5, −2, 1, 3, 1, 1)
u2 = (25 + 5, 25 −5, −5 + (−2), −5 −(−2), 1, 3, 1, 1)
= (30, 20, −7, −3, 1, 3, 1, 1)
u3 = (30 + 1, 30 −1, 20 + 3, 20 −3, −7 + 1, −7 −1, −3 + 1, −3 −1)
= (31, 29, 23, 17, −6, −8, −2, −4),
which gives back u = (31, 29, 23, 17, −6, −8, −2, −4).
4.3
Kronecker Product Construction of Haar Matrices
There is another recursive method for constructing the Haar matrix Wn
of dimension 2n that makes it clearer why the columns of Wn are pairwise
orthogonal, and why the above algorithms are indeed correct (which nobody
seems to prove!). If we split Wn into two 2n×2n−1 matrices, then the second
matrix containing the last 2n−1 columns of Wn has a very simple structure:
it consists of the vector
(1, −1, 0, . . . , 0)
|
{z
}
2n

4.3. Kronecker Product Construction of Haar Matrices
109
and 2n−1 −1 shifted copies of it, as illustrated below for n = 3:













1
0
0
0
−1 0
0
0
0
1
0
0
0 −1 0
0
0
0
1
0
0
0 −1 0
0
0
0
1
0
0
0 −1













.
Observe that this matrix can be obtained from the identity matrix I2n−1,
in our example
I4 =




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



,
by forming the 2n×2n−1 matrix obtained by replacing each 1 by the column
vector
 1
−1

and each zero by the column vector
0
0

.
Now the ﬁrst half of Wn, that is the matrix consisting of the ﬁrst 2n−1
columns of Wn, can be obtained from Wn−1 by forming the 2n × 2n−1
matrix obtained by replacing each 1 by the column vector
1
1

,
each −1 by the column vector
−1
−1

,
and each zero by the column vector
0
0

.

110
Haar Bases, Haar Wavelets, Hadamard Matrices
For n = 3, the ﬁrst half of W3 is the matrix













1 1
1
0
1 1
1
0
1 1 −1 0
1 1 −1 0
1 −1 0
1
1 −1 0
1
1 −1 0 −1
1 −1 0 −1













which is indeed obtained from
W2 =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1




using the process that we just described.
These matrix manipulations can be described conveniently using a prod-
uct operation on matrices known as the Kronecker product.
Deﬁnition 4.4. Given a m × n matrix A = (aij) and a p × q matrix
B = (bij), the Kronecker product (or tensor product) A ⊗B of A and B is
the mp × nq matrix
A ⊗B =





a11B a12B · · · a1nB
a21B a22B · · · a2nB
...
...
...
...
am1B am2B · · · amnB




.
It can be shown that ⊗is associative and that
(A ⊗B)(C ⊗D) = AC ⊗BD
(A ⊗B)⊤= A⊤⊗B⊤,
whenever AC and BD are well deﬁned. Then it is immediately veriﬁed
that Wn is given by the following neat recursive equations:
Wn =

Wn−1 ⊗
1
1

I2n−1 ⊗
 1
−1

,
with W0 = (1). If we let
B1 = 2
1 0
0 1

=
2 0
0 2


4.4. Multiresolution Signal Analysis with Haar Bases
111
and for n ≥1,
Bn+1 = 2
Bn 0
0 I2n

,
then it is not hard to use the Kronecker product formulation of Wn to
obtain a rigorous proof of the equation
W ⊤
n Wn = Bn,
for all n ≥1.
The above equation oﬀers a clean justiﬁcation of the fact that the columns
of Wn are pairwise orthogonal.
Observe that the right block (of size 2n × 2n−1) shows clearly how the
detail coeﬃcients in the second half of the vector c are added and subtracted
to the entries in the ﬁrst half of the partially reconstructed vector after n−1
steps.
4.4
Multiresolution Signal Analysis with Haar Bases
An important and attractive feature of the Haar basis is that it pro-
vides a multiresolution analysis of a signal. Indeed, given a signal u, if
c = (c1, . . . , c2n) is the vector of its Haar coeﬃcients, the coeﬃcients with
low index give coarse information about u, and the coeﬃcients with high
index represent ﬁne information. For example, if u is an audio signal cor-
responding to a Mozart concerto played by an orchestra, c1 corresponds to
the "background noise," c2 to the bass, c3 to the ﬁrst cello, c4 to the second
cello, c5, c6, c7, c7 to the violas, then the violins, etc. This multiresolution
feature of wavelets can be exploited to compress a signal, that is, to use
fewer coeﬃcients to represent it. Here is an example.
Consider the signal
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3),
whose Haar transform is
c = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).
The piecewise-linear curves corresponding to u and c are shown in Fig-
ure 4.6. Since some of the coeﬃcients in c are small (smaller than or equal
to 0.2) we can compress c by replacing them by 0. We get
c2 = (2, 0, 0, 3, 0, 0, 2, 0),
and the reconstructed signal is
u2 = (2, 2, 2, 2, 7, 3, −1, −1).

112
Haar Bases, Haar Wavelets, Hadamard Matrices
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2
1
0
1
2
3
4
5
6
7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
Fig. 4.6
A signal and its Haar transform.
The piecewise-linear curves corresponding to u2 and c2 are shown in Fig-
ure 4.7.
An interesting (and amusing) application of the Haar wavelets is to the
compression of audio signals. It turns out that if your type load handel in
Matlab an audio ﬁle will be loaded in a vector denoted by y, and if you type
sound(y), the computer will play this piece of music. You can convert y to
its vector of Haar coeﬃcients c. The length of y is 73113, so ﬁrst truncate
the tail of y to get a vector of length 65536 = 216. A plot of the signals
corresponding to y and c is shown in Figure 4.8. Then run a program that
sets all coeﬃcients of c whose absolute value is less that 0.05 to zero. This
sets 37272 coeﬃcients to 0. The resulting vector c2 is converted to a signal
y2. A plot of the signals corresponding to y2 and c2 is shown in Figure 4.9.
When you type sound(y2), you ﬁnd that the music doesn't diﬀer much

4.5. Haar Transform for Digital Images
113
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
0
1
2
3
4
5
6
7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
Fig. 4.7
A compressed signal and its compressed Haar transform.
from the original, although it sounds less crisp. You should play with other
numbers greater than or less than 0.05. You should hear what happens
when you type sound(c). It plays the music corresponding to the Haar
transform c of y, and it is quite funny.
4.5
Haar Transform for Digital Images
Another neat property of the Haar transform is that it can be instantly
generalized to matrices (even rectangular) without any extra eﬀort! This
allows for the compression of digital images. But ﬁrst we address the issue
of normalization of the Haar coeﬃcients. As we observed earlier, the 2n×2n
matrix Wn of Haar basis vectors has orthogonal columns, but its columns
do not have unit length. As a consequence, W ⊤
n is not the inverse of Wn,

114
Haar Bases, Haar Wavelets, Hadamard Matrices
0
1
2
3
4
5
6
7
x 10
4
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
6
7
x 10
4
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Fig. 4.8
The signal "handel" and its Haar transform.
but rather the matrix
W −1
n
= DnW ⊤
n
with
Dn = diag

2−n, 2−n
|{z}
20
, 2−(n−1), 2−(n−1)
|
{z
}
21
, 2−(n−2), . . . , 2−(n−2)
|
{z
}
22
, . . . ,
2−1, . . . , 2−1
|
{z
}
2n−1

.
Deﬁnition 4.5. The orthogonal matrix
Hn = WnD
1
2n

4.5. Haar Transform for Digital Images
115
0
1
2
3
4
5
6
7
x 10
4
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
x 10
4
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Fig. 4.9
The compressed signal "handel" and its Haar transform.
whose columns are the normalized Haar basis vectors, with
D
1
2n = diag

2−n
2 , 2−n
2
|{z}
20
, 2−n−1
2 , 2−n−1
2
|
{z
}
21
, 2−n−2
2 , . . . , 2−n−2
2
|
{z
}
22
, . . . , 2−1
2 , . . . , 2−1
2
|
{z
}
2n−1

is called the normalized Haar transform matrix. Given a vector (signal) u,
we call c = H⊤
n u the normalized Haar coeﬃcients of u.
Because Hn is orthogonal, H−1
n
= H⊤
n .
Then a moment of reﬂection shows that we have to slightly modify the
algorithms to compute H⊤
n u and Hnc as follows: When computing the

116
Haar Bases, Haar Wavelets, Hadamard Matrices
sequence of ujs, use
uj+1(2i −1) = (uj(i) + uj(2j + i))/
√
2
uj+1(2i) = (uj(i) −uj(2j + i))/
√
2,
and when computing the sequence of cjs, use
cj(i) = (cj+1(2i −1) + cj+1(2i))/
√
2
cj(2j + i) = (cj+1(2i −1) −cj+1(2i))/
√
2.
Note that things are now more symmetric, at the expense of a division
by
√
2. However, for long vectors, it turns out that these algorithms are
numerically more stable.
Remark: Some authors (for example, Stollnitz, Derose and Salesin [Stoll-
nitz et al. (1996)]) rescale c by 1/
√
2n and u by
√
2n. This is because the
norm of the basis functions ψj
k is not equal to 1 (under the inner product
⟨f, g⟩=
R 1
0 f(t)g(t)dt). The normalized basis functions are the functions
√
2jψj
k.
Let us now explain the 2D version of the Haar transform. We describe
the version using the matrix Wn, the method using Hn being identical
(except that H−1
n
= H⊤
n , but this does not hold for W −1
n ). Given a 2m ×2n
matrix A, we can ﬁrst convert the rows of A to their Haar coeﬃcients using
the Haar transform W −1
n , obtaining a matrix B, and then convert the
columns of B to their Haar coeﬃcients, using the matrix W −1
m . Because
columns and rows are exchanged in the ﬁrst step,
B = A(W −1
n )⊤,
and in the second step C = W −1
m B, thus, we have
C = W −1
m A(W −1
n )⊤= DmW ⊤
mAWn Dn.
In the other direction, given a 2m × 2n matrix C of Haar coeﬃcients, we
reconstruct the matrix A (the image) by ﬁrst applying Wm to the columns
of C, obtaining B, and then W ⊤
n to the rows of B. Therefore
A = WmCW ⊤
n .
Of course, we don't actually have to invert Wm and Wn and perform matrix
multiplications. We just have to use our algorithms using averaging and
diﬀerencing. Here is an example.

4.5. Haar Transform for Digital Images
117
If the data matrix (the image) is the 8 × 8 matrix
A =













64 2 3 61 60 6 7 57
9 55 54 12 13 51 50 16
17 47 46 20 21 43 42 24
40 26 27 37 36 30 31 33
32 34 35 29 28 38 39 25
41 23 22 44 45 19 18 48
49 15 14 52 53 11 10 56
8 58 59 5 4 62 63 1













,
then applying our algorithms, we ﬁnd that
C =













32.5 0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
4
−4
4
−4
0
0
0
0
4
−4
4
−4
0
0 0.5
0.5
27 −25 23 −21
0
0 −0.5 −0.5 −11
9
−7
5
0
0 0.5
0.5
−5
7
−9 11
0
0 −0.5 −0.5 21 −23 25 −27













.
As we can see, C has more zero entries than A; it is a compressed version of
A. We can further compress C by setting to 0 all entries of absolute value
at most 0.5. Then we get
C2 =













32.5 0 0 0
0
0
0
0
0
0 0 0
0
0
0
0
0
0 0 0
4
−4
4
−4
0
0 0 0
4
−4
4
−4
0
0 0 0 27 −25 23 −21
0
0 0 0 −11
9
−7
5
0
0 0 0 −5
7
−9 11
0
0 0 0 21 −23 25 −27













.
We ﬁnd that the reconstructed image is
A2 =













63.5 1.5 3.5 61.5 59.5 5.5 7.5 57.5
9.5 55.5 53.5 11.5 13.5 51.5 49.5 15.5
17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5
39.5 25.5 27.5 37.5 35.5 29.5 31.5 33.5
31.5 33.5 35.5 29.5 27.5 37.5 39.5 25.5
41.5 23.5 21.5 43.5 45.5 19.5 17.5 47.5
49.5 15.5 13.5 51.5 53.5 11.5 9.5 55.5
7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5













,

118
Haar Bases, Haar Wavelets, Hadamard Matrices
which is pretty close to the original image matrix A.
It turns out that Matlab has a wonderful command, image(X) (also
imagesc(X), which often does a better job), which displays the matrix X
has an image in which each entry is shown as a little square whose gray
level is proportional to the numerical value of that entry (lighter if the value
is higher, darker if the value is closer to zero; negative values are treated as
zero). The images corresponding to A and C are shown in Figure 4.10. The
compressed images corresponding to A2 and C2 are shown in Figure 4.11.
The compressed versions appear to be indistinguishable from the originals!
If we use the normalized matrices Hm and Hn, then the equations
Fig. 4.10
An image and its Haar transform.

4.5. Haar Transform for Digital Images
119
Fig. 4.11
Compressed image and its Haar transform.
relating the image matrix A and its normalized Haar transform C are
C = H⊤
mAHn
A = HmCH⊤
n .
The Haar transform can also be used to send large images progressively
over the internet. Indeed, we can start sending the Haar coeﬃcients of the
matrix C starting from the coarsest coeﬃcients (the ﬁrst column from top
down, then the second column, etc.), and at the receiving end we can start
reconstructing the image as soon as we have received enough data.
Observe that instead of performing all rounds of averaging and diﬀer-
encing on each row and each column, we can perform partial encoding (and
decoding). For example, we can perform a single round of averaging and

120
Haar Bases, Haar Wavelets, Hadamard Matrices
diﬀerencing for each row and each column. The result is an image consist-
ing of four subimages, where the top left quarter is a coarser version of the
original, and the rest (consisting of three pieces) contain the ﬁnest detail
coeﬃcients. We can also perform two rounds of averaging and diﬀerencing,
or three rounds, etc. The second round of averaging and diﬀerencing is
applied to the top left quarter of the image. Generally, the kth round is
applied to the 2m+1−k × 2n+1−k submatrix consisting of the ﬁrst 2m+1−k
rows and the ﬁrst 2n+1−k columns (1 ≤k ≤n) of the matrix obtained
at the end of the previous round. This process is illustrated on the image
shown in Figure 4.12. The result of performing one round, two rounds,
three rounds, and nine rounds of averaging is shown in Figure 4.13. Since
our images have size 512 × 512, nine rounds of averaging yields the Haar
transform, displayed as the image on the bottom right. The original im-
age has completely disappeared! We leave it as a fun exercise to modify
the algorithms involving averaging and diﬀerencing to perform k rounds of
averaging/diﬀerencing. The reconstruction algorithm is a little tricky.
Fig. 4.12
Original drawing by Durer.

4.5. Haar Transform for Digital Images
121
Fig. 4.13
Haar tranforms after one, two, three, and nine rounds of averaging.
A nice and easily accessible account of wavelets and their uses in image
processing and computer graphics can be found in Stollnitz, Derose and
Salesin [Stollnitz et al. (1996)]. A very detailed account is given in Strang
and and Nguyen [Strang and Truong (1997)], but this book assumes a fair
amount of background in signal processing.
We can ﬁnd easily a basis of 2n×2n = 22n vectors wij (2n×2n matrices)
for the linear map that reconstructs an image from its Haar coeﬃcients, in
the sense that for any 2n × 2n matrix C of Haar coeﬃcients, the image

122
Haar Bases, Haar Wavelets, Hadamard Matrices
matrix A is given by
A =
2n
X
i=1
2n
X
j=1
cijwij.
Indeed, the matrix wij is given by the so-called outer product
wij = wi(wj)⊤.
Similarly, there is a basis of 2n × 2n = 22n vectors hij (2n × 2n matrices)
for the 2D Haar transform, in the sense that for any 2n × 2n matrix A, its
matrix C of Haar coeﬃcients is given by
C =
2n
X
i=1
2n
X
j=1
aijhij.
If the columns of W −1 are w′
1, . . . , w′
2n, then
hij = w′
i(w′
j)⊤.
We leave it as exercise to compute the bases (wij) and (hij) for n = 2, and
to display the corresponding images using the command imagesc.
4.6
Hadamard Matrices
There is another famous family of matrices somewhat similar to Haar ma-
trices, but these matrices have entries +1 and −1 (no zero entries).
Deﬁnition 4.6. A real n × n matrix H is a Hadamard matrix if hij = ±1
for all i, j such that 1 ≤i, j ≤n and if
H⊤H = nIn.
Thus the columns of a Hadamard matrix are pairwise orthogonal. Be-
cause H is a square matrix, the equation H⊤H = nIn shows that H is
invertible, so we also have HH⊤= nIn. The following matrices are exam-
ple of Hadamard matrices:
H2 =
1 1
1 −1

,
H4 =




1 1
1
1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1



,

4.6. Hadamard Matrices
123
and
H8 =













1 1
1
1
1
1
1
1
1 −1 1 −1 1 −1 1 −1
1 1 −1 −1 1
1 −1 −1
1 −1 −1 1
1 −1 −1 1
1 1
1
1 −1 −1 −1 −1
1 −1 1 −1 −1 1 −1 1
1 1 −1 −1 −1 −1 1
1
1 −1 −1 1 −1 1
1 −1













.
A natural question is to determine the positive integers n for which a
Hadamard matrix of dimension n exists, but surprisingly this is an open
problem. The Hadamard conjecture is that for every positive integer of the
form n = 4k, there is a Hadamard matrix of dimension n.
What is known is a necessary condition and various suﬃcient conditions.
Theorem 4.1. If H is an n × n Hadamard matrix, then either n = 1, 2,
or n = 4k for some positive integer k.
Sylvester introduced a family of Hadamard matrices and proved that
there are Hadamard matrices of dimension n = 2m for all m ≥1 using the
following construction.
Proposition 4.1. (Sylvester, 1867) If H is a Hadamard matrix of dimen-
sion n, then the block matrix of dimension 2n,
H H
H −H

,
is a Hadamard matrix.
If we start with
H2 =
1 1
1 −1

,
we obtain an inﬁnite family of symmetric Hadamard matrices usually
called Sylvester-Hadamard matrices and denoted by H2m. The Sylvester-
Hadamard matrices H2, H4 and H8 are shown on the previous page.
In 1893, Hadamard gave examples of Hadamard matrices for n = 12
and n = 20. At the present, Hadamard matrices are known for all n =
4k ≤1000, except for n = 668, 716, and 892.
Hadamard matrices have various applications to error correcting codes,
signal processing, and numerical linear algebra; see Seberry, Wysocki and

124
Haar Bases, Haar Wavelets, Hadamard Matrices
Wysocki [Seberry et al. (2005)] and Tropp [Tropp (2011)]. For example,
there is a code based on H32 that can correct 7 errors in any 32-bit encoded
block, and can detect an eighth. This code was used on a Mariner spacecraft
in 1969 to transmit pictures back to the earth.
For every m ≥0, the piecewise aﬃne functions plf((H2m)i) associated
with the 2m rows of the Sylvester-Hadamard matrix H2m are functions on
[0, 1] known as the Walsh functions. It is customary to index these 2m
functions by the integers 0, 1, . . . , 2m −1 in such a way that the Walsh
function Wal(k, t) is equal to the function plf((H2m)i) associated with the
Row i of H2m that contains k changes of signs between consecutive groups
of +1 and consecutive groups of −1. For example, the ﬁfth row of H8,
namely
 1 −1 −1 1 1 −1 −1 1

,
has ﬁve consecutive blocks of +1s and −1s, four sign changes between these
blocks, and thus is associated with Wal(4, t). In particular, Walsh functions
corresponding to the rows of H8 (from top down) are:
Wal(0, t), Wal(7, t), Wal(3, t), Wal(4, t),
Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).
Because of the connection between Sylvester-Hadamard matrices and
Walsh functions, Sylvester-Hadamard matrices are called Walsh-Hadamard
matrices by some authors. For every m, the 2m Walsh functions are pair-
wise orthogonal.
The countable set of Walsh functions Wal(k, t) for all
m ≥0 and all k such that 0 ≤k ≤2m −1 can be ordered in such a way
that it is an orthogonal Hilbert basis of the Hilbert space L2([0, 1)]; see
Seberry, Wysocki and Wysocki [Seberry et al. (2005)].
The Sylvester-Hadamard matrix H2m plays a role in various algorithms
for dimension reduction and low-rank matrix approximation. There is a
type of structured dimension-reduction map known as the subsampled ran-
domized Hadamard transform, for short SRHT; see Tropp [Tropp (2011)]
and Halko, Martinsson and Tropp [Halko et al. (2011)]. For ℓ≪n = 2m,
an SRHT matrix is an ℓ× n matrix of the form
Φ =
rn
ℓRHD,
where
(1) D is a random n × n diagonal matrix whose entries are independent
random signs.

4.7. Summary
125
(2) H = n−1/2Hn, a normalized Sylvester-Hadamard matrix of dimension
n.
(3) R is a random ℓ× n matrix that restricts an n-dimensional vector to ℓ
coordinates, chosen uniformly at random.
It is explained in Tropp [Tropp (2011)] that for any input x such that
∥x∥2 = 1, the probability that |(HDx)i| ≥
p
n−1 log(n) for any i is quite
small. Thus HD has the eﬀect of "ﬂattening" the input x. The main result
about the SRHT is that it preserves the geometry of an entire subspace of
vectors; see Tropp [Tropp (2011)] (Theorem 1.3).
4.7
Summary
The main concepts and results of this chapter are listed below:
• Haar basis vectors and a glimpse at Haar wavelets.
• Kronecker product (or tensor product) of matrices.
• Hadamard and Sylvester-Hadamard matrices.
• Walsh functions.
4.8
Problems
Problem 4.1. (Haar extravaganza) Consider the matrix
W3,3 =













1 0 0 0 1
0
0
0
1 0 0 0 −1 0
0
0
0 1 0 0 0
1
0
0
0 1 0 0 0 −1 0
0
0 0 1 0 0
0
1
0
0 0 1 0 0
0 −1 0
0 0 0 1 0
0
0
1
0 0 0 1 0
0
0 −1













.
(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result
W3,3c of applying W3,3 to c is
W3,3c = (c1 + c5, c1 −c5, c2 + c6, c2 −c6, c3 + c7, c3 −c7, c4 + c8, c4 −c8),
the last step in reconstructing a vector from its Haar coeﬃcients.
(2) Prove that the inverse of W3,3 is (1/2)W ⊤
3,3. Prove that the columns
and the rows of W3,3 are orthogonal.

126
Haar Bases, Haar Wavelets, Hadamard Matrices
(3) Let W3,2 and W3,1 be the following matrices:
W3,2 =













1 0 1
0 0 0 0 0
1 0 −1 0 0 0 0 0
0 1 0
1 0 0 0 0
0 1 0 −1 0 0 0 0
0 0 0
0 1 0 0 0
0 0 0
0 0 1 0 0
0 0 0
0 0 0 1 0
0 0 0
0 0 0 0 1













,
W3,1 =













1 1 0 0 0 0 0 0
1 −1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1













.
Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c
of applying W3,2 to c is
W3,2c = (c1 + c3, c1 −c3, c2 + c4, c2 −c4, c5, c6, c7, c8),
the second step in reconstructing a vector from its Haar coeﬃcients, and
the result W3,1c of applying W3,1 to c is
W3,1c = (c1 + c2, c1 −c2, c3, c4, c5, c6, c7, c8),
the ﬁrst step in reconstructing a vector from its Haar coeﬃcients.
Conclude that
W3,3W3,2W3,1 = W3,
the Haar matrix
W3 =













1 1
1
0
1
0
0
0
1 1
1
0 −1 0
0
0
1 1 −1 0
0
1
0
0
1 1 −1 0
0 −1 0
0
1 −1 0
1
0
0
1
0
1 −1 0
1
0
0 −1 0
1 −1 0 −1 0
0
0
1
1 −1 0 −1 0
0
0 −1













.
Hint. First check that
W3,2W3,1 =
W2 04,4
04,4 I4

,
where
W2 =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1



.

4.8. Problems
127
(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogo-
nal. Deduce from this that the columns of W3 are orthogonal, and the rows
of W −1
3
are orthogonal. Are the rows of W3 orthogonal? Are the columns
of W −1
3
orthogonal? Find the inverse of W3,2 and the inverse of W3,1.
Problem 4.2. This is a continuation of Problem 4.1.
(1) For any n ≥2, the 2n × 2n matrix Wn,n is obtained form the two
rows
1, 0, . . . , 0
|
{z
}
2n−1
, 1, 0, . . . , 0
|
{z
}
2n−1
1, 0, . . . , 0
|
{z
}
2n−1
, −1, 0, . . . , 0
|
{z
}
2n−1
by shifting them 2n−1 −1 times over to the right by inserting a zero on the
left each time.
Given any vector c = (c1, c2, . . . , c2n), show that Wn,nc is the result
of the last step in the process of reconstructing a vector from its Haar
coeﬃcients c. Prove that W −1
n,n = (1/2)W ⊤
n,n, and that the columns and the
rows of Wn,n are orthogonal.
(2) Given a m × n matrix A = (aij) and a p × q matrix B = (bij), the
Kronecker product (or tensor product) A ⊗B of A and B is the mp × nq
matrix
A ⊗B =





a11B a12B · · · a1nB
a21B a22B · · · a2nB
...
...
...
...
am1B am2B · · · amnB




.
It can be shown (and you may use these facts without proof) that ⊗is
associative and that
(A ⊗B)(C ⊗D) = AC ⊗BD
(A ⊗B)⊤= A⊤⊗B⊤,
whenever AC and BD are well deﬁned.
Check that
Wn,n =

I2n−1 ⊗
1
1

I2n−1 ⊗
 1
−1

,
and that
Wn =

Wn−1 ⊗
1
1

I2n−1 ⊗
 1
−1

.

128
Haar Bases, Haar Wavelets, Hadamard Matrices
Use the above to reprove that
Wn,nW ⊤
n,n = 2I2n.
Let
B1 = 2
1 0
0 1

=
2 0
0 2

and for n ≥1,
Bn+1 = 2
Bn 0
0 I2n

.
Prove that
W ⊤
n Wn = Bn,
for all n ≥1.
(3) The matrix Wn,i is obtained from the matrix Wi,i (1 ≤i ≤n −1)
as follows:
Wn,i =

Wi,i
02i,2n−2i
02n−2i,2i
I2n−2i

.
It consists of four blocks, where 02i,2n−2i and 02n−2i,2i are matrices of zeros
and I2n−2i is the identity matrix of dimension 2n −2i.
Explain what Wn,i does to c and prove that
Wn,nWn,n−1 · · · Wn,1 = Wn,
where Wn is the Haar matrix of dimension 2n.
Hint. Use induction on k, with the induction hypothesis
Wn,kWn,k−1 · · · Wn,1 =

Wk
02k,2n−2k
02n−2k,2k
I2n−2k

.
Prove that the columns and rows of Wn,k are orthogonal, and use this
to prove that the columns of Wn and the rows of W −1
n
are orthogonal. Are
the rows of Wn orthogonal? Are the columns of W −1
n
orthogonal? Prove
that
W −1
n,k =

1
2W ⊤
k,k
02k,2n−2k
02n−2k,2k
I2n−2k

.
Problem 4.3. Prove that if H is a Hadamard matrix of dimension n, then
the block matrix of dimension 2n,
H H
H −H

,
is a Hadamard matrix.
Problem 4.4. Plot the graphs of the eight Walsh functions Wal(k, t) for
k = 0, 1, . . . , 7.
Problem 4.5. Describe a recursive algorithm to compute the product
H2m x of the Sylvester-Hadamard matrix H2m by a vector x ∈R2m that
uses m recursive calls.

Chapter 5
Direct Sums, Rank-Nullity Theorem,
Aﬃne Maps
In this chapter all vector spaces are deﬁned over an arbitrary ﬁeld K. For
the sake of concreteness, the reader may safely assume that K = R.
5.1
Direct Products
There are some useful ways of forming new vector spaces from older ones.
Deﬁnition 5.1. Given p ≥2 vector spaces E1, . . . , Ep, the product F =
E1 × · · · × Ep can be made into a vector space by deﬁning addition and
scalar multiplication as follows:
(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)
λ(u1, . . . , up) = (λu1, . . . , λup),
for all ui, vi ∈Ei and all λ ∈R. The zero vector of E1 × · · · × Ep is the
p-tuple
( 0, . . . , 0
| {z }
p
),
where the ith zero is the zero vector of Ei.
With the above addition and multiplication, the vector space F = E1 ×
· · · × Ep is called the direct product of the vector spaces E1, . . . , Ep.
As a special case, when E1 = · · · = Ep = R, we ﬁnd again the vector
space F = Rp. The projection maps pri : E1 × · · · × Ep →Ei given by
pri(u1, . . . , up) = ui
are clearly linear. Similarly, the maps ini : Ei →E1 × · · · × Ep given by
ini(ui) = (0, . . . , 0, ui, 0, . . . , 0)
129

130
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
are injective and linear. If dim(Ei) = ni and if (ei
1, . . . , ei
ni) is a basis of Ei
for i = 1, . . . , p, then it is easy to see that the n1 + · · · + np vectors
(e1
1, 0, . . . , 0),
. . . ,
(e1
n1, 0, . . . , 0),
...
...
...
(0, . . . , 0, ei
1, 0, . . . , 0), . . . , (0, . . . , 0, ei
ni, 0, . . . , 0),
...
...
...
(0, . . . , 0, ep
1),
. . . ,
(0, . . . , 0, ep
np)
form a basis of E1 × · · · × Ep, and so
dim(E1 × · · · × Ep) = dim(E1) + · · · + dim(Ep).
5.2
Sums and Direct Sums
Let us now consider a vector space E and p subspaces U1, . . . , Up of E. We
have a map
a: U1 × · · · × Up →E
given by
a(u1, . . . , up) = u1 + · · · + up,
with ui ∈Ui for i = 1, . . . , p. It is clear that this map is linear, and so its
image is a subspace of E denoted by
U1 + · · · + Up
and called the sum of the subspaces U1, . . . , Up. By deﬁnition,
U1 + · · · + Up = {u1 + · · · + up | ui ∈Ui, 1 ≤i ≤p},
and it is immediately veriﬁed that U1 + · · · + Up is the smallest subspace
of E containing U1, . . . , Up. This also implies that U1 + · · · + Up does not
depend on the order of the factors Ui; in particular,
U1 + U2 = U2 + U1.
Deﬁnition 5.2. For any vector space E and any p ≥2 subspaces U1, . . . , Up
of E, if the map a: U1 × · · · × Up →E deﬁned above is injective, then the
sum U1 + · · · + Up is called a direct sum and it is denoted by
U1 ⊕· · · ⊕Up.
The space E is the direct sum of the subspaces Ui if
E = U1 ⊕· · · ⊕Up.

5.2. Sums and Direct Sums
131
If the map a is injective, then by Proposition 2.13 we have Ker a =
{( 0, . . . , 0
| {z }
p
)} where each 0 is the zero vector of E, which means that if
ui ∈Ui for i = 1, . . . , p and if
u1 + · · · + up = 0,
then (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.
Proposition 5.1. If the map a: U1 × · · · × Up →E is injective, then every
u ∈U1 + · · · + Up has a unique expression as a sum
u = u1 + · · · + up,
with ui ∈Ui, for i = 1, . . . , p.
Proof. If
u = v1 + · · · + vp = w1 + · · · + wp,
with vi, wi ∈Ui, for i = 1, . . . , p, then we have
w1 −v1 + · · · + wp −vp = 0,
and since vi, wi ∈Ui and each Ui is a subspace, wi−vi ∈Ui. The injectivity
of a implies that wi −vi = 0, that is, wi = vi for i = 1, . . . , p, which shows
the uniqueness of the decomposition of u.
Proposition 5.2. If the map a: U1 × · · · × Up →E is injective, then any
p nonzero vectors u1, . . . , up with ui ∈Ui are linearly independent.
Proof. To see this, assume that
λ1u1 + · · · + λpup = 0
for some λi ∈R. Since ui ∈Ui and Ui is a subspace, λiui ∈Ui, and the
injectivity of a implies that λiui = 0, for i = 1, . . . , p. Since ui ̸= 0, we
must have λi = 0 for i = 1, . . . , p; that is, u1, . . . , up with ui ∈Ui and
ui ̸= 0 are linearly independent.
Observe that if a is injective, then we must have Ui ∩Uj = (0) whenever
i ̸= j. However, this condition is generally not suﬃcient if p ≥3. For
example, if E = R2 and U1 the line spanned by e1 = (1, 0), U2 is the
line spanned by d = (1, 1), and U3 is the line spanned by e2 = (0, 1), then
U1 ∩U2 = U1 ∩U3 = U2 ∩U3 = {(0, 0)}, but U1 +U2 = U1 +U3 = U2 +U3 =

132
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
e1
U1
e
U3
2
(1,1)
U2
Fig. 5.1
The linear subspaces U1, U2, and U3 illustrated as lines in R2.
R2, so U1 + U2 + U3 is not a direct sum. For example, d is expressed in two
diﬀerent ways as
d = (1, 1) = (1, 0) + (0, 1) = e1 + e2.
See Figure 5.1.
As in the case of a sum, U1 ⊕U2 = U2 ⊕U1. Observe that when the
map a is injective, then it is a linear isomorphism between U1 × · · · × Up
and U1 ⊕· · · ⊕Up. The diﬀerence is that U1 × · · · × Up is deﬁned even if
the spaces Ui are not assumed to be subspaces of some common space.
If E is a direct sum E = U1 ⊕· · · ⊕Up, since any p nonzero vectors
u1, . . . , up with ui ∈Ui are linearly independent, if we pick a basis (uk)k∈Ij
in Uj for j = 1, . . . , p, then (ui)i∈I with I = I1 ∪· · · ∪Ip is a basis of E.
Intuitively, E is split into p independent subspaces.
Conversely, given a basis (ui)i∈I of E, if we partition the index set I as
I = I1 ∪· · · ∪Ip, then each subfamily (uk)k∈Ij spans some subspace Uj of
E, and it is immediately veriﬁed that we have a direct sum
E = U1 ⊕· · · ⊕Up.
Deﬁnition 5.3. Let f : E →E be a linear map. For any subspace U of E,
if f(U) ⊆U we say that U is invariant under f.

5.2. Sums and Direct Sums
133
Assume that E is ﬁnite-dimensional, a direct sum E = U1 ⊕· · · ⊕Up,
and that each Uj is invariant under f. If we pick a basis (ui)i∈I as above
with I = I1 ∪· · · ∪Ip and with each (uk)k∈Ij a basis of Uj, since each Uj
is invariant under f, the image f(uk) of every basis vector uk with k ∈Ij
belongs to Uj, so the matrix A representing f over the basis (ui)i∈I is a
block diagonal matrix of the form
A =





A1
A2
...
Ap




,
with each block Aj a dj ×dj-matrix with dj = dim(Uj) and all other entries
equal to 0. If dj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.
There are natural injections from each Ui to E denoted by ini : Ui →E.
Now, if p = 2, it is easy to determine the kernel of the map a: U1×U2 →
E. We have
a(u1, u2) = u1 + u2 = 0
iﬀ
u1 = −u2, u1 ∈U1, u2 ∈U2,
which implies that
Ker a = {(u, −u) | u ∈U1 ∩U2}.
Now, U1∩U2 is a subspace of E and the linear map u 7→(u, −u) is clearly an
isomorphism between U1 ∩U2 and Ker a, so Ker a is isomorphic to U1 ∩U2.
As a consequence, we get the following result:
Proposition 5.3. Given any vector space E and any two subspaces U1 and
U2, the sum U1 + U2 is a direct sum iﬀU1 ∩U2 = (0).
An interesting illustration of the notion of direct sum is the decompo-
sition of a square matrix into its symmetric part and its skew-symmetric
part.
Recall that an n × n matrix A ∈Mn is symmetric if A⊤= A,
skew-symmetric if A⊤= −A. It is clear that
S(n) = {A ∈Mn | A⊤= A}
and
Skew(n) = {A ∈Mn | A⊤= −A}
are subspaces of Mn, and that S(n) ∩Skew(n) = (0). Observe that for
any matrix A ∈Mn, the matrix H(A) = (A + A⊤)/2 is symmetric and the
matrix S(A) = (A −A⊤)/2 is skew-symmetric. Since
A = H(A) + S(A) = A + A⊤
2
+ A −A⊤
2
,

134
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
we see that Mn = S(n) + Skew(n), and since S(n) ∩Skew(n) = (0), we
have the direct sum
Mn = S(n) ⊕Skew(n).
Remark: The vector space Skew(n) of skew-symmetric matrices is also
denoted by so(n). It is the Lie algebra of the group SO(n).
Proposition 5.3 can be generalized to any p ≥2 subspaces at the expense
of notation. The proof of the following proposition is left as an exercise.
Proposition 5.4. Given any vector space E and any p ≥2 subspaces
U1, . . . , Up, the following properties are equivalent:
(1) The sum U1 + · · · + Up is a direct sum.
(2) We have
Ui ∩

p
X
j=1,j̸=i
Uj

= (0),
i = 1, . . . , p.
(3) We have
Ui ∩
 i−1
X
j=1
Uj

= (0),
i = 2, . . . , p.
Because of the isomorphism
U1 × · · · × Up ≈U1 ⊕· · · ⊕Up,
we have
Proposition 5.5. If E is any vector space, for any (ﬁnite-dimensional)
subspaces U1, . . ., Up of E, we have
dim(U1 ⊕· · · ⊕Up) = dim(U1) + · · · + dim(Up).
If E is a direct sum
E = U1 ⊕· · · ⊕Up,
since every u ∈E can be written in a unique way as
u = u1 + · · · + up
with ui ∈Ui for i = 1 . . . , p, we can deﬁne the maps πi : E →Ui, called
projections, by
πi(u) = πi(u1 + · · · + up) = ui.

5.2. Sums and Direct Sums
135
It is easy to check that these maps are linear and satisfy the following
properties:
πj ◦πi =
(
πi
if i = j
0
if i ̸= j,
π1 + · · · + πp = idE.
For example, in the case of the direct sum
Mn = S(n) ⊕Skew(n),
the projection onto S(n) is given by
π1(A) = H(A) = A + A⊤
2
,
and the projection onto Skew(n) is given by
π2(A) = S(A) = A −A⊤
2
.
Clearly, H(A) + S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and
H(S(A)) = S(H(A)) = 0.
A function f such that f ◦f = f is said to be idempotent. Thus, the
projections πi are idempotent. Conversely, the following proposition can
be shown:
Proposition 5.6. Let E be a vector space. For any p ≥2 linear maps
fi : E →E, if
fj ◦fi =
(
fi
if i = j
0
if i ̸= j,
f1 + · · · + fp = idE,
then if we let Ui = fi(E), we have a direct sum
E = U1 ⊕· · · ⊕Up.
We also have the following proposition characterizing idempotent linear
maps whose proof is also left as an exercise.
Proposition 5.7. For every vector space E, if f : E →E is an idempotent
linear map, i.e., f ◦f = f, then we have a direct sum
E = Ker f ⊕Im f,
so that f is the projection onto its image Im f.
We are now ready to prove a very crucial result relating the rank and
the dimension of the kernel of a linear map.

136
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
5.3
The Rank-Nullity Theorem; Grassmann's Relation
We begin with the following theorem which shows that given a linear map
f : E →F, its domain E is the direct sum of its kernel Ker f with some
isomorphic copy of its image Im f.
Theorem 5.1. (Rank-nullity theorem) Let f : E →F be a linear map with
ﬁnite image. For any choice of a basis (f1, . . . , fr) of Im f, let (u1, . . . , ur)
be any vectors in E such that fi = f(ui), for i = 1, . . . , r. If s: Im f →E
is the unique linear map deﬁned by s(fi) = ui, for i = 1, . . . , r, then s is
injective, f ◦s = id, and we have a direct sum
E = Ker f ⊕Im s
as illustrated by the following diagram:
Ker f
 E = Ker f ⊕Im s
f
 Im f ⊆F.
s

See Figure 5.2. As a consequence, if E is ﬁnite-dimensional, then
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).
u = (0,1,1)
2
u = (1,0,1)
1
Ker f
f  = f(u  ) = (1,0)
1
1
f  =  f(u  ) = (0, 1)
2
2
f(u) = (1,1)
f(x,y,z) = (x,y)
f  = f(u  ) = (
1 1 
1 1 
 f(u  ) = (0, 1)
2 2 
f(u) =
u = (1,0,1)
1
s(x,y) = (x,y,x+y)
u = (1,1,1)
s (f(u)) = (1,1,2)
h = (0,0,-1)
Fig. 5.2
Let f : E →F be the linear map from R3 to R2 given by f(x, y, z) = (x, y).
Then s: R2 →R3 is given by s(x, y) = (x, y, x+y) and maps the pink R2 isomorphically
onto the slanted pink plane of R3 whose equation is −x −y + z = 0. Theorem 5.1 shows
that R3 is the direct sum of the plane −x −y + z = 0 and the kernel of f which the
orange z-axis.

5.3. The Rank-Nullity Theorem; Grassmann's Relation
137
Proof. The vectors u1, . . . , ur must be linearly independent since otherwise
we would have a nontrivial linear dependence
λ1u1 + · · · + λrur = 0,
and by applying f, we would get the nontrivial linear dependence
0 = λ1f(u1) + · · · + λrf(ur) = λ1f1 + · · · + λrfr,
contradicting the fact that (f1, . . . , fr) is a basis. Therefore, the unique
linear map s given by s(fi) = ui, for i = 1, . . . , r, is a linear isomorphism
between Im f and its image, the subspace spanned by (u1, . . . , ur). It is
also clear by deﬁnition that f ◦s = id. For any u ∈E, let
h = u −(s ◦f)(u).
Since f ◦s = id, we have
f(h) = f(u −(s ◦f)(u)) = f(u) −(f ◦s ◦f)(u)
= f(u) −(id ◦f)(u) = f(u) −f(u) = 0,
which shows that h ∈Ker f. Since h = u −(s ◦f)(u), it follows that
u = h + s(f(u)),
with h ∈Ker f and s(f(u)) ∈Im s, which proves that
E = Ker f + Im s.
Now if u ∈Ker f ∩Im s, then u = s(v) for some v ∈F and f(u) = 0 since
u ∈Ker f. Since u = s(v) and f ◦s = id, we get
0 = f(u) = f(s(v)) = v,
and so u = s(v) = s(0) = 0. Thus, Ker f ∩Im s = (0), which proves that
we have a direct sum
E = Ker f ⊕Im s.
The equation
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)
is an immediate consequence of the fact that the dimension is an additive
property for direct sums, that by deﬁnition the rank of f is the dimen-
sion of the image of f, and that dim(Im s) = dim(Im f), because s is an
isomorphism between Im f and Im s.

138
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
Remark: The statement E = Ker f ⊕Im s holds if E has inﬁnite dimen-
sion. It still holds if Im (f) also has inﬁnite dimension.
Deﬁnition 5.4.
The dimension dim(Ker f) of the kernel of a linear map
f is called the nullity of f.
We now derive some important results using Theorem 5.1.
Proposition 5.8. Given a vector space E, if U and V are any two ﬁnite-
dimensional subspaces of E, then
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩V ),
an equation known as Grassmann's relation.
Proof. Recall that U + V is the image of the linear map
a: U × V →E
given by
a(u, v) = u + v,
and that we proved earlier that the kernel Ker a of a is isomorphic to U ∩V .
By Theorem 5.1,
dim(U × V ) = dim(Ker a) + dim(Im a),
but dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩V ), and
Im a = U + V , so the Grassmann relation holds.
The Grassmann relation can be very useful to ﬁgure out whether two
subspace have a nontrivial intersection in spaces of dimension > 3. For
example, it is easy to see that in R5, there are subspaces U and V with
dim(U) = 3 and dim(V ) = 2 such that U ∩V = (0); for example, let U be
generated by the vectors (1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be
generated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim
that if dim(U) = 3 and dim(V ) = 3, then dim(U ∩V ) ≥1. Indeed, by the
Grassmann relation, we have
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩V ),
namely
3 + 3 = 6 = dim(U + V ) + dim(U ∩V ),
and since U + V is a subspace of R5, dim(U + V ) ≤5, which implies
6 ≤5 + dim(U ∩V ),

5.3. The Rank-Nullity Theorem; Grassmann's Relation
139
that is 1 ≤dim(U ∩V ).
As another consequence of Proposition 5.8, if U and V are two hy-
perplanes in a vector space of dimension n, so that dim(U) = n −1 and
dim(V ) = n −1, the reader should show that
dim(U ∩V ) ≥n −2,
and so, if U ̸= V , then
dim(U ∩V ) = n −2.
Here is a characterization of direct sums that follows directly from The-
orem 5.1.
Proposition 5.9. If U1, . . . , Up are any subspaces of a ﬁnite dimensional
vector space E, then
dim(U1 + · · · + Up) ≤dim(U1) + · · · + dim(Up),
and
dim(U1 + · · · + Up) = dim(U1) + · · · + dim(Up)
iﬀthe Uis form a direct sum U1 ⊕· · · ⊕Up.
Proof. If we apply Theorem 5.1 to the linear map
a: U1 × · · · × Up →U1 + · · · + Up
given by a(u1, . . . , up) = u1 + · · · + up, we get
dim(U1 + · · · + Up) = dim(U1 × · · · × Up) −dim(Ker a)
= dim(U1) + · · · + dim(Up) −dim(Ker a),
so the inequality follows. Since a is injective iﬀKer a = (0), the Uis form a
direct sum iﬀthe second equation holds.
Another important corollary of Theorem 5.1 is the following result:
Proposition 5.10. Let E and F be two vector spaces with the same ﬁnite
dimension dim(E) = dim(F) = n. For every linear map f : E →F, the
following properties are equivalent:
(a) f is bijective.
(b) f is surjective.
(c) f is injective.
(d) Ker f = (0).

140
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
Proof. Obviously, (a) implies (b).
If f is surjective, then Im f = F, and so dim(Im f) = n. By Theo-
rem 5.1,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which
means that Ker f = (0), and so f is injective (see Proposition 2.13). This
proves that (b) implies (c).
If f is injective, then by Proposition 2.13, Ker f = (0), so (c) implies
(d).
Finally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is
injective (by Proposition 2.13). By Theorem 5.1,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(Ker f) = 0, we get
dim(Im f) = dim(E) = dim(F),
which proves that f is also surjective, and thus bijective. This proves that
(d) implies (a) and concludes the proof.
One should be warned that Proposition 5.10 fails in inﬁnite dimension.
A linear map may be injective without being surjective and vice versa.
Here are a few applications of Proposition 5.10. Let A be an n × n
matrix and assume that A some right inverse B, which means that B is an
n × n matrix such that
AB = I.
The linear map associated with A is surjective, since for every u ∈Rn, we
have A(Bu) = u. By Proposition 5.10, this map is bijective so B is actually
the inverse of A; in particular BA = I.
Similarly, assume that A has a left inverse B, so that
BA = I.
This time the linear map associated with A is injective, because if Au =
0, then BAu = B0 = 0, and since BA = I we get u = 0.
Again, by
Proposition 5.10, this map is bijective so B is actually the inverse of A; in
particular AB = I.
Now assume that the linear system Ax = b has some solution for every b.
Then the linear map associated with A is surjective and by Proposition 5.10,
A is invertible.

5.3. The Rank-Nullity Theorem; Grassmann's Relation
141
Finally assume that the linear system Ax = b has at most one solution
for every b. Then the linear map associated with A is injective and by
Proposition 5.10, A is invertible.
We also have the following basic proposition about injective or surjective
linear maps.
Proposition 5.11. Let E and F be vector spaces, and let f : E →F be a
linear map. If f : E →F is injective, then there is a surjective linear map
r: F →E called a retraction, such that r ◦f = idE. See Figure 5.3. If
f : E →F is surjective, then there is an injective linear map s: F →E
called a section, such that f ◦s = idF . See Figure 5.2.
u = (1,0)
1
u = (1,1)
2
f(x,y) = (x,y,0)
v = f(u ) = (1,0,0)
1
1
v  = f(u ) = (1,1,0)
2
2
v = (0,0,1)
3
r(x,y,z) = (x,y)
E = !
2
F = !3
Fig. 5.3
Let f : E →F be the injective linear map from R2 to R3 given by f(x, y) =
(x, y, 0). Then a surjective retraction is given by r: R3 →R2 is given by r(x, y, z) =
(x, y). Observe that r(v1) = u1, r(v2) = u2, and r(v3) = 0.
Proof. Let (ui)i∈I be a basis of E. Since f : E →F is an injective linear
map, by Proposition 2.14, (f(ui))i∈I is linearly independent in F.
By
Theorem 2.1, there is a basis (vj)j∈J of F, where I ⊆J, and where vi =
f(ui), for all i ∈I. By Proposition 2.14, a linear map r: F →E can be
deﬁned such that r(vi) = ui, for all i ∈I, and r(vj) = w for all j ∈(J −I),
where w is any given vector in E, say w = 0. Since r(f(ui)) = ui for all
i ∈I, by Proposition 2.14, we have r ◦f = idE.
Now assume that f : E →F is surjective. Let (vj)j∈J be a basis of
F. Since f : E →F is surjective, for every vj ∈F, there is some uj ∈E

142
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
such that f(uj) = vj. Since (vj)j∈J is a basis of F, by Proposition 2.14,
there is a unique linear map s: F →E such that s(vj) = uj. Also since
f(s(vj)) = vj, by Proposition 2.14 (again), we must have f ◦s = idF .
Remark: Proposition 5.11 also holds if E or F has inﬁnite dimension.
The converse of Proposition 5.11 is obvious.
The notion of rank of a linear map or of a matrix important, both
theoretically and practically, since it is the key to the solvability of linear
equations. We have the following simple proposition.
Proposition 5.12. Given a linear map f : E →F, the following properties
hold:
(i) rk(f) + dim(Ker f) = dim(E).
(ii) rk(f) ≤min(dim(E), dim(F)).
Proof. Property (i) follows from Proposition 5.1. As for (ii), since Im f is
a subspace of F, we have rk(f) ≤dim(F), and since rk(f) + dim(Ker f) =
dim(E), we have rk(f) ≤dim(E).
The rank of a matrix is deﬁned as follows.
Deﬁnition 5.5. Given a m × n-matrix A = (ai j), the rank rk(A) of the
matrix A is the maximum number of linearly independent columns of A
(viewed as vectors in Rm).
In view of Proposition 2.8, the rank of a matrix A is the dimension of
the subspace of Rm generated by the columns of A. Let E and F be two
vector spaces, and let (u1, . . . , un) be a basis of E, and (v1, . . . , vm) a basis
of F. Let f : E →F be a linear map, and let M(f) be its matrix w.r.t.
the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the
dimension of Im f, which is generated by (f(u1), . . . , f(un)), the rank of f is
the maximum number of linearly independent vectors in (f(u1), . . . , f(un)),
which is equal to the number of linearly independent columns of M(f),
since F and Rm are isomorphic. Thus, we have rk(f) = rk(M(f)), for
every matrix representing f.
We will see later, using duality, that the rank of a matrix A is also equal
to the maximal number of linearly independent rows of A.

5.4. Aﬃne Maps
143
5.4
Aﬃne Maps
We showed in Section 2.7 that every linear map f must send the zero vector
to the zero vector; that is,
f(0) = 0.
Yet for any ﬁxed nonzero vector u ∈E (where E is any vector space), the
function tu given by
tu(x) = x + u,
for all
x ∈E
shows up in practice (for example, in robotics). Functions of this type are
called translations. They are not linear for u ̸= 0, since tu(0) = 0 + u = u.
More generally, functions combining linear maps and translations occur
naturally in many applications (robotics, computer vision, etc.), so it is
necessary to understand some basic properties of these functions. For this,
the notion of aﬃne combination turns out to play a key role.
Recall from Section 2.7 that for any vector space E, given any family
(ui)i∈I of vectors ui ∈E, an aﬃne combination of the family (ui)i∈I is an
expression of the form
X
i∈I
λiui
with
X
i∈I
λi = 1,
where (λi)i∈I is a family of scalars.
A linear combination places no restriction on the scalars involved, but
an aﬃne combination is a linear combination with the restriction that the
scalars λi must add up to 1. Nevertheless, a linear combination can always
be viewed as an aﬃne combination using the following trick involving 0.
For any family (ui)i∈I of vectors in E and for any family of scalars (λi)i∈I,
we can write the linear combination P
i∈I λiui as an aﬃne combination as
follows:
X
i∈I
λiui =
X
i∈I
λiui +

1 −
X
i∈I
λi

0.
Aﬃne combinations are also called barycentric combinations.
Although this is not obvious at ﬁrst glance, the condition that the scalars
λi add up to 1 ensures that aﬃne combinations are preserved under trans-
lations. To make this precise, consider functions f : E →F, where E and
F are two vector spaces, such that there is some linear map h: E →F and
some ﬁxed vector b ∈F (a translation vector), such that
f(x) = h(x) + b,
for all
x ∈E.

144
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
The map f given by
x1
x2

7→
 8/5 −6/5
3/10 2/5
 x1
x2

+
1
1

is an example of the composition of a linear map with a translation.
We claim that functions of this type preserve aﬃne combinations.
Proposition 5.13. For any two vector spaces E and F, given any function
f : E →F deﬁned such that
f(x) = h(x) + b,
for all
x ∈E,
where h: E →F is a linear map and b is some ﬁxed vector in F, for every
aﬃne combination P
i∈I λiui (with P
i∈I λi = 1), we have
f
 X
i∈I
λiui

=
X
i∈I
λif(ui).
In other words, f preserves aﬃne combinations.
Proof. By deﬁnition of f, using the fact that h is linear and the fact that
P
i∈I λi = 1, we have
f
 X
i∈
λiui

= h
 X
i∈I
λiui

+ b
=
X
i∈I
λih(ui) + 1b
=
X
i∈I
λih(ui) +
 X
i∈I
λi

b
=
X
i∈I
λi(h(ui) + b)
=
X
i∈I
λif(ui),
as claimed.
Observe how the fact that P
i∈I λi = 1 was used in a crucial way in
Line 3. Surprisingly, the converse of Proposition 5.13 also holds.
Proposition 5.14. For any two vector spaces E and F, let f : E →F
be any function that preserves aﬃne combinations, i.e., for every aﬃne
combination P
i∈I λiui (with P
i∈I λi = 1), we have
f
 X
i∈I
λiui

=
X
i∈I
λif(ui).

5.4. Aﬃne Maps
145
Then for any a ∈E, the function h: E →F given by
h(x) = f(a + x) −f(a)
is a linear map independent of a, and
f(a + x) = h(x) + f(a),
for all
x ∈E.
In particular, for a = 0, if we let c = f(0), then
f(x) = h(x) + c,
for all
x ∈E.
Proof. First, let us check that h is linear. Since f preserves aﬃne combi-
nations and since a + u + v = (a + u) + (a + v) −a is an aﬃne combination
(1 + 1 −1 = 1), we have
h(u + v) = f(a + u + v) −f(a)
= f((a + u) + (a + v) −a) −f(a)
= f(a + u) + f(a + v) −f(a) −f(a)
= f(a + u) −f(a) + f(a + v) −f(a)
= h(u) + h(v).
This proves that
h(u + v) = h(u) + h(v),
u, v ∈E.
Observe that a + λu = λ(a + u) + (1 −λ)a is also an aﬃne combination
(λ + 1 −λ = 1), so we have
h(λu) = f(a + λu) −f(a)
= f(λ(a + u) + (1 −λ)a) −f(a)
= λf(a + u) + (1 −λ)f(a) −f(a)
= λ(f(a + u) −f(a))
= λh(u).
This proves that
h(λu) = λh(u),
u ∈E, λ ∈R.
Therefore, h is indeed linear.
For any b ∈E, since b + u = (a + u) −a + b is an aﬃne combination
(1 −1 + 1 = 1), we have
f(b + u) −f(b) = f((a + u) −a + b) −f(b)
= f(a + u) −f(a) + f(b) −f(b)
= f(a + u) −f(a),

146
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
which proves that for all a, b ∈E,
f(b + u) −f(b) = f(a + u) −f(a),
u ∈E.
Therefore h(x) = f(a + u) −f(a) does not depend on a, and it is obvious
by the deﬁnition of h that
f(a + x) = h(x) + f(a),
for all
x ∈E.
For a = 0, we obtain the last part of our proposition.
We should think of a as a chosen origin in E. The function f maps
the origin a in E to the origin f(a) in F. Proposition 5.14 shows that the
deﬁnition of h does not depend on the origin chosen in E. Also, since
f(x) = h(x) + c,
for all
x ∈E
for some ﬁxed vector c ∈F, we see that f is the composition of the linear
map h with the translation tc (in F).
The unique linear map h as above is called the linear map associated
with f, and it is sometimes denoted by −→
f .
In view of Propositions 5.13 and 5.14, it is natural to make the following
deﬁnition.
Deﬁnition 5.6. For any two vector spaces E and F, a function f : E →F
is an aﬃne map if f preserves aﬃne combinations, i.e., for every aﬃne
combination P
i∈I λiui (with P
i∈I λi = 1), we have
f
 X
i∈I
λiui

=
X
i∈I
λif(ui).
Equivalently, a function f : E →F is an aﬃne map if there is some linear
map h: E →F (also denoted by −→
f ) and some ﬁxed vector c ∈F such that
f(x) = h(x) + c,
for all
x ∈E.
Note that a linear map always maps the standard origin 0 in E to the
standard origin 0 in F. However an aﬃne map usually maps 0 to a nonzero
vector c = f(0). This is the "translation component" of the aﬃne map.
When we deal with aﬃne maps, it is often fruitful to think of the el-
ements of E and F not only as vectors but also as points. In this point
of view, points can only be combined using aﬃne combinations, but vec-
tors can be combined in an unrestricted fashion using linear combinations.
We can also think of u + v as the result of translating the point u by the
translation tv. These ideas lead to the deﬁnition of aﬃne spaces.

5.4. Aﬃne Maps
147
The idea is that instead of a single space E, an aﬃne space consists
of two sets E and −→
E , where E is just an unstructured set of points, and
−→
E is a vector space. Furthermore, the vector space −→
E acts on E. We can
think of −→
E as a set of translations speciﬁed by vectors, and given any point
a ∈E and any vector (translation) u ∈−→
E , the result of translating a by u
is the point (not vector) a + u. Formally, we have the following deﬁnition.
Deﬁnition 5.7. An aﬃne space is either the degenerate space reduced to
the empty set, or a triple

E, −→
E , +

consisting of a nonempty set E (of
points), a vector space −→
E (of translations, or free vectors), and an action
+: E × −→
E →E, satisfying the following conditions.
(A1) a + 0 = a, for every a ∈E.
(A2) (a + u) + v = a + (u + v), for every a ∈E, and every u, v ∈−→
E .
(A3) For any two points a, b ∈E, there is a unique u ∈−→
E such that
a + u = b.
The unique vector u ∈−→
E such that a + u = b is denoted by −→
ab, or
sometimes by ab, or even by b −a. Thus, we also write
b = a + −→
ab
(or b = a + ab, or even b = a + (b −a)).
It is important to note that adding or rescaling points does not make
sense! However, using the fact that −→
E acts on E is a special way (this action
is transitive and faithful), it is possible to deﬁne rigorously the notion of
aﬃne combinations of points and to deﬁne aﬃne spaces, aﬃne maps, etc.
However, this would lead us to far aﬁeld, and for our purposes it is enough
to stick to vector spaces and we will not distinguish between vector addition
+ and translation of a point by a vector +. Still, one should be aware that
aﬃne combinations really apply to points, and that points are not vectors!
If E and F are ﬁnite dimensional vector spaces with dim(E) = n and
dim(F) = m, then it is useful to represent an aﬃne map with respect to
bases in E in F. However, the translation part c of the aﬃne map must be
somehow incorporated. There is a standard trick to do this which amounts
to viewing an aﬃne map as a linear map between spaces of dimension n+1
and m + 1. We also have the extra ﬂexibility of choosing origins a ∈E and
b ∈F.

148
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
Let (u1, . . . , un) be a basis of E, (v1, . . . , vm) be a basis of F, and let
a ∈E and b ∈F be any two ﬁxed vectors viewed as origins. Our aﬃne
map f has the property that if v = f(u), then
v −b = f(a + u −a) −b = f(a) −b + h(u −a),
where the last equality made use of the fact that h(x) = f(a + x) −f(a).
If we let y = v −b, x = u −a, and d = f(a) −b, then
y = h(x) + d,
x ∈E.
Over the basis U = (u1, . . . , un), we write
x = x1u1 + · · · + xnun,
and over the basis V = (v1, . . . , vm), we write
y = y1v1 + · · · + ymvm,
d = d1v1 + · · · + dmvm.
Then since
y = h(x) + d,
if we let A be the m × n matrix representing the linear map h, that is,
the jth column of A consists of the coordinates of h(uj) over the basis
(v1, . . . , vm), then we can write
yV = AxU + dV
where xU = (x1, . . . , xn)⊤, yV = (y1, . . . , ym)⊤, and dV = (d1, . . . , dm)⊤.
The above is the matrix representation of our aﬃne map f with respect to
(a, (u1, . . . , un)) and (b, (v1, . . . , vm)).
The reason for using the origins a and b is that it gives us more ﬂexibility.
In particular, we can choose b = f(a), and then f behaves like a linear map
with respect to the origins a and b = f(a).
When E = F, if there is some a ∈E such that f(a) = a (a is a ﬁxed
point of f), then we can pick b = a. Then because f(a) = a, we get
v = f(u) = f(a + u −a) = f(a) + h(u −a) = a + h(u −a),
that is
v −a = h(u −a).
With respect to the new origin a, if we deﬁne x and y by
x = u −a
y = v −a,

5.4. Aﬃne Maps
149
then we get
y = h(x).
Therefore, f really behaves like a linear map, but with respect to the new
origin a (not the standard origin 0). This is the case of a rotation around
an axis that does not pass through the origin.
Remark: A pair (a, (u1, . . . , un)) where (u1, . . . , un) is a basis of E and a
is an origin chosen in E is called an aﬃne frame.
We now describe the trick which allows us to incorporate the translation
part d into the matrix A.
We deﬁne the (m + 1) × (n + 1) matrix A′
obtained by ﬁrst adding d as the (n + 1)th column and then (0, . . . , 0
| {z }
n
, 1) as
the (m + 1)th row:
A′ =
A d
0n 1

.
It is clear that
y
1

=
A d
0n 1
 x
1

iﬀ
y = Ax + d.
This amounts to considering a point x ∈Rn as a point (x, 1) in the (aﬃne)
hyperplane Hn+1 in Rn+1 of equation xn+1 = 1. Then an aﬃne map is
the restriction to the hyperplane Hn+1 of the linear map bf from Rn+1
to Rm+1 corresponding to the matrix A′ which maps Hn+1 into Hm+1
( bf(Hn+1) ⊆Hm+1). Figure 5.4 illustrates this process for n = 2.
For example, the map
x1
x2

7→
1 1
1 3
 x1
x2

+
3
0

deﬁnes an aﬃne map f which is represented in R3 by


x1
x2
1

7→


1 1 3
1 3 0
0 0 1




x1
x2
1

.
It is easy to check that the point a = (6, −3) is ﬁxed by f, which means
that f(a) = a, so by translating the coordinate frame to the origin a, the
aﬃne map behaves like a linear map.
The idea of considering Rn as an hyperplane in Rn+1 can be used to
deﬁne projective maps.

150
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
x1
x2
x
(x1, x 2, 1)
H 3 : x3 = 1
x = ( x1, x 2)
3
Fig. 5.4
Viewing Rn as a hyperplane in Rn+1 (n = 2).
5.5
Summary
The main concepts and results of this chapter are listed below:
• Direct products, sums, direct sums.
• Projections.
• The fundamental equation
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)
(The rank-nullity theorem; Theorem 5.1).
• Grassmann's relation
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩V ).
• Characterizations of a bijective linear map f : E →F.
• Rank of a matrix.
• Aﬃne Maps.
5.6
Problems
Problem 5.1. Let V and W be two subspaces of a vector space E. Prove
that if V ∪W is a subspace of E, then either V ⊆W or W ⊆V .

5.6. Problems
151
Problem 5.2. Prove that for every vector space E, if f : E →E is an
idempotent linear map, i.e., f ◦f = f, then we have a direct sum
E = Ker f ⊕Im f,
so that f is the projection onto its image Im f.
Problem 5.3. Let U1, . . . , Up be any p ≥2 subspaces of some vector space
E and recall that the linear map
a: U1 × · · · × Up →E
is given by
a(u1, . . . , up) = u1 + · · · + up,
with ui ∈Ui for i = 1, . . . , p.
(1) If we let Zi ⊆U1 × · · · × Up be given by
Zi =




u1, . . . , ui−1, −
p
X
j=1,j̸=i
uj, ui+1, . . . , up


p
X
j=1,j̸=i
uj ∈Ui ∩

p
X
j=1,j̸=i
Uj


,
for i = 1, . . . , p, then prove that
Ker a = Z1 = · · · = Zp.
In general, for any given i, the condition Ui ∩
 Pp
j=1,j̸=i Uj

= (0) does
not necessarily imply that Zi = (0). Thus, let
Z =
(
u1, . . . , ui−1, ui, ui+1, . . . , up
 
ui = −
p
X
j=1,j̸=i
uj, ui ∈Ui ∩

p
X
j=1,j̸=i
Uj

, 1 ≤i ≤p


.
Since Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if
Ui ∩

p
X
j=1,j̸=i
Uj

= (0)
1 ≤i ≤p,
then Z = Ker a = (0).
(2) Prove that U1 + · · · + Up is a direct sum iﬀ
Ui ∩

p
X
j=1,j̸=i
Uj

= (0)
1 ≤i ≤p.

152
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
Problem 5.4. Assume that E is ﬁnite-dimensional, and let fi : E →E be
any p ≥2 linear maps such that
f1 + · · · + fp = idE.
Prove that the following properties are equivalent:
(1) f 2
i = fi, 1 ≤i ≤p.
(2) fj ◦fi = 0, for all i ̸= j, 1 ≤i, j ≤p.
Hint. Use Problem 5.2.
Let U1, . . . , Up be any p ≥2 subspaces of some vector space E. Prove
that U1 + · · · + Up is a direct sum iﬀ
Ui ∩
 i−1
X
j=1
Uj

= (0),
i = 2, . . . , p.
Problem 5.5. Given any vector space E, a linear map f : E →E is an
involution if f ◦f = id.
(1) Prove that an involution f is invertible. What is its inverse?
(2) Let E1 and E−1 be the subspaces of E deﬁned as follows:
E1 = {u ∈E | f(u) = u}
E−1 = {u ∈E | f(u) = −u}.
Prove that we have a direct sum
E = E1 ⊕E−1.
Hint. For every u ∈E, write
u = u + f(u)
2
+ u −f(u)
2
.
(3) If E is ﬁnite-dimensional and f is an involution, prove that there is
some basis of E with respect to which the matrix of f is of the form
Ik,n−k =
Ik
0
0 −In−k

,
where Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1).
Can you give a geometric interpretation of the action of f (especially when
k = n −1)?
Problem 5.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all
(j, k) such that j −k ≥0. An upper Hessenberg matrix is unreduced if
hi+1i ̸= 0 for i = 1, . . . , n −1.
Prove that if H is a singular unreduced upper Hessenberg matrix, then
dim(Ker (H)) = 1.

5.6. Problems
153
Problem 5.7. Let A be any n × k matrix.
(1) Prove that the k × k matrix A⊤A and the matrix A have the same
nullspace. Use this to prove that rank(A⊤A) = rank(A). Similarly, prove
that the n × n matrix AA⊤and the matrix A⊤have the same nullspace,
and conclude that rank(AA⊤) = rank(A⊤).
We will prove later that rank(A⊤) = rank(A).
(2) Let a1, . . . , ak be k linearly independent vectors in Rn (1 ≤k ≤n),
and let A be the n × k matrix whose ith column is ai. Prove that A⊤A has
rank k, and that it is invertible. Let P = A(A⊤A)−1A⊤(an n × n matrix).
Prove that
P 2 = P
P ⊤= P.
What is the matrix P when k = 1?
(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak,
or equivalently the set of all vectors in Rn of the form Ax, with x ∈Rk.
Prove that the nullspace U of P is the set of vectors u ∈Rn such that
A⊤u = 0. Can you give a geometric interpretation of U?
Conclude that P is a projection of Rn onto the subspace V spanned by
a1, . . . , ak, and that
Rn = U ⊕V.
Problem 5.8. A rotation Rθ in the plane R2 is given by the matrix
Rθ =
cos θ −sin θ
sin θ
cos θ

.
(1) Use Matlab to show the action of a rotation Rθ on a simple ﬁgure
such as a triangle or a rectangle, for various values of θ, including θ =
π/6, π/4, π/3, π/2.
(2) Prove that Rθ is invertible and that its inverse is R−θ.
(3) For any two rotations Rα and Rβ, prove that
Rβ ◦Rα = Rα ◦Rβ = Rα+β.
Use (2)-(3) to prove that the rotations in the plane form a commutative
group denoted SO(2).
Problem 5.9. Consider the aﬃne map Rθ,(a1,a2) in R2 given by
y1
y2

=
cos θ −sin θ
sin θ
cos θ
 x1
x2

+
a1
a2

.

154
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
(1) Prove that if θ ̸= k2π, with k ∈Z, then Rθ,(a1,a2) has a unique ﬁxed
point (c1, c2), that is, there is a unique point (c1, c2) such that
c1
c2

= Rθ,(a1,a2)
c1
c2

,
and this ﬁxed point is given by
c1
c2

=
1
2 sin(θ/2)
cos(π/2 −θ/2) −sin(π/2 −θ/2)
sin(π/2 −θ/2) cos(π/2 −θ/2)
 a1
a2

.
(2) In this question we still assume that θ ̸= k2π, with k ∈Z.
By
translating the coordinate system with origin (0, 0) to the new coordinate
system with origin (c1, c2), which means that if (x1, x2) are the coordinates
with respect to the standard origin (0, 0) and if (x′
1, x′
2) are the coordinates
with respect to the new origin (c1, c2), we have
x1 = x′
1 + c1
x2 = x′
2 + c2
and similarly for (y1, y2) and (y′
1, y′
2), then show that
y1
y2

= Rθ,(a1,a2)
x1
x2

becomes
y′
1
y′
2

= Rθ
x′
1
x′
2

.
Conclude that with respect to the new origin (c1, c2), the aﬃne map
Rθ,(a1,a2) becomes the rotation Rθ. We say that Rθ,(a1,a2) is a rotation of
center (c1, c2).
(3) Use Matlab to show the action of the aﬃne map Rθ,(a1,a2) on a
simple ﬁgure such as a triangle or a rectangle, for θ = π/3 and various
values of (a1, a2). Display the center (c1, c2) of the rotation.
What kind of transformations correspond to θ = k2π, with k ∈Z?
(4) Prove that the inverse of Rθ,(a1,a2) is of the form R−θ,(b1,b2), and
ﬁnd (b1, b2) in terms of θ and (a1, a2).
(5) Given two aﬃne maps Rα,(a1,a2) and Rβ,(b1,b2), prove that
Rβ,(b1,b2) ◦Rα,(a1,a2) = Rα+β,(t1,t2)
for some (t1, t2), and ﬁnd (t1, t2) in terms of β, (a1, a2) and (b1, b2).
Even in the case where (a1, a2) = (0, 0), prove that in general
Rβ,(b1,b2) ◦Rα ̸= Rα ◦Rβ,(b1,b2).

5.6. Problems
155
Use (4)-(5) to show that the aﬃne maps of the plane deﬁned in this
problem form a nonabelian group denoted SE(2).
Prove that Rβ,(b1,b2) ◦Rα,(a1,a2) is not a translation (possibly the iden-
tity) iﬀα + β ̸= k2π, for all k ∈Z.
Find its center of rotation when
(a1, a2) = (0, 0).
If α + β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2) is a pure translation. Find
the translation vector of Rβ,(b1,b2) ◦Rα,(a1,a2).
Problem 5.10. (Aﬃne subspaces) A subset A of Rn is called an aﬃne
subspace if either A = ∅, or there is some vector a ∈Rn and some subspace
U of Rn such that
A = a + U = {a + u | u ∈U}.
We deﬁne the dimension dim(A) of A as the dimension dim(U) of U.
(1) If A = a + U, why is a ∈A?
What are aﬃne subspaces of dimension 0? What are aﬃne subspaces
of dimension 1 (begin with R2)? What are aﬃne subspaces of dimension 2
(begin with R3)?
Prove that any nonempty aﬃne subspace is closed under aﬃne combi-
nations.
(2) Prove that if A = a + U is any nonempty aﬃne subspace, then
A = b + U for any b ∈A.
(3) Let A be any nonempty subset of Rn closed under aﬃne combina-
tions. For any a ∈A, prove that
Ua = {x −a ∈Rn | x ∈A}
is a (linear) subspace of Rn such that
A = a + Ua.
Prove that Ua does not depend on the choice of a ∈A; that is, Ua = Ub for
all a, b ∈A. In fact, prove that
Ua = U = {y −x ∈Rn | x, y ∈A},
for all a ∈A,
and so
A = a + U,
for any a ∈A.
Remark: The subspace U is called the direction of A.
(4) Two nonempty aﬃne subspaces A and B are said to be parallel iﬀ
they have the same direction. Prove that that if A ̸= B and A and B are
parallel, then A ∩B = ∅.
Remark: The above shows that aﬃne subspaces behave quite diﬀerently
from linear subspaces.

156
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
Problem 5.11. (Aﬃne frames and aﬃne maps) For any vector v =
(v1, . . . , vn) ∈Rn, let bv ∈Rn+1 be the vector bv = (v1, . . . , vn, 1). Equiva-
lently, bv = (bv1, . . . , bvn+1) ∈Rn+1 is the vector deﬁned by
bvi =
(
vi
if 1 ≤i ≤n,
1
if i = n + 1.
(1) For any m + 1 vectors (u0, u1, . . . , um) with ui ∈Rn and m ≤n,
prove that if the m vectors (u1 −u0, . . . , um −u0) are linearly independent,
then the m + 1 vectors (bu0, . . . , bum) are linearly independent.
(2) Prove that if the m+1 vectors (bu0, . . . , bum) are linearly independent,
then for any choice of i, with 0 ≤i ≤m, the m vectors uj −ui for j ∈
{0, . . . , m} with j −i ̸= 0 are linearly independent.
Any m + 1 vectors (u0, u1, . . . , um) such that the m + 1 vectors
(bu0, . . . , bum) are linearly independent are said to be aﬃnely independent.
From (1) and (2), the vector (u0, u1, . . . , um) are aﬃnely independent iﬀ
for any choice of i, with 0 ≤i ≤m, the m vectors uj −ui for j ∈{0, . . . , m}
with j −i ̸= 0 are linearly independent. If m = n, we say that n+1 aﬃnely
independent vectors (u0, u1, . . . , un) form an aﬃne frame of Rn.
(3) If (u0, u1, . . . , un) is an aﬃne frame of Rn, then prove that for every
vector v ∈Rn, there is a unique (n+1)-tuple (λ0, λ1, . . . , λn) ∈Rn+1, with
λ0 + λ1 + · · · + λn = 1, such that
v = λ0u0 + λ1u1 + · · · + λnun.
The scalars (λ0, λ1, . . . , λn) are called the barycentric (or aﬃne) coordinates
of v w.r.t. the aﬃne frame (u0, u1, . . . , un).
If we write ei = ui −u0, for i = 1, . . . , n, then prove that we have
v = u0 + λ1e1 + · · · + λnen,
and since (e1, . . . , en) is a basis of Rn (by (1) & (2)), the n-tuple (λ1, . . . , λn)
consists of the standard coordinates of v −u0 over the basis (e1, . . . , en).
Conversely, for any vector u0 ∈Rn and for any basis (e1, . . . , en) of Rn,
let ui = u0 + ei for i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an aﬃne
frame of Rn, and for any v ∈Rn, if
v = u0 + x1e1 + · · · + xnen,
with (x1, . . . , xn) ∈Rn (unique), then
v = (1 −(x1 + · · · + xx))u0 + x1u1 + · · · + xnun,

5.6. Problems
157
so that (1 −(x1 + · · · + xx)), x1, · · · , xn), are the barycentric coordinates of
v w.r.t. the aﬃne frame (u0, u1, . . . , un).
The above shows that there is a one-to-one correspondence between
aﬃne frames (u0, . . ., un) and pairs (u0, (e1, . . . , en)), with (e1, . . . , en) a
basis. Given an aﬃne frame (u0, . . . , un), we obtain the basis (e1, . . . , en)
with ei = ui −u0, for i = 1, . . . , n; given the pair (u0, (e1, . . ., en))
where (e1, . . . , en) is a basis, we obtain the aﬃne frame (u0, . . . , un), with
ui = u0 + ei, for i = 1, . . . , n. There is also a one-to-one correspondence
between barycentric coordinates w.r.t. the aﬃne frame (u0, . . . , un) and
standard coordinates w.r.t. the basis (e1, . . . , en). The barycentric cordi-
nates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the standard
coordinates (λ1, . . . , λn) of v −u0; the standard coordinates (x1, . . . , xn) of
v−u0 yield the barycentric coordinates (1−(x1 +· · ·+xn), x1, . . . , xn) of v.
(4) Let (u0, . . . , un) be any aﬃne frame in Rn and let (v0, . . . , vn) be
any vectors in Rm. Prove that there is a unique aﬃne map f : Rn →Rm
such that
f(ui) = vi,
i = 0, . . . , n.
(5) Let (a0, . . . , an) be any aﬃne frame in Rn and let (b0, . . . , bn) be any
n + 1 points in Rn. Prove that there is a unique (n + 1) × (n + 1) matrix
A =
B w
0 1

corresponding to the unique aﬃne map f such that
f(ai) = bi,
i = 0, . . . , n,
in the sense that
Abai = bbi,
i = 0, . . . , n,
and that A is given by
A =

bb0 bb1 · · · bbn
  ba0 ba1 · · · ban
−1 .
Make sure to prove that the bottom row of A is (0, . . . , 0, 1).
In the special case where (a0, . . . , an) is the canonical aﬃne frame with
ai = ei+1 for i = 0, . . . , n −1 and an = (0, . . . , 0) (where ei is the ith
canonical basis vector), show that
 ba0 ba1 · · · ban

=







1 0 · · · 0 0
0 1 · · · 0 0
... ... ... 0 0
0 0 · · · 1 0
1 1 · · · 1 1








158
Direct Sums, Rank-Nullity Theorem, Aﬃne Maps
and
 ba0 ba1 · · · ban
−1 =







1
0 · · ·
0 0
0
1 · · ·
0 0
...
...
...
0 0
0
0 · · ·
1 0
−1 −1 · · · −1 1







.
For example, when n = 2, if we write bi = (xi, yi), then we have
A =


x1 x2 x3
y1 y2 y3
1
1
1




1
0 0
0
1 0
−1 −1 1

=


x1 −x3 x2 −x3 x3
y1 −y3 y2 −y3 y3
0
0
1

.
(6) Recall that a nonempty aﬃne subspace A of Rn is any nonempty
subset of Rn closed under aﬃne combinations. For any aﬃne map f : Rn →
Rm, for any aﬃne subspace A of Rn, and any aﬃne subspace B of Rm, prove
that f(A) is an aﬃne subspace of Rm, and that f −1(B) is an aﬃne subspace
of Rn.

Chapter 6
Determinants
In this chapter all vector spaces are deﬁned over an arbitrary ﬁeld K. For
the sake of concreteness, the reader may safely assume that K = R.
6.1
Permutations, Signature of a Permutation
This chapter contains a review of determinants and their use in linear alge-
bra. We begin with permutations and the signature of a permutation. Next
we deﬁne multilinear maps and alternating multilinear maps. Determinants
are introduced as alternating multilinear maps taking the value 1 on the
unit matrix (following Emil Artin). It is then shown how to compute a
determinant using the Laplace expansion formula, and the connection with
the usual deﬁnition is made. It is shown how determinants can be used to
invert matrices and to solve (at least in theory!) systems of linear equa-
tions (the Cramer formulae). The determinant of a linear map is deﬁned.
We conclude by deﬁning the characteristic polynomial of a matrix (and of a
linear map) and by proving the celebrated Cayley-Hamilton theorem which
states that every matrix is a "zero" of its characteristic polynomial (we give
two proofs; one computational, the other one more conceptual).
Determinants can be deﬁned in several ways. For example, determinants
can be deﬁned in a fancy way in terms of the exterior algebra (or alternating
algebra) of a vector space. We will follow a more algorithmic approach
due to Emil Artin. No matter which approach is followed, we need a few
preliminaries about permutations on a ﬁnite set. We need to show that
every permutation on n elements is a product of transpositions and that
the parity of the number of transpositions involved is an invariant of the
permutation. Let [n] = {1, 2 . . . , n}, where n ∈N, and n > 0.
Deﬁnition 6.1. A permutation on n elements is a bijection π: [n] →[n].
159

160
Determinants
When n = 1, the only function from [1] to [1] is the constant map: 1 7→1.
Thus, we will assume that n ≥2. A transposition is a permutation τ : [n] →
[n] such that, for some i < j (with 1 ≤i < j ≤n), τ(i) = j, τ(j) = i, and
τ(k) = k, for all k ∈[n] −{i, j}. In other words, a transposition exchanges
two distinct elements i, j ∈[n].
If τ is a transposition, clearly, τ◦τ = id. We will also use the terminology
product of permutations (or transpositions) as a synonym for composition
of permutations.
A permutation σ on n elements, say σ(i) = ki for i = 1, . . . , n, can be
represented in functional notation by the 2 × n array
 1 · · · i · · · n
k1 · · · ki · · · kn

known as Cauchy two-line notation. For example, we have the permutation
σ denoted by
1 2 3 4 5 6
2 4 3 6 5 1

.
A more concise notation often used in computer science and in combi-
natorics is to represent a permutation by its image, namely by the sequence
σ(1) σ(2) · · · σ(n)
written as a row vector without commas separating the entries. The above
is known as the one-line notation. For example, in the one-line notation,
our previous permutation σ is represented by
2 4 3 6 5 1.
The reason for not enclosing the above sequence within parentheses is avoid
confusion with the notation for cycles, for which is it customary to include
parentheses.
Clearly, the composition of two permutations is a permutation and every
permutation has an inverse which is also a permutation. Therefore, the set
of permutations on [n] is a group often denoted Sn and called the symmetric
group on n elements.
It is easy to show by induction that the group Sn has n! elements. The
following proposition shows the importance of transpositions.
Proposition 6.1. For every n ≥2, every permutation π: [n] →[n] can be
written as a nonempty composition of transpositions.

6.1. Permutations, Signature of a Permutation
161
Proof. We proceed by induction on n. If n = 2, there are exactly two per-
mutations on [2], the transposition τ exchanging 1 and 2, and the identity.
However, id2 = τ 2. Now let n ≥3. If π(n) = n, since by the induction
hypothesis, the restriction of π to [n −1] can be written as a product of
transpositions, π itself can be written as a product of transpositions. If
π(n) = k ̸= n, letting τ be the transposition such that τ(n) = k and
τ(k) = n, it is clear that τ ◦π leaves n invariant, and by the induction
hypothesis, we have τ ◦π = τm ◦. . . ◦τ1 for some transpositions, and thus
π = τ ◦τm ◦. . . ◦τ1,
a product of transpositions (since τ ◦τ = idn).
Remark: When π = idn is the identity permutation, we can agree that
the composition of 0 transpositions is the identity. Proposition 6.1 shows
that the transpositions generate the group of permutations Sn.
A transposition τ that exchanges two consecutive elements k and k + 1
of [n] (1 ≤k ≤n −1) may be called a basic transposition. We leave it
as a simple exercise to prove that every transposition can be written as a
product of basic transpositions. In fact, the transposition that exchanges k
and k + p (1 ≤p ≤n −k) can be realized using 2p −1 basic transpositions.
Therefore, the group of permutations Sn is also generated by the basic
transpositions.
Given a permutation written as a product of transpositions, we now
show that the parity of the number of transpositions is an invariant. For
this, we introduce the following function.
Deﬁnition 6.2. For every n ≥2, let ∆: Zn →Z be the function given by
∆(x1, . . . , xn) =
Y
1≤i<j≤n
(xi −xj).
More generally, for any permutation σ ∈Sn, deﬁne ∆(xσ(1), . . . , xσ(n)) by
∆(xσ(1), . . . , xσ(n)) =
Y
1≤i<j≤n
(xσ(i) −xσ(j)).
The
expression
∆(x1, . . . , xn)
is
often
called
the
discriminant
of
(x1, . . . , xn).
∆(x1, . . . , xn) ̸= 0.
The discriminant consists of
 n
2

factors.
When
n = 3,
∆(x1, x2, x3) = (x1 −x2)(x1 −x3)(x2 −x3).

162
Determinants
If σ is the permutation
1 2 3
2 3 2

,
then
∆(xσ(1), xσ(2), xσ(3)) = (xσ(1) −xσ(2))(xσ(1) −xσ(3))(xσ(2) −xσ(3))
= (x2 −x3)(x2 −x1)(x3 −x1).
Observe that
∆(xσ(1), xσ(2), xσ(3)) = (−1)2∆(x1, x2, x3),
since two transpositions applied to the identity permutation 1 2 3 (written
in one-line notation) give rise to 2 3 1. This result regarding the parity of
∆(xσ(1), . . . , xσ(n)) is generalized by the following proposition.
Proposition 6.2. For every basic transposition τ of [n] (n ≥2), we have
∆(xτ(1), . . . , xτ(n)) = −∆(x1, . . . , xn).
The above also holds for every transposition, and more generally, for every
composition of transpositions σ = τp ◦· · · ◦τ1, we have
∆(xσ(1), . . . , xσ(n)) = (−1)p∆(x1, . . . , xn).
Consequently, for every permutation σ of [n], the parity of the number p of
transpositions involved in any decomposition of σ as σ = τp ◦· · · ◦τ1 is an
invariant (only depends on σ).
Proof. Suppose τ exchanges xk and xk+1. The terms xi −xj that are
aﬀected correspond to i = k, or i = k + 1, or j = k, or j = k + 1. The
contribution of these terms in ∆(x1, . . . , xn) is
(xk −xk+1)[(xk −xk+2) · · · (xk −xn)][(xk+1 −xk+2) · · · (xk+1 −xn)]
[(x1 −xk) · · · (xk−1 −xk)][(x1 −xk+1) · · · (xk−1 −xk+1)].
When we exchange xk and xk+1, the ﬁrst factor is multiplied by −1, the sec-
ond and the third factor are exchanged, and the fourth and the ﬁfth factor
are exchanged, so the whole product ∆(x1, . . . , xn) is is indeed multiplied
by −1, that is,
∆(xτ(1), . . . , xτ(n)) = −∆(x1, . . . , xn).
For the second statement, ﬁrst we observe that since every transposition τ
can be written as the composition of an odd number of basic transpositions
(see the the remark following Proposition 6.1), we also have
∆(xτ(1), . . . , xτ(n)) = −∆(x1, . . . , xn).

6.1. Permutations, Signature of a Permutation
163
Next we proceed by induction on the number p of transpositions involved
in the decomposition of a permutation σ.
The base case p = 1 has just been proven. If p ≥2, if we write ω =
τp−1 ◦· · · ◦τ1, then σ = τp ◦ω and
∆(xσ(1), . . . , xσ(n)) = ∆(xτp(ω(1)), . . . , xτp(ω(n)))
= −∆(xω(1), . . . , xω(n))
= −(−1)p−1∆(x1, . . . , xn)
= (−1)p∆(x1, . . . , xn),
where we used the induction hypothesis from the second to the third line,
establishing the induction hypothesis. Since ∆(xσ(1), . . . , xσ(n)) only de-
pends on σ, the equation
∆(xσ(1), . . . , xσ(n)) = (−1)p∆(x1, . . . , xn).
shows that the parity (−1)p of the number of transpositions in any decom-
position of σ is an invariant.
In view of Proposition 6.2, the following deﬁnition makes sense:
Deﬁnition 6.3. For every permutation σ of [n], the parity ϵ(σ) (or sgn(σ))
of the number of transpositions involved in any decomposition of σ is called
the signature (or sign) of σ.
Obviously ϵ(τ) = −1 for every transposition τ (since (−1)1 = −1).
A simple way to compute the signature of a permutation is to count its
number of inversions.
Deﬁnition 6.4. Given any permutation σ on n elements, we say that a
pair (i, j) of indices i, j ∈{1, . . . , n} such that i < j and σ(i) > σ(j) is an
inversion of the permutation σ.
For example, the permutation σ given by
1 2 3 4 5 6
2 4 3 6 5 1

has seven inversions
(1, 6), (2, 3), (2, 6), (3, 6), (4, 5), (4, 6), (5, 6).
Proposition 6.3. The signature ϵ(σ) of any permutation σ is equal to the
parity (−1)I(σ) of the number I(σ) of inversions in σ.

164
Determinants
Proof. In the product
∆(xσ(1), . . . , xσ(n)) =
Y
1≤i<j≤n
(xσ(i) −xσ(j)),
the terms xσ(i)−xσ(j) for which σ(i) < σ(j) occur in ∆(x1, . . . , xn), whereas
the terms xσ(i) −xσ(j) for which σ(i) > σ(j) occur in ∆(x1, . . . , xn) with a
minus sign. Therefore, the number ν of terms in ∆(xσ(1), . . . , xσ(n)) whose
sign is the opposite of a term in ∆(x1, . . . , xn), is equal to the number I(σ)
of inversions in σ, which implies that
∆(xσ(1), . . . , xσ(n)) = (−1)I(σ)∆(x1, . . . , xn).
By Proposition 6.2, the sign of (−1)I(σ) is equal to the signature of σ.
For example, the permutation
1 2 3 4 5 6
2 4 3 6 5 1

has odd signature since it has seven inversions and (−1)7 = −1.
Remark: When π = idn is the identity permutation, since we agreed that
the composition of 0 transpositions is the identity, it it still correct that
(−1)0 = ϵ(id) = +1. From Proposition 6.2, it is immediate that ϵ(π′ ◦π) =
ϵ(π′)ϵ(π). In particular, since π−1 ◦π = idn, we get ϵ(π−1) = ϵ(π).
We can now proceed with the deﬁnition of determinants.
6.2
Alternating Multilinear Maps
First we deﬁne multilinear maps, symmetric multilinear maps, and alter-
nating multilinear maps.
Remark: Most of the deﬁnitions and results presented in this section also
hold when K is a commutative ring and when we consider modules over K
(free modules, when bases are needed).
Let E1, . . . , En, and F, be vector spaces over a ﬁeld K, where n ≥1.
Deﬁnition 6.5. A function f : E1 × . . . × En →F is a multilinear map (or
an n-linear map) if it is linear in each argument, holding the others ﬁxed.
More explicitly, for every i, 1 ≤i ≤n, for all x1 ∈E1, . . ., xi−1 ∈Ei−1,
xi+1 ∈Ei+1, . . ., xn ∈En, for all x, y ∈Ei, for all λ ∈K,
f(x1, . . . , xi−1, x + y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)
+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),
f(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).

6.2. Alternating Multilinear Maps
165
When F = K, we call f an n-linear form (or multilinear form). If n ≥2
and E1 = E2 = . . . = En, an n-linear map f : E × . . . × E →F is called
symmetric, if f(x1, . . . , xn) = f(xπ(1), . . . , xπ(n)) for every permutation π
on {1, . . . , n}. An n-linear map f : E × . . . × E →F is called alternating,
if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤i ≤n −1 (in other
words, when two adjacent arguments are equal). It does no harm to agree
that when n = 1, a linear map is considered to be both symmetric and
alternating, and we will do so.
When n = 2, a 2-linear map f : E1 × E2 →F is called a bilinear map.
We have already seen several examples of bilinear maps. Multiplication
·: K × K →K is a bilinear map, treating K as a vector space over itself.
The operation ⟨−, −⟩: E∗× E →K applying a linear form to a vector
is a bilinear map.
Symmetric bilinear maps (and multilinear maps) play an important role
in geometry (inner products, quadratic forms) and in diﬀerential calculus
(partial derivatives).
A bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈E.
Alternating multilinear maps satisfy the following simple but crucial
properties.
Proposition 6.4. Let f : E × . . . × E →F be an n-linear alternating map,
with n ≥2. The following properties hold:
(1)
f(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .)
(2)
f(. . . , xi, . . . , xj, . . .) = 0,
where xi = xj, and 1 ≤i < j ≤n.
(3)
f(. . . , xi, . . . , xj, . . .) = −f(. . . , xj, . . . , xi, . . .),
where 1 ≤i < j ≤n.
(4)
f(. . . , xi, . . .) = f(. . . , xi + λxj, . . .),
for any λ ∈K, and where i ̸= j.

166
Determinants
Proof. (1) By multilinearity applied twice, we have
f(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi, xi, . . .) + f(. . . , xi, xi+1, . . .)
+ f(. . . , xi+1, xi, . . .) + f(. . . , xi+1, xi+1, . . .),
and since f is alternating, this yields
0 = f(. . . , xi, xi+1, . . .) + f(. . . , xi+1, xi, . . .),
that is, f(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .).
(2) If xi = xj and i and j are not adjacent, we can interchange xi and
xi+1, and then xi and xi+2, etc., until xi and xj become adjacent. By (1),
f(. . . , xi, . . . , xj, . . .) = ϵf(. . . , xi, xj, . . .),
where ϵ = +1 or −1, but f(. . . , xi, xj, . . .) = 0, since xi = xj, and (2) holds.
(3) follows from (2) as in (1). (4) is an immediate consequence of (2).
Proposition 6.4 will now be used to show a fundamental property of
alternating multilinear maps. First we need to extend the matrix notation
a little bit.
Let E be a vector space over K.
Given an n × n matrix
A = (ai j) over K, we can deﬁne a map L(A): En →En as follows:
L(A)1(u) = a1 1u1 + · · · + a1 nun,
. . .
L(A)n(u) = an 1u1 + · · · + an nun,
for all u1, . . . , un ∈E and with u = (u1, . . . , un). It is immediately veriﬁed
that L(A) is linear. Then given two n×n matrices A = (ai j) and B = (bi j),
by repeating the calculations establishing the product of matrices (just
before Deﬁnition 2.14), we can show that
L(AB) = L(A) ◦L(B).
It is then convenient to use the matrix notation to describe the eﬀect of the
linear map L(A), as





L(A)1(u)
L(A)2(u)
...
L(A)n(u)




=





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n










u1
u2
...
un




.
Lemma 6.1. Let f : E × . . . × E →F be an n-linear alternating map. Let
(u1, . . . , un) and (v1, . . . , vn) be two families of n vectors, such that,
v1 = a1 1u1 + · · · + an 1un,
. . .
vn = a1 nu1 + · · · + an nun.

6.2. Alternating Multilinear Maps
167
Equivalently, letting
A =





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n




,
assume that we have





v1
v2
...
vn




= A⊤





u1
u2
...
un




.
Then,
f(v1, . . . , vn) =
 X
π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n

f(u1, . . . , un),
where the sum ranges over all permutations π on {1, . . . , n}.
Proof. Expanding f(v1, . . . , vn) by multilinearity, we get a sum of terms
of the form
aπ(1) 1 · · · aπ(n) nf(uπ(1), . . . , uπ(n)),
for all possible functions π: {1, . . . , n} →{1, . . . , n}. However, because f
is alternating, only the terms for which π is a permutation are nonzero. By
Proposition 6.1, every permutation π is a product of transpositions, and
by Proposition 6.2, the parity ϵ(π) of the number of transpositions only
depends on π. Then applying Proposition 6.4 (3) to each transposition in
π, we get
aπ(1) 1 · · · aπ(n) nf(uπ(1), . . . , uπ(n)) = ϵ(π)aπ(1) 1 · · · aπ(n) nf(u1, . . . , un).
Thus, we get the expression of the lemma.
For the case of n = 2, the proof details of Lemma 6.1 become
f(v1, v2) = f(a11u1 + a21u2, a12u1 + a22u2)
= f(a11u1 + a21u2, a12u1) + f(a11u1 + a21u2, a22u2)
= f(a11u1, a12u1) + f(a21u2, a12u1)
+ f(a11ua, a22u2) + f(a21u2, a22u2)
= a11a12f(u1, u1) + a21a12f(u2, u1) + a11a22f(u1, u2)
+ a21a22f(u2, u2)
= a21a12f(u2, u1)a11a22f(u1, u2)
= (a11a22 −a12a22) f(u1, u2).

168
Determinants
Hopefully the reader will recognize the quantity a11a22 −a12a22. It is the
determinant of the 2 × 2 matrix
A =
a11 a12
a21 a22

.
This is no accident. The quantity
det(A) =
X
π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n
is in fact the value of the determinant of A (which, as we shall see shortly,
is also equal to the determinant of A⊤). However, working directly with
the above deﬁnition is quite awkward, and we will proceed via a slightly
indirect route
Remark: The reader might have been puzzled by the fact that it is the
transpose matrix A⊤rather than A itself that appears in Lemma 6.1. The
reason is that if we want the generic term in the determinant to be
ϵ(π)aπ(1) 1 · · · aπ(n) n,
where the permutation applies to the ﬁrst index, then we have to express
the vjs in terms of the uis in terms of A⊤as we did. Furthermore, since
vj = a1 ju1 + · · · + ai jui + · · · + an jun,
we see that vj corresponds to the jth column of the matrix A, and so the
determinant is viewed as a function of the columns of A.
The literature is split on this point. Some authors prefer to deﬁne a
determinant as we did. Others use A itself, which amounts to viewing det
as a function of the rows, in which case we get the expression
X
σ∈Sn
ϵ(σ)a1 σ(1) · · · an σ(n).
Corollary 6.1 show that these two expressions are equal, so it doesn't matter
which is chosen. This is a matter of taste.
6.3
Deﬁnition of a Determinant
Recall that the set of all square n × n-matrices with coeﬃcients in a ﬁeld
K is denoted by Mn(K).
Deﬁnition 6.6. A determinant is deﬁned as any map
D: Mn(K) →K,

6.3. Deﬁnition of a Determinant
169
which, when viewed as a map on (Kn)n, i.e., a map of the n columns of
a matrix, is n-linear alternating and such that D(In) = 1 for the identity
matrix In. Equivalently, we can consider a vector space E of dimension n,
some ﬁxed basis (e1, . . . , en), and deﬁne
D: En →K
as an n-linear alternating map such that D(e1, . . . , en) = 1.
First we will show that such maps D exist, using an inductive deﬁnition
that also gives a recursive method for computing determinants. Actually,
we will deﬁne a family (Dn)n≥1 of (ﬁnite) sets of maps D: Mn(K) →K.
Second we will show that determinants are in fact uniquely deﬁned, that
is, we will show that each Dn consists of a single map.
This will show
the equivalence of the direct deﬁnition det(A) of Lemma 6.1 with the in-
ductive deﬁnition D(A). Finally, we will prove some basic properties of
determinants, using the uniqueness theorem.
Given a matrix A ∈Mn(K), we denote its n columns by A1, . . . , An. In
order to describe the recursive process to deﬁne a determinant we need the
notion of a minor.
Deﬁnition 6.7. Given any n × n matrix with n ≥2, for any two indices
i, j with 1 ≤i, j ≤n, let Aij be the (n −1) × (n −1) matrix obtained by
deleting Row i and Column j from A and called a minor:
Aij =











×
×
× × × × × × ×
×
×
×
×











.
For example, if
A =






2 −1 0
0
0
−1 2 −1 0
0
0 −1 2 −1 0
0
0 −1 2 −1
0
0
0 −1 2






then
A2 3 =




2 −1 0
0
0 −1 −1 0
0 0
2 −1
0 0 −1 2



.

170
Determinants
Deﬁnition 6.8. For every n ≥1, we deﬁne a ﬁnite set Dn of maps
D: Mn(K) →K inductively as follows:
When n = 1, D1 consists of the single map D such that, D(A) = a,
where A = (a), with a ∈K.
Assume that Dn−1 has been deﬁned, where n ≥2. Then Dn consists of
all the maps D such that, for some i, 1 ≤i ≤n,
D(A) = (−1)i+1ai 1D(Ai 1) + · · · + (−1)i+nai nD(Ai n),
where for every j, 1 ≤j ≤n, D(Ai j) is the result of applying any D in
Dn−1 to the minor Ai j.

We confess that the use of the same letter D for the member
of Dn being deﬁned, and for members of Dn−1, may be slightly
confusing. We considered using subscripts to distinguish, but this seems to
complicate things unnecessarily. One should not worry too much anyway,
since it will turn out that each Dn contains just one map.
Each (−1)i+jD(Ai j) is called the cofactor of ai j, and the inductive
expression for D(A) is called a Laplace expansion of D according to the i-th
Row. Given a matrix A ∈Mn(K), each D(A) is called a determinant of A.
We can think of each member of Dn as an algorithm to evaluate "the"
determinant of A. The main point is that these algorithms, which recur-
sively evaluate a determinant using all possible Laplace row expansions, all
yield the same result, det(A).
We will prove shortly that D(A) is uniquely deﬁned (at the moment, it
is not clear that Dn consists of a single map). Assuming this fact, given a
n × n-matrix A = (ai j),
A =





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n




,
its determinant is denoted by D(A) or det(A), or more explicitly by
det(A) =

a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n

.
Let us ﬁrst consider some examples.

6.3. Deﬁnition of a Determinant
171
Example 6.1.
(1) When n = 2, if
A =
a b
c d

,
then by expanding according to any row, we have
D(A) = ad −bc.
(2) When n = 3, if
A =


a1 1 a1 2 a1 3
a2 1 a2 2 a2 3
a3 1 a3 2 a3 3

,
then by expanding according to the ﬁrst row, we have
D(A) = a1 1

a2 2 a2 3
a3 2 a3 3
 −a1 2

a2 1 a2 3
a3 1 a3 3
 + a1 3

a2 1 a2 2
a3 1 a3 2
 ,
that is,
D(A) = a1 1(a2 2a3 3 −a3 2a2 3) −a1 2(a2 1a3 3 −a3 1a2 3)
+ a1 3(a2 1a3 2 −a3 1a2 2),
which gives the explicit formula
D(A) = a1 1a2 2a3 3 + a2 1a3 2a1 3 + a3 1a1 2a2 3
−a1 1a3 2a2 3 −a2 1a1 2a3 3 −a3 1a2 2a1 3.
We now show that each D ∈Dn is a determinant (map).
Lemma 6.2. For every n ≥1, for every D ∈Dn as deﬁned in Deﬁni-
tion 6.8, D is an alternating multilinear map such that D(In) = 1.
Proof. By induction on n, it is obvious that D(In) = 1. Let us now prove
that D is multilinear. Let us show that D is linear in each column. Consider
any Column k. Since
D(A) = (−1)i+1ai 1D(Ai 1) + · · · + (−1)i+jai jD(Ai j) + · · ·
+ (−1)i+nai nD(Ai n),
if j ̸= k, then by induction, D(Ai j) is linear in Column k, and ai j does not
belong to Column k, so (−1)i+jai jD(Ai j) is linear in Column k. If j = k,
then D(Ai j) does not depend on Column k = j, since Ai j is obtained
from A by deleting Row i and Column j = k, and ai j belongs to Column
j = k. Thus, (−1)i+jai jD(Ai j) is linear in Column k. Consequently, in all

172
Determinants
cases, (−1)i+jai jD(Ai j) is linear in Column k, and thus, D(A) is linear in
Column k.
Let us now prove that D is alternating.
Assume that two adjacent
columns of A are equal, say Ak = Ak+1. Assume that j ̸= k and j ̸=
k +1. Then the matrix Ai j has two identical adjacent columns, and by the
induction hypothesis, D(Ai j) = 0. The remaining terms of D(A) are
(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1).
However, the two matrices Ai k and Ai k+1 are equal, since we are assuming
that Columns k and k +1 of A are identical and Ai k is obtained from A by
deleting Row i and Column k while Ai k+1 is obtained from A by deleting
Row i and Column k + 1. Similarly, ai k = ai k+1, since Columns k and
k + 1 of A are equal. But then,
(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1)
= (−1)i+kai kD(Ai k) −(−1)i+kai kD(Ai k) = 0.
This shows that D is alternating and completes the proof.
Lemma 6.2 shows the existence of determinants. We now prove their
uniqueness.
Theorem 6.1. For every n ≥1, for every D ∈Dn, for every matrix
A ∈Mn(K), we have
D(A) =
X
π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}. As a conse-
quence, Dn consists of a single map for every n ≥1, and this map is given
by the above explicit formula.
Proof. Consider the standard basis (e1, . . . , en) of Kn, where (ei)i = 1
and (ei)j = 0, for j ̸= i. Then each column Aj of A corresponds to a vector
vj whose coordinates over the basis (e1, . . . , en) are the components of Aj,
that is, we can write
v1 = a1 1e1 + · · · + an 1en,
. . .
vn = a1 ne1 + · · · + an nen.
Since by Lemma 6.2, each D is a multilinear alternating map, by applying
Lemma 6.1, we get
D(A) = D(v1, . . . , vn) =
 X
π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n

D(e1, . . . , en),

6.3. Deﬁnition of a Determinant
173
where the sum ranges over all permutations π on {1, . . . , n}.
But
D(e1, . . . , en) = D(In), and by Lemma 6.2, we have D(In) = 1. Thus,
D(A) =

π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}.
From now on we will favor the notation det(A) over D(A) for the de-
terminant of a square matrix.
Remark: There is a geometric interpretation of determinants which we
ﬁnd quite illuminating. Given n linearly independent vectors (u1, . . . , un)
in Rn, the set
Pn = {λ1u1 + · · · + λnun | 0 ≤λi ≤1, 1 ≤i ≤n}
is called a parallelotope. If n = 2, then P2 is a parallelogram and if n = 3,
then P3 is a parallelepiped, a skew box having u1, u2, u3 as three of its corner
sides. See Figures 6.1 and 6.2.
u = (1,0)
1
u = (1,1)
2
Fig. 6.1
The parallelogram in Rw spanned by the vectors u1 = (1, 0) and u2 = (1, 1).
Then it turns out that det(u1, . . . , un) is the signed volume of the paral-
lelotope Pn (where volume means n-dimensional volume). The sign of this
volume accounts for the orientation of Pn in Rn.
We can now prove some properties of determinants.
Corollary 6.1. For every matrix A ∈Mn(K), we have det(A) = det(A⊤).

174
Determinants
u = (1,1,0)
1
u = (0,1,0)
2
u = (1,1,1)
3
Fig. 6.2
The parallelepiped in R3 spanned by the vectors u1 = (1, 1, 0), u2 = (0, 1, 0),
and u3 = (0, 0, 1).
Proof. By Theorem 6.1, we have
det(A) =

π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}. Since a per-
mutation is invertible, every product
aπ(1) 1 · · · aπ(n) n
can be rewritten as
a1 π−1(1) · · · an π−1(n),
and since ϵ(π−1) = ϵ(π) and the sum is taken over all permutations on
{1, . . . , n}, we have

π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n =

σ∈Sn
ϵ(σ)a1 σ(1) · · · an σ(n),

6.3. Deﬁnition of a Determinant
175
where π and σ range over all permutations. But it is immediately veriﬁed
that
det(A⊤) =
X
σ∈Sn
ϵ(σ)a1 σ(1) · · · an σ(n).
A useful consequence of Corollary 6.1 is that the determinant of a ma-
trix is also a multilinear alternating map of its rows. This fact, combined
with the fact that the determinant of a matrix is a multilinear alternating
map of its columns, is often useful for ﬁnding short-cuts in computing de-
terminants. We illustrate this point on the following example which shows
up in polynomial interpolation.
Example 6.2. Consider the so-called Vandermonde determinant
V (x1, . . . , xn) =

1
1
. . .
1
x1
x2
. . .
xn
x2
1
x2
2
. . .
x2
n
...
...
...
...
xn−1
1
xn−1
2
. . . xn−1
n

.
We claim that
V (x1, . . . , xn) =
Y
1≤i<j≤n
(xj −xi),
with V (x1, . . . , xn) = 1, when n = 1. We prove it by induction on n ≥1.
The case n = 1 is obvious. Assume n ≥2. We proceed as follows: multiply
Row n −1 by x1 and subtract it from Row n (the last row), then multiply
Row n −2 by x1 and subtract it from Row n −1, etc., multiply Row i −1
by x1 and subtract it from row i, until we reach Row 1. We obtain the
following determinant:
V (x1, . . . , xn) =

1
1
. . .
1
0
x2 −x1
. . .
xn −x1
0
x2(x2 −x1)
. . .
xn(xn −x1)
...
...
...
...
0 xn−2
2
(x2 −x1) . . . xn−2
n
(xn −x1)

.
Now expanding this determinant according to the ﬁrst column and using
multilinearity, we can factor (xi −x1) from the column of index i −1 of the
matrix obtained by deleting the ﬁrst row and the ﬁrst column, and thus
V (x1, . . . , xn) = (x2 −x1)(x3 −x1) · · · (xn −x1)V (x2, . . . , xn),
which establishes the induction step.

176
Determinants
Remark: Observe that
∆(x1, . . . , xn) = V (xn, . . . , x1) = (−1)(
n
2)V (x1, . . . xn),
where ∆(x1, . . . , xn) is the discriminant of (x1, . . . , xn) introduced in Deﬁ-
nition 6.2.
Lemma 6.1 can be reformulated nicely as follows.
Proposition 6.5. Let f : E × . . . × E →F be an n-linear alternating map.
Let (u1, . . . , un) and (v1, . . . , vn) be two families of n vectors, such that
v1 = a1 1u1 + · · · + a1 nun,
. . .
vn = an 1u1 + · · · + an nun.
Equivalently, letting
A =





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n




,
assume that we have





v1
v2
...
vn




= A





u1
u2
...
un




.
Then,
f(v1, . . . , vn) = det(A)f(u1, . . . , un).
Proof. The only diﬀerence with Lemma 6.1 is that here we are using A⊤
instead of A. Thus, by Lemma 6.1 and Corollary 6.1, we get the desired
result.
As a consequence, we get the very useful property that the determinant
of a product of matrices is the product of the determinants of these matrices.
Proposition 6.6. For any two n×n-matrices A and B, we have det(AB) =
det(A) det(B).

6.4. Inverse Matrices and Determinants
177
Proof. We use Proposition 6.5 as follows: let (e1, . . . , en) be the standard
basis of Kn, and let





w1
w2
...
wn




= AB





e1
e2
...
en




.
Then we get
det(w1, . . . , wn) = det(AB) det(e1, . . . , en) = det(AB),
since det(e1, . . . , en) = 1. Now letting





v1
v2
...
vn




= B





e1
e2
...
en




,
we get
det(v1, . . . , vn) = det(B),
and since





w1
w2
...
wn




= A





v1
v2
...
vn




,
we get
det(w1, . . . , wn) = det(A) det(v1, . . . , vn) = det(A) det(B).
It should be noted that all the results of this section, up to now, also
hold when K is a commutative ring and not necessarily a ﬁeld. We can now
characterize when an n×n-matrix A is invertible in terms of its determinant
det(A).
6.4
Inverse Matrices and Determinants
In the next two sections, K is a commutative ring and when needed a ﬁeld.
Deﬁnition 6.9. Let K be a commutative ring.
Given a matrix A ∈
Mn(K), let eA = (bi j) be the matrix deﬁned such that
bi j = (−1)i+j det(Aj i),
the cofactor of aj i. The matrix eA is called the adjugate of A, and each
matrix Aj i is called a minor of the matrix A.

178
Determinants
For example, if
A =


1 1
1
2 −2 −2
3 3 −3

,
we have
b11 = det(A11) =

−2 −2
3 −3
 = 12
b12 = −det(A21) = −

1 1
3 −3
 = 6
b13 = det(A31) =

1
1
−2 −2
 = 0
b21 = −det(A12) = −

2 −2
3 −3
 = 0
b22 = det(A22) =

1 1
3 −3
 = −6
b23 = −det(A32) = −

1 1
2 −2
 = 4
b31 = det(A13) =

2 −2
3 3
 = 12
b32 = −det(A23) = −

1 1
3 3
 = 0
b33 = det(A33) =

1 1
2 −2
 = −4,
we ﬁnd that
eA =


12 6
0
0 −6 4
12 0 −4

.

Note the reversal of the indices in
bi j = (−1)i+j det(Aj i).
Thus, eA is the transpose of the matrix of cofactors of elements of A.
We have the following proposition.
Proposition 6.7. Let K be a commutative ring. For every matrix A ∈
Mn(K), we have
A eA = eAA = det(A)In.
As a consequence, A is invertible iﬀdet(A) is invertible, and if so, A−1 =
(det(A))−1 eA.
Proof. If eA = (bi j) and A eA = (ci j), we know that the entry ci j in row i
and column j of A eA is
ci j = ai 1b1 j + · · · + ai kbk j + · · · + ai nbn j,

6.4. Inverse Matrices and Determinants
179
which is equal to
ai 1(−1)j+1 det(Aj 1) + · · · + ai n(−1)j+n det(Aj n).
If j = i, then we recognize the expression of the expansion of det(A) ac-
cording to the i-th row:
ci i = det(A) = ai 1(−1)i+1 det(Ai 1) + · · · + ai n(−1)i+n det(Ai n).
If j ̸= i, we can form the matrix A′ by replacing the j-th row of A by the
i-th row of A. Now the matrix Aj k obtained by deleting row j and column
k from A is equal to the matrix A′
j k obtained by deleting row j and column
k from A′, since A and A′ only diﬀer by the j-th row. Thus,
det(Aj k) = det(A′
j k),
and we have
ci j = ai 1(−1)j+1 det(A′
j 1) + · · · + ai n(−1)j+n det(A′
j n).
However, this is the expansion of det(A′) according to the j-th row, since
the j-th row of A′ is equal to the i-th row of A. Furthermore, since A′ has
two identical rows i and j, because det is an alternating map of the rows
(see an earlier remark), we have det(A′) = 0. Thus, we have shown that
ci i = det(A), and ci j = 0, when j ̸= i, and so
A eA = det(A)In.
It is also obvious from the deﬁnition of eA, that
eA⊤= f
A⊤.
Then applying the ﬁrst part of the argument to A⊤, we have
A⊤f
A⊤= det(A⊤)In,
and since det(A⊤) = det(A), eA⊤= f
A⊤, and ( eAA)⊤= A⊤eA⊤, we get
det(A)In = A⊤f
A⊤= A⊤eA⊤= ( eAA)⊤,
that is,
( eAA)⊤= det(A)In,
which yields
eAA = det(A)In,
since I⊤
n = In. This proves that
A eA = eAA = det(A)In.
As a consequence, if det(A) is invertible, we have A−1 = (det(A))−1 eA.
Conversely, if A is invertible, from AA−1 = In, by Proposition 6.6, we have
det(A) det(A−1) = 1, and det(A) is invertible.

180
Determinants
For example, we saw earlier that
A =


1 1
1
2 −2 −2
3 3 −3


and
eA =


12 6
0
0 −6 4
12 0 −4

,
and we have


1 1
1
2 −2 −2
3 3 −3




12 6
0
0 −6 4
12 0 −4

= 24


1 0 0
0 1 0
0 0 1


with det(A) = 24.
When K is a ﬁeld, an element a ∈K is invertible iﬀa ̸= 0. In this
case, the second part of the proposition can be stated as A is invertible iﬀ
det(A) ̸= 0. Note in passing that this method of computing the inverse of
a matrix is usually not practical.
6.5
Systems of Linear Equations and Determinants
We now consider some applications of determinants to linear independence
and to solving systems of linear equations. Although these results hold for
matrices over certain rings, their proofs require more sophisticated methods.
Therefore, we assume again that K is a ﬁeld (usually, K = R or K = C).
Let A be an n×n-matrix, x a column vectors of variables, and b another
column vector, and let A1, . . . , An denote the columns of A. Observe that
the system of equations Ax = b,





a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
...
...
...
...
an 1 an 2 . . . an n










x1
x2
...
xn




=





b1
b2
...
bn





is equivalent to
x1A1 + · · · + xjAj + · · · + xnAn = b,
since the equation corresponding to the i-th row is in both cases
ai 1x1 + · · · + ai jxj + · · · + ai nxn = bi.
First we characterize linear independence of the column vectors of a
matrix A in terms of its determinant.
Proposition 6.8. Given an n × n-matrix A over a ﬁeld K, the columns
A1, . . . , An of A are linearly dependent iﬀdet(A) = det(A1, . . . , An) = 0.
Equivalently, A has rank n iﬀdet(A) ̸= 0.

6.5. Systems of Linear Equations and Determinants
181
Proof. First assume that the columns A1, . . . , An of A are linearly depen-
dent. Then there are x1, . . . , xn ∈K, such that
x1A1 + · · · + xjAj + · · · + xnAn = 0,
where xj ̸= 0 for some j. If we compute
det(A1, . . . , x1A1 + · · · + xjAj + · · · + xnAn, . . . , An)
= det(A1, . . . , 0, . . . , An) = 0,
where 0 occurs in the j-th position. By multilinearity, all terms containing
two identical columns Ak for k ̸= j vanish, and we get
det(A1, . . . , x1A1+· · ·+xjAj+· · ·+xnAn, . . . , An) = xj det(A1, . . . , An) = 0.
Since xj ̸= 0 and K is a ﬁeld, we must have det(A1, . . . , An) = 0.
Conversely, we show that if the columns A1, . . . , An of A are linearly
independent, then det(A1, . . . , An) ̸= 0. If the columns A1, . . . , An of A are
linearly independent, then they form a basis of Kn, and we can express the
standard basis (e1, . . . , en) of Kn in terms of A1, . . . , An. Thus, we have





e1
e2
...
en




=





b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
...
...
...
...
bn 1 bn 2 . . . bn n










A1
A2
...
An




,
for some matrix B = (bi j), and by Proposition 6.5, we get
det(e1, . . . , en) = det(B) det(A1, . . . , An),
and since det(e1, . . . , en) = 1, this implies that det(A1, . . . , An) ̸= 0 (and
det(B) ̸= 0). For the second assertion, recall that the rank of a matrix is
equal to the maximum number of linearly independent columns, and the
conclusion is clear.
We now characterize when a system of linear equations of the form
Ax = b has a unique solution.
Proposition 6.9. Given an n × n-matrix A over a ﬁeld K, the following
properties hold:
(1) For every column vector b, there is a unique column vector x such that
Ax = b iﬀthe only solution to Ax = 0 is the trivial vector x = 0, iﬀ
det(A) ̸= 0.

182
Determinants
(2) If det(A) ̸= 0, the unique solution of Ax = b is given by the expressions
xj = det(A1, . . . , Aj−1, b, Aj+1, . . . , An)
det(A1, . . . , Aj−1, Aj, Aj+1, . . . , An),
known as Cramer's rules.
(3) The system of linear equations Ax = 0 has a nonzero solution iﬀ
det(A) = 0.
Proof. (1) Assume that Ax = b has a single solution x0, and assume that
Ay = 0 with y ̸= 0. Then,
A(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,
and x0 +y ̸= x0 is another solution of Ax = b, contradicting the hypothesis
that Ax = b has a single solution x0. Thus, Ax = 0 only has the trivial so-
lution. Now assume that Ax = 0 only has the trivial solution. This means
that the columns A1, . . . , An of A are linearly independent, and by Propo-
sition 6.8, we have det(A) ̸= 0. Finally, if det(A) ̸= 0, by Proposition 6.7,
this means that A is invertible, and then for every b, Ax = b is equivalent
to x = A−1b, which shows that Ax = b has a single solution.
(2) Assume that Ax = b. If we compute
det(A1, . . . , x1A1+· · ·+xjAj+· · ·+xnAn, . . . , An) = det(A1, . . . , b, . . . , An),
where b occurs in the j-th position, by multilinearity, all terms containing
two identical columns Ak for k ̸= j vanish, and we get
xj det(A1, . . . , An) = det(A1, . . . , Aj−1, b, Aj+1, . . . , An),
for every j, 1 ≤j ≤n. Since we assumed that det(A) = det(A1, . . . , An) ̸=
0, we get the desired expression.
(3) Note that Ax = 0 has a nonzero solution iﬀA1, . . . , An are lin-
early dependent (as observed in the proof of Proposition 6.8), which, by
Proposition 6.8, is equivalent to det(A) = 0.
As pleasing as Cramer's rules are, it is usually impractical to solve
systems of linear equations using the above expressions. However, these
formula imply an interesting fact, which is that the solution of the system
Ax = b are continuous in A and b. If we assume that the entries in A
are continuous functions aij(t) and the entries in b are are also continuous
functions bj(t) of a real parameter t, since determinants are polynomial
functions of their entries, the expressions
xj(t) = det(A1, . . . , Aj−1, b, Aj+1, . . . , An)
det(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)
are ratios of polynomials, and thus are also continuous as long as det(A(t))
is nonzero. Similarly, if the functions aij(t) and bj(t) are diﬀerentiable, so
are the xj(t).

6.6. Determinant of a Linear Map
183
6.6
Determinant of a Linear Map
Given a vector space E of ﬁnite dimension n, given a basis (u1, . . . , un) of
E, for every linear map f : E →E, if M(f) is the matrix of f w.r.t. the
basis (u1, . . . , un), we can deﬁne det(f) = det(M(f)). If (v1, . . . , vn) is any
other basis of E, and if P is the change of basis matrix, by Corollary 3.1,
the matrix of f with respect to the basis (v1, . . . , vn) is P −1M(f)P. By
Proposition 6.6, we have
det(P −1M(f)P) = det(P −1) det(M(f)) det(P) =
det(P −1) det(P) det(M(f)) = det(M(f)).
Thus, det(f) is indeed independent of the basis of E.
Deﬁnition 6.10. Given a vector space E of ﬁnite dimension, for any linear
map f : E →E, we deﬁne the determinant det(f) of f as the determinant
det(M(f)) of the matrix of f in any basis (since, from the discussion just
before this deﬁnition, this determinant does not depend on the basis).
Then we have the following proposition.
Proposition 6.10. Given any vector space E of ﬁnite dimension n, a lin-
ear map f : E →E is invertible iﬀdet(f) ̸= 0.
Proof. The linear map f : E →E is invertible iﬀits matrix M(f) in any
basis is invertible (by Proposition 3.2), iﬀdet(M(f)) ̸= 0, by Proposi-
tion 6.7.
Given a vector space of ﬁnite dimension n, it is easily seen that the set
of bijective linear maps f : E →E such that det(f) = 1 is a group under
composition. This group is a subgroup of the general linear group GL(E).
It is called the special linear group (of E), and it is denoted by SL(E), or
when E = Kn, by SL(n, K), or even by SL(n).
6.7
The Cayley-Hamilton Theorem
We next discuss an interesting and important application of Proposition 6.7,
the Cayley-Hamilton theorem. The results of this section apply to matrices
over any commutative ring K. First we need the concept of the character-
istic polynomial of a matrix.
Deﬁnition 6.11. If K is any commutative ring, for every n × n matrix
A ∈Mn(K), the characteristic polynomial PA(X) of A is the determinant
PA(X) = det(XI −A).

184
Determinants
The characteristic polynomial PA(X) is a polynomial in K[X], the ring
of polynomials in the indeterminate X with coeﬃcients in the ring K. For
example, when n = 2, if
A =
a b
c d

,
then
PA(X) =

X −a
−b
−c
X −d
 = X2 −(a + d)X + ad −bc.
We can substitute the matrix A for the variable X in the polynomial PA(X),
obtaining a matrix PA. If we write
PA(X) = Xn + c1Xn−1 + · · · + cn,
then
PA = An + c1An−1 + · · · + cnI.
We have the following remarkable theorem.
Theorem 6.2. (Cayley-Hamilton) If K is any commutative ring, for every
n × n matrix A ∈Mn(K), if we let
PA(X) = Xn + c1Xn−1 + · · · + cn
be the characteristic polynomial of A, then
PA = An + c1An−1 + · · · + cnI = 0.
Proof. We can view the matrix B = XI −A as a matrix with coeﬃcients
in the polynomial ring K[X], and then we can form the matrix eB which is
the transpose of the matrix of cofactors of elements of B. Each entry in eB
is an (n−1)×(n−1) determinant, and thus a polynomial of degree a most
n −1, so we can write eB as
eB = Xn−1B0 + Xn−2B1 + · · · + Bn−1,
for some n × n matrices B0, . . . , Bn−1 with coeﬃcients in K. For example,
when n = 2, we have
B =
X −a
−b
−c
X −d

,
eB =
X −d
b
c
X −a

= X
1 0
0 1

+
−d b
c −a

.
By Proposition 6.7, we have
B eB = det(B)I = PA(X)I.

6.7. The Cayley-Hamilton Theorem
185
On the other hand, we have
B eB = (XI −A)(Xn−1B0 + Xn−2B1 + · · · + Xn−j−1Bj + · · · + Bn−1),
and by multiplying out the right-hand side, we get
B eB = XnD0 + Xn−1D1 + · · · + Xn−jDj + · · · + Dn,
with
D0 = B0
D1 = B1 −AB0
...
Dj = Bj −ABj−1
...
Dn−1 = Bn−1 −ABn−2
Dn = −ABn−1.
Since
PA(X)I = (Xn + c1Xn−1 + · · · + cn)I,
the equality
XnD0 + Xn−1D1 + · · · + Dn = (Xn + c1Xn−1 + · · · + cn)I
is an equality between two matrices, so it requires that all corresponding
entries are equal, and since these are polynomials, the coeﬃcients of these
polynomials must be identical, which is equivalent to the set of equations
I = B0
c1I = B1 −AB0
...
cjI = Bj −ABj−1
...
cn−1I = Bn−1 −ABn−2
cnI = −ABn−1,

186
Determinants
for all j, with 1 ≤j ≤n −1. If, as in the table below,
An = AnB0
c1An−1 = An−1(B1 −AB0)
...
cjAn−j = An−j(Bj −ABj−1)
...
cn−1A = A(Bn−1 −ABn−2)
cnI = −ABn−1,
we multiply the ﬁrst equation by An, the last by I, and generally the
(j + 1)th by An−j, when we add up all these new equations, we see that
the right-hand side adds up to 0, and we get our desired equation
An + c1An−1 + · · · + cnI = 0,
as claimed.
As a concrete example, when n = 2, the matrix
A =
a b
c d

satisﬁes the equation
A2 −(a + d)A + (ad −bc)I = 0.
Most readers will probably ﬁnd the proof of Theorem 6.2 rather clever
but very mysterious and unmotivated. The conceptual diﬃculty is that
we really need to understand how polynomials in one variable "act" on
vectors in terms of the matrix A.
This can be done and yields a more
"natural" proof. Actually, the reasoning is simpler and more general if we
free ourselves from matrices and instead consider a ﬁnite-dimensional vector
space E and some given linear map f : E →E. Given any polynomial
p(X) = a0Xn +a1Xn−1 +· · ·+an with coeﬃcients in the ﬁeld K, we deﬁne
the linear map p(f): E →E by
p(f) = a0f n + a1f n−1 + · · · + anid,
where f k = f ◦· · · ◦f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f n(u) + a1f n−1(u) + · · · + anu,

6.7. The Cayley-Hamilton Theorem
187
for every vector u ∈E. Then we deﬁne a new kind of scalar multiplication
·: K[X] × E →E by polynomials as follows: for every polynomial p(X) ∈
K[X], for every u ∈E,
p(X) · u = p(f)(u).
It is easy to verify that this is a "good action," which means that
p · (u + v) = p · u + p · v
(p + q) · u = p · u + q · u
(pq) · u = p · (q · u)
1 · u = u,
for all p, q ∈K[X] and all u, v ∈E. With this new scalar multiplication, E
is a K[X]-module.
If p = λ is just a scalar in K (a polynomial of degree 0), then
λ · u = (λid)(u) = λu,
which means that K acts on E by scalar multiplication as before. If p(X) =
X (the monomial X), then
X · u = f(u).
Now if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈K[X]
has the property that
p(X) · ei = 0,
i = 1, . . . , n,
then this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the
linear map p(f) vanishes on E. We can also check, as we did in Section 6.2,
that if A and B are two n×n matrices and if (u1, . . . , un) are any n vectors,
then
A ·


B ·



u1
...
un





= (AB) ·



u1
...
un


.
This suggests the plan of attack for our second proof of the Cayley-
Hamilton theorem. For simplicity, we prove the theorem for vector spaces
over a ﬁeld. The proof goes through for a free module over a commutative
ring.
Theorem 6.3. (Cayley-Hamilton) For every ﬁnite-dimensional vector
space over a ﬁeld K, for every linear map f : E →E, for every basis
(e1, . . . , en), if A is the matrix over f over the basis (e1, . . . , en) and if
PA(X) = Xn + c1Xn−1 + · · · + cn
is the characteristic polynomial of A, then
PA(f) = f n + c1f n−1 + · · · + cnid = 0.

188
Determinants
Proof. Since the columns of A consist of the vector f(ej) expressed over
the basis (e1, . . . , en), we have
f(ej) =
n
X
i=1
ai jei,
1 ≤j ≤n.
Using our action of K[X] on E, the above equations can be expressed as
X · ej =
n
X
i=1
ai j · ei,
1 ≤j ≤n,
which yields
j−1
X
i=1
−ai j · ei + (X −aj j) · ej +
n
X
i=j+1
−ai j · ei = 0,
1 ≤j ≤n.
Observe that the transpose of the characteristic polynomial shows up, so
the above system can be written as





X −a1 1
−a2 1
· · ·
−an 1
−a1 2
X −a2 2 · · ·
−an 2
...
...
...
...
−a1 n
−a2 n
· · · X −an n




·





e1
e2
...
en




=





0
0
...
0




.
If we let B = XI −A⊤, then as in the previous proof, if eB is the transpose
of the matrix of cofactors of B, we have
eBB = det(B)I = det(XI −A⊤)I = det(XI −A)I = PAI.
But since
B ·





e1
e2
...
en




=





0
0
...
0




,
and since eB is matrix whose entries are polynomials in K[X], it makes
sense to multiply on the left by eB and we get
eB · B ·





e1
e2
...
en




= ( eBB) ·





e1
e2
...
en




= PAI ·





e1
e2
...
en




= eB ·





0
0
...
0




=





0
0
...
0




;
that is,
PA · ej = 0,
j = 1, . . . , n,
which proves that PA(f) = 0, as claimed.

6.8. Permanents
189
If K is a ﬁeld, then the characteristic polynomial of a linear map f : E →
E is independent of the basis (e1, . . . , en) chosen in E.
To prove this,
observe that the matrix of f over another basis will be of the form P −1AP,
for some inverible matrix P, and then
det(XI −P −1AP) = det(XP −1IP −P −1AP)
= det(P −1(XI −A)P)
= det(P −1) det(XI −A) det(P)
= det(XI −A).
Therefore, the characteristic polynomial of a linear map is intrinsic to f,
and it is denoted by Pf.
The zeros (roots) of the characteristic polynomial of a linear map f are
called the eigenvalues of f. They play an important role in theory and
applications. We will come back to this topic later on.
6.8
Permanents
Recall that the explicit formula for the determinant of an n × n matrix is
det(A) =
X
π∈Sn
ϵ(π)aπ(1) 1 · · · aπ(n) n.
If we drop the sign ϵ(π) of every permutation from the above formula, we
obtain a quantity known as the permanent:
per(A) =
X
π∈Sn
aπ(1) 1 · · · aπ(n) n.
Permanents and determinants were investigated as early as 1812 by Cauchy.
It is clear from the above deﬁnition that the permanent is a multilinear
symmetric form. We also have
per(A) = per(A⊤),
and the following unsigned version of the Laplace expansion formula:
per(A) = ai 1per(Ai 1) + · · · + ai jper(Ai j) + · · · + ai nper(Ai n),
for i = 1, . . . , n. However, unlike determinants which have a clear geomet-
ric interpretation as signed volumes, permanents do not have any natural
geometric interpretation. Furthermore, determinants can be evaluated eﬃ-
ciently, for example using the conversion to row reduced echelon form, but
computing the permanent is hard.

190
Determinants
Permanents turn out to have various combinatorial interpretations. One
of these is in terms of perfect matchings of bipartite graphs which we now
discuss.
See Deﬁnition 18.5 for the deﬁnition of an undirected graph. A bipartite
(undirected) graph G = (V, E) is a graph whose set of nodes V can be
partitioned into two nonempty disjoint subsets V1 and V2, such that every
edge e ∈E has one endpoint in V1 and one endpoint in V2.
An example of a bipartite graph with 14 nodes is shown in Figure 6.3;
its nodes are partitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and
{y1, y2, y3, y4, y5, y6, y7}.
x1
x2
x3
x4
x5
x6
x7
y1
y2
y3
y4
y5
y6
y7
Fig. 6.3
A bipartite graph G.
A matching in a graph G = (V, E) (bipartite or not) is a set M of
pairwise non-adjacent edges, which means that no two edges in M share
a common vertex. A perfect matching is a matching such that every node
in V is incident to some edge in the matching M (every node in V is an
endpoint of some edge in M). Figure 6.4 shows a perfect matching (in red)
in the bipartite graph G.
Obviously, a perfect matching in a bipartite graph can exist only if its
set of nodes has a partition in two blocks of equal size, say {x1, . . . , xm}
and {y1, . . . , ym}. Then there is a bijection between perfect matchings and
bijections π: {x1, . . . , xm} →{y1, . . . , ym} such that π(xi) = yj iﬀthere is
an edge between xi and yj.
Now every bipartite graph G with a partition of its nodes into two
sets of equal size as above is represented by an m × m matrix A = (aij)
such that aij = 1 iﬀthere is an edge between xi and yj, and aij = 0
otherwise.
Using the interpretation of perfect matchings as bijections

6.8. Permanents
191
x1
x2
x3
x4
x5
x6
x7
y1
y2
y3
y4
y5
y6
y7
Fig. 6.4
A perfect matching in the bipartite graph G.
π: {x1, . . . , xm} →{y1, . . . , ym}, we see that the permanent per(A) of the
(0, 1)-matrix A representing the bipartite graph G counts the number of
perfect matchings in G.
In a famous paper published in 1979, Leslie Valiant proves that comput-
ing the permanent is a #P-complete problem. Such problems are suspected
to be intractable. It is known that if a polynomial-time algorithm existed
to solve a #P-complete problem, then we would have P = NP, which is
believed to be very unlikely.
Another combinatorial interpretation of the permanent can be given
in terms of systems of distinct representatives. Given a ﬁnite set S, let
(A1, . . . , An) be any sequence of nonempty subsets of S (not necessarily
distinct). A system of distinct representatives (for short SDR) of the sets
A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈
Ai for i = 1, . . . , n.
The number of SDR's of a sequence of sets plays
an important role in combinatorics. Now, if S = {1, 2, . . . , n} and if we
associate to any sequence (A1, . . . , An) of nonempty subsets of S the matrix
A = (aij) deﬁned such that aij = 1 if j ∈Ai and aij = 0 otherwise, then
the permanent per(A) counts the number of SDR's of the sets A1, . . . , An.
This interpretation of permanents in terms of SDR's can be used to
prove bounds for the permanents of various classes of matrices. Interested
readers are referred to van Lint and Wilson [van Lint and Wilson (2001)]
(Chapters 11 and 12). In particular, a proof of a theorem known as Van
der Waerden conjecture is given in Chapter 12. This theorem states that
for any n × n matrix A with nonnegative entries in which all row-sums and

192
Determinants
column-sums are 1 (doubly stochastic matrices), we have
per(A) ≥n!
nn ,
with equality for the matrix in which all entries are equal to 1/n.
6.9
Summary
The main concepts and results of this chapter are listed below:
• Permutations, transpositions, basics transpositions.
• Every permutation can be written as a composition of permutations.
• The parity of the number of transpositions involved in any decomposi-
tion of a permutation σ is an invariant; it is the signature ϵ(σ) of the
permutation σ.
• Multilinear maps (also called n-linear maps); bilinear maps.
• Symmetric and alternating multilinear maps.
• A basic property of alternating multilinear maps (Lemma 6.1) and the
introduction of the formula expressing a determinant.
• Deﬁnition
of
a
determinant
as
a
multlinear
alternating
map
D: Mn(K) →K such that D(I) = 1.
• We deﬁne the set of algorithms Dn, to compute the determinant of an
n × n matrix.
• Laplace expansion according to the ith row; cofactors.
• We prove that the algorithms in Dn compute determinants (Lemma
6.2).
• We prove that all algorithms in Dn compute the same determinant
(Theorem 6.1).
• We give an interpretation of determinants as signed volumes.
• We prove that det(A) = det(A⊤).
• We prove that det(AB) = det(A) det(B).
• The adjugate eA of a matrix A.
• Formula for the inverse in terms of the adjugate.
• A matrix A is invertible iﬀdet(A) ̸= 0.
• Solving linear equations using Cramer's rules.
• Determinant of a linear map.
• The characteristic polynomial of a matrix.
• The Cayley-Hamilton theorem.
• The action of the polynomial ring induced by a linear map on a vector
space.

6.10. Further Readings
193
• Permanents.
• Permanents count the number of perfect matchings in bipartite graphs.
• Computing the permanent is a #P-perfect problem (L. Valiant).
• Permanents count the number of SDRs of sequences of subsets of a
given set.
6.10
Further Readings
Thorough expositions of the material covered in Chapters 2-5 and 6 can be
found in Strang [Strang (1988, 1986)], Lax [Lax (2007)], Lang [Lang (1993)],
Artin [Artin (1991)], Mac Lane and Birkhoﬀ[Mac Lane and Birkhoﬀ
(1967)], Hoﬀman and Kunze [Kenneth and Ray (1971)], Dummit and Foote
[Dummit and Foote (1999)], Bourbaki [Bourbaki (1970, 1981a)], Van Der
Waerden [Van Der Waerden (1973)], Serre [Serre (2010)], Horn and Johnson
[Horn and Johnson (1990)], and Bertin [Bertin (1981)]. These notions of
linear algebra are nicely put to use in classical geometry, see Berger [Berger
(1990a,b)], Tisseron [Tisseron (1994)] and Dieudonn´e [Dieudonn´e (1965)].
6.11
Problems
Problem 6.1. Prove that every transposition can be written as a product
of basic transpositions.
Problem 6.2. (1) Given two vectors in R2 of coordinates (c1 −a1, c2 −a2)
and (b1 −a1, b2 −a2), prove that they are linearly dependent iﬀ

a1 b1 c1
a2 b2 c2
1 1 1

= 0.
(2) Given three vectors in R3 of coordinates (d1 −a1, d2 −a2, d3 −a3),
(c1 −a1, c2 −a2, c3 −a3), and (b1 −a1, b2 −a2, b3 −a3), prove that they are
linearly dependent iﬀ

a1 b1 c1 d1
a2 b2 c2 d2
a3 b3 c3 d3
1 1 1 1

= 0.
Problem 6.3. Let A be the (m+n)×(m+n) block matrix (over any ﬁeld
K) given by
A =
A1 A2
0 A4

,

194
Determinants
where A1 is an m × m matrix, A2 is an m × n matrix, and A4 is an n × n
matrix. Prove that det(A) = det(A1) det(A4).
Use the above result to prove that if A is an upper triangular n × n
matrix, then det(A) = a11a22 · · · ann.
Problem 6.4. Prove that if n ≥3, then
det





1 + x1y1 1 + x1y2 · · · 1 + x1yn
1 + x2y1 1 + x2y2 · · · 1 + x2yn
...
...
...
...
1 + xny1 1 + xny2 · · · 1 + xnyn




= 0.
Problem 6.5. Prove that

1 4 9 16
4 9 16 25
9 16 25 36
16 25 36 49

= 0.
Problem 6.6. Consider the n × n symmetric matrix
A =












1 2 0
0 . . . 0 0
2 5 2
0 . . . 0 0
0 2 5
2 . . . 0 0
... ... ... ... ... ... ...
0 0 . . . 2
5 2 0
0 0 . . . 0
2 5 2
0 0 . . . 0
0 2 5












.
(1) Find an upper-triangular matrix R such that A = R⊤R.
(2) Prove that det(A) = 1.
(3) Consider the sequence
p0(λ) = 1
p1(λ) = 1 −λ
pk(λ) = (5 −λ)pk−1(λ) −4pk−2(λ)
2 ≤k ≤n.
Prove that
det(A −λI) = pn(λ).
Remark: It can be shown that pn(λ) has n distinct (real) roots and that
the roots of pk(λ) separate the roots of pk+1(λ).

6.11. Problems
195
Problem 6.7. Let B be the n × n matrix (n ≥3) given by
B =












1 −1 −1 −1 · · · −1 −1
1 −1 1
1 · · ·
1
1
1 1 −1 1 · · ·
1
1
1 1
1 −1 · · ·
1
1
...
...
...
...
...
...
...
1 1
1
1 · · · −1 1
1 1
1
1 · · ·
1 −1












.
Prove that
det(B) = (−1)n(n −2)2n−1.
Problem 6.8. Given a ﬁeld K (say K = R or K = C), given any two
polynomials p(X), q(X) ∈K[X], we says that q(X) divides p(X) (and that
p(X) is a multiple of q(X)) iﬀthere is some polynomial s(X) ∈K[X] such
that
p(X) = q(X)s(X).
In this case we say that q(X) is a factor of p(X), and if q(X) has degree
at least one, we say that q(X) is a nontrivial factor of p(X).
Let f(X) and g(X) be two polynomials in K[X] with
f(X) = a0Xm + a1Xm−1 + · · · + am
of degree m ≥1 and
g(X) = b0Xn + b1Xn−1 + · · · + bn
of degree n ≥1 (with a0, b0 ̸= 0).
You will need the following result which you need not prove:
Two polynomials f(X) and g(X) with deg(f) = m ≥1 and deg(g) =
n ≥1 have some common nontrivial factor iﬀthere exist two nonzero
polynomials p(X) and q(X) such that
fp = gq,
with deg(p) ≤n −1 and deg(q) ≤m −1.
(1) Let Pm denote the vector space of all polynomials in K[X] of degree
at most m −1, and let T : Pn × Pm →Pm+n be the map given by
T(p, q) = fp + gq,
p ∈Pn, q ∈Pm,
where f and g are some ﬁxed polynomials of degree m ≥1 and n ≥1.
Prove that the map T is linear.

196
Determinants
(2) Prove that T is not injective iﬀf and g have a common nontrivial
factor.
(3) Prove that f and g have a nontrivial common factor iﬀR(f, g) = 0,
where R(f, g) is the determinant given by
R(f, g) =

a0 a1 · · · · · · am 0 · · · · · · · · · · · ·
0
0
a0 a1 · · · · · · am 0 · · · · · · · · ·
0
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · · · · · · · · · · ·
0
a0 a1 · · · · · · am
b0
b1 · · · · · · · · · · · · · · · bn
0 · · ·
0
0
b0
b1 · · · · · · · · · · · · · · · bn
0 · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · ·
0
b0
b1 · · · · · · · · · · · · · · · bn

.
The above determinant is called the resultant of f and g.
Note that the matrix of the resultant is an (n + m) × (n + m) matrix,
with the ﬁrst row (involving the ais) occurring n times, each time shifted
over to the right by one column, and the (n + 1)th row (involving the bjs)
occurring m times, each time shifted over to the right by one column.
Hint. Express the matrix of T over some suitable basis.
(4) Compute the resultant in the following three cases:
(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.
(b) m = 1 and n ≥2 arbitrary.
(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.
(5) Compute the resultant of f(X) = X3 +pX +q and g(X) = 3X2 +p,
and
f(X) = a0X2 + a1X + a2
g(X) = b0X2 + b1X + b2.
In the second case, you should get
4R(f, g) = (2a0b2 −a1b1 + 2a2b0)2 −(4a0a2 −a2
1)(4b0b2 −b2
1).
Problem 6.9. Let A, B, C, D be n × n real or complex matrices.
(1) Prove that if A is invertible and if AC = CA, then
det
A B
C D

= det(AD −CB).

6.11. Problems
197
(2) Prove that if H is an n × n Hadamard matrix (n ≥2), then
| det(H)| = nn/2.
(3) Prove that if H is an n × n Hadamard matrix (n ≥2), then
det
H H
H −H

= (2n)n.
Problem 6.10. Compute the product of the following determinants

a −b −c −d
b
a −d c
c
d
a −b
d −c b
a


x −y −z −t
y x −t z
z
t
x −y
t −z y
x

to prove the following identity (due to Euler):
(a2 + b2 + c2 + d2)(x2 + y2 + z2 + t2)
= (ax + by + cz + dt)2 + (ay −bx + ct −dz)2
+ (az −bt −cx + dy)2 + (at + bz −cy + dx)2.
Problem 6.11. Let A be an n × n matrix with integer entries. Prove that
A−1 exists and has integer entries if and only if det(A) = ±1.
Problem 6.12. Let A be an n × n real or complex matrix.
(1) Prove that if A⊤= −A (A is skew-symmetric) and if n is odd, then
det(A) = 0.
(2) Prove that

0
a
b
c
−a 0
d e
−b −d 0 f
−c −e −f 0

= (af −be + dc)2.
Problem 6.13. A Cauchy matrix is a matrix of the form










1
λ1 −σ1
1
λ1 −σ2
· · ·
1
λ1 −σn
1
λ2 −σ1
1
λ2 −σ2
· · ·
1
λ2 −σn
...
...
...
...
1
λn −σ1
1
λn −σ2
· · ·
1
λn −σn










where λi ̸= σj, for all i, j, with 1 ≤i, j ≤n. Prove that the determinant
Cn of a Cauchy matrix as above is given by
Cn =
Qn
i=2
Qi−1
j=1(λi −λj)(σj −σi)
Qn
i=1
Qn
j=1(λi −σj)
.

198
Determinants
Problem 6.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct
scalars in R and let (β1, . . . , βm+1) be any sequence of scalars in R, not
necessarily distinct.
(1) Prove that there is a unique polynomial P of degree at most m such
that
P(αi) = βi,
1 ≤i ≤m + 1.
Hint. Remember Vandermonde!
(2) Let Li(X) be the polynomial of degree m given by
Li(X) = (X −α1) · · · (X −αi−1)(X −αi+1) · · · (X −αm+1)
(αi −α1) · · · (αi −αi−1)(αi −αi+1) · · · (αi −αm+1),
1 ≤i ≤m + 1.
The polynomials Li(X) are known as Lagrange polynomial interpolants.
Prove that
Li(αj) = δi j
1 ≤i, j ≤m + 1.
Prove that
P(X) = β1L1(X) + · · · + βm+1Lm+1(X)
is the unique polynomial of degree at most m such that
P(αi) = βi,
1 ≤i ≤m + 1.
(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that
they form a basis of all polynomials of degree at most m.
How is 1 (the constant polynomial 1) expressed over the basis
(L1(X), . . . , Lm+1(X))?
Give the expression of every polynomial P(X) of degree at most m over
the basis (L1(X), . . . , Lm+1(X)).
(4)
Prove
that
the
dual
basis
(L∗
1, . . . , L∗
m+1)
of
the
basis
(L1(X), . . . , Lm+1(X)) consists of the linear forms L∗
i given by
L∗
i (P) = P(αi),
for every polynomial P of degree at most m; this is simply evaluation at
αi.

Chapter 7
Gaussian Elimination,
LU-Factorization, Cholesky
Factorization, Reduced Row Echelon
Form
In this chapter we assume that all vector spaces are over the ﬁeld R. All
results that do not rely on the ordering on R or on taking square roots hold
for arbitrary ﬁelds.
7.1
Motivating Example: Curve Interpolation
Curve interpolation is a problem that arises frequently in computer graphics
and in robotics (path planning).
There are many ways of tackling this
problem and in this section we will describe a solution using cubic splines.
Such splines consist of cubic B´ezier curves. They are often used because
they are cheap to implement and give more ﬂexibility than quadratic B´ezier
curves.
A cubic B´ezier curve C(t) (in R2 or R3) is speciﬁed by a list of four
control points (b0, b1, b2, b3) and is given parametrically by the equation
C(t) = (1 −t)3 b0 + 3(1 −t)2t b1 + 3(1 −t)t2 b2 + t3 b3.
Clearly, C(0) = b0, C(1) = b3, and for t ∈[0, 1], the point C(t) belongs to
the convex hull of the control points b0, b1, b2, b3. The polynomials
(1 −t)3,
3(1 −t)2t,
3(1 −t)t2,
t3
are the Bernstein polynomials of degree 3.
Typically, we are only interested in the curve segment corresponding to
the values of t in the interval [0, 1]. Still, the placement of the control points
drastically aﬀects the shape of the curve segment, which can even have a
self-intersection; See Figures 7.1, 7.2, 7.3 illustrating various conﬁgurations.
199

200
Gaussian Elimination, LU, Cholesky, Echelon Form
b0
b1
b2
b3
Fig. 7.1
A "standard" B´ezier curve.
b0
b1
b2
b3
Fig. 7.2
A B´ezier curve with an inﬂection point.
b0
b1
b2
b3
Fig. 7.3
A self-intersecting B´ezier curve.

7.1. Motivating Example: Curve Interpolation
201
Interpolation problems require ﬁnding curves passing through some
given data points and possibly satisfying some extra constraints.
A B´ezier spline curve F is a curve which is made up of curve segments
which are B´ezier curves, say C1, . . . , Cm (m ≥2). We will assume that F
deﬁned on [0, m], so that for i = 1, . . . , m,
F(t) = Ci(t −i + 1),
i −1 ≤t ≤i.
Typically, some smoothness is required between any two junction points,
that is, between any two points Ci(1) and Ci+1(0), for i = 1, . . . , m −1.
We require that Ci(1) = Ci+1(0) (C0-continuity), and typically that the
derivatives of Ci at 1 and of Ci+1 at 0 agree up to second order derivatives.
This is called C2-continuity, and it ensures that the tangents agree as well
as the curvatures.
There are a number of interpolation problems, and we consider one of
the most common problems which can be stated as follows:
Problem: Given N + 1 data points x0, . . . , xN, ﬁnd a C2 cubic spline
curve F such that F(i) = xi for all i, 0 ≤i ≤N (N ≥2).
A way to solve this problem is to ﬁnd N + 3 auxiliary points
d−1, . . . , dN+1, called de Boor control points, from which N B´ezier curves
can be found. Actually,
d−1 = x0
and
dN+1 = xN
so we only need to ﬁnd N + 1 points d0, . . . , dN.
It turns out that the C2-continuity constraints on the N B´ezier curves
yield only N −1 equations, so d0 and dN can be chosen arbitrarily. In
practice, d0 and dN are chosen according to various end conditions, such
as prescribed velocities at x0 and xN. For the time being, we will assume
that d0 and dN are given.
Figure 7.4 illustrates an interpolation problem involving N +1 = 7+1 =
8 data points. The control points d0 and d7 were chosen arbitrarily.
It can be shown that d1, . . . , dN−1 are given by the linear system







7
2 1
1 4
1
0
... ... ...
0
1
4 1
1
7
2














d1
d2
...
dN−2
dN−1







=







6x1 −3
2d0
6x2
...
6xN−2
6xN−1 −3
2dN







.
We will show later that the above matrix is invertible because it is
strictly diagonally dominant.

202
Gaussian Elimination, LU, Cholesky, Echelon Form
x0 = d−1
x1
x2
x3
x4
x5
x6
x7 = d8
d0
d1
d2
d3
d4
d5
d6
d7
Fig. 7.4
A C2 cubic interpolation spline curve passing through the points x0, x1, x2, x3,
x4, x5, x6, x7.
Once the above system is solved, the B´ezier cubics C1, . . ., CN are de-
termined as follows (we assume N ≥2): For 2 ≤i ≤N −1, the control
points (bi
0, bi
1, bi
2, bi
3) of Ci are given by
bi
0 = xi−1
bi
1 = 2
3di−1 + 1
3di
bi
2 = 1
3di−1 + 2
3di
bi
3 = xi.
The control points (b1
0, b1
1, b1
2, b1
3) of C1 are given by
b1
0 = x0
b1
1 = d0
b1
2 = 1
2d0 + 1
2d1
b1
3 = x1,

7.2. Gaussian Elimination
203
and the control points (bN
0 , bN
1 , bN
2 , bN
3 ) of CN are given by
bN
0 = xN−1
bN
1 = 1
2dN−1 + 1
2dN
bN
2 = dN
bN
3 = xN.
Figure 7.5 illustrates this process spline interpolation for N = 7.
x0 =
d1
x1
x2
x3
x4
x5
x6
x7 =
d8
d0
d1
d2
d3
d4
d5
d6
d7
1
1
b  =
1
2
b
b 2
1
b 2
2
b
b 1
3
b 2
3
b 1
4
b 2
4
b 1
5
b 2
5
b 1
6
b 2
6
1
7
b 7
2
=
Fig. 7.5
A C2 cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color
coded B´ezier cubics.
We will now describe various methods for solving linear systems. Since
the matrix of the above system is tridiagonal, there are specialized methods
which are more eﬃcient than the general methods. We will discuss a few
of these methods.
7.2
Gaussian Elimination
Let A be an n×n matrix, let b ∈Rn be an n-dimensional vector and assume
that A is invertible. Our goal is to solve the system Ax = b. Since A is
assumed to be invertible, we know that this system has a unique solution
x = A−1b. Experience shows that two counter-intuitive facts are revealed:

204
Gaussian Elimination, LU, Cholesky, Echelon Form
(1) One should avoid computing the inverse A−1 of A explicitly.
This
is ineﬃcient since it would amount to solving the n linear systems
Au(j) = ej for j = 1, . . . , n, where ej = (0, . . . , 1, . . . , 0) is the jth
canonical basis vector of Rn (with a 1 is the jth slot). By doing so, we
would replace the resolution of a single system by the resolution of n
systems, and we would still have to multiply A−1 by b.
(2) One does not solve (large) linear systems by computing determinants
(using Cramer's formulae) since this method requires a number of ad-
ditions (resp. multiplications) proportional to (n + 1)! (resp. (n + 2)!).
The key idea on which most direct methods (as opposed to iterative
methods, that look for an approximation of the solution) are based is that if
A is an upper-triangular matrix, which means that aij = 0 for 1 ≤j < i ≤n
(resp. lower-triangular, which means that aij = 0 for 1 ≤i < j ≤n), then
computing the solution x is trivial. Indeed, say A is an upper-triangular
matrix
A =











a1 1 a1 2 · · · a1 n−2
a1 n−1
a1 n
0
a2 2 · · · a2 n−2
a2 n−1
a2 n
0
0
...
...
...
...
...
...
...
0
0
· · ·
0
an−1 n−1 an−1 n
0
0
· · ·
0
0
an n











.
Then det(A) = a1 1a2 2 · · · an n ̸= 0, which implies that ai i ̸= 0 for i =
1, . . . , n, and we can solve the system Ax = b from bottom-up by back-
substitution.
That is, ﬁrst we compute xn from the last equation, next
plug this value of xn into the next to the last equation and compute xn−1
from it, etc. This yields
xn = a−1
n nbn
xn−1 = a−1
n−1 n−1(bn−1 −an−1 nxn)
...
x1 = a−1
1 1(b1 −a1 2x2 −· · · −a1 nxn).
Note that the use of determinants can be avoided to prove that if A is
invertible then ai i ̸= 0 for i = 1, . . . , n. Indeed, it can be shown directly
(by induction) that an upper (or lower) triangular matrix is invertible iﬀ
all its diagonal entries are nonzero.

7.2. Gaussian Elimination
205
If A is lower-triangular, we solve the system from top-down by forward-
substitution.
Thus, what we need is a method for transforming a matrix to an equiv-
alent one in upper-triangular form. This can be done by elimination. Let
us illustrate this method on the following example:
2x + y + z = 5
4x −6y
= −2
−2x + 7y + 2z = 9.
We can eliminate the variable x from the second and the third equation as
follows: Subtract twice the ﬁrst equation from the second and add the ﬁrst
equation to the third. We get the new system
2x + y + z =
5
−8y −2z = −12
8y + 3z = 14.
This time we can eliminate the variable y from the third equation by adding
the second equation to the third:
2x + y + z =
5
−8y −2z = −12
z = 2.
This last system is upper-triangular. Using back-substitution, we ﬁnd the
solution: z = 2, y = 1, x = 1.
Observe that we have performed only row operations.
The general
method is to iteratively eliminate variables using simple row operations
(namely, adding or subtracting a multiple of a row to another row of the
matrix) while simultaneously applying these operations to the vector b,
to obtain a system, MAx = Mb, where MA is upper-triangular. Such a
method is called Gaussian elimination. However, one extra twist is needed
for the method to work in all cases: It may be necessary to permute rows,
as illustrated by the following example:
x + y + z = 1
x + y + 3z = 1
2x + 5y + 8z = 1.
In order to eliminate x from the second and third row, we subtract the ﬁrst
row from the second and we subtract twice the ﬁrst row from the third:
x + y + z
= 1
2z
= 0
3y + 6z = −1.

206
Gaussian Elimination, LU, Cholesky, Echelon Form
Now the trouble is that y does not occur in the second row; so, we can't
eliminate y from the third row by adding or subtracting a multiple of the
second row to it. The remedy is simple: Permute the second and the third
row! We get the system:
x + y + z
= 1
3y + 6z = −1
2z = 0,
which is already in triangular form. Another example where some permu-
tations are needed is:
z =
1
−2x + 7y + 2z =
1
4x −6y
= −1.
First we permute the ﬁrst and the second row, obtaining
−2x + 7y + 2z =
1
z =
1
4x −6y
= −1,
and then we add twice the ﬁrst row to the third, obtaining:
−2x + 7y + 2z = 1
z = 1
8y + 4z = 1.
Again we permute the second and the third row, getting
−2x + 7y + 2z = 1
8y + 4z = 1
z = 1,
an upper-triangular system. Of course, in this example, z is already solved
and we could have eliminated it ﬁrst, but for the general method, we need
to proceed in a systematic fashion.
We now describe the method of Gaussian elimination applied to a linear
system Ax = b, where A is assumed to be invertible. We use the variable
k to keep track of the stages of elimination. Initially, k = 1.
(1) The ﬁrst step is to pick some nonzero entry ai 1 in the ﬁrst column of
A. Such an entry must exist, since A is invertible (otherwise, the ﬁrst
column of A would be the zero vector, and the columns of A would
not be linearly independent. Equivalently, we would have det(A) = 0).
The actual choice of such an element has some impact on the numerical

7.2. Gaussian Elimination
207
stability of the method, but this will be examined later. For the time
being, we assume that some arbitrary choice is made.
This chosen
element is called the pivot of the elimination step and is denoted π1
(so, in this ﬁrst step, π1 = ai 1).
(2) Next we permute the row (i) corresponding to the pivot with the ﬁrst
row. Such a step is called pivoting. So after this permutation, the ﬁrst
element of the ﬁrst row is nonzero.
(3) We now eliminate the variable x1 from all rows except the ﬁrst by
adding suitable multiples of the ﬁrst row to these rows. More precisely
we add −ai 1/π1 times the ﬁrst row to the ith row for i = 2, . . . , n. At
the end of this step, all entries in the ﬁrst column are zero except the
ﬁrst.
(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively
repeat Steps (1), (2), (3) on the (n −k + 1) × (n −k + 1) subsystem
obtained by deleting the ﬁrst k −1 rows and k −1 columns from the
current system.
If we let A1 = A and Ak = (a(k)
i j ) be the matrix obtained after k −1
elimination steps (2 ≤k ≤n), then the kth elimination step is applied to
the matrix Ak of the form
Ak =












a(k)
1 1 a(k)
1 2 · · · · · · · · · a(k)
1 n
0
a(k)
2 2 · · · · · · · · · a(k)
2 n
...
... ...
...
...
0
0
0 a(k)
k k · · · a(k)
k n
...
...
...
...
...
0
0
0 a(k)
n k · · · a(k)
n n












.
Actually, note that
a(k)
i j = a(i)
i j
for all i, j with 1 ≤i ≤k −2 and i ≤j ≤n, since the ﬁrst k −1 rows
remain unchanged after the (k −1)th step.
We will prove later that det(Ak) = ± det(A).
Consequently, Ak is
invertible.
The fact that Ak is invertible iﬀA is invertible can also be
shown without determinants from the fact that there is some invertible
matrix Mk such that Ak = MkA, as we will see shortly.
Since Ak is invertible, some entry a(k)
i k with k ≤i ≤n is nonzero.
Otherwise, the last n −k + 1 entries in the ﬁrst k columns of Ak would be

208
Gaussian Elimination, LU, Cholesky, Echelon Form
zero, and the ﬁrst k columns of Ak would yield k vectors in Rk−1. But then
the ﬁrst k columns of Ak would be linearly dependent and Ak would not
be invertible, a contradiction. This situation is illustrated by the following
matrix for n = 5 and k = 3:








a(3)
1 1 a(3)
1 2 a(3)
1 3 a(3)
1 3 a(3)
1 5
0
a(3)
2 2 a(3)
2 3 a(3)
2 4 a(3)
2 5
0
0
0
a(3)
3 4 a(3)
3 5
0
0
0
a(3)
4 4 a(3)
4 n
0
0
0
a(3)
5 4 a(3)
5 5








.
The ﬁrst three columns of the above matrix are linearly dependent.
So one of the entries a(k)
i k with k ≤i ≤n can be chosen as pivot, and we
permute the kth row with the ith row, obtaining the matrix α(k) = (α(k)
j l ).
The new pivot is πk = α(k)
k k, and we zero the entries i = k + 1, . . . , n in
column k by adding −α(k)
i k /πk times row k to row i. At the end of this
step, we have Ak+1. Observe that the ﬁrst k −1 rows of Ak are identical
to the ﬁrst k −1 rows of Ak+1.
The process of Gaussian elimination is illustrated in schematic form
below:




× × × ×
× × × ×
× × × ×
× × × ×



=⇒




× × × ×
0 × × ×
0 × × ×
0 × × ×



=⇒




× × × ×
0 × × ×
0 0 × ×
0 0 × ×



=⇒




× × × ×
0 × × ×
0 0 × ×
0 0 0 ×



.
7.3
Elementary Matrices and Row Operations
It is easy to ﬁgure out what kind of matrices perform the elementary row
operations used during Gaussian elimination. The key point is that if A =
PB, where A, B are m×n matrices and P is a square matrix of dimension m,
if (as usual) we denote the rows of A and B by A1, . . . , Am and B1, . . . , Bm,
then the formula
aij =
m
X
k=1
pikbkj
giving the (i, j)th entry in A shows that the ith row of A is a linear com-
bination of the rows of B:
Ai = pi1B1 + · · · + pimBm.

7.3. Elementary Matrices and Row Operations
209
Therefore, multiplication of a matrix on the left by a square matrix performs
row operations.
Similarly, multiplication of a matrix on the right by a
square matrix performs column operations
The permutation of the kth row with the ith row is achieved by multi-
plying A on the left by the transposition matrix P(i, k), which is the matrix
obtained from the identity matrix by permuting rows i and k, i.e.,
P(i, k) =
















1
1
0
1
1
...
1
1
0
1
1
















.
For example, if m = 3,
P(1, 3) =


0 0 1
0 1 0
1 0 0

,
then
P(1, 3)B =


0 0 1
0 1 0
1 0 0




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n

=


b31 b32 · · · · · · · · · b3n
b21 b22 · · · · · · · · · b2n
b11 b12 · · · · · · · · · b1n

.
Observe that det(P(i, k)) = −1.
Furthermore, P(i, k) is symmetric
(P(i, k)⊤= P(i, k)), and
P(i, k)−1 = P(i, k).
During the permutation Step (2), if row k and row i need to be per-
muted, the matrix A is multiplied on the left by the matrix Pk such that
Pk = P(i, k), else we set Pk = I.
Adding β times row j to row i (with i ̸= j) is achieved by multiplying
A on the left by the elementary matrix,
Ei,j;β = I + βei j,
where
(ei j)k l =
 1
if k = i and l = j
0
if k ̸= i or l ̸= j,

210
Gaussian Elimination, LU, Cholesky, Echelon Form
i.e.,
Ei,j;β =
















1
1
1
1
...
1
β
1
1
1
















or
Ei,j;β =
















1
1
1
β
1
...
1
1
1
1
















,
on the left, i > j, and on the right, i < j. The index i is the index of the
row that is changed by the multiplication. For example, if m = 3 and we
want to add twice row 1 to row 3, since β = 2, j = 1 and i = 3, we form
E3,1;2 = I + 2e31 =


1 0 0
0 1 0
0 0 1

+


0 0 0
0 0 0
2 0 0

=


1 0 0
0 1 0
2 0 1

,
and calculate
E3,1;2B =


1 0 0
0 1 0
2 0 1




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n


=


b11
b12
· · · · · ·
· · · b1n
b21
b22
· · · · · ·
· · · b2n
2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n

.
Observe that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I −βei j and that
det(Ei,j;β) = 1. Therefore, during Step 3 (the elimination step), the matrix
A is multiplied on the left by a product Ek of matrices of the form Ei,k;βi,k,
with i > k.
Consequently, we see that
Ak+1 = EkPkAk,
and then
Ak = Ek−1Pk−1 · · · E1P1A.
This justiﬁes the claim made earlier that Ak = MkA for some invertible
matrix Mk; we can pick
Mk = Ek−1Pk−1 · · · E1P1,

7.3. Elementary Matrices and Row Operations
211
a product of invertible matrices.
The fact that det(P(i, k)) = −1 and that det(Ei,j;β) = 1 implies imme-
diately the fact claimed above: We always have
det(Ak) = ± det(A).
Furthermore, since
Ak = Ek−1Pk−1 · · · E1P1A
and since Gaussian elimination stops for k = n, the matrix
An = En−1Pn−1 · · · E2P2E1P1A
is upper-triangular. Also note that if we let M = En−1Pn−1 · · · E2P2E1P1,
then det(M) = ±1, and
det(A) = ± det(An).
The matrices P(i, k) and Ei,j;β are called elementary matrices. We can
summarize the above discussion in the following theorem:
Theorem 7.1. (Gaussian elimination) Let A be an n×n matrix (invertible
or not). Then there is some invertible matrix M so that U = MA is upper-
triangular. The pivots are all nonzero iﬀA is invertible.
Proof. We already proved the theorem when A is invertible, as well as the
last assertion. Now A is singular iﬀsome pivot is zero, say at Stage k of
the elimination. If so, we must have a(k)
i k = 0 for i = k, . . . , n; but in this
case, Ak+1 = Ak and we may pick Pk = Ek = I.
Remark: Obviously, the matrix M can be computed as
M = En−1Pn−1 · · · E2P2E1P1,
but this expression is of no use. Indeed, what we need is M −1; when no per-
mutations are needed, it turns out that M −1 can be obtained immediately
from the matrices Ek's, in fact, from their inverses, and no multiplications
are necessary.
Remark: Instead of looking for an invertible matrix M so that MA is
upper-triangular, we can look for an invertible matrix M so that MA is a
diagonal matrix. Only a simple change to Gaussian elimination is needed.
At every Stage k, after the pivot has been found and pivoting been per-
formed, if necessary, in addition to adding suitable multiples of the kth

212
Gaussian Elimination, LU, Cholesky, Echelon Form
row to the rows below row k in order to zero the entries in column k for
i = k+1, . . . , n, also add suitable multiples of the kth row to the rows above
row k in order to zero the entries in column k for i = 1, . . . , k −1. Such
steps are also achieved by multiplying on the left by elementary matrices
Ei,k;βi,k, except that i < k, so that these matrices are not lower-triangular
matrices. Nevertheless, at the end of the process, we ﬁnd that An = MA,
is a diagonal matrix.
This method is called the Gauss-Jordan factorization. Because it is
more expensive than Gaussian elimination, this method is not used much
in practice. However, Gauss-Jordan factorization can be used to compute
the inverse of a matrix A. Indeed, we ﬁnd the jth column of A−1 by solving
the system Ax(j) = ej (where ej is the jth canonical basis vector of Rn). By
applying Gauss-Jordan, we are led to a system of the form Djx(j) = Mjej,
where Dj is a diagonal matrix, and we can immediately compute x(j).
It remains to discuss the choice of the pivot, and also conditions that
guarantee that no permutations are needed during the Gaussian elimination
process. We begin by stating a necessary and suﬃcient condition for an
invertible matrix to have an LU-factorization (i.e., Gaussian elimination
does not require pivoting).
7.4
LU-Factorization
Deﬁnition
7.1. We say that an invertible matrix A has an LU-
factorization if it can be written as A = LU, where U is upper-triangular
invertible and L is lower-triangular, with Li i = 1 for i = 1, . . . , n.
A lower-triangular matrix with diagonal entries equal to 1 is called a
unit lower-triangular matrix. Given an n × n matrix A = (ai j), for any k
with 1 ≤k ≤n, let A(1 : k, 1 : k) denote the submatrix of A whose entries
are ai j, where 1 ≤i, j ≤k.1 For example, if A is the 5 × 5 matrix
A =






a11 a12 a13 a14 a15
a21 a22 a23 a24 a25
a31 a32 a33 a34 a35
a41 a42 a43 a44 a45
a51 a52 a53 a54 a55






,
1We are using Matlab's notation.

7.4. LU-Factorization
213
then
A(1 : 3, 1 : 3) =


a11 a12 a13
a21 a22 a23
a31 a32 a33

.
Proposition 7.1. Let A be an invertible n × n-matrix. Then A has an
LU-factorization A = LU iﬀevery matrix A(1 : k, 1 : k) is invertible for
k = 1, . . . , n. Furthermore, when A has an LU-factorization, we have
det(A(1 : k, 1 : k)) = π1π2 · · · πk,
k = 1, . . . , n,
where πk is the pivot obtained after k −1 elimination steps. Therefore, the
kth pivot is given by
πk =



a11 = det(A(1 : 1, 1 : 1))
if k = 1
det(A(1 : k, 1 : k))
det(A(1 : k −1, 1 : k −1))
if k = 2, . . . , n.
Proof. First assume that A = LU is an LU-factorization of A. We can
write
A =
A(1 : k, 1 : k) A2
A3
A4

=
L1 0
L3 L4
 U1 U2
0 U4

=
L1U1
L1U2
L3U1 L3U2 + L4U4

,
where L1, L4 are unit lower-triangular and U1, U4 are upper-triangular.
(Note, A(1 : k, 1 : k), L1, and U1 are k × k matrices; A2 and U2 are
k × (n −k) matrices; A3 and L3 are (n −k) × k matrices; A4, L4, and U4
are (n −k) × (n −k) matrices.) Thus,
A(1 : k, 1 : k) = L1U1,
and since U is invertible, U1 is also invertible (the determinant of U is the
product of the diagonal entries in U, which is the product of the diagonal
entries in U1 and U4). As L1 is invertible (since its diagonal entries are
equal to 1), we see that A(1 : k, 1 : k) is invertible for k = 1, . . . , n.
Conversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We
just need to show that Gaussian elimination does not need pivoting. We
prove by induction on k that the kth step does not need pivoting.
This holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 ̸= 0. Assume
that no pivoting was necessary for the ﬁrst k −1 steps (2 ≤k ≤n −1). In
this case, we have
Ek−1 · · · E2E1A = Ak,

214
Gaussian Elimination, LU, Cholesky, Echelon Form
where L = Ek−1 · · · E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 :
k) is upper-triangular, so that LA = Ak can be written as
L1 0
L3 L4
 A(1 : k, 1 : k) A2
A3
A4

=
U1 B2
0 B4

,
where L1 is unit lower-triangular and U1 is upper-triangular. (Once again
A(1 : k, 1 : k), L1, and U1 are k × k matrices; A2 and B2 are k × (n −k)
matrices; A3 and L3 are (n −k) × k matrices; A4, L4, and B4 are (n −k) ×
(n −k) matrices.) But then,
L1A(1 : k, 1 : k)) = U1,
where L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 :
k, 1 : k) is invertible, U1 is also invertible, which implies that (U1)kk ̸= 0,
since U1 is upper-triangular. Therefore, no pivoting is needed in Step k,
establishing the induction step. Since det(L1) = 1, we also have
det(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k))
= det(A(1 : k, 1 : k)),
and since U1 is upper-triangular and has the pivots π1, . . . , πk on its diag-
onal, we get
det(A(1 : k, 1 : k)) = π1π2 · · · πk,
k = 1, . . . , n,
as claimed.
Remark: The use of determinants in the ﬁrst part of the proof of Propo-
sition 7.1 can be avoided if we use the fact that a triangular matrix is
invertible iﬀall its diagonal entries are nonzero.
Corollary 7.1. (LU-Factorization) Let A be an invertible n × n-matrix.
If every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian
elimination requires no pivoting and yields an LU-factorization A = LU.
Proof. We proved in Proposition 7.1 that in this case Gaussian elimination
requires no pivoting. Then since every elementary matrix Ei,k;β is lower-
triangular (since we always arrange that the pivot πk occurs above the
rows that it operates on), since E−1
i,k;β = Ei,k;−β and the Eks are products
of Ei,k;βi,ks, from
En−1 · · · E2E1A = U,

7.4. LU-Factorization
215
where U is an upper-triangular matrix, we get
A = LU,
where L = E−1
1 E−1
2
· · · E−1
n−1 is a lower-triangular matrix. Furthermore, as
the diagonal entries of each Ei,k;β are 1, the diagonal entries of each Ek are
also 1.
Example 7.1. The reader should verify that




2 1 1 0
4 3 3 1
8 7 9 5
6 7 9 8



=




1 0 0 0
2 1 0 0
4 3 1 0
3 4 1 1








2 1 1 0
0 1 1 1
0 0 2 2
0 0 0 2




is an LU-factorization.
One of the main reasons why the existence of an LU-factorization for
a matrix A is interesting is that if we need to solve several linear systems
Ax = b corresponding to the same matrix A, we can do this cheaply by
solving the two triangular systems
Lw = b,
and
Ux = w.
There is a certain asymmetry in the LU-decomposition A = LU of an
invertible matrix A. Indeed, the diagonal entries of L are all 1, but this is
generally false for U. This asymmetry can be eliminated as follows: if
D = diag(u11, u22, . . . , unn)
is the diagonal matrix consisting of the diagonal entries in U (the pivots),
then we if let U ′ = D−1U, we can write
A = LDU ′,
where L is lower-triangular, U ′ is upper-triangular, all diagonal entries
of both L and U ′ are 1, and D is a diagonal matrix of pivots.
Such a
decomposition leads to the following deﬁnition.
Deﬁnition 7.2. We say that an invertible n × n matrix A has an LDU-
factorization if it can be written as A = LDU ′, where L is lower-triangular,
U ′ is upper-triangular, all diagonal entries of both L and U ′ are 1, and D
is a diagonal matrix.

216
Gaussian Elimination, LU, Cholesky, Echelon Form
We will see shortly than if A is real symmetric, then U ′ = L⊤.
As we will see a bit later, real symmetric positive deﬁnite matrices satisfy
the condition of Proposition 7.1. Therefore, linear systems involving real
symmetric positive deﬁnite matrices can be solved by Gaussian elimination
without pivoting. Actually, it is possible to do better: this is the Cholesky
factorization.
If a square invertible matrix A has an LU-factorization, then it is pos-
sible to ﬁnd L and U while performing Gaussian elimination. Recall that
at Step k, we pick a pivot πk = a(k)
ik ̸= 0 in the portion consisting of the
entries of index j ≥k of the k-th column of the matrix Ak obtained so far,
we swap rows i and k if necessary (the pivoting step), and then we zero the
entries of index j = k + 1, . . . , n in column k. Schematically, we have the
following steps:







× × × × ×
0
× × × ×
0
× × × ×
0 a(k)
ik × × ×
0
× × × ×







pivot
=⇒







× × × × ×
0 a(k)
ik × × ×
0
× × × ×
0
× × × ×
0
× × × ×







elim
=⇒






× × × × ×
0 × × × ×
0 0 × × ×
0 0 × × ×
0 0 × × ×






.
More precisely, after permuting row k and row i (the pivoting step), if the
entries in column k below row k are αk+1k, . . . , αnk, then we add −αjk/πk
times row k to row j; this process is illustrated below:












a(k)
kk
a(k)
k+1k
...
a(k)
ik...
a(k)
nk












pivot
=⇒












a(k)
ik
a(k)
k+1k
...
a(k)
kk...
a(k)
nk












=











πk
αk+1k
...
αik
...
αnk











elim
=⇒











πk
0
...
0
...
0











.
Then if we write ℓjk = αjk/πk for j = k + 1, . . . , n, the kth column of L is













0
...
0
1
ℓk+1k
...
ℓnk













.

7.4. LU-Factorization
217
Observe that the signs of the multipliers −αjk/πk have been ﬂipped. Thus,
we obtain the unit lower triangular matrix
L =







1
0
0 · · · 0
ℓ21
1
0 · · · 0
ℓ31 ℓ32
1 · · · 0
...
...
...
... 0
ℓn1 ℓn2 ℓn3 · · · 1







.
It is easy to see (and this is proven in Theorem 7.2) that the inverse of L
is obtained from L by ﬂipping the signs of the ℓij:
L−1 =







1
0
0
· · · 0
−ℓ21
1
0
· · · 0
−ℓ31 −ℓ32
1
· · · 0
...
...
...
... 0
−ℓn1 −ℓn2 −ℓn3 · · · 1







.
Furthermore, if the result of Gaussian elimination (without pivoting) is
U = En−1 · · · E1A, then
Ek =











1 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
1
0 · · · 0
0 · · · −ℓk+1k 1 · · · 0
...
...
...
... ... ...
0 · · ·
−ℓnk
0 · · · 1











and
E−1
k
=











1 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
1
0 · · · 0
0 · · · ℓk+1k 1 · · · 0
...
...
...
... ... ...
0 · · ·
ℓnk
0 · · · 1











,
so the kth column of Ek is the kth column of L−1.
Here is an example illustrating the method.
Example 7.2. Given
A = A1 =




1 1
1
0
1 −1 0
1
1 1 −1 0
1 −1 0 −1



,
we have the following sequence of steps: The ﬁrst pivot is π1 = 1 in row 1,
and we subtract row 1 from rows 2, 3, and 4. We get
A2 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 −2 −1 −1




L1 =




1 0 0 0
1 1 0 0
1 0 1 0
1 0 0 1



.

218
Gaussian Elimination, LU, Cholesky, Echelon Form
The next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and
add 0 times row 2 to row 3). We get
A3 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2




L2 =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1



.
The next pivot is π3 = −2 in row 3, and since the fourth entry in column
3 is already a zero, we add 0 times row 3 to row 4. We get
A4 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2




L3 =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1



.
The procedure is ﬁnished, and we have
L = L3 =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1




U = A4 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2



.
It is easy to check that indeed
LU =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1








1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2



=




1 1
1
0
1 −1 0
1
1 1 −1 0
1 −1 0 −1



= A.
We now show how to extend the above method to deal with pivoting
eﬃciently. This is the PA = LU factorization.
7.5
P A = LU Factorization
The following easy proposition shows that, in principle, A can be premul-
tiplied by some permutation matrix P, so that PA can be converted to
upper-triangular form without using any pivoting. Permutations are dis-
cussed in some detail in Section 6.1, but for now we just need this deﬁnition.
For the precise connection between the notion of permutation (as discussed
in Section 6.1) and permutation matrices, see Problem 7.16.
Deﬁnition 7.3. A permutation matrix is a square matrix that has a single
1 in every row and every column and zeros everywhere else.

7.5. PA = LU Factorization
219
It is shown in Section 6.1 that every permutation matrix is a product of
transposition matrices (the P(i, k)s), and that P is invertible with inverse
P ⊤.
Proposition 7.2. Let A be an invertible n × n-matrix. There is some per-
mutation matrix P so that (PA)(1 : k, 1 : k) is invertible for k = 1, . . . , n.
Proof. The case n = 1 is trivial, and so is the case n = 2 (we swap
the rows if necessary).
If n ≥3, we proceed by induction.
Since A is
invertible, its columns are linearly independent; in particular, its ﬁrst n −
1 columns are also linearly independent.
Delete the last column of A.
Since the remaining n −1 columns are linearly independent, there are also
n −1 linearly independent rows in the corresponding n × (n −1) matrix.
Thus, there is a permutation of these n rows so that the (n −1) × (n −1)
matrix consisting of the ﬁrst n −1 rows is invertible. But then there is
a corresponding permutation matrix P1, so that the ﬁrst n −1 rows and
columns of P1A form an invertible matrix A′.
Applying the induction
hypothesis to the (n −1) × (n −1) matrix A′, we see that there some
permutation matrix P2 (leaving the nth row ﬁxed), so that (P2P1A)(1 :
k, 1 : k) is invertible, for k = 1, . . . , n −1. Since A is invertible in the ﬁrst
place and P1 and P2 are invertible, P1P2A is also invertible, and we are
done.
Remark: One can also prove Proposition 7.2 using a clever reordering of
the Gaussian elimination steps suggested by Trefethen and Bau [Trefethen
and Bau III (1997)] (Lecture 21). Indeed, we know that if A is invertible,
then there are permutation matrices Pi and products of elementary matrices
Ei, so that
An = En−1Pn−1 · · · E2P2E1P1A,
where U = An is upper-triangular. For example, when n = 4, we have
E3P3E2P2E1P1A = U. We can deﬁne new matrices E′
1, E′
2, E′
3 which are
still products of elementary matrices so that we have
E′
3E′
2E′
1P3P2P1A = U.
Indeed, if we let E′
3 = E3, E′
2 = P3E2P −1
3
, and E′
1 = P3P2E1P −1
2
P −1
3
, we
easily verify that each E′
k is a product of elementary matrices and that
E′
3E′
2E′
1P3P2P1 = E3(P3E2P −1
3
)(P3P2E1P −1
2
P −1
3
)P3P2P1
= E3P3E2P2E1P1.

220
Gaussian Elimination, LU, Cholesky, Echelon Form
It can also be proven that E′
1, E′
2, E′
3 are lower triangular (see Theorem 7.2).
In general, we let
E′
k = Pn−1 · · · Pk+1EkP −1
k+1 · · · P −1
n−1,
and we have
E′
n−1 · · · E′
1Pn−1 · · · P1A = U,
where each E′
j is a lower triangular matrix (see Theorem 7.2).
It is remarkable that if pivoting steps are necessary during Gaussian
elimination, a very simple modiﬁcation of the algorithm for ﬁnding an LU-
factorization yields the matrices L, U, and P, such that PA = LU. To
describe this new method, since the diagonal entries of L are 1s, it is con-
venient to write
L = I + Λ.
Then in assembling the matrix Λ while performing Gaussian elimination
with pivoting, we make the same transposition on the rows of Λ (really
Λk−1) that we make on the rows of A (really Ak) during a pivoting step
involving row k and row i. We also assemble P by starting with the identity
matrix and applying to P the same row transpositions that we apply to A
and Λ. Here is an example illustrating this method.
Example 7.3. Given
A = A1 =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1



,
we have the following sequence of steps: We initialize Λ0 = 0 and P0 = I4.
The ﬁrst pivot is π1 = 1 in row 1, and we subtract row 1 from rows 2, 3,
and 4. We get
A2 =




1 1
1
0
0 0 −2 0
0 −2 −1 1
0 −2 −1 −1




Λ1 =




0 0 0 0
1 0 0 0
1 0 0 0
1 0 0 0




P1 =




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1



.
The next pivot is π2 = −2 in row 3, so we permute row 2 and 3; we also
apply this permutation to Λ and P:
A′
3 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 −2 −1 −1




Λ′
2 =




0 0 0 0
1 0 0 0
1 0 0 0
1 0 0 0




P2 =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.

7.5. PA = LU Factorization
221
Next we subtract row 2 from row 4 (and add 0 times row 2 to row 3). We
get
A3 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2




Λ2 =




0 0 0 0
1 0 0 0
1 0 0 0
1 1 0 0




P2 =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.
The next pivot is π3 = −2 in row 3, and since the fourth entry in column
3 is already a zero, we add 0 times row 3 to row 4. We get
A4 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2




Λ3 =




0 0 0 0
1 0 0 0
1 0 0 0
1 1 0 0




P3 =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.
The procedure is ﬁnished, and we have
L = Λ3 + I =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1




U = A4 =




1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2




P = P3 =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.
It is easy to check that indeed
LU =




1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1








1 1
1
0
0 −2 −1 1
0 0 −2 0
0 0
0 −2



=




1 1
1
0
1 −1 0
1
1 1 −1 0
1 −1 0 −1




and
PA =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1








1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1



=




1 1
1
0
1 −1 0
1
1 1 −1 0
1 −1 0 −1



.
Using the idea in the remark before the above example, we can prove the
theorem below which shows the correctness of the algorithm for computing
P, L and U using a simple adaptation of Gaussian elimination.

222
Gaussian Elimination, LU, Cholesky, Echelon Form
We are not aware of a detailed proof of Theorem 7.2 in the standard
texts. Although Golub and Van Loan [Golub and Van Loan (1996)] state
a version of this theorem as their Theorem 3.1.4, they say that "The proof
is a messy subscripting argument." Meyer [Meyer (2000)] also provides a
sketch of proof (see the end of Section 3.10). In view of this situation, we
oﬀer a complete proof. It does involve a lot of subscripts and superscripts,
but in our opinion, it contains some techniques that go far beyond symbol
manipulation.
Theorem 7.2. For every invertible n × n-matrix A, the following hold:
(1) There is some permutation matrix P, some upper-triangular matrix U,
and some unit lower-triangular matrix L, so that PA = LU (recall,
Li i = 1 for i = 1, . . . , n).
Furthermore, if P = I, then L and U
are unique and they are produced as a result of Gaussian elimination
without pivoting.
(2) If En−1 . . . E1A = U is the result of Gaussian elimination without piv-
oting, write as usual Ak = Ek−1 . . . E1A (with Ak = (a(k)
ij )), and let
ℓik = a(k)
ik /a(k)
kk , with 1 ≤k ≤n −1 and k + 1 ≤i ≤n. Then
L =







1
0
0 · · · 0
ℓ21
1
0 · · · 0
ℓ31 ℓ32
1 · · · 0
...
...
...
... 0
ℓn1 ℓn2 ℓn3 · · · 1







,
where the kth column of L is the kth column of E−1
k , for k = 1, . . . , n−1.
(3) If En−1Pn−1 · · · E1P1A = U is the result of Gaussian elimination with
some pivoting, write Ak = Ek−1Pk−1 · · · E1P1A, and deﬁne Ek
j , with
1 ≤j ≤n −1 and j ≤k ≤n −1, such that, for j = 1, . . . , n −2,
Ej
j = Ej
Ek
j = PkEk−1
j
Pk,
for k = j + 1, . . . , n −1,
and
En−1
n−1 = En−1.
Then,
Ek
j = PkPk−1 · · · Pj+1EjPj+1 · · · Pk−1Pk
U = En−1
n−1 · · · En−1
1
Pn−1 · · · P1A,

7.5. PA = LU Factorization
223
and if we set
P = Pn−1 · · · P1
L = (En−1
1
)−1 · · · (En−1
n−1)−1,
then
PA = LU.
(7.1)
Furthermore,
(Ek
j )−1 = I + Ek
j ,
1 ≤j ≤n −1, j ≤k ≤n −1,
where Ek
j is a lower triangular matrix of the form
Ek
j =











0 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
0
0 · · · 0
0 · · · ℓ(k)
j+1j 0 · · · 0
...
...
...
... ... ...
0 · · ·
ℓ(k)
nj
0 · · · 0











,
we have
Ek
j = I −Ek
j ,
and
Ek
j = PkEk−1
j
,
1 ≤j ≤n −2, j + 1 ≤k ≤n −1,
where Pk = I or else Pk = P(k, i) for some i such that k + 1 ≤
i ≤n; if Pk ̸= I, this means that (Ek
j )−1 is obtained from (Ek−1
j
)−1
by permuting the entries on rows i and k in column j. Because the
matrices (Ek
j )−1 are all lower triangular, the matrix L is also lower
triangular.
In order to ﬁnd L, deﬁne lower triangular n × n matrices Λk of the
form
Λk =


















0
0
0
0
0 · · · · · · 0
λ(k)
21
0
0
0
0
...
...
0
λ(k)
31
λ(k)
32
...
0
0
...
...
0
...
...
...
0
0
...
...
...
λ(k)
k+11 λ(k)
k+12 · · · λ(k)
k+1k 0 · · · · · · 0
λ(k)
k+21 λ(k)
k+22 · · · λ(k)
k+2k 0 ... · · · 0
...
...
...
...
...
...
... ...
λ(k)
n1
λ(k)
n2
· · ·
λ(k)
nk
0 · · · · · · 0



















224
Gaussian Elimination, LU, Cholesky, Echelon Form
to assemble the columns of L iteratively as follows: let
(−ℓ(k)
k+1k, . . . , −ℓ(k)
nk )
be the last n −k elements of the kth column of Ek, and deﬁne Λk
inductively by setting
Λ1 =






0 0 · · · 0
ℓ(1)
21 0 · · · 0
...
... ... ...
ℓ(1)
n1 0 · · · 0






,
then for k = 2, . . . , n −1, deﬁne
Λ′
k = PkΛk−1,
(7.2)
and Λk = (I + Λ′
k)E−1
k
−I, with
Λk =



















0
0
0
0
0
· · · · · · 0
λ
′(k−1)
21
0
0
0
0
...
...
0
λ
′(k−1)
31
λ
′(k−1)
32
...
0
0
...
...
0
...
...
...
0
0
...
...
...
λ
′(k−1)
k1
λ
′(k−1)
k2
· · ·
λ
′(k−1)
k (k−1)
0
· · · · · · 0
λ
′(k−1)
k+11
λ
′(k−1)
k+12
· · · λ
′(k−1)
k+1 (k−1) ℓ(k)
k+1k
... · · · 0
...
...
...
...
...
...
... ...
λ
′(k−1)
n1
λ
′(k−1)
n2
· · ·
λ
′(k−1)
n k−1
ℓ(k)
nk
· · · · · · 0



















,
with Pk = I or Pk = P(k, i) for some i > k.
This means that in
assembling L, row k and row i of Λk−1 need to be permuted when a
pivoting step permuting row k and row i of Ak is required. Then
I + Λk = (Ek
1 )−1 · · · (Ek
k)−1
Λk = Ek
1 + · · · + Ek
k ,
for k = 1, . . . , n −1, and therefore
L = I + Λn−1.
The proof of Theorem 7.2, which is very technical, is given in Section 7.6.
We emphasize again that Part (3) of Theorem 7.2 shows the remarkable
fact that in assembling the matrix L while performing Gaussian elimination

7.5. PA = LU Factorization
225
with pivoting, the only change to the algorithm is to make the same trans-
position on the rows of Λk−1 that we make on the rows of A (really Ak)
during a pivoting step involving row k and row i. We can also assemble
P by starting with the identity matrix and applying to P the same row
transpositions that we apply to A and Λ. Here is an example illustrating
this method.
Example 7.4. Consider the matrix
A =




1
2 −3 4
4
8
12 −8
2
3
2
1
−3 −1 1 −4



.
We set P0 = I4, and we can also set Λ0 = 0. The ﬁrst step is to permute
row 1 and row 2, using the pivot 4. We also apply this permutation to P0:
A′
1 =




4
8
12 −8
1
2 −3 4
2
3
2
1
−3 −1 1 −4




P1 =




0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1



.
Next we subtract 1/4 times row 1 from row 2, 1/2 times row 1 from row 3,
and add 3/4 times row 1 to row 4, and start assembling Λ:
A2 =




4 8
12 −8
0 0 −6
6
0 −1 −4
5
0 5
10 −10




Λ1 =




0
0 0 0
1/4 0 0 0
1/2 0 0 0
−3/4 0 0 0




P1 =




0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1



.
Next we permute row 2 and row 4, using the pivot 5. We also apply this
permutation to Λ and P:
A′
3 =




4 8
12 −8
0 5
10 −10
0 −1 −4
5
0 0 −6
6




Λ′
2 =




0
0 0 0
−3/4 0 0 0
1/2 0 0 0
1/4 0 0 0




P2 =




0 1 0 0
0 0 0 1
0 0 1 0
1 0 0 0



.
Next we add 1/5 times row 2 to row 3, and update Λ′
2:
A3 =




4 8 12 −8
0 5 10 −10
0 0 −2
3
0 0 −6
6




Λ2 =




0
0
0 0
−3/4
0
0 0
1/2 −1/5 0 0
1/4
0
0 0




P2 =




0 1 0 0
0 0 0 1
0 0 1 0
1 0 0 0



.

226
Gaussian Elimination, LU, Cholesky, Echelon Form
Next we permute row 3 and row 4, using the pivot −6. We also apply this
permutation to Λ and P:
A′
4 =




4 8 12 −8
0 5 10 −10
0 0 −6
6
0 0 −2
3




Λ′
3 =




0
0
0 0
−3/4
0
0 0
1/4
0
0 0
1/2 −1/5 0 0




P3 =




0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0



.
Finally we subtract 1/3 times row 3 from row 4, and update Λ′
3:
A4 =




4 8 12 −8
0 5 10 −10
0 0 −6
6
0 0 0
1




Λ3 =




0
0
0 0
−3/4
0
0 0
1/4
0
0 0
1/2 −1/5 1/3 0




P3 =




0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0



.
Consequently, adding the identity to Λ3, we obtain
L =




1
0
0 0
−3/4
1
0 0
1/4
0
1 0
1/2 −1/5 1/3 1



,
U =




4 8 12 −8
0 5 10 −10
0 0 −6
6
0 0 0
1



,
P =




0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0



.
We check that
PA =




0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0








1
2 −3 4
4
8
12 −8
2
3
2
1
−3 −1 1 −4



=




4
8
12 −8
−3 −1 1 −4
1
2 −3 4
2
3
2
1



,
and that
LU =




1
0
0 0
−3/4
1
0 0
1/4
0
1 0
1/2 −1/5 1/3 1








4 8 12 −8
0 5 10 −10
0 0 −6
6
0 0 0
1



=




4
8
12 −8
−3 −1 1 −4
1
2 −3 4
2
3
2
1



= PA.
Note that if one willing to overwrite the lower triangular part of the
evolving matrix A, one can store the evolving Λ there, since these entries
will eventually be zero anyway! There is also no need to save explicitly the
permutation matrix P. One could instead record the permutation steps in
an extra column (record the vector (π(1), . . . , π(n)) corresponding to the
permutation π applied to the rows). We let the reader write such a bold
and space-eﬃcient version of LU-decomposition!
Remark: In Matlab the function lu returns the matrices P, L, U involved
in the PA = LU factorization using the call [L, U, P] = lu(A).

7.6. Proof of Theorem 7.2 ⊛
227
As a corollary of Theorem 7.2(1), we can show the following result.
Proposition 7.3. If an invertible real symmetric matrix A has an LU-
decomposition, then A has a factorization of the form
A = LDL⊤,
where L is a lower-triangular matrix whose diagonal entries are equal to 1,
and where D consists of the pivots. Furthermore, such a decomposition is
unique.
Proof. If A has an LU-factorization, then it has an LDU factorization
A = LDU,
where L is lower-triangular, U is upper-triangular, and the diagonal entries
of both L and U are equal to 1. Since A is symmetric, we have
LDU = A = A⊤= U ⊤DL⊤,
with U ⊤lower-triangular and DL⊤upper-triangular. By the uniqueness
of LU-factorization (Part (1) of Theorem 7.2), we must have L = U ⊤(and
DU = DL⊤), thus U = L⊤, as claimed.
Remark: It
can
be
shown
that
Gaussian
elimination
plus
back-
substitution requires n3/3 + O(n2) additions, n3/3 + O(n2) multiplications
and n2/2 + O(n) divisions.
7.6
Proof of Theorem 7.2 ⊛
Proof. (1) The only part that has not been proven is the uniqueness part
(when P = I). Assume that A is invertible and that A = L1U1 = L2U2,
with L1, L2 unit lower-triangular and U1, U2 upper-triangular. Then we
have
L−1
2 L1 = U2U −1
1 .
However, it is obvious that L−1
2
is lower-triangular and that U −1
1
is upper-
triangular, and so L−1
2 L1 is lower-triangular and U2U −1
1
is upper-triangular.
Since the diagonal entries of L1 and L2 are 1, the above equality is only
possible if U2U −1
1
= I, that is, U1 = U2, and so L1 = L2.
(2) When P = I, we have L = E−1
1 E−1
2
· · · E−1
n−1, where Ek is the
product of n −k elementary matrices of the form Ei,k;−ℓi, where Ei,k;−ℓi

228
Gaussian Elimination, LU, Cholesky, Echelon Form
subtracts ℓi times row k from row i, with ℓik = a(k)
ik /a(k)
kk , 1 ≤k ≤n −1,
and k + 1 ≤i ≤n. Then it is immediately veriﬁed that
Ek =











1 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
1
0 · · · 0
0 · · · −ℓk+1k 1 · · · 0
...
...
...
... ... ...
0 · · ·
−ℓnk
0 · · · 1











,
and that
E−1
k
=











1 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
1
0 · · · 0
0 · · · ℓk+1k 1 · · · 0
...
...
...
... ... ...
0 · · ·
ℓnk
0 · · · 1











.
If we deﬁne Lk by
Lk =
















1
0
0
0
0
...
0
ℓ21
1
0
0
0
...
0
ℓ31
ℓ32
...
0
0
...
0
...
...
...
1
0
...
0
ℓk+11 ℓk+12 · · · ℓk+1k 1 · · · 0
...
...
...
...
0
...
0
ℓn1
ℓn2
· · ·
ℓnk
0 · · · 1
















for k = 1, . . . , n −1, we easily check that L1 = E−1
1 , and that
Lk = Lk−1E−1
k ,
2 ≤k ≤n −1,
because multiplication on the right by E−1
k
adds ℓi times column i to column
k (of the matrix Lk−1) with i > k, and column i of Lk−1 has only the
nonzero entry 1 as its ith element. Since
Lk = E−1
1
· · · E−1
k ,
1 ≤k ≤n −1,
we conclude that L = Ln−1, proving our claim about the shape of L.

7.6. Proof of Theorem 7.2 ⊛
229
(3)
Step 1. Prove (7.1).
First we prove by induction on k that
Ak+1 = Ek
k · · · Ek
1 Pk · · · P1A,
k = 1, . . . , n −2.
For k = 1, we have A2 = E1P1A = E1
1P1A, since E1
1 = E1, so our
assertion holds trivially.
Now if k ≥2,
Ak+1 = EkPkAk,
and by the induction hypothesis,
Ak = Ek−1
k−1 · · · Ek−1
2
Ek−1
1
Pk−1 · · · P1A.
Because Pk is either the identity or a transposition, P 2
k = I, so by inserting
occurrences of PkPk as indicated below we can write
Ak+1 = EkPkAk
= EkPkEk−1
k−1 · · · Ek−1
2
Ek−1
1
Pk−1 · · · P1A
= EkPkEk−1
k−1(PkPk) · · · (PkPk)Ek−1
2
(PkPk)Ek−1
1
(PkPk)Pk−1 · · · P1A
= Ek(PkEk−1
k−1Pk) · · · (PkEk−1
2
Pk)(PkEk−1
1
Pk)PkPk−1 · · · P1A.
Observe that Pk has been "moved" to the right of the elimination steps.
However, by deﬁnition,
Ek
j = PkEk−1
j
Pk,
j = 1, . . . , k −1
Ek
k = Ek,
so we get
Ak+1 = Ek
kEk
k−1 · · · Ek
2 Ek
1 Pk · · · P1A,
establishing the induction hypothesis. For k = n −2, we get
U = An−1 = En−1
n−1 · · · En−1
1
Pn−1 · · · P1A,
as claimed, and the factorization PA = LU with
P = Pn−1 · · · P1
L = (En−1
1
)−1 · · · (En−1
n−1)−1
is clear.
Step 2. Prove that the matrices (Ek
j )−1 are lower-triangular. To achieve
this, we prove that the matrices Ek
j are strictly lower triangular matrices of
a very special form.

230
Gaussian Elimination, LU, Cholesky, Echelon Form
Since for j = 1, . . . , n −2, we have Ej
j = Ej,
Ek
j = PkEk−1
j
Pk,
k = j + 1, . . . , n −1,
since En−1
n−1 = En−1 and P −1
k
= Pk, we get (Ej
j)−1 = E−1
j
for j = 1, . . .,
n −1, and for j = 1, . . . , n −2, we have
(Ek
j )−1 = Pk(Ek−1
j
)−1Pk,
k = j + 1, . . . , n −1.
Since
(Ek−1
j
)−1 = I + Ek−1
j
and Pk = P(k, i) is a transposition or Pk = I, so P 2
k = I, and we get
(Ek
j )−1 = Pk(Ek−1
j
)−1Pk = Pk(I + Ek−1
j
)Pk = P 2
k + Pk Ek−1
j
Pk
= I + Pk Ek−1
j
Pk.
Therefore, we have
(Ek
j )−1 = I + Pk Ek−1
j
Pk,
1 ≤j ≤n −2, j + 1 ≤k ≤n −1.
We prove for j = 1, . . . , n −1, that for k = j, . . . , n −1, each Ek
j is a lower
triangular matrix of the form
Ek
j =











0 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
0
0 · · · 0
0 · · · ℓ(k)
j+1j 0 · · · 0
...
...
...
... ... ...
0 · · ·
ℓ(k)
nj
0 · · · 0











,
and that
Ek
j = Pk Ek−1
j
,
1 ≤j ≤n −2, j + 1 ≤k ≤n −1,
with Pk = I or Pk = P(k, i) for some i such that k + 1 ≤i ≤n.
For each j (1 ≤j ≤n −1) we proceed by induction on k = j, . . . , n −1.
Since (Ej
j)−1 = E−1
j
and since E−1
j
is of the above form, the base case
holds.
For the induction step, we only need to consider the case where Pk =
P(k, i) is a transposition, since the case where Pk = I is trivial. We have
to ﬁgure out what Pk Ek−1
j
Pk = P(k, i) Ek−1
j
P(k, i) is. However, since
Ek−1
j
=











0 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
0
0 · · · 0
0 · · · ℓ(k−1)
j+1j 0 · · · 0
...
...
...
... ... ...
0 · · · ℓ(k−1)
nj
0 · · · 0











,

7.6. Proof of Theorem 7.2 ⊛
231
and because k + 1 ≤i ≤n and j ≤k −1, multiplying Ek−1
j
on the right by
P(k, i) will permute columns i and k, which are columns of zeros, so
P(k, i) Ek−1
j
P(k, i) = P(k, i) Ek−1
j
,
and thus,
(Ek
j )−1 = I + P(k, i) Ek−1
j
.
But since
(Ek
j )−1 = I + Ek
j ,
we deduce that
Ek
j = P(k, i) Ek−1
j
.
We also know that multiplying Ek−1
j
on the left by P(k, i) will permute
rows i and k, which shows that Ek
j has the desired form, as claimed. Since
all Ek
j are strictly lower triangular, all (Ek
j )−1 = I +Ek
j are lower triangular,
so the product
L = (En−1
1
)−1 · · · (En−1
n−1)−1
is also lower triangular.
Step 3. Express L as L = I + Λn−1, with Λn−1 = E1
1 + · · · + En−1
n−1.
From Step 1 of Part (3), we know that
L = (En−1
1
)−1 · · · (En−1
n−1)−1.
We prove by induction on k that
I + Λk = (Ek
1 )−1 · · · (Ek
k)−1
Λk = Ek
1 + · · · + Ek
k ,
for k = 1, . . . , n −1.
If k = 1, we have E1
1 = E1 and
E1 =






1
0 · · · 0
−ℓ(1)
21 1 · · · 0
...
... ... ...
−ℓ(1)
n1 0 · · · 1






.
We also get
(E−1
1 )−1 =






1 0 · · · 0
ℓ(1)
21 1 · · · 0
...
... ... ...
ℓ(1)
n1 0 · · · 1






= I + Λ1.

232
Gaussian Elimination, LU, Cholesky, Echelon Form
Since (E−1
1 )−1 = I + E1
1, we ﬁnd that we get Λ1 = E1
1, and the base step
holds.
Since (Ek
j )−1 = I + Ek
j with
Ek
j =











0 · · ·
0
0 · · · 0
... ...
...
...
...
...
0 · · ·
0
0 · · · 0
0 · · · ℓ(k)
j+1j 0 · · · 0
...
...
...
... ... ...
0 · · ·
ℓ(k)
nj
0 · · · 0











and Ek
i Ek
j = 0 if i < j, as in part (2) for the computation involving the
products of Lk's, we get
(Ek−1
1
)−1 · · · (Ek−1
k−1)−1 = I + Ek−1
1
+ · · · + Ek−1
k−1,
2 ≤k ≤n.
(7.3)
Similarly, from the fact that Ek−1
j
P(k, i) = Ek−1
j
if i ≥k + 1 and j ≤k −1
and since
(Ek
j )−1 = I + PkEk−1
j
,
1 ≤j ≤n −2, j + 1 ≤k ≤n −1,
we get
(Ek
1 )−1 · · · (Ek
k−1)−1 = I + Pk(Ek−1
1
+ · · · + Ek−1
k−1),
2 ≤k ≤n −1. (7.4)
By the induction hypothesis,
I + Λk−1 = (Ek−1
1
)−1 · · · (Ek−1
k−1)−1,
and from (7.3), we get
Λk−1 = Ek−1
1
+ · · · + Ek−1
k−1.
Using (7.4), we deduce that
(Ek
1 )−1 · · · (Ek
k−1)−1 = I + PkΛk−1.
Since Ek
k = Ek, we obtain
(Ek
1 )−1 · · · (Ek
k−1)−1(Ek
k)−1 = (I + PkΛk−1)E−1
k .
However, by deﬁnition
I + Λk = (I + PkΛk−1)E−1
k ,
which proves that
I + Λk = (Ek
1 )−1 · · · (Ek
k−1)−1(Ek
k)−1,
(7.5)
and ﬁnishes the induction step for the proof of this formula.
If we apply equation (7.3) again with k + 1 in place of k, we have
(Ek
1 )−1 · · · (Ek
k)−1 = I + Ek
1 + · · · + Ek
k ,
and together with (7.5), we obtain,
Λk = Ek
1 + · · · + Ek
k ,
also ﬁnishing the induction step for the proof of this formula. For k = n−1
in (7.5), we obtain the desired equation: L = I + Λn−1.

7.7. Dealing with RoundoﬀErrors; Pivoting Strategies
233
7.7
Dealing with RoundoﬀErrors; Pivoting Strategies
Let us now brieﬂy comment on the choice of a pivot. Although theoretically,
any pivot can be chosen, the possibility of roundoﬀerrors implies that it is
not a good idea to pick very small pivots. The following example illustrates
this point. Consider the linear system
10−4x + y = 1
x
+ y = 2.
Since 10−4 is nonzero, it can be taken as pivot, and we get
10−4x +
y
=
1
(1 −104)y = 2 −104.
Thus, the exact solution is
x =
104
104 −1,
y = 104 −2
104 −1.
However, if roundoﬀtakes place on the fourth digit, then 104 −1 = 9999
and 104 −2 = 9998 will be rounded oﬀboth to 9990, and then the solution
is x = 0 and y = 1, very far from the exact solution where x ≈1 and y ≈1.
The problem is that we picked a very small pivot. If instead we permute
the equations, the pivot is 1, and after elimination we get the system
x +
y
=
2
(1 −10−4)y = 1 −2 × 10−4.
This time, 1 −10−4 = 0.9999 and 1 −2 × 10−4 = 0.9998 are rounded oﬀto
0.999 and the solution is x = 1, y = 1, much closer to the exact solution.
To remedy this problem, one may use the strategy of partial pivoting.
This consists of choosing during Step k (1 ≤k ≤n −1) one of the entries
a(k)
i k such that
|a(k)
i k | = max
k≤p≤n |a(k)
p k|.
By maximizing the value of the pivot, we avoid dividing by undesirably
small pivots.
Remark: A matrix, A, is called strictly column diagonally dominant iﬀ
|aj j| >
n
X
i=1, i̸=j
|ai j|,
for j = 1, . . . , n

234
Gaussian Elimination, LU, Cholesky, Echelon Form
(resp. strictly row diagonally dominant iﬀ
|ai i| >
n
X
j=1, j̸=i
|ai j|,
for i = 1, . . . , n.)
For example, the matrix







7
2 1
1 4
1
0
... ... ...
0
1
4 1
1
7
2







of the curve interpolation problem discussed in Section 7.1 is strictly column
(and row) diagonally dominant.
It has been known for a long time (before 1900, say by Hadamard) that
if a matrix A is strictly column diagonally dominant (resp. strictly row
diagonally dominant), then it is invertible. It can also be shown that if
A is strictly column diagonally dominant, then Gaussian elimination with
partial pivoting does not actually require pivoting (see Problem 7.12).
Another strategy, called complete pivoting, consists in choosing some
entry a(k)
i j , where k ≤i, j ≤n, such that
|a(k)
i j | =
max
k≤p,q≤n |a(k)
p q |.
However, in this method, if the chosen pivot is not in column k, it is also
necessary to permute columns.
This is achieved by multiplying on the
right by a permutation matrix. However, complete pivoting tends to be
too expensive in practice, and partial pivoting is the method of choice.
A special case where the LU-factorization is particularly eﬃcient is the
case of tridiagonal matrices, which we now consider.
7.8
Gaussian Elimination of Tridiagonal Matrices
Consider the tridiagonal matrix
A =












b1 c1
a2 b2
c2
a3
b3
c3
...
...
...
an−2 bn−2 cn−2
an−1 bn−1 cn−1
an
bn












.

7.8. Gaussian Elimination of Tridiagonal Matrices
235
Deﬁne the sequence
δ0 = 1,
δ1 = b1,
δk = bkδk−1 −akck−1δk−2,
2 ≤k ≤n.
Proposition 7.4. If A is the tridiagonal matrix above, then δk = det(A(1 :
k, 1 : k)) for k = 1, . . . , n.
Proof. By expanding det(A(1 : k, 1 : k)) with respect to its last row, the
proposition follows by induction on k.
Theorem 7.3. If A is the tridiagonal matrix above and δk ̸= 0 for k =
1, . . . , n, then A has the following LU-factorization:
A =
















1
a2
δ0
δ1
1
a3
δ1
δ2
1
...
...
an−1
δn−3
δn−2
1
an
δn−2
δn−1
1

































δ1
δ0
c1
δ2
δ1
c2
δ3
δ2
c3
...
...
δn−1
δn−2
cn−1
δn
δn−1

















.
Proof. Since δk = det(A(1 : k, 1 : k)) ̸= 0 for k = 1, . . . , n, by Theorem 7.2
(and Proposition 7.1), we know that A has a unique LU-factorization.
Therefore, it suﬃces to check that the proposed factorization works. We
easily check that
(LU)k k+1 = ck,
1 ≤k ≤n −1
(LU)k k−1 = ak,
2 ≤k ≤n
(LU)k l = 0,
|k −l| ≥2
(LU)1 1 = δ1
δ0
= b1
(LU)k k = akck−1δk−2 + δk
δk−1
= bk,
2 ≤k ≤n,
since δk = bkδk−1 −akck−1δk−2.
It follows that there is a simple method to solve a linear system Ax =
d where A is tridiagonal (and δk ̸= 0 for k = 1, . . . , n).
For this, it is

236
Gaussian Elimination, LU, Cholesky, Echelon Form
convenient to "squeeze" the diagonal matrix ∆deﬁned such that ∆k k =
δk/δk−1 into the factorization so that A = (L∆)(∆−1U), and if we let
z1 = c1
b1
,
zk = ck
δk−1
δk
,
2 ≤k ≤n −1,
zn =
δn
δn−1
= bn −anzn−1,
A = (L∆)(∆−1U) is written as
A =














c1
z1
a2
c2
z2
a3
c3
z3
...
...
an−1
cn−1
zn−1
an
zn







































1 z1
1 z2
1 z3
...
...
1 zn−2
1
zn−1
1

























.
As a consequence, the system Ax = d can be solved by constructing three
sequences: First, the sequence
z1 = c1
b1
,
zk =
ck
bk −akzk−1
,
k = 2, . . . , n −1,
zn = bn −anzn−1,
corresponding to the recurrence δk = bkδk−1 −akck−1δk−2 and obtained by
dividing both sides of this equation by δk−1, next
w1 = d1
b1
,
wk = dk −akwk−1
bk −akzk−1
,
k = 2, . . . , n,
corresponding to solving the system L∆w = d, and ﬁnally
xn = wn,
xk = wk −zkxk+1,
k = n −1, n −2, . . . , 1,
corresponding to solving the system ∆−1Ux = w.
Remark: It can be veriﬁed that this requires 3(n −1) additions, 3(n −1)
multiplications, and 2n divisions, a total of 8n−6 operations, which is much
less that the O(2n3/3) required by Gaussian elimination in general.
We now consider the special case of symmetric positive deﬁnite matrices
(SPD matrices).

7.9. SPD Matrices and the Cholesky Decomposition
237
7.9
SPD Matrices and the Cholesky Decomposition
Recall that an n × n real symmetric matrix A is positive deﬁnite iﬀ
x⊤Ax > 0
for all x ∈Rn with x ̸= 0.
Equivalently, A is symmetric positive deﬁnite iﬀall its eigenvalues are
strictly positive. The following facts about a symmetric positive deﬁnite
matrix A are easily established (some left as an exercise):
(1) The matrix A is invertible. (Indeed, if Ax = 0, then x⊤Ax = 0, which
implies x = 0.)
(2) We have ai i > 0 for i = 1, . . . , n. (Just observe that for x = ei, the ith
canonical basis vector of Rn, we have e⊤
i Aei = ai i > 0.)
(3) For every n × n real invertible matrix Z, the matrix Z⊤AZ is real
symmetric positive deﬁnite iﬀA is real symmetric positive deﬁnite.
(4) The set of n × n real symmetric positive deﬁnite matrices is convex.
This means that if A and B are two n × n symmetric positive deﬁnite
matrices, then for any λ ∈R such that 0 ≤λ ≤1, the matrix (1 −
λ)A+λB is also symmetric positive deﬁnite. Clearly since A and B are
symmetric, (1 −λ)A + λB is also symmetric. For any nonzero x ∈Rn,
we have x⊤Ax > 0 and x⊤Bx > 0, so
x⊤((1 −λ)A + λB)x = (1 −λ)x⊤Ax + λx⊤Bx > 0,
because 0 ≤λ ≤1, so 1 −λ ≥0 and λ ≥0, and 1 −λ and λ can't be
zero simultaneously.
(5) The set of n × n real symmetric positive deﬁnite matrices is a cone.
This means that if A is symmetric positive deﬁnite and if λ > 0 is any
real, then λA is symmetric positive deﬁnite. Clearly λA is symmetric,
and for nonzero x ∈Rn, we have x⊤Ax > 0, and since λ > 0, we have
x⊤λAx = λx⊤Ax > 0.
Remark: Given a complex m × n matrix A, we deﬁne the matrix A as
the m × n matrix A = (aij).
Then we deﬁne A∗as the n × m matrix
A∗= (A)⊤= (A⊤). The n × n complex matrix A is Hermitian if A∗= A.
This is the complex analog of the notion of a real symmetric matrix. A
Hermitian matrix A is positive deﬁnite if
z∗Az > 0
for all z ∈Cn with z ̸= 0.
It is easily veriﬁed that Properties (1)-(5) hold for Hermitian positive def-
inite matrices; replace ⊤by ∗.

238
Gaussian Elimination, LU, Cholesky, Echelon Form
It is instructive to characterize when a 2 × 2 real symmetric matrix A
is positive deﬁnite. Write
A =
a c
c b

.
Then we have
 x y
 a c
c b
 x
y

= ax2 + 2cxy + by2.
If the above expression is strictly positive for all nonzero vectors
 x
y

, then
for x = 1, y = 0 we get a > 0 and for x = 0, y = 1 we get b > 0. Then we
can write
ax2 + 2cxy + by2 =
√ax +
c
√ay
2
+ by2 −c2
a y2
=
√ax +
c
√ay
2
+ 1
a
 ab −c2
y2.
(7.6)
Since a > 0, if ab −c2 ≤0, then we can choose y > 0 so that the second
term is negative or zero, and we can set x = −(c/a)y to make the ﬁrst term
zero, in which case ax2 + 2cxy + by2 ≤0, so we must have ab −c2 > 0.
Conversely, if a > 0, b > 0 and ab > c2, then for any (x, y) ̸= (0, 0), if
y = 0, then x ̸= 0 and the ﬁrst term of (7.6) is positive, and if y ̸= 0, then
the second term of (7.6) is positive. Therefore, the symmetric matrix A is
positive deﬁnite iﬀ
a > 0, b > 0, ab > c2.
(7.7)
Note that ab −c2 = det(A), so the third condition says that det(A) > 0.
Observe that the condition b > 0 is redundant, since if a > 0 and
ab > c2, then we must have b > 0 (and similarly b > 0 and ab > c2 implies
that a > 0).
We can try to visualize the space of 2×2 real symmetric positive deﬁnite
matrices in R3, by viewing (a, b, c) as the coordinates along the x, y, z axes.
Then the locus determined by the strict inequalities in (7.7) corresponds to
the region on the side of the cone of equation xy = z2 that does not contain
the origin and for which x > 0 and y > 0. For z = δ ﬁxed, the equation
xy = δ2 deﬁne a hyperbola in the plane z = δ.
The cone of equation
xy = z2 consists of the lines through the origin that touch the hyperbola
xy = 1 in the plane z = 1. We only consider the branch of this hyperbola
for which x > 0 and y > 0. See Figure 7.6.

7.9. SPD Matrices and the Cholesky Decomposition
239
xy = 1
Fig. 7.6
Two views of the surface xy = z2 in R3. The intersection of the surface with a
constant z plane results in a hyperbola. The region associated with the 2 × 2 symmetric
positive deﬁnite matrices lies in "front" of the green side.
It is not hard to show that the inverse of a real symmetric positive
deﬁnite matrix is also real symmetric positive deﬁnite, but the product of
two real symmetric positive deﬁnite matrices may not be symmetric positive
deﬁnite, as the following example shows:
1 1
1 2
  1/
√
2 −1
√
2
−1/
√
2 3/
√
2

=

0
2/
√
2
−1/
√
2 5/
√
2

.
According to the above criterion, the two matrices on the left-hand side are
real symmetric positive deﬁnite, but the matrix on the right-hand side is
not even symmetric, and

−6 1
 
0
2/
√
2
−1/
√
2 5/
√
2
 −6
1

=

−6 1
  2/
√
2
11/
√
2

= −1/
√
5,

240
Gaussian Elimination, LU, Cholesky, Echelon Form
even though its eigenvalues are both real and positive.
Next we show that a real symmetric positive deﬁnite matrix has a special
LU-factorization of the form A = BB⊤, where B is a lower-triangular
matrix whose diagonal elements are strictly positive. This is the Cholesky
factorization.
First we note that a symmetric positive deﬁnite matrix satisﬁes the
condition of Proposition 7.1.
Proposition 7.5. If A is a real symmetric positive deﬁnite matrix, then
A(1 : k, 1 : k) is symmetric positive deﬁnite and thus invertible for k =
1, . . . , n.
Proof. Since A is symmetric, each A(1 : k, 1 : k) is also symmetric. If
w ∈Rk, with 1 ≤k ≤n, we let x ∈Rn be the vector with xi = wi for
i = 1, . . . , k and xi = 0 for i = k + 1, . . . , n. Now since A is symmetric
positive deﬁnite, we have x⊤Ax > 0 for all x ∈Rn with x ̸= 0. This holds
in particular for all vectors x obtained from nonzero vectors w ∈Rk as
deﬁned earlier, and clearly
x⊤Ax = w⊤A(1 : k, 1 : k) w,
which implies that A(1 : k, 1 : k) is positive deﬁnite. Thus, by Fact 1 above,
A(1 : k, 1 : k) is also invertible.
Proposition 7.5 also holds for a complex Hermitian positive deﬁnite
matrix. Proposition 7.5 can be strengthened as follows: A real symmetric
(or complex Hermitian) matrix A is positive deﬁnite iﬀdet(A(1 : k, 1 :
k)) > 0 for k = 1, . . . , n.
The above fact is known as Sylvester's criterion. We will prove it after
establishing the Cholesky factorization.
Let A be an n × n real symmetric positive deﬁnite matrix and write
A =
a1 1 W ⊤
W
C

,
where C is an (n −1) × (n −1) symmetric matrix and W is an (n −1) ×
1 matrix.
Since A is symmetric positive deﬁnite, a1 1 > 0, and we can
compute α = √a1 1. The trick is that we can factor A uniquely as
A =
a1 1 W ⊤
W
C

=
 α
0
W/α I
 1
0
0 C −WW ⊤/a1 1
 α W ⊤/α
0
I

,
i.e., as A = B1A1B⊤
1 , where B1 is lower-triangular with positive diagonal
entries. Thus, B1 is invertible, and by Fact (3) above, A1 is also symmetric
positive deﬁnite.

7.9. SPD Matrices and the Cholesky Decomposition
241
Remark: The matrix C −WW ⊤/a1 1 is known as the Schur complement
of the matrix (a11).
Theorem 7.4. (Cholesky factorization) Let A be a real symmetric positive
deﬁnite matrix. Then there is some real lower-triangular matrix B so that
A = BB⊤. Furthermore, B can be chosen so that its diagonal elements are
strictly positive, in which case B is unique.
Proof. We proceed by induction on the dimension n of A. For n = 1, we
must have a1 1 > 0, and if we let α = √a1 1 and B = (α), the theorem holds
trivially. If n ≥2, as we explained above, again we must have a1 1 > 0, and
we can write
A=
a1 1 W ⊤
W
C

=
 α
0
W/α I
 1
0
0 C −WW ⊤/a1 1
 α W ⊤/α
0
I

=B1A1B⊤
1 ,
where α = √a1 1, the matrix B1 is invertible and
A1 =
1
0
0 C −WW ⊤/a1 1

is symmetric positive deﬁnite. However, this implies that C −WW ⊤/a1 1
is also symmetric positive deﬁnite (consider x⊤A1x for every x ∈Rn with
x ̸= 0 and x1 = 0).
Thus, we can apply the induction hypothesis to
C −WW ⊤/a1 1 (which is an (n−1)×(n−1) matrix), and we ﬁnd a unique
lower-triangular matrix L with positive diagonal entries so that
C −WW ⊤/a1 1 = LL⊤.
But then we get
A =
 α
0
W/α I
 1
0
0 C −WW ⊤/a1 1
 α W ⊤/α
0
I

=
 α
0
W/α I
 1
0
0 LL⊤
 α W ⊤/α
0
I

=
 α
0
W/α I
 1 0
0 L
 1 0
0 L⊤
 α W ⊤/α
0
I

=
 α
0
W/α L
 α W ⊤/α
0
L⊤

.
Therefore, if we let
B =
 α
0
W/α L

,
we have a unique lower-triangular matrix with positive diagonal entries and
A = BB⊤.

242
Gaussian Elimination, LU, Cholesky, Echelon Form
Remark: The uniqueness of the Cholesky decomposition can also be es-
tablished using the uniqueness of an LU-decomposition. Indeed, if A =
B1B⊤
1 = B2B⊤
2 where B1 and B2 are lower triangular with positive diago-
nal entries, if we let ∆1 (resp. ∆2) be the diagonal matrix consisting of the
diagonal entries of B1 (resp. B2) so that (∆k)ii = (Bk)ii for k = 1, 2, then
we have two LU-decompositions
A = (B1∆−1
1 )(∆1B⊤
1 ) = (B2∆−1
2 )(∆2B⊤
2 )
with B1∆−1
1 , B2∆−1
2
unit lower triangular, and ∆1B⊤
1 , ∆2B⊤
2 upper trian-
gular. By uniqueness of LU-factorization (Theorem 7.2(1)), we have
B1∆−1
1
= B2∆−1
2 ,
∆1B⊤
1 = ∆2B⊤
2 ,
and the second equation yields
B1∆1 = B2∆2.
(7.8)
The diagonal entries of B1∆1 are (B1)2
ii and similarly the diagonal entries
of B2∆2 are (B2)2
ii, so the above equation implies that
(B1)2
ii = (B2)2
ii,
i = 1, . . . , n.
Since the diagonal entries of both B1 and B2 are assumed to be positive,
we must have
(B1)ii = (B2)ii,
i = 1, . . . , n;
that is, ∆1 = ∆2, and since both are invertible, we conclude from (7.8)
that B1 = B2.
Theorem 7.4 also holds for complex Hermitian positive deﬁnite matrices.
In this case, we have A = BB∗for some unique lower triangular matrix B
with positive diagonal entries.
The proof of Theorem 7.4 immediately yields an algorithm to compute
B from A by solving for a lower triangular matrix B such that A = BB⊤
(where both A and B are real matrices). For j = 1, . . . , n,
bj j =
 
aj j −
j−1
X
k=1
b2
j k
!1/2
,
and for i = j + 1, . . . , n (and j = 1, . . . , n −1)
bi j =
 
ai j −
j−1
X
k=1
bi kbj k
!
/bj j.

7.9. SPD Matrices and the Cholesky Decomposition
243
The above formulae are used to compute the jth column of B from top-
down, using the ﬁrst j −1 columns of B previously computed, and the
matrix A. In the case of n = 3, A = BB⊤yields


a11 a12 a31
a21 a22 a32
a31 a32 a33

=


b11 0
0
b21 b22 0
b31 b32 b33




b11 b21 b31
0 b22 b32
0
0 b33


=


b2
11
b11b21
b11b31
b11b21
b2
21 + b2
22
b21b31 + b22b32
b11b31 b21b31 + b22b32 b2
31 + b2
32 + b2
33

.
We work down the ﬁrst column of A, compare entries, and discover that
a11 = b2
11
b11 = √a11
a21 = b11b21
b21 = a21
b11
a31 = b11b31
b31 = a31
b11
.
Next we work down the second column of A using previously calculated
expressions for b21 and b31 to ﬁnd that
a22 = b2
21 + b2
22
b22 =
 a22 −b2
21
 1
2
a32 = b21b31 + b22b32
b32 = a32 −b21b31
b22
.
Finally, we use the third column of A and the previously calculated
expressions for b31 and b32 to determine b33 as
a33 = b2
31 + b2
32 + b2
33
b33 =
 a33 −b2
31 −b2
32
 1
2 .
For another example, if
A =









1 1 1 1 1 1
1 2 2 2 2 2
1 2 3 3 3 3
1 2 3 4 4 4
1 2 3 4 5 5
1 2 3 4 5 6









,
we ﬁnd that
B =









1 0 0 0 0 0
1 1 0 0 0 0
1 1 1 0 0 0
1 1 1 1 0 0
1 1 1 1 1 0
1 1 1 1 1 1









.

244
Gaussian Elimination, LU, Cholesky, Echelon Form
We leave it as an exercise to ﬁnd similar formulae (involving conjuga-
tion) to factor a complex Hermitian positive deﬁnite matrix A as A = BB∗.
The following Matlab program implements the Cholesky factorization.
function B = Cholesky(A)
n = size(A,1);
B = zeros(n,n);
for j = 1:n-1;
if j == 1
B(1,1) = sqrt(A(1,1));
for i = 2:n
B(i,1) = A(i,1)/B(1,1);
end
else
B(j,j) = sqrt(A(j,j) - B(j,1:j-1)*B(j,1:j-1)');
for i = j+1:n
B(i,j) = (A(i,j) - B(i,1:j-1)*B(j,1:j-1)')/B(j,j);
end
end
end
B(n,n) = sqrt(A(n,n) - B(n,1:n-1)*B(n,1:n-1)');
end
If we run the above algorithm on the following matrix
A =






4 1 0 0 0
1 4 1 0 0
0 1 4 1 0
0 0 1 4 1
0 0 0 1 4






,
we obtain
B =






2.0000
0
0
0
0
0.5000 1.9365
0
0
0
0
0.5164 1.9322
0
0
0
0
0.5175 1.9319
0
0
0
0
0.5176 1.9319






.
The Cholesky factorization can be used to solve linear systems Ax = b
where A is symmetric positive deﬁnite: Solve the two systems Bw = b and
B⊤x = w.

7.9. SPD Matrices and the Cholesky Decomposition
245
Remark: It can be shown that this methods requires n3/6 + O(n2) ad-
ditions, n3/6 + O(n2) multiplications, n2/2 + O(n) divisions, and O(n)
square root extractions. Thus, the Cholesky method requires half of the
number of operations required by Gaussian elimination (since Gaussian
elimination requires n3/3 + O(n2) additions, n3/3 + O(n2) multiplications,
and n2/2 + O(n) divisions). It also requires half of the space (only B is
needed, as opposed to both L and U). Furthermore, it can be shown that
Cholesky's method is numerically stable (see Trefethen and Bau [Trefethen
and Bau III (1997)], Lecture 23). In Matlab the function chol returns the
lower-triangular matrix B such that A = BB⊤using the call B = chol(A,
'lower').
Remark: If A = BB⊤, where B is any invertible matrix, then A is sym-
metric positive deﬁnite.
Proof. Obviously, BB⊤is symmetric, and since B is invertible, B⊤is
invertible, and from
x⊤Ax = x⊤BB⊤x = (B⊤x)⊤B⊤x,
it is clear that x⊤Ax > 0 if x ̸= 0.
We now give three more criteria for a symmetric matrix to be positive
deﬁnite.
Proposition 7.6. Let A be any n×n real symmetric matrix. The following
conditions are equivalent:
(a) A is positive deﬁnite.
(b) All principal minors of A are positive; that is: det(A(1 : k, 1 : k)) > 0
for k = 1, . . . , n (Sylvester's criterion).
(c) A has an LU-factorization and all pivots are positive.
(d) A has an LDL⊤-factorization and all pivots in D are positive.
Proof. By Proposition 7.5, if A is symmetric positive deﬁnite, then each
matrix A(1 : k, 1 : k) is symmetric positive deﬁnite for k = 1, . . . , n. By the
Cholsesky decomposition, A(1 : k, 1 : k) = Q⊤Q for some invertible matrix
Q, so det(A(1 : k, 1 : k)) = det(Q)2 > 0. This shows that (a) implies (b).
If det(A(1 : k, 1 : k)) > 0 for k = 1, . . . , n, then each A(1 : k, 1 : k) is
invertible. By Proposition 7.1, the matrix A has an LU-factorization, and

246
Gaussian Elimination, LU, Cholesky, Echelon Form
since the pivots πk are given by
πk =



a11 = det(A(1 : 1, 1 : 1))
if k = 1
det(A(1 : k, 1 : k))
det(A(1 : k −1, 1 : k −1))
if k = 2, . . . , n,
we see that πk > 0 for k = 1, . . . , n. Thus (b) implies (c).
Assume A has an LU-factorization and that the pivots are all positive.
Since A is symmetric, this implies that A has a factorization of the form
A = LDL⊤,
with L lower-triangular with 1s on its diagonal, and where D is a diagonal
matrix with positive entries on the diagonal (the pivots). This shows that
(c) implies (d).
Given a factorization A = LDL⊤with all pivots in D positive, if we
form the diagonal matrix
√
D = diag(√π1, . . . , √πn)
and if we let B = L
√
D, then we have
A = BB⊤,
with B lower-triangular and invertible.
By the remark before Proposi-
tion 7.6, A is positive deﬁnite. Hence, (d) implies (a).
Criterion (c) yields a simple computational test to check whether a
symmetric matrix is positive deﬁnite. There is one more criterion for a
symmetric matrix to be positive deﬁnite: its eigenvalues must be positive.
We will have to learn about the spectral theorem for symmetric matrices
to establish this criterion.
Proposition 7.6 also holds for complex Hermitian positive deﬁnite ma-
trices, where in (d), the factorization LDL⊤is replaced by LDL∗.
For more on the stability analysis and eﬃcient implementation methods
of Gaussian elimination, LU-factoring and Cholesky factoring, see Demmel
[Demmel (1997)], Trefethen and Bau [Trefethen and Bau III (1997)], Cia-
rlet [Ciarlet (1989)], Golub and Van Loan [Golub and Van Loan (1996)],
Meyer [Meyer (2000)], Strang [Strang (1986, 1988)], and Kincaid and Ch-
eney [Kincaid and Cheney (1996)].

7.10. Reduced Row Echelon Form (RREF)
247
7.10
Reduced Row Echelon Form (RREF)
Gaussian elimination described in Section 7.2 can also be applied to rect-
angular matrices. This yields a method for determining whether a system
Ax = b is solvable and a description of all the solutions when the system is
solvable, for any rectangular m × n matrix A.
It turns out that the discussion is simpler if we rescale all pivots to be
1, and for this we need a third kind of elementary matrix. For any λ ̸= 0,
let Ei,λ be the n × n diagonal matrix
Ei,λ =













1
...
1
λ
1
...
1













,
with (Ei,λ)ii = λ (1 ≤i ≤n). Note that Ei,λ is also given by
Ei,λ = I + (λ −1)ei i,
and that Ei,λ is invertible with
E−1
i,λ = Ei,λ−1.
Now after k −1 elimination steps, if the bottom portion
(a(k)
kk , a(k)
k+1k, . . . , a(k)
mk)
of the kth column of the current matrix Ak is nonzero so that a pivot
πk can be chosen, after a permutation of rows if necessary, we also divide
row k by πk to obtain the pivot 1, and not only do we zero all the entries
i = k+1, . . . , m in column k, but also all the entries i = 1, . . . , k−1, so that
the only nonzero entry in column k is a 1 in row k. These row operations
are achieved by multiplication on the left by elementary matrices.
If a(k)
kk = a(k)
k+1k = · · · = a(k)
mk = 0, we move on to column k + 1.
When the kth column contains a pivot, the kth stage of the procedure for
converting a matrix to rref consists of the following three steps illustrated
below:

248
Gaussian Elimination, LU, Cholesky, Echelon Form









1 × 0 × × × ×
0 0 1 × × × ×
0 0 0 × × × ×
0 0 0 × × × ×
0 0 0 a(k)
ik × × ×
0 0 0 × × × ×









pivot
=⇒









1 × 0 × × × ×
0 0 1 × × × ×
0 0 0 a(k)
ik × × ×
0 0 0 × × × ×
0 0 0 × × × ×
0 0 0 × × × ×









rescale
=⇒









1 × 0 × × × ×
0 0 1 × × × ×
0 0 0 1 × × ×
0 0 0 × × × ×
0 0 0 × × × ×
0 0 0 × × × ×









elim
=⇒









1 × 0 0 × × ×
0 0 1 0 × × ×
0 0 0 1 × × ×
0 0 0 0 × × ×
0 0 0 0 × × ×
0 0 0 0 × × ×









.
If the kth column does not contain a pivot, we simply move on to the next
column.
The result is that after performing such elimination steps, we obtain a
matrix that has a special shape known as a reduced row echelon matrix, for
short rref.
Here is an example illustrating this process: Starting from the matrix
A1 =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12

,
we perform the following steps
A1 −→A2 =


1 0 2 1 5
0 1 3 1 2
0 2 6 3 7

,
by subtracting row 1 from row 2 and row 3;
A2 −→


1 0 2 1 5
0 2 6 3 7
0 1 3 1 2

−→


1 0 2 1
5
0 1 3 3/2 7/2
0 1 3 1
2

−→A3 =


1 0 2
1
5
0 1 3 3/2
7/2
0 0 0 −1/2 −3/2

,
after choosing the pivot 2 and permuting row 2 and row 3, dividing row 2
by 2, and subtracting row 2 from row 3;
A3 −→


1 0 2
1
5
0 1 3 3/2 7/2
0 0 0
1
3

−→A4 =


1 0 2 0 2
0 1 3 0 −1
0 0 0 1 3

,

7.10. Reduced Row Echelon Form (RREF)
249
after dividing row 3 by −1/2, subtracting row 3 from row 1, and subtracting
(3/2) × row 3 from row 2.
It is clear that columns 1, 2 and 4 are linearly independent, that column
3 is a linear combination of columns 1 and 2, and that column 5 is a linear
combination of columns 1, 2, 4.
In general, the sequence of steps leading to a reduced echelon matrix is
not unique. For example, we could have chosen 1 instead of 2 as the second
pivot in matrix A2. Nevertheless, the reduced row echelon matrix obtained
from any given matrix is unique; that is, it does not depend on the the
sequence of steps that are followed during the reduction process. This fact
is not so easy to prove rigorously, but we will do it later.
If we want to solve a linear system of equations of the form Ax = b, we
apply elementary row operations to both the matrix A and the right-hand
side b. To do this conveniently, we form the augmented matrix (A, b), which
is the m × (n + 1) matrix obtained by adding b as an extra column to the
matrix A. For example if
A =


1 0 2 1
1 1 5 2
1 2 8 4


and
b =


5
7
12

,
then the augmented matrix is
(A, b) =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12

.
Now for any matrix M, since
M(A, b) = (MA, Mb),
performing elementary row operations on (A, b) is equivalent to simultane-
ously performing operations on both A and b. For example, consider the
system
x1
+ 2x3 + x4 = 5
x1 + x2 + 5x3 + 2x4 = 7
x1 + 2x2 + 8x3 + 4x4 = 12.
Its augmented matrix is the matrix
(A, b) =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12



250
Gaussian Elimination, LU, Cholesky, Echelon Form
considered above, so the reduction steps applied to this matrix yield the
system
x1
+ 2x3
= 2
x2 + 3x3
= −1
x4 = 3.
This reduced system has the same set of solutions as the original, and
obviously x3 can be chosen arbitrarily. Therefore, our system has inﬁnitely
many solutions given by
x1 = 2 −2x3,
x2 = −1 −3x3,
x4 = 3,
where x3 is arbitrary.
The following proposition shows that the set of solutions of a system
Ax = b is preserved by any sequence of row operations.
Proposition 7.7. Given any m × n matrix A and any vector b ∈Rm, for
any sequence of elementary row operations E1, . . . , Ek, if P = Ek · · · E1
and (A′, b′) = P(A, b), then the solutions of Ax = b are the same as the
solutions of A′x = b′.
Proof. Since each elementary row operation Ei is invertible, so is P, and
since (A′, b′) = P(A, b), then A′ = PA and b′ = Pb. If x is a solution of the
original system Ax = b, then multiplying both sides by P we get PAx = Pb;
that is, A′x = b′, so x is a solution of the new system. Conversely, assume
that x is a solution of the new system, that is A′x = b′. Then because
A′ = PA, b′ = Pb, and P is invertible, we get
Ax = P −1A′x = P −1b′ = b,
so x is a solution of the original system Ax = b.
Another important fact is this:
Proposition 7.8. Given an m × n matrix A, for any sequence of row
operations E1, . . . , Ek, if P = Ek · · · E1 and B = PA, then the subspaces
spanned by the rows of A and the rows of B are identical. Therefore, A and
B have the same row rank. Furthermore, the matrices A and B also have
the same (column) rank.
Proof. Since B = PA, from a previous observation, the rows of B are
linear combinations of the rows of A, so the span of the rows of B is a
subspace of the span of the rows of A. Since P is invertible, A = P −1B, so

7.10. Reduced Row Echelon Form (RREF)
251
by the same reasoning the span of the rows of A is a subspace of the span
of the rows of B. Therefore, the subspaces spanned by the rows of A and
the rows of B are identical, which implies that A and B have the same row
rank.
Proposition 7.7 implies that the systems Ax = 0 and Bx = 0 have the
same solutions. Since Ax is a linear combinations of the columns of A and
Bx is a linear combinations of the columns of B, the maximum number
of linearly independent columns in A is equal to the maximum number
of linearly independent columns in B; that is, A and B have the same
rank.
Remark: The subspaces spanned by the columns of A and B can be dif-
ferent! However, their dimension must be the same.
We will show in Section 7.14 that the row rank is equal to the column
rank.
This will also be proven in Proposition 10.11 Let us now deﬁne
precisely what is a reduced row echelon matrix.
Deﬁnition 7.4. An m×n matrix A is a reduced row echelon matrix iﬀthe
following conditions hold:
(a) The ﬁrst nonzero entry in every row is 1. This entry is called a pivot.
(b) The ﬁrst nonzero entry of row i + 1 is to the right of the ﬁrst nonzero
entry of row i.
(c) The entries above a pivot are zero.
If a matrix satisﬁes the above conditions, we also say that it is in reduced
row echelon form, for short rref .
Note that Condition (b) implies that the entries below a pivot are also
zero. For example, the matrix
A =


1 6 0 1
0 0 1 2
0 0 0 0


is a reduced row echelon matrix.
In general, a matrix in rref has the

252
Gaussian Elimination, LU, Cholesky, Echelon Form
following shape:











1 0 0 × × 0 0 ×
0 1 0 × × 0 0 ×
0 0 1 × × 0 0 ×
0 0 0 0 0 1 0 ×
0 0 0 0 0 0 1 ×
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0











if the last row consists of zeros, or









1 0 0 × × 0 0 × 0 ×
0 1 0 × × 0 0 × 0 ×
0 0 1 × × 0 0 × 0 ×
0 0 0 0 0 1 0 × 0 ×
0 0 0 0 0 0 1 × × 0
0 0 0 0 0 0 0 0 1 ×









if the last row contains a pivot.
The following proposition shows that every matrix can be converted to
a reduced row echelon form using row operations.
Proposition 7.9. Given any m × n matrix A, there is a sequence of row
operations E1, . . . , Ek such that if P = Ek · · · E1, then U = PA is a reduced
row echelon matrix.
Proof. We proceed by induction on m. If m = 1, then either all entries
on this row are zero, so A = 0, or if aj is the ﬁrst nonzero entry in A, let
P = (a−1
j ) (a 1 × 1 matrix); clearly, PA is a reduced row echelon matrix.
Let us now assume that m ≥2. If A = 0, we are done, so let us assume
that A ̸= 0. Since A ̸= 0, there is a leftmost column j which is nonzero,
so pick any pivot π = aij in the jth column, permute row i and row 1 if
necessary, multiply the new ﬁrst row by π−1, and clear out the other entries
in column j by subtracting suitable multiples of row 1. At the end of this
process, we have a matrix A1 that has the following shape:
A1 =





0 · · · 0 1 ∗· · · ∗
0 · · · 0 0 ∗· · · ∗
...
... ... ...
...
0 · · · 0 0 ∗· · · ∗




,
where ∗stands for an arbitrary scalar, or more concisely
A1 =
0 1 B
0 0 D

,

7.11. RREF, Free Variables, and Homogenous Linear Systems
253
where D is a (m−1)×(n−j) matrix (and B is a 1×n−j matrix). If j = n,
we are done. Otherwise, by the induction hypothesis applied to D, there
is a sequence of row operations that converts D to a reduced row echelon
matrix R′, and these row operations do not aﬀect the ﬁrst row of A1, which
means that A1 is reduced to a matrix of the form
R =
0 1 B
0 0 R′

.
Because R′ is a reduced row echelon matrix, the matrix R satisﬁes Con-
ditions (a) and (b) of the reduced row echelon form. Finally, the entries
above all pivots in R′ can be cleared out by subtracting suitable multiples
of the rows of R′ containing a pivot. The resulting matrix also satisﬁes
Condition (c), and the induction step is complete.
Remark: There is a Matlab function named rref that converts any matrix
to its reduced row echelon form.
If A is any matrix and if R is a reduced row echelon form of A, the
second part of Proposition 7.8 can be sharpened a little, since the structure
of a reduced row echelon matrix makes it clear that its rank is equal to the
number of pivots.
Proposition 7.10. The rank of a matrix A is equal to the number of pivots
in its rref R.
7.11
RREF, Free Variables, and Homogenous
Linear Systems
Given a system of the form Ax = b, we can apply the reduction procedure
to the augmented matrix (A, b) to obtain a reduced row echelon matrix
(A′, b′) such that the system A′x = b′ has the same solutions as the original
system Ax = b. The advantage of the reduced system A′x = b′ is that
there is a simple test to check whether this system is solvable, and to ﬁnd
its solutions if it is solvable.
Indeed, if any row of the matrix A′ is zero and if the corresponding
entry in b′ is nonzero, then it is a pivot and we have the "equation"
0 = 1,
which means that the system A′x = b′ has no solution. On the other hand,
if there is no pivot in b′, then for every row i in which b′
i ̸= 0, there is some

254
Gaussian Elimination, LU, Cholesky, Echelon Form
column j in A′ where the entry on row i is 1 (a pivot). Consequently, we
can assign arbitrary values to the variable xk if column k does not contain
a pivot, and then solve for the pivot variables.
For example, if we consider the reduced row echelon matrix
(A′, b′) =


1 6 0 1 0
0 0 1 2 0
0 0 0 0 1

,
there is no solution to A′x = b′ because the third equation is 0 = 1. On
the other hand, the reduced system
(A′, b′) =


1 6 0 1 1
0 0 1 2 3
0 0 0 0 0


has solutions. We can pick the variables x2, x4 corresponding to nonpivot
columns arbitrarily, and then solve for x3 (using the second equation) and
x1 (using the ﬁrst equation).
The above reasoning proves the following theorem:
Theorem 7.5. Given any system Ax = b where A is a m×n matrix, if the
augmented matrix (A, b) is a reduced row echelon matrix, then the system
Ax = b has a solution iﬀthere is no pivot in b. In that case, an arbitrary
value can be assigned to the variable xj if column j does not contain a pivot.
Deﬁnition 7.5. Nonpivot variables are often called free variables.
Putting Proposition 7.9 and Theorem 7.5 together we obtain a criterion
to decide whether a system Ax = b has a solution: Convert the augmented
system (A, b) to a row reduced echelon matrix (A′, b′) and check whether
b′ has no pivot.
Remark: When writing a program implementing row reduction, we may
stop when the last column of the matrix A is reached. In this case, the test
whether the system Ax = b is solvable is that the row-reduced matrix A′
has no zero row of index i > r such that b′
i ̸= 0 (where r is the number of
pivots, and b′ is the row-reduced right-hand side).
If we have a homogeneous system Ax = 0, which means that b = 0,
of course x = 0 is always a solution, but Theorem 7.5 implies that if the
system Ax = 0 has more variables than equations, then it has some nonzero
solution (we call it a nontrivial solution).
Proposition 7.11. Given any homogeneous system Ax = 0 of m equations

7.11. RREF, Free Variables, and Homogenous Linear Systems
255
in n variables, if m < n, then there is a nonzero vector x ∈Rn such that
Ax = 0.
Proof. Convert the matrix A to a reduced row echelon matrix A′. We
know that Ax = 0 iﬀA′x = 0. If r is the number of pivots of A′, we must
have r ≤m, so by Theorem 7.5 we may assign arbitrary values to n−r > 0
nonpivot variables and we get nontrivial solutions.
Theorem 7.5 can also be used to characterize when a square matrix is
invertible. First, note the following simple but important fact:
If a square n × n matrix A is a row reduced echelon matrix, then either
A is the identity or the bottom row of A is zero.
Proposition 7.12. Let A be a square matrix of dimension n. The following
conditions are equivalent:
(a) The matrix A can be reduced to the identity by a sequence of elementary
row operations.
(b) The matrix A is a product of elementary matrices.
(c) The matrix A is invertible.
(d) The system of homogeneous equations Ax = 0 has only the trivial so-
lution x = 0.
Proof. First we prove that (a) implies (b).
If (a) can be reduced to
the identity by a sequence of row operations E1, . . . , Ep, this means that
Ep · · · E1A = I. Since each Ei is invertible, we get
A = E−1
1
· · · E−1
p ,
where each E−1
i
is also an elementary row operation, so (b) holds. Now
if (b) holds, since elementary row operations are invertible, A is invertible
and (c) holds. If A is invertible, we already observed that the homogeneous
system Ax = 0 has only the trivial solution x = 0, because from Ax = 0,
we get A−1Ax = A−10; that is, x = 0. It remains to prove that (d) implies
(a) and for this we prove the contrapositive: if (a) does not hold, then (d)
does not hold.
Using our basic observation about reducing square matrices, if A does
not reduce to the identity, then A reduces to a row echelon matrix A′ whose
bottom row is zero. Say A′ = PA, where P is a product of elementary row
operations. Because the bottom row of A′ is zero, the system A′x = 0 has
at most n −1 nontrivial equations, and by Proposition 7.11, this system
has a nontrivial solution x. But then, Ax = P −1A′x = 0 with x ̸= 0,

256
Gaussian Elimination, LU, Cholesky, Echelon Form
contradicting the fact that the system Ax = 0 is assumed to have only the
trivial solution. Therefore, (d) implies (a) and the proof is complete.
Proposition 7.12 yields a method for computing the inverse of an invert-
ible matrix A: reduce A to the identity using elementary row operations,
obtaining
Ep · · · E1A = I.
Multiplying both sides by A−1 we get
A−1 = Ep · · · E1.
From a practical point of view, we can build up the product Ep · · · E1 by
reducing to row echelon form the augmented n×2n matrix (A, In) obtained
by adding the n columns of the identity matrix to A. This is just another
way of performing the Gauss-Jordan procedure.
Here is an example: let us ﬁnd the inverse of the matrix
A =
5 4
6 5

.
We form the 2 × 4 block matrix
(A, I) =
5 4 1 0
6 5 0 1

and apply elementary row operations to reduce A to the identity.
For
example:
(A, I) =
5 4 1 0
6 5 0 1

−→
5 4 1 0
1 1 −1 1

by subtracting row 1 from row 2,
5 4 1 0
1 1 −1 1

−→
1 0 5 −4
1 1 −1 1

by subtracting 4 × row 2 from row 1,
1 0 5 −4
1 1 −1 1

−→
1 0 5 −4
0 1 −6 5

= (I, A−1),
by subtracting row 1 from row 2. Thus
A−1 =
 5 −4
−6 5

.
Proposition 7.12 can also be used to give an elementary proof of the
fact that if a square matrix A has a left inverse B (resp. a right inverse B),
so that BA = I (resp. AB = I), then A is invertible and A−1 = B. This is
an interesting exercise, try it!

7.12. Uniqueness of RREF Form
257
7.12
Uniqueness of RREF Form
For the sake of completeness, we prove that the reduced row echelon form
of a matrix is unique. The neat proof given below is borrowed and adapted
from W. Kahan.
Proposition 7.13. Let A be any m × n matrix.
If U and V are two
reduced row echelon matrices obtained from A by applying two sequences of
elementary row operations E1, . . . , Ep and F1, . . . , Fq, so that
U = Ep · · · E1A
and
V = Fq · · · F1A,
then U = V and Ep · · · E1 = Fq · · · F1. In other words, the reduced row
echelon form of any matrix is unique.
Proof. Let
C = Ep · · · E1F −1
1
· · · F −1
q
so that
U = CV
and
V = C−1U.
We prove by induction on n that U = V (and C = I).
Let ℓj denote the jth column of the identity matrix In, and let uj = Uℓj,
vj = V ℓj, cj = Cℓj, and aj = Aℓj, be the jth column of U, V , C, and A
respectively.
First I claim that uj = 0 iﬀvj = 0 iﬀaj = 0.
Indeed, if vj = 0, then (because U = CV ) uj = Cvj = 0, and if uj = 0,
then vj = C−1uj = 0. Since U = Ep · · · E1A, we also get aj = 0 iﬀuj = 0.
Therefore, we may simplify our task by striking out columns of zeros
from U, V , and A, since they will have corresponding indices. We still use
n to denote the number of columns of A. Observe that because U and
V are reduced row echelon matrices with no zero columns, we must have
u1 = v1 = ℓ1.
Claim.
If U and V are reduced row echelon matrices without zero
columns such that U = CV , for all k ≥1, if k ≤n, then ℓk occurs in U iﬀ
ℓk occurs in V , and if ℓk does occur in U, then
(1) ℓk occurs for the same column index jk in both U and V ;
(2) the ﬁrst jk columns of U and V match;
(3) the subsequent columns in U and V (of column index > jk) whose
coordinates of index k +1 through m are all equal to 0 also match. Let
nk be the rightmost index of such a column, with nk = jk if there is
none.

258
Gaussian Elimination, LU, Cholesky, Echelon Form
(4) the ﬁrst nk columns of C match the ﬁrst nk columns of In.
We prove this claim by induction on k.
For the base case k = 1, we already know that u1 = v1 = ℓ1. We also
have
c1 = Cℓ1 = Cv1 = u1 = ℓ1.
If vj = λℓ1 for some λ ∈R, then
uj = Uℓj = CV ℓj = Cvj = λCℓ1 = λc1 = λℓ1 = vj.
A similar argument using C−1 shows that if uj = λℓ1, then vj = uj.
Therefore, all the columns of U and V proportional to ℓ1 match, which
establishes the base case. Observe that if ℓ2 appears in U, then it must
appear in both U and V for the same index, and if not then n1 = n and
U = V .
Next us now prove the induction step. If nk = n, then U = V and we
are done. Otherwise, ℓk+1 appears in both U and V , in which case, by (2)
and (3) of the induction hypothesis, it appears in both U and V for the
same index, say jk+1. Thus, ujk+1 = vjk+1 = ℓk+1. It follows that
ck+1 = Cℓk+1 = Cvjk+1 = ujk+1 = ℓk+1,
so the ﬁrst jk+1 columns of C match the ﬁrst jk+1 columns of In.
Consider any subsequent column vj (with j > jk+1) whose elements
beyond the (k +1)th all vanish. Then vj is a linear combination of columns
of V to the left of vj, so
uj = Cvj = vj
because the ﬁrst k+1 columns of C match the ﬁrst column of In. Similarly,
any subsequent column uj (with j > jk+1) whose elements beyond the
(k +1)th all vanish is equal to vj. Therefore, all the subsequent columns in
U and V (of index > jk+1) whose elements beyond the (k + 1)th all vanish
also match, so the ﬁrst nk+1 columns of C match the ﬁrst nk+1 columns of
C, which completes the induction hypothesis.
We can now prove that U = V (recall that we may assume that U and
V have no zero columns). We noted earlier that u1 = v1 = ℓ1, so there is
a largest k ≤n such that ℓk occurs in U. Then the previous claim implies
that all the columns of U and V match, which means that U = V .
The reduction to row echelon form also provides a method to describe
the set of solutions of a linear system of the form Ax = b.

7.13. Solving Linear Systems Using RREF
259
7.13
Solving Linear Systems Using RREF
First we have the following simple result.
Proposition 7.14. Let A be any m × n matrix and let b ∈Rm be any
vector. If the system Ax = b has a solution, then the set Z of all solutions
of this system is the set
Z = x0 + Ker (A) = {x0 + x | Ax = 0},
where x0 ∈Rn is any solution of the system Ax = b, which means that
Ax0 = b (x0 is called a special solution), and where Ker (A) = {x ∈Rn |
Ax = 0}, the set of solutions of the homogeneous system associated with
Ax = b.
Proof. Assume that the system Ax = b is solvable and let x0 and x1 be
any two solutions so that Ax0 = b and Ax1 = b. Subtracting the ﬁrst
equation from the second, we get
A(x1 −x0) = 0,
which means that x1 −x0 ∈Ker (A). Therefore, Z ⊆x0 + Ker (A), where
x0 is a special solution of Ax = b. Conversely, if Ax0 = b, then for any
z ∈Ker (A), we have Az = 0, and so
A(x0 + z) = Ax0 + Az = b + 0 = b,
which shows that x0 + Ker (A) ⊆Z. Therefore, Z = x0 + Ker (A).
Given a linear system Ax = b, reduce the augmented matrix (A, b) to
its row echelon form (A′, b′). As we showed before, the system Ax = b has
a solution iﬀb′ contains no pivot. Assume that this is the case. Then, if
(A′, b′) has r pivots, which means that A′ has r pivots since b′ has no pivot,
we know that the ﬁrst r columns of Im appear in A′.
We can permute the columns of A′ and renumber the variables in x
correspondingly so that the ﬁrst r columns of Im match the ﬁrst r columns
of A′, and then our reduced echelon matrix is of the form (R, b′) with
R =

Ir
F
0m−r,r 0m−r,n−r

and
b′ =

d
0m−r

,

260
Gaussian Elimination, LU, Cholesky, Echelon Form
where F is a r × (n −r) matrix and d ∈Rr. Note that R has m −r zero
rows.
Then because

Ir
F
0m−r,r 0m−r,n−r
  d
0n−r

=

d
0m−r

= b′,
we see that
x0 =
 d
0n−r

is a special solution of Rx = b′, and thus to Ax = b. In other words, we
get a special solution by assigning the ﬁrst r components of b′ to the pivot
variables and setting the nonpivot variables (the free variables) to zero.
Here is an example of the preceding construction taken from Kumpel
and Thorpe [Kumpel and Thorpe (1983)]. The linear system
x1 −x2 + x3 + x4 −2x5 = −1
−2x1 + 2x2 −x3 + x5 = 2
x1 −x2 + 2x3 + 3x4 −5x5 = −1,
is represented by the augmented matrix
(A, b) =


1 −1 1 1 −2 −1
−2 2 −1 0 1
2
1 −1 2 3 −5 −1

,
where A is a 3 × 5 matrix. The reader should ﬁnd that the row echelon
form of this system is
(A′, b′) =


1 −1 0 −1 1 −1
0 0 1 2 −3 0
0 0 0 0
0
0

.
The 3×5 matrix A′ has rank 2. We permute the second and third columns
(which is equivalent to interchanging variables x2 and x3) to form
R =
 I2
F
01,2 01,3

,
F =
−1 −1 1
0
2 −3

.
Then a special solution to this linear system is given by
x0 =
 d
03

=


−1
0
03

.

7.13. Solving Linear Systems Using RREF
261
We can also ﬁnd a basis of the kernel (nullspace) of A using F.
If
x = (u, v) is in the kernel of A, with u ∈Rr and v ∈Rn−r, then x is also
in the kernel of R, which means that Rx = 0; that is,

Ir
F
0m−r,r 0m−r,n−r
 u
v

=
u + Fv
0m−r

=
 0r
0m−r

.
Therefore, u = −Fv, and Ker (A) consists of all vectors of the form
−Fv
v

=
 −F
In−r

v,
for any arbitrary v ∈Rn−r. It follows that the n −r columns of the matrix
N =
 −F
In−r

form a basis of the kernel of A. This is because N contains the identity ma-
trix In−r as a submatrix, so the columns of N are linearly independent. In
summary, if N 1, . . . , N n−r are the columns of N, then the general solution
of the equation Ax = b is given by
x =
 d
0n−r

+ xr+1N 1 + · · · + xnN n−r,
where xr+1, . . . , xn are the free variables; that is, the nonpivot variables.
Going back to our example from Kumpel and Thorpe [Kumpel and
Thorpe (1983)], we see that
N =
−F
I3

=






1 1 −1
0 −2 −3
1 0
0
0 1
0
0 0
1






,
and that the general solution is given by
x =






−1
0
0
0
0






+ x3






1
0
1
0
0






+ x4






1
−2
0
1
0






+ x5






−1
−3
0
0
1






.
In the general case where the columns corresponding to pivots are mixed
with the columns corresponding to free variables, we ﬁnd the special solu-
tion as follows. Let i1 < · · · < ir be the indices of the columns correspond-
ing to pivots. Assign b′
k to the pivot variable xik for k = 1, . . . , r, and set all

262
Gaussian Elimination, LU, Cholesky, Echelon Form
other variables to 0. To ﬁnd a basis of the kernel, we form the n−r vectors
N k obtained as follows. Let j1 < · · · < jn−r be the indices of the columns
corresponding to free variables. For every column jk corresponding to a
free variable (1 ≤k ≤n−r), form the vector N k deﬁned so that the entries
N k
i1, . . . , N k
ir are equal to the negatives of the ﬁrst r entries in column jk
(ﬂip the sign of these entries); let N k
jk = 1, and set all other entries to zero.
Schematically, if the column of index jk (corresponding to the free variable
xjk) is











α1
...
αr
0
...
0











,
then the vector N k is given by
1
...
i1 −1
i1
i1 + 1
...
ir −1
ir
ir + 1
...
jk −1
jk
jk + 1
...
n
































0
...
0
−α1
0
...
0
−αr
0
...
0
1
0
...
0
































.
The presence of the 1 in position jk guarantees that N 1, . . . , N n−r are
linearly independent.
As an illustration of the above method, consider the problem of ﬁnding
a basis of the subspace V of n × n matrices A ∈Mn(R) satisfying the
following properties:

7.13. Solving Linear Systems Using RREF
263
(1) The sum of the entries in every row has the same value (say c1);
(2) The sum of the entries in every column has the same value (say c2).
It turns out that c1 = c2 and that the 2n −2 equations corresponding to
the above conditions are linearly independent. We leave the proof of these
facts as an interesting exercise. It can be shown using the duality theorem
(Theorem 10.1) that the dimension of the space V of matrices satisfying
the above equations is n2 −(2n −2). Let us consider the case n = 4. There
are 6 equations, and the space V has dimension 10. The equations are
a11 + a12 + a13 + a14 −a21 −a22 −a23 −a24 = 0
a21 + a22 + a23 + a24 −a31 −a32 −a33 −a34 = 0
a31 + a32 + a33 + a34 −a41 −a42 −a43 −a44 = 0
a11 + a21 + a31 + a41 −a12 −a22 −a32 −a42 = 0
a12 + a22 + a32 + a42 −a13 −a23 −a33 −a43 = 0
a13 + a23 + a33 + a43 −a14 −a24 −a34 −a44 = 0,
and the corresponding matrix is
A =









1 1
1
1 −1 −1 −1 −1 0
0
0
0
0
0
0
0
0 0
0
0
1
1
1
1 −1 −1 −1 −1 0
0
0
0
0 0
0
0
0
0
0
0
1
1
1
1 −1 −1 −1 −1
1 −1 0
0
1 −1 0
0
1 −1 0
0
1 −1 0
0
0 1 −1 0
0
1 −1 0
0
1 −1 0
0
1 −1 0
0 0
1 −1 0
0
1 −1 0
0
1 −1 0
0
1 −1









.
The result of performing the reduction to row echelon form yields the
following matrix in rref:
U =









1 0 0 0 0 −1 −1 −1 0 −1 −1 −1 2
1
1
1
0 1 0 0 0 1
0
0 0 1
0
0 −1 0 −1 −1
0 0 1 0 0 0
1
0 0 0
1
0 −1 −1 0 −1
0 0 0 1 0 0
0
1 0 0
0
1 −1 −1 −1 0
0 0 0 0 1 1
1
1 0 0
0
0 −1 −1 −1 −1
0 0 0 0 0 0
0
0 1 1
1
1 −1 −1 −1 −1









The list pivlist of indices of the pivot variables and the list freelist of
indices of the free variables is given by
pivlist = (1, 2, 3, 4, 5, 9),
freelist = (6, 7, 8, 10, 11, 12, 13, 14, 15, 16).

264
Gaussian Elimination, LU, Cholesky, Echelon Form
After applying the algorithm to ﬁnd a basis of the kernel of U, we ﬁnd the
following 16 × 10 matrix
BK =






























1
1
1
1
1
1 −2 −1 −1 −1
−1 0
0 −1 0
0
1
0
1
1
0 −1 0
0 −1 0
1
1
0
1
0
0 −1 0
0 −1 1
1
1
0
−1 −1 −1 0
0
0
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0 −1 −1 −1 1
1
1
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1






























.
The reader should check that that in each column j of BK, the lowest
bold 1 belongs to the row whose index is the jth element in freelist, and that
in each column j of BK, the signs of the entries whose indices belong to
pivlist are the ﬂipped signs of the 6 entries in the column U corresponding
to the jth index in freelist. We can now read oﬀfrom BK the 4×4 matrices
that form a basis of V : every column of BK corresponds to a matrix whose
rows have been concatenated. We get the following 10 matrices:
M1 =




1 −1 0 0
−1 1 0 0
0
0 0 0
0
0 0 0



,
M2 =




1 0 −1 0
−1 0 1 0
0 0 0 0
0 0 0 0



,
M3 =




1 0 0 −1
−1 0 0 1
0 0 0 0
0 0 0 0



,
M4 =




1 −1 0 0
0
0 0 0
−1 1 0 0
0
0 0 0



,
M5 =




1 0 −1 0
0 0 0 0
−1 0 1 0
0 0 0 0



,
M6 =




1 0 0 −1
0 0 0 0
−1 0 0 1
0 0 0 0



,

7.13. Solving Linear Systems Using RREF
265
M7 =




−2 1 1 1
1 0 0 0
1 0 0 0
1 0 0 0



,
M8 =




−1 0 1 1
1 0 0 0
1 0 0 0
0 1 0 0



,
M9 =




−1 1 0 1
1 0 0 0
1 0 0 0
0 0 1 0



,
M10 =




−1 1 1 0
1 0 0 0
1 0 0 0
0 0 0 1



.
Recall that a magic square is a square matrix that satisﬁes the two
conditions about the sum of the entries in each row and in each column
to be the same number, and also the additional two constraints that the
main descending and the main ascending diagonals add up to this common
number. Furthermore, the entries are also required to be positive integers.
For n = 4, the additional two equations are
a22 + a33 + a44 −a12 −a13 −a14 = 0
a41 + a32 + a23 −a11 −a12 −a13 = 0,
and the 8 equations stating that a matrix is a magic square are linearly
independent.
Again, by running row elimination, we get a basis of the
"generalized magic squares" whose entries are not restricted to be positive
integers. We ﬁnd a basis of 8 matrices. For n = 3, we ﬁnd a basis of 3
matrices.
A magic square is said to be normal if its entries are precisely the
integers 1, 2 . . . , n2. Then since the sum of these entries is
1 + 2 + 3 + · · · + n2 = n2(n2 + 1)
2
,
and since each row (and column) sums to the same number, this common
value (the magic sum) is
n(n2 + 1)
2
.
It is easy to see that there are no normal magic squares for n = 2. For
n = 3, the magic sum is 15, for n = 4, it is 34, and for n = 5, it is 65.
In the case n = 3, we have the additional condition that the rows and
columns add up to 15, so we end up with a solution parametrized by two
numbers x1, x2; namely,


x1 + x2 −5 10 −x2
10 −x1
20 −2x1 −x2
5
2x1 + x2 −10
x1
x2
15 −x1 −x2

.

266
Gaussian Elimination, LU, Cholesky, Echelon Form
Thus, in order to ﬁnd a normal magic square, we have the additional
inequality constraints
x1 + x2 > 5
x1 < 10
x2 < 10
2x1 + x2 < 20
2x1 + x2 > 10
x1 > 0
x2 > 0
x1 + x2 < 15,
and all 9 entries in the matrix must be distinct.
After a tedious case
analysis, we discover the remarkable fact that there is a unique normal
magic square (up to rotations and reﬂections):


2 7 6
9 5 1
4 3 8

.
It turns out that there are 880 diﬀerent normal magic squares for n = 4,
and 275, 305, 224 normal magic squares for n = 5 (up to rotations and
reﬂections). Even for n = 4, it takes a fair amount of work to enumerate
them all!
Finding the number of magic squares for n > 5 is an open
problem!
7.14
Elementary Matrices and Columns Operations
Instead of performing elementary row operations on a matrix A, we can
perform elementary columns operations, which means that we multiply
A by elementary matrices on the right. As elementary row and column
operations, P(i, k), Ei,j;β, Ei,λ perform the following actions:
(1) As a row operation, P(i, k) permutes row i and row k.
(2) As a column operation, P(i, k) permutes column i and column k.
(3) The inverse of P(i, k) is P(i, k) itself.
(4) As a row operation, Ei,j;β adds β times row j to row i.
(5) As a column operation, Ei,j;β adds β times column i to column j (note
the switch in the indices).
(6) The inverse of Ei,j;β is Ei,j;−β.

7.15. Transvections and Dilatations ⊛
267
(7) As a row operation, Ei,λ multiplies row i by λ.
(8) As a column operation, Ei,λ multiplies column i by λ.
(9) The inverse of Ei,λ is Ei,λ−1.
We can deﬁne the notion of a reduced column echelon matrix and show
that every matrix can be reduced to a unique reduced column echelon form.
Now given any m × n matrix A, if we ﬁrst convert A to its reduced row
echelon form R, it is easy to see that we can apply elementary column
operations that will reduce R to a matrix of the form

Ir
0r,n−r
0m−r,r 0m−r,n−r

,
where r is the number of pivots (obtained during the row reduction). There-
fore, for every m × n matrix A, there exist two sequences of elementary
matrices E1, . . . , Ep and F1, . . . , Fq, such that
Ep · · · E1AF1 · · · Fq =

Ir
0r,n−r
0m−r,r 0m−r,n−r

.
The matrix on the right-hand side is called the rank normal form of A.
Clearly, r is the rank of A. As a corollary we obtain the following important
result whose proof is immediate.
Proposition 7.15. A matrix A and its transpose A⊤have the same rank.
7.15
Transvections and Dilatations ⊛
In this section we characterize the linear isomorphisms of a vector space
E that leave every vector in some hyperplane ﬁxed. These maps turn out
to be the linear maps that are represented in some suitable basis by ele-
mentary matrices of the form Ei,j;β (transvections) or Ei,λ (dilatations).
Furthermore, the transvections generate the group SL(E), and the dilata-
tions generate the group GL(E).
Let H be any hyperplane in E, and pick some (nonzero) vector v ∈E
such that v /∈H, so that
E = H ⊕Kv.
Assume that f : E →E is a linear isomorphism such that f(u) = u for all
u ∈H, and that f is not the identity. We have
f(v) = h + αv,
for some h ∈H and some α ∈K,

268
Gaussian Elimination, LU, Cholesky, Echelon Form
with α ̸= 0, because otherwise we would have f(v) = h = f(h) since h ∈H,
contradicting the injectivity of f (v ̸= h since v /∈H). For any x ∈E, if
we write
x = y + tv,
for some y ∈H and some t ∈K,
then
f(x) = f(y) + f(tv) = y + tf(v) = y + th + tαv,
and since αx = αy + tαv, we get
f(x) −αx = (1 −α)y + th
f(x) −x = t(h + (α −1)v).
Observe that if E is ﬁnite-dimensional, by picking a basis of E consisting of
v and basis vectors of H, then the matrix of f is a lower triangular matrix
whose diagonal entries are all 1 except the ﬁrst entry which is equal to α.
Therefore, det(f) = α.
Case 1. α ̸= 1.
We have f(x) = αx iﬀ(1 −α)y + th = 0 iﬀ
y =
t
α −1h.
Then if we let w = h + (α −1)v, for y = (t/(α −1))h, we have
x = y + tv =
t
α −1h + tv =
t
α −1(h + (α −1)v) =
t
α −1w,
which shows that f(x) = αx iﬀx ∈Kw. Note that w /∈H, since α ̸= 1
and v /∈H. Therefore,
E = H ⊕Kw,
and f is the identity on H and a magniﬁcation by α on the line D = Kw.
Deﬁnition 7.6. Given a vector space E, for any hyperplane H in E, any
nonzero vector u ∈E such that u ̸∈H, and any scalar α ̸= 0, 1, a linear map
f such that f(x) = x for all x ∈H and f(x) = αx for every x ∈D = Ku
is called a dilatation of hyperplane H, direction D, and scale factor α.
If πH and πD are the projections of E onto H and D, then we have
f(x) = πH(x) + απD(x).
The inverse of f is given by
f −1(x) = πH(x) + α−1πD(x).

7.15. Transvections and Dilatations ⊛
269
When α = −1, we have f 2 = id, and f is a symmetry about the hyperplane
H in the direction D. This situation includes orthogonal reﬂections about
H.
Case 2. α = 1.
In this case,
f(x) −x = th,
that is, f(x) −x ∈Kh for all x ∈E. Assume that the hyperplane H is
given as the kernel of some linear form ϕ, and let a = ϕ(v). We have a ̸= 0,
since v /∈H. For any x ∈E, we have
ϕ(x −a−1ϕ(x)v) = ϕ(x) −a−1ϕ(x)ϕ(v) = ϕ(x) −ϕ(x) = 0,
which shows that x −a−1ϕ(x)v ∈H for all x ∈E. Since every vector in H
is ﬁxed by f, we get
x −a−1ϕ(x)v = f(x −a−1ϕ(x)v)
= f(x) −a−1ϕ(x)f(v),
so
f(x) = x + ϕ(x)(f(a−1v) −a−1v).
Since f(z) −z ∈Kh for all z ∈E, we conclude that u = f(a−1v) −a−1v =
βh for some β ∈K, so ϕ(u) = 0, and we have
f(x) = x + ϕ(x)u,
ϕ(u) = 0.
(7.9)
A linear map deﬁned as above is denoted by τϕ,u.
Conversely for any linear map f = τϕ,u given by equation (7.9), where
ϕ is a nonzero linear form and u is some vector u ∈E such that ϕ(u) = 0,
if u = 0, then f is the identity, so assume that u ̸= 0.
If so, we have
f(x) = x iﬀϕ(x) = 0, that is, iﬀx ∈H. We also claim that the inverse of
f is obtained by changing u to −u. Actually, we check the slightly more
general fact that
τϕ,u ◦τϕ,w = τϕ,u+w.
Indeed, using the fact that ϕ(w) = 0, we have
τϕ,u(τϕ,w(x)) = τϕ,w(x) + ϕ(τϕ,w(x))u
= τϕ,w(x) + (ϕ(x) + ϕ(x)ϕ(w))u
= τϕ,w(x) + ϕ(x)u
= x + ϕ(x)w + ϕ(x)u
= x + ϕ(x)(u + w).

270
Gaussian Elimination, LU, Cholesky, Echelon Form
For v = −u, we have τϕ,u+v = ϕϕ,0 = id, so τ −1
ϕ,u = τϕ,−u, as claimed.
Therefore, we proved that every linear isomorphism of E that leaves
every vector in some hyperplane H ﬁxed and has the property that f(x) −
x ∈H for all x ∈E is given by a map τϕ,u as deﬁned by Equation (∗),
where ϕ is some nonzero linear form deﬁning H and u is some vector in H.
We have τϕ,u = id iﬀu = 0.
Deﬁnition 7.7. Given any hyperplane H in E, for any nonzero nonlinear
form ϕ ∈E∗deﬁning H (which means that H = Ker (ϕ)) and any nonzero
vector u ∈H, the linear map f = τϕ,u given by
τϕ,u(x) = x + ϕ(x)u,
ϕ(u) = 0,
for all x ∈E is called a transvection of hyperplane H and direction u. The
map f = τϕ,u leaves every vector in H ﬁxed, and f(x) −x ∈Ku for all
x ∈E.
The above arguments show the following result.
Proposition 7.16. Let f : E →E be a bijective linear map and assume
that f ̸= id and that f(x) = x for all x ∈H, where H is some hyperplane in
E. If there is some nonzero vector u ∈E such that u /∈H and f(u)−u ∈H,
then f is a transvection of hyperplane H; otherwise, f is a dilatation of
hyperplane H.
Proof. Using the notation as above, for some v /∈H, we have f(v) = h+αv
with α ̸= 0, and write u = y + tv with y ∈H and t ̸= 0 since u /∈H. If
f(u) −u ∈H, from
f(u) −u = t(h + (α −1)v),
we get (α −1)v ∈H, and since v /∈H, we must have α = 1, and we proved
that f is a transvection. Otherwise, α ̸= 0, 1, and we proved that f is a
dilatation.
If E is ﬁnite-dimensional, then α = det(f), so we also have the following
result.
Proposition 7.17. Let f : E →E be a bijective linear map of a ﬁnite-
dimensional vector space E and assume that f ̸= id and that f(x) = x for
all x ∈H, where H is some hyperplane in E. If det(f) = 1, then f is a
transvection of hyperplane H; otherwise, f is a dilatation of hyperplane H.

7.15. Transvections and Dilatations ⊛
271
Suppose that f is a dilatation of hyperplane H and direction u, and say
det(f) = α ̸= 0, 1. Pick a basis (u, e2, . . . , en) of E where (e2, . . . , en) is a
basis of H. Then the matrix of f is of the form





α 0 · · · 0
0 1
0
...
... ...
0 0 · · · 1




,
which is an elementary matrix of the form E1,α. Conversely, it is clear that
every elementary matrix of the form Ei,α with α ̸= 0, 1 is a dilatation.
Now, assume that f is a transvection of hyperplane H and direction
u ∈H. Pick some v /∈H, and pick some basis (u, e3, . . . , en) of H, so that
(v, u, e3, . . . , en) is a basis of E. Since f(v) −v ∈Ku, the matrix of f is of
the form





1 0 · · · 0
α 1
0
...
... ...
0 0 · · · 1




,
which is an elementary matrix of the form E2,1;α. Conversely, it is clear
that every elementary matrix of the form Ei,j;α (α ̸= 0) is a transvection.
The following proposition is an interesting exercise that requires good
mastery of the elementary row operations Ei,j;β; see Problems 7.10 and
7.11.
Proposition 7.18. Given any invertible n × n matrix A, there is a matrix
S such that
SA =
In−1 0
0
α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the
form Ei,j;β; that is, S is a composition of transvections.
Surprisingly, every transvection is the composition of two dilatations!
Proposition 7.19. If the ﬁeld K is not of characteristic 2, then every
transvection f of hyperplane H can be written as f = d2 ◦d1, where d1, d2
are dilatations of hyperplane H, where the direction of d1 can be chosen
arbitrarily.

272
Gaussian Elimination, LU, Cholesky, Echelon Form
Proof. Pick some dilatation d1 of hyperplane H and scale factor α ̸= 0, 1.
Then, d2 = f ◦d−1
1
leaves every vector in H ﬁxed, and det(d2) = α−1 ̸= 1.
By Proposition 7.17, the linear map d2 is a dilatation of hyperplane H, and
we have f = d2 ◦d1, as claimed.
Observe that in Proposition 7.19, we can pick α = −1; that is, every
transvection of hyperplane H is the compositions of two symmetries about
the hyperplane H, one of which can be picked arbitrarily.
Remark: Proposition 7.19 holds as long as K ̸= {0, 1}.
The following important result is now obtained.
Theorem 7.6. Let E be any ﬁnite-dimensional vector space over a ﬁeld K
of characteristic not equal to 2. Then the group SL(E) is generated by the
transvections, and the group GL(E) is generated by the dilatations.
Proof. Consider any f ∈SL(E), and let A be its matrix in any basis. By
Proposition 7.18, there is a matrix S such that
SA =
In−1 0
0
α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the
form Ei,j;β. Since det(A) = 1, we have α = 1, and the result is proven.
Otherwise, if f is invertible but f /∈SL(E), the above equation shows En,α
is a dilatation, S is a product of transvections, and by Proposition 7.19,
every transvection is the composition of two dilatations. Thus, the second
result is also proven.
We conclude this section by proving that any two transvections are
conjugate in GL(E). Let τϕ,u (u ̸= 0) be a transvection and let g ∈GL(E)
be any invertible linear map. We have
(g ◦τϕ,u ◦g−1)(x) = g(g−1(x) + ϕ(g−1(x))u)
= x + ϕ(g−1(x))g(u).
Let us ﬁnd the hyperplane determined by the linear form x 7→ϕ(g−1(x)).
This is the set of vectors x ∈E such that ϕ(g−1(x)) = 0, which holds iﬀ
g−1(x) ∈H iﬀx ∈g(H). Therefore, Ker (ϕ ◦g−1) = g(H) = H′, and we
have g(u) ∈g(H) = H′, so g ◦τϕ,u ◦g−1 is the transvection of hyperplane
H′ = g(H) and direction u′ = g(u) (with u′ ∈H′).

7.15. Transvections and Dilatations ⊛
273
Conversely, let τψ,u′ be some transvection (u′ ̸= 0). Pick some vectors
v, v′ such that ϕ(v) = ψ(v′) = 1, so that
E = H ⊕Kv = H′ ⊕Kv′.
There is a linear map g ∈GL(E) such that g(u) = u′, g(v) = v′,
and g(H) = H′.
To deﬁne g, pick a basis (v, u, e2, . . . , en−1) where
(u, e2, . . . , en−1) is a basis of H and pick a basis (v′, u′, e′
2, . . . , e′
n−1) where
(u′, e′
2, . . . , e′
n−1) is a basis of H′; then g is deﬁned so that g(v) = v′,
g(u) = u′, and g(ei) = g(e′
i), for i = 2, . . . , n −1. If n = 2, then ei and e′
i
are missing. Then, we have
(g ◦τϕ,u ◦g−1)(x) = x + ϕ(g−1(x))u′.
Now ϕ◦g−1 also determines the hyperplane H′ = g(H), so we have ϕ◦g−1 =
λψ for some nonzero λ in K. Since v′ = g(v), we get
ϕ(v) = ϕ ◦g−1(v′) = λψ(v′),
and since ϕ(v) = ψ(v′) = 1, we must have λ = 1. It follows that
(g ◦τϕ,u ◦g−1)(x) = x + ψ(x)u′ = τψ,u′(x).
In summary, we proved almost all parts the following result.
Proposition 7.20. Let E be any ﬁnite-dimensional vector space. For every
transvection τϕ,u (u ̸= 0) and every linear map g ∈GL(E), the map g ◦
τϕ,u ◦g−1 is the transvection of hyperplane g(H) and direction g(u) (that
is, g ◦τϕ,u ◦g−1 = τϕ◦g−1,g(u)). For every other transvection τψ,u′ (u′ ̸= 0),
there is some g ∈GL(E) such τψ,u′ = g ◦τϕ,u ◦g−1; in other words any
two transvections (̸= id) are conjugate in GL(E). Moreover, if n ≥3, then
the linear isomorphism g as above can be chosen so that g ∈SL(E).
Proof. We just need to prove that if n ≥3, then for any two transvections
τϕ,u and τψ,u′ (u, u′ ̸= 0), there is some g ∈SL(E) such that τψ,u′ = g◦τϕ,u◦
g−1. As before, we pick a basis (v, u, e2, . . . , en−1) where (u, e2, . . . , en−1) is
a basis of H, we pick a basis (v′, u′, e′
2, . . . , e′
n−1) where (u′, e′
2, . . . , e′
n−1) is
a basis of H′, and we deﬁne g as the unique linear map such that g(v) = v′,
g(u) = u′, and g(ei) = e′
i, for i = 1, . . . , n −1. But in this case, both H
and H′ = g(H) have dimension at least 2, so in any basis of H′ including
u′, there is some basis vector e′
2 independent of u′, and we can rescale e′
2 in
such a way that the matrix of g over the two bases has determinant +1.

274
Gaussian Elimination, LU, Cholesky, Echelon Form
7.16
Summary
The main concepts and results of this chapter are listed below:
• One does not solve (large) linear systems by computing determinants.
• Upper-triangular (lower-triangular) matrices.
• Solving by back-substitution (forward-substitution).
• Gaussian elimination.
• Permuting rows.
• The pivot of an elimination step; pivoting.
• Transposition matrix; elementary matrix.
• The Gaussian elimination theorem (Theorem 7.1).
• Gauss-Jordan factorization.
• LU-factorization; Necessary and suﬃcient condition for the existence
of an
LU-factorization (Proposition 7.1).
• LDU-factorization.
• "PA = LU theorem" (Theorem 7.2).
• LDL⊤-factorization of a symmetric matrix.
• Avoiding small pivots: partial pivoting; complete pivoting.
• Gaussian elimination of tridiagonal matrices.
• LU-factorization of tridiagonal matrices.
• Symmetric positive deﬁnite matrices (SPD matrices).
• Cholesky factorization (Theorem 7.4).
• Criteria for a symmetric matrix to be positive deﬁnite; Sylvester's cri-
terion.
• Reduced row echelon form.
• Reduction of a rectangular matrix to its row echelon form.
• Using the reduction to row echelon form to decide whether a system
Ax = b is solvable, and to ﬁnd its solutions, using a special solution
and a basis of the homogeneous system Ax = 0.
• Magic squares.
• Transvections and dilatations.

7.17. Problems
275
7.17
Problems
Problem 7.1. Solve the following linear systems by Gaussian elimination:


2
3
1
1
2 −1
−3 −5 1




x
y
z

=


6
2
−7

,


1 1 1
1 1 2
1 2 3




x
y
z

=


6
9
14

.
Problem 7.2. Solve the following linear system by Gaussian elimination:




1
2 1 1
2
3 2 3
−1 0 1 −1
−2 −1 4 0








x1
x2
x3
x4



=




7
14
−1
2



.
Problem 7.3. Consider the matrix
A =


1 c 0
2 4 1
3 5 1

.
When applying Gaussian elimination, which value of c yields zero in the
second pivot position?
Which value of c yields zero in the third pivot
position? In this case, what can your say about the matrix A?
Problem 7.4. Solve the system




2 1 1 0
4 3 3 1
8 7 9 5
6 7 9 8








x1
x2
x3
x4



=




1
−1
−1
1




using the LU-factorization of Example 7.1.
Problem 7.5. Apply rref to the matrix
A2 =




1
2 1 1
2
3 2 3
−1 0 1 −1
−2 −1 3 0



.
Problem 7.6. Apply rref to the matrix




1 4 9 16
4 9 16 25
9 16 25 36
16 25 36 49



.

276
Gaussian Elimination, LU, Cholesky, Echelon Form
Problem 7.7. (1) Prove that the dimension of the subspace of 2 × 2 ma-
trices A, such that the sum of the entries of every row is the same (say c1)
and the sum of entries of every column is the same (say c2) is 2.
(2) Prove that the dimension of the subspace of 2 × 2 matrices A, such
that the sum of the entries of every row is the same (say c1), the sum of
entries of every column is the same (say c2), and c1 = c2 is also 2. Prove
that every such matrix is of the form
a b
b a

,
and give a basis for this subspace.
(3) Prove that the dimension of the subspace of 3 × 3 matrices A, such
that the sum of the entries of every row is the same (say c1), the sum of
entries of every column is the same (say c2), and c1 = c2 is 5. Begin by
showing that the above constraints are given by the set of equations






1 1
1 −1 −1 −1 0
0
0
0 0
0
1
1
1 −1 −1 −1
1 −1 0
1 −1 0
1 −1 0
0 1 −1 0
1 −1 0
1 −1
0 1
1 −1 0
0 −1 0
0





















a11
a12
a13
a21
a22
a23
a31
a32
a33















=






0
0
0
0
0






.
Prove that every matrix satisfying the above constraints is of the form


a + b −c
−a + c + e −b + c + d
−a −b + c + d + e
a
b
c
d
e

,
with a, b, c, d, e ∈R. Find a basis for this subspace. (Use the method to
ﬁnd a basis for the kernel of a matrix.)
Problem 7.8. If A is an n × n symmetric matrix and B is any n × n
invertible matrix, prove that A is positive deﬁnite iﬀB⊤AB is positive
deﬁnite.
Problem 7.9. (1) Consider the matrix
A4 =




2 −1 0
0
−1 2 −1 0
0 −1 2 −1
0
0 −1 2



.

7.17. Problems
277
Find three matrices of the form E2,1;β1, E3,2;β2, E4,3;β3, such that
E4,3;β3E3,2;β2E2,1;β1A4 = U4
where U4 is an upper triangular matrix. Compute
M = E4,3;β3E3,2;β2E2,1;β1
and check that
MA4 = U4 =




2 −1
0
0
0 3/2 −1
0
0 0 4/3 −1
0 0
0 5/4



.
(2) Now consider the matrix
A5 =






2 −1 0
0
0
−1 2 −1 0
0
0 −1 2 −1 0
0
0 −1 2 −1
0
0
0 −1 2






.
Find four matrices of the form E2,1;β1, E3,2;β2, E4,3;β3, E5,4;β4, such that
E5,4;β4E4,3;β3E3,2;β2E2,1;β1A5 = U5
where U5 is an upper triangular matrix. Compute
M = E5,4;β4E4,3;β3E3,2;β2E2,1;β1
and check that
MA5 = U5 =






2 −1
0
0
0
0 3/2 −1
0
0
0 0 4/3 −1
0
0 0
0 5/4 −1
0 0
0
0 6/5






.
(3) Write a Matlab program deﬁning the function Ematrix(n, i, j, b)
which is the n × n matrix that adds b times row j to row i. Also write
some Matlab code that produces an n × n matrix An generalizing the ma-
trices A4 and A5.
Use your program to ﬁgure out which ﬁve matrices Ei,j;β reduce A6 to
the upper triangular matrix
U6 =









2 −1
0
0
0
0
0 3/2 −1
0
0
0
0 0 4/3 −1
0
0
0 0
0 5/4 −1
0
0 0
0
0 6/5 −1
0 0
0
0
0 7/6









.

278
Gaussian Elimination, LU, Cholesky, Echelon Form
Also use your program to ﬁgure out which six matrices Ei,j;β reduce A7 to
the upper triangular matrix
U7 =











2 −1
0
0
0
0
0
0 3/2 −1
0
0
0
0
0 0 4/3 −1
0
0
0
0 0
0 5/4 −1
0
0
0 0
0
0 6/5 −1
0
0 0
0
0
0 7/6 −1
0 0
0
0
0
0 8/7











.
(4) Find the lower triangular matrices L6 and L7 such that
L6U6 = A6
and
L7U7 = A7.
(5) It is natural to conjecture that there are n −1 matrices of the form
Ei,j;β that reduce An to the upper triangular matrix
Un =













2 −1
0
0
0
0
0
0 3/2 −1
0
0
0
0
0 0 4/3 −1
0
0
0
0 0
0 5/4 −1 0
0
0 0
0
0 6/5 ...
...
...
...
...
...
... ...
−1
0 0
0
0
· · ·
0 (n + 1)/n













,
namely,
E2,1;1/2, E3,2;2/3, E4,3;3/4, · · · , En,n−1;(n−1)/n.
It is also natural to conjecture that the lower triangular matrix Ln such
that
LnUn = An
is given by
Ln = E2,1;−1/2E3,2;−2/3E4,3;−3/4 · · · En,n−1;−(n−1)/n,

7.17. Problems
279
that is,
Ln =













1
0
0
0
0
0
0
−1/2
1
0
0
0
0
0
0
−2/3
1
0
0
0
0
0
0
−3/4
1
0
0
0
0
0
0
−4/5 1
...
...
...
...
...
...
...
...
0
0
0
0
0
· · · −(n −1)/n 1













.
Prove the above conjectures.
(6) Prove that the last column of A−1
n
is





1/(n + 1)
2/(n + 1)
...
n/(n + 1)




.
Problem 7.10. (1) Let A be any invertible 2 × 2 matrix
A =
a b
c d

.
Prove that there is an invertible matrix S such that
SA =
1
0
0 ad −bc

,
where S is the product of at most four elementary matrices of the form
Ei,j;β.
Conclude that every matrix A in SL(2) (the group of invertible 2 × 2
matrices A with det(A) = +1) is the product of at most four elementary
matrices of the form Ei,j;β.
For any a ̸= 0, 1, give an explicit factorization as above for
A =
a
0
0 a−1

.
What is this decomposition for a = −1?
(2) Recall that a rotation matrix R (a member of the group SO(2)) is
a matrix of the form
R =
cos θ −sin θ
sin θ
cos θ

.

280
Gaussian Elimination, LU, Cholesky, Echelon Form
Prove that if θ ̸= kπ (with k ∈Z), any rotation matrix can be written as a
product
R = ULU,
where U is upper triangular and L is lower triangular of the form
U =
1 u
0 1

,
L =
1 0
v 1

.
Therefore, every plane rotation (except a ﬂip about the origin when
θ = π) can be written as the composition of three shear transformations!
Problem 7.11. (1) Recall that Ei,d is the diagonal matrix
Ei,d = diag(1, . . . , 1, d, 1, . . . , 1),
whose diagonal entries are all +1, except the (i, i)th entry which is equal
to d.
Given any n × n matrix A, for any pair (i, j) of distinct row indices
(1 ≤i, j ≤n), prove that there exist two elementary matrices E1(i, j) and
E2(i, j) of the form Ek,ℓ;β, such that
Ej,−1E1(i, j)E2(i, j)E1(i, j)A = P(i, j)A,
the matrix obtained from the matrix A by permuting row i and row j.
Equivalently, we have
E1(i, j)E2(i, j)E1(i, j)A = Ej,−1P(i, j)A,
the matrix obtained from A by permuting row i and row j and multiplying
row j by −1.
Prove that for every i = 2, . . . , n, there exist four elementary matrices
E3(i, d), E4(i, d), E5(i, d), E6(i, d) of the form Ek,ℓ;β, such that
E6(i, d)E5(i, d)E4(i, d)E3(i, d)En,d = Ei,d.
What happens when d = −1, that is, what kind of simpliﬁcations occur?
Prove that all permutation matrices can be written as products of ele-
mentary operations of the form Ek,ℓ;β and the operation En,−1.
(2) Prove that for every invertible n × n matrix A, there is a matrix S
such that
SA =
In−1 0
0
d

= En,d,
with d = det(A), and where S is a product of elementary matrices of the
form Ek,ℓ;β.

7.17. Problems
281
In particular, every matrix in SL(n) (the group of invertible n×n matri-
ces A with det(A) = +1) can be written as a product of elementary matrices
of the form Ek,ℓ;β. Prove that at most n(n + 1) −2 such transformations
are needed.
(3) Prove that every matrix in SL(n) can be written as a product of at
most (n −1)(max{n, 3} + 1) elementary matrices of the form Ek,ℓ;β.
Problem 7.12. A matrix A is called strictly column diagonally dominant
iﬀ
|aj j| >
n
X
i=1, i̸=j
|ai j|,
for j = 1, . . . , n
Prove that if A is strictly column diagonally dominant, then Gaussian
elimination with partial pivoting does not require pivoting, and A is invert-
ible.
Problem 7.13. (1) Find a lower triangular matrix E such that
E




1 0 0 0
1 1 0 0
1 2 1 0
1 3 3 1



=




1 0 0 0
0 1 0 0
0 1 1 0
0 1 2 1



.
(2) What is the eﬀect of the product (on the left) with
E4,3;−1E3,2;−1E4,3;−1E2,1;−1E3,2;−1E4,3;−1
on the matrix
Pa3 =




1 0 0 0
1 1 0 0
1 2 1 0
1 3 3 1



.
(3) Find the inverse of the matrix Pa3.
(4) Consider the (n + 1) × (n + 1) Pascal matrix Pan whose ith row is
given by the binomial coeﬃcients
i −1
j −1

,
with 1 ≤i ≤n + 1, 1 ≤j ≤n + 1, and with the usual convention that
0
0

= 1,
i
j

= 0
if
j > i.

282
Gaussian Elimination, LU, Cholesky, Echelon Form
The matrix Pa3 is shown in Question (c) and Pa4 is shown below:
Pa4 =






1 0 0 0 0
1 1 0 0 0
1 2 1 0 0
1 3 3 1 0
1 4 6 4 1






.
Find n elementary matrices Eik,jk;βk such that
Ein,jn;βn · · · Ei1,j1;β1Pan =
1
0
0 Pan−1

.
Use the above to prove that the inverse of Pan is the lower triangular
matrix whose ith row is given by the signed binomial coeﬃcients
(−1)i+j−2
i −1
j −1

,
with 1 ≤i ≤n + 1, 1 ≤j ≤n + 1. For example,
Pa−1
4
=






1
0
0
0 0
−1 1
0
0 0
1 −2 1
0 0
−1 3 −3 1 0
1 −4 6 −4 1






.
Hint. Given any n × n matrix A, multiplying A by the elementary matrix
Ei,j;β on the right yields the matrix AEi,j;β in which β times the ith column
is added to the jth column.
Problem 7.14. (1) Implement the method for converting a rectangular
matrix to reduced row echelon form in Matlab.
(2) Use the above method to ﬁnd the inverse of an invertible n × n
matrix A by applying it to the the n × 2n matrix [A I] obtained by adding
the n columns of the identity matrix to A.
(3) Consider the matrix
A =







1
2
3
4
· · ·
n
2
3
4
5
· · · n + 1
3
4
5
6
· · · n + 2
...
...
...
...
...
n n + 1 n + 2 n + 3 · · · 2n −1







.
Using your program, ﬁnd the row reduced echelon form of A for n =
4, . . . , 20.

7.17. Problems
283
Also run the Matlab rref function and compare results.
Your program probably disagrees with rref even for small values of n.
The problem is that some pivots are very small and the normalization step
(to make the pivot 1) causes roundoﬀerrors. Use a tolerance parameter to
ﬁx this problem.
What can you conjecture about the rank of A?
(4) Prove that the matrix A has the following row reduced form:
R =







1 0 −1 −2 · · · −(n −2)
0 1 2
3 · · ·
n −1
0 0 0
0 · · ·
0
... ...
...
...
...
0 0 0
0 · · ·
0







.
Deduce from the above that A has rank 2.
Hint. Some well chosen sequence of row operations.
(5) Use your program to show that if you add any number greater than
or equal to (2/25)n2 to every diagonal entry of A you get an invertible
matrix! In fact, running the Matlab fuction chol should tell you that these
matrices are SPD (symmetric, positive deﬁnite).
Problem 7.15. Let A be an n × n complex Hermitian positive deﬁnite
matrix. Prove that the lower-triangular matrix B with positive diagonal
entries such that A = BB∗is given by the following formulae: For j =
1, . . . , n,
bj j =
 
aj j −
j−1
X
k=1
|bj k|2
!1/2
,
and for i = j + 1, . . . , n (and j = 1, . . . , n −1)
bi j =
 
ai j −
j−1
X
k=1
bi kbj k
!
/bj j.
Problem 7.16. (Permutations and permutation matrices) A permutation
can be viewed as an operation permuting the rows of a matrix. For example,
the permutation
1 2 3 4
3 4 2 1


284
Gaussian Elimination, LU, Cholesky, Echelon Form
corresponds to the matrix
Pπ =




0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0



.
Observe that the matrix Pπ has a single 1 on every row and every
column, all other entries being zero, and that if we multiply any 4 × 4
matrix A by Pπ on the left, then the rows of A are permuted according to
the permutation π; that is, the π(i)th row of PπA is the ith row of A. For
example,
PπA =




0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0








a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a42 a43 a44



=




a41 a42 a43 a44
a31 a32 a33 a34
a11 a12 a13 a14
a21 a22 a23 a24



.
Equivalently, the ith row of PπA is the π−1(i)th row of A. In order for the
matrix Pπ to move the ith row of A to the π(i)th row, the π(i)th row of
Pπ must have a 1 in column i and zeros everywhere else; this means that
the ith column of Pπ contains the basis vector eπ(i), the vector that has a
1 in position π(i) and zeros everywhere else.
This is the general situation and it leads to the following deﬁnition.
Deﬁnition 7.8. Given any permutation π: [n] →[n], the permutation
matrix Pπ = (pij) representing π is the matrix given by
pij =
(
1
if i = π(j)
0
if i ̸= π(j);
equivalently, the jth column of Pπ is the basis vector eπ(j). A permutation
matrix P is any matrix of the form Pπ (where P is an n × n matrix, and
π: [n] →[n] is a permutation, for some n ≥1).
Remark: There is a confusing point about the notation for permutation
matrices. A permutation matrix P acts on a matrix A by multiplication
on the left by permuting the rows of A. As we said before, this means that
the π(i)th row of PπA is the ith row of A, or equivalently that the ith row
of PπA is the π−1(i)th row of A. But then observe that the row index of

7.17. Problems
285
the entries of the ith row of PA is π−1(i), and not π(i)! See the following
example:




0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0








a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a42 a43 a44



=




a41 a42 a43 a44
a31 a32 a33 a34
a11 a12 a13 a14
a21 a22 a23 a24



,
where
π−1(1) = 4
π−1(2) = 3
π−1(3) = 1
π−1(4) = 2.
Prove the following results
(1) Given any two permutations π1, π2 : [n] →[n], the permutation matrix
Pπ2◦π1 representing the composition of π1 and π2 is equal to the product
Pπ2Pπ1 of the permutation matrices Pπ1 and Pπ2 representing π1 and
π2; that is,
Pπ2◦π1 = Pπ2Pπ1.
(2) The matrix Pπ−1
1
representing the inverse of the permutation π1 is the
inverse P −1
π1
of the matrix Pπ1 representing the permutation π1; that
is,
Pπ−1
1
= P −1
π1 .
Furthermore,
P −1
π1 = (Pπ1)⊤.
(3) Prove that if P is the matrix associated with a transposition, then
det(P) = −1.
(4) Prove that if P is a permutation matrix, then det(P) = ±1.
(5) Use permutation matrices to give another proof of the fact that the
parity of the number of transpositions used to express a permutation
π depends only on π.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 8
Vector Norms and Matrix Norms
8.1
Normed Vector Spaces
In order to deﬁne how close two vectors or two matrices are, and in order
to deﬁne the convergence of sequences of vectors or matrices, we can use
the notion of a norm. Recall that R+ = {x ∈R | x ≥0}. Also recall that
if z = a + ib ∈C is a complex number, with a, b ∈R, then z = a −ib and
|z| =
√
zz =
√
a2 + b2 (|z| is the modulus of z).
Deﬁnition 8.1. Let E be a vector space over a ﬁeld K, where K is either
the ﬁeld R of reals, or the ﬁeld C of complex numbers. A norm on E is
a function ∥∥: E →R+, assigning a nonnegative real number ∥u∥to any
vector u ∈E, and satisfying the following conditions for all x, y, z ∈E and
λ ∈K:
(N1) ∥x∥≥0, and ∥x∥= 0 iﬀx = 0.
(positivity)
(N2) ∥λx∥= |λ| ∥x∥.
(homogeneity (or scaling))
(N3) ∥x + y∥≤∥x∥+ ∥y∥.
(triangle inequality)
A vector space E together with a norm ∥∥is called a normed vector
space.
By (N2), setting λ = −1, we obtain
∥−x∥= ∥(−1)x∥= | −1| ∥x∥= ∥x∥;
that is, ∥−x∥= ∥x∥. From (N3), we have
∥x∥= ∥x −y + y∥≤∥x −y∥+ ∥y∥,
which implies that
∥x∥−∥y∥≤∥x −y∥.
287

288
Vector Norms and Matrix Norms
By exchanging x and y and using the fact that by (N2),
∥y −x∥= ∥−(x −y)∥= ∥x −y∥,
we also have
∥y∥−∥x∥≤∥x −y∥.
Therefore,
|∥x∥−∥y∥| ≤∥x −y∥,
for all x, y ∈E.
(8.1)
Observe that setting λ = 0 in (N2), we deduce that ∥0∥= 0 without
assuming (N1). Then by setting y = 0 in (8.1), we obtain
|∥x∥| ≤∥x∥,
for all x ∈E.
Therefore, the condition ∥x∥≥0 in (N1) follows from (N2) and (N3), and
(N1) can be replaced by the weaker condition
(N1') For all x ∈E, if ∥x∥= 0, then x = 0,
A function ∥∥: E →R satisfying Axioms (N2) and (N3) is called a semi-
norm. From the above discussion, a seminorm also has the properties
∥x∥≥0 for all x ∈E, and ∥0∥= 0.
However, there may be nonzero vectors x ∈E such that ∥x∥= 0.
Let us give some examples of normed vector spaces.
Example 8.1.
(1) Let E = R, and ∥x∥= |x|, the absolute value of x.
(2) Let E = C, and ∥z∥= |z|, the modulus of z.
(3) Let E = Rn (or E = Cn). There are three standard norms. For every
(x1, . . . , xn) ∈E, we have the norm ∥x∥1, deﬁned such that,
∥x∥1 = |x1| + · · · + |xn|,
we have the Euclidean norm ∥x∥2, deﬁned such that,
∥x∥2 =
 |x1|2 + · · · + |xn|2 1
2 ,
and the sup-norm ∥x∥∞, deﬁned such that,
∥x∥∞= max{|xi| | 1 ≤i ≤n}.
More generally, we deﬁne the ℓp-norm (for p ≥1) by
∥x∥p = (|x1|p + · · · + |xn|p)1/p.
See Figures 8.1 through 8.4.

8.1. Normed Vector Spaces
289
K1
K0.5
0
0.5
1
K1
K0.5
0.5
1
Fig. 8.1
The top ﬁgure is {x ∈R2 | ∥x∥1 ≤1}, while the bottom ﬁgure is {x ∈R3 |
∥x∥1 ≤1}.
There are other norms besides the ℓp-norms. Here are some examples.
(1) For E = R2,
∥(u1, u2)∥= |u1| + 2|u2|.
See Figure 8.5.
(2) For E = R2,
∥(u1, u2)∥=
 (u1 + u2)2 + u2
1
1/2.
See Figure 8.6.
(3) For E = C2,
∥(u1, u2)∥= |u1 + iu2| + |u1 −iu2|.

290
Vector Norms and Matrix Norms
K1
K0.5
0
0.5
1
K1
K0.5
0.5
1
Fig. 8.2
The top ﬁgure is {x ∈R2 | ∥x∥2 ≤1}, while the bottom ﬁgure is {x ∈R3 |
∥x∥2 ≤1}.
The reader should check that they satisfy all the axioms of a norm.
Some work is required to show the triangle inequality for the ℓp-norm.
Proposition 8.1. If E = Cn or E = Rn, for every real number p ≥1, the
ℓp-norm is indeed a norm.
Proof. The cases p = 1 and p = ∞are easy and left to the reader. If
p > 1, then let q > 1 such that
1
p + 1
q = 1.

8.1. Normed Vector Spaces
291
K1
K0.5
0
0.5
1
K1
K0.5
0.5
1
Fig. 8.3
The top ﬁgure is {x ∈R2 | ∥x∥∞≤1}, while the bottom ﬁgure is {x ∈R3 |
∥x∥∞≤1}.
We will make use of the following fact: for all α, β ∈R, if α, β ≥0, then
αβ ≤αp
p + βq
q .
(8.2)
To prove the above inequality, we use the fact that the exponential function
t 7→et satisﬁes the following convexity inequality:
eθx+(1−θ)y ≤θex + (1 −θ)ey,
for all x, y ∈R and all θ with 0 ≤θ ≤1.
Since the case αβ = 0 is trivial, let us assume that α > 0 and β > 0. If
we replace θ by 1/p, x by p log α and y by q log β, then we get
e
1
p p log α+ 1
q q log β ≤1
pep log α + 1
q eq log β,

292
Vector Norms and Matrix Norms
K1
K0.5
0
0.5
1
K1
K0.5
0.5
1
Fig. 8.4
The relationships between the closed unit balls from the ℓ1-norm, the Euclidean
norm, and the sup-norm.
which simpliﬁes to
αβ ≤αp
p + βq
q ,
as claimed.
We will now prove that for any two vectors u, v ∈E, (where E is of
dimension n), we have
n

i=1
|uivi| ≤∥u∥p ∥v∥q .
(8.3)
Since the above is trivial if u = 0 or v = 0, let us assume that u ̸= 0 and

8.1. Normed Vector Spaces
293
Fig. 8.5
The unit closed unit ball {(u1, u2) ∈R2 | ∥(u1, u2)∥≤1}, where ∥(u1, u2)∥=
|u1| + 2|u2|.
v ̸= 0. Then Inequality (8.2) with α = |ui|/ ∥u∥p and β = |vi|/ ∥v∥q yields
|uivi|
∥u∥p ∥v∥q
≤|ui|p
p ∥u∥p
p
+ |vi|q
q ∥u∥q
q
,
for i = 1, . . . , n, and by summing up these inequalities, we get
n

i=1
|uivi| ≤∥u∥p ∥v∥q ,
as claimed. To ﬁnish the proof, we simply have to prove that property (N3)
holds, since (N1) and (N2) are clear. For i = 1, . . . , n, we can write
(|ui| + |vi|)p = |ui|(|ui| + |vi|)p−1 + |vi|(|ui| + |vi|)p−1,
so that by summing up these equations we get
n

i=1
(|ui| + |vi|)p =
n

i=1
|ui|(|ui| + |vi|)p−1 +
n

i=1
|vi|(|ui| + |vi|)p−1,
and using Inequality (8.3), with V ∈E where Vi = (|ui| + |vi|)p−1, we get
n

i=1
(|ui| + |vi|)p ≤∥u∥p ∥V ∥q + ∥v∥p ∥V ∥q
= (∥u∥p + ∥v∥p)

n

i=1
(|ui| + |vi|)(p−1)q
1/q
.

294
Vector Norms and Matrix Norms
Fig. 8.6
The unit closed unit ball {(u1, u2) ∈R2 | ∥(u1, u2)∥≤1}, where ∥(u1, u2)∥=

(u1 + u2)2 + u2
1
1/2.
However, 1/p+1/q = 1 implies pq = p+q, that is, (p−1)q = p, so we have
n

i=1
(|ui| + |vi|)p ≤(∥u∥p + ∥v∥p)

n

i=1
(|ui| + |vi|)p
1/q
,
which yields

n

i=1
(|ui| + |vi|)p
1−1/q
=

n

i=1
(|ui| + |vi|)p
1/p
≤∥u∥p + ∥v∥p .
Since |ui + vi| ≤|ui| + |vi|, the above implies the triangle inequality
∥u + v∥p ≤∥u∥p + ∥v∥p, as claimed.
For p > 1 and 1/p + 1/q = 1, the inequality
n

i=1
|uivi| ≤

n

i=1
|ui|p
1/p
n

i=1
|vi|q
1/q
is known as H¨older's inequality.
For p = 2, it is the Cauchy-Schwarz
inequality.
Actually, if we deﬁne the Hermitian inner product ⟨−, −⟩on Cn by
⟨u, v⟩=
n

i=1
uivi,

8.1. Normed Vector Spaces
295
where u = (u1, . . . , un) and v = (v1, . . . , vn), then
|⟨u, v⟩| ≤
n
X
i=1
|uivi| =
n
X
i=1
|uivi|,
so H¨older's inequality implies the following inequalities.
Corollary 8.1. (H¨older's inequalities) For any real numbers p, q, such that
p, q ≥1 and
1
p + 1
q = 1,
(with q = +∞if p = 1 and p = +∞if q = 1), we have the inequalities
n
X
i=1
|uivi| ≤

n
X
i=1
|ui|p
1/p
n
X
i=1
|vi|q
1/q
and
|⟨u, v⟩| ≤∥u∥p ∥v∥q ,
u, v ∈Cn.
For p = 2, this is the standard Cauchy-Schwarz inequality. The triangle
inequality for the ℓp-norm,

n
X
i=1
(|ui + vi|)p
1/p
≤

n
X
i=1
|ui|p
1/p
+

n
X
i=1
|vi|q
1/q
,
is known as Minkowski's inequality.
When we restrict the Hermitian inner product to real vectors, u, v ∈Rn,
we get the Euclidean inner product
⟨u, v⟩=
n
X
i=1
uivi.
It is very useful to observe that if we represent (as usual) u = (u1, . . . , un)
and v = (v1, . . . , vn) (in Rn) by column vectors, then their Euclidean inner
product is given by
⟨u, v⟩= u⊤v = v⊤u,
and when u, v ∈Cn, their Hermitian inner product is given by
⟨u, v⟩= v∗u = u∗v.
In particular, when u = v, in the complex case we get
∥u∥2
2 = u∗u,

296
Vector Norms and Matrix Norms
and in the real case this becomes
∥u∥2
2 = u⊤u.
As convenient as these notations are, we still recommend that you do not
abuse them; the notation ⟨u, v⟩is more intrinsic and still "works" when our
vector space is inﬁnite dimensional.
Remark: If 0 < p < 1, then x 7→∥x∥p is not a norm because the triangle
inequality fails. For example, consider x = (2, 0) and y = (0, 2). Then
x+y = (2, 2), and we have ∥x∥p = (2p+0p)1/p = 2, ∥y∥p = (0p+2p)1/p = 2,
and ∥x + y∥p = (2p + 2p)1/p = 2(p+1)/p. Thus
∥x + y∥p = 2(p+1)/p,
∥x∥p + ∥y∥p = 4 = 22.
Since 0 < p < 1, we have 2p < p + 1, that is, (p + 1)/p > 2, so 2(p+1)/p >
22 = 4, and the triangle inequality ∥x + y∥p ≤∥x∥p + ∥y∥p fails.
Observe that
∥(1/2)x∥p = (1/2) ∥x∥p = ∥(1/2)y∥p = (1/2) ∥y∥p = 1,
∥(1/2)(x + y)∥p = 21/p,
and since p < 1, we have 21/p > 2, so
∥(1/2)(x + y)∥p = 21/p > 2 = (1/2) ∥x∥p + (1/2) ∥y∥p ,
and the map x 7→∥x∥p is not convex.
For p = 0, for any x ∈Rn, we have
∥x∥0 = |{i ∈{1, . . . , n} | xi ̸= 0}|,
the number of nonzero components of x. The map x 7→∥x∥0 is not a norm
this time because Axiom (N2) fails. For example,
∥(1, 0)∥0 = ∥(10, 0)∥0 = 1 ̸= 10 = 10 ∥(1, 0)∥0 .
The map x 7→∥x∥0 is also not convex. For example,
∥(1/2)(2, 2)∥0 = ∥(1, 1)∥0 = 2,
and
∥(2, 0)∥0 = ∥(0, 2)∥0 = 1,
but
∥(1/2)(2, 2)∥0 = 2 > 1 = (1/2) ∥(2, 0)∥0 + (1/2) ∥(0, 2)∥0 .

8.1. Normed Vector Spaces
297
Nevertheless, the "zero-norm" x 7→∥x∥0 is used in machine learning as
a regularizing term which encourages sparsity, namely increases the number
of zero components of the vector x.
The following proposition is easy to show.
Proposition 8.2. The following inequalities hold for all x ∈Rn (or x ∈
Cn):
∥x∥∞≤∥x∥1 ≤n∥x∥∞,
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
∥x∥2 ≤∥x∥1 ≤√n∥x∥2.
Proposition 8.2 is actually a special case of a very important result: in
a ﬁnite-dimensional vector space, any two norms are equivalent.
Deﬁnition 8.2. Given any (real or complex) vector space E, two norms
∥∥a and ∥∥b are equivalent iﬀthere exists some positive reals C1, C2 > 0,
such that
∥u∥a ≤C1 ∥u∥b
and
∥u∥b ≤C2 ∥u∥a , for all u ∈E.
Given any norm ∥∥on a vector space of dimension n, for any basis
(e1, . . . , en) of E, observe that for any vector x = x1e1 + · · · + xnen, we
have
∥x∥= ∥x1e1 + · · · + xnen∥≤|x1| ∥e1∥+ · · · + |xn| ∥en∥
≤C(|x1| + · · · + |xn|) = C ∥x∥1 ,
with C = max1≤i≤n ∥ei∥and with the norm ∥x∥1 deﬁned as
∥x∥1 = ∥x1e1 + · · · + xnen∥= |x1| + · · · + |xn|.
The above implies that
| ∥u∥−∥v∥| ≤∥u −v∥≤C ∥u −v∥1 ,
and this implies the following corollary.
Corollary 8.2. For any norm u 7→∥u∥on a ﬁnite-dimensional (complex
or real) vector space E, the map u 7→∥u∥is continuous with respect to the
norm ∥∥1.
Let Sn−1
1
be the unit sphere with respect to the norm ∥∥1, namely
Sn−1
1
= {x ∈E | ∥x∥1 = 1}.

298
Vector Norms and Matrix Norms
Now Sn−1
1
is a closed and bounded subset of a ﬁnite-dimensional vector
space, so by Heine-Borel (or equivalently, by Bolzano-Weiertrass), Sn−1
1
is
compact. On the other hand, it is a well known result of analysis that any
continuous real-valued function on a nonempty compact set has a minimum
and a maximum, and that they are achieved. Using these facts, we can
prove the following important theorem:
Theorem 8.1. If E is any real or complex vector space of ﬁnite dimension,
then any two norms on E are equivalent.
Proof. It is enough to prove that any norm ∥∥is equivalent to the 1-norm.
We already proved that the function x 7→∥x∥is continuous with respect
to the norm ∥∥1, and we observed that the unit sphere Sn−1
1
is compact.
Now we just recalled that because the function f : x 7→∥x∥is continuous
and because Sn−1
1
is compact, the function f has a minimum m and a
maximum M, and because ∥x∥is never zero on Sn−1
1
, we must have m > 0.
Consequently, we just proved that if ∥x∥1 = 1, then
0 < m ≤∥x∥≤M,
so for any x ∈E with x ̸= 0, we get
m ≤∥x/ ∥x∥1∥≤M,
which implies
m ∥x∥1 ≤∥x∥≤M ∥x∥1 .
Since the above inequality holds trivially if x = 0, we just proved that ∥∥
and ∥∥1 are equivalent, as claimed.
Remark: Let P be a n × n symmetric positive deﬁnite matrix. It is im-
mediately veriﬁed that the map x 7→∥x∥P given by
∥x∥P = (x⊤Px)1/2
is a norm on Rn called a quadratic norm. Using some convex analysis (the
L¨owner-John ellipsoid), it can be shown that any norm ∥∥on Rn can be
approximated by a quadratic norm in the sense that there is a quadratic
norm ∥∥P such that
∥x∥P ≤∥x∥≤√n ∥x∥P
for all x ∈Rn;
see Boyd and Vandenberghe [Boyd and Vandenberghe (2004)], Section 8.4.1.
Next we will consider norms on matrices.

8.2. Matrix Norms
299
8.2
Matrix Norms
For simplicity of exposition, we will consider the vector spaces Mn(R) and
Mn(C) of square n × n matrices.
Most results also hold for the spaces
Mm,n(R) and Mm,n(C) of rectangular m×n matrices. Since n×n matrices
can be multiplied, the idea behind matrix norms is that they should behave
"well" with respect to matrix multiplication.
Deﬁnition 8.3. A matrix norm ∥∥on the space of square n × n matrices
in Mn(K), with K = R or K = C, is a norm on the vector space Mn(K),
with the additional property called submultiplicativity that
∥AB∥≤∥A∥∥B∥,
for all A, B ∈Mn(K). A norm on matrices satisfying the above property
is often called a submultiplicative matrix norm.
Since I2 = I, from ∥I∥=
I2 ≤∥I∥2, we get ∥I∥≥1, for every matrix
norm.
Before giving examples of matrix norms, we need to review some basic
deﬁnitions about matrices. Given any matrix A = (aij) ∈Mm,n(C), the
conjugate A of A is the matrix such that
Aij = aij,
1 ≤i ≤m, 1 ≤j ≤n.
The transpose of A is the n × m matrix A⊤such that
A⊤
ij = aji,
1 ≤i ≤m, 1 ≤j ≤n.
The adjoint of A is the n × m matrix A∗such that
A∗= (A⊤) = (A)⊤.
When A is a real matrix, A∗= A⊤. A matrix A ∈Mn(C) is Hermitian if
A∗= A.
If A is a real matrix (A ∈Mn(R)), we say that A is symmetric if
A⊤= A.
A matrix A ∈Mn(C) is normal if
AA∗= A∗A,
and if A is a real matrix, it is normal if
AA⊤= A⊤A.

300
Vector Norms and Matrix Norms
A matrix U ∈Mn(C) is unitary if
UU ∗= U ∗U = I.
A real matrix Q ∈Mn(R) is orthogonal if
QQ⊤= Q⊤Q = I.
Given any matrix A = (aij) ∈Mn(C), the trace tr(A) of A is the sum
of its diagonal elements
tr(A) = a11 + · · · + ann.
It is easy to show that the trace is a linear map, so that
tr(λA) = λtr(A)
and
tr(A + B) = tr(A) + tr(B).
Moreover, if A is an m × n matrix and B is an n × m matrix, it is not hard
to show that
tr(AB) = tr(BA).
We also review eigenvalues and eigenvectors. We content ourselves with
deﬁnition involving matrices. A more general treatment will be given later
on (see Chapter 14).
Deﬁnition 8.4. Given any square matrix A ∈Mn(C), a complex number
λ ∈C is an eigenvalue of A if there is some nonzero vector u ∈Cn, such
that
Au = λu.
If λ is an eigenvalue of A, then the nonzero vectors u ∈Cn such that
Au = λu are called eigenvectors of A associated with λ; together with the
zero vector, these eigenvectors form a subspace of Cn denoted by Eλ(A),
and called the eigenspace associated with λ.
Remark: Note that Deﬁnition 8.4 requires an eigenvector to be nonzero.
A somewhat unfortunate consequence of this requirement is that the set
of eigenvectors is not a subspace, since the zero vector is missing! On the
positive side, whenever eigenvectors are involved, there is no need to say
that they are nonzero. The fact that eigenvectors are nonzero is implicitly

8.2. Matrix Norms
301
used in all the arguments involving them, so it seems safer (but perhaps
not as elegant) to stipulate that eigenvectors should be nonzero.
If A is a square real matrix A ∈Mn(R), then we restrict Deﬁnition 8.4
to real eigenvalues λ ∈R and real eigenvectors.
However, it should be
noted that although every complex matrix always has at least some complex
eigenvalue, a real matrix may not have any real eigenvalues. For example,
the matrix
A =
0 −1
1 0

has the complex eigenvalues i and −i, but no real eigenvalues. Thus, typi-
cally even for real matrices, we consider complex eigenvalues.
Observe that λ ∈C is an eigenvalue of A
• iﬀAu = λu for some nonzero vector u ∈Cn
• iﬀ(λI −A)u = 0
• iﬀthe matrix λI −A deﬁnes a linear map which has a nonzero kernel,
that is,
• iﬀλI −A not invertible.
However, from Proposition 6.7, λI −A is not invertible iﬀ
det(λI −A) = 0.
Now det(λI −A) is a polynomial of degree n in the indeterminate λ, in
fact, of the form
λn −tr(A)λn−1 + · · · + (−1)n det(A).
Thus we see that the eigenvalues of A are the zeros (also called roots) of the
above polynomial. Since every complex polynomial of degree n has exactly
n roots, counted with their multiplicity, we have the following deﬁnition:
Deﬁnition 8.5. Given any square n×n matrix A ∈Mn(C), the polynomial
det(λI −A) = λn −tr(A)λn−1 + · · · + (−1)n det(A)
is called the characteristic polynomial of A. The n (not necessarily distinct)
roots λ1, . . . , λn of the characteristic polynomial are all the eigenvalues of
A and constitute the spectrum of A. We let
ρ(A) = max
1≤i≤n |λi|
be the largest modulus of the eigenvalues of A, called the spectral radius of
A.

302
Vector Norms and Matrix Norms
Since the eigenvalue λ1, . . . , λn of A are the zeros of the polynomial
det(λI −A) = λn −tr(A)λn−1 + · · · + (−1)n det(A),
we deduce (see Section 14.1 for details) that
tr(A) = λ1 + · · · + λn
det(A) = λ1 · · · λn.
Proposition 8.3. For any matrix norm ∥∥on Mn(C) and for any square
n × n matrix A ∈Mn(C), we have
ρ(A) ≤∥A∥.
Proof. Let λ be some eigenvalue of A for which |λ| is maximum, that is,
such that |λ| = ρ(A). If u (̸= 0) is any eigenvector associated with λ and if
U is the n × n matrix whose columns are all u, then Au = λu implies
AU = λU,
and since
|λ| ∥U∥= ∥λU∥= ∥AU∥≤∥A∥∥U∥
and U ̸= 0, we have ∥U∦= 0, and get
ρ(A) = |λ| ≤∥A∥,
as claimed.
Proposition 8.3 also holds for any real matrix norm ∥∥on Mn(R) but
the proof is more subtle and requires the notion of induced norm. We prove
it after giving Deﬁnition 8.7.
It turns out that if A is a real n × n symmetric matrix, then the eigen-
values of A are all real and there is some orthogonal matrix Q such that
A = Qdiag(λ1, . . . , λn)Q⊤,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if
any) are its diagonal entries, which are the (real) eigenvalues of A. Similarly,
if A is a complex n × n Hermitian matrix, then the eigenvalues of A are all
real and there is some unitary matrix U such that
A = Udiag(λ1, . . . , λn)U ∗,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if
any) are its diagonal entries, which are the (real) eigenvalues of A. See
Chapter 16 for the proof of these results.

8.2. Matrix Norms
303
We now return to matrix norms. We begin with the so-called Frobenius
norm, which is just the norm ∥∥2 on Cn2, where the n × n matrix A is
viewed as the vector obtained by concatenating together the rows (or the
columns) of A. The reader should check that for any n × n complex matrix
A = (aij),

n
X
i,j=1
|aij|2
1/2
=
p
tr(A∗A) =
p
tr(AA∗).
Deﬁnition 8.6. The Frobenius norm ∥∥F is deﬁned so that for every
square n × n matrix A ∈Mn(C),
∥A∥F =

n
X
i,j=1
|aij|2
1/2
=
p
tr(AA∗) =
p
tr(A∗A).
The following proposition show that the Frobenius norm is a matrix
norm satisfying other nice properties.
Proposition 8.4. The Frobenius norm ∥∥F on Mn(C) satisﬁes the follow-
ing properties:
(1) It is a matrix norm; that is, ∥AB∥F ≤∥A∥F ∥B∥F , for all A, B ∈
Mn(C).
(2) It is unitarily invariant, which means that for all unitary matrices U, V ,
we have
∥A∥F = ∥UA∥F = ∥AV ∥F = ∥UAV ∥F .
(3)
p
ρ(A∗A) ≤∥A∥F ≤√n
p
ρ(A∗A), for all A ∈Mn(C).
Proof. (1) The only property that requires a proof is the fact ∥AB∥F ≤
∥A∥F ∥B∥F . This follows from the Cauchy-Schwarz inequality:
∥AB∥2
F =
n
X
i,j=1

n
X
k=1
aikbkj

2
≤
n
X
i,j=1

n
X
h=1
|aih|2

n
X
k=1
|bkj|2

=

n
X
i,h=1
|aih|2

n
X
k,j=1
|bkj|2

= ∥A∥2
F ∥B∥2
F .
(2) We have
∥A∥2
F = tr(A∗A) = tr(V V ∗A∗A) = tr(V ∗A∗AV ) = ∥AV ∥2
F ,

304
Vector Norms and Matrix Norms
and
∥A∥2
F = tr(A∗A) = tr(A∗U ∗UA) = ∥UA∥2
F .
The identity
∥A∥F = ∥UAV ∥F
follows from the previous two.
(3) It is well known that the trace of a matrix is equal to the sum
of its eigenvalues.
Furthermore, A∗A is symmetric positive semideﬁnite
(which means that its eigenvalues are nonnegative), so ρ(A∗A) is the largest
eigenvalue of A∗A and
ρ(A∗A) ≤tr(A∗A) ≤nρ(A∗A),
which yields (3) by taking square roots.
Remark: The Frobenius norm is also known as the Hilbert-Schmidt norm
or the Schur norm. So many famous names associated with such a simple
thing!
8.3
Subordinate Norms
We now give another method for obtaining matrix norms using subordinate
norms. First we need a proposition that shows that in a ﬁnite-dimensional
space, the linear map induced by a matrix is bounded, and thus continuous.
Proposition 8.5. For every norm ∥∥on Cn (or Rn), for every matrix
A ∈Mn(C) (or A ∈Mn(R)), there is a real constant CA ≥0, such that
∥Au∥≤CA ∥u∥,
for every vector u ∈Cn (or u ∈Rn if A is real).
Proof. For every basis (e1, . . . , en) of Cn (or Rn), for every vector u =
u1e1 + · · · + unen, we have
∥Au∥= ∥u1A(e1) + · · · + unA(en)∥
≤|u1| ∥A(e1)∥+ · · · + |un| ∥A(en)∥
≤C1(|u1| + · · · + |un|) = C1 ∥u∥1 ,
where C1 = max1≤i≤n ∥A(ei)∥. By Theorem 8.1, the norms ∥∥and ∥∥1
are equivalent, so there is some constant C2 > 0 so that ∥u∥1 ≤C2 ∥u∥for
all u, which implies that
∥Au∥≤CA ∥u∥,
where CA = C1C2.

8.3. Subordinate Norms
305
Proposition 8.5 says that every linear map on a ﬁnite-dimensional space
is bounded.
This implies that every linear map on a ﬁnite-dimensional
space is continuous. Actually, it is not hard to show that a linear map on
a normed vector space E is bounded iﬀit is continuous, regardless of the
dimension of E.
Proposition 8.5 implies that for every matrix A ∈Mn(C) (or A ∈
Mn(R)),
sup
x∈Cn
x̸=0
∥Ax∥
∥x∥≤CA.
Since ∥λu∥= |λ| ∥u∥, for every nonzero vector x, we have
∥Ax∥
∥x∥= ∥x∥∥A(x/ ∥x∥)∥
∥x∥
= ∥A(x/ ∥x∥)∥,
which implies that
sup
x∈Cn
x̸=0
∥Ax∥
∥x∥= sup
x∈Cn
∥x∥=1
∥Ax∥.
Similarly
sup
x∈Rn
x̸=0
∥Ax∥
∥x∥= sup
x∈Rn
∥x∥=1
∥Ax∥.
The above considerations justify the following deﬁnition.
Deﬁnition 8.7. If ∥∥is any norm on Cn, we deﬁne the function ∥∥op on
Mn(C) by
∥A∥op = sup
x∈Cn
x̸=0
∥Ax∥
∥x∥= sup
x∈Cn
∥x∥=1
∥Ax∥.
The function A 7→∥A∥op is called the subordinate matrix norm or op-
erator norm induced by the norm ∥∥.
Another notation for the operator norm of a matrix A (in particular,
used by Horn and Johnson [Horn and Johnson (1990)]), is |||A|||.
It is easy to check that the function A 7→∥A∥op is indeed a norm, and
by deﬁnition, it satisﬁes the property
∥Ax∥≤∥A∥op ∥x∥,
for all x ∈Cn.

306
Vector Norms and Matrix Norms
A norm ∥∥op on Mn(C) satisfying the above property is said to be sub-
ordinate to the vector norm ∥∥on Cn. As a consequence of the above
inequality, we have
∥ABx∥≤∥A∥op ∥Bx∥≤∥A∥op ∥B∥op ∥x∥,
for all x ∈Cn, which implies that
∥AB∥op ≤∥A∥op ∥B∥op
for all A, B ∈Mn(C),
showing that A 7→∥A∥op is a matrix norm (it is submultiplicative).
Observe that the operator norm is also deﬁned by
∥A∥op = inf{λ ∈R | ∥Ax∥≤λ ∥x∥, for all x ∈Cn}.
Since the function x 7→∥Ax∥is continuous (because | ∥Ay∥−∥Ax∥| ≤
∥Ay −Ax∥≤CA ∥x −y∥) and the unit sphere Sn−1 = {x ∈Cn | ∥x∥= 1}
is compact, there is some x ∈Cn such that ∥x∥= 1 and
∥Ax∥= ∥A∥op .
Equivalently, there is some x ∈Cn such that x ̸= 0 and
∥Ax∥= ∥A∥op ∥x∥.
The deﬁnition of an operator norm also implies that
∥I∥op = 1.
The above shows that the Frobenius norm is not a subordinate matrix norm
(why?).
If ∥∥is a vector norm on Cn, the operator norm ∥∥op that it induces
applies to matrices in Mn(C).
If we are careful to denote vectors and
matrices so that no confusion arises, for example, by using lower case letters
for vectors and upper case letters for matrices, it should be clear that ∥A∥op
is the operator norm of the matrix A and that ∥x∥is the vector norm of
x. Consequently, following common practice to alleviate notation, we will
drop the subscript "op" and simply write ∥A∥instead of ∥A∥op.
The notion of subordinate norm can be slightly generalized.
Deﬁnition 8.8. If K = R or K = C, for any norm ∥∥on Mm,n(K), and
for any two norms ∥∥a on Kn and ∥∥b on Km, we say that the norm ∥∥
is subordinate to the norms ∥∥a and ∥∥b if
∥Ax∥b ≤∥A∥∥x∥a
for all A ∈Mm,n(K) and all x ∈Kn.

8.3. Subordinate Norms
307
Remark: For any norm ∥∥on Cn, we can deﬁne the function ∥∥R on
Mn(R) by
∥A∥R = sup
x∈Rn
x̸=0
∥Ax∥
∥x∥= sup
x∈Rn
∥x∥=1
∥Ax∥.
The function A 7→∥A∥R is a matrix norm on Mn(R), and
∥A∥R ≤∥A∥,
for all real matrices A ∈Mn(R). However, it is possible to construct vector
norms ∥∥on Cn and real matrices A such that
∥A∥R < ∥A∥.
In order to avoid this kind of diﬃculties, we deﬁne subordinate matrix
norms over Mn(C). Luckily, it turns out that ∥A∥R = ∥A∥for the vector
norms, ∥∥1 , ∥∥2, and ∥∥∞.
We now prove Proposition 8.3 for real matrix norms.
Proposition 8.6. For any matrix norm ∥∥on Mn(R) and for any square
n × n matrix A ∈Mn(R), we have
ρ(A) ≤∥A∥.
Proof. We follow the proof in Denis Serre's book [Serre (2010)]. If A is
a real matrix, the problem is that the eigenvectors associated with the
eigenvalue of maximum modulus may be complex. We use a trick based on
the fact that for every matrix A (real or complex),
ρ(Ak) = (ρ(A))k,
which is left as an exercise (use Proposition 14.4 which shows that
if (λ1, . . . , λn) are the (not necessarily distinct) eigenvalues of A, then
(λk
1, . . . , λk
n) are the eigenvalues of Ak, for k ≥1).
Pick any complex matrix norm ∥∥c on Cn (for example, the Frobenius
norm, or any subordinate matrix norm induced by a norm on Cn). The
restriction of ∥∥c to real matrices is a real norm that we also denote by
∥∥c. Now by Theorem 8.1, since Mn(R) has ﬁnite dimension n2, there is
some constant C > 0 so that
∥B∥c ≤C ∥B∥,
for all
B ∈Mn(R).
Furthermore, for every k ≥1 and for every real n×n matrix A, by Proposi-
tion 8.3, ρ(Ak) ≤
Ak
c, and because ∥∥is a matrix norm,
Ak ≤∥A∥k,
so we have
(ρ(A))k = ρ(Ak) ≤
Ak
c ≤C
Ak ≤C ∥A∥k ,

308
Vector Norms and Matrix Norms
for all k ≥1. It follows that
ρ(A) ≤C1/k ∥A∥,
for all
k ≥1.
However because C
>
0,
we have limk7→∞C1/k
=
1 (we have
limk7→∞1
k log(C) = 0). Therefore, we conclude that
ρ(A) ≤∥A∥,
as desired.
We now determine explicitly what are the subordinate matrix norms
associated with the vector norms ∥∥1 , ∥∥2, and ∥∥∞.
Proposition 8.7. For every square matrix A = (aij) ∈Mn(C), we have
∥A∥1 =
sup
x∈Cn
∥x∥1=1
∥Ax∥1 = max
j
n
X
i=1
|aij|
∥A∥∞=
sup
x∈Cn
∥x∥∞=1
∥Ax∥∞= max
i
n
X
j=1
|aij|
∥A∥2 =
sup
x∈Cn
∥x∥2=1
∥Ax∥2 =
p
ρ(A∗A) =
p
ρ(AA∗).
Note that ∥A∥1 is the maximum of the ℓ1-norms of the columns of A and
∥A∥∞is the maximum of the ℓ1-norms of the rows of A. Furthermore,
∥A∗∥2 = ∥A∥2, the norm ∥∥2 is unitarily invariant, which means that
∥A∥2 = ∥UAV ∥2
for all unitary matrices U, V , and if A is a normal matrix, then ∥A∥2 =
ρ(A).
Proof. For every vector u, we have
∥Au∥1 =
X
i

X
j
aijuj
 ≤
X
j
|uj|
X
i
|aij| ≤

max
j
X
i
|aij|

∥u∥1 ,
which implies that
∥A∥1 ≤max
j
n
X
i=1
|aij|.
It remains to show that equality can be achieved. For this let j0 be some
index such that
max
j
X
i
|aij| =
X
i
|aij0|,

8.3. Subordinate Norms
309
and let ui = 0 for all i ̸= j0 and uj0 = 1.
In a similar way, we have
∥Au∥∞= max
i

X
j
aijuj
 ≤

max
i
X
j
|aij|

∥u∥∞,
which implies that
∥A∥∞≤max
i
n
X
j=1
|aij|.
To achieve equality, let i0 be some index such that
max
i
X
j
|aij| =
X
j
|ai0j|.
The reader should check that the vector given by
uj =
( ai0j
|ai0j|
if ai0j ̸= 0
1
if ai0j = 0
works.
We have
∥A∥2
2 = sup
x∈Cn
x∗x=1
∥Ax∥2
2 = sup
x∈Cn
x∗x=1
x∗A∗Ax.
Since the matrix A∗A is symmetric, it has real eigenvalues and it can be
diagonalized with respect to a unitary matrix. These facts can be used to
prove that the function x 7→x∗A∗Ax has a maximum on the sphere x∗x = 1
equal to the largest eigenvalue of A∗A, namely, ρ(A∗A). We postpone the
proof until we discuss optimizing quadratic functions. Therefore,
∥A∥2 =
p
ρ(A∗A).
Let us now prove that ρ(A∗A) = ρ(AA∗). First assume that ρ(A∗A) > 0.
In this case, there is some eigenvector u (̸= 0) such that
A∗Au = ρ(A∗A)u,
and since ρ(A∗A) > 0, we must have Au ̸= 0. Since Au ̸= 0,
AA∗(Au) = A(A∗Au) = ρ(A∗A)Au
which means that ρ(A∗A) is an eigenvalue of AA∗, and thus
ρ(A∗A) ≤ρ(AA∗).
Because (A∗)∗= A, by replacing A by A∗, we get
ρ(AA∗) ≤ρ(A∗A),

310
Vector Norms and Matrix Norms
and so ρ(A∗A) = ρ(AA∗).
If ρ(A∗A) = 0, then we must have ρ(AA∗) = 0, since otherwise by the
previous reasoning we would have ρ(A∗A) = ρ(AA∗) > 0. Hence, in all
case
∥A∥2
2 = ρ(A∗A) = ρ(AA∗) = ∥A∗∥2
2 .
For any unitary matrices U and V , it is an easy exercise to prove that
V ∗A∗AV and A∗A have the same eigenvalues, so
∥A∥2
2 = ρ(A∗A) = ρ(V ∗A∗AV ) = ∥AV ∥2
2 ,
and also
∥A∥2
2 = ρ(A∗A) = ρ(A∗U ∗UA) = ∥UA∥2
2 .
Finally, if A is a normal matrix (AA∗= A∗A), it can be shown that there
is some unitary matrix U so that
A = UDU ∗,
where D = diag(λ1, . . . , λn) is a diagonal matrix consisting of the eigenval-
ues of A, and thus
A∗A = (UDU ∗)∗UDU ∗= UD∗U ∗UDU ∗= UD∗DU ∗.
However, D∗D = diag(|λ1|2, . . . , |λn|2), which proves that
ρ(A∗A) = ρ(D∗D) = max
i
|λi|2 = (ρ(A))2,
so that ∥A∥2 = ρ(A).
Deﬁnition 8.9. For A = (aij) ∈Mn(C), the norm ∥A∥2 = is often called
the spectral norm.
Observe that Property (3) of Proposition 8.4 says that
∥A∥2 ≤∥A∥F ≤√n ∥A∥2 ,
which shows that the Frobenius norm is an upper bound on the spectral
norm. The Frobenius norm is much easier to compute than the spectral
norm.
The reader will check that the above proof still holds if the matrix A is
real (change unitary to orthogonal), conﬁrming the fact that ∥A∥R = ∥A∥
for the vector norms ∥∥1 , ∥∥2, and ∥∥∞. It is also easy to verify that the
proof goes through for rectangular m×n matrices, with the same formulae.
Similarly, the Frobenius norm given by
∥A∥F =
 m
X
i=1
n
X
j=1
|aij|2
1/2
=
p
tr(A∗A) =
p
tr(AA∗)

8.3. Subordinate Norms
311
is also a norm on rectangular matrices. For these norms, whenever AB
makes sense, we have
∥AB∥≤∥A∥∥B∥.
Remark: It can be shown that for any two real numbers p, q ≥1 such that
1
p + 1
q = 1, we have
∥A∗∥q = ∥A∥p = sup{ℜ(y∗Ax) | ∥x∥p = 1, ∥y∥q = 1}
= sup{|⟨Ax, y⟩| | ∥x∥p = 1, ∥y∥q = 1},
where ∥A∗∥q and ∥A∥p are the operator norms.
Remark: Let (E, ∥∥) and (F, ∥∥) be two normed vector spaces (for sim-
plicity of notation, we use the same symbol ∥∥for the norms on E and F;
this should not cause any confusion). Recall that a function f : E →F is
continuous if for every a ∈E, for every ϵ > 0, there is some η > 0 such
that for all x ∈E,
if
∥x −a∥≤η
then
∥f(x) −f(a)∥≤ϵ.
It is not hard to show that a linear map f : E →F is continuous iﬀthere
is some constant C ≥0 such that
∥f(x)∥≤C ∥x∥for all x ∈E.
If so, we say that f is bounded (or a linear bounded operator).
We let
L(E; F) denote the set of all continuous (equivalently, bounded) linear maps
from E to F. Then we can deﬁne the operator norm (or subordinate norm)
∥∥on L(E; F) as follows: for every f ∈L(E; F),
∥f∥= sup
x∈E
x̸=0
∥f(x)∥
∥x∥
= sup
x∈E
∥x∥=1
∥f(x)∥,
or equivalently by
∥f∥= inf{λ ∈R | ∥f(x)∥≤λ ∥x∥, for all x ∈E}.
It is not hard to show that the map f 7→∥f∥is a norm on L(E; F) satisfying
the property
∥f(x)∥≤∥f∥∥x∥
for all x ∈E, and that if f ∈L(E; F) and g ∈L(F; G), then
∥g ◦f∥≤∥g∥∥f∥.
Operator norms play an important role in functional analysis, especially
when the spaces E and F are complete.

312
Vector Norms and Matrix Norms
8.4
Inequalities Involving Subordinate Norms
In this section we discuss two technical inequalities which will be needed
for certain proofs in the last three sections of this chapter. First we prove a
proposition which will be needed when we deal with the condition number
of a matrix.
Proposition 8.8. Let ∥∥be any matrix norm, and let B ∈Mn(C) such
that ∥B∥< 1.
(1) If ∥∥is a subordinate matrix norm, then the matrix I + B is invertible
and
(I + B)−1 ≤
1
1 −∥B∥.
(2) If a matrix of the form I +B is singular, then ∥B∥≥1 for every matrix
norm (not necessarily subordinate).
Proof. (1) Observe that (I + B)u = 0 implies Bu = −u, so
∥u∥= ∥Bu∥.
Recall that
∥Bu∥≤∥B∥∥u∥
for every subordinate norm. Since ∥B∥< 1, if u ̸= 0, then
∥Bu∥< ∥u∥,
which contradicts ∥u∥= ∥Bu∥. Therefore, we must have u = 0, which
proves that I + B is injective, and thus bijective, i.e., invertible. Then we
have
(I + B)−1 + B(I + B)−1 = (I + B)(I + B)−1 = I,
so we get
(I + B)−1 = I −B(I + B)−1,
which yields
(I + B)−1 ≤1 + ∥B∥
(I + B)−1 ,
and ﬁnally,
(I + B)−1 ≤
1
1 −∥B∥.
(2) If I+B is singular, then −1 is an eigenvalue of B, and by Proposition 8.3,
we get ρ(B) ≤∥B∥, which implies 1 ≤ρ(B) ≤∥B∥.

8.4. Inequalities Involving Subordinate Norms
313
The second inequality is a result is that is needed to deal with the
convergence of sequences of powers of matrices.
Proposition 8.9. For every matrix A ∈Mn(C) and for every ϵ > 0, there
is some subordinate matrix norm ∥∥such that
∥A∥≤ρ(A) + ϵ.
Proof. By Theorem 14.1, there exists some invertible matrix U and some
upper triangular matrix T such that
A = UTU −1,
and say that
T =







λ1 t12 t13
· · ·
t1n
0 λ2 t23
· · ·
t2n
...
...
...
...
...
0
0 · · · λn−1 tn−1 n
0
0 · · ·
0
λn







,
where λ1, . . . , λn are the eigenvalues of A.
For every δ ̸= 0, deﬁne the
diagonal matrix
Dδ = diag(1, δ, δ2, . . . , δn−1),
and consider the matrix
(UDδ)−1A(UDδ) = D−1
δ TDδ =







λ1 δt12 δ2t13 · · ·
δn−1t1n
0
λ2
δt23
· · ·
δn−2t2n
...
...
...
...
...
0
0
· · ·
λn−1 δtn−1 n
0
0
· · ·
0
λn







.
Now deﬁne the function ∥∥: Mn(C) →R by
∥B∥=
(UDδ)−1B(UDδ)

∞,
for every B ∈Mn(C). Then it is easy to verify that the above function is
the matrix norm subordinate to the vector norm
v 7→
(UDδ)−1v

∞.
Furthermore, for every ϵ > 0, we can pick δ so that
n
X
j=i+1
|δj−itij| ≤ϵ,
1 ≤i ≤n −1,
and by deﬁnition of the norm ∥∥∞, we get
∥A∥≤ρ(A) + ϵ,
which shows that the norm that we have constructed satisﬁes the required
properties.

314
Vector Norms and Matrix Norms
Note that equality is generally not possible; consider the matrix
A =
0 1
0 0

,
for which ρ(A) = 0 < ∥A∥, since A ̸= 0.
8.5
Condition Numbers of Matrices
Unfortunately, there exist linear systems Ax = b whose solutions are not
stable under small perturbations of either b or A. For example, consider
the system




10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10








x1
x2
x3
x4



=




32
23
33
31



.
The reader should check that it has the solution x = (1, 1, 1, 1).
If we
perturb slightly the right-hand side as b + ∆b, where
∆b =




0.1
−0.1
0.1
−0.1



,
we obtain the new system




10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10








x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4



=




32.1
22.9
33.1
30.9



.
The new solution turns out to be x + ∆x = (9.2, −12.6, 4.5, −1.1), where
∆x = (9.2, −12.6, 4.5, −1.1) −(1, 1, 1, 1) = (8.2, −13.6, 3.5, −2.1).
Then a relative error of the data in terms of the one-norm,
∥∆b∥1
∥b∥1
= 0.4
119 =
4
1190 ≈
1
300,
produces a relative error in the input
∥∆x∥1
∥x∥1
= 27.4
4
≈7.

8.5. Condition Numbers of Matrices
315
So a relative order of the order 1/300 in the data produces a relative error
of the order 7/1 in the solution, which represents an ampliﬁcation of the
relative error of the order 2100.
Now let us perturb the matrix slightly, obtaining the new system




10
7
8.1 7.2
7.08 5.04
6
5
8
5.98 9.98
9
6.99 4.99
9
9.98








x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4



=




32
23
33
31



.
This time the solution is x+∆x = (−81, 137, −34, 22). Again a small change
in the data alters the result rather drastically. Yet the original system is
symmetric, has determinant 1, and has integer entries. The problem is that
the matrix of the system is badly conditioned, a concept that we will now
explain.
Given an invertible matrix A, ﬁrst assume that we perturb b to b + ∆b,
and let us analyze the change between the two exact solutions x and x+∆x
of the two systems
Ax = b
A(x + ∆x) = b + ∆b.
We also assume that we have some norm ∥∥and we use the subordinate
matrix norm on matrices. From
Ax = b
Ax + A∆x = b + ∆b,
we get
∆x = A−1∆b,
and we conclude that
∥∆x∥≤
A−1 ∥∆b∥
∥b∥≤∥A∥∥x∥.
Consequently, the relative error in the result ∥∆x∥/ ∥x∥is bounded in terms
of the relative error ∥∆b∥/ ∥b∥in the data as follows:
∥∆x∥
∥x∥
≤
 ∥A∥
A−1 ∥∆b∥
∥b∥.
Now let us assume that A is perturbed to A + ∆A, and let us analyze
the change between the exact solutions of the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.

316
Vector Norms and Matrix Norms
The second equation yields Ax+A∆x+∆A(x+∆x) = b, and by subtracting
the ﬁrst equation we get
∆x = −A−1∆A(x + ∆x).
It follows that
∥∆x∥≤
A−1 ∥∆A∥∥x + ∆x∥,
which can be rewritten as
∥∆x∥
∥x + ∆x∥≤
 ∥A∥
A−1 ∥∆A∥
∥A∥.
Observe that the above reasoning is valid even if the matrix A + ∆A is
singular, as long as x+∆x is a solution of the second system. Furthermore,
if ∥∆A∥is small enough, it is not unreasonable to expect that the ratio
∥∆x∥/ ∥x + ∆x∥is close to ∥∆x∥/ ∥x∥. This will be made more precise
later.
In summary, for each of the two perturbations, we see that the relative
error in the result is bounded by the relative error in the data, multiplied
the number ∥A∥
A−1. In fact, this factor turns out to be optimal and
this suggests the following deﬁnition:
Deﬁnition 8.10. For any subordinate matrix norm ∥∥, for any invertible
matrix A, the number
cond(A) = ∥A∥
A−1
is called the condition number of A relative to ∥∥.
The condition number cond(A) measures the sensitivity of the linear
system Ax = b to variations in the data b and A; a feature referred to as
the condition of the system. Thus, when we says that a linear system is
ill-conditioned, we mean that the condition number of its matrix is large.
We can sharpen the preceding analysis as follows:
Proposition 8.10. Let A be an invertible matrix and let x and x + ∆x be
the solutions of the linear systems
Ax = b
A(x + ∆x) = b + ∆b.
If b ̸= 0, then the inequality
∥∆x∥
∥x∥
≤cond(A)∥∆b∥
∥b∥
holds and is the best possible. This means that for a given matrix A, there
exist some vectors b ̸= 0 and ∆b ̸= 0 for which equality holds.

8.5. Condition Numbers of Matrices
317
Proof. We already proved the inequality. Now, because ∥∥is a subordi-
nate matrix norm, there exist some vectors x ̸= 0 and ∆b ̸= 0 for which
A−1∆b
 =
A−1 ∥∆b∥
and
∥Ax∥= ∥A∥∥x∥.
Proposition 8.11. Let A be an invertible matrix and let x and x + ∆x be
the solutions of the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.
If b ̸= 0, then the inequality
∥∆x∥
∥x + ∆x∥≤cond(A)∥∆A∥
∥A∥
holds and is the best possible. This means that given a matrix A, there exist
a vector b ̸= 0 and a matrix ∆A ̸= 0 for which equality holds. Furthermore,
if ∥∆A∥is small enough (for instance, if ∥∆A∥< 1/
A−1), we have
∥∆x∥
∥x∥
≤cond(A)∥∆A∥
∥A∥(1 + O(∥∆A∥));
in fact, we have
∥∆x∥
∥x∥
≤cond(A)∥∆A∥
∥A∥

1
1 −∥A−1∥∥∆A∥

.
Proof. The ﬁrst inequality has already been proven. To show that equality
can be achieved, let w be any vector such that w ̸= 0 and
A−1w
 =
A−1 ∥w∥,
and let β ̸= 0 be any real number. Now the vectors
∆x = −βA−1w
x + ∆x = w
b = (A + βI)w
and the matrix
∆A = βI
satisfy the equations
Ax = b
(A + ∆A)(x + ∆x) = b
∥∆x∥= |β|
A−1w
 = ∥∆A∥
A−1 ∥x + ∆x∥.

318
Vector Norms and Matrix Norms
Finally we can pick β so that −β is not equal to any of the eigenvalues of
A, so that A + ∆A = A + βI is invertible and b is is nonzero.
If ∥∆A∥< 1/
A−1, then
A−1∆A
 ≤
A−1 ∥∆A∥< 1,
so by Proposition 8.8, the matrix I + A−1∆A is invertible and
(I + A−1∆A)−1 ≤
1
1 −∥A−1∆A∥≤
1
1 −∥A−1∥∥∆A∥.
Recall that we proved earlier that
∆x = −A−1∆A(x + ∆x),
and by adding x to both sides and moving the right-hand side to the left-
hand side yields
(I + A−1∆A)(x + ∆x) = x,
and thus
x + ∆x = (I + A−1∆A)−1x,
which yields
∆x = ((I + A−1∆A)−1 −I)x = (I + A−1∆A)−1(I −(I + A−1∆A))x
= −(I + A−1∆A)−1A−1(∆A)x.
From this and
(I + A−1∆A)−1 ≤
1
1 −∥A−1∥∥∆A∥,
we get
∥∆x∥≤
A−1 ∥∆A∥
1 −∥A−1∥∥∆A∥∥x∥,
which can be written as
∥∆x∥
∥x∥
≤cond(A)∥∆A∥
∥A∥

1
1 −∥A−1∥∥∆A∥

,
which is the kind of inequality that we were seeking.
Remark: If A and b are perturbed simultaneously, so that we get the
"perturbed" system
(A + ∆A)(x + ∆x) = b + ∆b,

8.5. Condition Numbers of Matrices
319
it can be shown that if ∥∆A∥< 1/
A−1 (and b ̸= 0), then
∥∆x∥
∥x∥
≤
cond(A)
1 −∥A−1∥∥∆A∥
∥∆A∥
∥A∥+ ∥∆b∥
∥b∥

;
see Demmel [Demmel (1997)], Section 2.2 and Horn and Johnson [Horn and
Johnson (1990)], Section 5.8.
We now list some properties of condition numbers and ﬁgure out what
cond(A) is in the case of the spectral norm (the matrix norm induced by
∥∥2). First, we need to introduce a very important factorization of matrices,
the singular value decomposition, for short, SVD.
It can be shown (see Section 20.2) that given any n × n matrix A ∈
Mn(C), there exist two unitary matrices U and V , and a real diagonal
matrix Σ = diag(σ1, . . . , σn), with σ1 ≥σ2 ≥· · · ≥σn ≥0, such that
A = V ΣU ∗.
Deﬁnition 8.11. Given a complex n × n matrix A, a triple (U, V, Σ) such
that A = V ΣU ⊤, where U and V are n × n unitary matrices and Σ =
diag(σ1, . . . , σn) is a diagonal matrix of real numbers σ1 ≥σ2 ≥· · · ≥σn ≥
0, is called a singular decomposition (for short SVD) of A. If A is a real
matrix, then U and V are orthogonal matrices. The nonnegative numbers
σ1, . . . , σn are called the singular values of A.
The factorization A = V ΣU ∗implies that
A∗A = UΣ2U ∗
and
AA∗= V Σ2V ∗,
which shows that σ2
1, . . . , σ2
n are the eigenvalues of both A∗A and AA∗, that
the columns of U are corresponding eigenvectors for A∗A, and that the
columns of V are corresponding eigenvectors for AA∗.
Since σ2
1 is the largest eigenvalue of A∗A (and AA∗), note that
p
ρ(A∗A) =
p
ρ(AA∗) = σ1.
Corollary 8.3. The spectral norm ∥A∥2 of a matrix A is equal to the largest
singular value of A. Equivalently, the spectral norm ∥A∥2 of a matrix A is
equal to the ℓ∞-norm of its vector of singular values,
∥A∥2 = max
1≤i≤n σi = ∥(σ1, . . . , σn)∥∞.
Since the Frobenius norm of a matrix A is deﬁned by ∥A∥F =
p
tr(A∗A)
and since
tr(A∗A) = σ2
1 + · · · + σ2
n

320
Vector Norms and Matrix Norms
where σ2
1, . . . , σ2
n are the eigenvalues of A∗A, we see that
∥A∥F = (σ2
1 + · · · + σ2
n)1/2 = ∥(σ1, . . . , σn)∥2 .
Corollary 8.4. The Frobenius norm of a matrix is given by the ℓ2-norm
of its vector of singular values; ∥A∥F = ∥(σ1, . . . , σn)∥2.
In the case of a normal matrix if λ1, . . . , λn are the (complex) eigenvalues
of A, then
σi = |λi|,
1 ≤i ≤n.
Proposition 8.12. For every invertible matrix A ∈Mn(C), the following
properties hold:
(1)
cond(A) ≥1,
cond(A) = cond(A−1)
cond(αA) = cond(A)
for all α ∈C −{0}.
(2) If cond2(A) denotes the condition number of A with respect to the spec-
tral norm, then
cond2(A) = σ1
σn
,
where σ1 ≥· · · ≥σn are the singular values of A.
(3) If the matrix A is normal, then
cond2(A) = |λ1|
|λn|,
where λ1, . . . , λn are the eigenvalues of A sorted so that |λ1| ≥· · · ≥
|λn|.
(4) If A is a unitary or an orthogonal matrix, then
cond2(A) = 1.
(5) The condition number cond2(A) is invariant under unitary transfor-
mations, which means that
cond2(A) = cond2(UA) = cond2(AV ),
for all unitary matrices U and V .

8.5. Condition Numbers of Matrices
321
Proof. The properties in (1) are immediate consequences of the properties
of subordinate matrix norms. In particular, AA−1 = I implies
1 = ∥I∥≤∥A∥
A−1 = cond(A).
(2) We showed earlier that ∥A∥2
2 = ρ(A∗A), which is the square of the
modulus of the largest eigenvalue of A∗A.
Since we just saw that the
eigenvalues of A∗A are σ2
1 ≥· · · ≥σ2
n, where σ1, . . . , σn are the singular
values of A, we have
∥A∥2 = σ1.
Now if A is invertible, then σ1 ≥· · · ≥σn > 0, and it is easy to show that
the eigenvalues of (A∗A)−1 are σ−2
n
≥· · · ≥σ−2
1 , which shows that
A−1
2 = σ−1
n ,
and thus
cond2(A) = σ1
σn
.
(3) This follows from the fact that ∥A∥2 = ρ(A) for a normal matrix.
(4) If A is a unitary matrix, then A∗A = AA∗= I, so ρ(A∗A) = 1, and
∥A∥2 =
p
ρ(A∗A) = 1. We also have
A−1
2 = ∥A∗∥2 =
p
ρ(AA∗) = 1,
and thus cond(A) = 1.
(5) This follows immediately from the unitary invariance of the spectral
norm.
Proposition 8.12 (4) shows that unitary and orthogonal transformations
are very well-conditioned, and Part (5) shows that unitary transformations
preserve the condition number.
In order to compute cond2(A), we need to compute the top and bottom
singular values of A, which may be hard. The inequality
∥A∥2 ≤∥A∥F ≤√n ∥A∥2 ,
may be useful in getting an approximation of cond2(A) = ∥A∥2
A−1
2, if
A−1 can be determined.
Remark: There is an interesting geometric characterization of cond2(A).
If θ(A) denotes the least angle between the vectors Au and Av as u and v
range over all pairs of orthonormal vectors, then it can be shown that
cond2(A) = cot(θ(A)/2)).
Thus if A is nearly singular, then there will be some orthonormal pair u, v
such that Au and Av are nearly parallel; the angle θ(A) will the be small

322
Vector Norms and Matrix Norms
and cot(θ(A)/2)) will be large. For more details, see Horn and Johnson
[Horn and Johnson (1990)] (Section 5.8 and Section 7.4).
It should be noted that in general (if A is not a normal matrix) a
matrix could have a very large condition number even if all its eigenvalues
are identical! For example, if we consider the n × n matrix
A =












1 2 0
0 . . . 0 0
0 1 2
0 . . . 0 0
0 0 1
2 . . . 0 0
... ... ... ... ... ... ...
0 0 . . . 0
1 2 0
0 0 . . . 0
0 1 2
0 0 . . . 0
0 0 1












,
it turns out that cond2(A) ≥2n−1.
A classical example of matrix with a very large condition number is the
Hilbert matrix H(n), the n × n matrix with
H(n)
ij
=

1
i + j −1

.
For example, when n = 5,
H(5) =









1 1
2
1
3
1
4
1
5
1
2
1
3
1
4
1
5
1
6
1
3
1
4
1
5
1
6
1
7
1
4
1
5
1
6
1
7
1
8
1
5
1
6
1
7
1
8
1
9









.
It can be shown that
cond2(H(5)) ≈4.77 × 105.
Hilbert introduced these matrices in 1894 while studying a problem
in approximation theory. The Hilbert matrix H(n) is symmetric positive
deﬁnite. A closed-form formula can be given for its determinant (it is a
special form of the so-called Cauchy determinant); see Problem 8.15. The
inverse of H(n) can also be computed explicitly; see Problem 8.15. It can
be shown that
cond2(H(n)) = O((1 +
√
2)4n/√n).

8.6. An Application of Norms: Solving Inconsistent Linear Systems
323
Going back to our matrix
A =




10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10



,
which is a symmetric positive deﬁnite matrix, it can be shown that its
eigenvalues, which in this case are also its singular values because A is
SPD, are
λ1 ≈30.2887 > λ2 ≈3.858 > λ3 ≈0.8431 > λ4 ≈0.01015,
so that
cond2(A) = λ1
λ4
≈2984.
The reader should check that for the perturbation of the right-hand side
b used earlier, the relative errors ∥∆x∥/∥x∥and ∥∆x∥/∥x∥satisfy the
inequality
∥∆x∥
∥x∥
≤cond(A)∥∆b∥
∥b∥
and comes close to equality.
8.6
An Application of Norms: Solving Inconsistent
Linear Systems
The problem of solving an inconsistent linear system Ax = b often arises
in practice. This is a system where b does not belong to the column space
of A, usually with more equations than variables. Thus, such a system
has no solution. Yet we would still like to "solve" such a system, at least
approximately.
Such systems often arise when trying to ﬁt some data. For example, we
may have a set of 3D data points
{p1, . . . , pn},
and we have reason to believe that these points are nearly coplanar. We
would like to ﬁnd a plane that best ﬁts our data points. Recall that the
equation of a plane is
αx + βy + γz + δ = 0,

324
Vector Norms and Matrix Norms
with (α, β, γ) ̸= (0, 0, 0). Thus, every plane is either not parallel to the
x-axis (α ̸= 0) or not parallel to the y-axis (β ̸= 0) or not parallel to the
z-axis (γ ̸= 0).
Say we have reasons to believe that the plane we are looking for is not
parallel to the z-axis. If we are wrong, in the least squares solution, one of
the coeﬃcients, α, β, will be very large. If γ ̸= 0, then we may assume that
our plane is given by an equation of the form
z = ax + by + d,
and we would like this equation to be satisﬁed for all the pi's, which leads
to a system of n equations in 3 unknowns a, b, d, with pi = (xi, yi, zi);
ax1 + by1 + d = z1
...
...
axn + byn + d = zn.
However, if n is larger than 3, such a system generally has no solution.
Since the above system can't be solved exactly, we can try to ﬁnd a solution
(a, b, d) that minimizes the least-squares error
n
X
i=1
(axi + byi + d −zi)2.
This is what Legendre and Gauss ﬁgured out in the early 1800's!
In general, given a linear system
Ax = b,
we solve the least squares problem: minimize ∥Ax −b∥2
2.
Fortunately, every n × m-matrix A can be written as
A = V DU ⊤
where U and V are orthogonal and D is a rectangular diagonal matrix with
non-negative entries (singular value decomposition, or SVD); see Chap-
ter 20.
The SVD can be used to solve an inconsistent system. It is shown in
Chapter 21 that there is a vector x of smallest norm minimizing ∥Ax −b∥2.
It is given by the (Penrose) pseudo-inverse of A (itself given by the SVD).
It has been observed that solving in the least-squares sense may give
too much weight to "outliers," that is, points clearly outside the best-ﬁt
plane. In this case, it is preferable to minimize (the ℓ1-norm)
n
X
i=1
|axi + byi + d −zi|.

8.7. Limits of Sequences and Series
325
This does not appear to be a linear problem, but we can use a trick to
convert this minimization problem into a linear program (which means a
problem involving linear constraints).
Note that |x| = max{x, −x}. So by introducing new variables e1, . . . , en,
our minimization problem is equivalent to the linear program (LP):
minimize
e1 + · · · + en
subject to
axi + byi + d −zi ≤ei
−(axi + byi + d −zi) ≤ei
1 ≤i ≤n.
Observe that the constraints are equivalent to
ei ≥|axi + byi + d −zi|,
1 ≤i ≤n.
For an optimal solution, we must have equality, since otherwise we could
decrease some ei and get an even better solution. Of course, we are no longer
dealing with "pure" linear algebra, since our constraints are inequalities.
We prefer not getting into linear programming right now, but the above
example provides a good reason to learn more about linear programming!
8.7
Limits of Sequences and Series
If x ∈R or x ∈C and if |x| < 1, it is well known that the sums Pn
k=0 xk =
1+x+x2 +· · ·+xn converge to the limit 1/(1−x) when n goes to inﬁnity,
and we write
∞
X
k=0
xk =
1
1 −x.
For example,
∞
X
k=0
1
2k = 2.
Similarly, the sums
Sn =
n
X
k=0
xk
k!
converge to ex when n goes to inﬁnity, for every x (in R or C). What if we
replace x by a real of complex n × n matrix A?

326
Vector Norms and Matrix Norms
The partial sums Pn
k=0 Ak and Pn
k=0
Ak
k! still make sense, but we have
to deﬁne what is the limit of a sequence of matrices. This can be done in
any normed vector space.
Deﬁnition 8.12. Let (E, ∥∥) be a normed vector space.
A sequence
(un)n∈N in E is any function u: N →E. For any v ∈E, the sequence
(un) converges to v (and v is the limit of the sequence (un)) if for every
ϵ > 0, there is some integer N > 0 such that
∥un −v∥< ϵ
for all n ≥N.
Often we assume that a sequence is indexed by N −{0}, that is, its ﬁrst
term is u1 rather than u0.
If the sequence (un) converges to v, then since by the triangle inequality
∥um −un∥≤∥um −v∥+ ∥v −un∥,
we see that for every ϵ > 0, we can ﬁnd N > 0 such that ∥um −v∥< ϵ/2
and ∥un −v∥< ϵ/2, and so
∥um −un∥< ϵ
for all m, n ≥N.
The above property is necessary for a convergent sequence, but not
necessarily suﬃcient. For example, if E = Q, there are sequences of ra-
tionals satisfying the above condition, but whose limit is not a rational
number. For example, the sequence Pn
k=1
1
k! converges to e, and the se-
quence Pn
k=0(−1)k
1
2k+1 converges to π/4, but e and π/4 are not rational
(in fact, they are transcendental). However, R is constructed from Q to
guarantee that sequences with the above property converge, and so is C.
Deﬁnition 8.13. Given a normed vector space (E, ∥∥), a sequence (un) is
a Cauchy sequence if for every ϵ > 0, there is some N > 0 such that
∥um −un∥< ϵ
for all m, n ≥N.
If every Cauchy sequence converges, then we say that E is complete. A
complete normed vector spaces is also called a Banach space.
A fundamental property of R is that it is complete. It follows immedi-
ately that C is also complete. If E is a ﬁnite-dimensional real or complex
vector space, since any two norms are equivalent, we can pick the ℓ∞norm,
and then by picking a basis in E, a sequence (un) of vectors in E con-
verges iﬀthe n sequences of coordinates (ui
n) (1 ≤i ≤n) converge, so any
ﬁnite-dimensional real or complex vector space is a Banach space.

8.7. Limits of Sequences and Series
327
Let us now consider the convergence of series.
Deﬁnition 8.14. Given a normed vector space (E, ∥∥), a series is an in-
ﬁnite sum P∞
k=0 uk of elements uk ∈E. We denote by Sn the partial sum
of the ﬁrst n + 1 elements,
Sn =
n
X
k=0
uk.
Deﬁnition 8.15. We say that the series P∞
k=0 uk converges to the limit
v ∈E if the sequence (Sn) converges to v, i.e., given any ϵ > 0, there exists
a positive integer N such that for all n ≥N,
∥Sn −v∥< ϵ.
In this case, we say that the series is convergent. We say that the series
P∞
k=0 uk converges absolutely if the series of norms P∞
k=0 ∥uk∥is conver-
gent.
If the series P∞
k=0 uk converges to v, since for all m, n with m > n we
have
m
X
k=0
uk −Sn =
m
X
k=0
uk −
n
X
k=0
uk =
m
X
k=n+1
uk,
if we let m go to inﬁnity (with n ﬁxed), we see that the series P∞
k=n+1 uk
converges and that
v −Sn =
∞
X
k=n+1
uk.
There are series that are convergent but not absolutely convergent; for
example, the series
∞
X
k=1
(−1)k−1
k
converges to ln 2, but P∞
k=1
1
k does not converge (this sum is inﬁnite).
If E is complete, the converse is an enormously useful result.
Proposition 8.13. Assume (E, ∥∥) is a complete normed vector space. If
a series P∞
k=0 uk is absolutely convergent, then it is convergent.

328
Vector Norms and Matrix Norms
Proof. If P∞
k=0 uk is absolutely convergent, then we prove that the se-
quence (Sm) is a Cauchy sequence; that is, for every ϵ > 0, there is some
p > 0 such that for all n ≥m ≥p,
∥Sn −Sm∥≤ϵ.
Observe that
∥Sn −Sm∥= ∥um+1 + · · · + un∥≤∥um+1∥+ · · · + ∥un∥,
and since the sequence P∞
k=0 ∥uk∥converges, it satisﬁes Cauchy's criterion.
Thus, the sequence (Sm) also satisﬁes Cauchy's criterion, and since E is a
complete vector space, the sequence (Sm) converges.
Remark: It can be shown that if (E, ∥∥) is a normed vector space such
that every absolutely convergent series is also convergent, then E must be
complete (see Schwartz [Schwartz (1991)]).
An important corollary of absolute convergence is that if the terms in
series P∞
k=0 uk are rearranged, then the resulting series is still absolutely
convergent and has the same sum. More precisely, let σ be any permuta-
tion (bijection) of the natural numbers. The series P∞
k=0 uσ(k) is called a
rearrangement of the original series. The following result can be shown (see
Schwartz [Schwartz (1991)]).
Proposition 8.14. Assume (E, ∥∥) is a normed vector space.
If a se-
ries P∞
k=0 uk is convergent as well as absolutely convergent, then for every
permutation σ of N, the series P∞
k=0 uσ(k) is convergent and absolutely
convergent, and its sum is equal to the sum of the original series:
∞
X
k=0
uσ(k) =
∞
X
k=0
uk.
In particular, if (E, ∥∥) is a complete normed vector space, then Propo-
sition 8.14 holds.
We now apply Proposition 8.13 to the matrix exponential.
8.8
The Matrix Exponential
Proposition 8.15. For any n × n real or complex matrix A, the series
∞
X
k=0
Ak
k!
converges absolutely for any operator norm on Mn(C) (or Mn(R)).

8.8. The Matrix Exponential
329
Proof. Pick any norm on Cn (or Rn) and let ∥∥be the corresponding
operator norm on Mn(C). Since Mn(C) has dimension n2, it is complete.
By Proposition 8.13, it suﬃces to show that the series of nonnegative reals
Pn
k=0
 Ak
k!
 converges. Since ∥∥is an operator norm, this a matrix norm,
so we have
n
X
k=0

Ak
k!
 ≤
n
X
k=0
∥A∥k
k!
≤e∥A∥.
Thus, the nondecreasing sequence of positive real numbers Pn
k=0
 Ak
k!
 is
bounded by e∥A∥, and by a fundamental property of R, it has a least upper
bound which is its limit.
Deﬁnition 8.16. Let E be a ﬁnite-dimensional real of complex normed
vector space. For any n × n matrix A, the limit of the series
∞
X
k=0
Ak
k!
is the exponential of A and is denoted eA.
A basic property of the exponential x 7→ex with x ∈C is
ex+y = exey,
for all x, y ∈C.
As a consequence, ex is always invertible and (ex)−1 = e−x. For matrices,
because matrix multiplication is not commutative, in general,
eA+B = eAeB
fails! This result is salvaged as follows.
Proposition 8.16. For any two n × n complex matrices A and B, if A
and B commute, that is, AB = BA, then
eA+B = eAeB.
A proof of Proposition 8.16 can be found in Gallier [Gallier (2011b)].
Since A and −A commute, as a corollary of Proposition 8.16, we see
that eA is always invertible and that
(eA)−1 = e−A.
It is also easy to see that
(eA)⊤= eA⊤.

330
Vector Norms and Matrix Norms
In general, there is no closed-form formula for the exponential eA of a
matrix A, but for skew symmetric matrices of dimension 2 and 3, there
are explicit formulae. Everyone should enjoy computing the exponential
eA where
A =
0 −θ
θ 0

.
If we write
J =
0 −1
1 0

,
then
A = θJ.
The key property is that
J2 = −I.
Proposition 8.17. If A = θJ, then
eA = cos θI + sin θJ =
cos θ −sin θ
sin θ
cos θ

.
Proof. We have
A4n = θ4nI2,
A4n+1 = θ4n+1J,
A4n+2 = −θ4n+2I2,
A4n+3 = −θ4n+3J,
and so
eA = I2 + θ
1!J −θ2
2! I2 −θ3
3! J + θ4
4! I2 + θ5
5! J −θ6
6! I2 −θ7
7! J + · · · .
Rearranging the order of the terms, we have
eA =

1 −θ2
2! + θ4
4! −θ6
6! + · · ·

I2 +
 θ
1! −θ3
3! + θ5
5! −θ7
7! + · · ·

J.
We recognize the power series for cos θ and sin θ, and thus
eA = cos θI2 + sin θJ,
that is
eA =
cos θ −sin θ
sin θ
cos θ

,
as claimed.

8.9. Summary
331
Thus, we see that the exponential of a 2 × 2 skew-symmetric matrix is
a rotation matrix. This property generalizes to any dimension. An explicit
formula when n = 3 (the Rodrigues' formula) is given in Section 11.7.
Proposition 8.18.
If B is an n × n (real) skew symmetric matrix, that
is, B⊤= −B, then Q = eB is an orthogonal matrix, that is
Q⊤Q = QQ⊤= I.
Proof. Since B⊤= −B, we have
Q⊤= (eB)⊤= eB⊤= e−B.
Since B and −B commute, we have
Q⊤Q = e−BeB = e−B+B = e0 = I.
Similarly,
QQ⊤= eBe−B = eB−B = e0 = I,
which concludes the proof.
It can also be shown that det(Q) = det(eB) = 1, but this requires a
better understanding of the eigenvalues of eB (see Section 14.5). Further-
more, for every n×n rotation matrix Q (an orthogonal matrix Q such that
det(Q) = 1), there is a skew symmetric matrix B such that Q = eB. This
is a fundamental property which has applications in robotics for n = 3.
All familiar series have matrix analogs. For example, if ∥A∥< 1 (where
∥∥is an operator norm), then the series P∞
k=0 Ak converges absolutely, and
it can be shown that its limit is (I −A)−1.
Another interesting series is the logarithm.
For any n × n complex
matrix A, if ∥A∥< 1 (where ∥∥is an operator norm), then the series
log(I + A) =
∞
X
k=1
(−1)k+1 Ak
k
converges absolutely.
8.9
Summary
The main concepts and results of this chapter are listed below:
• Norms and normed vector spaces.
• The triangle inequality.
• The Euclidean norm; the ℓp-norms.

332
Vector Norms and Matrix Norms
• H¨older's inequality; the Cauchy-Schwarz inequality; Minkowski's in-
equality.
• Hermitian inner product and Euclidean inner product.
• Equivalent norms.
• All norms on a ﬁnite-dimensional vector space are equivalent (Theo-
rem 8.1).
• Matrix norms.
• Hermitian, symmetric and normal matrices. Orthogonal and unitary
matrices.
• The trace of a matrix.
• Eigenvalues and eigenvectors of a matrix.
• The characteristic polynomial of a matrix.
• The spectral radius ρ(A) of a matrix A.
• The Frobenius norm.
• The Frobenius norm is a unitarily invariant matrix norm.
• Bounded linear maps.
• Subordinate matrix norms.
• Characterization of the subordinate matrix norms for the vector norms
∥∥1 , ∥∥2, and ∥∥∞.
• The spectral norm.
• For every matrix A ∈Mn(C) and for every ϵ > 0, there is some subor-
dinate matrix norm ∥∥such that ∥A∥≤ρ(A) + ϵ.
• Condition numbers of matrices.
• Perturbation analysis of linear systems.
• The singular value decomposition (SVD).
• Properties of conditions numbers.
Characterization of cond2(A) in
terms of the largest and smallest singular values of A.
• The Hilbert matrix: a very badly conditioned matrix.
• Solving inconsistent linear systems by the method of least-squares; lin-
ear programming.
• Convergence of sequences of vectors in a normed vector space.
• Cauchy sequences, complex normed vector spaces, Banach spaces.
• Convergence of series. Absolute convergence.
• The matrix exponential.
• Skew symmetric matrices and orthogonal matrices.

8.10. Problems
333
8.10
Problems
Problem 8.1. Let A be the following matrix:
B =

1
1/
√
2
1/
√
2 3/2

.
Compute the operator 2-norm ∥A∥2 of A.
Problem 8.2. Prove Proposition 8.2, namely that the following inequali-
ties hold for all x ∈Rn (or x ∈Cn):
∥x∥∞≤∥x∥1 ≤n∥x∥∞,
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
∥x∥2 ≤∥x∥1 ≤√n∥x∥2.
Problem 8.3. For any p ≥1, prove that for all x ∈Rn,
lim
p7→∞∥x∥p = ∥x∥∞.
Problem 8.4. Let A be an n × n matrix which is strictly row diagonally
dominant, which means that
|ai i| >
X
j̸=i
|ai j|,
for i = 1, . . . , n, and let
δ = min
i

|ai i| −
X
j̸=i
|ai j|

.
The fact that A is strictly row diagonally dominant is equivalent to the
condition δ > 0.
(1) For any nonzero vector v, prove that
∥Av∥∞≥∥v∥∞δ.
Use the above to prove that A is invertible.
(2) Prove that
A−1
∞≤δ−1.
Hint. Prove that
sup
v̸=0
A−1v

∞
∥v∥∞
= sup
w̸=0
∥w∥∞
∥Aw∥∞
.

334
Vector Norms and Matrix Norms
Problem 8.5. Let A be any invertible complex n × n matrix.
(1) For any vector norm ∥∥on Cn, prove that the function ∥∥A : Cn →
R given by
∥x∥A = ∥Ax∥
for all
x ∈Cn,
is a vector norm.
(2) Prove that the operator norm induced by ∥∥A, also denoted by ∥∥A,
is given by
∥B∥A =
ABA−1
for every n × n matrix
B,
where
ABA−1 uses the operator norm induced by ∥∥.
Problem 8.6. Give an example of a norm on Cn and of a real matrix A
such that
∥A∥R < ∥A∥,
where ∥−∥R and ∥−∥are the operator norms associated with the vector
norm ∥−∥.
Hint. This can already be done for n = 2.
Problem 8.7. Let ∥∥be any operator norm. Given an invertible n × n
matrix A, if c = 1/(2
A−1), then for every n × n matrix H, if ∥H∥≤
c, then A + H is invertible.
Furthermore, show that if ∥H∥≤c, then
(A + H)−1 ≤1/c.
Problem 8.8. Let A be any m × n matrix and let λ ∈R be any positive
real number λ > 0.
(1) Prove that A⊤A + λIn and AA⊤+ λIm are invertible.
(2) Prove that
A⊤(AA⊤+ λIm)−1 = (A⊤A + λIn)−1A⊤.
Remark: The expressions above correspond to the matrix for which the
function
Φ(x) = (Ax −b)⊤(Ax −b) + λx⊤x
achieves a minimum. It shows up in machine learning (kernel methods).
Problem 8.9. Let Z be a q × p real matrix. Prove that if Ip −Z⊤Z is
positive deﬁnite, then the (p + q) × (p + q) matrix
S =
Ip Z⊤
Z Iq

is symmetric positive deﬁnite.

8.10. Problems
335
Problem 8.10. Prove that for any real or complex square matrix A, we
have
∥A∥2
2 ≤∥A∥1 ∥A∥∞,
where the above norms are operator norms.
Hint. Use Proposition 8.7 (among other things, it shows that ∥A∥1 =
A⊤
∞).
Problem 8.11. Show that the map A 7→ρ(A) (where ρ(A) is the spectral
radius of A) is neither a norm nor a matrix norm. In particular, ﬁnd two
2 × 2 matrices A and B such that
ρ(A + B) > ρ(A) + ρ(B) = 0
and
ρ(AB) > ρ(A)ρ(B) = 0.
Problem 8.12. Deﬁne the map A 7→M(A) (deﬁned on n × n real or
complex n × n matrices) by
M(A) = max{|aij| | 1 ≤i, j ≤n}.
(1) Prove that
M(AB) ≤nM(A)M(B)
for all n × n matrices A and B.
(2) Give a counter-example of the inequality
M(AB) ≤M(A)M(B).
(3) Prove that the map A 7→∥A∥M given by
∥A∥M = nM(A) = n max{|aij| | 1 ≤i, j ≤n}
is a matrix norm.
Problem 8.13. Let S be a real symmetric positive deﬁnite matrix.
(1) Use the Cholesky factorization to prove that there is some upper-
triangular matrix C, unique if its diagonal elements are strictly positive,
such that S = C⊤C.
(2) For any x ∈Rn, deﬁne
∥x∥S = (x⊤Sx)1/2.
Prove that
∥x∥S = ∥Cx∥2 ,
and that the map x 7→∥x∥S is a norm.

336
Vector Norms and Matrix Norms
Problem 8.14. Let A be a real 2 × 2 matrix
A =
a1 1 a1 2
a2 1 a2 2

.
(1) Prove that the squares of the singular values σ1 ≥σ2 of A are the
roots of the quadratic equation
X2 −tr(A⊤A)X + | det(A)|2 = 0.
(2) If we let
µ(A) = a2
1 1 + a2
1 2 + a2
2 1 + a2
2 2
2|a1 1a2 2 −a1 2a2 1|
,
prove that
cond2(A) = σ1
σ2
= µ(A) + (µ(A)2 −1)1/2.
(3) Consider the subset S of 2 × 2 invertible matrices whose entries ai j
are integers such that 0 ≤aij ≤100.
Prove that the functions cond2(A) and µ(A) reach a maximum on the
set S for the same values of A.
Check that for the matrix
Am =
100 99
99 98

we have
µ(Am) = 19, 603
det(Am) = −1
and
cond2(Am) ≈39, 206.
(4) Prove that for all A ∈S, if | det(A)| ≥2 then µ(A) ≤10, 000.
Conclude that the maximum of µ(A) on S is achieved for matrices such
that det(A) = ±1. Prove that ﬁnding matrices that maximize µ on S is
equivalent to ﬁnding some integers n1, n2, n3, n4 such that
0 ≤n4 ≤n3 ≤n2 ≤n1 ≤100
n2
1 + n2
2 + n2
3 + n2
4 ≥1002 + 992 + 992 + 982 = 39, 206
|n1n4 −n2n3| = 1.
You may use without proof that the fact that the only solution to the
above constraints is the multiset
{100, 99, 99, 98}.

8.10. Problems
337
(5) Deduce from part (4) that the matrices in S for which µ has a
maximum value are
Am =
100 99
99 98

98 99
99 100

99 100
98 99

 99 98
100 99

and check that µ has the same value for these matrices. Conclude that
max
A∈S cond2(A) = cond2(Am).
(6) Solve the system
100 99
99 98
 x1
x2

=
199
197

.
Perturb the right-hand side b by
∆b =
−0.0097
0.0106

and solve the new system
Amy = b + ∆b
where y = (y1, y2). Check that
∆x = y −x =

2
−2.0203

.
Compute ∥x∥2, ∥∆x∥2, ∥b∥2, ∥∆b∥2, and estimate
c = ∥∆x∥2
∥x∥2
∥∆b∥2
∥b∥2
−1
.
Check that
c ≈cond2(Am) = 39, 206.
Problem 8.15. Consider a real 2 × 2 matrix with zero trace of the form
A =
a b
c −a

.
(1) Prove that
A2 = (a2 + bc)I2 = −det(A)I2.
If a2 + bc = 0, prove that
eA = I2 + A.

338
Vector Norms and Matrix Norms
(2) If a2 + bc < 0, let ω > 0 be such that ω2 = −(a2 + bc). Prove that
eA = cos ω I2 + sin ω
ω
A.
(3) If a2 + bc > 0, let ω > 0 be such that ω2 = a2 + bc. Prove that
eA = cosh ω I2 + sinh ω
ω
A.
(3) Prove that in all cases
det
 eA
= 1
and
tr(A) ≥−2.
(4) Prove that there exist some real 2 × 2 matrix B with det(B) = 1
such that there is no real 2 × 2 matrix A with zero trace such that eA = B.
Problem 8.16. Recall that the Hilbert matrix is given by
H(n)
ij
=

1
i + j −1

.
(1) Prove that
det(H(n)) = (1!2! · · · (n −1)!)4
1!2! · · · (2n −1)! ,
thus the reciprocal of an integer.
Hint. Use Problem 6.13.
(2) Amazingly, the entries of the inverse of H(n) are integers. Prove
that (H(n))−1 = (αij), with
αij = (−1)i+j(i + j −1)
n + i −1
n −j
n + j −1
n −i
i + j −2
i −1
2
.

Chapter 9
Iterative Methods for Solving Linear
Systems
9.1
Convergence of Sequences of Vectors and Matrices
In Chapter 7 we discussed some of the main methods for solving systems of
linear equations. These methods are direct methods, in the sense that they
yield exact solutions (assuming inﬁnite precision!).
Another class of methods for solving linear systems consists in approx-
imating solutions using iterative methods. The basic idea is this: Given a
linear system Ax = b (with A a square invertible matrix in Mn(C)), ﬁnd
another matrix B ∈Mn(C) and a vector c ∈Cn, such that
(1) The matrix I −B is invertible.
(2) The unique solution ex of the system Ax = b is identical to the unique
solution eu of the system
u = Bu + c,
and then starting from any vector u0, compute the sequence (uk) given by
uk+1 = Buk + c,
k ∈N.
Under certain conditions (to be clariﬁed soon), the sequence (uk) con-
verges to a limit eu which is the unique solution of u = Bu + c, and thus of
Ax = b.
Consequently, it is important to ﬁnd conditions that ensure the conver-
gence of the above sequences and to have tools to compare the "rate" of
convergence of these sequences. Thus, we begin with some general results
about the convergence of sequences of vectors and matrices.
Let (E, ∥∥) be a normed vector space. Recall from Section 8.7 that a
sequence (uk) of vectors uk ∈E converges to a limit u ∈E, if for every
ϵ > 0, there some natural number N such that
∥uk −u∥≤ϵ,
for all k ≥N.
339

340
Iterative Methods for Solving Linear Systems
We write
u = lim
k7→∞uk.
If E is a ﬁnite-dimensional vector space and dim(E) = n, we know
from Theorem 8.1 that any two norms are equivalent, and if we choose the
norm ∥∥∞, we see that the convergence of the sequence of vectors uk is
equivalent to the convergence of the n sequences of scalars formed by the
components of these vectors (over any basis). The same property applies to
the ﬁnite-dimensional vector space Mm,n(K) of m×n matrices (with K = R
or K = C), which means that the convergence of a sequence of matrices
Ak = (a(k)
ij ) is equivalent to the convergence of the m × n sequences of
scalars (a(k)
ij ), with i, j ﬁxed (1 ≤i ≤m, 1 ≤j ≤n).
The ﬁrst theorem below gives a necessary and suﬃcient condition for
the sequence (Bk) of powers of a matrix B to converge to the zero matrix.
Recall that the spectral radius ρ(B) of a matrix B is the maximum of the
moduli |λi| of the eigenvalues of B.
Theorem 9.1. For any square matrix B, the following conditions are
equivalent:
(1) limk7→∞Bk = 0,
(2) limk7→∞Bkv = 0, for all vectors v,
(3) ρ(B) < 1,
(4) ∥B∥< 1, for some subordinate matrix norm ∥∥.
Proof. Assume (1) and let ∥∥be a vector norm on E and ∥∥be the
corresponding matrix norm.
For every vector v ∈E, because ∥∥is a
matrix norm, we have
∥Bkv∥≤∥Bk∥∥v∥,
and since limk7→∞Bk = 0 means that limk7→∞∥Bk∥= 0, we conclude that
limk7→∞∥Bkv∥= 0, that is, limk7→∞Bkv = 0. This proves that (1) implies
(2).
Assume (2). If we had ρ(B) ≥1, then there would be some eigenvector
u (̸= 0) and some eigenvalue λ such that
Bu = λu,
|λ| = ρ(B) ≥1,
but then the sequence (Bku) would not converge to 0, because Bku = λku
and |λk| = |λ|k ≥1. It follows that (2) implies (3).

9.1. Convergence of Sequences of Vectors and Matrices
341
Assume that (3) holds, that is, ρ(B) < 1. By Proposition 8.9, we can
ﬁnd ϵ > 0 small enough that ρ(B) + ϵ < 1, and a subordinate matrix norm
∥∥such that
∥B∥≤ρ(B) + ϵ,
which is (4).
Finally, assume (4). Because ∥∥is a matrix norm,
∥Bk∥≤∥B∥k,
and since ∥B∥< 1, we deduce that (1) holds.
The following proposition is needed to study the rate of convergence of
iterative methods.
Proposition 9.1. For every square matrix B ∈Mn(C) and every matrix
norm ∥∥, we have
lim
k7→∞∥Bk∥1/k = ρ(B).
Proof. We know from Proposition 8.3 that ρ(B) ≤∥B∥, and since ρ(B) =
(ρ(Bk))1/k, we deduce that
ρ(B) ≤∥Bk∥1/k
for all k ≥1,
and so
ρ(B) ≤lim
k7→∞∥Bk∥1/k.
Now let us prove that for every ϵ > 0, there is some integer N(ϵ) such
that
∥Bk∥1/k ≤ρ(B) + ϵ
for all k ≥N(ϵ),
which proves that
lim
k7→∞∥Bk∥1/k ≤ρ(B),
and our proposition.
For any given ϵ > 0, let Bϵ be the matrix
Bϵ =
B
ρ(B) + ϵ.
Since ∥Bϵ∥< 1, Theorem 9.1 implies that limk7→∞Bk
ϵ = 0. Consequently,
there is some integer N(ϵ) such that for all k ≥N(ϵ), we have
∥Bk∥=
∥Bk∥
(ρ(B) + ϵ)k ≤1,
which implies that
∥Bk∥1/k ≤ρ(B) + ϵ,
as claimed.
We now apply the above results to the convergence of iterative methods.

342
Iterative Methods for Solving Linear Systems
9.2
Convergence of Iterative Methods
Recall that iterative methods for solving a linear system Ax = b (with
A ∈Mn(C) invertible) consists in ﬁnding some matrix B and some vector
c, such that I −B is invertible, and the unique solution ex of Ax = b is equal
to the unique solution eu of u = Bu + c. Then starting from any vector u0,
compute the sequence (uk) given by
uk+1 = Buk + c,
k ∈N,
and say that the iterative method is convergent iﬀ
lim
k7→∞uk = eu,
for every initial vector u0.
Here is a fundamental criterion for the convergence of any iterative
methods based on a matrix B, called the matrix of the iterative method.
Theorem 9.2. Given a system u = Bu + c as above, where I −B is
invertible, the following statements are equivalent:
(1) The iterative method is convergent.
(2) ρ(B) < 1.
(3) ∥B∥< 1, for some subordinate matrix norm ∥∥.
Proof. Deﬁne the vector ek (error vector) by
ek = uk −eu,
where eu is the unique solution of the system u = Bu + c. Clearly, the
iterative method is convergent iﬀ
lim
k7→∞ek = 0.
We claim that
ek = Bke0,
k ≥0,
where e0 = u0 −eu.
This is proven by induction on k. The base case k = 0 is trivial. By the
induction hypothesis, ek = Bke0, and since uk+1 = Buk + c, we get
uk+1 −eu = Buk + c −eu,
and because eu = Beu + c and ek = Bke0 (by the induction hypothesis), we
obtain
uk+1 −eu = Buk −Beu = B(uk −eu) = Bek = BBke0 = Bk+1e0,
proving the induction step. Thus, the iterative method converges iﬀ
lim
k7→∞Bke0 = 0.
Consequently, our theorem follows by Theorem 9.1.

9.2. Convergence of Iterative Methods
343
The next proposition is needed to compare the rate of convergence of
iterative methods. It shows that asymptotically, the error vector ek = Bke0
behaves at worst like (ρ(B))k.
Proposition 9.2. Let ∥∥be any vector norm, let B ∈Mn(C) be a matrix
such that I −B is invertible, and let eu be the unique solution of u = Bu+c.
(1) If (uk) is any sequence deﬁned iteratively by
uk+1 = Buk + c,
k ∈N,
then
lim
k7→∞

sup
∥u0−eu∥=1
∥uk −eu∥1/k

= ρ(B).
(2) Let B1 and B2 be two matrices such that I −B1 and I −B2 are
invertible, assume that both u = B1u + c1 and u = B2u + c2 have the same
unique solution eu, and consider any two sequences (uk) and (vk) deﬁned
inductively by
uk+1 = B1uk + c1
vk+1 = B2vk + c2,
with u0 = v0. If ρ(B1) < ρ(B2), then for any ϵ > 0, there is some integer
N(ϵ), such that for all k ≥N(ϵ), we have
sup
∥u0−eu∥=1
 ∥vk −eu∥
∥uk −eu∥
1/k
≥
ρ(B2)
ρ(B1) + ϵ.
Proof. Let ∥∥be the subordinate matrix norm. Recall that
uk −eu = Bke0,
with e0 = u0 −eu. For every k ∈N, we have
(ρ(B1))k = ρ(Bk
1) ≤∥Bk
1∥=
sup
∥e0∥=1
∥Bk
1e0∥,
which implies
ρ(B1) =
sup
∥e0∥=1
∥Bk
1e0∥1/k = ∥Bk
1∥1/k,
and Statement (1) follows from Proposition 9.1.
Because u0 = v0, we have
uk −eu = Bk
1e0
vk −eu = Bk
2e0,

344
Iterative Methods for Solving Linear Systems
with e0 = u0 −eu = v0 −eu. Again, by Proposition 9.1, for every ϵ > 0, there
is some natural number N(ϵ) such that if k ≥N(ϵ), then
sup
∥e0∥=1
∥Bk
1e0∥1/k ≤ρ(B1) + ϵ.
Furthermore, for all k ≥N(ϵ), there exists a vector e0 = e0(k) such that
∥e0∥= 1
and
∥Bk
2e0∥1/k = ∥Bk
2∥1/k ≥ρ(B2),
which implies Statement (2).
In light of the above, we see that when we investigate new iterative
methods, we have to deal with the following two problems:
(1) Given an iterative method with matrix B, determine whether the
method is convergent. This involves determining whether ρ(B) < 1,
or equivalently whether there is a subordinate matrix norm such that
∥B∥< 1. By Proposition 8.8, this implies that I −B is invertible (since
∥−B∥= ∥B∥, Proposition 8.8 applies).
(2) Given two convergent iterative methods, compare them. The iterative
method which is faster is that whose matrix has the smaller spectral
radius.
We now discuss three iterative methods for solving linear systems:
(1) Jacobi's method
(2) Gauss-Seidel's method
(3) The relaxation method.
9.3
Description of the Methods of Jacobi,
Gauss-Seidel, and Relaxation
The methods described in this section are instances of the following scheme:
Given a linear system Ax = b, with A invertible, suppose we can write A
in the form
A = M −N,
with M invertible, and "easy to invert," which means that M is close to
being a diagonal or a triangular matrix (perhaps by blocks). Then Au = b
is equivalent to
Mu = Nu + b,

9.3. Description of the Methods of Jacobi, Gauss-Seidel, and Relaxation
345
that is,
u = M −1Nu + M −1b.
Therefore, we are in the situation described in the previous sections with
B = M −1N and c = M −1b. In fact, since A = M −N, we have
B = M −1N = M −1(M −A) = I −M −1A,
(9.1)
which shows that I −B = M −1A is invertible.
The iterative method
associated with the matrix B = M −1N is given by
uk+1 = M −1Nuk + M −1b,
k ≥0,
(9.2)
starting from any arbitrary vector u0. From a practical point of view, we
do not invert M, and instead we solve iteratively the systems
Muk+1 = Nuk + b,
k ≥0.
Various methods correspond to various ways of choosing M and N from
A. The ﬁrst two methods choose M and N as disjoint submatrices of A,
but the relaxation method allows some overlapping of M and N.
To describe the various choices of M and N, it is convenient to write A
in terms of three submatrices D, E, F, as
A = D −E −F,
where the only nonzero entries in D are the diagonal entries in A, the only
nonzero entries in E are entries in A below the the diagonal, and the only
nonzero entries in F are entries in A above the diagonal. More explicitly, if
A =















a11
a12
a13
· · ·
a1n−1
a1n
a21
a22
a23
· · ·
a2n−1
a2n
a31
a32
a33
· · ·
a3n−1
a3n
...
...
...
...
...
...
an−1 1 an−1 2 an−1 3 · · · an−1 n−1 an−1 n
an 1
an 2
an 3
· · ·
an n−1
an n















,
then
D =















a11 0
0 · · ·
0
0
0 a22 0 · · ·
0
0
0
0 a33 · · ·
0
0
...
...
...
...
...
...
0
0
0 · · · an−1 n−1
0
0
0
0 · · ·
0
an n















,

346
Iterative Methods for Solving Linear Systems
−E =
















0
0
0
· · ·
0
0
a21
0
0
· · ·
0
0
a31
a32
0
· · ·
0
0
...
...
...
...
...
...
an−1 1 an−1 2 an−1 3
...
0
0
an 1
an 2
an 3
· · · an n−1 0
















,
−F =
















0 a12 a13 · · · a1n−1
a1n
0 0 a23 · · · a2n−1
a2n
0 0
0
... a3n−1
a3n
...
...
...
...
...
...
0 0
0 · · ·
0
an−1 n
0 0
0 · · ·
0
0
















.
In Jacobi's method, we assume that all diagonal entries in A are nonzero,
and we pick
M = D
N = E + F,
so that by (9.1),
B = M −1N = D−1(E + F) = I −D−1A.
As a matter of notation, we let
J = I −D−1A = D−1(E + F),
which is called Jacobi's matrix. The corresponding method, Jacobi's iter-
ative method, computes the sequence (uk) using the recurrence
uk+1 = D−1(E + F)uk + D−1b,
k ≥0.
In practice, we iteratively solve the systems
Duk+1 = (E + F)uk + b,
k ≥0.

9.3. Description of the Methods of Jacobi, Gauss-Seidel, and Relaxation
347
If we write uk = (uk
1, . . . , uk
n), we solve iteratively the following system:
a11uk+1
1
=
−a12uk
2
· · ·
−a1nuk
n
+ b1
a22uk+1
2
=
−a21uk
1
· · ·
−a2nuk
n
+ b2
...
...
...
an−1 n−1uk+1
n−1 = −an−1 1uk
1
· · ·
−an−1 nuk
n
+ bn−1
an nuk+1
n
=
−an 1uk
1
−an 2uk
2 −an n−1uk
n−1
+ bn
.
In Matlab one step of Jacobi iteration is achieved by the following func-
tion:
function v = Jacobi2(A,b,u)
n = size(A,1);
v = zeros(n,1);
for i = 1:n
v(i,1)
= u(i,1) + (-A(i,:)*u + b(i))/A(i,i);
end
end
In order to run m iteration steps, run the following function:
function u = jacobi(A,b,u0,m)
u = u0;
for j = 1:m
u = Jacobi2(A,b,u);
end
end
Example 9.1. Consider the linear system




2 −1 0
0
−1 2 −1 0
0 −1 2 −1
0
0 −1 2








x1
x2
x3
x4



=




25
−24
21
−15



.
We check immediately that the solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
It is easy to see that the Jacobi matrix is
J = 1
2




0 1 0 0
1 0 1 0
0 1 0 1
0 0 1 0



.

348
Iterative Methods for Solving Linear Systems
After 10 Jacobi iterations, we ﬁnd the approximate solution
x1 = 10.2588, x2 = −2.5244, x3 = 5.8008, x4 = −3.7061.
After 20 iterations, we ﬁnd the approximate solution
x1 = 10.9110, x2 = −2.9429, x3 = 6.8560, x4 = −3.9647.
After 50 iterations, we ﬁnd the approximate solution
x1 = 10.9998, x2 = −2.9999, x3 = 6.9998, x4 = −3.9999,
and after 60 iterations, we ﬁnd the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals.
It can be shown (see Problem 9.6) that the eigenvalues of J are
cos
π
5

, cos
2π
5

, cos
3π
5

, cos
4π
5

,
so the spectral radius of J = B is
ρ(J) = cos
π
5

= 0.8090 < 1.
By Theorem 9.2, Jacobi's method converges for the matrix of this example.
Observe that we can try to "speed up" the method by using the new
value uk+1
1
instead of uk
1 in solving for uk+2
2
using the second equations,
and more generally, use uk+1
1
, . . . , uk+1
i−1 instead of uk
1, . . . , uk
i−1 in solving
for uk+1
i
in the ith equation. This observation leads to the system
a11uk+1
1
=
−a12uk
2
· · ·
−a1nuk
n
+ b1
a22uk+1
2
=
−a21uk+1
1
· · ·
−a2nuk
n
+ b2
...
...
...
an−1 n−1uk+1
n−1 = −an−1 1uk+1
1
· · ·
−an−1 nuk
n + bn−1
an nuk+1
n
=
−an 1uk+1
1
−an 2uk+1
2
−an n−1uk+1
n−1
+ bn
,
which, in matrix form, is written
Duk+1 = Euk+1 + Fuk + b.
Because D is invertible and E is lower triangular, the matrix D −E is
invertible, so the above equation is equivalent to
uk+1 = (D −E)−1Fuk + (D −E)−1b,
k ≥0.

9.3. Description of the Methods of Jacobi, Gauss-Seidel, and Relaxation
349
The above corresponds to choosing M and N to be
M = D −E
N = F,
and the matrix B is given by
B = M −1N = (D −E)−1F.
Since M = D −E is invertible, we know that I −B = M −1A is also
invertible.
The method that we just described is the iterative method of Gauss-
Seidel, and the matrix B is called the matrix of Gauss-Seidel and denoted
by L1, with
L1 = (D −E)−1F.
One of the advantages of the method of Gauss-Seidel is that is requires
only half of the memory used by Jacobi's method, since we only need
uk+1
1
, . . . , uk+1
i−1 , uk
i+1, . . . , uk
n
to compute uk+1
i
. We also show that in certain important cases (for ex-
ample, if A is a tridiagonal matrix), the method of Gauss-Seidel converges
faster than Jacobi's method (in this case, they both converge or diverge
simultaneously).
In Matlab one step of Gauss-Seidel iteration is achieved by the following
function:
function u = GaussSeidel3(A,b,u)
n = size(A,1);
for i = 1:n
u(i,1)
= u(i,1) + (-A(i,:)*u + b(i))/A(i,i);
end
end
It is remarkable that the only diﬀerence with Jacobi2 is that the same
variable u is used on both sides of the assignment.
In order to run m
iteration steps, run the following function:
function u = GaussSeidel1(A,b,u0,m)
u = u0;
for j = 1:m
u = GaussSeidel3(A,b,u);
end
end

350
Iterative Methods for Solving Linear Systems
Example 9.2. Consider the same linear system




2 −1 0
0
−1 2 −1 0
0 −1 2 −1
0
0 −1 2








x1
x2
x3
x4



=




25
−24
21
−15




as in Example 9.1, whose solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
After 10 Gauss-Seidel iterations, we ﬁnd the approximate solution
x1 = 10.9966, x2 = −3.0044, x3 = 6.9964, x4 = −4.0018.
After 20 iterations, we ﬁnd the approximate solution
x1 = 11.0000, x2 = −3.0001, x3 = 6.9999, x4 = −4.0000.
After 25 iterations, we ﬁnd the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals. We observe that for this example,
Gauss-Seidel's method converges about twice as fast as Jacobi's method.
It will be shown in Proposition 9.5 that for a tridiagonal matrix, the spectral
radius of the Gauss-Seidel matrix L1 is given by
ρ(L1) = (ρ(J))2,
so our observation is consistent with the theory.
The new ingredient in the relaxation method is to incorporate part of
the matrix D into N: we deﬁne M and N by
M = D
ω −E
N = 1 −ω
ω
D + F,
where ω ̸= 0 is a real parameter to be suitably chosen. Actually, we show
in Section 9.4 that for the relaxation method to converge, we must have
ω ∈(0, 2). Note that the case ω = 1 corresponds to the method of Gauss-
Seidel.
If we assume that all diagonal entries of D are nonzero, the matrix M
is invertible.
The matrix B is denoted by Lω and called the matrix of
relaxation, with
Lω =
D
ω −E
−11 −ω
ω
D + F

= (D −ωE)−1((1 −ω)D + ωF).

9.3. Description of the Methods of Jacobi, Gauss-Seidel, and Relaxation
351
The number ω is called the parameter of relaxation.
When ω > 1, the relaxation method is known as successive overrelax-
ation, abbreviated as SOR.
At ﬁrst glance the relaxation matrix Lω seems at lot more complicated
than the Gauss-Seidel matrix L1, but the iterative system associated with
the relaxation method is very similar to the method of Gauss-Seidel, and
is quite simple. Indeed, the system associated with the relaxation method
is given by
D
ω −E

uk+1 =
1 −ω
ω
D + F

uk + b,
which is equivalent to
(D −ωE)uk+1 = ((1 −ω)D + ωF)uk + ωb,
and can be written
Duk+1 = Duk −ω(Duk −Euk+1 −Fuk −b).
Explicitly, this is the system
a11uk+1
1
= a11uk
1 −ω(a11uk
1 + · · · + a1n−1uk
n−1 + a1nuk
n −b1)
a22uk+1
2
= a22uk
2 −ω(a21uk+1
1
+ · · · + a2n−1uk
n−1 + a2nuk
n −b2)
...
an nuk+1
n
= an nuk
n −ω(an 1uk+1
1
+ + · · · + an n−1uk+1
n−1 + an nuk
n −bn).
In Matlab one step of relaxation iteration is achieved by the following
function:
function u = relax3(A,b,u,omega)
n = size(A,1);
for i = 1:n
u(i,1)
= u(i,1) + omega*(-A(i,:)*u + b(i))/A(i,i);
end
end
Observe that function relax3 is obtained from the function GaussSeidel3
by simply inserting ω in front of the expression (−A(i, :) ∗u + b(i))/A(i, i).
In order to run m iteration steps, run the following function:
function u = relax(A,b,u0,omega,m)
u = u0;
for j = 1:m

352
Iterative Methods for Solving Linear Systems
u = relax3(A,b,u,omega);
end
end
Example 9.3. Consider the same linear system as in Examples 9.1 and
9.2, whose solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
After 10 relaxation iterations with ω = 1.1, we ﬁnd the approximate solu-
tion
x1 = 11.0026, x2 = −2.9968, x3 = 7.0024, x4 = −3.9989.
After 10 iterations with ω = 1.2, we ﬁnd the approximate solution
x1 = 11.0014, x2 = −2.9985, x3 = 7.0010, x4 = −3.9996.
After 10 iterations with ω = 1.3, we ﬁnd the approximate solution
x1 = 10.9996, x2 = −3.0001, x3 = 6.9999, x4 = −4.0000.
After 10 iterations with ω = 1.27, we ﬁnd the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals. We observe that for this example the
method of relaxation with ω = 1.27 converges faster than the method of
Gauss-Seidel. This observation will be conﬁrmed by Proposition 9.7.
What remains to be done is to ﬁnd conditions that ensure the conver-
gence of the relaxation method (and the Gauss-Seidel method), that is:
(1) Find conditions on ω, namely some interval I ⊆R so that ω ∈I implies
ρ(Lω) < 1; we will prove that ω ∈(0, 2) is a necessary condition.
(2) Find if there exist some optimal value ω0 of ω ∈I, so that
ρ(Lω0) = inf
ω∈I ρ(Lω).
We will give partial answers to the above questions in the next section.
It is also possible to extend the methods of this section by using block
decompositions of the form A = D −E −F, where D, E, and F consist of
blocks, and D is an invertible block-diagonal matrix. See Figure 9.1.

9.4. Convergence of the Methods of Gauss-Seidel and Relaxation
353
D
D
D
D
E
E
E
F
F
F
1
1
1
2
2
2
3
3
3
4
Fig. 9.1
A schematic representation of a block decomposition A = D −E −F, where
D = ∪4
i=1Di, E = ∪3
i=1Ei, and F = ∪3
i=1Fi.
9.4
Convergence of the Methods of Gauss-Seidel
and Relaxation
We begin with a general criterion for the convergence of an iterative method
associated with a (complex) Hermitian positive deﬁnite matrix, A = M−N.
Next we apply this result to the relaxation method.
Proposition 9.3. Let A be any Hermitian positive deﬁnite matrix, written
as
A = M −N,
with M invertible. Then M ∗+N is Hermitian, and if it is positive deﬁnite,
then
ρ(M −1N) < 1,
so that the iterative method converges.
Proof. Since M = A + N and A is Hermitian, A∗= A, so we get
M ∗+ N = A∗+ N ∗+ N = A + N + N ∗= M + N ∗= (M ∗+ N)∗,
which shows that M ∗+ N is indeed Hermitian.

354
Iterative Methods for Solving Linear Systems
Because A is Hermitian positive deﬁnite, the function
v 7→(v∗Av)1/2
from Cn to R is a vector norm ∥∥, and let ∥∥also denote its subordinate
matrix norm. We prove that
∥M −1N∥< 1,
which by Theorem 9.1 proves that ρ(M −1N) < 1. By deﬁnition
∥M −1N∥= ∥I −M −1A∥= sup
∥v∥=1
∥v −M −1Av∥,
which leads us to evaluate ∥v −M −1Av∥when ∥v∥= 1. If we write w =
M −1Av, using the facts that ∥v∥= 1, v = A−1Mw, A∗= A, and A =
M −N, we have
∥v −w∥2 = (v −w)∗A(v −w)
= ∥v∥2 −v∗Aw −w∗Av + w∗Aw
= 1 −w∗M ∗w −w∗Mw + w∗Aw
= 1 −w∗(M ∗+ N)w.
Now since we assumed that M ∗+ N is positive deﬁnite, if w ̸= 0, then
w∗(M ∗+ N)w > 0, and we conclude that
if
∥v∥= 1,
then
∥v −M −1Av∥< 1.
Finally, the function
v 7→∥v −M −1Av∥
is continuous as a composition of continuous functions, therefore it achieves
its maximum on the compact subset {v ∈Cn | ∥v∥= 1}, which proves that
sup
∥v∥=1
∥v −M −1Av∥< 1,
and completes the proof.
Now as in the previous sections, we assume that A is written as A =
D −E −F, with D invertible, possibly in block form. The next theorem
provides a suﬃcient condition (which turns out to be also necessary) for the
relaxation method to converge (and thus, for the method of Gauss-Seidel
to converge). This theorem is known as the Ostrowski-Reich theorem.
Theorem 9.3. If A = D −E −F is Hermitian positive deﬁnite, and if
0 < ω < 2, then the relaxation method converges. This also holds for a
block decomposition of A.

9.4. Convergence of the Methods of Gauss-Seidel and Relaxation
355
Proof. Recall that for the relaxation method, A = M −N with
M = D
ω −E
N = 1 −ω
ω
D + F,
and because D∗= D, E∗= F (since A is Hermitian) and ω ̸= 0 is real, we
have
M ∗+ N = D∗
ω −E∗+ 1 −ω
ω
D + F = 2 −ω
ω
D.
If D consists of the diagonal entries of A, then we know from Section 7.8
that these entries are all positive, and since ω ∈(0, 2), we see that the
matrix ((2−ω)/ω)D is positive deﬁnite. If D consists of diagonal blocks of
A, because A is positive, deﬁnite, by choosing vectors z obtained by picking
a nonzero vector for each block of D and padding with zeros, we see that
each block of D is positive deﬁnite, and thus D itself is positive deﬁnite.
Therefore, in all cases, M ∗+ N is positive deﬁnite, and we conclude by
using Proposition 9.3.
Remark: What if we allow the parameter ω to be a nonzero complex
number ω ∈C? In this case, we get
M ∗+ N = D∗
ω −E∗+ 1 −ω
ω
D + F =
 1
ω + 1
ω −1

D.
But,
1
ω + 1
ω −1 = ω + ω −ωω
ωω
= 1 −(ω −1)(ω −1)
|ω|2
= 1 −|ω −1|2
|ω|2
,
so the relaxation method also converges for ω ∈C, provided that
|ω −1| < 1.
This condition reduces to 0 < ω < 2 if ω is real.
Unfortunately, Theorem 9.3 does not apply to Jacobi's method, but in
special cases, Proposition 9.3 can be used to prove its convergence. On the
positive side, if a matrix is strictly column (or row) diagonally dominant,
then it can be shown that the method of Jacobi and the method of Gauss-
Seidel both converge. The relaxation method also converges if ω ∈(0, 1],
but this is not a very useful result because the speed-up of convergence
usually occurs for ω > 1.

356
Iterative Methods for Solving Linear Systems
We now prove that, without any assumption on A = D −E −F, other
than the fact that A and D are invertible, in order for the relaxation method
to converge, we must have ω ∈(0, 2).
Proposition 9.4. Given any matrix A = D −E −F, with A and D in-
vertible, for any ω ̸= 0, we have
ρ(Lω) ≥|ω −1|,
where Lω =

D
ω −E
−1
1−ω
ω D + F

. Therefore, the relaxation method
(possibly by blocks) does not converge unless ω ∈(0, 2). If we allow ω to be
complex, then we must have
|ω −1| < 1
for the relaxation method to converge.
Proof. Observe that the product λ1 · · · λn of the eigenvalues of Lω, which
is equal to det(Lω), is given by
λ1 · · · λn = det(Lω) =
det
1 −ω
ω
D + F

det
D
ω −E

= (1 −ω)n.
It follows that
ρ(Lω) ≥|λ1 · · · λn|1/n = |ω −1|.
The proof is the same if ω ∈C.
9.5
Convergence of the Methods of Jacobi,
Gauss-Seidel, and Relaxation for
Tridiagonal Matrices
We now consider the case where A is a tridiagonal matrix, possibly by
blocks. In this case, we obtain precise results about the spectral radius of
J and Lω, and as a consequence, about the convergence of these methods.
We also obtain some information about the rate of convergence of these
methods. We begin with the case ω = 1, which is technically easier to deal
with. The following proposition gives us the precise relationship between

9.5. Convergence Methods for Tridiagonal Matrices
357
the spectral radii ρ(J) and ρ(L1) of the Jacobi matrix and the Gauss-Seidel
matrix.
Proposition 9.5. Let A be a tridiagonal matrix (possibly by blocks). If
ρ(J) is the spectral radius of the Jacobi matrix and ρ(L1) is the spectral
radius of the Gauss-Seidel matrix, then we have
ρ(L1) = (ρ(J))2.
Consequently, the method of Jacobi and the method of Gauss-Seidel both
converge or both diverge simultaneously (even when A is tridiagonal by
blocks); when they converge, the method of Gauss-Seidel converges faster
than Jacobi's method.
Proof. We begin with a preliminary result. Let A(µ) with a tridiagonal
matrix by block of the form
A(µ) =











A1 µ−1C1
0
0
· · ·
0
µB1
A2
µ−1C2
0
· · ·
0
0
...
...
...
· · ·
...
...
· · ·
...
...
...
0
0
· · ·
0
µBp−2 Ap−1 µ−1Cp−1
0
· · ·
· · ·
0
µBp−1
Ap











,
then
det(A(µ)) = det(A(1)),
µ ̸= 0.
To prove this fact, form the block diagonal matrix
P(µ) = diag(µI1, µ2I2, . . . , µpIp),
where Ij is the identity matrix of the same dimension as the block Aj. Then
it is easy to see that
A(µ) = P(µ)A(1)P(µ)−1,
and thus,
det(A(µ)) = det(P(µ)A(1)P(µ)−1) = det(A(1)).
Since the Jacobi matrix is J = D−1(E + F), the eigenvalues of J are
the zeros of the characteristic polynomial
pJ(λ) = det(λI −D−1(E + F)),

358
Iterative Methods for Solving Linear Systems
and thus, they are also the zeros of the polynomial
qJ(λ) = det(λD −E −F) = det(D)pJ(λ).
Similarly, since the Gauss-Seidel matrix is L1 = (D −E)−1F, the zeros of
the characteristic polynomial
pL1(λ) = det(λI −(D −E)−1F)
are also the zeros of the polynomial
qL1(λ) = det(λD −λE −F) = det(D −E)pL1(λ).
Since A = D−E−F is tridiagonal (or tridiagonal by blocks), λ2D−λ2E−F
is also tridiagonal (or tridiagonal by blocks), and by using our preliminary
result with µ = λ ̸= 0, we get
qL1(λ2) = det(λ2D −λ2E −F) = det(λ2D −λE −λF) = λnqJ(λ).
By continuity, the above equation also holds for λ = 0. But then we deduce
that:
(1) For any β ̸= 0, if β is an eigenvalue of L1, then β1/2 and −β1/2 are
both eigenvalues of J, where β1/2 is one of the complex square roots of
β.
(2) For any α ̸= 0, if α and −α are both eigenvalues of J, then α2 is an
eigenvalue of L1.
The above immediately implies that ρ(L1) = (ρ(J))2.
We now consider the more general situation where ω is any real in (0, 2).
Proposition 9.6. Let A be a tridiagonal matrix (possibly by blocks), and
assume that the eigenvalues of the Jacobi matrix are all real. If ω ∈(0, 2),
then the method of Jacobi and the method of relaxation both converge or both
diverge simultaneously (even when A is tridiagonal by blocks). When they
converge, the function ω 7→ρ(Lω) (for ω ∈(0, 2)) has a unique minimum
equal to ω0 −1 for
ω0 =
2
1 +
p
1 −(ρ(J))2 ,
where 1 < ω0 < 2 if ρ(J) > 0. We also have ρ(L1) = (ρ(J))2, as before.

9.5. Convergence Methods for Tridiagonal Matrices
359
Proof. The proof is very technical and can be found in Serre [Serre (2010)]
and Ciarlet [Ciarlet (1989)]. As in the proof of the previous proposition,
we begin by showing that the eigenvalues of the matrix Lω are the zeros of
the polynomial
qLω(λ) = det
λ + ω −1
ω
D −λE −F

= det
D
ω −E

pLω(λ),
where pLω(λ) is the characteristic polynomial of Lω. Then using the pre-
liminary fact from Proposition 9.5, it is easy to show that
qLω(λ2) = λnqJ
λ2 + ω −1
λω

,
for all λ ∈C, with λ ̸= 0. This time we cannot extend the above equation
to λ = 0. This leads us to consider the equation
λ2 + ω −1
λω
= α,
which is equivalent to
λ2 −αωλ + ω −1 = 0,
for all λ ̸= 0. Since λ ̸= 0, the above equivalence does not hold for ω = 1,
but this is not a problem since the case ω = 1 has already been considered
in the previous proposition. Then we can show the following:
(1) For any β ̸= 0, if β is an eigenvalue of Lω, then
β + ω −1
β1/2ω
,
−β + ω −1
β1/2ω
are eigenvalues of J.
(2) For every α ̸= 0, if α and −α are eigenvalues of J, then µ+(α, ω) and
µ−(α, ω) are eigenvalues of Lω, where µ+(α, ω) and µ−(α, ω) are the
squares of the roots of the equation
λ2 −αωλ + ω −1 = 0.
It follows that
ρ(Lω) =
max
λ | pJ(λ)=0{max(|µ+(α, ω)|, |µ−(α, ω)|)},
and since we are assuming that J has real roots, we are led to study the
function
M(α, ω) = max{|µ+(α, ω)|, |µ−(α, ω)|},

360
Iterative Methods for Solving Linear Systems
where α ∈R and ω ∈(0, 2). Actually, because M(−α, ω) = M(α, ω), it is
only necessary to consider the case where α ≥0.
Note that for α ̸= 0, the roots of the equation
λ2 −αωλ + ω −1 = 0.
are
αω ±
√
α2ω2 −4ω + 4
2
.
In turn, this leads to consider the roots of the equation
ω2α2 −4ω + 4 = 0,
which are
2(1 ±
√
1 −α2)
α2
,
for α ̸= 0. Since we have
2(1 +
√
1 −α2)
α2
= 2(1 +
√
1 −α2)(1 −
√
1 −α2)
α2(1 −
√
1 −α2)
=
2
1 −
√
1 −α2
and
2(1 −
√
1 −α2)
α2
= 2(1 +
√
1 −α2)(1 −
√
1 −α2)
α2(1 +
√
1 −α2)
=
2
1 +
√
1 −α2 ,
these roots are
ω0(α) =
2
1 +
√
1 −α2 ,
ω1(α) =
2
1 −
√
1 −α2 .
Observe that the expression for ω0(α) is exactly the expression in the state-
ment of our proposition! The rest of the proof consists in analyzing the
variations of the function M(α, ω) by considering various cases for α. In
the end, we ﬁnd that the minimum of ρ(Lω) is obtained for ω0(ρ(J)). The
details are tedious and we omit them. The reader will ﬁnd complete proofs
in Serre [Serre (2010)] and Ciarlet [Ciarlet (1989)].
Combining the results of Theorem 9.3 and Proposition 9.6, we obtain
the following result which gives precise information about the spectral radii
of the matrices J, L1, and Lω.
Proposition 9.7. Let A be a tridiagonal matrix (possibly by blocks) which
is Hermitian positive deﬁnite. Then the methods of Jacobi, Gauss-Seidel,

9.5. Convergence Methods for Tridiagonal Matrices
361
and relaxation, all converge for ω ∈(0, 2).
There is a unique optimal
relaxation parameter
ω0 =
2
1 +
p
1 −(ρ(J))2 ,
such that
ρ(Lω0) =
inf
0<ω<2 ρ(Lω) = ω0 −1.
Furthermore, if ρ(J) > 0, then
ρ(Lω0) < ρ(L1) = (ρ(J))2 < ρ(J),
and if ρ(J) = 0, then ω0 = 1 and ρ(L1) = ρ(J) = 0.
Proof. In order to apply Proposition 9.6, we have to check that J =
D−1(E + F) has real eigenvalues. However, if α is any eigenvalue of J
and if u is any corresponding eigenvector, then
D−1(E + F)u = αu
implies that
(E + F)u = αDu,
and since A = D −E −F, the above shows that (D −A)u = αDu, that is,
Au = (1 −α)Du.
Consequently,
u∗Au = (1 −α)u∗Du,
and since A and D are Hermitian positive deﬁnite, we have u∗Au > 0
and u∗Du > 0 if u ̸= 0, which proves that α ∈R. The rest follows from
Theorem 9.3 and Proposition 9.6.
Remark: It is preferable to overestimate rather than underestimate the
relaxation parameter when the optimum relaxation parameter is not known
exactly.

362
Iterative Methods for Solving Linear Systems
9.6
Summary
The main concepts and results of this chapter are listed below:
• Iterative methods. Splitting A as A = M −N.
• Convergence of a sequence of vectors or matrices.
• A criterion for the convergence of the sequence (Bk) of powers of a
matrix B to zero in terms of the spectral radius ρ(B).
• A characterization of the spectral radius ρ(B) as the limit of the se-
quence (∥Bk∥1/k).
• A criterion of the convergence of iterative methods.
• Asymptotic behavior of iterative methods.
• Splitting A as A = D−E−F, and the methods of Jacobi, Gauss-Seidel,
and relaxation (and SOR).
• The Jacobi matrix, J = D−1(E + F).
• The Gauss-Seidel matrix, L1 = (D −E)−1F.
• The matrix of relaxation, Lω = (D −ωE)−1((1 −ω)D + ωF).
• Convergence of iterative methods: a general result when A = M −N
is Hermitian positive deﬁnite.
• A suﬃcient condition for the convergence of the methods of Jacobi,
Gauss-Seidel, and relaxation. The Ostrowski-Reich theorem: A is Her-
mitian positive deﬁnite and ω ∈(0, 2).
• A necessary condition for the convergence of the methods of Jacobi,
Gauss-Seidel,and relaxation: ω ∈(0, 2).
• The case of tridiagonal matrices (possibly by blocks).
Simultane-
ous convergence or divergence of Jacobi's method and Gauss-Seidel's
method, and comparison of the spectral radii of ρ(J) and ρ(L1):
ρ(L1) = (ρ(J))2.
• The case of tridiagonal Hermitian positive deﬁnite matrices (possibly
by blocks). The methods of Jacobi, Gauss-Seidel, and relaxation, all
converge.
• In the above case, there is a unique optimal relaxation parameter for
which ρ(Lω0) < ρ(L1) = (ρ(J))2 < ρ(J) (if ρ(J) ̸= 0).

9.7. Problems
363
9.7
Problems
Problem 9.1. Consider the matrix
A =


1 2 −2
1 1 1
2 2 1

.
Prove that ρ(J) = 0 and ρ(L1) = 2, so
ρ(J) < 1 < ρ(L1),
where J is Jacobi's matrix and L1 is the matrix of Gauss-Seidel.
Problem 9.2. Consider the matrix
A =


2 −1 1
2
2 2
−1 −1 2

.
Prove that ρ(J) =
√
5/2 and ρ(L1) = 1/2, so
ρ(L1) < ρ(J),
where where J is Jacobi's matrix and L1 is the matrix of Gauss-Seidel.
Problem 9.3. Consider the following linear system:




2 −1 0
0
−1 2 −1 0
0 −1 2 −1
0
0 −1 2








x1
x2
x3
x4



=




19
19
−3
−12



.
(1) Solve the above system by Gaussian elimination.
(2) Compute the sequences of vectors uk = (uk
1, uk
2, uk
3, uk
4) for k =
1, . . . , 10, using the methods of Jacobi, Gauss-Seidel, and relaxation for
the following values of ω: ω = 1.1, 1.2, . . . , 1.9.
In all cases, the initial
vector is u0 = (0, 0, 0, 0).
Problem 9.4. Recall that a complex or real n × n matrix A is strictly row
diagonally dominant if |aii| > Pn
j=1,j̸=i |aij| for i = 1, . . . , n.
(1) Prove that if A is strictly row diagonally dominant, then Jacobi's
method converges.
(2) Prove that if A is strictly row diagonally dominant, then Gauss-
Seidel's method converges.

364
Iterative Methods for Solving Linear Systems
Problem 9.5. Prove that the converse of Proposition 9.3 holds. That is,
if A is a Hermitian positive deﬁnite matrix written as A = M −N with
M invertible, if the Hermitan matrix M ∗+ N is positive deﬁnite, and if
ρ(M −1N) < 1, then A is positive deﬁnite.
Problem 9.6. Consider the following tridiagonal n × n matrix:
A =
1
(n + 1)2







2 −1 0
−1 2 −1
... ... ...
−1 2 −1
0 −1 2







.
(1) Prove that the eigenvalues of the Jacobi matrix J are given by
λk = cos
 kπ
n + 1

,
k = 1, . . . , n.
Hint. First show that the Jacobi matrix is
J = 1
2







0 1
0
1 0
1
... ... ...
1
0 1
0
1 0







.
Then the eigenvalues and the eigenvectors of J are solutions of the system
of equations
y0 = 0
yk+1 + yk−1 = 2λyk,
k = 1, . . . , n
yn+1 = 0.
It is well known that the general solution to the above recurrence is given
by
yk = αzk
1 + βzk
2,
k = 0, . . . , n + 1,
(with α, β ̸= 0) where z1 and z2 are the zeros of the equation
z2 −2λz + 1 = 0.
It follows that z2 = z−1
1
and z1 + z2 = 2λ. The boundary condition y0 = 0
yields α+β = 0, so yk = α(zk
1 −z−k
1 ), and the boundary condition yn+1 = 0
yields
z2(n+1)
1
= 1.

9.7. Problems
365
Deduce that we may assume that the n possible values (z1)k for z1 are given
by
(z1)k = e
kπi
n+1 ,
k = 1, . . . , n,
and ﬁnd
2λk = (z1)k + (z1)−1
k .
Show that an eigenvector (y(k)
1 , . . . , y(k)
n ) associated with the eigenvalue λk
is given by
y(k)
j
= sin
 kjπ
n + 1

,
j = 1, . . . , n.
(2) Find the spectral radius ρ(J), ρ(L1), and ρ(Lω0), as functions of
h = 1/(n + 1).

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 10
The Dual Space and Duality
In this chapter all vector spaces are deﬁned over an arbitrary ﬁeld K. For
the sake of concreteness, the reader may safely assume that K = R.
10.1
The Dual Space E∗and Linear Forms
In Section 2.8 we deﬁned linear forms, the dual space E∗= Hom(E, K) of
a vector space E, and showed the existence of dual bases for vector spaces
of ﬁnite dimension.
In this chapter we take a deeper look at the connection between a space
E and its dual space E∗. As we will see shortly, every linear map f : E →F
gives rise to a linear map f ⊤: F ∗→E∗, and it turns out that in a suitable
basis, the matrix of f ⊤is the transpose of the matrix of f.
Thus, the
notion of dual space provides a conceptual explanation of the phenomena
associated with transposition.
But it does more, because it allows us to view a linear equation as an
element of the dual space E∗, and thus to view subspaces of E as solutions of
sets of linear equations and vice-versa. The relationship between subspaces
and sets of linear forms is the essence of duality, a term which is often used
loosely, but can be made precise as a bijection between the set of subspaces
of a given vector space E and the set of subspaces of its dual E∗. In this
correspondence, a subspace V of E yields the subspace V 0 of E∗consisting
of all linear forms that vanish on V (that is, have the value zero for all
input in V ).
Consider the following set of two "linear equations" in R3,
x −y + z = 0
x −y −z = 0,
367

368
The Dual Space and Duality
and let us ﬁnd out what is their set V of common solutions (x, y, z) ∈R3.
By subtracting the second equation from the ﬁrst, we get 2z = 0, and by
adding the two equations, we ﬁnd that 2(x−y) = 0, so the set V of solutions
is given by
y = x
z = 0.
This is a one dimensional subspace of R3. Geometrically, this is the line of
equation y = x in the plane z = 0 as illustrated by Figure 10.1.
Fig. 10.1
The intersection of the magenta plane x −y + z = 0 with the blue-gray plane
x −y −z = 0 is the pink line y = x.
Now why did we say that the above equations are linear? Because as
functions of (x, y, z), both maps f1 : (x, y, z) 
→x−y+z and f2 : (x, y, z) 
→
x −y −z are linear. The set of all such linear functions from R3 to R
is a vector space; we used this fact to form linear combinations of the
"equations" f1 and f2. Observe that the dimension of the subspace V is 1.
The ambient space has dimension n = 3 and there are two "independent"
equations f1, f2, so it appears that the dimension dim(V ) of the subspace
V deﬁned by m independent equations is
dim(V ) = n −m,
which is indeed a general fact (proven in Theorem 10.1).

10.1. The Dual Space E∗and Linear Forms
369
More generally, in Rn, a linear equation is determined by an n-tuple
(a1, . . . , an) ∈Rn, and the solutions of this linear equation are given by the
n-tuples (x1, . . . , xn) ∈Rn such that
a1x1 + · · · + anxn = 0;
these solutions constitute the kernel of the linear map (x1, . . . , xn) 7→a1x1+
· · · + anxn. The above considerations assume that we are working in the
canonical basis (e1, . . . , en) of Rn, but we can deﬁne "linear equations"
independently of bases and in any dimension, by viewing them as elements
of the vector space Hom(E, K) of linear maps from E to the ﬁeld K.
Deﬁnition 10.1. Given a vector space E, the vector space Hom(E, K) of
linear maps from E to the ﬁeld K is called the dual space (or dual) of E.
The space Hom(E, K) is also denoted by E∗, and the linear maps in E∗
are called the linear forms, or covectors. The dual space E∗∗of the space
E∗is called the bidual of E.
As a matter of notation, linear forms f : E →K will also be denoted
by starred symbol, such as u∗, x∗, etc.
Given a vector space E and any basis (ui)i∈I for E, we can associate to
each ui a linear form u∗
i ∈E∗, and the u∗
i have some remarkable properties.
Deﬁnition 10.2. Given a vector space E and any basis (ui)i∈I for E, by
Proposition 2.14, for every i ∈I, there is a unique linear form u∗
i such that
u∗
i (uj) =
 1
if i = j
0
if i ̸= j,
for every j ∈I. The linear form u∗
i is called the coordinate form of index i
w.r.t. the basis (ui)i∈I.
The reason for the terminology coordinate form was explained in Sec-
tion 2.8.
We proved in Theorem 2.3 that if (u1, . . . , un) is a basis of E, then
(u∗
1, . . . , u∗
n) is a basis of E∗called the dual basis.
If (u1, . . . , un) is a basis of Rn (more generally Kn), it is possible to ﬁnd
explicitly the dual basis (u∗
1, . . . , u∗
n), where each u∗
i is represented by a row
vector.
Example 10.1. For example, consider the columns of the B´ezier matrix
B4 =




1 −3 3 −1
0 3 −6 3
0 0
3 −3
0 0
0
1



.

370
The Dual Space and Duality
In other words, we have the basis
u1 =




1
0
0
0




u2 =




−3
3
0
0




u3 =




3
−6
3
0




u4 =




−1
3
−3
1



.
Since the form u∗
1 is deﬁned by the conditions u∗
1(u1) = 1, u∗
1(u2) =
0, u∗
1(u3) = 0, u∗
1(u4) = 0, it is represented by a row vector (λ1 λ2 λ3 λ4)
such that
 λ1 λ2 λ3 λ4





1 −3 3 −1
0 3 −6 3
0 0
3 −3
0 0
0
1



=
 1 0 0 0

.
This implies that u∗
1 is the ﬁrst row of the inverse of B4. Since
B−1
4
=




1 1
1 1
0 1/3 2/3 1
0 0 1/3 1
0 0
0 1



,
the linear forms (u∗
1, u∗
2, u∗
3, u∗
4) correspond to the rows of B−1
4 . In particu-
lar, u∗
1 is represented by (1 1 1 1).
The above method works for any n. Given any basis (u1, . . . , un) of Rn,
if P is the n × n matrix whose jth column is uj, then the dual form u∗
i is
given by the ith row of the matrix P −1.
When E is of ﬁnite dimension n and (u1, . . . , un) is a basis of E, by
Theorem 10.1 (1), the family (u∗
1, . . . , u∗
n) is a basis of the dual space E∗.
Let us see how the coordinates of a linear form ϕ∗∈E∗over the dual basis
(u∗
1, . . . , u∗
n) vary under a change of basis.
Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and let P = (ai j)
be the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn), so that
vj =
n
X
i=1
ai jui,
and let P −1 = (bi j) be the inverse of P, so that
ui =
n
X
j=1
bj ivj.
For ﬁxed j, where 1 ≤j ≤n, we want to ﬁnd scalars (ci)n
i=1 such that
v∗
j = c1u∗
1 + c2u∗
2 + · · · + cnu∗
n.

10.1. The Dual Space E∗and Linear Forms
371
To ﬁnd each ci, we evaluate the above expression at ui. Since u∗
i (uj) = δi j
and v∗
i (vj) = δi j, we get
v∗
j (ui) = (c1u∗
1 + c2u∗
2 + · · · + cnu∗
n)(ui) = ci
v∗
j (ui) = v∗
j
 n
X
k=1
bk ivk
!
= bj i,
and thus
v∗
j =
n
X
i=1
bj iu∗
i .
Similar calculations show that
u∗
i =
n
X
j=1
ai jv∗
j .
This means that the change of basis from the dual basis (u∗
1, . . . , u∗
n) to the
dual basis (v∗
1, . . . , v∗
n) is (P −1)⊤. Since
ϕ∗=
n
X
i=1
ϕiu∗
i =
n
X
i=1
ϕi
n
X
j=1
aijv∗
j =
n
X
j=1
 n
X
i=1
aijϕi
!
vj =
n
X
i=1
ϕ′
iv∗
i ,
we get
ϕ′
j =
n
X
i=1
ai jϕi,
so the new coordinates ϕ′
j are expressed in terms of the old coordinates
ϕi using the matrix P ⊤.
If we use the row vectors (ϕ1, . . . , ϕn) and
(ϕ′
1, . . . , ϕ′
n), we have
(ϕ′
1, . . . , ϕ′
n) = (ϕ1, . . . , ϕn)P.
These facts are summarized in the following proposition.
Proposition 10.1. Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and
let P = (ai j) be the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn),
so that
vj =
n
X
i=1
ai jui.
Then the change of basis from the dual basis (u∗
1, . . . , u∗
n) to the dual basis
(v∗
1, . . . , v∗
n) is (P −1)⊤, and for any linear form ϕ, the new coordinates ϕ′
j
of ϕ are expressed in terms of the old coordinates ϕi of ϕ using the matrix
P ⊤; that is,
(ϕ′
1, . . . , ϕ′
n) = (ϕ1, . . . , ϕn)P.

372
The Dual Space and Duality
To best understand the preceding paragraph, recall Example 3.1, in
which E = R2, u1 = (1, 0), u2 = (0, 1), and v1 = (1, 1), v2 = (−1, 1). Then
P, the change of basis matrix from (u1, u2) to (v1, v2), is given by
P =
1 −1
1 1

,
with (v1, v2) = (u1, u2)P, and (u1, u2) = (v1, v2)P −1, where
P −1 =
 1/2 1/2
−1/2 1/2

.
Let (u∗
1, u∗
2) be the dual basis for (u1, u2) and (v∗
1, v∗
2) be the dual basis for
(v1, v2). We claim that
(v∗
1, v∗
2) = (u∗
1, u∗
2)
1/2 −1/2
1/2 1/2

= (u∗
1, u∗
2)(P −1)⊤.
Indeed, since v∗
1 = c1u∗
1 + c2u∗
2 and v∗
2 = C1u∗
1 + C2u∗
2 we ﬁnd that
c1 = v∗
1(u1) = v∗
1(1/2v1 −1/2v2) = 1/2
c2 = v∗
1(u2) = v∗
1(1/2v1 + 1/2v2) = 1/2
C1 = v∗
2(u1) = v∗
2(1/2v1 −1/2v2) = −1/2
C2 = v∗
2(u2) = v∗
1(1/2v1 + 1/2v2) = 1/2.
Furthermore, since (u∗
1, u∗
2) = (v∗
1, v∗
2)P ⊤(since (v∗
1, v∗
2) = (u∗
1, u∗
2)(P ⊤)−1),
we ﬁnd that
ϕ∗= ϕ1u∗
1 + ϕ2u∗
2 = ϕ1(v∗
1 −v∗
2) + ϕ(v∗
1 + v∗
2)
= (ϕ1 + ϕ2)v∗
1 + (−ϕ1 + ϕ2)v∗
2 = ϕ′
1v∗
1 + ϕ′
2v.
2
Hence
 1 1
−1 1
 ϕ1
ϕ2

=
ϕ′
1
ϕ′
2

,
where
P ⊤=
 1 1
−1 1

.
Comparing with the change of basis
vj =
n
X
i=1
ai jui,
we note that this time, the coordinates (ϕi) of the linear form ϕ∗change in
the same direction as the change of basis. For this reason, we say that the

10.1. The Dual Space E∗and Linear Forms
373
coordinates of linear forms are covariant. By abuse of language, it is often
said that linear forms are covariant, which explains why the term covector
is also used for a linear form.
Observe that if (e1, . . . , en) is a basis of the vector space E, then, as a
linear map from E to K, every linear form f ∈E∗is represented by a 1×n
matrix, that is, by a row vector
(λ1 · · · λn),
with respect to the basis (e1, . . . , en) of E, and 1 of K, where f(ei) = λi.
A vector u = Pn
i=1 uiei ∈E is represented by a n × 1 matrix, that is, by a
column vector



u1
...
un


,
and the action of f on u, namely f(u), is represented by the matrix product
 λ1 · · · λn




u1
...
un


= λ1u1 + · · · + λnun.
On the other hand, with respect to the dual basis (e∗
1, . . . , e∗
n) of E∗, the
linear form f is represented by the column vector



λ1
...
λn


.
Remark: In many texts using tensors, vectors are often indexed with lower
indices. If so, it is more convenient to write the coordinates of a vector x
over the basis (u1, . . . , un) as (xi), using an upper index, so that
x =
n
X
i=1
xiui,
and in a change of basis, we have
vj =
n
X
i=1
ai
jui
and
xi =
n
X
j=1
ai
jx′j.

374
The Dual Space and Duality
Dually, linear forms are indexed with upper indices.
Then it is more
convenient to write the coordinates of a covector ϕ∗over the dual basis
(u∗1, . . . , u∗n) as (ϕi), using a lower index, so that
ϕ∗=
n
X
i=1
ϕiu∗i
and in a change of basis, we have
u∗i =
n
X
j=1
ai
jv∗j
and
ϕ′
j =
n
X
i=1
ai
jϕi.
With these conventions, the index of summation appears once in upper
position and once in lower position, and the summation sign can be safely
omitted, a trick due to Einstein. For example, we can write
ϕ′
j = ai
jϕi
as an abbreviation for
ϕ′
j =
n
X
i=1
ai
jϕi.
For another example of the use of Einstein's notation, if the vectors
(v1, . . . , vn) are linear combinations of the vectors (u1, . . . , un), with
vi =
n
X
j=1
aijuj,
1 ≤i ≤n,
then the above equations are written as
vi = aj
iuj,
1 ≤i ≤n.
Thus, in Einstein's notation, the n × n matrix (aij) is denoted by (aj
i), a
(1, 1)-tensor.

Beware that some authors view a matrix as a mapping between
coordinates, in which case the matrix (aij) is denoted by (ai
j).

10.2. Pairing and Duality Between E and E∗
375
10.2
Pairing and Duality Between E and E∗
Given a linear form u∗∈E∗and a vector v ∈E, the result u∗(v) of
applying u∗to v is also denoted by ⟨u∗, v⟩. This deﬁnes a binary operation
⟨−, −⟩: E∗× E →K satisfying the following properties:
⟨u∗
1 + u∗
2, v⟩= ⟨u∗
1, v⟩+ ⟨u∗
2, v⟩
⟨u∗, v1 + v2⟩= ⟨u∗, v1⟩+ ⟨u∗, v2⟩
⟨λu∗, v⟩= λ⟨u∗, v⟩
⟨u∗, λv⟩= λ⟨u∗, v⟩.
The above identities mean that ⟨−, −⟩is a bilinear map, since it is linear
in each argument. It is often called the canonical pairing between E∗and
E. In view of the above identities, given any ﬁxed vector v ∈E, the map
evalv : E∗→K (evaluation at v) deﬁned such that
evalv(u∗) = ⟨u∗, v⟩= u∗(v)
for every u∗∈E∗
is a linear map from E∗to K, that is, evalv is a linear form in E∗∗. Again,
from the above identities, the map evalE : E →E∗∗, deﬁned such that
evalE(v) = evalv
for every v ∈E,
is a linear map. Observe that
evalE(v)(u∗) = evalv(u∗) = ⟨u∗, v⟩= u∗(v),
for all v ∈E and all u∗∈E∗.
We shall see that the map evalE is injective, and that it is an isomorphism
when E has ﬁnite dimension.
We now formalize the notion of the set V 0 of linear equations vanishing
on all vectors in a given subspace V ⊆E, and the notion of the set U 0 of
common solutions of a given set U ⊆E∗of linear equations. The duality
theorem (Theorem 10.1) shows that the dimensions of V and V 0, and the
dimensions of U and U 0, are related in a crucial way. It also shows that,
in ﬁnite dimension, the maps V 7→V 0 and U 7→U 0 are inverse bijections
from subspaces of E to subspaces of E∗.
Deﬁnition 10.3. Given a vector space E and its dual E∗, we say that a
vector v ∈E and a linear form u∗∈E∗are orthogonal iﬀ⟨u∗, v⟩= 0.
Given a subspace V of E and a subspace U of E∗, we say that V and U are
orthogonal iﬀ⟨u∗, v⟩= 0 for every u∗∈U and every v ∈V . Given a subset
V of E (resp. a subset U of E∗), the orthogonal V 0 of V is the subspace
V 0 of E∗deﬁned such that
V 0 = {u∗∈E∗| ⟨u∗, v⟩= 0, for every v ∈V }
(resp. the orthogonal U 0 of U is the subspace U 0 of E deﬁned such that
U 0 = {v ∈E | ⟨u∗, v⟩= 0, for every u∗∈U}).

376
The Dual Space and Duality
The subspace V 0 ⊆E∗is also called the annihilator of V . The subspace
U 0 ⊆E annihilated by U ⊆E∗does not have a special name. It seems
reasonable to call it the linear subspace (or linear variety) deﬁned by U.
Informally, V 0 is the set of linear equations that vanish on V , and U 0
is the set of common zeros of all linear equations in U. We can also deﬁne
V 0 by
V 0 = {u∗∈E∗| V ⊆Ker u∗}
and U 0 by
U 0 =

u∗∈U
Ker u∗.
Observe that E0 = {0} = (0), and {0}0 = E∗.
Proposition 10.2. If V1 ⊆V2 ⊆E, then V 0
2 ⊆V 0
1 ⊆E∗, and if U1 ⊆
U2 ⊆E∗, then U 0
2 ⊆U 0
1 ⊆E. See Figure 10.2.
E
E
E *
E *
V
V
V
V
1
2
1
0
0
2
V
V
U
U
1
2
U
1
0
U
2
0
Fig. 10.2
The top pair of ﬁgures schematically illustrates the relation if V1 ⊆V2 ⊆E,
then V 0
2
⊆V 0
1
⊆E∗, while the bottom pair of ﬁgures illustrates the relationship if
U1 ⊆U2 ⊆E∗, then U0
2 ⊆U0
1 ⊆E.
Proof. Indeed, if V1 ⊆V2 ⊆E, then for any f ∗∈V 0
2 we have f ∗(v) = 0
for all v ∈V2, and thus f ∗(v) = 0 for all v ∈V1, so f ∗∈V 0
1 . Similarly, if
U1 ⊆U2 ⊆E∗, then for any v ∈U 0
2 , we have f ∗(v) = 0 for all f ∗∈U2, so
f ∗(v) = 0 for all f ∗∈U1, which means that v ∈U 0
1 .

10.2. Pairing and Duality Between E and E∗
377
Here are some examples.
Example 10.2. Let E = M2(R), the space of real 2 × 2 matrices, and let
V be the subspace of M2(R) spanned by the matrices
0 1
1 0

,
1 0
0 0

,
0 0
0 1

.
We check immediately that the subspace V consists of all matrices of the
form
b a
a c

,
that is, all symmetric matrices. The matrices
a11 a12
a21 a22

in V satisfy the equation
a12 −a21 = 0,
and all scalar multiples of these equations, so V 0 is the subspace of E∗
spanned by the linear form given by u∗(a11, a12, a21, a22) = a12 −a21. By
the duality theorem (Theorem 10.1) we have
dim(V 0) = dim(E) −dim(V ) = 4 −3 = 1.
Example 10.3. The above example generalizes to E = Mn(R) for any
n ≥1, but this time, consider the space U of linear forms asserting that a
matrix A is symmetric; these are the linear forms spanned by the n(n−1)/2
equations
aij −aji = 0,
1 ≤i < j ≤n.
Note there are no constraints on diagonal entries, and half of the equations
aij −aji = 0,
1 ≤i ̸= j ≤n
are redundant. It is easy to check that the equations (linear forms) for
which i < j are linearly independent. To be more precise, let U be the
space of linear forms in E∗spanned by the linear forms
u∗
ij(a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij −aji,
1 ≤i < j ≤n.
The dimension of U is n(n −1)/2. Then the set U 0 of common solutions
of these equations is the space S(n) of symmetric matrices. By the duality
theorem (Theorem 10.1), this space has dimension
n(n + 1)
2
= n2 −n(n −1)
2
.
We leave it as an exercise to ﬁnd a basis of S(n).

378
The Dual Space and Duality
Example 10.4. If E = Mn(R), consider the subspace U of linear forms in
E∗spanned by the linear forms
u∗
ij(a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij + aji,
1 ≤i < j ≤n
u∗
ii(a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aii,
1 ≤i ≤n.
It is easy to see that these linear forms are linearly independent, so
dim(U) = n(n + 1)/2. The space U 0 of matrices A ∈Mn(R) satisfying
all of the above equations is clearly the space Skew(n) of skew-symmetric
matrices. By the duality theorem (Theorem 10.1), the dimension of U 0 is
n(n −1)
2
= n2 −n(n + 1)
2
.
We leave it as an exercise to ﬁnd a basis of Skew(n).
Example 10.5. For yet another example with E = Mn(R), for any A ∈
Mn(R), consider the linear form in E∗given by
tr(A) = a11 + a22 + · · · + ann,
called the trace of A. The subspace U 0 of E consisting of all matrices A
such that tr(A) = 0 is a space of dimension n2 −1. We leave it as an
exercise to ﬁnd a basis of this space.
The dimension equations
dim(V ) + dim(V 0) = dim(E)
dim(U) + dim(U 0) = dim(E)
are always true (if E is ﬁnite-dimensional).
This is part of the duality
theorem (Theorem 10.1).
Remark: In contrast with the previous examples, given a matrix A ∈
Mn(R), the equations asserting that A⊤A = I are not linear constraints.
For example, for n = 2, we have
a2
11 + a2
21 = 1
a2
21 + a2
22 = 1
a11a12 + a21a22 = 0.

10.2. Pairing and Duality Between E and E∗
379
Remarks:
(1) The notation V 0 (resp. U 0) for the orthogonal of a subspace V of E
(resp. a subspace U of E∗) is not universal.
Other authors use the
notation V ⊥(resp. U ⊥). However, the notation V ⊥is also used to
denote the orthogonal complement of a subspace V with respect to an
inner product on a space E, in which case V ⊥is a subspace of E and
not a subspace of E∗(see Chapter 11). To avoid confusion, we prefer
using the notation V 0.
(2) Since linear forms can be viewed as linear equations (at least in ﬁnite
dimension), given a subspace (or even a subset) U of E∗, we can deﬁne
the set Z(U) of common zeros of the equations in U by
Z(U) = {v ∈E | u∗(v) = 0, for all u∗∈U}.
Of course Z(U) = U 0, but the notion Z(U) can be generalized to more
general kinds of equations, namely polynomial equations. In this more
general setting, U is a set of polynomials in n variables with coeﬃcients
in a ﬁeld K (where n = dim(E)). Sets of the form Z(U) are called
algebraic varieties. Linear forms correspond to the special case where
homogeneous polynomials of degree 1 are considered.
If V is a subset of E, it is natural to associate with V the set of poly-
nomials in K[X1, . . . , Xn] that vanish on V . This set, usually denoted
I(V ), has some special properties that make it an ideal. If V is a linear
subspace of E, it is natural to restrict our attention to the space V 0 of
linear forms that vanish on V , and in this case we identify I(V ) and
V 0 (although technically, I(V ) is no longer an ideal).
For any arbitrary set of polynomials U ⊆K[X1, . . . , Xn] (resp. subset
V ⊆E), the relationship between I(Z(U)) and U (resp. Z(I(V )) and
V ) is generally not simple, even though we always have
U ⊆I(Z(U))
(resp.
V ⊆Z(I(V ))).
However, when the ﬁeld K is algebraically closed, then I(Z(U)) is
equal to the radical of the ideal U, a famous result due to Hilbert
known as the Nullstellensatz (see Lang [Lang (1993)] or Dummit and
Foote [Dummit and Foote (1999)]). The study of algebraic varieties
is the main subject of algebraic geometry, a beautiful but formidable
subject. For a taste of algebraic geometry, see Lang [Lang (1993)] or
Dummit and Foote [Dummit and Foote (1999)].
The duality theorem (Theorem 10.1) shows that the situation is much
simpler if we restrict our attention to linear subspaces; in this case
U = I(Z(U))
and
V = Z(I(V )).

380
The Dual Space and Duality
Proposition 10.3. We have V ⊆V 00 for every subspace V of E, and
U ⊆U 00 for every subspace U of E∗.
Proof. Indeed, for any v ∈V , to show that v ∈V 00 we need to prove that
u∗(v) = 0 for all u∗∈V 0. However, V 0 consists of all linear forms u∗such
that u∗(y) = 0 for all y ∈V ; in particular, for a ﬁxed v ∈V , we have
u∗(v) = 0 for all u∗∈V 0, as required.
Similarly, for any u∗∈U, to show that u∗∈U 00 we need to prove
that u∗(v) = 0 for all v ∈U 0. However, U 0 consists of all vectors v such
that f ∗(v) = 0 for all f ∗∈U; in particular, for a ﬁxed u∗∈U, we have
u∗(v) = 0 for all v ∈U 0, as required.
We will see shortly that in ﬁnite dimension, we have V = V 00 and
U = U 00.
10.3
The Duality Theorem and Some Consequences
Given a vector space E of dimension n ≥1 and a subspace U of E, by
Theorem 2.2, every basis (u1, . . . , um) of U can be extended to a basis
(u1, . . . , un) of E. We have the following important theorem adapted from
E. Artin [Artin (1957)] (Chapter 1).
Theorem 10.1. (Duality theorem) Let E be a vector space of dimension
n. The following properties hold:
(a) For every basis (u1, . . . , un) of E, the family of coordinate forms
(u∗
1, . . . , u∗
n) is a basis of E∗(called the dual basis of (u1, . . . , un)).
(b) For every subspace V of E, we have V 00 = V .
(c) For every pair of subspaces V and W of E such that E = V ⊕W,
with V of dimension m, for every basis (u1, . . . , un) of E such that
(u1, . . . , um) is a basis of V and (um+1, . . . , un) is a basis of W, the
family (u∗
1, . . . , u∗
m) is a basis of the orthogonal W 0 of W in E∗, so that
dim(W) + dim(W 0) = dim(E).
Furthermore, we have W 00 = W.
(d) For every subspace U of E∗, we have
dim(U) + dim(U 0) = dim(E),
where U 0 is the orthogonal of U in E, and U 00 = U.

10.3. The Duality Theorem and Some Consequences
381
Proof. (a) This part was proven in Theorem 2.3.
(b) By Proposition 10.3 we have V ⊆V 00.
If V ̸= V 00, then let
(u1, . . . , up) be a basis of V 00 such that (u1, . . . , um) is a basis of V , with
m < p. Since um+1 ∈V 00, um+1 is orthogonal to every linear form in
V 0. By deﬁnition we have u∗
m+1(ui) = 0 for all i = 1, . . . , m, and thus
u∗
m+1 ∈V 0. However, u∗
m+1(um+1) = 1, contradicting the fact that um+1
is orthogonal to every linear form in V 0. Thus, V = V 00.
(c) Every linear form f ∗∈W 0 is orthogonal to every uj for j = m +
1, . . . , n, and thus, f ∗(uj) = 0 for j = m + 1, . . . , n. For such a linear form
f ∗∈W 0, let
g∗= f ∗(u1)u∗
1 + · · · + f ∗(um)u∗
m.
We have g∗(ui) = f ∗(ui), for every i, 1 ≤i ≤m. Furthermore, by deﬁni-
tion, g∗vanishes on all uj with j = m + 1, . . . , n. Thus, f ∗and g∗agree on
the basis (u1, . . . , un) of E, and so g∗= f ∗. This shows that (u∗
1, . . . , u∗
m)
generates W 0, and since it is also a linearly independent family, (u∗
1, . . . , u∗
m)
is a basis of W 0. It is then obvious that dim(W) + dim(W 0) = dim(E),
and by Part (b), we have W 00 = W.
(d) The only remaining fact to prove is that U 00 = U. Let (f ∗
1 , . . . , f ∗
m)
be a basis of U. Note that the map h: E →Km deﬁned such that
h(v) = (f ∗
1 (v), . . . , f ∗
m(v))
for every v ∈E is a linear map, and that its kernel Ker h is precisely U 0.
Then by Proposition 5.1,
n = dim(E) = dim(Ker h) + dim(Im h) ≤dim(U 0) + m,
since dim(Im h) ≤m.
Thus, n −dim(U 0) ≤m.
By (c), we have
dim(U 0) + dim(U 00) = dim(E) = n, so we get dim(U 00) ≤m. However,
by Proposition 10.3 it is clear that U ⊆U 00, which implies m = dim(U) ≤
dim(U 00), so dim(U) = dim(U 00) = m, and we must have U = U 00.
Part (a) of Theorem 10.1 shows that
dim(E) = dim(E∗),
and if (u1, . . . , un) is a basis of E, then (u∗
1, . . . , u∗
n) is a basis of the dual
space E∗called the dual basis of (u1, . . . , un).
Deﬁne the function E (E for equations) from subspaces of E to subspaces
of E∗and the function Z (Z for zeros) from subspaces of E∗to subspaces
of E by
E(V ) = V 0,
V ⊆E
Z(U) = U 0,
U ⊆E∗.

382
The Dual Space and Duality
By Parts (c) and (d) of Theorem 10.1,
(Z ◦E)(V ) = V 00 = V
(E ◦Z)(U) = U 00 = U,
so Z ◦E = id and E ◦Z = id, and the maps E and Z are inverse bijections.
These maps set up a duality between subspaces of E and subspaces of E∗. In
particular, every subspace V ⊆E of dimension m is the set of common zeros
of the space of linear forms (equations) V 0, which has dimension n−m. This
conﬁrms the claim we made about the dimension of the subspace deﬁned
by a set of linear equations.

One should be careful that this bijection does not hold if E has
inﬁnite dimension. Some restrictions on the dimensions of U and
V are needed.
Remark:
However, even if E is inﬁnite-dimensional, the identity V = V 00
holds for every subspace V of E. The proof is basically the same but uses
an inﬁnite basis of V 00 extending a basis of V .
We now discuss some applications of the duality theorem.
Problem 1. Suppose that V is a subspace of Rn of dimension m and
that (v1, . . . , vm) is a basis of V . The problem is to ﬁnd a basis of V 0.
We ﬁrst extend (v1, . . . , vm) to a basis (v1, . . . , vn) of Rn, and then by
part (c) of Theorem 10.1, we know that (v∗
m+1, . . . , v∗
n) is a basis of V 0.
Example 10.6. For example, suppose that V is the subspace of R4 spanned
by the two linearly independent vectors
v1 =




1
1
1
1




v2 =




1
1
−1
−1



,
the ﬁrst two vectors of the Haar basis in R4. The four columns of the Haar
matrix
W =




1 1
1
0
1 1 −1 0
1 −1 0
1
1 −1 0 −1




form a basis of R4, and the inverse of W is given by
W −1 =




1/4
0
0
0
0
1/4
0
0
0
0
1/2
0
0
0
0 1/2








1 1
1
1
1 1 −1 −1
1 −1 0
0
0 0
1 −1



=




1/4 1/4
1/4
1/4
1/4 1/4 −1/4 −1/4
1/2 −1/2
0
0
0
0
1/2 −1/2



.

10.3. The Duality Theorem and Some Consequences
383
Since the dual basis (v∗
1, v∗
2, v∗
3, v∗
4) is given by the rows of W −1, the last
two rows of W −1,
1/2 −1/2 0
0
0
0
1/2 −1/2

,
form a basis of V 0. We also obtain a basis by rescaling by the factor 1/2,
so the linear forms given by the row vectors
1 −1 0 0
0 0 1 −1

form a basis of V 0, the space of linear forms (linear equations) that vanish
on the subspace V .
The method that we described to ﬁnd V 0 requires ﬁrst extending a basis
of V and then inverting a matrix, but there is a more direct method. Indeed,
let A be the n×m matrix whose columns are the basis vectors (v1, . . . , vm)
of V . Then a linear form u represented by a row vector belongs to V 0 iﬀ
uvi = 0 for i = 1, . . . , m iﬀ
uA = 0
iﬀ
A⊤u⊤= 0.
Therefore, all we need to do is to ﬁnd a basis of the nullspace of A⊤. This
can be done quite eﬀectively using the reduction of a matrix to reduced row
echelon form (rref); see Section 7.10.
Example 10.7. For example, if we reconsider the previous example,
A⊤u⊤= 0 becomes
1 1 1
1
1 1 −1 −1





u1
u2
u3
u4



=
0
0

.
Since the rref of A⊤is
1 1 0 0
0 0 1 1

,
the above system is equivalent to
1 1 0 0
0 0 1 1





u1
u2
u3
u4



=
u1 + u2
u3 + u4

=
0
0

,

384
The Dual Space and Duality
where the free variables are associated with u2 and u4. Thus to determine
a basis for the kernel of A⊤, we set u2 = 1, u4 = 0 and u2 = 0, u4 = 1 and
obtain a basis for V 0 as
 1 −1 0 0

,
 0 0 1 −1

.
Problem 2. Let us now consider the problem of ﬁnding a basis of the
hyperplane H in Rn deﬁned by the equation
c1x1 + · · · + cnxn = 0.
More precisely, if u∗(x1, . . . , xn) is the linear form in (Rn)∗given by
u∗(x1, . . . , xn) = c1x1 + · · · + cnxn, then the hyperplane H is the ker-
nel of u∗. Of course we assume that some cj is nonzero, in which case the
linear form u∗spans a one-dimensional subspace U of (Rn)∗, and U 0 = H
has dimension n −1.
Since u∗is not the linear form which is identically zero, there is a small-
est positive index j ≤n such that cj ̸= 0, so our linear form is really
u∗(x1, . . . , xn) = cjxj + · · · + cnxn.
We claim that the following n −1
vectors (in Rn) form a basis of H:
1 2 . . . j −1 j
j + 1
. . .
n −1
1
2
...
j −1
j
j + 1
j + 2
...
n

















1 0 . . . 0
0
0
. . .
0
0 1 . . . 0
0
0
. . .
0
...
... ... ...
...
...
...
...
0 0 . . . 1
0
0
. . .
0
0 0 . . . 0 −cj+1/cj −cj+2/cj . . . −cn/cj
0 0 . . . 0
1
0
. . .
0
0 0 . . . 0
0
1
. . .
0
...
... ... ...
...
...
...
...
0 0 . . . 0
0
0
. . .
1

















.
Observe that the (n −1) × (n −1) matrix obtained by deleting row
j is the identity matrix, so the columns of the above matrix are lin-
early independent. A simple calculation also shows that the linear form
u∗(x1, . . . , xn) = cjxj+· · ·+cnxn vanishes on every column of the above ma-
trix. For a concrete example in R6, if u∗(x1, . . . , x6) = x3 +2x4 +3x5 +4x6,
we obtain the basis for the hyperplane H of equation
x3 + 2x4 + 3x5 + 4x6 = 0

10.3. The Duality Theorem and Some Consequences
385
given by the following matrix:









1 0 0
0
0
0 1 0
0
0
0 0 −2 −3 −4
0 0 1
0
0
0 0 0
1
0
0 0 0
0
1









.
Problem 3. Conversely, given a hyperplane H in Rn given as the span
of n −1 linearly vectors (u1, . . . , un−1), it is possible using determinants to
ﬁnd a linear form (λ1, . . . , λn) that vanishes on H.
In the case n = 3, we are looking for a row vector (λ1, λ2, λ3) such that
if
u =


u1
u2
u3


and
v =


v1
v2
v3


are two linearly independent vectors, then
u1 u2 u2
v1 v2 v2
 

λ1
λ2
λ3

=
0
0

,
and the cross-product u × v of u and v given by
u × v =


u2v3 −u3v2
u3v1 −u1v3
u1v2 −u2v1


is a solution. In other words, the equation of the plane spanned by u and
v is
(u2v3 −u3v2)x + (u3v1 −u1v3)y + (u1v2 −u2v1)z = 0.
Problem 4. Here is another example illustrating the power of Theo-
rem 10.1. Let E = Mn(R), and consider the equations asserting that the
sum of the entries in every row of a matrix A ∈Mn(R) is equal to the same
number. We have n −1 equations
n
X
j=1
(aij −ai+1j) = 0,
1 ≤i ≤n −1,
and it is easy to see that they are linearly independent. Therefore, the space
U of linear forms in E∗spanned by the above linear forms (equations) has
dimension n−1, and the space U 0 of matrices satisfying all these equations
has dimension n2 −n + 1. It is not so obvious to ﬁnd a basis for this space.
We will now pin down the relationship between a vector space E and
its bidual E∗∗.

386
The Dual Space and Duality
10.4
The Bidual and Canonical Pairings
Proposition 10.4. Let E be a vector space. The following properties hold:
(a) The linear map evalE : E →E∗∗deﬁned such that
evalE(v) = evalv
for all v ∈E,
that is, evalE(v)(u∗) = ⟨u∗, v⟩= u∗(v) for every u∗∈E∗, is injective.
(b) When E is of ﬁnite dimension n, the linear map evalE : E →E∗∗is
an isomorphism (called the canonical isomorphism).
Proof. (a) Let (ui)i∈I be a basis of E, and let v = P
i∈I viui. If evalE(v) =
0, then in particular evalE(v)(u∗
i ) = 0 for all u∗
i , and since
evalE(v)(u∗
i ) = ⟨u∗
i , v⟩= vi,
we have vi = 0 for all i ∈I, that is, v = 0, showing that evalE : E →E∗∗
is injective.
If E is of ﬁnite dimension n, by Theorem 10.1, for every basis
(u1, . . . , un), the family (u∗
1, . . . , u∗
n) is a basis of the dual space E∗, and
thus the family (u∗∗
1 , . . . , u∗∗
n ) is a basis of the bidual E∗∗.
This shows
that dim(E) = dim(E∗∗) = n, and since by Part (a), we know that
evalE : E →E∗∗is injective, in fact, evalE : E →E∗∗is bijective (by
Proposition 5.10).
When E is of ﬁnite dimension and (u1, . . . , un) is a basis of E, in view
of the canonical isomorphism evalE : E →E∗∗, the basis (u∗∗
1 , . . . , u∗∗
n ) of
the bidual is identiﬁed with (u1, . . . , un).
Proposition 10.4 can be reformulated very fruitfully in terms of pairings,
a remarkably useful concept discovered by Pontrjagin in 1931 (adapted from
E. Artin [Artin (1957)], Chapter 1). Given two vector spaces E and F over
a ﬁeld K, we say that a function ϕ: E × F →K is bilinear if for every
v ∈V , the map u 7→ϕ(u, v) (from E to K) is linear, and for every u ∈E,
the map v 7→ϕ(u, v) (from F to K) is linear.
Deﬁnition 10.4. Given two vector spaces E and F over K, a pairing
between E and F is a bilinear map ϕ: E × F →K.
Such a pairing is
nondegenerate iﬀ
(1) for every u ∈E, if ϕ(u, v) = 0 for all v ∈F, then u = 0, and
(2) for every v ∈F, if ϕ(u, v) = 0 for all u ∈E, then v = 0.

10.4. The Bidual and Canonical Pairings
387
A pairing ϕ: E × F →K is often denoted by ⟨−, −⟩: E × F →K. For
example, the map ⟨−, −⟩: E∗× E →K deﬁned earlier is a nondegenerate
pairing (use the proof of (a) in Proposition 10.4). If E = F and K = R, any
inner product on E is a nondegenerate pairing (because an inner product is
positive deﬁnite); see Chapter 11. Other interesting nondegenerate pairings
arise in exterior algebra and diﬀerential geometry.
Given a pairing ϕ: E × F →K, we can deﬁne two maps lϕ : E →F ∗
and rϕ : F →E∗as follows: For every u ∈E, we deﬁne the linear form
lϕ(u) in F ∗such that
lϕ(u)(y) = ϕ(u, y)
for every y ∈F,
and for every v ∈F, we deﬁne the linear form rϕ(v) in E∗such that
rϕ(v)(x) = ϕ(x, v)
for every x ∈E.
We have the following useful proposition.
Proposition 10.5. Given two vector spaces E and F over K, for every
nondegenerate pairing ϕ: E ×F →K between E and F, the maps lϕ : E →
F ∗and rϕ : F →E∗are linear and injective. Furthermore, if E and F
have ﬁnite dimension, then this dimension is the same and lϕ : E →F ∗
and rϕ : F →E∗are bijections.
Proof. The maps lϕ : E →F ∗and rϕ : F →E∗are linear because a pairing
is bilinear. If lϕ(u) = 0 (the null form), then
lϕ(u)(v) = ϕ(u, v) = 0
for every v ∈F,
and since ϕ is nondegenerate, u = 0.
Thus, lϕ : E →F ∗is injective.
Similarly, rϕ : F →E∗is injective. When F has ﬁnite dimension n, we
have seen that F and F ∗have the same dimension. Since lϕ : E →F ∗is
injective, we have m = dim(E) ≤dim(F) = n. The same argument applies
to E, and thus n = dim(F) ≤dim(E) = m. But then, dim(E) = dim(F),
and lϕ : E →F ∗and rϕ : F →E∗are bijections.
When E has ﬁnite dimension, the nondegenerate pairing ⟨−, −⟩: E∗×
E →K yields another proof of the existence of a natural isomorphism
between E and E∗∗. When E = F, the nondegenerate pairing induced by
an inner product on E yields a natural isomorphism between E and E∗
(see Section 11.2).
We now show the relationship between hyperplanes and linear forms.

388
The Dual Space and Duality
10.5
Hyperplanes and Linear Forms
Actually Proposition 10.6 below follows from Parts (c) and (d) of Theo-
rem 10.1, but we feel that it is also interesting to give a more direct proof.
Proposition 10.6. Let E be a vector space. The following properties hold:
(a) Given any nonnull linear form f ∗∈E∗, its kernel H = Ker f ∗is a
hyperplane.
(b) For any hyperplane H in E, there is a (nonnull) linear form f ∗∈E∗
such that H = Ker f ∗.
(c) Given any hyperplane H in E and any (nonnull) linear form f ∗∈E∗
such that H = Ker f ∗, for every linear form g∗∈E∗, H = Ker g∗iﬀ
g∗= λf ∗for some λ ̸= 0 in K.
Proof. (a) If f ∗∈E∗is nonnull, there is some vector v0 ∈E such that
f ∗(v0) ̸= 0. Let H = Ker f ∗. For every v ∈E, we have
f ∗

v −f ∗(v)
f ∗(v0)v0

= f ∗(v) −f ∗(v)
f ∗(v0)f ∗(v0) = f ∗(v) −f ∗(v) = 0.
Thus,
v −f ∗(v)
f ∗(v0)v0 = h ∈H,
and
v = h + f ∗(v)
f ∗(v0)v0,
that is, E = H + Kv0. Also since f ∗(v0) ̸= 0, we have v0 /∈H, that is,
H ∩Kv0 = 0. Thus, E = H ⊕Kv0, and H is a hyperplane.
(b) If H is a hyperplane, E = H ⊕Kv0 for some v0 /∈H. Then every
v ∈E can be written in a unique way as v = h + λv0. Thus there is a well-
deﬁned function f ∗: E →K, such that, f ∗(v) = λ, for every v = h + λv0.
We leave as a simple exercise the veriﬁcation that f ∗is a linear form. Since
f ∗(v0) = 1, the linear form f ∗is nonnull. Also, by deﬁnition, it is clear
that λ = 0 iﬀv ∈H, that is, Ker f ∗= H.
(c) Let H be a hyperplane in E, and let f ∗∈E∗be any (nonnull)
linear form such that H = Ker f ∗. Clearly, if g∗= λf ∗for some λ ̸= 0,
then H = Ker g∗. Conversely, assume that H = Ker g∗for some nonnull
linear form g∗. From (a), we have E = H ⊕Kv0, for some v0 such that
f ∗(v0) ̸= 0 and g∗(v0) ̸= 0. Then observe that
g∗−g∗(v0)
f ∗(v0)f ∗

10.6. Transpose of a Linear Map and of a Matrix
389
is a linear form that vanishes on H, since both f ∗and g∗vanish on H, but
also vanishes on Kv0. Thus, g∗= λf ∗, with
λ = g∗(v0)
f ∗(v0).
We leave as an exercise the fact that every subspace V ̸= E of a vector
space E is the intersection of all hyperplanes that contain V .
We now
consider the notion of transpose of a linear map and of a matrix.
10.6
Transpose of a Linear Map and of a Matrix
Given a linear map f : E →F, it is possible to deﬁne a map f ⊤: F ∗→E∗
which has some interesting properties.
Deﬁnition 10.5. Given a linear map f : E →F, the transpose f ⊤: F ∗→
E∗of f is the linear map deﬁned such that
f ⊤(v∗) = v∗◦f,
for every v∗∈F ∗,
as shown in the diagram below:
E
f
/
f ⊤(v∗)
 A
A
A
A
A
A
A
A
F
v∗

K.
Equivalently, the linear map f ⊤: F ∗→E∗is deﬁned such that
⟨v∗, f(u)⟩= ⟨f ⊤(v∗), u⟩,
(10.1)
for all u ∈E and all v∗∈F ∗.
It is easy to verify that the following properties hold:
(f + g)⊤= f ⊤+ g⊤
(g ◦f)⊤= f ⊤◦g⊤
id⊤
E = idE∗.

Note the reversal of composition on the right-hand side of (g ◦
f)⊤= f ⊤◦g⊤.

390
The Dual Space and Duality
The equation (g◦f)⊤= f ⊤◦g⊤implies the following useful proposition.
Proposition 10.7. If f : E →F is any linear map, then the following
properties hold:
(1) If f is injective, then f ⊤is surjective.
(2) If f is surjective, then f ⊤is injective.
Proof. If f : E →F is injective, then it has a retraction r: F →E such
that r ◦f = idE, and if f : E →F is surjective, then it has a section
s: F →E such that f ◦s = idF . Now if f : E →F is injective, then we
have
(r ◦f)⊤= f ⊤◦r⊤= idE∗,
which implies that f ⊤is surjective, and if f is surjective, then we have
(f ◦s)⊤= s⊤◦f ⊤= idF ∗,
which implies that f ⊤is injective.
The following proposition shows the relationship between orthogonality
and transposition.
Proposition 10.8. Given a linear map f : E →F, for any subspace V of
E, we have
f(V )0 = (f ⊤)−1(V 0) = {w∗∈F ∗| f ⊤(w∗) ∈V 0}.
As a consequence,
Ker f ⊤= (Im f)0.
We also have
Ker f = (Im f ⊤)0.
Proof. We have
⟨w∗, f(v)⟩= ⟨f ⊤(w∗), v⟩,
for all v ∈E and all w∗∈F ∗, and thus, we have ⟨w∗, f(v)⟩= 0 for every
v ∈V , i.e. w∗∈f(V )0 iﬀ⟨f ⊤(w∗), v⟩= 0 for every v ∈V iﬀf ⊤(w∗) ∈V 0,
i.e. w∗∈(f ⊤)−1(V 0), proving that
f(V )0 = (f ⊤)−1(V 0).
Since we already observed that E0 = (0), letting V = E in the above
identity we obtain that
Ker f ⊤= (Im f)0.

10.6. Transpose of a Linear Map and of a Matrix
391
From the equation
⟨w∗, f(v)⟩= ⟨f ⊤(w∗), v⟩,
we deduce that v ∈(Im f ⊤)0 iﬀ⟨f ⊤(w∗), v⟩= 0 for all w∗∈F ∗iﬀ
⟨w∗, f(v)⟩= 0 for all w∗∈F ∗. Assume that v ∈(Im f ⊤)0. If we pick
a basis (wi)i∈I of F, then we have the linear forms w∗
i : F →K such that
w∗
i (wj) = δij, and since we must have ⟨w∗
i , f(v)⟩= 0 for all i ∈I and
(wi)i∈I is a basis of F, we conclude that f(v) = 0, and thus v ∈Ker f (this
is because ⟨w∗
i , f(v)⟩is the coeﬃcient of f(v) associated with the basis vec-
tor wi). Conversely, if v ∈Ker f, then ⟨w∗, f(v)⟩= 0 for all w∗∈F ∗, so
we conclude that v ∈(Im f ⊤)0. Therefore, v ∈(Im f ⊤)0 iﬀv ∈Ker f; that
is,
Ker f = (Im f ⊤)0,
as claimed.
The following theorem shows the relationship between the rank of f and
the rank of f ⊤.
Theorem 10.2. Given a linear map f : E →F, the following properties
hold.
(a) The dual (Im f)∗of Im f is isomorphic to Im f ⊤= f ⊤(F ∗); that is,
(Im f)∗∼= Im f ⊤.
(b) If F is ﬁnite dimensional, then rk(f) = rk(f ⊤).
Proof. (a) Consider the linear maps
E
p
−→Im f
j
−→F,
where E
p
−→Im f is the surjective map induced by E
f
−→F, and
Im f
j
−→F is the injective inclusion map of Im f into F. By deﬁnition,
f = j ◦p. To simplify the notation, let I = Im f. By Proposition 10.7,
since E
p
−→I is surjective, I∗
p⊤
−→E∗is injective, and since Im f
j
−→F
is injective, F ∗
j⊤
−→I∗is surjective. Since f = j ◦p, we also have
f ⊤= (j ◦p)⊤= p⊤◦j⊤,
and since F ∗
j⊤
−→I∗is surjective, and I∗
p⊤
−→E∗is injective, we have an
isomorphism between (Im f)∗and f ⊤(F ∗).

392
The Dual Space and Duality
(b) We already noted that Part (a) of Theorem 10.1 shows that
dim(F) = dim(F ∗), for every vector space F of ﬁnite dimension.
Con-
sequently, dim(Im f) = dim((Im f)∗), and thus, by Part (a) we have
rk(f) = rk(f ⊤).
Remark: When both E and F are ﬁnite-dimensional, there is also a simple
proof of (b) that doesn't use the result of Part (a). By Theorem 10.1(c)
dim(Im f) + dim((Im f)0) = dim(F),
and by Theorem 5.1
dim(Ker f ⊤) + dim(Im f ⊤) = dim(F ∗).
Furthermore, by Proposition 10.8, we have
Ker f ⊤= (Im f)0,
and since F is ﬁnite-dimensional dim(F) = dim(F ∗), so we deduce
dim(Im f) + dim((Im f)0) = dim((Im f)0) + dim(Im f ⊤),
which yields dim(Im f) = dim(Im f ⊤); that is, rk(f) = rk(f ⊤).
The following proposition can be shown, but it requires a generalization
of the duality theorem, so its proof is omitted.
Proposition 10.9. If f : E →F is any linear map, then the following
identities hold:
Im f ⊤= (Ker (f))0
Ker (f ⊤) = (Im f)0
Im f = (Ker (f ⊤)0
Ker (f) = (Im f ⊤)0.
Observe that the second and the fourth equation have already be
proven in Proposition 10.8. Since for any subspace V ⊆E, even inﬁnite-
dimensional, we have V 00 = V , the third equation follows from the second
equation by taking orthogonals. Actually, the fourth equation follows from
the ﬁrst also by taking orthogonals. Thus the only equation to be proven
is the ﬁrst equation.
We will give a proof later in the case where E is
ﬁnite-dimensional (see Proposition 10.16).

10.6. Transpose of a Linear Map and of a Matrix
393
The following proposition shows the relationship between the matrix
representing a linear map f : E →F and the matrix representing its trans-
pose f ⊤: F ∗→E∗.
Proposition 10.10. Let E and F be two vector spaces, and let (u1, . . . , un)
be a basis for E and (v1, . . . , vm) be a basis for F. Given any linear map
f : E →F, if M(f) is the m × n-matrix representing f w.r.t. the bases
(u1, . . . , un) and (v1, . . . , vm), then the n × m-matrix M(f ⊤) representing
f ⊤: F ∗→E∗w.r.t. the dual bases (v∗
1, . . . , v∗
m) and (u∗
1, . . . , u∗
n) is the
transpose M(f)⊤of M(f).
Proof. Recall that the entry ai j in row i and column j of M(f) is the i-th
coordinate of f(uj) over the basis (v1, . . . , vm). By deﬁnition of v∗
i , we have
⟨v∗
i , f(uj)⟩= ai j. The entry a⊤
j i in row j and column i of M(f ⊤) is the
j-th coordinate of
f ⊤(v∗
i ) = a⊤
1 iu∗
1 + · · · + a⊤
j iu∗
j + · · · + a⊤
n iu∗
n
over the basis (u∗
1, . . . , u∗
n), which is just a⊤
j i = f ⊤(v∗
i )(uj) = ⟨f ⊤(v∗
i ), uj⟩.
Since
⟨v∗
i , f(uj)⟩= ⟨f ⊤(v∗
i ), uj⟩,
we have ai j = a⊤
j i, proving that M(f ⊤) = M(f)⊤.
We now can give a very short proof of the fact that the rank of a matrix
is equal to the rank of its transpose.
Proposition 10.11. Given an m × n matrix A over a ﬁeld K, we have
rk(A) = rk(A⊤).
Proof. The matrix A corresponds to a linear map f : Kn →Km, and
by Theorem 10.2, rk(f) = rk(f ⊤). By Proposition 10.10, the linear map
f ⊤corresponds to A⊤. Since rk(A) = rk(f), and rk(A⊤) = rk(f ⊤), we
conclude that rk(A) = rk(A⊤).
Thus, given an m × n-matrix A, the maximum number of linearly inde-
pendent columns is equal to the maximum number of linearly independent
rows. There are other ways of proving this fact that do not involve the dual
space, but instead some elementary transformations on rows and columns.
Proposition 10.11 immediately yields the following criterion for deter-
mining the rank of a matrix:
Proposition 10.12. Given any m × n matrix A over a ﬁeld K (typically
K = R or K = C), the rank of A is the maximum natural number r such

394
The Dual Space and Duality
that there is an invertible r × r submatrix of A obtained by selecting r rows
and r columns of A.
For example, the 3 × 2 matrix
A =


a11 a12
a21 a22
a31 a32


has rank 2 iﬀone of the three 2 × 2 matrices
a11 a12
a21 a22

a11 a12
a31 a32

a21 a22
a31 a32

is invertible.
If we combine Proposition 6.8 with Proposition 10.12, we obtain the
following criterion for ﬁnding the rank of a matrix.
Proposition 10.13. Given any m × n matrix A over a ﬁeld K (typically
K = R or K = C), the rank of A is the maximum natural number r such
that there is an r × r submatrix B of A obtained by selecting r rows and r
columns of A, such that det(B) ̸= 0.
This is not a very eﬃcient way of ﬁnding the rank of a matrix. We will
see that there are better ways using various decompositions such as LU,
QR, or SVD.
10.7
Properties of the Double Transpose
First we have the following property showing the naturality of the eval map.
Proposition 10.14. For any linear map f : E →F, we have
f ⊤⊤◦evalE = evalF ◦f,
or equivalently the following diagram commutes:
E∗∗
f ⊤⊤/ F ∗∗
E
evalE
O
f
/ F.
evalF
O

10.7. Properties of the Double Transpose
395
Proof. For every u ∈E and every ϕ ∈F ∗, we have
(f ⊤⊤◦evalE)(u)(ϕ) = ⟨f ⊤⊤(evalE(u)), ϕ⟩
= ⟨evalE(u), f ⊤(ϕ)⟩
= ⟨f ⊤(ϕ), u⟩
= ⟨ϕ, f(u)⟩
= ⟨evalF (f(u)), ϕ⟩
= ⟨(evalF ◦f)(u), ϕ⟩
= (evalF ◦f)(u)(ϕ),
which proves that f ⊤⊤◦evalE = evalF ◦f, as claimed.
If E and F are ﬁnite-dimensional, then evalE and evalF are isomor-
phisms, so Proposition 10.14 shows that
f ⊤⊤= evalF ◦f ◦eval−1
E .
(10.2)
The above equation is often interpreted as follows: if we identify E with its
bidual E∗∗and F with its bidual F ∗∗, then f ⊤⊤= f. This is an abuse of
notation; the rigorous statement is (10.2).
As a corollary of Proposition 10.14, we obtain the following result.
Proposition 10.15. If dim(E) is ﬁnite, then we have
Ker (f ⊤⊤) = evalE(Ker (f)).
Proof. Indeed, if E is ﬁnite-dimensional, the map evalE : E →E∗∗is an
isomorphism, so every ϕ ∈E∗∗is of the form ϕ = evalE(u) for some u ∈E,
the map evalF : F →F ∗∗is injective, and we have
f ⊤⊤(ϕ) = 0
iﬀ
f ⊤⊤(evalE(u)) = 0
iﬀ
evalF (f(u)) = 0
iﬀ
f(u) = 0
iﬀ
u ∈Ker (f)
iﬀ
ϕ ∈evalE(Ker (f)),
which proves that Ker (f ⊤⊤) = evalE(Ker (f)).
Remarks: If dim(E) is ﬁnite, following an argument of Dan Guralnik, the
fact that rk(f) = rk(f ⊤) can be proven using Proposition 10.15.

396
The Dual Space and Duality
Proof. We know from Proposition 10.8 applied to f ⊤: F ∗→E∗that
Ker (f ⊤⊤) = (Im f ⊤)0,
and we showed in Proposition 10.15 that
Ker (f ⊤⊤) = evalE(Ker (f)).
It follows (since evalE is an isomorphism) that
dim((Im f ⊤)0) = dim(Ker (f ⊤⊤)) = dim(Ker (f)) = dim(E) −dim(Im f),
and since
dim(Im f ⊤) + dim((Im f ⊤)0) = dim(E),
we get
dim(Im f ⊤) = dim(Im f).
As indicated by Dan Guralnik, if dim(E) is ﬁnite, the above result can
be used to prove the following result.
Proposition 10.16. If dim(E) is ﬁnite, then for any linear map f : E →
F, we have
Im f ⊤= (Ker (f))0.
Proof. From
⟨f ⊤(ϕ), u⟩= ⟨ϕ, f(u)⟩
for all ϕ ∈F ∗and all u ∈E, we see that if u ∈Ker (f), then ⟨f ⊤(ϕ), u⟩=
⟨ϕ, 0⟩= 0, which means that f ⊤(ϕ) ∈(Ker (f))0, and thus, Im f ⊤⊆
(Ker (f))0. For the converse, since dim(E) is ﬁnite, we have
dim((Ker (f))0) = dim(E) −dim(Ker (f)) = dim(Im f),
but we just proved that dim(Im f ⊤) = dim(Im f), so we get
dim((Ker (f))0) = dim(Im f ⊤),
and since Im f ⊤⊆(Ker (f))0, we obtain
Im f ⊤= (Ker (f))0,
as claimed.
Remarks:
(1) By the duality theorem, since (Ker (f))00 = Ker (f), the above equation
yields another proof of the fact that
Ker (f) = (Im f ⊤)0,
when E is ﬁnite-dimensional.
(2) The equation
Im f ⊤= (Ker (f))0
is actually valid even if when E if inﬁnite-dimensional, but we will not
prove this here.

10.8. The Four Fundamental Subspaces
397
10.8
The Four Fundamental Subspaces
Given a linear map f : E →F (where E and F are ﬁnite-dimensional),
Proposition 10.8 revealed that the four spaces
Im f, Im f ⊤, Ker f, Ker f ⊤
play a special role. They are often called the fundamental subspaces as-
sociated with f. These spaces are related in an intimate manner, since
Proposition 10.8 shows that
Ker f = (Im f ⊤)0
Ker f ⊤= (Im f)0,
and Theorem 10.2 shows that
rk(f) = rk(f ⊤).
It is instructive to translate these relations in terms of matrices (actually,
certain linear algebra books make a big deal about this!). If dim(E) = n
and dim(F) = m, given any basis (u1, . . . , un) of E and a basis (v1, . . . , vm)
of F, we know that f is represented by an m×n matrix A = (ai j), where the
jth column of A is equal to f(uj) over the basis (v1, . . . , vm). Furthermore,
the transpose map f ⊤is represented by the n×m matrix A⊤(with respect
to the dual bases). Consequently, the four fundamental spaces
Im f, Im f ⊤, Ker f, Ker f ⊤
correspond to
(1) The column space of A, denoted by Im A or R(A); this is the subspace
of Rm spanned by the columns of A, which corresponds to the image
Im f of f.
(2) The kernel or nullspace of A, denoted by Ker A or N(A); this is the
subspace of Rn consisting of all vectors x ∈Rn such that Ax = 0.
(3) The row space of A, denoted by Im A⊤or R(A⊤); this is the subspace of
Rn spanned by the rows of A, or equivalently, spanned by the columns
of A⊤, which corresponds to the image Im f ⊤of f ⊤.
(4) The left kernel or left nullspace of A denoted by Ker A⊤or N(A⊤);
this is the kernel (nullspace) of A⊤, the subspace of Rm consisting of
all vectors y ∈Rm such that A⊤y = 0, or equivalently, y⊤A = 0.
Recall that the dimension r of Im f, which is also equal to the dimension
of the column space Im A = R(A), is the rank of A (and f). Then, some
our previous results can be reformulated as follows:

398
The Dual Space and Duality
(1) The column space R(A) of A has dimension r.
(2) The nullspace N(A) of A has dimension n −r.
(3) The row space R(A⊤) has dimension r.
(4) The left nullspace N(A⊤) of A has dimension m −r.
The above statements constitute what Strang calls the Fundamental
Theorem of Linear Algebra, Part I (see Strang [Strang (1988)]).
The two statements
Ker f = (Im f ⊤)0
Ker f ⊤= (Im f)0
translate to
(1) The nullspace of A is the orthogonal of the row space of A.
(2) The left nullspace of A is the orthogonal of the column space of A.
The above statements constitute what Strang calls the Fundamental The-
orem of Linear Algebra, Part II (see Strang [Strang (1988)]).
Since vectors are represented by column vectors and linear forms by row
vectors (over a basis in E or F), a vector x ∈Rn is orthogonal to a linear
form y iﬀ
yx = 0.
Then, a vector x ∈Rn is orthogonal to the row space of A iﬀx is orthogonal
to every row of A, namely Ax = 0, which is equivalent to the fact that x
belong to the nullspace of A. Similarly, the column vector y ∈Rm (repre-
senting a linear form over the dual basis of F ∗) belongs to the nullspace of
A⊤iﬀA⊤y = 0, iﬀy⊤A = 0, which means that the linear form given by
y⊤(over the basis in F) is orthogonal to the column space of A.
Since (2) is equivalent to the fact that the column space of A is equal
to the orthogonal of the left nullspace of A, we get the following criterion
for the solvability of an equation of the form Ax = b:
The equation Ax = b has a solution iﬀfor all y ∈Rm, if A⊤y = 0, then
y⊤b = 0.
Indeed, the condition on the right-hand side says that b is orthogonal
to the left nullspace of A; that is, b belongs to the column space of A.
This criterion can be cheaper to check that checking directly that b is
spanned by the columns of A. For example, if we consider the system
x1 −x2 = b1
x2 −x3 = b2
x3 −x1 = b3

10.9. Summary
399
which, in matrix form, is written Ax = b as below:


1 −1 0
0
1 −1
−1 0
1




x1
x2
x3

=


b1
b2
b3

,
we see that the rows of the matrix A add up to 0. In fact, it is easy to
convince ourselves that the left nullspace of A is spanned by y = (1, 1, 1),
and so the system is solvable iﬀy⊤b = 0, namely
b1 + b2 + b3 = 0.
Note that the above criterion can also be stated negatively as follows:
The equation Ax = b has no solution iﬀthere is some y ∈Rm such that
A⊤y = 0 and y⊤b ̸= 0.
Since A⊤y = 0 iﬀy⊤A = 0, we can view y⊤as a row vector representing
a linear form, and y⊤A = 0 asserts that the linear form y⊤vanishes on the
columns A1, . . . , An of A but does not vanish on b. Since the linear form y⊤
deﬁnes the hyperplane H of equation y⊤z = 0 (with z ∈Rm), geometrically
the equation Ax = b has no solution iﬀthere is a hyperplane H containing
A1, . . . , An and not containing b.
10.9
Summary
The main concepts and results of this chapter are listed below:
• The dual space E∗and linear forms (covector). The bidual E∗∗.
• The bilinear pairing ⟨−, −⟩: E∗× E →K (the canonical pairing).
• Evaluation at v: evalv : E∗→K.
• The map evalE : E →E∗∗.
• Othogonality between a subspace V of E and a subspace U of E∗; the
orthogonal V 0 and the orthogonal U 0.
• Coordinate forms.
• The Duality theorem (Theorem 10.1).
• The dual basis of a basis.
• The isomorphism evalE : E →E∗∗when dim(E) is ﬁnite.
• Pairing between two vector spaces; nondegenerate pairing; Proposi-
tion 10.5.
• Hyperplanes and linear forms.
• The transpose f ⊤: F ∗→E∗of a linear map f : E →F.
• The fundamental identities:
Ker f ⊤= (Im f)0
and
Ker f = (Im f ⊤)0
(Proposition 10.8).

400
The Dual Space and Duality
• If F is ﬁnite-dimensional, then
rk(f) = rk(f ⊤).
(Theorem 10.2).
• The matrix of the transpose map f ⊤is equal to the transpose of the
matrix of the map f (Proposition 10.10).
• For any m × n matrix A,
rk(A) = rk(A⊤).
• Characterization of the rank of a matrix in terms of a maximal invert-
ible submatrix (Proposition 10.12).
• The four fundamental subspaces:
Im f, Im f ⊤, Ker f, Ker f ⊤.
• The column space, the nullspace, the row space, and the left nullspace
(of a matrix).
• Criterion for the solvability of an equation of the form Ax = b in terms
of the left nullspace.
10.10
Problems
Problem 10.1. Prove the following properties of transposition:
(f + g)⊤= f ⊤+ g⊤
(g ◦f)⊤= f ⊤◦g⊤
id⊤
E = idE∗.
Problem 10.2. Let (u1, . . . , un−1) be n −1 linearly independent vectors
ui ∈Cn. Prove that the hyperlane H spanned by (u1, . . . , un−1) is the
nullspace of the linear form
x 7→det(u1, . . . , un−1, x),
x ∈Cn.
Prove that if A is the n × n matrix whose columns are (u1, . . . , un−1, x),
and if ci = (−1)i+n det(Ain) is the cofactor of ain = xi for i = 1, . . . , n,
then H is deﬁned by the equation
c1x1 + · · · + cnxn = 0.

10.10. Problems
401
Problem 10.3. (1) Let ϕ: Rn × Rn →R be the map deﬁned by
ϕ((x1, . . . , xn), (y1, . . . , yn)) = x1y1 + · · · + xnyn.
Prove that ϕ is a bilinear nondegenerate pairing. Deduce that (Rn)∗is
isomorphic to Rn.
Prove that ϕ(x, x) = 0 iﬀx = 0.
(2) Let ϕL : R4 × R4 →R be the map deﬁned by
ϕL((x1, x2, x3, x4), (y1, y2, y3, , y4)) = x1y1 −x2y2 −x3y3 −x4y4.
Prove that ϕ is a bilinear nondegenerate pairing.
Show that there exist nonzero vectors x ∈R4 such that ϕL(x, x) = 0.
Remark: The vector space R4 equipped with the above bilinear form called
the Lorentz form is called Minkowski space.
Problem 10.4. Given any two subspaces V1, V2 of a ﬁnite-dimensional
vector space E, prove that
(V1 + V2)0 = V 0
1 ∩V 0
2
(V1 ∩V2)0 = V 0
1 + V 0
2 .
Beware that in the second equation, V1 and V2 are subspaces of E, not
E∗.
Hint. To prove the second equation, prove the inclusions V 0
1 + V 0
2 ⊆
(V1 ∩V2)0 and (V1 ∩V2)0 ⊆V 0
1 +V 0
2 . Proving the second inclusion is a little
tricky. First, prove that we can pick a subspace W1 of V1 and a subspace
W2 of V2 such that
(1) V1 is the direct sum V1 = (V1 ∩V2) ⊕W1.
(2) V2 is the direct sum V2 = (V1 ∩V2) ⊕W2.
(3) V1 + V2 is the direct sum V1 + V2 = (V1 ∩V2) ⊕W1 ⊕W2.
Problem 10.5. (1) Let A be any n × n matrix such that the sum of the
entries of every row of A is the same (say c1), and the sum of entries of
every column of A is the same (say c2). Prove that c1 = c2.
(2) Prove that for any n ≥2, the 2n −2 equations asserting that the
sum of the entries of every row of A is the same, and the sum of entries
of every column of A is the same are linearly independent. For example,

402
The Dual Space and Duality
when n = 4, we have the following 6 equations
a11 + a12 + a13 + a14 −a21 −a22 −a23 −a24 = 0
a21 + a22 + a23 + a24 −a31 −a32 −a33 −a34 = 0
a31 + a32 + a33 + a34 −a41 −a42 −a43 −a44 = 0
a11 + a21 + a31 + a41 −a12 −a22 −a32 −a42 = 0
a12 + a22 + a32 + a42 −a13 −a23 −a33 −a43 = 0
a13 + a23 + a33 + a43 −a14 −a24 −a34 −a44 = 0.
Hint. Group the equations as above; that is, ﬁrst list the n −1 equations
relating the rows, and then list the n −1 equations relating the columns.
Prove that the ﬁrst n −1 equations are linearly independent, and that the
last n−1 equations are also linearly independent. Then, ﬁnd a relationship
between the two groups of equations that will allow you to prove that they
span subspace V r and V c such that V r ∩V c = (0).
(3) Now consider magic squares. Such matrices satisfy the two condi-
tions about the sum of the entries in each row and in each column to be
the same number, and also the additional two constraints that the main
descending and the main ascending diagonals add up to this common num-
ber. Traditionally, it is also required that the entries in a magic square are
positive integers, but we will consider generalized magic square with arbi-
trary real entries. For example, in the case n = 4, we have the following
system of 8 equations:
a11 + a12 + a13 + a14 −a21 −a22 −a23 −a24 = 0
a21 + a22 + a23 + a24 −a31 −a32 −a33 −a34 = 0
a31 + a32 + a33 + a34 −a41 −a42 −a43 −a44 = 0
a11 + a21 + a31 + a41 −a12 −a22 −a32 −a42 = 0
a12 + a22 + a32 + a42 −a13 −a23 −a33 −a43 = 0
a13 + a23 + a33 + a43 −a14 −a24 −a34 −a44 = 0
a22 + a33 + a44 −a12 −a13 −a14 = 0
a41 + a32 + a23 −a11 −a12 −a13 = 0.
In general, the equation involving the descending diagonal is
a22 + a33 + · · · + ann −a12 −a13 −· · · −a1n = 0
(10.3)
and the equation involving the ascending diagonal is
an1 + an−12 + · · · + a2n−1 −a11 −a12 −· · · −a1n−1 = 0.
(10.4)

10.10. Problems
403
Prove that if n ≥3, then the 2n equations asserting that a matrix is a
generalized magic square are linearly independent.
Hint. Equations are really linear forms, so ﬁnd some matrix annihilated by
all equations except equation (10.3), and some matrix annihilated by all
equations except equation (10.4).
Problem 10.6. Let U1, . . . , Up be some subspaces of a vector space E,
and assume that they form a direct sum U = U1 ⊕· · · ⊕Up. Let ji : Ui →
U1 ⊕· · · ⊕Up be the canonical injections, and let πi : U ∗
1 × · · · × U ∗
p →U ∗
i
be the canonical projections. Prove that there is an isomorphism f from
(U1 ⊕· · · ⊕Up)∗to U ∗
1 × · · · × U ∗
p such that
πi ◦f = j⊤
i ,
1 ≤i ≤p.
Problem 10.7. Let U and V be two subspaces of a vector space E such
that E = U ⊕V . Prove that
E∗= U 0 ⊕V 0.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 11
Euclidean Spaces
Rien n'est beau que le vrai.
—Hermann Minkowski
11.1
Inner Products, Euclidean Spaces
So far the framework of vector spaces allows us to deal with ratios of vectors
and linear combinations, but there is no way to express the notion of angle
or to talk about orthogonality of vectors. A Euclidean structure allows us
to deal with metric notions such as angles, orthogonality, and length (or
distance).
This chapter covers the bare bones of Euclidean geometry. Deeper as-
pects of Euclidean geometry are investigated in Chapter 12. One of our
main goals is to give the basic properties of the transformations that pre-
serve the Euclidean structure, rotations and reﬂections, since they play an
important role in practice. Euclidean geometry is the study of properties
invariant under certain aﬃne maps called rigid motions. Rigid motions are
the maps that preserve the distance between points.
We begin by deﬁning inner products and Euclidean spaces.
The
Cauchy-Schwarz inequality and the Minkowski inequality are shown. We
deﬁne orthogonality of vectors and of subspaces, orthogonal bases, and or-
thonormal bases. We prove that every ﬁnite-dimensional Euclidean space
has orthonormal bases. The ﬁrst proof uses duality and the second one the
Gram-Schmidt orthogonalization procedure.
The QR-decomposition for
invertible matrices is shown as an application of the Gram-Schmidt proce-
dure. Linear isometries (also called orthogonal transformations) are deﬁned
and studied brieﬂy. We conclude with a short section in which some ap-
plications of Euclidean geometry are sketched. One of the most important
405

406
Euclidean Spaces
applications, the method of least squares, is discussed in Chapter 21.
For a more detailed treatment of Euclidean geometry see Berger [Berger
(1990a,b)], Snapper and Troyer [Snapper and Troyer (1989)], or any other
book on geometry, such as Pedoe [Pedoe (1988)], Coxeter [Coxeter (1989)],
Fresnel [Fresnel (1998)], Tisseron [Tisseron (1994)], or Cagnac, Ramis, and
Commeau [Cagnac et al. (1965)].
Serious readers should consult Emil
Artin's famous book [Artin (1957)], which contains an in-depth study of
the orthogonal group, as well as other groups arising in geometry. It is still
worth consulting some of the older classics, such as Hadamard [Hadamard
(1947, 1949)] and Rouch´e and de Comberousse [Rouch´e and de Comber-
ousse (1900)]. The ﬁrst edition of [Hadamard (1947)] was published in 1898
and ﬁnally reached its thirteenth edition in 1947! In this chapter it is as-
sumed that all vector spaces are deﬁned over the ﬁeld R of real numbers
unless speciﬁed otherwise (in a few cases, over the complex numbers C).
First we deﬁne a Euclidean structure on a vector space. Technically,
a Euclidean structure over a vector space E is provided by a symmetric
bilinear form on the vector space satisfying some extra properties. Recall
that a bilinear form ϕ: E × E →R is deﬁnite if for every u ∈E, u ̸= 0
implies that ϕ(u, u) ̸= 0, and positive if for every u ∈E, ϕ(u, u) ≥0.
Deﬁnition 11.1. A Euclidean space is a real vector space E equipped with
a symmetric bilinear form ϕ: E × E →R that is positive deﬁnite. More
explicitly, ϕ: E × E →R satisﬁes the following axioms:
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v),
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2),
ϕ(λu, v) = λϕ(u, v),
ϕ(u, λv) = λϕ(u, v),
ϕ(u, v) = ϕ(v, u),
u ̸= 0 implies that ϕ(u, u) > 0.
The real number ϕ(u, v) is also called the inner product (or scalar product)
of u and v.
We also deﬁne the quadratic form associated with ϕ as the
function Φ: E →R+ such that
Φ(u) = ϕ(u, u),
for all u ∈E.
Since ϕ is bilinear, we have ϕ(0, 0) = 0, and since it is positive deﬁnite,
we have the stronger fact that
ϕ(u, u) = 0
iﬀ
u = 0,

11.1. Inner Products, Euclidean Spaces
407
that is, Φ(u) = 0 iﬀu = 0.
Given an inner product ϕ: E × E →R on a vector space E, we also
denote ϕ(u, v) by
u · v
or
⟨u, v⟩
or
(u|v),
and
p
Φ(u) by ∥u∥.
Example 11.1. The standard example of a Euclidean space is Rn, under
the inner product · deﬁned such that
(x1, . . . , xn) · (y1, . . . , yn) = x1y1 + x2y2 + · · · + xnyn.
This Euclidean space is denoted by En.
There are other examples.
Example 11.2. For instance, let E be a vector space of dimension 2, and
let (e1, e2) be a basis of E. If a > 0 and b2 −ac < 0, the bilinear form
deﬁned such that
ϕ(x1e1 + y1e2, x2e1 + y2e2) = ax1x2 + b(x1y2 + x2y1) + cy1y2
yields a Euclidean structure on E. In this case,
Φ(xe1 + ye2) = ax2 + 2bxy + cy2.
Example
11.3. Let C[a, b] denote the set of continuous functions
f : [a, b] →R. It is easily checked that C[a, b] is a vector space of inﬁnite
dimension. Given any two functions f, g ∈C[a, b], let
⟨f, g⟩=
Z b
a
f(t)g(t)dt.
We leave it as an easy exercise that ⟨−, −⟩is indeed an inner product on
C[a, b]. In the case where a = −π and b = π (or a = 0 and b = 2π, this
makes basically no diﬀerence), one should compute
⟨sin px, sin qx⟩,
⟨sin px, cos qx⟩,
and
⟨cos px, cos qx⟩,
for all natural numbers p, q ≥1. The outcome of these calculations is what
makes Fourier analysis possible!
Example 11.4. Let E = Mn(R) be the vector space of real n×n matrices.
If we view a matrix A ∈Mn(R) as a "long" column vector obtained by
concatenating together its columns, we can deﬁne the inner product of two
matrices A, B ∈Mn(R) as
⟨A, B⟩=
n
X
i,j=1
aijbij,

408
Euclidean Spaces
which can be conveniently written as
⟨A, B⟩= tr(A⊤B) = tr(B⊤A).
Since this can be viewed as the Euclidean product on Rn2, it is an inner
product on Mn(R). The corresponding norm
∥A∥F =
q
tr(A⊤A)
is the Frobenius norm (see Section 8.2).
Let us observe that ϕ can be recovered from Φ.
Proposition 11.1. We have
ϕ(u, v) = 1
2[Φ(u + v) −Φ(u) −Φ(v)]
for all u, v ∈E. We say that ϕ is the polar form of Φ.
Proof. By bilinearity and symmetry, we have
Φ(u + v) = ϕ(u + v, u + v)
= ϕ(u, u + v) + ϕ(v, u + v)
= ϕ(u, u) + 2ϕ(u, v) + ϕ(v, v)
= Φ(u) + 2ϕ(u, v) + Φ(v).
If E is ﬁnite-dimensional and if ϕ: E × E →R is a bilinear form on
E, given any basis (e1, . . . , en) of E, we can write x = Pn
i=1 xiei and
y = Pn
j=1 yjej, and we have
ϕ(x, y) = ϕ
 n
X
i=1
xiei,
n
X
j=1
yjej

=
n
X
i,j=1
xiyjϕ(ei, ej).
If we let G be the matrix G = (ϕ(ei, ej)), and if x and y are the column
vectors associated with (x1, . . . , xn) and (y1, . . . , yn), then we can write
ϕ(x, y) = x⊤Gy = y⊤G⊤x.
Note that we are committing an abuse of notation since x = Pn
i=1 xiei is
a vector in E, but the column vector associated with (x1, . . . , xn) belongs
to Rn.
To avoid this minor abuse, we could denote the column vector
associated with (x1, . . . , xn) by x (and similarly y for the column vector
associated with (y1, . . . , yn)), in which case the "correct" expression for
ϕ(x, y) is
ϕ(x, y) = x⊤Gy.

11.1. Inner Products, Euclidean Spaces
409
However, in view of the isomorphism between E and Rn, to keep notation
as simple as possible, we will use x and y instead of x and y.
Also observe that ϕ is symmetric iﬀG = G⊤, and ϕ is positive deﬁnite
iﬀthe matrix G is positive deﬁnite, that is,
x⊤Gx > 0
for all x ∈Rn, x ̸= 0.
The matrix G associated with an inner product is called the Gram matrix
of the inner product with respect to the basis (e1, . . . , en).
Conversely, if A is a symmetric positive deﬁnite n × n matrix, it is easy
to check that the bilinear form
⟨x, y⟩= x⊤Ay
is an inner product. If we make a change of basis from the basis (e1, . . . , en)
to the basis (f1, . . . , fn), and if the change of basis matrix is P (where the
jth column of P consists of the coordinates of fj over the basis (e1, . . . , en)),
then with respect to coordinates x′ and y′ over the basis (f1, . . . , fn), we
have
x⊤Gy = x′⊤P ⊤GPy′,
so the matrix of our inner product over the basis (f1, . . . , fn) is P ⊤GP. We
summarize these facts in the following proposition.
Proposition 11.2. Let E be a ﬁnite-dimensional vector space, and let
(e1, . . . , en) be a basis of E.
(1) For any inner product ⟨−, −⟩on E, if G = (⟨ei, ej⟩) is the Gram ma-
trix of the inner product ⟨−, −⟩w.r.t. the basis (e1, . . . , en), then G is
symmetric positive deﬁnite.
(2) For any change of basis matrix P, the Gram matrix of ⟨−, −⟩with
respect to the new basis is P ⊤GP.
(3) If A is any n × n symmetric positive deﬁnite matrix, then
⟨x, y⟩= x⊤Ay
is an inner product on E.
We will see later that a symmetric matrix is positive deﬁnite iﬀits
eigenvalues are all positive.
One of the very important properties of an inner product ϕ is that the
map u 7→
p
Φ(u) is a norm.
Proposition 11.3. Let E be a Euclidean space with inner product ϕ, and
let Φ be the corresponding quadratic form. For all u, v ∈E, we have the
Cauchy-Schwarz inequality
ϕ(u, v)2 ≤Φ(u)Φ(v),

410
Euclidean Spaces
the equality holding iﬀu and v are linearly dependent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p
Φ(u) +
p
Φ(v),
the equality holding iﬀu and v are linearly dependent, where in addition if
u ̸= 0 and v ̸= 0, then u = λv for some λ > 0.
Proof. For any vectors u, v ∈E, we deﬁne the function T : R →R such
that
T(λ) = Φ(u + λv),
for all λ ∈R. Using bilinearity and symmetry, we have
Φ(u + λv) = ϕ(u + λv, u + λv)
= ϕ(u, u + λv) + λϕ(v, u + λv)
= ϕ(u, u) + 2λϕ(u, v) + λ2ϕ(v, v)
= Φ(u) + 2λϕ(u, v) + λ2Φ(v).
Since ϕ is positive deﬁnite, Φ is nonnegative, and thus T(λ) ≥0 for all
λ ∈R. If Φ(v) = 0, then v = 0, and we also have ϕ(u, v) = 0. In this
case, the Cauchy-Schwarz inequality is trivial, and v = 0 and u are linearly
dependent.
Now assume Φ(v) > 0. Since T(λ) ≥0, the quadratic equation
λ2Φ(v) + 2λϕ(u, v) + Φ(u) = 0
cannot have distinct real roots, which means that its discriminant
∆= 4(ϕ(u, v)2 −Φ(u)Φ(v))
is null or negative, which is precisely the Cauchy-Schwarz inequality
ϕ(u, v)2 ≤Φ(u)Φ(v).
Let us now consider the case where we have the equality
ϕ(u, v)2 = Φ(u)Φ(v).
There are two cases. If Φ(v) = 0, then v = 0 and u and v are linearly
dependent. If Φ(v) ̸= 0, then the above quadratic equation has a double
root λ0, and we have Φ(u + λ0v) = 0. Since ϕ is positive deﬁnite, Φ(u +
λ0v) = 0 implies that u + λ0v = 0, which shows that u and v are linearly
dependent. Conversely, it is easy to check that we have equality when u
and v are linearly dependent.

11.1. Inner Products, Euclidean Spaces
411
The Minkowski inequality
p
Φ(u + v) ≤
p
Φ(u) +
p
Φ(v)
is equivalent to
Φ(u + v) ≤Φ(u) + Φ(v) + 2
p
Φ(u)Φ(v).
However, we have shown that
2ϕ(u, v) = Φ(u + v) −Φ(u) −Φ(v),
and so the above inequality is equivalent to
ϕ(u, v) ≤
p
Φ(u)Φ(v),
which is trivial when ϕ(u, v) ≤0, and follows from the Cauchy-Schwarz
inequality when ϕ(u, v) ≥0. Thus, the Minkowski inequality holds. Finally
assume that u ̸= 0 and v ̸= 0, and that
p
Φ(u + v) =
p
Φ(u) +
p
Φ(v).
When this is the case, we have
ϕ(u, v) =
p
Φ(u)Φ(v),
and we know from the discussion of the Cauchy-Schwarz inequality that the
equality holds iﬀu and v are linearly dependent. The Minkowski inequality
is an equality when u or v is null. Otherwise, if u ̸= 0 and v ̸= 0, then
u = λv for some λ ̸= 0, and since
ϕ(u, v) = λϕ(v, v) =
p
Φ(u)Φ(v),
by positivity, we must have λ > 0.
Note that the Cauchy-Schwarz inequality can also be written as
|ϕ(u, v)| ≤
p
Φ(u)
p
Φ(v).
Remark: It is easy to prove that the Cauchy-Schwarz and the Minkowski
inequalities still hold for a symmetric bilinear form that is positive, but not
necessarily deﬁnite (i.e., ϕ(u, v) ≥0 for all u, v ∈E). However, u and v
need not be linearly dependent when the equality holds.
The Minkowski inequality
p
Φ(u + v) ≤
p
Φ(u) +
p
Φ(v)

412
Euclidean Spaces
shows that the map u 7→
p
Φ(u) satisﬁes the convexity inequality (also
known as triangle inequality), condition (N3) of Deﬁnition 8.1, and since ϕ
is bilinear and positive deﬁnite, it also satisﬁes conditions (N1) and (N2)
of Deﬁnition 8.1, and thus it is a norm on E. The norm induced by ϕ is
called the Euclidean norm induced by ϕ.
The Cauchy-Schwarz inequality can be written as
|u · v| ≤∥u∥∥v∥,
and the Minkowski inequality as
∥u + v∥≤∥u∥+ ∥v∥.
If u and v are nonzero vectors then the Cauchy-Schwarz inequality
implies that
−1 ≤
u · v
∥u∥∥v∥≤+1.
Then there is a unique θ ∈[0, π] such that
cos θ =
u · v
∥u∥∥v∥.
We have u = v iﬀθ = 0 and u = −v iﬀθ = π.
For 0 < θ < π, the
vectors u and v are linearly independent and there is an orientation of the
plane spanned by u and v such that θ is the angle between u and v. See
Problem 11.8 for the precise notion of orientation. If u is a unit vector
(which means that ∥u∥= 1), then the vector
(∥v∥cos θ)u = (u · v)u = (v · u)u
is called the orthogonal projection of v onto the space spanned by u.
Remark: One might wonder if every norm on a vector space is induced
by some Euclidean inner product. In general this is false, but remarkably,
there is a simple necessary and suﬃcient condition, which is that the norm
must satisfy the parallelogram law:
∥u + v∥2 + ∥u −v∥2 = 2(∥u∥2 + ∥v∥2).
See Figure 11.1.
If ⟨−, −⟩is an inner product, then we have
∥u + v∥2 = ∥u∥2 + ∥v∥2 + 2⟨u, v⟩
∥u −v∥2 = ∥u∥2 + ∥v∥2 −2⟨u, v⟩,

11.1. Inner Products, Euclidean Spaces
413
u
v
u + v
u-v
Fig. 11.1
The parallelogram law states that the sum of the lengths of the diagonals of
the parallelogram determined by vectors u and v equals the sum of all the sides.
and by adding and subtracting these identities, we get the parallelogram
law and the equation
⟨u, v⟩= 1
4(∥u + v∥2 −∥u −v∥2),
which allows us to recover ⟨−, −⟩from the norm.
Conversely, if ∥∥is a norm satisfying the parallelogram law, and if it
comes from an inner product, then this inner product must be given by
⟨u, v⟩= 1
4(∥u + v∥2 −∥u −v∥2).
We need to prove that the above form is indeed symmetric and bilinear.
Symmetry holds because ∥u −v∥= ∥−(u −v)∥= ∥v −u∥. Let us prove
additivity in the variable u. By the parallelogram law, we have
2(∥x + z∥2 + ∥y∥2) = ∥x + y + z∥2 + ∥x −y + z∥2
which yields
∥x + y + z∥2 = 2(∥x + z∥2 + ∥y∥2) −∥x −y + z∥2
∥x + y + z∥2 = 2(∥y + z∥2 + ∥x∥2) −∥y −x + z∥2 ,
where the second formula is obtained by swapping x and y. Then by adding
up these equations, we get
∥x + y + z∥2 = ∥x∥2 + ∥y∥2 + ∥x + z∥2 + ∥y + z∥2
−1
2 ∥x −y + z∥2 −1
2 ∥y −x + z∥2 .

414
Euclidean Spaces
Replacing z by −z in the above equation, we get
∥x + y −z∥2 = ∥x∥2 + ∥y∥2 + ∥x −z∥2 + ∥y −z∥2
−1
2 ∥x −y −z∥2 −1
2 ∥y −x −z∥2 ,
Since ∥x −y + z∥= ∥−(x −y + z)∥= ∥y −x −z∥and ∥y −x + z∥=
∥−(y −x + z)∥= ∥x −y −z∥, by subtracting the last two equations, we
get
⟨x + y, z⟩= 1
4(∥x + y + z∥2 −∥x + y −z∥2)
= 1
4(∥x + z∥2 −∥x −z∥2) + 1
4(∥y + z∥2 −∥y −z∥2)
= ⟨x, z⟩+ ⟨y, z⟩,
as desired.
Proving that
⟨λx, y⟩= λ⟨x, y⟩
for all λ ∈R
is a little tricky. The strategy is to prove the identity for λ ∈Z, then to
promote it to Q, and then to R by continuity.
Since
⟨−u, v⟩= 1
4(∥−u + v∥2 −∥−u −v∥2)
= 1
4(∥u −v∥2 −∥u + v∥2)
= −⟨u, v⟩,
the property holds for λ = −1. By linearity and by induction, for any n ∈N
with n ≥1, writing n = n −1 + 1, we get
⟨λx, y⟩= λ⟨x, y⟩
for all λ ∈N,
and since the above also holds for λ = −1, it holds for all λ ∈Z. For
λ = p/q with p, q ∈Z and q ̸= 0, we have
q⟨(p/q)u, v⟩= ⟨pu, v⟩= p⟨u, v⟩,
which shows that
⟨(p/q)u, v⟩= (p/q)⟨u, v⟩,
and thus
⟨λx, y⟩= λ⟨x, y⟩
for all λ ∈Q.
To ﬁnish the proof, we use the fact that a norm is a continuous map x 7→
∥x∥. Then, the continuous function t 7→1
t ⟨tu, v⟩deﬁned on R −{0} agrees
with ⟨u, v⟩on Q −{0}, so it is equal to ⟨u, v⟩on R −{0}. The case λ = 0
is trivial, so we are done.
We now deﬁne orthogonality.

11.2. Orthogonality and Duality in Euclidean Spaces
415
11.2
Orthogonality and Duality in Euclidean Spaces
An inner product on a vector space gives the ability to deﬁne the notion
of orthogonality. Families of nonnull pairwise orthogonal vectors must be
linearly independent. They are called orthogonal families. In a vector space
of ﬁnite dimension it is always possible to ﬁnd orthogonal bases. This is
very useful theoretically and practically. Indeed, in an orthogonal basis,
ﬁnding the coordinates of a vector is very cheap: It takes an inner product.
Fourier series make crucial use of this fact. When E has ﬁnite dimension, we
prove that the inner product on E induces a natural isomorphism between
E and its dual space E∗. This allows us to deﬁne the adjoint of a linear
map in an intrinsic fashion (i.e., independently of bases). It is also possible
to orthonormalize any basis (certainly when the dimension is ﬁnite). We
give two proofs, one using duality, the other more constructive using the
Gram-Schmidt orthonormalization procedure.
Deﬁnition 11.2. Given a Euclidean space E, any two vectors u, v ∈E are
orthogonal, or perpendicular, if u · v = 0. Given a family (ui)i∈I of vectors
in E, we say that (ui)i∈I is orthogonal if ui · uj = 0 for all i, j ∈I, where
i ̸= j. We say that the family (ui)i∈I is orthonormal if ui · uj = 0 for all
i, j ∈I, where i ̸= j, and ∥ui∥= ui · ui = 1, for all i ∈I. For any subset F
of E, the set
F ⊥= {v ∈E | u · v = 0, for all u ∈F},
of all vectors orthogonal to all vectors in F, is called the orthogonal com-
plement of F.
Since inner products are positive deﬁnite, observe that for any vector
u ∈E, we have
u · v = 0
for all v ∈E
iﬀ
u = 0.
It is immediately veriﬁed that the orthogonal complement F ⊥of F is a
subspace of E.
Example 11.5. Going back to Example 11.3 and to the inner product
⟨f, g⟩=
Z π
−π
f(t)g(t)dt
on the vector space C[−π, π], it is easily checked that
⟨sin px, sin qx⟩=
 π
if p = q, p, q ≥1,
0
if p ̸= q, p, q ≥1,

416
Euclidean Spaces
⟨cos px, cos qx⟩=
 π
if p = q, p, q ≥1,
0
if p ̸= q, p, q ≥0,
and
⟨sin px, cos qx⟩= 0,
for all p ≥1 and q ≥0, and of course, ⟨1, 1⟩=
R π
−π dx = 2π.
As a consequence, the family (sin px)p≥1∪(cos qx)q≥0 is orthogonal. It is
not orthonormal, but becomes so if we divide every trigonometric function
by √π, and 1 by
√
2π.
Proposition 11.4. Given a Euclidean space E, for any family (ui)i∈I of
nonnull vectors in E, if (ui)i∈I is orthogonal, then it is linearly independent.
Proof. Assume there is a linear dependence
X
j∈J
λjuj = 0
for some λj ∈R and some ﬁnite subset J of I. By taking the inner product
with ui for any i ∈J, and using the the bilinearity of the inner product
and the fact that ui · uj = 0 whenever i ̸= j, we get
0 = ui · 0 = ui ·

X
j∈J
λjuj


=
X
j∈J
λj(ui · uj) = λi(ui · ui),
so
λi(ui · ui) = 0,
for all i ∈J,
and since ui ̸= 0 and an inner product is positive deﬁnite, ui · ui ̸= 0, so we
obtain
λi = 0,
for all i ∈J,
which shows that the family (ui)i∈I is linearly independent.
We leave the following simple result as an exercise.
Proposition 11.5. Given a Euclidean space E, any two vectors u, v ∈E
are orthogonal iﬀ
∥u + v∥2 = ∥u∥2 + ∥v∥2.
See Figure 11.2 for a geometrical interpretation.

11.2. Orthogonality and Duality in Euclidean Spaces
417
u + v
u
v
Fig. 11.2
The sum of the lengths of the two sides of a right triangle is equal to the
length of the hypotenuse; i.e. the Pythagorean theorem.
One of the most useful features of orthonormal bases is that they aﬀord
a very simple method for computing the coordinates of a vector over any
basis vector. Indeed, assume that (e1, . . . , em) is an orthonormal basis. For
any vector
x = x1e1 + · · · + xmem,
if we compute the inner product x · ei, we get
x · ei = x1e1 · ei + · · · + xiei · ei + · · · + xmem · ei = xi,
since
ei · ej =
 1
if i = j,
0
if i ̸= j
is the property characterizing an orthonormal family. Thus,
xi = x · ei,
which means that xiei = (x · ei)ei is the orthogonal projection of x onto
the subspace generated by the basis vector ei. See Figure 11.3. If the basis
is orthogonal but not necessarily orthonormal, then
xi = x · ei
ei · ei
= x · ei
∥ei∥2 .
All this is true even for an inﬁnite orthonormal (or orthogonal) basis (ei)i∈I.

418
Euclidean Spaces
e i
x
x e
i i
Θ
Fig. 11.3
The orthogonal projection of the red vector x onto the black basis vector ei
is the maroon vector xiei. Observe that x · ei = ∥x∥cos θ.

However, remember that every vector x is expressed as a linear
combination
x =
X
i∈I
xiei
where the family of scalars (xi)i∈I has ﬁnite support, which means that
xi = 0 for all i ∈I −J, where J is a ﬁnite set. Thus, even though the
family (sin px)p≥1 ∪(cos qx)q≥0 is orthogonal (it is not orthonormal, but
becomes so if we divide every trigonometric function by √π, and 1 by
√
2π;
we won't because it looks messy!), the fact that a function f ∈C0[−π, π]
can be written as a Fourier series as
f(x) = a0 +
∞
X
k=1
(ak cos kx + bk sin kx)
does not mean that (sin px)p≥1 ∪(cos qx)q≥0 is a basis of this vector space
of functions, because in general, the families (ak) and (bk) do not have
ﬁnite support! In order for this inﬁnite linear combination to make sense,
it is necessary to prove that the partial sums
a0 +
n
X
k=1
(ak cos kx + bk sin kx)
of the series converge to a limit when n goes to inﬁnity. This requires a
topology on the space.

11.2. Orthogonality and Duality in Euclidean Spaces
419
A very important property of Euclidean spaces of ﬁnite dimension is
that the inner product induces a canonical bijection (i.e., independent of
the choice of bases) between the vector space E and its dual E∗. The reason
is that an inner product ·: E × E →R deﬁnes a nondegenerate pairing, as
deﬁned in Deﬁnition 10.4. Indeed, if u · v = 0 for all v ∈E then u = 0, and
similarly if u · v = 0 for all u ∈E then v = 0 (since an inner product is
positive deﬁnite and symmetric). By Proposition 10.5, there is a canonical
isomorphism between E and E∗. We feel that the reader will appreciate if
we exhibit this mapping explicitly and reprove that it is an isomorphism.
The mapping from E to E∗is deﬁned as follows.
Deﬁnition 11.3. For any vector u ∈E, let ϕu : E →R be the map deﬁned
such that
ϕu(v) = u · v,
for all v ∈E.
Since the inner product is bilinear, the map ϕu is a linear form in E∗.Thus,
we have a map ♭: E →E∗, deﬁned such that
♭(u) = ϕu.
Theorem 11.1. Given a Euclidean space E, the map ♭: E →E∗deﬁned
such that
♭(u) = ϕu
is linear and injective. When E is also of ﬁnite dimension, the map ♭: E →
E∗is a canonical isomorphism.
Proof. That ♭: E →E∗is a linear map follows immediately from the fact
that the inner product is bilinear. If ϕu = ϕv, then ϕu(w) = ϕv(w) for all
w ∈E, which by deﬁnition of ϕu means that u · w = v · w for all w ∈E,
which by bilinearity is equivalent to
(v −u) · w = 0
for all w ∈E, which implies that u = v, since the inner product is positive
deﬁnite. Thus, ♭: E →E∗is injective. Finally, when E is of ﬁnite dimen-
sion n, we know that E∗is also of dimension n, and then ♭: E →E∗is
bijective.
The inverse of the isomorphism ♭: E →E∗is denoted by ♯: E∗→E.

420
Euclidean Spaces
As a consequence of Theorem 11.1 we have the following corollary.
Corollary 11.1. If E is a Euclidean space of ﬁnite dimension, every linear
form f ∈E∗corresponds to a unique u ∈E such that
f(v) = u · v,
for every v ∈E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane
H, is precisely the set of vectors that are orthogonal to u.
Remarks:
(1) The "musical map" ♭: E →E∗is not surjective when E has inﬁnite
dimension. The result can be salvaged by restricting our attention to
continuous linear maps, and by assuming that the vector space E is
a Hilbert space (i.e., E is a complete normed vector space w.r.t. the
Euclidean norm). This is the famous "little" Riesz theorem (or Riesz
representation theorem).
(2) Theorem 11.1 still holds if the inner product on E is replaced by a
nondegenerate symmetric bilinear form ϕ. We say that a symmetric
bilinear form ϕ: E × E →R is nondegenerate if for every u ∈E,
if
ϕ(u, v) = 0
for all v ∈E,
then
u = 0.
For example, the symmetric bilinear form on R4 (the Lorentz form)
deﬁned such that
ϕ((x1, x2, x3, x4), (y1, y2, y3, y4)) = x1y1 + x2y2 + x3y3 −x4y4
is nondegenerate. However, there are nonnull vectors u ∈R4 such that
ϕ(u, u) = 0, which is impossible in a Euclidean space. Such vectors are
called isotropic.
Example 11.6. Consider Rn with its usual Euclidean inner product. Given
any diﬀerentiable function f : U →R, where U is some open subset of Rn,
by deﬁnition, for any x ∈U, the total derivative dfx of f at x is the linear
form deﬁned so that for all u = (u1, . . . , un) ∈Rn,
dfx(u) =
 ∂f
∂x1
(x) · · ·
∂f
∂xn
(x)




u1
...
un


=
n
X
i=1
∂f
∂xi
(x) ui.
The unique vector v ∈Rn such that
v · u = dfx(u)
for all u ∈Rn

11.2. Orthogonality and Duality in Euclidean Spaces
421
is the transpose of the Jacobian matrix of f at x, the 1 × n matrix
 ∂f
∂x1
(x) · · ·
∂f
∂xn
(x)

.
This is the gradient grad(f)x of f at x, given by
grad(f)x =






∂f
∂x1
(x)
...
∂f
∂xn
(x)






.
Example 11.7. Given any two vectors u, v ∈R3, let c(u, v) be the linear
form given by
c(u, v)(w) = det(u, v, w)
for all w ∈R3.
Since
det(u, v, w) =

u1 v1 w1
u2 v2 w2
u3 v3 w3

= w1

u2 v2
u3 v3
 −w2

u1 v1
u3 v3
 + w3

u1 v1
u2 v2

= w1(u2v3 −u3v2) + w2(u3v1 −u1v3) + w3(u1v2 −u2v1),
we see that the unique vector z ∈R3 such that
z · w = c(u, v)(w) = det(u, v, w)
for all w ∈R3
is the vector
z =


u2v3 −u3v2
u3v1 −u1v3
u1v2 −u2v1

.
This is just the cross-product u × v of u and v.
Since det(u, v, u) =
det(u, v, v) = 0, we see that u × v is orthogonal to both u and v. The
above allows us to generalize the cross-product to Rn. Given any n −1
vectors u1, . . . , un−1 ∈Rn, the cross-product u1 × · · · × un−1 is the unique
vector in Rn such that
(u1 × · · · × un−1) · w = det(u1, . . . , un−1, w)
for all w ∈Rn.
Example 11.8. Consider the vector space Mn(R) of real n × n matrices
with the inner product
⟨A, B⟩= tr(A⊤B).

422
Euclidean Spaces
Let s: Mn(R) →R be the function given by
s(A) =
n
X
i,j=1
aij,
where A = (aij). It is immediately veriﬁed that s is a linear form. It is
easy to check that the unique matrix Z such that
⟨Z, A⟩= s(A)
for all A ∈Mn(R)
is the matrix Z = ones(n, n) whose entries are all equal to 1.
11.3
Adjoint of a Linear Map
The existence of the isomorphism ♭: E →E∗is crucial to the existence
of adjoint maps.
The importance of adjoint maps stems from the fact
that the linear maps arising in physical problems are often self-adjoint,
which means that f = f ∗. Moreover, self-adjoint maps can be diagonalized
over orthonormal bases of eigenvectors. This is the key to the solution of
many problems in mechanics and engineering in general (see Strang [Strang
(1986)]).
Let E be a Euclidean space of ﬁnite dimension n, and let f : E →E be
a linear map. For every u ∈E, the map
v 7→u · f(v)
is clearly a linear form in E∗, and by Theorem 11.1, there is a unique vector
in E denoted by f ∗(u) such that
f ∗(u) · v = u · f(v),
for every v ∈E. The following simple proposition shows that the map f ∗
is linear.
Proposition 11.6. Given a Euclidean space E of ﬁnite dimension, for
every linear map f : E →E, there is a unique linear map f ∗: E →E such
that
f ∗(u) · v = u · f(v),
for all u, v ∈E.
Proof. Given u1, u2 ∈E, since the inner product is bilinear, we have
(u1 + u2) · f(v) = u1 · f(v) + u2 · f(v),
for all v ∈E, and
(f ∗(u1) + f ∗(u2)) · v = f ∗(u1) · v + f ∗(u2) · v,

11.3. Adjoint of a Linear Map
423
for all v ∈E, and since by assumption,
f ∗(u1) · v = u1 · f(v)
and
f ∗(u2) · v = u2 · f(v),
for all v ∈E. Thus we get
(f ∗(u1) + f ∗(u2)) · v = (u1 + u2) · f(v) = f ∗(u1 + u2) · v,
for all v ∈E. Since ♭is bijective, this implies that
f ∗(u1 + u2) = f ∗(u1) + f ∗(u2).
Similarly,
(λu) · f(v) = λ(u · f(v)),
for all v ∈E, and
(λf ∗(u)) · v = λ(f ∗(u) · v),
for all v ∈E, and since by assumption,
f ∗(u) · v = u · f(v),
for all v ∈E, we get
(λf ∗(u)) · v = λ(u · f(v)) = (λu) · f(v) = f ∗(λu) · v
for all v ∈E. Since ♭is bijective, this implies that
f ∗(λu) = λf ∗(u).
Thus, f ∗is indeed a linear map, and it is unique since ♭is a bijection.
Deﬁnition 11.4. Given a Euclidean space E of ﬁnite dimension, for every
linear map f : E →E, the unique linear map f ∗: E →E such that
f ∗(u) · v = u · f(v),
for all u, v ∈E
given by Proposition 11.6 is called the adjoint of f (w.r.t. to the inner
product). Linear maps f : E →E such that f = f ∗are called self-adjoint
maps.
Self-adjoint linear maps play a very important role because they have
real eigenvalues, and because orthonormal bases arise from their eigenvec-
tors. Furthermore, many physical problems lead to self-adjoint linear maps
(in the form of symmetric matrices).
Remark: Proposition 11.6 still holds if the inner product on E is replaced
by a nondegenerate symmetric bilinear form ϕ.

424
Euclidean Spaces
Linear maps such that f −1 = f ∗, or equivalently
f ∗◦f = f ◦f ∗= id,
also play an important role.
They are linear isometries, or isometries.
Rotations are special kinds of isometries. Another important class of linear
maps are the linear maps satisfying the property
f ∗◦f = f ◦f ∗,
called normal linear maps.
We will see later on that normal maps can
always be diagonalized over orthonormal bases of eigenvectors, but this
will require using a Hermitian inner product (over C).
Given two Euclidean spaces E and F, where the inner product on E
is denoted by ⟨−, −⟩1 and the inner product on F is denoted by ⟨−, −⟩2,
given any linear map f : E →F, it is immediately veriﬁed that the proof
of Proposition 11.6 can be adapted to show that there is a unique linear
map f ∗: F →E such that
⟨f(u), v⟩2 = ⟨u, f ∗(v)⟩1
for all u ∈E and all v ∈F. The linear map f ∗is also called the adjoint of
f.
The following properties immediately follow from the deﬁnition of the
adjoint map:
(1) For any linear map f : E →F, we have
f ∗∗= f.
(2) For any two linear maps f, g: E →F and any scalar λ ∈R:
(f + g)∗= f ∗+ g∗
(λf)∗= λf ∗.
(3) If E, F, G are Euclidean spaces with respective inner products
⟨−, −⟩1, ⟨−, −⟩2, and ⟨−, −⟩3, and if f : E →F and g: F →G are
two linear maps, then
(g ◦f)∗= f ∗◦g∗.
Remark: Given any basis for E and any basis for F, it is possible to
characterize the matrix of the adjoint f ∗of f in terms of the matrix of f
and the Gram matrices deﬁning the inner products; see Problem 11.5. We
will do so with respect to orthonormal bases in Proposition 11.12(2). Also,
since inner products are symmetric, the adjoint f ∗of f is also characterized
by
f(u) · v = u · f ∗(v),
for all u, v ∈E.

11.4. Existence and Construction of Orthonormal Bases
425
11.4
Existence and Construction of Orthonormal Bases
We can also use Theorem 11.1 to show that any Euclidean space of ﬁnite
dimension has an orthonormal basis.
Proposition 11.7. Given any nontrivial Euclidean space E of ﬁnite di-
mension n ≥1, there is an orthonormal basis (u1, . . . , un) for E.
Proof. We proceed by induction on n. When n = 1, take any nonnull
vector v ∈E, which exists since we assumed E nontrivial, and let
u =
v
∥v∥.
If n ≥2, again take any nonnull vector v ∈E, and let
u1 =
v
∥v∥.
Consider the linear form ϕu1 associated with u1. Since u1 ̸= 0, by Theo-
rem 11.1, the linear form ϕu1 is nonnull, and its kernel is a hyperplane H.
Since ϕu1(w) = 0 iﬀu1 ·w = 0, the hyperplane H is the orthogonal comple-
ment of {u1}. Furthermore, since u1 ̸= 0 and the inner product is positive
deﬁnite, u1 · u1 ̸= 0, and thus, u1 /∈H, which implies that E = H ⊕Ru1.
However, since E is of ﬁnite dimension n, the hyperplane H has dimension
n −1, and by the induction hypothesis, we can ﬁnd an orthonormal basis
(u2, . . . , un) for H. Now because H and the one dimensional space Ru1 are
orthogonal and E = H ⊕Ru1, it is clear that (u1, . . . , un) is an orthonormal
basis for E.
As a consequence of Proposition 11.7, given any Euclidean space of ﬁnite
dimension n, if (e1, . . . , en) is an orthonormal basis for E, then for any two
vectors u = u1e1 + · · · + unen and v = v1e1 + · · · + vnen, the inner product
u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
n
X
i=1
uivi,
and the norm ∥u∥as
∥u∥= ∥u1e1 + · · · + unen∥=

n
X
i=1
u2
i
1/2
.

426
Euclidean Spaces
The fact that a Euclidean space always has an orthonormal basis implies
that any Gram matrix G can be written as
G = Q⊤Q,
for some invertible matrix Q. Indeed, we know that in a change of basis
matrix, a Gram matrix G becomes G′ = P ⊤GP. If the basis corresponding
to G′ is orthonormal, then G′ = I, so G = (P −1)⊤P −1.
There is a more constructive way of proving Proposition 11.7, using
a procedure known as the Gram-Schmidt orthonormalization procedure.
Among other things, the Gram-Schmidt orthonormalization procedure
yields the QR-decomposition for matrices, an important tool in numeri-
cal methods.
Proposition 11.8. Given any nontrivial Euclidean space E of ﬁnite di-
mension n ≥1, from any basis (e1, . . . , en) for E we can construct an
orthonormal basis (u1, . . . , un) for E, with the property that for every k,
1 ≤k ≤n, the families (e1, . . . , ek) and (u1, . . . , uk) generate the same
subspace.
Proof. We proceed by induction on n. For n = 1, let
u1 =
e1
∥e1∥.
For n ≥2, we also let
u1 =
e1
∥e1∥,
and assuming that (u1, . . . , uk) is an orthonormal system that generates
the same subspace as (e1, . . . , ek), for every k with 1 ≤k < n, we note that
the vector
u′
k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui
is nonnull, since otherwise, because (u1, . . . , uk) and (e1, . . . , ek) generate
the same subspace, (e1, . . . , ek+1) would be linearly dependent, which is
absurd, since (e1, . . ., en) is a basis. Thus, the norm of the vector u′
k+1
being nonzero, we use the following construction of the vectors uk and u′
k:
u′
1 = e1,
u1 =
u′
1
∥u′
1∥,
and for the inductive step
u′
k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui,
uk+1 =
u′
k+1
∥u′
k+1∥,

11.4. Existence and Construction of Orthonormal Bases
427
where 1 ≤k ≤n −1. It is clear that ∥uk+1∥= 1, and since (u1, . . . , uk) is
an orthonormal system, we have
u′
k+1 · ui = ek+1 · ui −(ek+1 · ui)ui · ui = ek+1 · ui −ek+1 · ui = 0,
for all i with 1 ≤i ≤k.
This shows that the family (u1, . . . , uk+1) is
orthonormal, and since (u1, . . . , uk) and (e1, . . . , ek) generates the same
subspace, it is clear from the deﬁnition of uk+1 that (u1, . . . , uk+1) and
(e1, . . . , ek+1) generate the same subspace. This completes the induction
step and the proof of the proposition.
Note that u′
k+1 is obtained by subtracting from ek+1 the projection of
ek+1 itself onto the orthonormal vectors u1, . . . , uk that have already been
computed. Then u′
k+1 is normalized.
Example 11.9. For a speciﬁc example of this procedure, let E = R3 with
the standard Euclidean norm. Take the basis
e1 =


1
1
1


e2 =


1
0
1


e3 =


1
1
0

.
Then
u1 =
1
√
3


1
1
1

,
and
u′
2 = e2 −(e2 · u1)u1 =


1
0
1

−2
3


1
1
1

= 1
3


1
−2
1

.
This implies that
u2 =
1
√
6


1
−2
1

,
and that
u′
3 = e3 −(e3 ·u1)u1 −(e3 ·u2)u2 =


1
1
0

−2
3


1
1
1

+ 1
6


1
−2
1

= 1
2


1
0
−1

.
To complete the orthonormal basis, normalize u′
3 to obtain
u3 =
1
√
2


1
0
−1

.
An illustration of this example is provided by Figure 11.4.

428
Euclidean Spaces
e2
u
u
 
1
'2
u
 
1direction
u2direction
e3
u3'
Fig. 11.4
The top ﬁgure shows the construction of the blue u′
2 as perpendicular to the
orthogonal projection of e2 onto u1, while the bottom ﬁgure shows the construction of
the green u′
3 as normal to the plane determined by u1 and u2.
Remarks:
(1) The QR-decomposition can now be obtained very easily, but we post-
pone this until Section 11.6.
(2) The proof of Proposition 11.8 also works for a countably inﬁnite basis
for E, producing a countably inﬁnite orthonormal basis.
It should also be said that the Gram-Schmidt orthonormalization pro-
cedure that we have presented is not very stable numerically, and instead,
one should use the modiﬁed Gram-Schmidt method.
To compute u′
k+1,
instead of projecting ek+1 onto u1, . . . , uk in a single step, it is better to
perform k projections. We compute uk+1
1
, uk+1
2
, . . . , uk+1
k
as follows:
uk+1
1
= ek+1 −(ek+1 · u1) u1,
uk+1
i+1 = uk+1
i
−(uk+1
i
· ui+1) ui+1,
where 1 ≤i ≤k −1. It is easily shown that u′
k+1 = uk+1
k
.
Example 11.10. Let us apply the modiﬁed Gram-Schmidt method to the

11.4. Existence and Construction of Orthonormal Bases
429
(e1, e2, e3) basis of Example 11.9. The only change is the computation of
u′
3. For the modiﬁed Gram-Schmidt procedure, we ﬁrst calculate
u3
1 = e3 −(e3 · u1)u1 =


1
1
0

−2
3


1
1
1

= 1
3


1
1
−2

.
Then
u3
2 = u3
1 −(u3
1 · u2)u2 = 1
3


1
1
−2

+ 1
6


1
−2
1

= 1
2


1
0
−1

,
and observe that u3
2 = u′
3. See Figure 11.5.
u
 
1direction
u2direction
e3
u3
1
u
 
1direction
u2direction
u1
3
u3
2
Fig. 11.5
The top ﬁgure shows the construction of the blue u3
1 as perpendicular to the
orthogonal projection of e3 onto u1, while the bottom ﬁgure shows the construction of
the sky blue u3
2 as perpendicular to the orthogonal projection of u3
1 onto u2.
The following Matlab program implements the modiﬁed Gram-Schmidt
procedure.
function q = gramschmidt4(e)
n = size(e,1);

430
Euclidean Spaces
for i = 1:n
q(:,i) = e(:,i);
for j = 1:i-1
r = q(:,j)'*q(:,i);
q(:,i) = q(:,i) - r*q(:,j);
end
r = sqrt(q(:,i)'*q(:,i));
q(:,i) = q(:,i)/r;
end
end
If we apply the above function to the matrix


1 1 1
1 0 1
1 1 0

,
the output is the matrix


0.5774 0.4082
0.7071
0.5774 −0.8165 −0.0000
0.5774 0.4082 −0.7071

,
which matches the result of Example 11.9.
Example 11.11. If we consider polynomials and the inner product
⟨f, g⟩=
Z 1
−1
f(t)g(t)dt,
applying the Gram-Schmidt orthonormalization procedure to the polyno-
mials
1, x, x2, . . . , xn, . . . ,
which form a basis of the polynomials in one variable with real coeﬃcients,
we get a family of orthonormal polynomials Qn(x) related to the Legendre
polynomials.
The Legendre polynomials Pn(x) have many nice properties. They are
orthogonal, but their norm is not always 1.
The Legendre polynomials
Pn(x) can be deﬁned as follows. Letting fn be the function
fn(x) = (x2 −1)n,
we deﬁne Pn(x) as follows:
P0(x) = 1,
and
Pn(x) =
1
2nn! f (n)
n (x),

11.4. Existence and Construction of Orthonormal Bases
431
where f (n)
n
is the nth derivative of fn.
They can also be deﬁned inductively as follows:
P0(x) = 1,
P1(x) = x,
Pn+1(x) = 2n + 1
n + 1 xPn(x) −
n
n + 1 Pn−1(x).
Here is an explicit summation for Pn(x):
Pn(x) = 1
2n
⌊n/2⌋
X
k=0
(−1)k
n
k
2n −2k
n

xn−2k.
The polynomials Qn are related to the Legendre polynomials Pn as
follows:
Qn(x) =
r
2n + 1
2
Pn(x).
Example 11.12. Consider polynomials over [−1, 1], with the symmetric
bilinear form
⟨f, g⟩=
Z 1
−1
1
√
1 −t2 f(t)g(t)dt.
We leave it as an exercise to prove that the above deﬁnes an inner product.
It can be shown that the polynomials Tn(x) given by
Tn(x) = cos(n arccos x),
n ≥0,
(equivalently, with x = cos θ, we have Tn(cos θ) = cos(nθ)) are orthogo-
nal with respect to the above inner product. These polynomials are the
Chebyshev polynomials. Their norm is not equal to 1. Instead, we have
⟨Tn, Tn⟩=
(
π
2
if n > 0,
π
if n = 0.
Using the identity (cos θ + i sin θ)n = cos nθ + i sin nθ and the binomial
formula, we obtain the following expression for Tn(x):
Tn(x) =
⌊n/2⌋
X
k=0
 n
2k

(x2 −1)kxn−2k.
The Chebyshev polynomials are deﬁned inductively as follows:
T0(x) = 1
T1(x) = x
Tn+1(x) = 2xTn(x) −Tn−1(x),
n ≥1.

432
Euclidean Spaces
Using these recurrence equations, we can show that
Tn(x) = (x −
√
x2 −1)n + (x +
√
x2 −1)n
2
.
The polynomial Tn has n distinct roots in the interval [−1, 1]. The Cheby-
shev polynomials play an important role in approximation theory. They
are used as an approximation to a best polynomial approximation of a
continuous function under the sup-norm (∞-norm).
The inner products of the last two examples are special cases of an inner
product of the form
⟨f, g⟩=
Z 1
−1
W(t)f(t)g(t)dt,
where W(t) is a weight function. If W is a nonzero continuous function
such that W(x) ≥0 on (−1, 1), then the above bilinear form is indeed
positive deﬁnite. Families of orthogonal polynomials used in approximation
theory and in physics arise by a suitable choice of the weight function W.
Besides the previous two examples, the Hermite polynomials correspond to
W(x) = e−x2, the Laguerre polynomials to W(x) = e−x, and the Jacobi
polynomials to W(x) = (1 −x)α(1 + x)β, with α, β > −1. Comprehensive
treatments of orthogonal polynomials can be found in Lebedev [Lebedev
(1972)], Sansone [Sansone (1991)], and Andrews, Askey and Roy [Andrews
et al. (2000)].
We can also prove the following proposition regarding orthogonal spaces.
Proposition 11.9. Given any nontrivial Euclidean space E of ﬁnite di-
mension n ≥1, for any subspace F of dimension k, the orthogonal comple-
ment F ⊥of F has dimension n −k, and E = F ⊕F ⊥. Furthermore, we
have F ⊥⊥= F.
Proof. From Proposition 11.7, the subspace F has some orthonormal basis
(u1, . . . , uk). This linearly independent family (u1, . . . , uk) can be extended
to a basis (u1, . . . , uk, vk+1, . . . , vn), and by Proposition 11.8, it can be
converted to an orthonormal basis (u1, . . . , un), which contains (u1, . . . , uk)
as an orthonormal basis of F. Now any vector w = w1u1 + · · · + wnun ∈E
is orthogonal to F iﬀw · ui = 0, for every i, where 1 ≤i ≤k, iﬀwi = 0 for
every i, where 1 ≤i ≤k. Clearly, this shows that (uk+1, . . . , un) is a basis
of F ⊥, and thus E = F ⊕F ⊥, and F ⊥has dimension n −k. Similarly, any
vector w = w1u1 + · · · + wnun ∈E is orthogonal to F ⊥iﬀw · ui = 0, for
every i, where k + 1 ≤i ≤n, iﬀwi = 0 for every i, where k + 1 ≤i ≤n.
Thus, (u1, . . . , uk) is a basis of F ⊥⊥, and F ⊥⊥= F.

11.5. Linear Isometries (Orthogonal Transformations)
433
11.5
Linear Isometries (Orthogonal Transformations)
In this section we consider linear maps between Euclidean spaces that pre-
serve the Euclidean norm. These transformations, sometimes called rigid
motions, play an important role in geometry.
Deﬁnition 11.5. Given any two nontrivial Euclidean spaces E and F of
the same ﬁnite dimension n, a function f : E →F is an orthogonal trans-
formation, or a linear isometry, if it is linear and
∥f(u)∥= ∥u∥,
for all u ∈E.
Remarks:
(1) A linear isometry is often deﬁned as a linear map such that
∥f(v) −f(u)∥= ∥v −u∥,
for all u, v ∈E.
Since the map f is linear, the two deﬁnitions are
equivalent. The second deﬁnition just focuses on preserving the dis-
tance between vectors.
(2) Sometimes, a linear map satisfying the condition of Deﬁnition 11.5 is
called a metric map, and a linear isometry is deﬁned as a bijective
metric map.
An isometry (without the word linear) is sometimes deﬁned as a function
f : E →F (not necessarily linear) such that
∥f(v) −f(u)∥= ∥v −u∥,
for all u, v ∈E, i.e., as a function that preserves the distance. This require-
ment turns out to be very strong. Indeed, the next proposition shows that
all these deﬁnitions are equivalent when E and F are of ﬁnite dimension,
and for functions such that f(0) = 0.
Proposition 11.10. Given any two nontrivial Euclidean spaces E and F
of the same ﬁnite dimension n, for every function f : E →F, the following
properties are equivalent:
(1) f is a linear map and ∥f(u)∥= ∥u∥, for all u ∈E;
(2) ∥f(v) −f(u)∥= ∥v −u∥, for all u, v ∈E, and f(0) = 0;
(3) f(u) · f(v) = u · v, for all u, v ∈E.
Furthermore, such a map is bijective.

434
Euclidean Spaces
Proof. Clearly, (1) implies (2), since in (1) it is assumed that f is linear.
Assume that (2) holds. In fact, we shall prove a slightly stronger result.
We prove that if
∥f(v) −f(u)∥= ∥v −u∥
for all u, v ∈E, then for any vector τ ∈E, the function g: E →F deﬁned
such that
g(u) = f(τ + u) −f(τ)
for all u ∈E is a linear map such that g(0) = 0 and (3) holds. Clearly,
g(0) = f(τ) −f(τ) = 0.
Note that from the hypothesis
∥f(v) −f(u)∥= ∥v −u∥
for all u, v ∈E, we conclude that
∥g(v) −g(u)∥= ∥f(τ + v) −f(τ) −(f(τ + u) −f(τ))∥,
= ∥f(τ + v) −f(τ + u)∥,
= ∥τ + v −(τ + u)∥,
= ∥v −u∥,
for all u, v ∈E. Since g(0) = 0, by setting u = 0 in
∥g(v) −g(u)∥= ∥v −u∥,
we get
∥g(v)∥= ∥v∥
for all v ∈E. In other words, g preserves both the distance and the norm.
To prove that g preserves the inner product, we use the simple fact that
2u · v = ∥u∥2 + ∥v∥2 −∥u −v∥2
for all u, v ∈E. Then since g preserves distance and norm, we have
2g(u) · g(v) = ∥g(u)∥2 + ∥g(v)∥2 −∥g(u) −g(v)∥2
= ∥u∥2 + ∥v∥2 −∥u −v∥2
= 2u · v,
and thus g(u) · g(v) = u · v, for all u, v ∈E, which is (3). In particular,
if f(0) = 0, by letting τ = 0, we have g = f, and f preserves the scalar
product, i.e., (3) holds.

11.5. Linear Isometries (Orthogonal Transformations)
435
Now assume that (3) holds. Since E is of ﬁnite dimension, we can pick
an orthonormal basis (e1, . . . , en) for E. Since f preserves inner products,
(f(e1), . . ., f(en)) is also orthonormal, and since F also has dimension n,
it is a basis of F. Then note that since (e1, . . . , en) and (f(e1), . . . , f(en))
are orthonormal bases, for any u ∈E we have
u =
n
X
i=1
(u · ei)ei =
n
X
i=1
uiei
and
f(u) =
n
X
i=1
(f(u) · f(ei))f(ei),
and since f preserves inner products, this shows that
f(u) =
n
X
i=1
(f(u) · f(ei))f(ei) =
n
X
i=1
(u · ei)f(ei) =
n
X
i=1
uif(ei),
which proves that f is linear. Obviously, f preserves the Euclidean norm,
and (3) implies (1).
Finally, if f(u) = f(v), then by linearity f(v−u) = 0, so that ∥f(v−u)∥
= 0, and since f preserves norms, we must have ∥v−u∥= 0, and thus u = v.
Thus, f is injective, and since E and F have the same ﬁnite dimension, f
is bijective.
Remarks:
(i) The dimension assumption is needed only to prove that (3) implies (1)
when f is not known to be linear, and to prove that f is surjective, but
the proof shows that (1) implies that f is injective.
(ii) The implication that (3) implies (1) holds if we also assume that f is
surjective, even if E has inﬁnite dimension.
In (2), when f does not satisfy the condition f(0) = 0, the proof shows
that f is an aﬃne map. Indeed, taking any vector τ as an origin, the map
g is linear, and
f(τ + u) = f(τ) + g(u)
for all u ∈E.
By Proposition 5.14, this shows that f is aﬃne with associated linear map g.

436
Euclidean Spaces
This fact is worth recording as the following proposition.
Proposition 11.11. Given any two nontrivial Euclidean spaces E and F
of the same ﬁnite dimension n, for every function f : E →F, if
∥f(v) −f(u)∥= ∥v −u∥
for all u, v ∈E,
then f is an aﬃne map, and its associated linear map g is an isometry.
In view of Proposition 11.10, we usually abbreviate "linear isometry"
as "isometry," unless we wish to emphasize that we are dealing with a map
between vector spaces.
We are now going to take a closer look at the isometries f : E →E of
a Euclidean space of ﬁnite dimension.
11.6
The Orthogonal Group, Orthogonal Matrices
In this section we explore some of the basic properties of the orthogonal
group and of orthogonal matrices.
Proposition 11.12. Let E be any Euclidean space of ﬁnite dimension n,
and let f : E →E be any linear map. The following properties hold:
(1) The linear map f : E →E is an isometry iﬀ
f ◦f ∗= f ∗◦f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A,
then the matrix of f ∗is the transpose A⊤of A, and f is an isometry
iﬀA satisﬁes the identities
A A⊤= A⊤A = In,
where In denotes the identity matrix of order n, iﬀthe columns of A
form an orthonormal basis of Rn, iﬀthe rows of A form an orthonormal
basis of Rn.
Proof. (1) The linear map f : E →E is an isometry iﬀ
f(u) · f(v) = u · v,
for all u, v ∈E, iﬀ
f ∗(f(u)) · v = f(u) · f(v) = u · v
for all u, v ∈E, which implies
(f ∗(f(u)) −u) · v = 0

11.6. The Orthogonal Group, Orthogonal Matrices
437
for all u, v ∈E. Since the inner product is positive deﬁnite, we must have
f ∗(f(u)) −u = 0
for all u ∈E, that is,
f ∗◦f = id.
But an endomorphism f of a ﬁnite-dimensional vector space that has a left
inverse is an isomorphism, so f ◦f ∗= id. The converse is established by
doing the above steps backward.
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j) be the
matrix of f, and let B = (bi j) be the matrix of f ∗. Since f ∗is characterized
by
f ∗(u) · v = u · f(v)
for all u, v ∈E, using the fact that if w = w1e1 + · · · + wnen we have
wk = w · ek for all k, 1 ≤k ≤n, letting u = ei and v = ej, we get
bj i = f ∗(ei) · ej = ei · f(ej) = ai j,
for all i, j, 1 ≤i, j ≤n. Thus, B = A⊤. Now if X and Y are arbitrary
matrices over the basis (e1, . . . , en), denoting as usual the jth column of X
by Xj, and similarly for Y , a simple calculation shows that
X⊤Y = (Xi · Y j)1≤i,j≤n.
Then it is immediately veriﬁed that if X = Y = A, then
A⊤A = A A⊤= In
iﬀthe column vectors (A1, . . . , An) form an orthonormal basis. Thus, from
(1), we see that (2) is clear (also because the rows of A are the columns of
A⊤).
Proposition 11.12 shows that the inverse of an isometry f is its adjoint
f ∗. Recall that the set of all real n × n matrices is denoted by Mn(R).
Proposition 11.12 also motivates the following deﬁnition.
Deﬁnition 11.6. A real n × n matrix is an orthogonal matrix if
A A⊤= A⊤A = In.
Remark: It is easy to show that the conditions A A⊤= In, A⊤A = In, and
A−1 = A⊤, are equivalent. Given any two orthonormal bases (u1, . . . , un)
and (v1, . . . , vn), if P is the change of basis matrix from (u1, . . . , un) to

438
Euclidean Spaces
(v1, . . . , vn), since the columns of P are the coordinates of the vectors vj
with respect to the basis (u1, . . . , un), and since (v1, . . . , vn) is orthonormal,
the columns of P are orthonormal, and by Proposition 11.12 (2), the matrix
P is orthogonal.
The proof of Proposition 11.10 (3) also shows that if f is an isometry,
then the image of an orthonormal basis (u1, . . . , un) is an orthonormal basis.
Students often ask why orthogonal matrices are not called orthonormal
matrices, since their columns (and rows) are orthonormal bases! I have
no good answer, but isometries do preserve orthogonality, and orthogonal
matrices correspond to isometries.
Recall that the determinant det(f) of a linear map f : E →E is inde-
pendent of the choice of a basis in E. Also, for every matrix A ∈Mn(R), we
have det(A) = det(A⊤), and for any two n × n matrices A and B, we have
det(AB) = det(A) det(B). Then if f is an isometry, and A is its matrix
with respect to any orthonormal basis, A A⊤= A⊤A = In implies that
det(A)2 = 1, that is, either det(A) = 1, or det(A) = −1. It is also clear
that the isometries of a Euclidean space of dimension n form a group, and
that the isometries of determinant +1 form a subgroup. This leads to the
following deﬁnition.
Deﬁnition 11.7. Given a Euclidean space E of dimension n, the set of
isometries f : E →E forms a subgroup of GL(E) denoted by O(E), or
O(n) when E = Rn, called the orthogonal group (of E). For every isometry
f, we have det(f) = ±1, where det(f) denotes the determinant of f. The
isometries such that det(f) = 1 are called rotations, or proper isometries, or
proper orthogonal transformations, and they form a subgroup of the special
linear group SL(E) (and of O(E)), denoted by SO(E), or SO(n) when
E = Rn, called the special orthogonal group (of E). The isometries such
that det(f) = −1 are called improper isometries, or improper orthogonal
transformations, or ﬂip transformations.
11.7
The Rodrigues Formula
When n = 3 and A is a skew symmetric matrix, it is possible to work out
an explicit formula for eA. For any 3 × 3 real skew symmetric matrix
A =


0 −c b
c
0 −a
−b a
0

,

11.7. The Rodrigues Formula
439
if we let θ =
√
a2 + b2 + c2 and
B =


a2 ab ac
ab b2 bc
ac bc c2

,
then we have the following result known as Rodrigues' formula (1840). The
(real) vector space of n × n skew symmetric matrices is denoted by so(n).
Proposition 11.13.
The exponential map exp: so(3) →SO(3) is given
by
eA = cos θ I3 + sin θ
θ
A + (1 −cos θ)
θ2
B,
or, equivalently, by
eA = I3 + sin θ
θ
A + (1 −cos θ)
θ2
A2
if θ ̸= 0, with e03 = I3.
Proof sketch. First observe that
A2 = −θ2I3 + B,
since
A2 =


0 −c b
c
0 −a
−b a
0




0 −c b
c
0 −a
−b a
0

=


−c2 −b2
ba
ca
ab
−c2 −a2
cb
ac
cb
−b2 −a2


=


−a2 −b2 −c2
0
0
0
−a2 −b2 −c2
0
0
0
−a2 −b2 −c2

+


a2 ba ca
ab b2 cb
ac cb c2


= −θ2I3 + B,
and that
AB = BA = 0.
From the above, deduce that
A3 = −θ2A,
and for any k ≥0,
A4k+1 = θ4kA,
A4k+2 = θ4kA2,
A4k+3 = −θ4k+2A,
A4k+4 = −θ4k+2A2.

440
Euclidean Spaces
Then prove the desired result by writing the power series for eA and re-
grouping terms so that the power series for cos θ and sin θ show up. In
particular
eA = I3 +
X
p≥1
Ap
p! = I3 +
X
p≥0
A2p+1
(2p + 1)! +
X
p≥1
A2p
(2p)!
= I3 +
X
p≥0
(−1)pθ2p
(2p + 1)! A +
X
p≥1
(−1)p−1θ2(p−1)
(2p)!
A2
= I3 + A
θ
X
p≥0
(−1)pθ2p+1
(2p + 1)!
−A2
θ2
X
p≥1
(−1)pθ2p
(2p)!
= I3 + sin θ
θ
A −A2
θ2
X
p≥0
(−1)pθ2p
(2p)!
+ A2
θ2
= I3 + sin θ
θ
A + (1 −cos θ)
θ2
A2,
as claimed.
The above formulae are the well-known formulae expressing a rotation
of axis speciﬁed by the vector (a, b, c) and angle θ.
The Rodrigues formula can used to show that the exponential map
exp: so(3) →SO(3) is surjective.
Given any rotation matrix R ∈SO(3), we have the following cases:
(1) The case R = I is trivial.
(2) If R ̸= I and tr(R) ̸= −1, then
exp−1(R) =

θ
2 sin θ(R −RT )
 1 + 2 cos θ = tr(R)

.
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R.)
Then there is a unique skew-symmetric B with corresponding θ satis-
fying 0 < θ < π such that eB = R.
(3) If R ̸= I and tr(R) = −1, then R is a rotation by the angle π and
things are more complicated, but a matrix B can be found. We leave
this part as a good exercise: see Problem 16.8.
The computation of a logarithm of a rotation in SO(3) as sketched
above has applications in kinematics, robotics, and motion interpolation.
As an immediate corollary of the Gram-Schmidt orthonormalization
procedure, we obtain the QR-decomposition for invertible matrices.

11.8. QR-Decomposition for Invertible Matrices
441
11.8
QR-Decomposition for Invertible Matrices
Now that we have the deﬁnition of an orthogonal matrix, we can explain
how the Gram-Schmidt orthonormalization procedure immediately yields
the QR-decomposition for matrices.
Deﬁnition 11.8. Given any real n × n matrix A, a QR-decomposition of
A is any pair of n × n matrices (Q, R), where Q is an orthogonal matrix
and R is an upper triangular matrix such that A = QR.
Note that if A is not invertible, then some diagonal entry in R must be
zero.
Proposition 11.14. Given any real n × n matrix A, if A is invertible,
then there is an orthogonal matrix Q and an upper triangular matrix R
with positive diagonal entries such that A = QR.
Proof. We can view the columns of A as vectors A1, . . . , An in En. If A
is invertible, then they are linearly independent, and we can apply Propo-
sition 11.8 to produce an orthonormal basis using the Gram-Schmidt or-
thonormalization procedure. Recall that we construct vectors Qk and Q
′k
as follows:
Q
′1 = A1,
Q1 =
Q
′1
∥Q
′1∥,
and for the inductive step
Q
′k+1 = Ak+1 −
k
X
i=1
(Ak+1 · Qi) Qi,
Qk+1 =
Q
′k+1
∥Q
′k+1∥,
where 1 ≤k ≤n −1. If we express the vectors Ak in terms of the Qi and
Q
′i, we get the triangular system
A1 = ∥Q
′1∥Q1,
...
Aj = (Aj · Q1) Q1 + · · · + (Aj · Qi) Qi + · · · + (Aj · Qj−1) Qj−1 + ∥Q
′j∥Qj,
...
An = (An · Q1) Q1 + · · · + (An · Qn−1) Qn−1 + ∥Q
′n∥Qn.
Letting rk k = ∥Q
′k∥, and ri j = Aj · Qi (the reversal of i and j on
the right-hand side is intentional!), where 1 ≤k ≤n, 2 ≤j ≤n, and

442
Euclidean Spaces
1 ≤i ≤j −1, and letting qi j be the ith component of Qj, we note that
ai j, the ith component of Aj, is given by
ai j = r1 jqi 1+· · ·+ri jqi i+· · ·+rj jqi j = qi 1r1 j +· · ·+qi iri j +· · ·+qi jrj j.
If we let Q = (qi j), the matrix whose columns are the components of the
Qj, and R = (ri j), the above equations show that A = QR, where R is
upper triangular. The diagonal entries rk k = ∥Q
′k∥= Ak · Qk are indeed
positive.
The reader should try the above procedure on some concrete examples
for 2 × 2 and 3 × 3 matrices.
Remarks:
(1) Because the diagonal entries of R are positive, it can be shown that Q
and R are unique. More generally, if A is invertible and if A = Q1R1 =
Q2R2 are two QR-decompositions for A, then
R1R−1
2
= Q⊤
1 Q2.
The matrix Q⊤
1 Q2 is orthogonal and it is easy to see that R1R−1
2
is
upper triangular. But an upper triangular matrix which is orthogonal
must be a diagonal matrix D with diagonal entries ±1, so Q2 = Q1D
and R2 = DR1.
(2) The QR-decomposition holds even when A is not invertible. In this
case, R has some zero on the diagonal.
However, a diﬀerent proof
is needed. We will give a nice proof using Householder matrices (see
Proposition 12.1, and also Strang [Strang (1986, 1988)], Golub and
Van Loan [Golub and Van Loan (1996)], Trefethen and Bau [Trefethen
and Bau III (1997)], Demmel [Demmel (1997)], Kincaid and Cheney
[Kincaid and Cheney (1996)], or Ciarlet [Ciarlet (1989)]).
For better numerical stability, it is preferable to use the modiﬁed Gram-
Schmidt method to implement the QR-factorization method.
Here is a
Matlab program implementing QR-factorization using modiﬁed Gram-
Schmidt.
function [Q,R] = qrv4(A)
n = size(A,1);
for i = 1:n
Q(:,i) = A(:,i);
for j = 1:i-1

11.8. QR-Decomposition for Invertible Matrices
443
R(j,i) = Q(:,j)'*Q(:,i);
Q(:,i) = Q(:,i) - R(j,i)*Q(:,j);
end
R(i,i) = sqrt(Q(:,i)'*Q(:,i));
Q(:,i) = Q(:,i)/R(i,i);
end
end
Example 11.13. Consider the matrix
A =


0 0 5
0 4 1
1 1 1

.
To determine the QR-decomposition of A, we ﬁrst use the Gram-Schmidt
orthonormalization procedure to calculate Q = (Q1Q2Q3). By deﬁnition
A1 = Q′1 = Q1 =


0
0
1

,
and since A2 =


0
4
1

, we discover that
Q′2 = A2 −(A2 · Q1)Q1 =


0
4
1

−


0
0
1

=


0
4
0

.
Hence, Q2 =


0
1
0

. Finally,
Q′3 = A3 −(A3 · Q1)Q1 −(A3 · Q2)Q2 =


5
1
1

−


0
0
1

−


0
1
0

=


5
0
0

,
which implies that Q3 =


1
0
0

. According to Proposition 11.14, in order
to determine R we need to calculate
r11 =
Q′1 = 1
r12 = A2 · Q1 = 1
r13 = A3 · Q1 = 1
r22 =
Q′2 = 4
r23 = A3 · Q2 = 1
r33 =
Q′3 = 5.

444
Euclidean Spaces
In summary, we have found that the QR-decomposition of A =


0 0 5
0 4 1
1 1 1

is
Q =


0 0 1
0 1 0
1 0 0


and
R =


1 1 1
0 4 1
0 0 5

.
Example 11.14. Another example of QR-decomposition is
A =


1 1 2
0 0 1
1 0 0

=


1/
√
2 1/
√
2 0
0
0
1
1/
√
2 −1/
√
2 0




√
2 1/
√
2
√
2
0 1/
√
2
√
2
0
0
1

.
Example 11.15. If we apply the above Matlab function to the matrix
A =






4 1 0 0 0
1 4 1 0 0
0 1 4 1 0
0 0 1 4 1
0 0 0 1 4






,
we obtain
Q =






0.9701 −0.2339 0.0619 −0.0166 0.0046
0.2425 0.9354 −0.2477 0.0663 −0.0184
0
0.2650
0.9291 −0.2486 0.0691
0
0
0.2677
0.9283 −0.2581
0
0
0
0.2679
0.9634






and
R =






4.1231 1.9403 0.2425
0
0
0
3.7730 1.9956 0.2650
0
0
0
3.7361 1.9997 0.2677
0
0
073.7324 2.0000
0
0
0
0
3.5956






.
Remark: The Matlab function qr, called by [Q, R] = qr(A), does not
necessarily return an upper-triangular matrix whose diagonal entries are
positive.
The QR-decomposition yields a rather eﬃcient and numerically stable
method for solving systems of linear equations.
Indeed, given a system

11.8. QR-Decomposition for Invertible Matrices
445
Ax = b, where A is an n × n invertible matrix, writing A = QR, since Q is
orthogonal, we get
Rx = Q⊤b,
and since R is upper triangular, we can solve it by Gaussian elimination, by
solving for the last variable xn ﬁrst, substituting its value into the system,
then solving for xn−1, etc. The QR-decomposition is also very useful in
solving least squares problems (we will come back to this in Chapter 21),
and for ﬁnding eigenvalues; see Chapter 17. It can be easily adapted to
the case where A is a rectangular m × n matrix with independent columns
(thus, n ≤m). In this case, Q is not quite orthogonal. It is an m×n matrix
whose columns are orthogonal, and R is an invertible n×n upper triangular
matrix with positive diagonal entries. For more on QR, see Strang [Strang
(1986, 1988)], Golub and Van Loan [Golub and Van Loan (1996)], Demmel
[Demmel (1997)], Trefethen and Bau [Trefethen and Bau III (1997)], or
Serre [Serre (2010)].
A somewhat surprising consequence of the QR-decomposition is a fa-
mous determinantal inequality due to Hadamard.
Proposition 11.15. (Hadamard) For any real n × n matrix A = (aij), we
have
| det(A)| ≤
n
Y
i=1
 n
X
j=1
a2
ij
1/2
and
| det(A)| ≤
n
Y
j=1
 n
X
i=1
a2
ij
1/2
.
Moreover, equality holds iﬀeither A has a zero row in the left inequality or
a zero column in the right inequality, or A is orthogonal.
Proof. If det(A) = 0, then the inequality is trivial. In addition, if the
right-hand side is also 0, then either some column or some row is zero. If
det(A) ̸= 0, then we can factor A as A = QR, with Q is orthogonal and
R = (rij) upper triangular with positive diagonal entries. Then since Q is
orthogonal det(Q) = ±1, so
| det(A)| = | det(Q)| | det(R)| =
Y
j=1
rjj.
Now as Q is orthogonal, it preserves the Euclidean norm, so
n
X
i=1
a2
ij =
Aj2
2 =
QRj2
2 =
Rj2
2 =
n
X
i=1
r2
ij ≥r2
jj,
which implies that
| det(A)| =
n
Y
j=1
rjj ≤
n
Y
j=1
Rj
2 =
n
Y
j=1
 n
X
i=1
a2
ij
1/2
.

446
Euclidean Spaces
The other inequality is obtained by replacing A by A⊤. Finally, if det(A) ̸=
0 and equality holds, then we must have
rjj =
Aj
2 ,
1 ≤j ≤n,
which can only occur if A is orthogonal.
Another version of Hadamard's inequality applies to symmetric positive
semideﬁnite matrices.
Proposition 11.16. (Hadamard) For any real n × n matrix A = (aij), if
A is symmetric positive semideﬁnite, then we have
det(A) ≤
n
Y
i=1
aii.
Moreover, if A is positive deﬁnite, then equality holds iﬀA is a diagonal
matrix.
Proof. If det(A) = 0, the inequality is trivial. Otherwise, A is positive
deﬁnite, and by Theorem 7.4 (the Cholesky Factorization), there is a unique
upper triangular matrix B with positive diagonal entries such that
A = B⊤B.
Thus, det(A) = det(B⊤B) = det(B⊤) det(B) = det(B)2. If we apply the
Hadamard inequality (Proposition 11.15) to B, we obtain
det(B) ≤
n
Y
j=1
 n
X
i=1
b2
ij
1/2
.
(11.1)
However, the diagonal entries ajj of A = B⊤B are precisely the square
norms
Bj2
2 = Pn
i=1 b2
ij, so by squaring (11.1), we obtain
det(A) = det(B)2 ≤
n
Y
j=1
 n
X
i=1
b2
ij

=
n
Y
j=1
ajj.
If det(A) ̸= 0 and equality holds, then B must be orthogonal, which implies
that B is a diagonal matrix, and so is A.
We derived the second Hadamard inequality (Proposition 11.16) from
the ﬁrst (Proposition 11.15).
We leave it as an exercise to prove that
the ﬁrst Hadamard inequality can be deduced from the second Hadamard
inequality.

11.9. Some Applications of Euclidean Geometry
447
11.9
Some Applications of Euclidean Geometry
Euclidean geometry has applications in computational geometry, in par-
ticular Voronoi diagrams and Delaunay triangulations. In turn, Voronoi
diagrams have applications in motion planning (see O'Rourke [O'Rourke
(1998)]).
Euclidean geometry also has applications to matrix analysis.
Recall
that a real n × n matrix A is symmetric if it is equal to its transpose A⊤.
One of the most important properties of symmetric matrices is that they
have real eigenvalues and that they can be diagonalized by an orthogonal
matrix (see Chapter 16). This means that for every symmetric matrix A,
there is a diagonal matrix D and an orthogonal matrix P such that
A = PDP ⊤.
Even though it is not always possible to diagonalize an arbitrary matrix,
there are various decompositions involving orthogonal matrices that are of
great practical interest. For example, for every real matrix A, there is the
QR-decomposition, which says that a real matrix A can be expressed as
A = QR,
where Q is orthogonal and R is an upper triangular matrix.
This can
be obtained from the Gram-Schmidt orthonormalization procedure, as we
saw in Section 11.8, or better, using Householder matrices, as shown in
Section 12.2. There is also the polar decomposition, which says that a real
matrix A can be expressed as
A = QS,
where Q is orthogonal and S is symmetric positive semideﬁnite (which
means that the eigenvalues of S are nonnegative). Such a decomposition
is important in continuum mechanics and in robotics, since it separates
stretching from rotation.
Finally, there is the wonderful singular value
decomposition, abbreviated as SVD, which says that a real matrix A can
be expressed as
A = V DU ⊤,
where U and V are orthogonal and D is a diagonal matrix with nonneg-
ative entries (see Chapter 20). This decomposition leads to the notion of
pseudo-inverse, which has many applications in engineering (least squares
solutions, etc.). For an excellent presentation of all these notions, we highly
recommend Strang [Strang (1988, 1986)], Golub and Van Loan [Golub and

448
Euclidean Spaces
Van Loan (1996)], Demmel [Demmel (1997)], Serre [Serre (2010)], and Tre-
fethen and Bau [Trefethen and Bau III (1997)].
The method of least squares, invented by Gauss and Legendre around
1800, is another great application of Euclidean geometry. Roughly speak-
ing, the method is used to solve inconsistent linear systems Ax = b, where
the number of equations is greater than the number of variables. Since
this is generally impossible, the method of least squares consists in ﬁnding
a solution x minimizing the Euclidean norm ∥Ax −b∥2, that is, the sum
of the squares of the "errors." It turns out that there is always a unique
solution x+ of smallest norm minimizing ∥Ax−b∥2, and that it is a solution
of the square system
A⊤Ax = A⊤b,
called the system of normal equations. The solution x+ can be found either
by using the QR-decomposition in terms of Householder transformations,
or by using the notion of pseudo-inverse of a matrix. The pseudo-inverse
can be computed using the SVD decomposition. Least squares methods are
used extensively in computer vision. More details on the method of least
squares and pseudo-inverses can be found in Chapter 21.
11.10
Summary
The main concepts and results of this chapter are listed below:
• Bilinear forms; positive deﬁnite bilinear forms.
• Inner products, scalar products, Euclidean spaces.
• Quadratic form associated with a bilinear form.
• The Euclidean space En.
• The polar form of a quadratic form.
• Gram matrix associated with an inner product.
• The Cauchy-Schwarz inequality; the Minkowski inequality.
• The parallelogram law.
• Orthogonality, orthogonal complement F ⊥; orthonormal family.
• The musical isomorphisms ♭: E →E∗and ♯: E∗→E (when E is
ﬁnite-dimensional); Theorem 11.1.
• The adjoint of a linear map (with respect to an inner product).
• Existence of an orthonormal basis in a ﬁnite-dimensional Euclidean
space (Proposition 11.7).
• The Gram-Schmidt orthonormalization procedure (Proposition 11.8).

11.11. Problems
449
• The Legendre and the Chebyshev polynomials.
• Linear isometries (orthogonal transformations, rigid motions).
• The orthogonal group, orthogonal matrices.
• The matrix representing the adjoint f ∗of a linear map f is the trans-
pose of the matrix representing f.
• The orthogonal group O(n) and the special orthogonal group SO(n).
• QR-decomposition for invertible matrices.
• The Hadamard inequality for arbitrary real matrices.
• The Hadamard inequality for symmetric positive semideﬁnite matrices.
• The Rodrigues formula for rotations in SO(3).
11.11
Problems
Problem 11.1. E be a vector space of dimension 2, and let (e1, e2) be a
basis of E. Prove that if a > 0 and b2 −ac < 0, then the bilinear form
deﬁned such that
ϕ(x1e1 + y1e2, x2e1 + y2e2) = ax1x2 + b(x1y2 + x2y1) + cy1y2
is a Euclidean inner product.
Problem
11.2. Let C[a, b] denote the
set of continuous functions
f : [a, b] →R. Given any two functions f, g ∈C[a, b], let
⟨f, g⟩=
Z b
a
f(t)g(t)dt.
Prove that the above bilinear form is indeed a Euclidean inner product.
Problem 11.3. Consider the inner product
⟨f, g⟩=
Z π
−π
f(t)g(t)dt
of Problem 11.2 on the vector space C[−π, π]. Prove that
⟨sin px, sin qx⟩=
 π
if p = q, p, q ≥1,
0
if p ̸= q, p, q ≥1,
⟨cos px, cos qx⟩=
 π
if p = q, p, q ≥1,
0
if p ̸= q, p, q ≥0,
⟨sin px, cos qx⟩= 0,
for all p ≥1 and q ≥0, and ⟨1, 1⟩=
R π
−π dx = 2π.

450
Euclidean Spaces
Problem 11.4. Prove that the following matrix is orthogonal and skew-
symmetric:
M =
1
√
3




0
1
1
1
−1 0 −1 1
−1 1
0 −1
−1 −1 1
0



.
Problem
11.5. Let E and F
be two ﬁnite Euclidean spaces,
let
(u1, . . . , un) be a basis of E, and let (v1, . . . , vm) be a basis of F. For any
linear map f : E →F, if A is the matrix of f w.r.t. the basis (u1, . . . , un)
and B is the matrix of f ∗w.r.t. the basis (v1, . . . , vm), if G1 is the Gram
matrix of the inner product on E (w.r.t. (u1, . . . , un)) and if G2 is the Gram
matrix of the inner product on F (w.r.t. (v1, . . . , vm)), then
B = G−1
1 A⊤G2.
Problem 11.6. Let A be an invertible matrix. Prove that if A = Q1R1 =
Q2R2 are two QR-decompositions of A and if the diagonal entries of R1
and R2 are positive, then Q1 = Q2 and R1 = R2.
Problem 11.7. Prove that the ﬁrst Hadamard inequality can be deduced
from the second Hadamard inequality.
Problem 11.8. Let E be a real vector space of ﬁnite dimension, n ≥1.
Say that two bases, (u1, . . . , un) and (v1, . . . , vn), of E have the same orien-
tation iﬀdet(P) > 0, where P the change of basis matrix from (u1, . . . , un)
and (v1, . . . , vn), namely, the matrix whose jth columns consist of the co-
ordinates of vj over the basis (u1, . . . , un).
(1) Prove that having the same orientation is an equivalence relation
with two equivalence classes.
An orientation of a vector space, E, is the choice of any ﬁxed basis, say
(e1, . . . , en), of E. Any other basis, (v1, . . . , vn), has the same orientation
as (e1, . . . , en) (and is said to be positive or direct) iﬀdet(P) > 0, else it
is said to have the opposite orientation of (e1, . . . , en) (or to be negative
or indirect), where P is the change of basis matrix from (e1, . . . , en) to
(v1, . . . , vn). An oriented vector space is a vector space with some chosen
orientation (a positive basis).
(2) Let B1
=
(u1, . . . , un) and B2
=
(v1, . . . , vn) be two or-
thonormal bases.
For any sequence of vectors, (w1, . . . , wn), in E,
let detB1(w1, . . . , wn) be the determinant of the matrix whose columns

11.11. Problems
451
are the coordinates of the wj's over the basis B1 and similarly for
detB2(w1, . . . , wn).
Prove that if B1 and B2 have the same orientation, then
detB1(w1, . . . , wn) = detB2(w1, . . . , wn).
Given any oriented vector space, E, for any sequence of vectors,
(w1, . . . , wn), in E, the common value, detB(w1, . . . , wn), for all positive
orthonormal bases, B, of E is denoted
λE(w1, . . . , wn)
and called a volume form of (w1, . . . , wn).
(3) Given any Euclidean oriented vector space, E, of dimension n for
any n −1 vectors, w1, . . . , wn−1, in E, check that the map
x 7→λE(w1, . . . , wn−1, x)
is a linear form. Then prove that there is a unique vector, denoted w1 ×
· · · × wn−1, such that
λE(w1, . . . , wn−1, x) = (w1 × · · · × wn−1) · x,
for all x ∈E. The vector w1 × · · · × wn−1 is called the cross-product of
(w1, . . . , wn−1).
It is a generalization of the cross-product in R3 (when
n = 3).
Problem 11.9. Given p vectors (u1, . . . , up) in a Euclidean space E of
dimension n ≥p, the Gram determinant (or Gramian) of the vectors
(u1, . . . , up) is the determinant
Gram(u1, . . . , up) =

∥u1∥2 ⟨u1, u2⟩. . . ⟨u1, up⟩
⟨u2, u1⟩∥u2∥2 . . . ⟨u2, up⟩
...
...
...
...
⟨up, u1⟩⟨up, u2⟩. . . ∥up∥2

.
(1) Prove that
Gram(u1, . . . , un) = λE(u1, . . . , un)2.
Hint. If (e1, . . . , en) is an orthonormal basis and A is the matrix of the
vectors (u1, . . . , un) over this basis,
det(A)2 = det(A⊤A) = det(Ai · Aj),
where Ai denotes the ith column of the matrix A, and (Ai ·Aj) denotes the
n × n matrix with entries Ai · Aj.

452
Euclidean Spaces
(2) Prove that
∥u1 × · · · × un−1∥2 = Gram(u1, . . . , un−1).
Hint. Letting w = u1 × · · · × un−1, observe that
λE(u1, . . . , un−1, w) = ⟨w, w⟩= ∥w∥2,
and show that
∥w∥4 = λE(u1, . . . , un−1, w)2 = Gram(u1, . . . , un−1, w)
= Gram(u1, . . . , un−1)∥w∥2.
Problem 11.10. Let ϕ: E × E →R be a bilinear form on a real vector
space E of ﬁnite dimension n. Given any basis (e1, . . . , en) of E, let A =
(ai j) be the matrix deﬁned such that
ai j = ϕ(ei, ej),
1 ≤i, j ≤n. We call A the matrix of ϕ w.r.t. the basis (e1, . . . , en).
(1) For any two vectors x and y, if X and Y denote the column vectors
of coordinates of x and y w.r.t. the basis (e1, . . . , en), prove that
ϕ(x, y) = X⊤AY.
(2) Recall that A is a symmetric matrix if A = A⊤. Prove that ϕ is
symmetric if A is a symmetric matrix.
(3) If (f1, . . . , fn) is another basis of E and P is the change of basis
matrix from (e1, . . . , en) to (f1, . . . , fn), prove that the matrix of ϕ w.r.t.
the basis (f1, . . . , fn) is
P ⊤AP.
The common rank of all matrices representing ϕ is called the rank of ϕ.
Problem 11.11. Let ϕ: E × E →R be a symmetric bilinear form on a
real vector space E of ﬁnite dimension n. Two vectors x and y are said to
be conjugate or orthogonal w.r.t. ϕ if ϕ(x, y) = 0. The main purpose of
this problem is to prove that there is a basis of vectors that are pairwise
conjugate w.r.t. ϕ.
(1) Prove that if ϕ(x, x) = 0 for all x ∈E, then ϕ is identically null on
E.
Otherwise, we can assume that there is some vector x ∈E such that
ϕ(x, x) ̸= 0.
Use induction to prove that there is a basis of vectors (u1, . . . , un) that
are pairwise conjugate w.r.t. ϕ.

11.11. Problems
453
Hint. For the induction step, proceed as follows. Let (u1, e2, . . . , en) be a
basis of E, with ϕ(u1, u1) ̸= 0. Prove that there are scalars λ2, . . . , λn such
that each of the vectors
vi = ei + λiu1
is conjugate to u1 w.r.t. ϕ, where 2 ≤i ≤n, and that (u1, v2, . . . , vn) is a
basis.
(2) Let (e1, . . . , en) be a basis of vectors that are pairwise conjugate
w.r.t. ϕ and assume that they are ordered such that
ϕ(ei, ei) =
 θi ̸= 0
if 1 ≤i ≤r,
0
if r + 1 ≤i ≤n,
where r is the rank of ϕ. Show that the matrix of ϕ w.r.t. (e1, . . . , en) is a
diagonal matrix, and that
ϕ(x, y) =
r
X
i=1
θixiyi,
where x = Pn
i=1 xiei and y = Pn
i=1 yiei.
Prove that for every symmetric matrix A, there is an invertible matrix
P such that
P ⊤AP = D,
where D is a diagonal matrix.
(3) Prove that there is an integer p, 0 ≤p ≤r (where r is the rank of
ϕ), such that ϕ(ui, ui) > 0 for exactly p vectors of every basis (u1, . . . , un)
of vectors that are pairwise conjugate w.r.t. ϕ (Sylvester's inertia theorem).
Proceed as follows. Assume that in the basis (u1, . . . , un), for any x ∈E,
we have
ϕ(x, x) = α1x2
1 + · · · + αpx2
p −αp+1x2
p+1 −· · · −αrx2
r,
where x = Pn
i=1 xiui, and that in the basis (v1, . . . , vn), for any x ∈E, we
have
ϕ(x, x) = β1y2
1 + · · · + βqy2
q −βq+1y2
q+1 −· · · −βry2
r,
where x = Pn
i=1 yivi, with αi > 0, βi > 0, 1 ≤i ≤r.
Assume that p > q and derive a contradiction. First consider x in the
subspace F spanned by
(u1, . . . , up, ur+1, . . . , un),

454
Euclidean Spaces
and observe that ϕ(x, x) ≥0 if x ̸= 0. Next consider x in the subspace G
spanned by
(vq+1, . . . , vr),
and observe that ϕ(x, x) < 0 if x ̸= 0. Prove that F ∩G is nontrivial (i.e.,
contains some nonnull vector), and derive a contradiction. This implies
that p ≤q. Finish the proof.
The pair (p, r −p) is called the signature of ϕ.
(4) A symmetric bilinear form ϕ is deﬁnite if for every x ∈E, if ϕ(x, x) =
0, then x = 0.
Prove that a symmetric bilinear form is deﬁnite iﬀits signature is either
(n, 0) or (0, n). In other words, a symmetric deﬁnite bilinear form has rank
n and is either positive or negative.
Problem 11.12. Consider the n × n matrices Ri,j deﬁned for all i, j with
1 ≤i < j ≤n and n ≥3, such that the only nonzero entries are
Ri,j(i, j) = −1
Ri,j(i, i) = 0
Ri,j(j, i) = 1
Ri,j(j, j) = 0
Ri,j(k, k) = 1,
1 ≤k ≤n, k ̸= i, j.
For example,
Ri,j =






















1
...
1
0 0 · · · 0 −1
0 1 · · · 0 0
... ... ... ...
...
0 0 · · · 1 0
1 0 · · · 0 0
1
...
1






















.
(1) Prove that the Ri,j are rotation matrices. Use the matrices Rij to
form a basis of the n × n skew-symmetric matrices.

11.11. Problems
455
(2) Consider the n × n symmetric matrices Si,j deﬁned for all i, j with
1 ≤i < j ≤n and n ≥3, such that the only nonzero entries are
Si,j(i, j) = 1
Si,j(i, i) = 0
Si,j(j, i) = 1
Si,j(j, j) = 0
Si,j(k, k) = 1,
1 ≤k ≤n, k ̸= i, j,
and if i + 2 ≤j then Si,j(i + 1, i + 1) = −1, else if i > 1 and j = i + 1 then
Si,j(1, 1) = −1, and if i = 1 and j = 2, then Si,j(3, 3) = −1.
For example,
Si,j =






















1
...
1
0 0 · · · 0 1
0 −1 · · · 0 0
...
...
... ... ...
0 0 · · · 1 0
1 0 · · · 0 0
1
...
1






















.
Note that Si,j has a single diagonal entry equal to −1. Prove that the
Si,j are rotations matrices.
Use Problem 2.15 together with the Si,j to form a basis of the n × n
symmetric matrices.
(3) Prove that if n ≥3, the set of all linear combinations of matrices in
SO(n) is the space Mn(R) of all n × n matrices.
Prove that if n ≥3 and if a matrix A ∈Mn(R) commutes with all
rotations matrices, then A commutes with all matrices in Mn(R).
What happens for n = 2?
Problem 11.13. Let A be an n × n real invertible matrix. Prove that if
A = Q1R1 and A = Q2R2 are two QR-decompositions of A where R1 and
R2 are upper-triangular with positive diagonal entries, then Q1 = Q2 and
R1 = R2.

456
Euclidean Spaces
Problem 11.14. (1) Let H be the aﬃne hyperplane in Rn given by the
equation
a1x1 + · · · + anxn = c,
with ai ̸= 0 for some i, 1 ≤i ≤n. The linear hyperplane H0 parallel to H
is given by the equation
a1x1 + · · · + anxn = 0,
and we say that a vector y ∈Rn is orthogonal (or perpendicular) to H iﬀy
is orthogonal to H0. Let h be the intersection of H with the line through
the origin and perpendicular to H. Prove that the coordinates of h are
given by
c
a2
1 + · · · + a2n
(a1, . . . , an).
(2) For any point p ∈H, prove that ∥h∥≤∥p∥. Thus, it is natural
to deﬁne the distance d(O, H) from the origin O to the hyperplane H as
d(O, H) = ∥h∥. Prove that
d(O, H) =
|c|
(a2
1 + · · · + a2n)
1
2 .
(3) Let S be a ﬁnite set of n ≥3 points in the plane (R2). Prove that
if for every pair of distinct points pi, pj ∈S, there is a third point pk ∈S
(distinct from pi and pj) such that pi, pj, pk belong to the same (aﬃne)
line, then all points in S belong to a common (aﬃne) line.
Hint. Proceed by contradiction and use a minimality argument. This is
either ∞-hard or relatively easy, depending how you proceed!
Problem 11.15. (The space of closed polygons in R2, after Hausmann and
Knutson.)
An open polygon P in the plane is a sequence P = (v1, . . . , vn+1) of
points vi ∈R2 called vertices (with n ≥1). A closed polygon, for short a
polygon, is an open polygon P = (v1, . . . , vn+1) such that vn+1 = v1. The
sequence of edge vectors (e1, . . . , en) associated with the open (or closed)
polygon P = (v1, . . . , vn+1) is deﬁned by
ei = vi+1 −vi,
i = 1, . . . , n.
Thus, a closed or open polygon is also deﬁned by a pair (v1, (e1, . . . , en)),
with the vertices given by
vi+1 = vi + ei,
i = 1, . . . , n.

11.11. Problems
457
Observe that a polygon (v1, (e1, . . . , en)) is closed iﬀ
e1 + · · · + en = 0.
Since every polygon (v1, (e1, . . . , en)) can be translated by −v1, so that
v1 = (0, 0), we may assume that our polygons are speciﬁed by a sequence
of edge vectors.
Recall that the plane R2 is isomorphic to C, via the isomorphism
(x, y) 7→x + iy.
We will represent each edge vector ek by the square of a complex number
wk = ak + ibk. Thus, every sequence of complex numbers (w1, . . . , wn)
deﬁnes a polygon (namely, (w2
1, . . . , w2
n)). This representation is many-to-
one: the sequences (±w1, . . . , ±wn) describe the same polygon. To every
sequence of complex numbers (w1, . . . , wn), we associate the pair of vectors
(a, b), with a, b ∈Rn, such that if wk = ak + ibk, then
a = (a1, . . . , an),
b = (b1, . . . , bn).
The mapping
(w1, . . . , wn) 7→(a, b)
is clearly a bijection, so we can also represent polygons by pairs of vectors
(a, b) ∈Rn × Rn.
(1) Prove that a polygon P represented by a pair of vectors (a, b) ∈
Rn × Rn is closed iﬀa · b = 0 and ∥a∥2 = ∥b∥2.
(2) Given a polygon P represented by a pair of vectors (a, b) ∈Rn ×Rn,
the length l(P) of the polygon P is deﬁned by l(P) = |w1|2 + · · · + |wn|2,
with wk = ak + ibk. Prove that
l(P) = ∥a∥2
2 + ∥b∥2
2 .
Deduce from (a) and (b) that every closed polygon of length 2 with n
edges is represented by a n × 2 matrix A such that A⊤A = I.
Remark: The space of all a n × 2 real matrices A such that A⊤A = I is
a space known as the Stiefel manifold S(2, n).
(3) Recall that in R2, the rotation of angle θ speciﬁed by the matrix
Rθ =
cos θ −sin θ
sin θ
cos θ

is expressed in terms of complex numbers by the map
z 7→zeiθ.

458
Euclidean Spaces
Let P be a polygon represented by a pair of vectors (a, b) ∈Rn × Rn.
Prove that the polygon Rθ(P) obtained by applying the rotation Rθ to
every vertex w2
k = (ak + ibk)2 of P is speciﬁed by the pair of vectors
(cos(θ/2)a −sin(θ/2)b, sin(θ/2)a + cos(θ/2)b)
=





a1 b1
a2 b2
...
...
an bn





 cos(θ/2) sin(θ/2)
−sin(θ/2) cos(θ/2)

.
(4) The reﬂection ρx about the x-axis corresponds to the map
z 7→z,
whose matrix is,
1 0
0 −1

.
Prove that the polygon ρx(P) obtained by applying the reﬂection ρx to
every vertex w2
k = (ak + ibk)2 of P is speciﬁed by the pair of vectors
(a, −b) =





a1 b1
a2 b2
...
...
an bn





1 0
0 −1

.
(5) Let Q ∈O(2) be any isometry such that det(Q) = −1 (a reﬂection).
Prove that there is a rotation R−θ ∈SO(2) such that
Q = ρx ◦R−θ.
Prove that the isometry Q, which is given by the matrix
Q =
cos θ
sin θ
sin θ −cos θ

,
is the reﬂection about the line corresponding to the angle θ/2 (the line of
equation y = tan(θ/2)x).
Prove that the polygon Q(P) obtained by applying the reﬂection Q =
ρx ◦R−θ to every vertex w2
k = (ak + ibk)2 of P, is speciﬁed by the pair of
vectors
(cos(θ/2)a + sin(θ/2)b, sin(θ/2)a −cos(θ/2)b)
=





a1 b1
a2 b2
...
...
an bn





cos(θ/2)
sin(θ/2)
sin(θ/2) −cos(θ/2)

.

11.11. Problems
459
(6) Deﬁne an equivalence relation ∼on S(2, n) such that if A1, A2 ∈
S(2, n) are any n × 2 matrices such that A⊤
1 A1 = A⊤
2 A2 = I, then
A1 ∼A2
iﬀ
A2 = A1Q
for some Q ∈O(2).
Prove that the quotient G(2, n) = S(2, n)/ ∼is in bijection with the set of
all 2-dimensional subspaces (the planes) of Rn. The space G(2, n) is called
a Grassmannian manifold.
Prove that up to translations and isometries in O(2) (rotations and re-
ﬂections), the n-sided closed polygons of length 2 are represented by planes
in G(2, n).
Problem 11.16. (1) Find two symmetric matrices, A and B, such that
AB is not symmetric.
(2) Find two matrices A and B such that
eAeB ̸= eA+B.
Hint. Try
A = π


0 0 0
0 0 −1
0 1 0


and
B = π


0 0 1
0 0 0
−1 0 0

,
and use the Rodrigues formula.
(3) Find some square matrices A, B such that AB ̸= BA, yet
eAeB = eA+B.
Hint. Look for 2 × 2 matrices with zero trace and use Problem 8.15.
Problem 11.17. Given a ﬁeld K and any nonempty set I, let K(I) be the
subset of the cartesian product KI consisting of all functions λ: I →K
with ﬁnite support, which means that λ(i) = 0 for all but ﬁnitely many
i ∈I. We usually denote the function deﬁned by λ as (λi)i∈I, and call is a
family indexed by I. We deﬁne addition and multiplication by a scalar as
follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I,
and
α · (µi)i∈I = (αµi)i∈I.
(1) Check that K(I) is a vector space.

460
Euclidean Spaces
(2) If I is any nonempty subset, for any i ∈I, we denote by ei the
family (ej)j∈I deﬁned so that
ej =
(
1
if j = i
0
if j ̸= i.
Prove that the family (ei)i∈I is linearly independent and spans K(I), so
that it is a basis of K(I) called the canonical basis of K(I). When I is
ﬁnite, say of cardinality n, then prove that K(I) is isomorphic to Kn.
(3) The function ι: I →K(I), such that ι(i) = ei for every i ∈I, is
clearly an injection.
For any other vector space F, for any function f : I →F, prove that
there is a unique linear map f : K(I) →F, such that
f = f ◦ι,
as in the following commutative diagram:
I
ι
/
f
!C
C
C
C
C
C
C
C
C
K(I)
f

F
.
We call the vector space K(I) the vector space freely generated by the set
I.
Problem 11.18. (Some pitfalls of inﬁnite dimension) Let E be the vector
space freely generated by the set of natural numbers, N = {0, 1, 2, . . .}, and
let (e0, e1, e2, . . . , en, . . .) be its canonical basis. We deﬁne the function ϕ
such that
ϕ(ei, ej) =











δij
if i, j ≥1,
1
if i = j = 0,
1/2j
if i = 0, j ≥1,
1/2i
if i ≥1, j = 0,
and we extend ϕ by bilinearity to a function ϕ: E × E →K. This means
that if u = P
i∈N λiei and v = P
j∈N µjej, then
ϕ
X
i∈N
λiei,
X
j∈N
µjej

=
X
i,j∈N
λiµjϕ(ei, ej),
but remember that λi ̸= 0 and µj ̸= 0 only for ﬁnitely many indices i, j.

11.11. Problems
461
(1) Prove that ϕ is positive deﬁnite, so that it is an inner product on
E.
What would happen if we changed 1/2j to 1 (or any constant)?
(2) Let H be the subspace of E spanned by the family (ei)i≥1, a hyper-
plane in E. Find H⊥and H⊥⊥, and prove that
H ̸= H⊥⊥.
(3) Let U be the subspace of E spanned by the family (e2i)i≥1, and let
V be the subspace of E spanned by the family (e2i−1)i≥1. Prove that
U ⊥= V
V ⊥= U
U ⊥⊥= U
V ⊥⊥= V,
yet
(U ∩V )⊥̸= U ⊥+ V ⊥
and
(U + V )⊥⊥̸= U + V.
If W is the subspace spanned by e0 and e1, prove that
(W ∩H)⊥̸= W ⊥+ H⊥.
(4) Consider the dual space E∗of E, and let (e∗
i )i∈N be the family of
dual forms of the basis (ei)i∈N. Check that the family (e∗
i )i∈N is linearly
independent.
(5) Let f ∈E∗be the linear form deﬁned by
f(ei) = 1
for all i ∈N.
Prove that f is not in the subspace spanned by the e∗
i . If F is the subspace
of E∗spanned by the e∗
i and f, ﬁnd F 0 and F 00, and prove that
F ̸= F 00.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 12
QR-Decomposition for Arbitrary
Matrices
12.1
Orthogonal Reﬂections
Hyperplane reﬂections are represented by matrices called Householder ma-
trices. These matrices play an important role in numerical methods, for
instance for solving systems of linear equations, solving least squares prob-
lems, for computing eigenvalues, and for transforming a symmetric matrix
into a tridiagonal matrix. We prove a simple geometric lemma that im-
mediately yields the QR-decomposition of arbitrary matrices in terms of
Householder matrices.
Orthogonal symmetries are a very important example of isometries.
First let us review the deﬁnition of projections, introduced in Section 5.2,
just after Proposition 5.5. Given a vector space E, let F and G be sub-
spaces of E that form a direct sum E = F ⊕G. Since every u ∈E can
be written uniquely as u = v + w, where v ∈F and w ∈G, we can deﬁne
the two projections pF : E →F and pG : E →G such that pF (u) = v and
pG(u) = w. In Section 5.2 we used the notation π1 and π2, but in this
section it is more convenient to use pF and pG.
It is immediately veriﬁed that pG and pF are linear maps, and that
p2
F = pF , p2
G = pG, pF ◦pG = pG ◦pF = 0,
and
pF + pG = id.
Deﬁnition 12.1. Given a vector space E, for any two subspaces F and
G that form a direct sum E = F ⊕G, the symmetry (or reﬂection) with
respect to F and parallel to G is the linear map s: E →E deﬁned such that
s(u) = 2pF (u) −u,
for every u ∈E.
463

464
QR-Decomposition for Arbitrary Matrices
Because pF + pG = id, note that we also have
s(u) = pF (u) −pG(u)
and
s(u) = u −2pG(u),
s2 = id, s is the identity on F, and s = −id on G.
We now assume that E is a Euclidean space of ﬁnite dimension.
Deﬁnition 12.2. Let E be a Euclidean space of ﬁnite dimension n. For
any two subspaces F and G, if F and G form a direct sum E = F ⊕G
and F and G are orthogonal, i.e., F = G⊥, the orthogonal symmetry (or
reﬂection) with respect to F and parallel to G is the linear map s: E →E
deﬁned such that
s(u) = 2pF (u) −u = pF (u) −pG(u),
for every u ∈E. When F is a hyperplane, we call s a hyperplane symmetry
with respect to F (or reﬂection about F), and when G is a plane (and thus
dim(F) = n −2), we call s a ﬂip about F.
A reﬂection about a hyperplane F is shown in Figure 12.1.
u
s ( u )
p G( u )
−p G ( u )
p F ( u )
F
G
s ( u )
−p G
p G
p
( u )
p F ( u )
F
Fig. 12.1
A reﬂection about the peach hyperplane F. Note that u is purple, pF (u) is
blue and pG(u) is red.
For any two vectors u, v ∈E, it is easily veriﬁed using the bilinearity of
the inner product that
∥u + v∥2 −∥u −v∥2 = 4(u · v).
(12.1)

12.1. Orthogonal Reﬂections
465
In particular, if u · v = 0, then ∥u + v∥= ∥u −v∥. Then since
u = pF (u) + pG(u)
and
s(u) = pF (u) −pG(u),
and since F and G are orthogonal, it follows that
pF (u) · pG(v) = 0,
and thus by (12.1)
∥s(u)∥= ∥pF (u) −pG(u)∥= ∥pF (u) + pG(u)∥= ∥u∥,
so that s is an isometry.
Using Proposition 11.8, it is possible to ﬁnd an orthonormal basis
(e1, . . . , en) of E consisting of an orthonormal basis of F and an orthonor-
mal basis of G. Assume that F has dimension p, so that G has dimension
n −p. With respect to the orthonormal basis (e1, . . . , en), the symmetry s
has a matrix of the form
Ip
0
0 −In−p

.
Thus, det(s) = (−1)n−p, and s is a rotation iﬀn −p is even. In particular,
when F is a hyperplane H, we have p = n −1 and n −p = 1, so that s is
an improper orthogonal transformation. When F = {0}, we have s = −id,
which is called the symmetry with respect to the origin. The symmetry
with respect to the origin is a rotation iﬀn is even, and an improper
orthogonal transformation iﬀn is odd. When n is odd, since s ◦s = id
and det(s) = (−1)n = −1, we observe that every improper orthogonal
transformation f is the composition f = (f ◦s)◦s of the rotation f ◦s with
s, the symmetry with respect to the origin. When G is a plane, p = n −2,
and det(s) = (−1)2 = 1, so that a ﬂip about F is a rotation. In particular,
when n = 3, F is a line, and a ﬂip about the line F is indeed a rotation of
measure π as illustrated by Figure 12.2.
Remark: Given any two orthogonal subspaces F, G forming a direct sum
E = F ⊕G, let f be the symmetry with respect to F and parallel to G,
and let g be the symmetry with respect to G and parallel to F. We leave
as an exercise to show that
f ◦g = g ◦f = −id.

466
QR-Decomposition for Arbitrary Matrices
F
G
u
p  (u)
F
s(u)
Fig. 12.2
A ﬂip in R3 is a rotation of π about the F axis.
When F = H is a hyperplane, we can give an explicit formula for s(u)
in terms of any nonnull vector w orthogonal to H. Indeed, from
u = pH(u) + pG(u),
since pG(u) ∈G and G is spanned by w, which is orthogonal to H, we have
pG(u) = λw
for some λ ∈R, and we get
u · w = λ∥w∥2,
and thus
pG(u) = (u · w)
∥w∥2 w.
Since
s(u) = u −2pG(u),
we get
s(u) = u −2 (u · w)
∥w∥2 w.
Since the above formula is important, we record it in the following propo-
sition.
Proposition 12.1. Let E be a ﬁnite-dimensional Euclidean space and let
H be a hyperplane in E. For any nonzero vector w orthogonal to H, the
hyperplane reﬂection s about H is given by
s(u) = u −2 (u · w)
∥w∥2 w,
u ∈E.

12.1. Orthogonal Reﬂections
467
Such reﬂections are represented by matrices called Householder matri-
ces, which play an important role in numerical matrix analysis (see Kincaid
and Cheney [Kincaid and Cheney (1996)] or Ciarlet [Ciarlet (1989)]).
Deﬁnition 12.3. A Householder matrix if a matrix of the form
H = In −2 WW ⊤
∥W∥2 = In −2 WW ⊤
W ⊤W ,
where W ∈Rn is a nonzero vector.
Householder matrices are symmetric and orthogonal. It is easily checked
that over an orthonormal basis (e1, . . . , en), a hyperplane reﬂection about
a hyperplane H orthogonal to a nonzero vector w is represented by the
matrix
H = In −2 WW ⊤
∥W∥2 ,
where W is the column vector of the coordinates of w over the basis
(e1, . . . , en). Since
pG(u) = (u · w)
∥w∥2 w,
the matrix representing pG is
WW ⊤
W ⊤W ,
and since pH + pG = id, the matrix representing pH is
In −WW ⊤
W ⊤W .
These formulae can be used to derive a formula for a rotation of R3, given
the direction w of its axis of rotation and given the angle θ of rotation.
The following fact is the key to the proof that every isometry can be
decomposed as a product of reﬂections.
Proposition 12.2. Let E be any nontrivial Euclidean space. For any two
vectors u, v ∈E, if ∥u∥= ∥v∥, then there is a hyperplane H such that
the reﬂection s about H maps u to v, and if u ̸= v, then this reﬂection is
unique. See Figure 12.3.

468
QR-Decomposition for Arbitrary Matrices
H
v-u
u
s(u) = v
Fig. 12.3
In R3, the (hyper)plane perpendicular to v −u reﬂects u onto v.
Proof. If u = v, then any hyperplane containing u does the job. Otherwise,
we must have H = {v −u}⊥, and by the above formula,
s(u) = u −2 (u · (v −u))
∥(v −u)∥2 (v −u) = u + 2∥u∥2 −2u · v
∥(v −u)∥2
(v −u),
and since
∥(v −u)∥2 = ∥u∥2 + ∥v∥2 −2u · v
and ∥u∥= ∥v∥, we have
∥(v −u)∥2 = 2∥u∥2 −2u · v,
and thus, s(u) = v.

If E is a complex vector space and the inner product is Hermitian,
Proposition 12.2 is false. The problem is that the vector v −u
does not work unless the inner product u · v is real! The proposition can
be salvaged enough to yield the QR-decomposition in terms of Householder
transformations; see Section 13.5.

12.2. QR-Decomposition Using Householder Matrices
469
We now show that hyperplane reﬂections can be used to obtain another
proof of the QR-decomposition.
12.2
QR-Decomposition Using Householder Matrices
First we state the result geometrically. When translated in terms of House-
holder matrices, we obtain the fact advertised earlier that every matrix (not
necessarily invertible) has a QR-decomposition.
Proposition 12.3. Let E be a nontrivial Euclidean space of dimension
n. For any orthonormal basis (e1, . . ., en) and for any n-tuple of vectors
(v1, . . ., vn), there is a sequence of n isometries h1, . . . , hn such that hi is
a hyperplane reﬂection or the identity, and if (r1, . . . , rn) are the vectors
given by
rj = hn ◦· · · ◦h2 ◦h1(vj),
then every rj is a linear combination of the vectors (e1, . . . , ej), 1 ≤j ≤n.
Equivalently, the matrix R whose columns are the components of the rj
over the basis (e1, . . . , en) is an upper triangular matrix. Furthermore, the
hi can be chosen so that the diagonal entries of R are nonnegative.
Proof. We proceed by induction on n. For n = 1, we have v1 = λe1 for
some λ ∈R. If λ ≥0, we let h1 = id, else if λ < 0, we let h1 = −id, the
reﬂection about the origin.
For n ≥2, we ﬁrst have to ﬁnd h1. Let
r1,1 = ∥v1∥.
If v1 = r1,1e1, we let h1 = id. Otherwise, there is a unique hyperplane
reﬂection h1 such that
h1(v1) = r1,1 e1,
deﬁned such that
h1(u) = u −2 (u · w1)
∥w1∥2 w1
for all u ∈E, where
w1 = r1,1 e1 −v1.
The map h1 is the reﬂection about the hyperplane H1 orthogonal to the
vector w1 = r1,1 e1 −v1. See Figure 12.4. Letting
r1 = h1(v1) = r1,1 e1,

470
QR-Decomposition for Arbitrary Matrices
e2
v1
H1
r1,1e1
Fig. 12.4
The construction of h1 in Proposition 12.3.
it is obvious that r1 belongs to the subspace spanned by e1, and r1,1 = ∥v1∥
is nonnegative.
Next assume that we have found k linear maps h1, . . . , hk, hyperplane
reﬂections or the identity, where 1 ≤k ≤n −1, such that if (r1, . . . , rk) are
the vectors given by
rj = hk ◦· · · ◦h2 ◦h1(vj),
then every rj is a linear combination of the vectors (e1, . . . , ej), 1 ≤j ≤k.
See Figure 12.5.
The vectors (e1, . . . , ek) form a basis for the subspace
denoted by U ′
k, the vectors (ek+1, . . . , en) form a basis for the subspace
denoted by U ′′
k , the subspaces U ′
k and U ′′
k are orthogonal, and E = U ′
k ⊕U ′′
k .
Let
uk+1 = hk ◦· · · ◦h2 ◦h1(vk+1).
We can write
uk+1 = u′
k+1 + u′′
k+1,
where u′
k+1 ∈U ′
k and u′′
k+1 ∈U ′′
k . See Figure 12.6. Let
rk+1,k+1 = ∥u′′
k+1∥.
If u′′
k+1 = rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, there is a unique
hyperplane reﬂection hk+1 such that
hk+1(u′′
k+1) = rk+1,k+1 ek+1,

12.2. QR-Decomposition Using Householder Matrices
471
e direction
e direction
e direction
1
2
3
v
v
1
2
e direction
e direction
e direction
1
2
3
v1
h1
r1
Fig. 12.5
The construction of r1 = h1(v1) in Proposition 12.3.
e direction
e direction
e direction
2
3
v2
h1(v2)
e direction
e direction
e direction
1
2
3
h1(v2)
u2
u2
'
''
2
Fig. 12.6
The construction of u2 = h1(v2) and its decomposition as u2 = u′
2 + u′′
2 .
deﬁned such that
hk+1(u) = u −2 (u · wk+1)
∥wk+1∥2 wk+1
for all u ∈E, where
wk+1 = rk+1,k+1 ek+1 −u′′
k+1.

472
QR-Decomposition for Arbitrary Matrices
The map hk+1 is the reﬂection about the hyperplane Hk+1 orthogonal to
the vector wk+1 = rk+1,k+1 ek+1 −u′′
k+1. However, since u′′
k+1, ek+1 ∈U ′′
k
and U ′
k is orthogonal to U ′′
k , the subspace U ′
k is contained in Hk+1, and
thus, the vectors (r1, . . . , rk) and u′
k+1, which belong to U ′
k, are invariant
under hk+1. This proves that
hk+1(uk+1) = hk+1(u′
k+1) + hk+1(u′′
k+1) = u′
k+1 + rk+1,k+1 ek+1
is a linear combination of (e1, . . . , ek+1). Letting
rk+1 = hk+1(uk+1) = u′
k+1 + rk+1,k+1 ek+1,
since uk+1 = hk ◦· · · ◦h2 ◦h1(vk+1), the vector
rk+1 = hk+1 ◦· · · ◦h2 ◦h1(vk+1)
is a linear combination of (e1, . . . , ek+1). See Figure 12.7. The coeﬃcient of
rk+1 over ek+1 is rk+1,k+1 = ∥u′′
k+1∥, which is nonnegative. This concludes
the induction step, and thus the proof.
Remarks:
(1) Since every hi is a hyperplane reﬂection or the identity,
ρ = hn ◦· · · ◦h2 ◦h1
is an isometry.
(2) If we allow negative diagonal entries in R, the last isometry hn may be
omitted.
(3) Instead of picking rk,k = ∥u′′
k∥, which means that
wk = rk,k ek −u′′
k,
where 1 ≤k ≤n, it might be preferable to pick rk,k = −∥u′′
k∥if this
makes ∥wk∥2 larger, in which case
wk = rk,k ek + u′′
k.
Indeed, since the deﬁnition of hk involves division by ∥wk∥2, it is de-
sirable to avoid division by very small numbers.
(4) The method also applies to any m-tuple of vectors (v1, . . . , vm), with
m ≤n. Then R is an upper triangular m × m matrix and Q is an
n × m matrix with orthogonal columns (Q⊤Q = Im). We leave the
minor adjustments to the method as an exercise to the reader

12.2. QR-Decomposition Using Householder Matrices
473
e direction
e direction
e direction
e direction
1
2
3
u2''
e direction
e direction
e direction
e direction
1
2
3
h1(v2)
u2'
u2''
u2'
h2(u2'')
h2 h1(v2)
2
2
Fig. 12.7
The construction of h2 and r2 = h2 ◦h1(v2) in Proposition 12.3.
Proposition 12.3 directly yields the QR-decomposition in terms of
Householder transformations (see Strang [Strang (1986, 1988)], Golub and
Van Loan [Golub and Van Loan (1996)], Trefethen and Bau [Trefethen
and Bau III (1997)], Kincaid and Cheney [Kincaid and Cheney (1996)], or
Ciarlet [Ciarlet (1989)]).
Theorem 12.1. For every real n×n matrix A, there is a sequence H1, . . .,
Hn of matrices, where each Hi is either a Householder matrix or the iden-
tity, and an upper triangular matrix R such that
R = Hn · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is orthogonal
and R is upper triangular, such that A = QR (a QR-decomposition of A).

474
QR-Decomposition for Arbitrary Matrices
Furthermore, R can be chosen so that its diagonal entries are nonnegative.
Proof. The jth column of A can be viewed as a vector vj over the canonical
basis (e1, . . . , en) of En (where (ej)i = 1 if i = j, and 0 otherwise, 1 ≤
i, j ≤n). Applying Proposition 12.3 to (v1, . . . , vn), there is a sequence
of n isometries h1, . . . , hn such that hi is a hyperplane reﬂection or the
identity, and if (r1, . . . , rn) are the vectors given by
rj = hn ◦· · · ◦h2 ◦h1(vj),
then every rj is a linear combination of the vectors (e1, . . . , ej), 1 ≤j ≤n.
Letting R be the matrix whose columns are the vectors rj, and Hi the
matrix associated with hi, it is clear that
R = Hn · · · H2H1A,
where R is upper triangular and every Hi is either a Householder matrix
or the identity. However, hi ◦hi = id for all i, 1 ≤i ≤n, and so
vj = h1 ◦h2 ◦· · · ◦hn(rj)
for all j, 1 ≤j ≤n. But ρ = h1 ◦h2 ◦· · · ◦hn is an isometry represented by
the orthogonal matrix Q = H1H2 · · · Hn. It is clear that A = QR, where R
is upper triangular. As we noted in Proposition 12.3, the diagonal entries
of R can be chosen to be nonnegative.
Remarks:
(1) Letting
Ak+1 = Hk · · · H2H1A,
with A1 = A, 1 ≤k ≤n, the proof of Proposition 12.3 can be
interpreted in terms of the computation of the sequence of matrices
A1, . . . , An+1 = R. The matrix Ak+1 has the shape
Ak+1 =















× × × uk+1
1
× × × ×
0 × ...
...
... ... ... ...
0 0 × uk+1
k
× × × ×
0 0 0 uk+1
k+1 × × × ×
0 0 0 uk+1
k+2 × × × ×
... ... ...
...
... ... ... ...
0 0 0 uk+1
n−1 × × × ×
0 0 0 uk+1
n
× × × ×















,

12.2. QR-Decomposition Using Householder Matrices
475
where the (k + 1)th column of the matrix is the vector
uk+1 = hk ◦· · · ◦h2 ◦h1(vk+1),
and thus
u′
k+1 =
 uk+1
1
, . . . , uk+1
k

and
u′′
k+1 =
 uk+1
k+1, uk+1
k+2, . . . , uk+1
n

.
If the last n−k −1 entries in column k +1 are all zero, there is nothing
to do, and we let Hk+1 = I. Otherwise, we kill these n−k−1 entries by
multiplying Ak+1 on the left by the Householder matrix Hk+1 sending
 0, . . . , 0, uk+1
k+1, . . . , uk+1
n

to
(0, . . . , 0, rk+1,k+1, 0, . . . , 0),
where rk+1,k+1 = ∥(uk+1
k+1, . . . , uk+1
n
)∥.
(2) If A is invertible and the diagonal entries of R are positive, it can be
shown that Q and R are unique.
(3) If we allow negative diagonal entries in R, the matrix Hn may be omit-
ted (Hn = I).
(4) The method allows the computation of the determinant of A. We have
det(A) = (−1)mr1,1 · · · rn,n,
where m is the number of Householder matrices (not the identity)
among the Hi.
(5) The "condition number" of the matrix A is preserved (see Strang
[Strang (1988)], Golub and Van Loan [Golub and Van Loan (1996)],
Trefethen and Bau [Trefethen and Bau III (1997)], Kincaid and Ch-
eney [Kincaid and Cheney (1996)], or Ciarlet [Ciarlet (1989)]). This is
very good for numerical stability.
(6) The method also applies to a rectangular m×n matrix. If m ≥n, then
R is an n × n upper triangular matrix and Q is an m × n matrix such
that Q⊤Q = In.
The following Matlab functions implement the QR-factorization method
of a real square (possibly singular) matrix A using Householder reﬂections
The main function houseqr computes the upper triangular matrix R
obtained by applying Householder reﬂections to A. It makes use of the
function house, which computes a unit vector u such that given a vector
x ∈Rp, the Householder transformation P = I−2uu⊤sets to zero all entries
in x but the ﬁrst entry x1. It only applies if ∥x(2 : p)∥1 = |x2|+· · ·+|xp| > 0.

476
QR-Decomposition for Arbitrary Matrices
Since computations are done in ﬂoating point, we use a tolerance factor
tol, and if ∥x(2 : p)∥1 ≤tol, then we return u = 0, which indicates that the
corresponding Householder transformation is the identity. To make sure
that ∥Px∥is as large as possible, we pick uu = x + sign(x1) ∥x∥2 e1, where
sign(z) = 1 if z ≥0 and sign(z) = −1 if z < 0. Note that as a result,
diagonal entries in R may be negative. We will take care of this issue later.
function s = signe(x)
%
if x >= 0, then signe(x) = 1
%
else if x < 0 then signe(x) = -1
%
if x < 0
s = -1;
else
s = 1;
end
end
function [uu, u] = house(x)
% This constructs the unnormalized
vector uu
% defining the Householder reflection that
% zeros all but the first entries in x.
% u is the normalized vector uu/||uu||
%
tol = 2*10^(-15);
% tolerance
uu = x;
p = size(x,1);
% computes l^1-norm of x(2:p,1)
n1 = sum(abs(x(2:p,1)));
if n1 <= tol
u = zeros(p,1);
uu = u;
else
l = sqrt(x'*x);
% l^2 norm of x
uu(1) = x(1) + signe(x(1))*l;
u = uu/sqrt(uu'*uu);
end
end

12.2. QR-Decomposition Using Householder Matrices
477
The Householder transformations are recorded in an array u of n −1
vectors. There are more eﬃcient implementations, but for the sake of clarity
we present the following version.
function [R,
u] = houseqr(A)
%
This function computes the upper triangular R in the QR
%
factorization of A using Householder reflections, and an
%
implicit representation of Q as
a sequence of n - 1
%
vectors u_i representing Householder reflections
n = size(A, 1);
R = A;
u = zeros(n,n-1);
for i = 1:n-1
[~, u(i:n,i)] = house(R(i:n,i));
if u(i:n,i) == zeros(n - i + 1,1)
R(i+1:n,i) = zeros(n - i,1);
else
R(i:n,i:n) = R(i:n,i:n)
- 2*u(i:n,i)*(u(i:n,i)'*R(i:n,i:n));
end
end
end
If only R is desired, then houseqr does the job. In order to obtain R,
we need to compose the Householder transformations. We present a simple
method which is not the most eﬃcient (there is a way to avoid multiplying
explicity the Householder matrices).
The function buildhouse creates a Householder reﬂection from a vec-
tor v.
function P = buildhouse(v,i)
% This function builds a Householder reflection
%
[I 0 ]
%
[0 PP]
%
from a Householder reflection
%
PP = I - 2uu*uu'
%
where uu = v(i:n)
%
If uu = 0 then P - I
%

478
QR-Decomposition for Arbitrary Matrices
n = size(v,1);
if v(i:n) == zeros(n - i + 1,1)
P = eye(n);
else
PP = eye(n - i + 1) - 2*v(i:n)*v(i:n)';
P = [eye(i-1) zeros(i-1, n - i + 1);
zeros(n - i + 1, i - 1) PP];
end
end
The function buildQ builds the matrix Q in the QR-decomposition of
A.
function Q = buildQ(u)
% Builds the matrix Q in the QR decomposition
% of an nxn
matrix A using Householder matrices,
% where u is a representation of the n - 1
% Householder reflection by a list u of vectors produced by
% houseqr
n = size(u,1);
Q = buildhouse(u(:,1),1);
for i = 2:n-1
Q = Q*buildhouse(u(:,i),i);
end
end
The function buildhouseQR computes a QR-factorization of A. At the
end, if some entries on the diagonal of R are negative, it creates a diagonal
orthogonal matrix P such that PR has nonnegative diagonal entries, so
that A = (QP)(PR) is the desired QR-factorization of A.
function [Q,R] = buildhouseQR(A)
%
%
Computes the QR decomposition of a square
%
matrix A (possibly singular) using Householder reflections
n = size(A,1);
[R,u] = houseqr(A);
Q = buildQ(u);
% Produces a matrix R whose diagonal entries are

12.3. Summary
479
% nonnegative
P = eye(n);
for i = 1:n
if R(i,i) < 0
P(i,i) = -1;
end
end
Q = Q*P; R = P*R;
end
Example 12.1. Consider the matrix
A =




1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7



.
Running the function buildhouseQR, we get
Q =




0.1826 0.8165
0.4001
0.3741
0.3651 0.4082 −0.2546 −0.7970
0.5477 −0.0000 −0.6910 0.4717
0.7303 −0.4082 0.5455 −0.0488




and
R =




5.4772 7.3030 9.1287 10.9545
0
0.8165 1.6330 2.4495
0
−0.0000 0.0000 0.0000
0
−0.0000
0
0.0000



.
Observe that A has rank 2. The reader should check that A = QR.
Remark: Curiously, running Matlab built-in function qr, the same R is
obtained (up to column signs) but a diﬀerent Q is obtained (the last two
columns are diﬀerent).
12.3
Summary
The main concepts and results of this chapter are listed below:
• Symmetry (or reﬂection) with respect to F and parallel to G.
• Orthogonal symmetry (or reﬂection) with respect to F and parallel to
G; reﬂections, ﬂips.

480
QR-Decomposition for Arbitrary Matrices
• Hyperplane reﬂections and Householder matrices.
• A key fact about reﬂections (Proposition 12.2).
• QR-decomposition in terms of Householder transformations (Theo-
rem 12.1).
12.4
Problems
Problem 12.1. (1) Given a unit vector (−sin θ, cos θ), prove that the
Householder matrix determined by the vector (−sin θ, cos θ) is
cos 2θ
sin 2θ
sin 2θ −cos 2θ

.
Give a geometric interpretation (i.e., why the choice (−sin θ, cos θ)?).
(2) Given any matrix
A =
a b
c d

,
Prove that there is a Householder matrix H such that AH is lower trian-
gular, i.e.,
AH =
a′ 0
c′ d′

for some a′, c′, d′ ∈R.
Problem 12.2. Given a Euclidean space E of dimension n, if h is a re-
ﬂection about some hyperplane orthogonal to a nonzero vector u and f is
any isometry, prove that f ◦h ◦f −1 is the reﬂection about the hyperplane
orthogonal to f(u).
Problem 12.3. (1) Given a matrix
A =
a b
c d

,
prove that there are Householder matrices G, H such that
GAH =
cos θ
sin θ
sin θ −cos θ
 a b
c d
 cos ϕ
sin ϕ
sin ϕ −cos ϕ

= D,
where D is a diagonal matrix, iﬀthe following equations hold:
(b + c) cos(θ + ϕ) = (a −d) sin(θ + ϕ),
(c −b) cos(θ −ϕ) = (a + d) sin(θ −ϕ).

12.4. Problems
481
(2) Discuss the solvability of the system. Consider the following cases:
Case 1: a −d = a + d = 0.
Case 2a: a −d = b + c = 0, a + d ̸= 0.
Case 2b: a −d = 0, b + c ̸= 0, a + d ̸= 0.
Case 3a: a + d = c −b = 0, a −d ̸= 0.
Case 3b: a + d = 0, c −b ̸= 0, a −d ̸= 0.
Case 4: a + d ̸= 0, a −d ̸= 0. Show that the solution in this case is
θ = 1
2

arctan
 b + c
a −d

+ arctan
 c −b
a + d

,
ϕ = 1
2

arctan
 b + c
a −d

−arctan
 c −b
a + d

.
If b = 0, show that the discussion is simpler: basically, consider c = 0
or c ̸= 0.
(3) Expressing everything in terms of u = cot θ and v = cot ϕ, show
that the equations in (2) become
(b + c)(uv −1) = (u + v)(a −d),
(c −b)(uv + 1) = (−u + v)(a + d).
Problem 12.4. Let A be an n × n real invertible matrix.
(1) Prove that A⊤A is symmetric positive deﬁnite.
(2) Use the Cholesky factorization A⊤A = R⊤R with R upper triangular
with positive diagonal entries to prove that Q = AR−1 is orthogonal, so
that A = QR is the QR-factorization of A.
Problem 12.5. Modify the function houseqr so that it applies to an m×n
matrix with m ≥n, to produce an m × n upper-triangular matrix whose
last m −n rows are zeros.
Problem 12.6. The purpose of this problem is to prove that given any
self-adjoint linear map f : E →E (i.e., such that f ∗= f), where E is a Eu-
clidean space of dimension n ≥3, given an orthonormal basis (e1, . . . , en),
there are n −2 isometries hi, hyperplane reﬂections or the identity, such
that the matrix of
hn−2 ◦· · · ◦h1 ◦f ◦h1 ◦· · · ◦hn−2
is a symmetric tridiagonal matrix.
(1) Prove that for any isometry f : E →E, we have f = f ∗= f −1 iﬀ
f ◦f = id.

482
QR-Decomposition for Arbitrary Matrices
Prove that if f and h are self-adjoint linear maps (f ∗= f and h∗= h),
then h ◦f ◦h is a self-adjoint linear map.
(2) Let Vk be the subspace spanned by (ek+1, . . . , en). Proceed by in-
duction. For the base case, proceed as follows.
Let
f(e1) = a0
1e1 + · · · + a0
nen,
and let
r1, 2 =
a0
2e2 + · · · + a0
nen
 .
Find an isometry h1 (reﬂection or id) such that
h1(f(e1) −a0
1e1) = r1, 2 e2.
Observe that
w1 = r1, 2 e2 + a0
1e1 −f(e1) ∈V1,
and prove that h1(e1) = e1, so that
h1 ◦f ◦h1(e1) = a0
1e1 + r1, 2 e2.
Let f1 = h1 ◦f ◦h1.
Assuming by induction that
fk = hk ◦· · · ◦h1 ◦f ◦h1 ◦· · · ◦hk
has a tridiagonal matrix up to the kth row and column, 1 ≤k ≤n −3, let
fk(ek+1) = ak
kek + ak
k+1ek+1 + · · · + ak
nen,
and let
rk+1, k+2 =
ak
k+2ek+2 + · · · + ak
nen
 .
Find an isometry hk+1 (reﬂection or id) such that
hk+1(fk(ek+1) −ak
kek −ak
k+1ek+1) = rk+1, k+2 ek+2.
Observe that
wk+1 = rk+1, k+2 ek+2 + ak
kek + ak
k+1ek+1 −fk(ek+1) ∈Vk+1,
and prove that hk+1(ek) = ek and hk+1(ek+1) = ek+1, so that
hk+1 ◦fk ◦hk+1(ek+1) = ak
kek + ak
k+1ek+1 + rk+1, k+2 ek+2.
Let fk+1 = hk+1 ◦fk ◦hk+1, and ﬁnish the proof.
(3) Prove that given any symmetric n × n-matrix A, there are n −2
matrices H1, . . . , Hn−2, Householder matrices or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is a symmetric tridiagonal matrix.
(4) Write a computer program implementing the above method.

12.4. Problems
483
Problem 12.7. Recall from Problem 5.6 that an n × n matrix H is upper
Hessenberg if hjk = 0 for all (j, k) such that j −k ≥0. Adapt the proof of
Problem 12.6 to prove that given any n × n-matrix A, there are n −2 ≥1
matrices H1, . . . , Hn−2, Householder matrices or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is upper Hessenberg.
Problem 12.8. The purpose of this problem is to prove that given any
linear map f : E →E, where E is a Euclidean space of dimension n ≥2,
given an orthonormal basis (e1, . . . , en), there are isometries gi, hi, hyper-
plane reﬂections or the identity, such that the matrix of
gn ◦· · · ◦g1 ◦f ◦h1 ◦· · · ◦hn
is a lower bidiagonal matrix, which means that the nonzero entries (if any)
are on the main descending diagonal and on the diagonal below it.
(1) Let U ′
k be the subspace spanned by (e1, . . . , ek) and U ′′
k be the sub-
space spanned by (ek+1, . . . , en), 1 ≤k ≤n −1. Proceed by induction. For
the base case, proceed as follows.
Let v1 = f ∗(e1) and r1, 1 = ∥v1∥. Find an isometry h1 (reﬂection or id)
such that
h1(f ∗(e1)) = r1, 1e1.
Observe that h1(f ∗(e1)) ∈U ′
1, so that
⟨h1(f ∗(e1)), ej⟩= 0
for all j, 2 ≤j ≤n, and conclude that
⟨e1, f ◦h1(ej)⟩= 0
for all j, 2 ≤j ≤n.
Next let
u1 = f ◦h1(e1) = u′
1 + u′′
1,
where u′
1 ∈U ′
1 and u′′
1 ∈U ′′
1 , and let r2, 1 = ∥u′′
1∥. Find an isometry g1
(reﬂection or id) such that
g1(u′′
1) = r2, 1e2.
Show that g1(e1) = e1,
g1 ◦f ◦h1(e1) = u′
1 + r2, 1e2,

484
QR-Decomposition for Arbitrary Matrices
and that
⟨e1, g1 ◦f ◦h1(ej)⟩= 0
for all j, 2 ≤j ≤n. At the end of this stage, show that g1 ◦f ◦h1 has
a matrix such that all entries on its ﬁrst row except perhaps the ﬁrst are
zero, and that all entries on the ﬁrst column, except perhaps the ﬁrst two,
are zero.
Assume by induction that some isometries g1, . . . , gk and h1, . . . , hk have
been found, either reﬂections or the identity, and such that
fk = gk ◦· · · ◦g1 ◦f ◦h1 · · · ◦hk
has a matrix which is lower bidiagonal up to and including row and column
k, where 1 ≤k ≤n −2.
Let
vk+1 = f ∗
k(ek+1) = v′
k+1 + v′′
k+1,
where v′
k+1 ∈U ′
k and v′′
k+1 ∈U ′′
k , and let rk+1, k+1 =
v′′
k+1
. Find an
isometry hk+1 (reﬂection or id) such that
hk+1(v′′
k+1) = rk+1, k+1ek+1.
Show that if hk+1 is a reﬂection, then U ′
k ⊆Hk+1, where Hk+1 is the
hyperplane deﬁning the reﬂection hk+1. Deduce that hk+1(v′
k+1) = v′
k+1,
and that
hk+1(f ∗
k(ek+1)) = v′
k+1 + rk+1, k+1ek+1.
Observe that hk+1(f ∗
k(ek+1)) ∈U ′
k+1, so that
⟨hk+1(f ∗
k(ek+1)), ej⟩= 0
for all j, k + 2 ≤j ≤n, and thus,
⟨ek+1, fk ◦hk+1(ej)⟩= 0
for all j, k + 2 ≤j ≤n.
Next let
uk+1 = fk ◦hk+1(ek+1) = u′
k+1 + u′′
k+1,
where u′
k+1 ∈U ′
k+1 and u′′
k+1 ∈U ′′
k+1, and let rk+2, k+1 =
u′′
k+1
. Find an
isometry gk+1 (reﬂection or id) such that
gk+1(u′′
k+1) = rk+2, k+1ek+2.

12.4. Problems
485
Show that if gk+1 is a reﬂection, then U ′
k+1 ⊆Gk+1, where Gk+1 is the
hyperplane deﬁning the reﬂection gk+1. Deduce that gk+1(ei) = ei for all
i, 1 ≤i ≤k + 1, and that
gk+1 ◦fk ◦hk+1(ek+1) = u′
k+1 + rk+2, k+1ek+2.
Since by induction hypothesis,
⟨ei, fk ◦hk+1(ej)⟩= 0
for all i, j, 1 ≤i ≤k + 1, k + 2 ≤j ≤n, and since gk+1(ei) = ei for all i,
1 ≤i ≤k + 1, conclude that
⟨ei, gk+1 ◦fk ◦hk+1(ej)⟩= 0
for all i, j, 1 ≤i ≤k + 1, k + 2 ≤j ≤n. Finish the proof.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 13
Hermitian Spaces
13.1
Sesquilinear and Hermitian Forms, Pre-Hilbert
Spaces and Hermitian Spaces
In this chapter we generalize the basic results of Euclidean geometry pre-
sented in Chapter 11 to vector spaces over the complex numbers. Such a
generalization is inevitable and not simply a luxury. For example, linear
maps may not have real eigenvalues, but they always have complex eigen-
values. Furthermore, some very important classes of linear maps can be
diagonalized if they are extended to the complexiﬁcation of a real vector
space. This is the case for orthogonal matrices and, more generally, nor-
mal matrices. Also, complex vector spaces are often the natural framework
in physics or engineering, and they are more convenient for dealing with
Fourier series. However, some complications arise due to complex conjuga-
tion.
Recall that for any complex number z ∈C, if z = x+iy where x, y ∈R,
we let ℜz = x, the real part of z, and ℑz = y, the imaginary part of z. We
also denote the conjugate of z = x + iy by z = x −iy, and the absolute
value (or length, or modulus) of z by |z|. Recall that |z|2 = zz = x2 + y2.
There are many natural situations where a map ϕ: E × E →C is
linear in its ﬁrst argument and only semilinear in its second argument,
which means that ϕ(u, µv) = µϕ(u, v), as opposed to ϕ(u, µv) = µϕ(u, v).
For example, the natural inner product to deal with functions f : R →C,
especially Fourier series, is
⟨f, g⟩=
Z π
−π
f(x)g(x)dx,
which is semilinear (but not linear) in g. Thus, when generalizing a result
from the real case of a Euclidean space to the complex case, we always
487

488
Hermitian Spaces
have to check very carefully that our proofs do not rely on linearity in the
second argument. Otherwise, we need to revise our proofs, and sometimes
the result is simply wrong!
Before deﬁning the natural generalization of an inner product, it is
convenient to deﬁne semilinear maps.
Deﬁnition 13.1. Given two vector spaces E and F over the complex ﬁeld
C, a function f : E →F is semilinear if
f(u + v) = f(u) + f(v),
f(λu) = λf(u),
for all u, v ∈E and all λ ∈C.
Remark: Instead of deﬁning semilinear maps, we could have deﬁned the
vector space E as the vector space with the same carrier set E whose
addition is the same as that of E, but whose multiplication by a complex
number is given by
(λ, u) 7→λu.
Then it is easy to check that a function f : E →C is semilinear iﬀf : E →C
is linear.
We can now deﬁne sesquilinear forms and Hermitian forms.
Deﬁnition 13.2. Given a complex vector space E, a function ϕ: E ×E →
C is a sesquilinear form if it is linear in its ﬁrst argument and semilinear
in its second argument, which means that
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v),
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2),
ϕ(λu, v) = λϕ(u, v),
ϕ(u, µv) = µϕ(u, v),
for all u, v, u1, u2, v1, v2 ∈E, and all λ, µ ∈C. A function ϕ: E × E →C
is a Hermitian form if it is sesquilinear and if
ϕ(v, u) = ϕ(u, v)
for all all u, v ∈E.

13.1. Hermitian Spaces, Pre-Hilbert Spaces
489
Obviously, ϕ(0, v) = ϕ(u, 0) = 0. Also note that if ϕ: E × E →C is
sesquilinear, we have
ϕ(λu + µv, λu + µv) = |λ|2ϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + |µ|2ϕ(v, v),
and if ϕ: E × E →C is Hermitian, we have
ϕ(λu + µv, λu + µv) = |λ|2ϕ(u, u) + 2ℜ(λµϕ(u, v)) + |µ|2ϕ(v, v).
Note that restricted to real coeﬃcients, a sesquilinear form is bilinear
(we sometimes say R-bilinear).
Deﬁnition 13.3. Given a sesquilinear form ϕ: E × E →C, the function
Φ: E →C deﬁned such that Φ(u) = ϕ(u, u) for all u ∈E is called the
quadratic form associated with ϕ.
The standard example of a Hermitian form on Cn is the map ϕ deﬁned
such that
ϕ((x1, . . . , xn), (y1, . . . , yn)) = x1y1 + x2y2 + · · · + xnyn.
This map is also positive deﬁnite, but before dealing with these issues, we
show the following useful proposition.
Proposition 13.1. Given a complex vector space E, the following proper-
ties hold:
(1) A sesquilinear form ϕ: E ×E →C is a Hermitian form iﬀϕ(u, u) ∈R
for all u ∈E.
(2) If ϕ: E × E →C is a sesquilinear form, then
4ϕ(u, v) = ϕ(u + v, u + v) −ϕ(u −v, u −v)
+ iϕ(u + iv, u + iv) −iϕ(u −iv, u −iv),
and
2ϕ(u, v) = (1+i)(ϕ(u, u)+ϕ(v, v))−ϕ(u−v, u−v)−iϕ(u−iv, u−iv).
These are called polarization identities.
Proof. (1) If ϕ is a Hermitian form, then
ϕ(v, u) = ϕ(u, v)
implies that
ϕ(u, u) = ϕ(u, u),

490
Hermitian Spaces
and thus ϕ(u, u) ∈R. If ϕ is sesquilinear and ϕ(u, u) ∈R for all u ∈E,
then
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v),
which proves that
ϕ(u, v) + ϕ(v, u) = α,
where α is real, and changing u to iu, we have
i(ϕ(u, v) −ϕ(v, u)) = β,
where β is real, and thus
ϕ(u, v) = α −iβ
2
and
ϕ(v, u) = α + iβ
2
,
proving that ϕ is Hermitian.
(2) These identities are veriﬁed by expanding the right-hand side, and
we leave them as an exercise.
Proposition 13.1 shows that a sesquilinear form is completely deter-
mined by the quadratic form Φ(u) = ϕ(u, u), even if ϕ is not Hermitian.
This is false for a real bilinear form, unless it is symmetric. For example,
the bilinear form ϕ: R2 × R2 →R deﬁned such that
ϕ((x1, y1), (x2, y2)) = x1y2 −x2y1
is not identically zero, and yet it is null on the diagonal. However, a real
symmetric bilinear form is indeed determined by its values on the diagonal,
as we saw in Chapter 11.
As in the Euclidean case, Hermitian forms for which ϕ(u, u) ≥0 play
an important role.
Deﬁnition 13.4. Given a complex vector space E, a Hermitian form
ϕ: E × E →C is positive if ϕ(u, u) ≥0 for all u ∈E, and positive deﬁnite
if ϕ(u, u) > 0 for all u ̸= 0. A pair ⟨E, ϕ⟩where E is a complex vector
space and ϕ is a Hermitian form on E is called a pre-Hilbert space if ϕ is
positive, and a Hermitian (or unitary) space if ϕ is positive deﬁnite.
We warn our readers that some authors, such as Lang [Lang (1996)],
deﬁne a pre-Hilbert space as what we deﬁne as a Hermitian space. We prefer
following the terminology used in Schwartz [Schwartz (1991)] and Bourbaki
[Bourbaki (1981b)]. The quantity ϕ(u, v) is usually called the Hermitian
product of u and v. We will occasionally call it the inner product of u and
v.

13.1. Hermitian Spaces, Pre-Hilbert Spaces
491
Given a pre-Hilbert space ⟨E, ϕ⟩, as in the case of a Euclidean space,
we also denote ϕ(u, v) by
u · v
or
⟨u, v⟩
or
(u|v),
and
p
Φ(u) by ∥u∥.
Example 13.1. The complex vector space Cn under the Hermitian form
ϕ((x1, . . . , xn), (y1, . . . , yn)) = x1y1 + x2y2 + · · · + xnyn
is a Hermitian space.
Example 13.2. Let ℓ2 denote the set of all countably inﬁnite sequences
x = (xi)i∈N of complex numbers such that P∞
i=0 |xi|2 is deﬁned (i.e., the
sequence Pn
i=0 |xi|2 converges as n →∞). It can be shown that the map
ϕ: ℓ2 × ℓ2 →C deﬁned such that
ϕ ((xi)i∈N, (yi)i∈N) =
∞
X
i=0
xiyi
is well deﬁned, and ℓ2 is a Hermitian space under ϕ. Actually, ℓ2 is even a
Hilbert space.
Example 13.3. Let Cpiece[a, b] be the set of bounded piecewise continuous
functions
f : [a, b] →C under the Hermitian form
⟨f, g⟩=
Z b
a
f(x)g(x)dx.
It is easy to check that this Hermitian form is positive, but it is not deﬁnite.
Thus, under this Hermitian form, Cpiece[a, b] is only a pre-Hilbert space.
Example 13.4. Let C[a, b] be the set of complex-valued continuous func-
tions f : [a, b] →C under the Hermitian form
⟨f, g⟩=
Z b
a
f(x)g(x)dx.
It is easy to check that this Hermitian form is positive deﬁnite. Thus, C[a, b]
is a Hermitian space.
Example 13.5. Let E = Mn(C) be the vector space of complex n × n
matrices. If we view a matrix A ∈Mn(C) as a "long" column vector ob-
tained by concatenating together its columns, we can deﬁne the Hermitian
product of two matrices A, B ∈Mn(C) as
⟨A, B⟩=
n
X
i,j=1
aijbij,

492
Hermitian Spaces
which can be conveniently written as
⟨A, B⟩= tr(A⊤B) = tr(B∗A).
Since this can be viewed as the standard Hermitian product on Cn2, it is a
Hermitian product on Mn(C). The corresponding norm
∥A∥F =
p
tr(A∗A)
is the Frobenius norm (see Section 8.2).
If E is ﬁnite-dimensional and if ϕ: E × E →R is a sequilinear form
on E, given any basis (e1, . . . , en) of E, we can write x = Pn
i=1 xiei and
y = Pn
j=1 yjej, and we have
ϕ(x, y) = ϕ
 n
X
i=1
xiei,
n
X
j=1
yjej

=
n
X
i,j=1
xiyjϕ(ei, ej).
If we let G = (gij) be the matrix given by gij = ϕ(ej, ei), and if x and y
are the column vectors associated with (x1, . . . , xn) and (y1, . . . , yn), then
we can write
ϕ(x, y) = x⊤G⊤y = y∗Gx,
where y corresponds to (y1, . . . , yn).
As in Section 11.1, we are com-
mitting the slight abuse of notation of letting x denote both the vector
x = Pn
i=1 xiei and the column vector associated with (x1, . . . , xn) (and
similarly for y). The "correct" expression for ϕ(x, y) is
ϕ(x, y) = y∗Gx = x⊤G⊤y.

Observe that in ϕ(x, y) = y∗Gx, the matrix involved is the trans-
pose of the matrix (ϕ(ei, ej)). The reason for this is that we want
G to be positive deﬁnite when ϕ is positive deﬁnite, not G⊤.
Furthermore, observe that ϕ is Hermitian iﬀG = G∗, and ϕ is positive
deﬁnite iﬀthe matrix G is positive deﬁnite, that is,
(Gx)⊤x = x∗Gx > 0
for all x ∈Cn, x ̸= 0.
Deﬁnition 13.5. The matrix G associated with a Hermitian product is
called the Gram matrix of the Hermitian product with respect to the basis
(e1, . . . , en).

13.1. Hermitian Spaces, Pre-Hilbert Spaces
493
Conversely, if A is a Hermitian positive deﬁnite n × n matrix, it is easy
to check that the Hermitian form
⟨x, y⟩= y∗Ax
is positive deﬁnite. If we make a change of basis from the basis (e1, . . . , en)
to the basis (f1, . . . , fn), and if the change of basis matrix is P (where the
jth column of P consists of the coordinates of fj over the basis (e1, . . . , en)),
then with respect to coordinates x′ and y′ over the basis (f1, . . . , fn), we
have
y∗Gx = (y′)∗P ∗GPx′,
so the matrix of our inner product over the basis (f1, . . . , fn) is P ∗GP. We
summarize these facts in the following proposition.
Proposition 13.2. Let E be a ﬁnite-dimensional vector space, and let
(e1, . . . , en) be a basis of E.
(1) For any Hermitian inner product ⟨−, −⟩on E, if G = (gij) with gij =
⟨ej, ei⟩is the Gram matrix of the Hermitian product ⟨−, −⟩w.r.t. the
basis (e1, . . . , en), then G is Hermitian positive deﬁnite.
(2) For any change of basis matrix P, the Gram matrix of ⟨−, −⟩with
respect to the new basis is P ∗GP.
(3) If A is any n × n Hermitian positive deﬁnite matrix, then
⟨x, y⟩= y∗Ax
is a Hermitian product on E.
We will see later that a Hermitian matrix is positive deﬁnite iﬀits
eigenvalues are all positive.
The following result reminiscent of the ﬁrst polarization identity of
Proposition 13.1 can be used to prove that two linear maps are identical.
Proposition 13.3. Given any Hermitian space E with Hermitian product
⟨−, −⟩, for any linear map f : E →E, if ⟨f(x), x⟩= 0 for all x ∈E, then
f = 0.
Proof. Compute ⟨f(x + y), x + y⟩and ⟨f(x −y), x −y⟩:
⟨f(x + y), x + y⟩= ⟨f(x), x⟩+ ⟨f(x), y⟩+ ⟨f(y), x⟩+ ⟨y, y⟩
⟨f(x −y), x −y⟩= ⟨f(x), x⟩−⟨f(x), y⟩−⟨f(y), x⟩+ ⟨y, y⟩;
then subtract the second equation from the ﬁrst to obtain
⟨f(x + y), x + y⟩−⟨f(x −y), x −y⟩= 2(⟨f(x), y⟩+ ⟨f(y), x⟩).

494
Hermitian Spaces
If ⟨f(u), u⟩= 0 for all u ∈E, we get
⟨f(x), y⟩+ ⟨f(y), x⟩= 0
for all x, y ∈E.
Then the above equation also holds if we replace x by ix, and we obtain
i⟨f(x), y⟩−i⟨f(y), x⟩= 0,
for all x, y ∈E,
so we have
⟨f(x), y⟩+ ⟨f(y), x⟩= 0
⟨f(x), y⟩−⟨f(y), x⟩= 0,
which implies that ⟨f(x), y⟩= 0 for all x, y ∈E. Since ⟨−, −⟩is positive
deﬁnite, we have f(x) = 0 for all x ∈E; that is, f = 0.
One should be careful not to apply Proposition 13.3 to a linear map
on a real Euclidean space because it is false!
The reader should ﬁnd a
counterexample.
The Cauchy-Schwarz inequality and the Minkowski inequalities extend
to pre-Hilbert spaces and to Hermitian spaces.
Proposition 13.4. Let ⟨E, ϕ⟩be a pre-Hilbert space with associated
quadratic form Φ. For all u, v ∈E, we have the Cauchy-Schwarz inequality
|ϕ(u, v)| ≤
p
Φ(u)
p
Φ(v).
Furthermore, if ⟨E, ϕ⟩is a Hermitian space, the equality holds iﬀu and v
are linearly dependent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p
Φ(u) +
p
Φ(v).
Furthermore, if ⟨E, ϕ⟩is a Hermitian space, the equality holds iﬀu and v
are linearly dependent, where in addition, if u ̸= 0 and v ̸= 0, then u = λv
for some real λ such that λ > 0.
Proof. For all u, v ∈E and all µ ∈C, we have observed that
ϕ(u + µv, u + µv) = ϕ(u, u) + 2ℜ(µϕ(u, v)) + |µ|2ϕ(v, v).
Let ϕ(u, v) = ρeiθ, where |ϕ(u, v)| = ρ (ρ ≥0). Let F : R →R be the
function deﬁned such that
F(t) = Φ(u + teiθv),

13.1. Hermitian Spaces, Pre-Hilbert Spaces
495
for all t ∈R. The above shows that
F(t) = ϕ(u, u) + 2t|ϕ(u, v)| + t2ϕ(v, v) = Φ(u) + 2t|ϕ(u, v)| + t2Φ(v).
Since ϕ is assumed to be positive, we have F(t) ≥0 for all t ∈R.
If
Φ(v) = 0, we must have ϕ(u, v) = 0, since otherwise, F(t) could be made
negative by choosing t negative and small enough. If Φ(v) > 0, in order for
F(t) to be nonnegative, the equation
Φ(u) + 2t|ϕ(u, v)| + t2Φ(v) = 0
must not have distinct real roots, which is equivalent to
|ϕ(u, v)|2 ≤Φ(u)Φ(v).
Taking the square root on both sides yields the Cauchy-Schwarz inequality.
For the second part of the claim, if ϕ is positive deﬁnite, we argue as
follows. If u and v are linearly dependent, it is immediately veriﬁed that
we get an equality. Conversely, if
|ϕ(u, v)|2 = Φ(u)Φ(v),
then there are two cases. If Φ(v) = 0, since ϕ is positive deﬁnite, we must
have v = 0, so u and v are linearly dependent. Otherwise, the equation
Φ(u) + 2t|ϕ(u, v)| + t2Φ(v) = 0
has a double root t0, and thus
Φ(u + t0eiθv) = 0.
Since ϕ is positive deﬁnite, we must have
u + t0eiθv = 0,
which shows that u and v are linearly dependent.
If we square the Minkowski inequality, we get
Φ(u + v) ≤Φ(u) + Φ(v) + 2
p
Φ(u)
p
Φ(v).
However, we observed earlier that
Φ(u + v) = Φ(u) + Φ(v) + 2ℜ(ϕ(u, v)).
Thus, it is enough to prove that
ℜ(ϕ(u, v)) ≤
p
Φ(u)
p
Φ(v),
but this follows from the Cauchy-Schwarz inequality
|ϕ(u, v)| ≤
p
Φ(u)
p
Φ(v)

496
Hermitian Spaces
and the fact that ℜz ≤|z|.
If ϕ is positive deﬁnite and u and v are linearly dependent, it is imme-
diately veriﬁed that we get an equality. Conversely, if equality holds in the
Minkowski inequality, we must have
ℜ(ϕ(u, v)) =
p
Φ(u)
p
Φ(v),
which implies that
|ϕ(u, v)| =
p
Φ(u)
p
Φ(v),
since otherwise, by the Cauchy-Schwarz inequality, we would have
ℜ(ϕ(u, v)) ≤|ϕ(u, v)| <
p
Φ(u)
p
Φ(v).
Thus, equality holds in the Cauchy-Schwarz inequality, and
ℜ(ϕ(u, v)) = |ϕ(u, v)|.
But then we proved in the Cauchy-Schwarz case that u and v are linearly
dependent. Since we also just proved that ϕ(u, v) is real and nonnegative,
the coeﬃcient of proportionality between u and v is indeed nonnegative.
As in the Euclidean case, if ⟨E, ϕ⟩is a Hermitian space, the Minkowski
inequality
p
Φ(u + v) ≤
p
Φ(u) +
p
Φ(v)
shows that the map u 7→
p
Φ(u) is a norm on E. The norm induced by ϕ
is called the Hermitian norm induced by ϕ. We usually denote
p
Φ(u) by
∥u∥, and the Cauchy-Schwarz inequality is written as
|u · v| ≤∥u∥∥v∥.
Since a Hermitian space is a normed vector space, it is a topological
space under the topology induced by the norm (a basis for this topology is
given by the open balls B0(u, ρ) of center u and radius ρ > 0, where
B0(u, ρ) = {v ∈E | ∥v −u∥< ρ}.
If E has ﬁnite dimension, every linear map is continuous; see Chapter 8 (or
Lang [Lang (1996, 1997)], Dixmier [Dixmier (1984)], or Schwartz [Schwartz
(1991, 1992)]). The Cauchy-Schwarz inequality
|u · v| ≤∥u∥∥v∥
shows that ϕ: E × E →C is continuous, and thus, that ∥∥is continuous.

13.2. Orthogonality, Duality, Adjoint of a Linear Map
497
If ⟨E, ϕ⟩is only pre-Hilbertian, ∥u∥is called a seminorm. In this case,
the condition
∥u∥= 0
implies
u = 0
is not necessarily true. However, the Cauchy-Schwarz inequality shows that
if ∥u∥= 0, then u · v = 0 for all v ∈E.
Remark: As in the case of real vector spaces, a norm on a complex vector
space is induced by some positive deﬁnite Hermitian product ⟨−, −⟩iﬀit
satisﬁes the parallelogram law:
∥u + v∥2 + ∥u −v∥2 = 2(∥u∥2 + ∥v∥2).
This time the Hermitian product is recovered using the polarization identity
from Proposition 13.1:
4⟨u, v⟩= ∥u + v∥2 −∥u −v∥2 + i ∥u + iv∥2 −i ∥u −iv∥2 .
It is easy to check that ⟨u, u⟩= ∥u∥2, and
⟨v, u⟩= ⟨u, v⟩
⟨iu, v⟩= i⟨u, v⟩,
so it is enough to check linearity in the variable u, and only for real scalars.
This is easily done by applying the proof from Section 11.1 to the real and
imaginary part of ⟨u, v⟩; the details are left as an exercise.
We will now basically mirror the presentation of Euclidean geometry
given in Chapter 11 rather quickly, leaving out most proofs, except when
they need to be seriously amended.
13.2
Orthogonality, Duality, Adjoint of a Linear Map
In this section we assume that we are dealing with Hermitian spaces. We
denote the Hermitian inner product by u · v or ⟨u, v⟩.
The concepts of
orthogonality, orthogonal family of vectors, orthonormal family of vectors,
and orthogonal complement of a set of vectors are unchanged from the
Euclidean case (Deﬁnition 11.2).
For example, the set C[−π, π] of continuous functions f : [−π, π] →C is
a Hermitian space under the product
⟨f, g⟩=
Z π
−π
f(x)g(x)dx,
and the family (eikx)k∈Z is orthogonal.

498
Hermitian Spaces
Propositions 11.4 and 11.5 hold without any changes. It is easy to show
that

n
X
i=1
ui

2
=
n
X
i=1
∥ui∥2 +
X
1≤i<j≤n
2ℜ(ui · uj).
Analogously to the case of Euclidean spaces of ﬁnite dimension, the
Hermitian product induces a canonical bijection (i.e., independent of the
choice of bases) between the vector space E and the space E∗. This is one
of the places where conjugation shows up, but in this case, troubles are
minor.
Given a Hermitian space E, for any vector u ∈E, let ϕl
u : E →C be
the map deﬁned such that
ϕl
u(v) = u · v,
for all v ∈E.
Similarly, for any vector v ∈E, let ϕr
v : E →C be the map deﬁned such
that
ϕr
v(u) = u · v,
for all u ∈E.
Since the Hermitian product is linear in its ﬁrst argument u, the map
ϕr
v is a linear form in E∗, and since it is semilinear in its second argument v,
the map ϕl
u is also a linear form in E∗. Thus, we have two maps ♭l : E →E∗
and ♭r : E →E∗, deﬁned such that
♭l(u) = ϕl
u,
and
♭r(v) = ϕr
v.
Proposition 13.5. The equations ϕl
u = ϕr
u and ♭l = ♭r hold.
Proof. Indeed, for all u, v ∈E, we have
♭l(u)(v) = ϕl
u(v)
= u · v
= v · u
= ϕr
u(v)
= ♭r(u)(v).
Therefore, we use the notation ϕu for both ϕl
u and ϕr
u, and ♭for both ♭l
and ♭r.
Theorem 13.1. Let E be a Hermitian space E.
The map ♭: E →E∗
deﬁned such that
♭(u) = ϕl
u = ϕr
u
for all u ∈E
is semilinear and injective. When E is also of ﬁnite dimension, the map
♭: E →E∗is a canonical isomorphism.

13.2. Orthogonality, Duality, Adjoint of a Linear Map
499
Proof. That ♭: E →E∗is a semilinear map follows immediately from the
fact that ♭= ♭r, and that the Hermitian product is semilinear in its second
argument.
If ϕu = ϕv, then ϕu(w) = ϕv(w) for all w ∈E, which by
deﬁnition of ϕu and ϕv means that
w · u = w · v
for all w ∈E, which by semilinearity on the right is equivalent to
w · (v −u) = 0
for all w ∈E,
which implies that u = v, since the Hermitian product is positive deﬁnite.
Thus, ♭: E →E∗is injective. Finally, when E is of ﬁnite dimension n, E∗
is also of dimension n, and then ♭: E →E∗is bijective. Since ♭is semilinar,
the map ♭: E →E∗is an isomorphism.
The inverse of the isomorphism ♭: E →E∗is denoted by ♯: E∗→E.
As a corollary of the isomorphism ♭: E →E∗we have the following
result.
Proposition 13.6. If E is a Hermitian space of ﬁnite dimension, then
every linear form f ∈E∗corresponds to a unique v ∈E, such that
f(u) = u · v,
for every u ∈E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane
H, is precisely the set of vectors that are orthogonal to v.
Remarks:
(1) The "musical map" ♭: E →E∗is not surjective when E has inﬁnite
dimension. This result can be salvaged by restricting our attention to
continuous linear maps and by assuming that the vector space E is a
Hilbert space.
(2) Dirac's "bra-ket" notation. Dirac invented a notation widely used in
quantum mechanics for denoting the linear form ϕu = ♭(u) associated
to the vector u ∈E via the duality induced by a Hermitian inner
product. Dirac's proposal is to denote the vectors u in E by |u⟩, and
call them kets; the notation |u⟩is pronounced "ket u." Given two kets
(vectors) |u⟩and |v⟩, their inner product is denoted by
⟨u|v⟩
(instead of |u⟩· |v⟩). The notation ⟨u|v⟩for the inner product of |u⟩
and |v⟩anticipates duality. Indeed, we deﬁne the dual (usually called

500
Hermitian Spaces
adjoint) bra u of ket u, denoted by ⟨u|, as the linear form whose value
on any ket v is given by the inner product, so
⟨u|(|v⟩) = ⟨u|v⟩.
Thus, bra u = ⟨u| is Dirac's notation for our ♭(u). Since the map ♭is
semi-linear, we have
⟨λu| = λ⟨u|.
Using the bra-ket notation, given an orthonormal basis (|u1⟩, . . . , |un⟩),
ket v (a vector) is written as
|v⟩=
n
X
i=1
⟨v|ui⟩|ui⟩,
and the corresponding linear form bra v is written as
⟨v| =
n
X
i=1
⟨v|ui⟩⟨ui| =
n
X
i=1
⟨ui|v⟩⟨ui|
over the dual basis (⟨u1|, . . . , ⟨un|).
As cute as it looks, we do not
recommend using the Dirac notation.
The existence of the isomorphism ♭: E →E∗is crucial to the existence
of adjoint maps. Indeed, Theorem 13.1 allows us to deﬁne the adjoint of
a linear map on a Hermitian space. Let E be a Hermitian space of ﬁnite
dimension n, and let f : E →E be a linear map. For every u ∈E, the map
v 7→u · f(v)
is clearly a linear form in E∗, and by Theorem 13.1, there is a unique vector
in E denoted by f ∗(u), such that
f ∗(u) · v = u · f(v),
that is,
f ∗(u) · v = u · f(v),
for every v ∈E.
The following proposition shows that the map f ∗is linear.
Proposition 13.7. Given a Hermitian space E of ﬁnite dimension, for
every linear map f : E →E there is a unique linear map f ∗: E →E such
that
f ∗(u) · v = u · f(v),
for all u, v ∈E.

13.2. Orthogonality, Duality, Adjoint of a Linear Map
501
Proof. Careful inspection of the proof of Proposition 11.6 reveals that it
applies unchanged. The only potential problem is in proving that f ∗(λu) =
λf ∗(u), but everything takes place in the ﬁrst argument of the Hermitian
product, and there, we have linearity.
Deﬁnition 13.6. Given a Hermitian space E of ﬁnite dimension, for every
linear map f : E →E, the unique linear map f ∗: E →E such that
f ∗(u) · v = u · f(v),
for all u, v ∈E
given by Proposition 13.7 is called the adjoint of f (w.r.t. to the Hermitian
product).
The fact that
v · u = u · v
implies that the adjoint f ∗of f is also characterized by
f(u) · v = u · f ∗(v),
for all u, v ∈E.
Given two Hermitian spaces E and F, where the Hermitian product on
E is denoted by ⟨−, −⟩1 and the Hermitian product on F is denoted by
⟨−, −⟩2, given any linear map f : E →F, it is immediately veriﬁed that
the proof of Proposition 13.7 can be adapted to show that there is a unique
linear map f ∗: F →E such that
⟨f(u), v⟩2 = ⟨u, f ∗(v)⟩1
for all u ∈E and all v ∈F. The linear map f ∗is also called the adjoint of
f.
As in the Euclidean case, the following properties immediately follow
from the deﬁnition of the adjoint map.
Proposition 13.8.
(1) For any linear map f : E →F, we have
f ∗∗= f.
(2) For any two linear maps f, g: E →F and any scalar λ ∈R:
(f + g)∗= f ∗+ g∗
(λf)∗= λf ∗.

502
Hermitian Spaces
(3) If
E, F, G
are
Hermitian
spaces
with
respective
inner
products
⟨−, −⟩1, ⟨−, −⟩2, and ⟨−, −⟩3, and if f : E →F and g: F →G are
two linear maps, then
(g ◦f)∗= f ∗◦g∗.
As in the Euclidean case, a linear map f : E →E (where E is a ﬁnite-
dimensional Hermitian space) is self-adjoint if f = f ∗.
The map f is
positive semideﬁnite iﬀ
⟨f(x), x⟩≥0
all x ∈E;
positive deﬁnite iﬀ
⟨f(x), x⟩> 0
all x ∈E, x ̸= 0.
An interesting corollary of Proposition 13.3 is that a positive semideﬁnite
linear map must be self-adjoint.
In fact, we can prove a slightly more
general result.
Proposition 13.9. Given any ﬁnite-dimensional Hermitian space E with
Hermitian product ⟨−, −⟩, for any linear map f : E →E, if ⟨f(x), x⟩∈R
for all x ∈E, then f is self-adjoint. In particular, any positive semideﬁnite
linear map f : E →E is self-adjoint.
Proof. Since ⟨f(x), x⟩∈R for all x ∈E, we have
⟨f(x), x⟩= ⟨f(x), x⟩
= ⟨x, f(x)⟩
= ⟨f ∗(x), x⟩,
so we have
⟨(f −f ∗)(x), x⟩= 0
all x ∈E,
and Proposition 13.3 implies that f −f ∗= 0.
Beware that Proposition 13.9 is false if E is a real Euclidean space.
As in the Euclidean case, Theorem 13.1 can be used to show that any
Hermitian space of ﬁnite dimension has an orthonormal basis. The proof
is unchanged.
Proposition 13.10. Given any nontrivial Hermitian space E of ﬁnite di-
mension n ≥1, there is an orthonormal basis (u1, . . . , un) for E.

13.2. Orthogonality, Duality, Adjoint of a Linear Map
503
The Gram-Schmidt orthonormalization procedure also applies to Her-
mitian spaces of ﬁnite dimension, without any changes from the Euclidean
case!
Proposition 13.11. Given a nontrivial Hermitian space E of ﬁnite di-
mension n ≥1, from any basis (e1, . . . , en) for E we can construct an
orthonormal basis (u1, . . . , un) for E with the property that for every k,
1 ≤k ≤n, the families (e1, . . . , ek) and (u1, . . . , uk) generate the same
subspace.
Remark: The remarks made after Proposition 11.8 also apply here, except
that in the QR-decomposition, Q is a unitary matrix.
As a consequence of Proposition 11.7 (or Proposition 13.11), given any
Hermitian space of ﬁnite dimension n, if (e1, . . . , en) is an orthonormal
basis for E, then for any two vectors u = u1e1 + · · · + unen and v =
v1e1 + · · · + vnen, the Hermitian product u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
n
X
i=1
uivi,
and the norm ∥u∥as
∥u∥= ∥u1e1 + · · · + unen∥=

n
X
i=1
|ui|2
1/2
.
The fact that a Hermitian space always has an orthonormal basis implies
that any Gram matrix G can be written as
G = Q∗Q,
for some invertible matrix Q. Indeed, we know that in a change of basis
matrix, a Gram matrix G becomes G′ = P ∗GP. If the basis corresponding
to G′ is orthonormal, then G′ = I, so G = (P −1)∗P −1.
Proposition 11.9 also holds unchanged.
Proposition 13.12. Given any nontrivial Hermitian space E of ﬁnite di-
mension n ≥1, for any subspace F of dimension k, the orthogonal comple-
ment F ⊥of F has dimension n −k, and E = F ⊕F ⊥. Furthermore, we
have F ⊥⊥= F.

504
Hermitian Spaces
13.3
Linear Isometries (Also Called Unitary
Transformations)
In this section we consider linear maps between Hermitian spaces that pre-
serve the Hermitian norm. All deﬁnitions given for Euclidean spaces in
Section 11.5 extend to Hermitian spaces, except that orthogonal transfor-
mations are called unitary transformation, but Proposition 11.10 extends
only with a modiﬁed Condition (2). Indeed, the old proof that (2) implies
(3) does not work, and the implication is in fact false! It can be repaired
by strengthening Condition (2). For the sake of completeness, we state the
Hermitian version of Deﬁnition 11.5.
Deﬁnition 13.7. Given any two nontrivial Hermitian spaces E and F of
the same ﬁnite dimension n, a function f : E →F is a unitary transforma-
tion, or a linear isometry, if it is linear and
∥f(u)∥= ∥u∥,
for all u ∈E.
Proposition 11.10 can be salvaged by strengthening Condition (2).
Proposition 13.13. Given any two nontrivial Hermitian spaces E and F
of the same ﬁnite dimension n, for every function f : E →F, the following
properties are equivalent:
(1) f is a linear map and ∥f(u)∥= ∥u∥, for all u ∈E;
(2) ∥f(v) −f(u)∥= ∥v −u∥and f(iu) = if(u), for all u, v ∈E.
(3) f(u) · f(v) = u · v, for all u, v ∈E.
Furthermore, such a map is bijective.
Proof. The proof that (2) implies (3) given in Proposition 11.10 needs to
be revised as follows. We use the polarization identity
2ϕ(u, v) = (1 + i)(∥u∥2 + ∥v∥2) −∥u −v∥2 −i∥u −iv∥2.
Since f(iv) = if(v), we get f(0) = 0 by setting v = 0, so the function f
preserves distance and norm, and we get
2ϕ(f(u), f(v)) = (1 + i)(∥f(u)∥2 + ∥f(v)∥2) −∥f(u) −f(v)∥2
−i∥f(u) −if(v)∥2
= (1 + i)(∥f(u)∥2 + ∥f(v)∥2) −∥f(u) −f(v)∥2
−i∥f(u) −f(iv)∥2
= (1 + i)(∥u∥2 + ∥v∥2) −∥u −v∥2 −i∥u −iv∥2
= 2ϕ(u, v),

13.4. The Unitary Group, Unitary Matrices
505
which shows that f preserves the Hermitian inner product as desired. The
rest of the proof is unchanged.
Remarks:
(i) In the Euclidean case, we proved that the assumption
∥f(v) −f(u)∥= ∥v −u∥
for all u, v ∈E and f(0) = 0
(13.1)
implies (3). For this we used the polarization identity
2u · v = ∥u∥2 + ∥v∥2 −∥u −v∥2.
In the Hermitian case the polarization identity involves the complex
number i.
In fact, the implication (13.1) implies (3) is false in the
Hermitian case! Conjugation z 7→z satisﬁes (2′) since
|z2 −z1| = |z2 −z1| = |z2 −z1|,
and yet, it is not linear!
(ii) If we modify (2) by changing the second condition by now requiring
that there be some τ ∈E such that
f(τ + iu) = f(τ) + i(f(τ + u) −f(τ))
for all u ∈E, then the function g: E →E deﬁned such that
g(u) = f(τ + u) −f(τ)
satisﬁes the old conditions of (2), and the implications (2) →(3) and
(3) →(1) prove that g is linear, and thus that f is aﬃne. In view of
the ﬁrst remark, some condition involving i is needed on f, in addition
to the fact that f is distance-preserving.
13.4
The Unitary Group, Unitary Matrices
In this section, as a mirror image of our treatment of the isometries of a Eu-
clidean space, we explore some of the fundamental properties of the unitary
group and of unitary matrices. As an immediate corollary of the Gram-
Schmidt orthonormalization procedure, we obtain the QR-decomposition
for invertible matrices.
In the Hermitian framework, the matrix of the adjoint of a linear map
is not given by the transpose of the original matrix, but by its conjugate.
Deﬁnition 13.8. Given a complex m × n matrix A, the transpose A⊤of
A is the n × m matrix A⊤=
 a⊤
i j

deﬁned such that
a⊤
i j = aj i,

506
Hermitian Spaces
and the conjugate A of A is the m × n matrix A = (bi j) deﬁned such that
bi j = ai j
for all i, j, 1 ≤i ≤m, 1 ≤j ≤n. The adjoint A∗of A is the matrix deﬁned
such that
A∗= (A⊤) =
 A
⊤.
Proposition 13.14. Let E be any Hermitian space of ﬁnite dimension n,
and let f : E →E be any linear map. The following properties hold:
(1) The linear map f : E →E is an isometry iﬀ
f ◦f ∗= f ∗◦f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A,
then the matrix of f ∗is the adjoint A∗of A, and f is an isometry iﬀ
A satisﬁes the identities
A A∗= A∗A = In,
where In denotes the identity matrix of order n, iﬀthe columns of A
form an orthonormal basis of Cn, iﬀthe rows of A form an orthonormal
basis of Cn.
Proof. (1) The proof is identical to that of Proposition 11.12 (1).
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j) be the
matrix of f, and let B = (bi j) be the matrix of f ∗. Since f ∗is characterized
by
f ∗(u) · v = u · f(v)
for all u, v ∈E, using the fact that if w = w1e1 + · · · + wnen, we have
wk = w · ek, for all k, 1 ≤k ≤n; letting u = ei and v = ej, we get
bj i = f ∗(ei) · ej = ei · f(ej) = f(ej) · ei = ai j,
for all i, j, 1 ≤i, j ≤n. Thus, B = A∗. Now if X and Y are arbitrary
matrices over the basis (e1, . . . , en), denoting as usual the jth column of X
by Xj, and similarly for Y , a simple calculation shows that
Y ∗X = (Xj · Y i)1≤i,j≤n.
Then it is immediately veriﬁed that if X = Y = A, then A∗A = A A∗= In
iﬀthe column vectors (A1, . . . , An) form an orthonormal basis. Thus, from
(1), we see that (2) is clear.

13.4. The Unitary Group, Unitary Matrices
507
Proposition 11.12 shows that the inverse of an isometry f is its adjoint
f ∗. Proposition 11.12 also motivates the following deﬁnition.
Deﬁnition 13.9. A complex n × n matrix is a unitary matrix if
A A∗= A∗A = In.
Remarks:
(1) The conditions A A∗= In, A∗A = In, and A−1 = A∗are equivalent.
Given any two orthonormal bases (u1, . . . , un) and (v1, . . . , vn), if P is
the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn), it is easy to
show that the matrix P is unitary. The proof of Proposition 13.13 (3)
also shows that if f is an isometry, then the image of an orthonormal
basis (u1, . . . , un) is an orthonormal basis.
(2) Using the explicit formula for the determinant, we see immediately that
det(A) = det(A).
If f is a unitary transformation and A is its matrix with respect to any
orthonormal basis, from AA∗= I, we get
det(AA∗) = det(A) det(A∗) = det(A)det(A⊤)
= det(A)det(A) = | det(A)|2,
and so | det(A)| = 1. It is clear that the isometries of a Hermitian space
of dimension n form a group, and that the isometries of determinant
+1 form a subgroup.
This leads to the following deﬁnition.
Deﬁnition 13.10. Given a Hermitian space E of dimension n, the set of
isometries f : E →E forms a subgroup of GL(E, C) denoted by U(E), or
U(n) when E = Cn, called the unitary group (of E). For every isometry
f we have | det(f)| = 1, where det(f) denotes the determinant of f. The
isometries such that det(f) = 1 are called rotations, or proper isometries,
or proper unitary transformations, and they form a subgroup of the spe-
cial linear group SL(E, C) (and of U(E)), denoted by SU(E), or SU(n)
when E = Cn, called the special unitary group (of E).
The isometries
such that det(f) ̸= 1 are called improper isometries, or improper unitary
transformations, or ﬂip transformations.

508
Hermitian Spaces
A very important example of unitary matrices is provided by Fourier
matrices (up to a factor of √n), matrices that arise in the various versions
of the discrete Fourier transform. For more on this topic, see the problems,
and Strang [Strang (1986); Strang and Truong (1997)].
The group SU(2) turns out to be the group of unit quaternions, invented
by Hamilton. This group plays an important role in the representation of
rotations in SO(3) used in computer graphics and robotics; see Chapter 15.
Now that we have the deﬁnition of a unitary matrix, we can explain
how the Gram-Schmidt orthonormalization procedure immediately yields
the QR-decomposition for matrices.
Deﬁnition 13.11. Given any complex n×n matrix A, a QR-decomposition
of A is any pair of n × n matrices (U, R), where U is a unitary matrix and
R is an upper triangular matrix such that A = UR.
Proposition 13.15. Given any n×n complex matrix A, if A is invertible,
then there is a unitary matrix U and an upper triangular matrix R with
positive diagonal entries such that A = UR.
The proof is absolutely the same as in the real case!
Remark: If A is invertible and if A = U1R1 = U2R2 are two QR-
decompositions for A, then
R1R−1
2
= U ∗
1 U2.
Then it is easy to show that there is a diagonal matrix D with diagonal
entries such that |dii| = 1 for i = 1, . . . , n, and U2 = U1D, R2 = D∗R1.
We have the following version of the Hadamard inequality for complex
matrices. The proof is essentially the same as in the Euclidean case but it
uses Proposition 13.15 instead of Proposition 11.14.
Proposition 13.16. (Hadamard) For any complex n×n matrix A = (aij),
we have
| det(A)| ≤
n
Y
i=1
 n
X
j=1
|aij|2
1/2
and
| det(A)| ≤
n
Y
j=1
 n
X
i=1
|aij|2
1/2
.
Moreover, equality holds iﬀeither A has a zero row in the left inequality or
a zero column in the right inequality, or A is unitary.
We also have the following version of Proposition 11.16 for Hermitian
matrices. The proof of Proposition 11.16 goes through because the Cholesky

13.5. Hermitian Reﬂections and QR-Decomposition
509
decomposition for a Hermitian positive deﬁnite A matrix holds in the form
A = B∗B, where B is upper triangular with positive diagonal entries. The
details are left to the reader.
Proposition 13.17. (Hadamard) For any complex n×n matrix A = (aij),
if A is Hermitian positive semideﬁnite, then we have
det(A) ≤
n
Y
i=1
aii.
Moreover, if A is positive deﬁnite, then equality holds iﬀA is a diagonal
matrix.
13.5
Hermitian Reﬂections and QR-Decomposition
If A is an n × n complex singular matrix, there is some (not necessarily
unique) QR-decomposition A = QR with Q a unitary matrix which is
a product of Householder reﬂections and R an upper triangular matrix,
but the proof is more involved. One way to proceed is to generalize the
notion of hyperplane reﬂection. This is not really surprising since in the
Hermitian case there are improper isometries whose determinant can be any
unit complex number. Hyperplane reﬂections are generalized as follows.
Deﬁnition 13.12. Let E be a Hermitian space of ﬁnite dimension. For
any hyperplane H, for any nonnull vector w orthogonal to H, so that
E = H ⊕G, where G = Cw, a Hermitian reﬂection about H of angle θ is a
linear map of the form ρH, θ : E →E, deﬁned such that
ρH, θ(u) = pH(u) + eiθpG(u),
for any unit complex number eiθ ̸= 1 (i.e. θ ̸= k2π). For any nonzero vector
w ∈E, we denote by ρw,θ the Hermitian reﬂection given by ρH,θ, where H
is the hyperplane orthogonal to w.
Since u = pH(u)+pG(u), the Hermitian reﬂection ρw, θ is also expressed
as
ρw, θ(u) = u + (eiθ −1)pG(u),
or as
ρw, θ(u) = u + (eiθ −1) (u · w)
∥w∥2 w.

510
Hermitian Spaces
Note that the case of a standard hyperplane reﬂection is obtained when
eiθ = −1, i.e., θ = π. In this case,
ρw, π(u) = u −2 (u · w)
∥w∥2 w,
and the matrix of such a reﬂection is a Householder matrix, as in Sec-
tion 12.1, except that w may be a complex vector.
We leave as an easy exercise to check that ρw, θ is indeed an isometry,
and that the inverse of ρw, θ is ρw, −θ.
If we pick an orthonormal basis
(e1, . . . , en) such that (e1, . . . , en−1) is an orthonormal basis of H, the ma-
trix of ρw, θ is
In−1 0
0
eiθ

We now come to the main surprise. Given any two distinct vectors u and
v such that ∥u∥= ∥v∥, there isn't always a hyperplane reﬂection mapping
u to v, but this can be done using two Hermitian reﬂections!
Proposition 13.18. Let E be any nontrivial Hermitian space.
(1) For any two vectors u, v ∈E such that u ̸= v and ∥u∥= ∥v∥, if
u · v = eiθ|u · v|, then the (usual) reﬂection s about the hyperplane
orthogonal to the vector v −e−iθu is such that s(u) = eiθv.
(2) For any nonnull vector v ∈E, for any unit complex number eiθ ̸= 1,
there is a Hermitian reﬂection ρv,θ such that
ρv,θ(v) = eiθv.
As a consequence, for u and v as in (1), we have ρv,−θ ◦s(u) = v.
Proof. (1) Consider the (usual) reﬂection about the hyperplane orthogonal
to w = v −e−iθu. We have
s(u) = u −2 (u · (v −e−iθu))
∥v −e−iθu∥2
(v −e−iθu).
We need to compute
−2u · (v −e−iθu)
and
(v −e−iθu) · (v −e−iθu).
Since u · v = eiθ|u · v|, we have
e−iθu · v = |u · v|
and
eiθv · u = |u · v|.
Using the above and the fact that ∥u∥= ∥v∥, we get
−2u · (v −e−iθu) = 2eiθ ∥u∥2 −2u · v,
= 2eiθ(∥u∥2 −|u · v|),

13.5. Hermitian Reﬂections and QR-Decomposition
511
and
(v −e−iθu) · (v −e−iθu) = ∥v∥2 + ∥u∥2 −e−iθu · v −eiθv · u,
= 2(∥u∥2 −|u · v|),
and thus,
−2 (u · (v −e−iθu))
∥(v −e−iθu)∥2 (v −e−iθu) = eiθ(v −e−iθu).
But then,
s(u) = u + eiθ(v −e−iθu) = u + eiθv −u = eiθv,
and s(u) = eiθv, as claimed.
(2) This part is easier. Consider the Hermitian reﬂection
ρv,θ(u) = u + (eiθ −1) (u · v)
∥v∥2 v.
We have
ρv,θ(v) = v + (eiθ −1) (v · v)
∥v∥2 v,
= v + (eiθ −1)v,
= eiθv.
Thus, ρv,θ(v) = eiθv. Since ρv,θ is linear, changing the argument v to eiθv,
we get
ρv,−θ(eiθv) = v,
and thus, ρv,−θ ◦s(u) = v.
Remarks:
(1) If we use the vector v+e−iθu instead of v−e−iθu, we get s(u) = −eiθv.
(2) Certain authors, such as Kincaid and Cheney [Kincaid and Cheney
(1996)] and Ciarlet [Ciarlet (1989)], use the vector u + eiθv instead of
the vector v + e−iθu. The eﬀect of this choice is that they also get
s(u) = −eiθv.
(3) If v = ∥u∥e1, where e1 is a basis vector, u·e1 = a1, where a1 is just the
coeﬃcient of u over the basis vector e1. Then, since u·e1 = eiθ|a1|, the
choice of the plus sign in the vector ∥u∥e1 + e−iθu has the eﬀect that
the coeﬃcient of this vector over e1 is ∥u∥+ |a1|, and no cancellations
takes place, which is preferable for numerical stability (we need to di-
vide by the square norm of this vector).

512
Hermitian Spaces
We now show that the QR-decomposition in terms of (complex) House-
holder matrices holds for complex matrices. We need the version of Propo-
sition 13.18 and a trick at the end of the argument, but the proof is basically
unchanged.
Proposition 13.19. Let E be a nontrivial Hermitian space of dimension
n.
Given any orthonormal basis (e1, . . . , en), for any n-tuple of vectors
(v1, . . . , vn), there is a sequence of n −1 isometries h1, . . . , hn−1, such that
hi is a (standard) hyperplane reﬂection or the identity, and if (r1, . . . , rn)
are the vectors given by
rj = hn−1 ◦· · · ◦h2 ◦h1(vj),
1 ≤j ≤n,
then every rj is a linear combination of the vectors (e1, . . . , ej), (1 ≤j ≤n).
Equivalently, the matrix R whose columns are the components of the rj
over the basis (e1, . . . , en) is an upper triangular matrix. Furthermore, if
we allow one more isometry hn of the form
hn = ρen, ϕn ◦· · · ◦ρe1,ϕ1
after h1, . . . , hn−1, we can ensure that the diagonal entries of R are non-
negative.
Proof. The proof is very similar to the proof of Proposition 12.3, but it
needs to be modiﬁed a little bit since Proposition 13.18 is weaker than
Proposition 12.2. We explain how to modify the induction step, leaving the
base case and the rest of the proof as an exercise.
As in the proof of Proposition 12.3, the vectors (e1, . . . , ek) form a basis
for the subspace denoted as U ′
k, the vectors (ek+1, . . . , en) form a basis for
the subspace denoted as U ′′
k , the subspaces U ′
k and U ′′
k are orthogonal, and
E = U ′
k ⊕U ′′
k . Let
uk+1 = hk ◦· · · ◦h2 ◦h1(vk+1).
We can write
uk+1 = u′
k+1 + u′′
k+1,
where u′
k+1 ∈U ′
k and u′′
k+1 ∈U ′′
k . Let
rk+1,k+1 =
u′′
k+1
 ,
and
eiθk+1|u′′
k+1 · ek+1| = u′′
k+1 · ek+1.
If u′′
k+1 = eiθk+1rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, by Proposi-
tion 13.18(1) (with u = u′′
k+1 and v = rk+1,k+1 ek+1), there is a unique
hyperplane reﬂection hk+1 such that
hk+1(u′′
k+1) = eiθk+1rk+1,k+1 ek+1,

13.5. Hermitian Reﬂections and QR-Decomposition
513
where hk+1 is the reﬂection about the hyperplane Hk+1 orthogonal to the
vector
wk+1 = rk+1,k+1 ek+1 −e−iθk+1u′′
k+1.
At the end of the induction, we have a triangular matrix R, but the
diagonal entries eiθjrj, j of R may be complex. Letting
hn = ρen, −θn ◦· · · ◦ρe1,−θ1,
we observe that the diagonal entries of the matrix of vectors
r′
j = hn ◦hn−1 ◦· · · ◦h2 ◦h1(vj)
is triangular with nonnegative entries.
Remark: For numerical stability,
it is preferable to use wk+1
=
rk+1,k+1 ek+1 +e−iθk+1u′′
k+1 instead of wk+1 = rk+1,k+1 ek+1 −e−iθk+1u′′
k+1.
The eﬀect of that choice is that the diagonal entries in R will be of the form
−eiθjrj, j = ei(θj+π)rj, j. Of course, we can make these entries nonnegative
by applying
hn = ρen, π−θn ◦· · · ◦ρe1,π−θ1
after hn−1.
As in the Euclidean case, Proposition 13.19 immediately implies the
QR-decomposition for arbitrary complex n × n-matrices, where Q is now
unitary (see Kincaid and Cheney [Kincaid and Cheney (1996)] and Ciarlet
[Ciarlet (1989)]).
Proposition 13.20. For every complex n×n-matrix A, there is a sequence
H1, . . . , Hn−1 of matrices, where each Hi is either a Householder matrix or
the identity, and an upper triangular matrix R, such that
R = Hn−1 · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is unitary and
R is upper triangular, such that A = QR (a QR-decomposition of A).
Furthermore, R can be chosen so that its diagonal entries are nonnegative.
This can be achieved by a diagonal matrix D with entries such that |dii| = 1
for i = 1, . . . , n, and we have A = eQ eR with
eQ = H1 · · · Hn−1D,
eR = D∗R,
where eR is upper triangular and has nonnegative diagonal entries.
Proof. It is essentially identical to the proof of Proposition 12.1, and we
leave the details as an exercise. For the last statement, observe that hn ◦
· · · ◦h1 is also an isometry.

514
Hermitian Spaces
13.6
Orthogonal Projections and Involutions
In this section we begin by assuming that the ﬁeld K is not a ﬁeld of
characteristic 2. Recall that a linear map f : E →E is an involution iﬀ
f 2 = id, and is idempotent iﬀf 2 = f. We know from Proposition 5.7 that
if f is idempotent, then
E = Im(f) ⊕Ker (f),
and that the restriction of f to its image is the identity. For this reason, a
linear involution is called a projection. The connection between involutions
and projections is given by the following simple proposition.
Proposition 13.21. For any linear map f : E →E, we have f 2 = id iﬀ
1
2(id−f) is a projection iﬀ1
2(id+f) is a projection; in this case, f is equal
to the diﬀerence of the two projections 1
2(id + f) and 1
2(id −f).
Proof. We have
1
2(id −f)
2
= 1
4(id −2f + f 2)
so
1
2(id −f)
2
= 1
2(id −f)
iﬀ
f 2 = id.
We also have
1
2(id + f)
2
= 1
4(id + 2f + f 2),
so
1
2(id + f)
2
= 1
2(id + f)
iﬀ
f 2 = id.
Obviously, f = 1
2(id + f) −1
2(id −f).
Proposition 13.22. For any linear map f : E →E, let U + = Ker ( 1
2(id −
f)) and let U −= Im( 1
2(id −f)). If f 2 = id, then
U + = Ker
1
2(id −f)

= Im
1
2(id + f)

,
and so, f(u) = u on U + and f(u) = −u on U −.

13.6. Orthogonal Projections and Involutions
515
Proof. If f 2 = id, then
(id + f) ◦(id −f) = id −f 2 = id −id = 0,
which implies that
Im
1
2(id + f)

⊆Ker
1
2(id −f)

.
Conversely, if u ∈Ker
  1
2(id −f)

, then f(u) = u, so
1
2(id + f)(u) = 1
2(u + u) = u,
and thus
Ker
1
2(id −f)

⊆Im
1
2(id + f)

.
Therefore,
U + = Ker
1
2(id −f)

= Im
1
2(id + f)

,
and so, f(u) = u on U + and f(u) = −u on U −.
We now assume that K = C. The involutions of E that are unitary
transformations are characterized as follows.
Proposition 13.23. Let f ∈GL(E) be an involution. The following prop-
erties are equivalent:
(a) The map f is unitary; that is, f ∈U(E).
(b) The subspaces U −= Im( 1
2(id −f)) and U + = Im( 1
2(id + f)) are or-
thogonal.
Furthermore, if E is ﬁnite-dimensional, then (a) and (b) are equivalent
to (c) below:
(c) The map is self-adjoint; that is, f = f ∗.
Proof. If f is unitary, then from ⟨f(u), f(v)⟩= ⟨u, v⟩for all u, v ∈E, we
see that if u ∈U + and v ∈U −, we get
⟨u, v⟩= ⟨f(u), f(v)⟩= ⟨u, −v⟩= −⟨u, v⟩,
so 2⟨u, v⟩= 0, which implies ⟨u, v⟩= 0, that is, U + and U −are orthogonal.
Thus, (a) implies (b).
Conversely, if (b) holds, since f(u) = u on U + and f(u) = −u on U −, we
see that ⟨f(u), f(v)⟩= ⟨u, v⟩if u, v ∈U + or if u, v ∈U −. Since E = U + ⊕
U −and since U + and U −are orthogonal, we also have ⟨f(u), f(v)⟩= ⟨u, v⟩
for all u, v ∈E, and (b) implies (a).
If E is ﬁnite-dimensional, the adjoint f ∗of f exists, and we know that
f −1 = f ∗. Since f is an involution, f 2 = id, which implies that f ∗= f −1 =
f.

516
Hermitian Spaces
A unitary involution is the identity on U + = Im( 1
2(id + f)), and f(v) =
−v for all v ∈U −= Im( 1
2(id−f)). Furthermore, E is an orthogonal direct
sum E = U + ⊕U −. We say that f is an orthogonal reﬂection about U +.
In the special case where U + is a hyperplane, we say that f is a hyperplane
reﬂection. We already studied hyperplane reﬂections in the Euclidean case;
see Chapter 12.
If f : E →E is a projection (f 2 = f), then
(id −2f)2 = id −4f + 4f 2 = id −4f + 4f = id,
so id −2f is an involution. As a consequence, we get the following result.
Proposition 13.24. If f : E →E is a projection (f 2 = f), then Ker (f)
and Im(f) are orthogonal iﬀf ∗= f.
Proof. Apply Proposition 13.23 to g = id −2f. Since id −g = 2f we have
U + = Ker
1
2(id −g)

= Ker (f)
and
U −= Im
1
2(id −g)

= Im(f),
which proves the proposition.
A projection such that f = f ∗is called an orthogonal projection.
If (a1 . . . , ak) are k linearly independent vectors in Rn, let us determine
the matrix P of the orthogonal projection onto the subspace of Rn spanned
by (a1, . . . , ak). Let A be the n × k matrix whose jth column consists of
the coordinates of the vector aj over the canonical basis (e1, . . . , en).
Any vector in the subspace (a1, . . . , ak) is a linear combination of the
form Ax, for some x ∈Rk. Given any y ∈Rn, the orthogonal projection
Py = Ax of y onto the subspace spanned by (a1, . . . , ak) is the vector Ax
such that y −Ax is orthogonal to the subspace spanned by (a1, . . . , ak)
(prove it). This means that y −Ax is orthogonal to every aj, which is
expressed by
A⊤(y −Ax) = 0;
that is,
A⊤Ax = A⊤y.
The matrix A⊤A is invertible because A has full rank k, thus we get
x = (A⊤A)−1A⊤y,

13.7. Dual Norms
517
and so
Py = Ax = A(A⊤A)−1A⊤y.
Therefore, the matrix P of the projection onto the subspace spanned by
(a1 . . . , ak) is given by
P = A(A⊤A)−1A⊤.
The reader should check that P 2 = P and P ⊤= P.
13.7
Dual Norms
In the remark following the proof of Proposition 8.7, we explained that if
(E, ∥∥) and (F, ∥∥) are two normed vector spaces and if we let L(E; F)
denote the set of all continuous (equivalently, bounded) linear maps from
E to F, then, we can deﬁne the operator norm (or subordinate norm) ∥∥
on L(E; F) as follows: for every f ∈L(E; F),
∥f∥= sup
x∈E
x̸=0
∥f(x)∥
∥x∥
= sup
x∈E
∥x∥=1
∥f(x)∥.
In particular, if F = C, then L(E; F) = E′ is the dual space of E, and we
get the operator norm denoted by ∥∥∗given by
∥f∥∗= sup
x∈E
∥x∥=1
|f(x)|.
The norm ∥∥∗is called the dual norm of ∥∥on E′.
Let us now assume that E is a ﬁnite-dimensional Hermitian space, in
which case E′ = E∗.
Theorem 13.1 implies that for every linear form
f ∈E∗, there is a unique vector y ∈E so that
f(x) = ⟨x, y⟩,
for all x ∈E, and so we can write
∥f∥∗= sup
x∈E
∥x∥=1
|⟨x, y⟩|.
The above suggests deﬁning a norm ∥∥D on E.
Deﬁnition 13.13. If E is a ﬁnite-dimensional Hermitian space and ∥∥is
any norm on E, for any y ∈E we let
∥y∥D = sup
x∈E
∥x∥=1
|⟨x, y⟩|,

518
Hermitian Spaces
be the dual norm of ∥∥(on E). If E is a real Euclidean space, then the
dual norm is deﬁned by
∥y∥D = sup
x∈E
∥x∥=1
⟨x, y⟩
for all y ∈E.
Beware that ∥∥is generally not the Hermitian norm associated with the
Hermitian inner product. The dual norm shows up in convex programming;
see Boyd and Vandenberghe [Boyd and Vandenberghe (2004)], Chapters 2,
3, 6, 9.
The fact that ∥∥D is a norm follows from the fact that ∥∥∗is a norm and
can also be checked directly. It is worth noting that the triangle inequality
for ∥∥D comes "for free," in the sense that it holds for any function p: E →
R.
Proposition 13.25. For any function p: E →R, if we deﬁne pD by
pD(x) = sup
p(z)=1
|⟨z, x⟩|,
then we have
pD(x + y) ≤pD(x) + pD(y).
Proof. We have
pD(x + y) = sup
p(z)=1
|⟨z, x + y⟩|
= sup
p(z)=1
(|⟨z, x⟩+ ⟨z, y⟩|)
≤sup
p(z)=1
(|⟨z, x⟩| + |⟨z, y⟩|)
≤sup
p(z)=1
|⟨z, x⟩| + sup
p(z)=1
|⟨z, y⟩|
= pD(x) + pD(y).
Deﬁnition 13.14. If p: E →R is a function such that
(1) p(x) ≥0 for all x ∈E, and p(x) = 0 iﬀx = 0;
(2) p(λx) = |λ|p(x), for all x ∈E and all λ ∈C;
(3) p is continuous, in the sense that for some basis (e1, . . . , en) of E, the
function
(x1, . . . , xn) 7→p(x1e1 + · · · + xnen)
from Cn to R is continuous,

13.7. Dual Norms
519
then we say that p is a pre-norm.
Obviously, every norm is a pre-norm, but a pre-norm may not satisfy
the triangle inequality.
Corollary 13.1. The dual norm of any pre-norm is actually a norm.
Proposition 13.26. For all y ∈E, we have
∥y∥D = sup
x∈E
∥x∥=1
|⟨x, y⟩| = sup
x∈E
∥x∥=1
ℜ⟨x, y⟩.
Proof. Since E is ﬁnite dimensional, the unit sphere Sn−1 = {x ∈E |
∥x∥= 1} is compact, so there is some x0 ∈Sn−1 such that
∥y∥D = |⟨x0, y⟩|.
If ⟨x0, y⟩= ρeiθ, with ρ ≥0, then
|⟨e−iθx0, y⟩| = |e−iθ⟨x0, y⟩| = |e−iθρeiθ| = ρ,
so
∥y∥D = ρ = ⟨e−iθx0, y⟩,
(13.2)
with
e−iθx0
 = ∥x0∥= 1. On the other hand,
ℜ⟨x, y⟩≤|⟨x, y⟩|,
so by (13.2) we get
∥y∥D = sup
x∈E
∥x∥=1
|⟨x, y⟩| = sup
x∈E
∥x∥=1
ℜ⟨x, y⟩,
as claimed.
Proposition 13.27. For all x, y ∈E, we have
|⟨x, y⟩| ≤∥x∥∥y∥D
|⟨x, y⟩| ≤∥x∥D ∥y∥.
Proof. If x = 0, then ⟨x, y⟩= 0 and these inequalities are trivial. If x ̸= 0,
since ∥x/ ∥x∥∥= 1, by deﬁnition of ∥y∥D, we have
|⟨x/ ∥x∥, y⟩| ≤sup
∥z∥=1
|⟨z, y⟩| = ∥y∥D ,
which yields
|⟨x, y⟩| ≤∥x∥∥y∥D .
The second inequality holds because |⟨x, y⟩| = |⟨y, x⟩|.

520
Hermitian Spaces
It is not hard to show that for all y ∈Cn,
∥y∥D
1 = ∥y∥∞
∥y∥D
∞= ∥y∥1
∥y∥D
2 = ∥y∥2 .
Thus, the Euclidean norm is autodual. More generally, the following propo-
sition holds.
Proposition 13.28. If p, q ≥1 and 1/p + 1/q = 1, then for all y ∈Cn, we
have
∥y∥D
p = ∥y∥q .
Proof. By H¨older's inequality (Corollary 8.1), for all x, y ∈Cn, we have
|⟨x, y⟩| ≤∥x∥p ∥y∥q ,
so
∥y∥D
p =
sup
x∈Cn
∥x∥p=1
|⟨x, y⟩| ≤∥y∥q .
For the converse, we consider the cases p = 1, 1 < p < +∞, and p = +∞.
First assume p = 1. The result is obvious for y = 0, so assume y ̸= 0. Given
y, if we pick xj = 1 for some index j such that ∥y∥∞= max1≤i≤n |yi| = |yj|,
and xk = 0 for k ̸= j, then |⟨x, y⟩| = |yj| = ∥y∥∞, so ∥y∥D
1 = ∥y∥∞.
Now we turn to the case 1 < p < +∞. Then we also have 1 < q <
+∞, and the equation 1/p + 1/q = 1 is equivalent to pq = p + q, that is,
p(q −1) = q. Pick zj = yj|yj|q−2 for j = 1, . . . , n, so that
∥z∥p =


n
X
j=1
|zj|p


1/p
=


n
X
j=1
|yj|(q−1)p


1/p
=


n
X
j=1
|yj|q


1/p
.
Then if x = z/ ∥z∥p, we have
|⟨x, y⟩| =
Pn
j=1 zjyj

∥z∥p
=
Pn
j=1 yjyj|yj|q−2
∥z∥p
=
Pn
j=1 |yj|q
Pn
j=1 |yj|q
1/p =


n
X
j=1
|yj|q


1/q
= ∥y∥q .
Thus ∥y∥D
p = ∥y∥q.

13.7. Dual Norms
521
Finally, if p = ∞, then pick xj = yj/|yj| if yj ̸= 0, and xj = 0 if yj = 0.
Then
|⟨x, y⟩| =

n
X
yj̸=0
yjyj/|yj|

=
X
yj̸=0
|yj| = ∥y∥1 .
Thus ∥y∥D
∞= ∥y∥1.
We can show that the dual of the spectral norm is the trace norm (or
nuclear norm) also discussed in Section 20.5. Recall from Proposition 8.7
that the spectral norm ∥A∥2 of a matrix A is the square root of the largest
eigenvalue of A∗A, that is, the largest singular value of A.
Proposition 13.29. The dual of the spectral norm is given by
∥A∥D
2 = σ1 + · · · + σr,
where σ1 > · · · > σr > 0 are the singular values of A ∈Mn(C) (which has
rank r).
Proof. In this case the inner product on Mn(C) is the Frobenius inner
product ⟨A, B⟩= tr(B∗A), and the dual norm of the spectral norm is given
by
∥A∥D
2 = sup{|tr(A∗B)| | ∥B∥2 = 1}.
If we factor A using an SVD as A = V ΣU ∗, where U and V are unitary
and Σ is a diagonal matrix whose r nonzero entries are the singular values
σ1 > · · · > σr > 0, where r is the rank of A, then
|tr(A∗B)| = |tr(UΣV ∗B)| = |tr(ΣV ∗BU)|,
so if we pick B = V U ∗, a unitary matrix such that ∥B∥2 = 1, we get
|tr(A∗B)| = tr(Σ) = σ1 + · · · + σr,
and thus
∥A∥D
2 ≥σ1 + · · · + σr.
Since ∥B∥2 = 1 and U and V are unitary, by Proposition 8.7 we have
∥V ∗BU∥2 = ∥B∥2 = 1. If Z = V ∗BU, by deﬁnition of the operator norm
1 = ∥Z∥2 = sup{∥Zx∥2 | ∥x∥2 = 1},
so by picking x to be the canonical vector ej, we see that
Zj
2 ≤1 where
Zj is the jth column of Z, so |zjj| ≤1, and since
|tr(ΣV ∗BU)| = |tr(ΣZ)| =

r
X
j=1
σjzjj

≤
r
X
j=1
σj|zjj| ≤
r
X
j=1
σj,

522
Hermitian Spaces
and we conclude that
|tr(ΣV ∗BU)| ≤
r
X
j=1
σj.
The above implies that
∥A∥D
2 ≤σ1 + · · · + σr,
and since we also have ∥A∥D
2 ≥σ1 + · · · + σr, we conclude that
∥A∥D
2 = σ1 + · · · + σr,
proving our proposition.
Deﬁnition 13.15. Given any complex matrix n × n matrix A of rank r,
its nuclear norm (or trace norm) is given by
∥A∥N = σ1 + · · · + σr.
The nuclear norm can be generalized to m × n matrices (see Sec-
tion 20.5). The nuclear norm σ1 +· · ·+σr of an m×n matrix A (where r is
the rank of A) is denoted by ∥A∥N. The nuclear norm plays an important
role in matrix completion. The problem is this. Given a matrix A0 with
missing entries (missing data), one would like to ﬁll in the missing entries
in A0 to obtain a matrix A of minimal rank. For example, consider the
matrices
A0 =
1 2
∗∗

,
B0 =
1 ∗
∗4

,
C0 =
1 2
3 ∗

.
All can be completed with rank 1. For A0, use any multiple of (1, 2) for
the second row. For B0, use any numbers b and c such that bc = 4. For
C0, the only possibility is d = 6.
A famous example of this problem is the Netﬂix competition. The rat-
ings of m ﬁlms by n viewers goes into A0. But the customers didn't see all
the movies. Many ratings were missing. Those had to be predicted by a
recommender system. The nuclear norm gave a good solution that needed
to be adjusted for human psychology.
Since the rank of a matrix is not a norm, in order to solve the matrix
completion problem we can use the following "convex relaxation." Let A0
be an incomplete m × n matrix:
Minimize ∥A∥N subject to A = A0 in the known entries.
The above problem has been extensively studied, in particular by
Cand`es and Recht.
Roughly, they showed that if A is an n × n ma-
trix of rank r and K entries are known in A, then if K is large enough

13.7. Dual Norms
523
(K > Cn5/4r log n), with high probability, the recovery of A is perfect. See
Strang [Strang (2019)] for details (Section III.5).
We close this section by stating the following duality theorem.
Theorem 13.2. If E is a ﬁnite-dimensional Hermitian space, then for any
norm ∥∥on E, we have
∥y∥DD = ∥y∥
for all y ∈E.
Proof. By Proposition 13.27, we have
|⟨x, y⟩| ≤∥x∥D ∥y∥,
so we get
∥y∥DD =
sup
∥x∥D=1
|⟨x, y⟩| ≤∥y∥,
for all y ∈E.
It remains to prove that
∥y∥≤∥y∥DD ,
for all y ∈E.
Proofs of this fact can be found in Horn and Johnson [Horn and Johnson
(1990)] (Section 5.5), and in Serre [Serre (2010)] (Chapter 7). The proof
makes use of the fact that a nonempty, closed, convex set has a support-
ing hyperplane through each of its boundary points, a result known as
Minkowski's lemma. For a geometric interpretation of supporting hyper-
plane see Figure 13.1. This result is a consequence of the Hahn-Banach
theorem; see Gallier [Gallier (2011b)]. We give the proof in the case where
E is a real Euclidean space. Some minor modiﬁcations have to be made
when dealing with complex vector spaces and are left as an exercise.
Since the unit ball B = {z ∈E | ∥z∥≤1} is closed and convex, the
Minkowski lemma says for every x such that ∥x∥= 1, there is an aﬃne
map g of the form
g(z) = ⟨z, w⟩−⟨x, w⟩
with ∥w∥= 1, such that g(x) = 0 and g(z) ≤0 for all z such that ∥z∥≤1.
Then it is clear that
sup
∥z∥=1
⟨z, w⟩= ⟨x, w⟩,
and so
∥w∥D = ⟨x, w⟩.

524
Hermitian Spaces
x
Fig. 13.1
The orange tangent plane is a supporting hyperplane to the unit ball in R3
since this ball is entirely contained in "one side" of the tangent plane.
It follows that
∥x∥DD ≥⟨w/ ∥w∥D , x⟩= ⟨x, w⟩
∥w∥D = 1 = ∥x∥
for all x such that ∥x∥= 1. By homogeneity, this is true for all y ∈E,
which completes the proof in the real case. When E is a complex vector
space, we have to view the unit ball B as a closed convex set in R2n and
we use the fact that there is real aﬃne map of the form
g(z) = ℜ⟨z, w⟩−ℜ⟨x, w⟩
such that g(x) = 0 and g(z) ≤0 for all z with ∥z∥= 1, so that ∥w∥D =
ℜ⟨x, w⟩.
More details on dual norms and unitarily invariant norms can be found
in Horn and Johnson [Horn and Johnson (1990)] (Chapters 5 and 7).
13.8
Summary
The main concepts and results of this chapter are listed below:
• Semilinear maps.
• Sesquilinear forms; Hermitian forms.
• Quadratic form associated with a sesquilinear form.

13.9. Problems
525
• Polarization identities.
• Positive and positive deﬁnite Hermitian forms; pre-Hilbert spaces, Her-
mitian spaces.
• Gram matrix associated with a Hermitian product.
• The Cauchy-Schwarz inequality and the Minkowski inequality.
• Hermitian inner product, Hermitian norm.
• The parallelogram law.
• The musical isomorphisms ♭: E →E∗and ♯: E∗→E; Theorem 13.1
(E is ﬁnite-dimensional).
• The adjoint of a linear map (with respect to a Hermitian inner product).
• Existence of orthonormal bases in a Hermitian space (Proposi-
tion 13.10).
• Gram-Schmidt orthonormalization procedure.
• Linear isometries (unitary transformations).
• The unitary group, unitary matrices.
• The unitary group U(n).
• The special unitary group SU(n).
• QR-Decomposition for arbitrary complex matrices.
• The Hadamard inequality for complex matrices.
• The Hadamard inequality for Hermitian positive semideﬁnite matrices.
• Orthogonal projections and involutions; orthogonal reﬂections.
• Dual norms.
• Nuclear norm (also called trace norm).
• Matrix completion.
13.9
Problems
Problem 13.1. Let (E, ⟨−, −⟩) be a Hermitian space of ﬁnite dimension.
Prove that if f : E →E is a self-adjoint linear map (that is, f ∗= f), then
⟨f(x), x⟩∈R for all x ∈E.
Problem 13.2. Prove the polarization identities of Proposition 13.1.
Problem 13.3. Let E be a real Euclidean space. Give an example of a
nonzero linear map f : E →E such that ⟨f(u), u⟩= 0 for all u ∈E.
Problem 13.4. Prove Proposition 13.8.

526
Hermitian Spaces
Problem 13.5. (1) Prove that every matrix in SU(2) is of the form
A =
 a + ib c + id
−c + id a −ib

,
a2 + b2 + c2 + d2 = 1, a, b, c, d ∈R,
(2) Prove that the matrices
1 0
0 1

,
i 0
0 −i

,
 0 1
−1 0

,
0 i
i 0

all belong to SU(2) and are linearly independent over C.
(3) Prove that the linear span of SU(2) over C is the complex vector
space M2(C) of all complex 2 × 2 matrices.
Problem 13.6. The purpose of this problem is to prove that the linear
span of SU(n) over C is Mn(C) for all n ≥3. One way to prove this result
is to adapt the method of Problem 11.12, so please review this problem.
Every complex matrix A ∈Mn(C) can be written as
A = A + A∗
2
+ A −A∗
2
where the ﬁrst matrix is Hermitian and the second matrix is skew-
Hermitian. Observe that if A = (zij) is a Hermitian matrix, that is A∗= A,
then zji = zij, so if zij = aij + ibij with aij, bij ∈R, then aij = aji and
bij = −bji. On the other hand, if A = (zij) is a skew-Hermitian matrix,
that is A∗= −A, then zji = −zij, so aij = −aji and bij = bji.
The Hermitian and the skew-Hermitian matrices do not form complex
vector spaces because they are not closed under multiplication by a complex
number, but we can get around this problem by treating the real part and
the complex part of these matrices separately and using multiplication by
reals.
(1) Consider the matrices of the form
Ri,j
c
=






















1
...
1
0 0 · · · 0 i
0 1 · · · 0 0
...
... ... ...
...
0 0 · · · 1 0
i 0 · · · 0 0
1
...
1






















.

13.9. Problems
527
Prove that (Ri,j
c )∗Ri,j
c
= I and det(Ri,j
c ) = +1.
Use the matrices
Ri,j, Ri,j
c
∈SU(n) and the matrices (Ri,j−(Ri,j)∗)/2 (from Problem 11.12)
to form the real part of a skew-Hermitian matrix and the matrices
(Ri,j
c
−(Ri,j
c )∗)/2 to form the imaginary part of a skew-Hermitian matrix.
Deduce that the matrices in SU(n) span all skew-Hermitian matrices.
(2) Consider matrices of the form
Type 1
S1,2
c
=










0 −i 0 0 . . . 0
i 0
0 0 . . . 0
0 0 −1 0 . . . 0
0 0
0 1 . . . 0
...
...
...
... ... ...
0 0
0 0 . . . 1










.
Type 2
Si,i+1
c
=

















−1
1
...
1
0 −i
i 0
1
...
1

















.
Type 3
Si,j
c
=






















1
...
1
0 0 · · · 0 −i
0 −1 · · · 0 0
...
...
... ...
...
0 0 · · · 1 0
i 0 · · · 0 0
1
...
1






















.

528
Hermitian Spaces
Prove that Si,j, Si,j
c
∈SU(n), and using diagonal matrices as in Prob-
lem 11.12, prove that the matrices Si,j can be used to form the real part of
a Hermitian matrix and the matrices Si,j
c
can be used to form the imaginary
part of a Hermitian matrix.
(3) Use (1) and (2) to prove that the matrices in SU(n) span all Her-
mitian matrices. It follows that SU(n) spans Mn(C) for n ≥3.
Problem 13.7. Consider the complex matrix
A =
i 1
1 −i

.
Check that this matrix is symmetric but not Hermitian. Prove that
det(λI −A) = λ2,
and so the eigenvalues of A are 0, 0.
Problem 13.8. Let (E, ⟨−, −⟩) be a Hermitian space of ﬁnite dimension
and let f : E →E be a linear map. Prove that the following conditions are
equivalent.
(1) f ◦f ∗= f ∗◦f (f is normal).
(2) ⟨f(x), f(y)⟩= ⟨f ∗(x), f ∗(y)⟩for all x, y ∈E.
(3) ∥f(x)∥= ∥f ∗(x)∥for all x ∈E.
(4) The map f can be diagonalized with respect to an orthonormal basis
of eigenvectors.
(5) There exist some linear maps g, h: E →E such that, g = g∗,
⟨x, g(x)⟩≥0 for all x ∈E, h−1 = h∗, and f = g ◦h = h ◦g.
(6) There exist some linear map h: E →E such that h−1 = h∗and f ∗=
h ◦f.
(7) There is a polynomial P (with complex coeﬃcients) such that f ∗=
P(f).
Problem 13.9. Recall from Problem 12.7 that a complex n × n matrix H
is upper Hessenberg if hjk = 0 for all (j, k) such that j −k ≥0. Adapt
the proof of Problem 12.7 to prove that given any complex n×n-matrix A,
there are n −2 ≥1 complex matrices H1, . . . , Hn−2, Householder matrices
or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is upper Hessenberg.

13.9. Problems
529
Problem 13.10. Prove that all y ∈Cn,
∥y∥D
1 = ∥y∥∞
∥y∥D
∞= ∥y∥1
∥y∥D
2 = ∥y∥2 .
Problem 13.11. The purpose of this problem is to complete each of the
matrices A0, B0, C0 of Section 13.7 to a matrix A in such way that the
nuclear norm ∥A∥N is minimized.
(1) Prove that the squares σ2
1 and σ2
2 of the singular values of
A =
1 2
c d

are the zeros of the equation
λ2 −(5 + c2 + d2)λ + (2c −d)2 = 0.
(2) Using the fact that
∥A∥N = σ1 + σ2 =
q
σ2
1 + σ2
2 + 2σ1σ2,
prove that
∥A∥2
N = 5 + c2 + d2 + 2|2c −d|.
Consider the cases where 2c −d ≥0 and 2c −d ≤0, and show that in
both cases we must have c = −2d, and that the minimum of f(c, d) =
5 + c2 + d2 + 2|2c −d| is achieved by c = d = 0. Conclude that the matrix
A completing A0 that minimizes ∥A∥N is
A =
1 2
0 0

.
(3) Prove that the squares σ2
1 and σ2
2 of the singular values of
A =
1 b
c 4

are the zeros of the equation
λ2 −(17 + b2 + c2)λ + (4 −bc)2 = 0.
(4) Prove that
∥A∥2
N = 17 + b2 + c2 + 2|4 −bc|.
Consider the cases where 4 −bc ≥0 and 4 −bc ≤0, and show that in both
cases we must have b2 = c2. Then show that the minimum of f(c, d) =

530
Hermitian Spaces
17 + b2 + c2 + 2|4 −bc| is achieved by b = c with −2 ≤b ≤2. Conclude
that the matrices A completing B0 that minimize ∥A∥N are given by
A =
1 b
b 4

,
−2 ≤b ≤2.
(5) Prove that the squares σ2
1 and σ2
2 of the singular values of
A =
1 2
3 d

are the zeros of the equation
λ2 −(14 + d2)λ + (6 −d)2 = 0
(6) Prove that
∥A∥2
N = 14 + d2 + 2|6 −d|.
Consider the cases where 6 −d ≥0 and 6 −d ≤0, and show that the
minimum of f(c, d) = 14 + d2 + 2|6 −d| is achieved by d = 1. Conclude
that the the matrix A completing C0 that minimizes ∥A∥N is given by
A =
1 2
3 1

.
Problem 13.12. Prove Theorem 13.2 when E is a ﬁnite-dimensional Her-
mitian space.

Chapter 14
Eigenvectors and Eigenvalues
In this chapter all vector spaces are deﬁned over an arbitrary ﬁeld K. For
the sake of concreteness, the reader may safely assume that K = R or
K = C.
14.1
Eigenvectors and Eigenvalues of a Linear Map
Given a ﬁnite-dimensional vector space E, let f : E →E be any linear
map. If by luck there is a basis (e1, . . . , en) of E with respect to which f is
represented by a diagonal matrix
D =






λ1 0 . . . 0
0 λ2
... ...
... ... ... 0
0 . . . 0 λn






,
then the action of f on E is very simple; in every "direction" ei, we have
f(ei) = λiei.
We can think of f as a transformation that stretches or shrinks space along
the direction e1, . . . , en (at least if E is a real vector space).
In terms
of matrices, the above property translates into the fact that there is an
invertible matrix P and a diagonal matrix D such that a matrix A can be
factored as
A = PDP −1.
When this happens, we say that f (or A) is diagonalizable, the λi's are called
the eigenvalues of f, and the ei's are eigenvectors of f. For example, we
531

532
Eigenvectors and Eigenvalues
will see that every symmetric matrix can be diagonalized. Unfortunately,
not every matrix can be diagonalized. For example, the matrix
A1 =
1 1
0 1

can't be diagonalized. Sometimes a matrix fails to be diagonalizable be-
cause its eigenvalues do not belong to the ﬁeld of coeﬃcients, such as
A2 =
0 −1
1 0

,
whose eigenvalues are ±i. This is not a serious problem because A2 can
be diagonalized over the complex numbers. However, A1 is a "fatal" case!
Indeed, its eigenvalues are both 1 and the problem is that A1 does not have
enough eigenvectors to span E.
The next best thing is that there is a basis with respect to which f is
represented by an upper triangular matrix. In this case we say that f can
be triangularized, or that f is triangulable. As we will see in Section 14.2,
if all the eigenvalues of f belong to the ﬁeld of coeﬃcients K, then f can
be triangularized. In particular, this is the case if K = C.
Now an alternative to triangularization is to consider the representation
of f with respect to two bases (e1, . . . , en) and (f1, . . . , fn), rather than a
single basis. In this case, if K = R or K = C, it turns out that we can even
pick these bases to be orthonormal, and we get a diagonal matrix Σ with
nonnegative entries, such that
f(ei) = σifi,
1 ≤i ≤n.
The nonzero σi's are the singular values of f, and the corresponding rep-
resentation is the singular value decomposition, or SVD. The SVD plays
a very important role in applications, and will be considered in detail in
Chapter 20.
In this section we focus on the possibility of diagonalizing a linear map,
and we introduce the relevant concepts to do so. Given a vector space E
over a ﬁeld K, let id denote the identity map on E.
The notion of eigenvalue of a linear map f : E →E deﬁned on an
inﬁnite-dimensional space E is quite subtle because it cannot be deﬁned
in terms of eigenvectors as in the ﬁnite-dimensional case. The problem is
that the map λ id −f (with λ ∈C) could be noninvertible (because it is
not surjective) and yet injective. In ﬁnite dimension this cannot happen,
so until further notice we assume that E is of ﬁnite dimension n.
Deﬁnition 14.1. Given any vector space E of ﬁnite dimension n and any
linear map f : E →E, a scalar λ ∈K is called an eigenvalue, or proper

14.1. Eigenvectors and Eigenvalues of a Linear Map
533
value, or characteristic value of f if there is some nonzero vector u ∈E
such that
f(u) = λu.
Equivalently, λ is an eigenvalue of f if Ker (λ id −f) is nontrivial (i.e.,
Ker (λ id −f) ̸= {0}) iﬀλ id −f is not invertible (this is where the fact
that E is ﬁnite-dimensional is used; a linear map from E to itself is injective
iﬀit is invertible). A vector u ∈E is called an eigenvector, or proper vector,
or characteristic vector of f if u ̸= 0 and if there is some λ ∈K such that
f(u) = λu;
the scalar λ is then an eigenvalue, and we say that u is an eigenvector
associated with λ. Given any eigenvalue λ ∈K, the nontrivial subspace
Ker (λ id −f) consists of all the eigenvectors associated with λ together
with the zero vector; this subspace is denoted by Eλ(f), or E(λ, f), or even
by Eλ, and is called the eigenspace associated with λ, or proper subspace
associated with λ.
Note that distinct eigenvectors may correspond to the same eigenvalue,
but distinct eigenvalues correspond to disjoint sets of eigenvectors.
Remark: As we emphasized in the remark following Deﬁnition 8.4, we re-
quire an eigenvector to be nonzero. This requirement seems to have more
beneﬁts than inconveniences, even though it may considered somewhat in-
elegant because the set of all eigenvectors associated with an eigenvalue is
not a subspace since the zero vector is excluded.
The next proposition shows that the eigenvalues of a linear map f : E →
E are the roots of a polynomial associated with f.
Proposition 14.1. Let E be any vector space of ﬁnite dimension n and let
f be any linear map f : E →E. The eigenvalues of f are the roots (in K)
of the polynomial
det(λ id −f).
Proof. A scalar λ ∈K is an eigenvalue of f iﬀthere is some vector u ̸= 0
in E such that
f(u) = λu
iﬀ
(λ id −f)(u) = 0
iﬀ(λ id −f) is not invertible iﬀ, by Proposition 6.10,
det(λ id −f) = 0.

534
Eigenvectors and Eigenvalues
In view of the importance of the polynomial det(λ id −f), we have the
following deﬁnition.
Deﬁnition 14.2. Given any vector space E of dimension n, for any linear
map f : E →E, the polynomial Pf(X) = χf(X) = det(X id −f) is called
the characteristic polynomial of f. For any square matrix A, the polynomial
PA(X) = χA(X) = det(XI −A) is called the characteristic polynomial of
A.
Note that we already encountered the characteristic polynomial in Sec-
tion 6.7; see Deﬁnition 6.11.
Given any basis (e1, . . . , en), if A = M(f) is the matrix of f w.r.t.
(e1, . . . , en), we can compute the characteristic polynomial χf(X) =
det(X id −f) of f by expanding the following determinant:
det(XI −A) =

X −a1 1
−a1 2
. . .
−a1 n
−a2 1
X −a2 2 . . .
−a2 n
...
...
...
...
−an 1
−an 2
. . . X −an n

.
If we expand this determinant, we ﬁnd that
χA(X) = det(XI −A) = Xn −(a1 1 +· · ·+an n)Xn−1 +· · ·+(−1)n det(A).
The sum tr(A) = a1 1 +· · ·+an n of the diagonal elements of A is called the
trace of A. Since we proved in Section 6.7 that the characteristic polynomial
only depends on the linear map f, the above shows that tr(A) has the same
value for all matrices A representing f. Thus, the trace of a linear map is
well-deﬁned; we have tr(f) = tr(A) for any matrix A representing f.
Remark: The characteristic polynomial of a linear map is sometimes de-
ﬁned as det(f −X id). Since
det(f −X id) = (−1)n det(X id −f),
this makes essentially no diﬀerence but the version det(X id −f) has the
small advantage that the coeﬃcient of Xn is +1.
If we write
χA(X) = det(XI −A)
= Xn −τ1(A)Xn−1 + · · · + (−1)kτk(A)Xn−k + · · · + (−1)nτn(A),

14.1. Eigenvectors and Eigenvalues of a Linear Map
535
then we just proved that
τ1(A) = tr(A)
and
τn(A) = det(A).
It is also possible to express τk(A) in terms of determinants of certain
submatrices of A. For any nonempty subset, I ⊆{1, . . . , n}, say I = {i1 <
. . . < ik}, let AI,I be the k × k submatrix of A whose jth column consists
of the elements aih ij, where h = 1, . . . , k. Equivalently, AI,I is the matrix
obtained from A by ﬁrst selecting the columns whose indices belong to I,
and then the rows whose indices also belong to I. Then it can be shown
that
τk(A) =
X
I⊆{1,...,n}
|I|=k
det(AI,I).
If all the roots, λ1, . . . , λn, of the polynomial det(XI −A) belong to the
ﬁeld K, then we can write
χA(X) = det(XI −A) = (X −λ1) · · · (X −λn),
where some of the λi's may appear more than once. Consequently,
χA(X) = det(XI −A)
= Xn −σ1(λ)Xn−1 + · · · + (−1)kσk(λ)Xn−k + · · · + (−1)nσn(λ),
where
σk(λ) =
X
I⊆{1,...,n}
|I|=k
Y
i∈I
λi,
the kth elementary symmetric polynomial (or function) of the λi's, where
λ = (λ1, . . . , λn).
The elementary symmetric polynomial σk(λ) is often
denoted Ek(λ), but this notation may be confusing in the context of linear
algebra. For n = 5, the elementary symmetric polynomials are listed below:
σ0(λ) = 1
σ1(λ) = λ1 + λ2 + λ3 + λ4 + λ5
σ2(λ) = λ1λ2 + λ1λ3 + λ1λ4 + λ1λ5 + λ2λ3 + λ2λ4 + λ2λ5
+ λ3λ4 + λ3λ5 + λ4λ5
σ3(λ) = λ3λ4λ5 + λ2λ4λ5 + λ2λ3λ5 + λ2λ3λ4 + λ1λ4λ5
+ λ1λ3λ5 + λ1λ3λ4 + λ1λ2λ5 + λ1λ2λ4 + λ1λ2λ3
σ4(λ) = λ1λ2λ3λ4 + λ1λ2λ3λ5 + λ1λ2λ4λ5 + λ1λ3λ4λ5 + λ2λ3λ4λ5
σ5(λ) = λ1λ2λ3λ4λ5.

536
Eigenvectors and Eigenvalues
Since
χA(X) = Xn −τ1(A)Xn−1 + · · · + (−1)kτk(A)Xn−k + · · · + (−1)nτn(A)
= Xn −σ1(λ)Xn−1 + · · · + (−1)kσk(λ)Xn−k + · · · + (−1)nσn(λ),
we have
σk(λ) = τk(A),
k = 1, . . . , n,
and in particular, the product of the eigenvalues of f is equal to det(A) =
det(f), and the sum of the eigenvalues of f is equal to the trace tr(A) =
tr(f), of f; for the record,
tr(f) = λ1 + · · · + λn
det(f) = λ1 · · · λn,
where λ1, . . . , λn are the eigenvalues of f (and A), where some of the λi's
may appear more than once. In particular, f is not invertible iﬀit admits
0 has an eigenvalue (since f is singular iﬀλ1 · · · λn = det(f) = 0).
Remark: Depending on the ﬁeld K,
the characteristic polynomial
χA(X) = det(XI −A) may or may not have roots in K. This motivates
considering algebraically closed ﬁelds, which are ﬁelds K such that every
polynomial with coeﬃcients in K has all its root in K. For example, over
K = R, not every polynomial has real roots. If we consider the matrix
A =
cos θ −sin θ
sin θ
cos θ

,
then the characteristic polynomial det(XI −A) has no real roots unless
θ = kπ. However, over the ﬁeld C of complex numbers, every polynomial
has roots. For example, the matrix above has the roots cos θ±i sin θ = e±iθ.
Remark: It is possible to show that every linear map f over a complex vec-
tor space E must have some (complex) eigenvalue without having recourse
to determinants (and the characteristic polynomial). Let n = dim(E), pick
any nonzero vector u ∈E, and consider the sequence
u, f(u), f 2(u), . . . , f n(u).
Since the above sequence has n + 1 vectors and E has dimension n, these
vectors must be linearly dependent, so there are some complex numbers
c0, . . . , cm, not all zero, such that
c0f m(u) + c1f m−1(u) + · · · + cmu = 0,

14.1. Eigenvectors and Eigenvalues of a Linear Map
537
where m ≤n is the largest integer such that the coeﬃcient of f m(u) is
nonzero (m must exits since we have a nontrivial linear dependency). Now
because the ﬁeld C is algebraically closed, the polynomial
c0Xm + c1Xm−1 + · · · + cm
can be written as a product of linear factors as
c0Xm + c1Xm−1 + · · · + cm = c0(X −λ1) · · · (X −λm)
for some complex numbers λ1, . . . , λm ∈C, not necessarily distinct. But
then since c0 ̸= 0,
c0f m(u) + c1f m−1(u) + · · · + cmu = 0
is equivalent to
(f −λ1 id) ◦· · · ◦(f −λm id)(u) = 0.
If all the linear maps f −λi id were injective, then (f −λ1 id)◦· · ·◦(f −λm id)
would be injective, contradicting the fact that u ̸= 0. Therefore, some linear
map f −λi id must have a nontrivial kernel, which means that there is some
v ̸= 0 so that
f(v) = λiv;
that is, λi is some eigenvalue of f and v is some eigenvector of f.
As nice as the above argument is, it does not provide a method for
ﬁnding the eigenvalues of f, and even if we prefer avoiding determinants as
a much as possible, we are forced to deal with the characteristic polynomial
det(X id −f).
Deﬁnition 14.3. Let A be an n × n matrix over a ﬁeld K. Assume that
all the roots of the characteristic polynomial χA(X) = det(XI −A) of A
belong to K, which means that we can write
det(XI −A) = (X −λ1)k1 · · · (X −λm)km,
where λ1, . . . , λm ∈K are the distinct roots of det(XI −A) and k1 + · · · +
km = n. The integer ki is called the algebraic multiplicity of the eigenvalue
λi, and the dimension of the eigenspace Eλi = Ker(λiI −A) is called the
geometric multiplicity of λi. We denote the algebraic multiplicity of λi by
alg(λi), and its geometric multiplicity by geo(λi).

538
Eigenvectors and Eigenvalues
By deﬁnition, the sum of the algebraic multiplicities is equal to n, but
the sum of the geometric multiplicities can be strictly smaller.
Proposition 14.2. Let A be an n × n matrix over a ﬁeld K and assume
that all the roots of the characteristic polynomial χA(X) = det(XI −A) of
A belong to K. For every eigenvalue λi of A, the geometric multiplicity of
λi is always less than or equal to its algebraic multiplicity, that is,
geo(λi) ≤alg(λi).
Proof. To see this, if ni is the dimension of the eigenspace Eλi associated
with the eigenvalue λi, we can form a basis of Kn obtained by picking a
basis of Eλi and completing this linearly independent family to a basis of
Kn. With respect to this new basis, our matrix is of the form
A′ =
λiIni B
0
D

,
and a simple determinant calculation shows that
det(XI −A) = det(XI −A′) = (X −λi)ni det(XIn−ni −D).
Therefore, (X −λi)ni divides the characteristic polynomial of A′, and thus,
the characteristic polynomial of A. It follows that ni is less than or equal
to the algebraic multiplicity of λi.
The following proposition shows an interesting property of eigenspaces.
Proposition 14.3. Let E be any vector space of ﬁnite dimension n and
let f be any linear map.
If u1, . . . , um are eigenvectors associated with
pairwise distinct eigenvalues λ1, . . . , λm, then the family (u1, . . . , um) is
linearly independent.
Proof. Assume that (u1, . . . , um) is linearly dependent. Then there exists
µ1, . . . , µk ∈K such that
µ1ui1 + · · · + µkuik = 0,
where 1 ≤k ≤m, µi ̸= 0 for all i, 1 ≤i ≤k, {i1, . . . , ik} ⊆{1, . . . , m}, and
no proper subfamily of (ui1, . . . , uik) is linearly dependent (in other words,
we consider a dependency relation with k minimal). Applying f to this
dependency relation, we get
µ1λi1ui1 + · · · + µkλikuik = 0,

14.1. Eigenvectors and Eigenvalues of a Linear Map
539
and if we multiply the original dependency relation by λi1 and subtract it
from the above, we get
µ2(λi2 −λi1)ui2 + · · · + µk(λik −λi1)uik = 0,
which is a nontrivial linear dependency among a proper subfamily of
(ui1, . . . , uik) since the λj are all distinct and the µi are nonzero, a contra-
diction.
As a corollary of Proposition 14.3 we have the following result.
Corollary 14.1. If λ1, . . . , λm are all the pairwise distinct eigenvalues of
f (where m ≤n), we have a direct sum
Eλ1 ⊕· · · ⊕Eλm
of the eigenspaces Eλi.
Unfortunately, it is not always the case that
E = Eλ1 ⊕· · · ⊕Eλm.
Deﬁnition 14.4. When
E = Eλ1 ⊕· · · ⊕Eλm,
we say that f is diagonalizable (and similarly for any matrix associated
with f).
Indeed, picking a basis in each Eλi, we obtain a matrix which is a
diagonal matrix consisting of the eigenvalues, each λi occurring a number
of times equal to the dimension of Eλi.
This happens if the algebraic
multiplicity and the geometric multiplicity of every eigenvalue are equal.
In particular, when the characteristic polynomial has n distinct roots, then
f is diagonalizable. It can also be shown that symmetric matrices have real
eigenvalues and can be diagonalized.
For a negative example, we leave it as exercise to show that the matrix
M =
1 1
0 1

cannot be diagonalized, even though 1 is an eigenvalue. The problem is
that the eigenspace of 1 only has dimension 1. The matrix
A =
cos θ −sin θ
sin θ
cos θ

cannot be diagonalized either, because it has no real eigenvalues, unless
θ = kπ. However, over the ﬁeld of complex numbers, it can be diagonalized.

540
Eigenvectors and Eigenvalues
14.2
Reduction to Upper Triangular Form
Unfortunately, not every linear map on a complex vector space can be
diagonalized. The next best thing is to "triangularize," which means to
ﬁnd a basis over which the matrix has zero entries below the main diagonal.
Fortunately, such a basis always exist.
We say that a square matrix A is an upper triangular matrix if it has
the following shape,









a1 1 a1 2 a1 3 . . .
a1 n−1
a1 n
0
a2 2 a2 3 . . .
a2 n−1
a2 n
0
0
a3 3 . . .
a3 n−1
a3 n
...
...
...
...
...
...
0
0
0
. . . an−1 n−1 an−1 n
0
0
0
. . .
0
an n










,
i.e., ai j = 0 whenever j < i, 1 ≤i, j ≤n.
Theorem 14.1. Given any ﬁnite dimensional vector space over a ﬁeld K,
for any linear map f : E →E, there is a basis (u1, . . . , un) with respect
to which f is represented by an upper triangular matrix (in Mn(K)) iﬀ
all the eigenvalues of f belong to K. Equivalently, for every n × n matrix
A ∈Mn(K), there is an invertible matrix P and an upper triangular matrix
T (both in Mn(K)) such that
A = PTP −1
iﬀall the eigenvalues of A belong to K.
Proof. If there is a basis (u1, . . . , un) with respect to which f is represented
by an upper triangular matrix T in Mn(K), then since the eigenvalues of f
are the diagonal entries of T, all the eigenvalues of f belong to K.
For the converse, we proceed by induction on the dimension n of E.
For n = 1 the result is obvious. If n > 1, since by assumption f has all
its eigenvalue in K, pick some eigenvalue λ1 ∈K of f, and let u1 be some
corresponding (nonzero) eigenvector. We can ﬁnd n−1 vectors (v2, . . . , vn)
such that (u1, v2, . . . , vn) is a basis of E, and let F be the subspace of
dimension n −1 spanned by (v2, . . . , vn). In the basis (u1, v2 . . . , vn), the
matrix of f is of the form
U =





λ1 a1 2 . . . a1 n
0 a2 2 . . . a2 n
...
...
...
...
0 an 2 . . . an n




,

14.2. Reduction to Upper Triangular Form
541
since its ﬁrst column contains the coordinates of λ1u1 over the basis (u1, v2,
. . . , vn). If we let p: E →F be the projection deﬁned such that p(u1) = 0
and p(vi) = vi when 2 ≤i ≤n, the linear map g: F →F deﬁned as the
restriction of p ◦f to F is represented by the (n −1) × (n −1) matrix
V = (ai j)2≤i,j≤n over the basis (v2, . . . , vn). We need to prove that all the
eigenvalues of g belong to K. However, since the ﬁrst column of U has a
single nonzero entry, we get
χU(X) = det(XI −U) = (X −λ1) det(XI −V ) = (X −λ1)χV (X),
where χU(X) is the characteristic polynomial of U and χV (X) is the char-
acteristic polynomial of V . It follows that χV (X) divides χU(X), and since
all the roots of χU(X) are in K, all the roots of χV (X) are also in K.
Consequently, we can apply the induction hypothesis, and there is a basis
(u2, . . . , un) of F such that g is represented by an upper triangular matrix
(bi j)1≤i,j≤n−1. However,
E = Ku1 ⊕F,
and thus (u1, . . . , un) is a basis for E. Since p is the projection from E =
Ku1 ⊕F onto F and g: F →F is the restriction of p ◦f to F, we have
f(u1) = λ1u1
and
f(ui+1) = a1 iu1 +
i
X
j=1
bi juj+1
for some a1 i ∈K, when 1 ≤i ≤n −1. But then the matrix of f with
respect to (u1, . . . , un) is upper triangular.
For the matrix version, we assume that A is the matrix of f with respect
to some basis. Then we just proved that there is a change of basis matrix
P such that A = PTP −1 where T is upper triangular.
If A = PTP −1 where T is upper triangular, note that the diagonal
entries of T are the eigenvalues λ1, . . . , λn of A. Indeed, A and T have the
same characteristic polynomial. Also, if A is a real matrix whose eigenvalues
are all real, then P can be chosen to real, and if A is a rational matrix
whose eigenvalues are all rational, then P can be chosen rational. Since
any polynomial over C has all its roots in C, Theorem 14.1 implies that
every complex n × n matrix can be triangularized.
If E is a Hermitian space (see Chapter 13), the proof of Theo-
rem 14.1 can be easily adapted to prove that there is an orthonormal basis

542
Eigenvectors and Eigenvalues
(u1, . . . , un) with respect to which the matrix of f is upper triangular. This
is usually known as Schur's lemma.
Theorem 14.2. (Schur decomposition) Given any linear map f : E →
E over a complex Hermitian space E, there is an orthonormal basis
(u1, . . . , un) with respect to which f is represented by an upper triangu-
lar matrix. Equivalently, for every n × n matrix A ∈Mn(C), there is a
unitary matrix U and an upper triangular matrix T such that
A = UTU ∗.
If A is real and if all its eigenvalues are real, then there is an orthogonal
matrix Q and a real upper triangular matrix T such that
A = QTQ⊤.
Proof. During the induction, we choose F to be the orthogonal comple-
ment of Cu1 and we pick orthonormal bases (use Propositions 13.12 and
13.11). If E is a real Euclidean space and if the eigenvalues of f are all
real, the proof also goes through with real matrices (use Propositions 11.9
and 11.8).
If λ is an eigenvalue of the matrix A and if u is an eigenvector associated
with λ, from
Au = λu,
we obtain
A2u = A(Au) = A(λu) = λAu = λ2u,
which shows that λ2 is an eigenvalue of A2 for the eigenvector u. An obvious
induction shows that λk is an eigenvalue of Ak for the eigenvector u, for
all k ≥1. Now, if all eigenvalues λ1, . . . , λn of A are in K, it follows that
λk
1, . . . , λk
n are eigenvalues of Ak. However, it is not obvious that Ak does
not have other eigenvalues.
In fact, this can't happen, and this can be
proven using Theorem 14.1.
Proposition 14.4. Given any n×n matrix A ∈Mn(K) with coeﬃcients in
a ﬁeld K, if all eigenvalues λ1, . . . , λn of A are in K, then for every polyno-
mial q(X) ∈K[X], the eigenvalues of q(A) are exactly (q(λ1), . . . , q(λn)).
Proof. By Theorem 14.1, there is an upper triangular matrix T and an
invertible matrix P (both in Mn(K)) such that
A = PTP −1.

14.2. Reduction to Upper Triangular Form
543
Since A and T are similar, they have the same eigenvalues (with the same
multiplicities), so the diagonal entries of T are the eigenvalues of A. Since
Ak = PT kP −1,
k ≥1,
for any polynomial q(X) = c0Xm + · · · + cm−1X + cm, we have
q(A) = c0Am + · · · + cm−1A + cmI
= c0PT mP −1 + · · · + cm−1PTP −1 + cmPIP −1
= P(c0T m + · · · + cm−1T + cmI)P −1
= Pq(T)P −1.
Furthermore, it is easy to check that q(T) is upper triangular and that
its diagonal entries are q(λ1), . . . , q(λn), where λ1, . . . , λn are the diagonal
entries of T, namely the eigenvalues of A. It follows that q(λ1), . . . , q(λn)
are the eigenvalues of q(A).
Remark: There is another way to prove Proposition 14.4 that does not
use Theorem 14.1, but instead uses the fact that given any ﬁeld K, there
is ﬁeld extension K of K (K ⊆K) such that every polynomial q(X) =
c0Xm + · · · + cm−1X + cm (of degree m ≥1) with coeﬃcients ci ∈K
factors as
q(X) = c0(X −α1) · · · (X −αn),
αi ∈K, i = 1, . . . , n.
The ﬁeld K is called an algebraically closed ﬁeld (and an algebraic closure
of K).
Assume that all eigenvalues λ1, . . . , λn of A belong to K. Let q(X) be
any polynomial (in K[X]) and let µ ∈K be any eigenvalue of q(A) (this
means that µ is a zero of the characteristic polynomial χq(A)(X) ∈K[X]
of q(A). Since K is algebraically closed, χq(A)(X) has all its roots in K).
We claim that µ = q(λi) for some eigenvalue λi of A.
Proof. (After Lax [Lax (2007)], Chapter 6).
Since K is algebraically
closed, the polynomial µ −q(X) factors as
µ −q(X) = c0(X −α1) · · · (X −αn),
for some αi ∈K. Now µI −q(A) is a matrix in Mn(K), and since µ is an
eigenvalue of q(A), it must be singular. We have
µI −q(A) = c0(A −α1I) · · · (A −αnI),

544
Eigenvectors and Eigenvalues
and since the left-hand side is singular, so is the right-hand side, which
implies that some factor A −αiI is singular. This means that αi is an
eigenvalue of A, say αi = λi. As αi = λi is a zero of µ −q(X), we get
µ = q(λi),
which proves that µ is indeed of the form q(λi) for some eigenvalue λi of
A.
Using Theorem 14.2, we can derive two very important results.
Proposition 14.5. If A is a Hermitian matrix (i.e. A∗= A), then its
eigenvalues are real and A can be diagonalized with respect to an orthonor-
mal basis of eigenvectors. In matrix terms, there is a unitary matrix U and
a real diagonal matrix D such that A = UDU ∗. If A is a real symmetric
matrix (i.e. A⊤= A), then its eigenvalues are real and A can be diagonal-
ized with respect to an orthonormal basis of eigenvectors. In matrix terms,
there is an orthogonal matrix Q and a real diagonal matrix D such that
A = QDQ⊤.
Proof. By Theorem 14.2, we can write A = UTU ∗where T = (tij) is
upper triangular and U is a unitary matrix. If A∗= A, we get
UTU ∗= UT ∗U ∗,
and this implies that T = T ∗. Since T is an upper triangular matrix, T ∗
is a lower triangular matrix, which implies that T is a diagonal matrix.
Furthermore, since T = T ∗, we have tii = tii for i = 1, . . . , n, which means
that the tii are real, so T is indeed a real diagonal matrix, say D.
If we apply this result to a (real) symmetric matrix A, we obtain the
fact that all the eigenvalues of a symmetric matrix are real, and by applying
Theorem 14.2 again, we conclude that A = QDQ⊤, where Q is orthogonal
and D is a real diagonal matrix.
More general versions of Proposition 14.5 are proven in Chapter 16.
When a real matrix A has complex eigenvalues, there is a version of
Theorem 14.2 involving only real matrices provided that we allow T to be
block upper-triangular (the diagonal entries may be 2 × 2 matrices or real
entries).
Theorem 14.2 is not a very practical result but it is a useful theoretical
result to cope with matrices that cannot be diagonalized. For example, it
can be used to prove that every complex matrix is the limit of a sequence
of diagonalizable matrices that have distinct eigenvalues!

14.3. Location of Eigenvalues
545
14.3
Location of Eigenvalues
If A is an n × n complex (or real) matrix A, it would be useful to know,
even roughly, where the eigenvalues of A are located in the complex plane
C. The Gershgorin discs provide some precise information about this.
Deﬁnition 14.5. For any complex n × n matrix A, for i = 1, . . . , n, let
R′
i(A) =
n

j=1
j̸=i
|ai j|
and let
G(A) =
n

i=1
{z ∈C | |z −ai i| ≤R′
i(A)}.
Each disc {z ∈C | |z −ai i| ≤R′
i(A)} is called a Gershgorin disc and their
union G(A) is called the Gershgorin domain. An example of Gershgorin
domain for A =


1 2
3
4 i
6
7 8 1 + i

is illustrated in Figure 14.1.
Fig. 14.1
Let A be the 3 × 3 matrix speciﬁed at the end of Deﬁnition 14.5. For this
particular A, we ﬁnd that R′
1(A) = 5, R′
2(A) = 10, and R′
3(A) = 15. The blue/purple
disk is |z −1| ≤5, the pink disk is |z −i| ≤10, the peach disk is |z −1 −i| ≤15, and
G(A) is the union of these three disks.

546
Eigenvectors and Eigenvalues
Although easy to prove, the following theorem is very useful:
Theorem 14.3. (Gershgorin's disc theorem) For any complex n × n ma-
trix A, all the eigenvalues of A belong to the Gershgorin domain G(A).
Furthermore the following properties hold:
(1) If A is strictly row diagonally dominant, that is
|ai i| >
n
X
j=1, j̸=i
|ai j|,
for i = 1, . . . , n,
then A is invertible.
(2) If A is strictly row diagonally dominant, and if ai i > 0 for i = 1, . . . , n,
then every eigenvalue of A has a strictly positive real part.
Proof. Let λ be any eigenvalue of A and let u be a corresponding eigen-
vector (recall that we must have u ̸= 0). Let k be an index such that
|uk| = max
1≤i≤n |ui|.
Since Au = λu, we have
(λ −ak k)uk =
n
X
j=1
j̸=k
ak juj,
which implies that
|λ −ak k||uk| ≤
n
X
j=1
j̸=k
|ak j||uj| ≤|uk|
n
X
j=1
j̸=k
|ak j|.
Since u ̸= 0 and |uk| = max1≤i≤n |ui|, we must have |uk| ̸= 0, and it follows
that
|λ −ak k| ≤
n
X
j=1
j̸=k
|ak j| = R′
k(A),
and thus
λ ∈{z ∈C | |z −ak k| ≤R′
k(A)} ⊆G(A),
as claimed.
(1) Strict row diagonal dominance implies that 0 does not belong to
any of the Gershgorin discs, so all eigenvalues of A are nonzero, and A is
invertible.
(2) If A is strictly row diagonally dominant and ai i > 0 for i = 1, . . . , n,
then each of the Gershgorin discs lies strictly in the right half-plane, so
every eigenvalue of A has a strictly positive real part.

14.3. Location of Eigenvalues
547
In particular, Theorem 14.3 implies that if a symmetric matrix is strictly
row diagonally dominant and has strictly positive diagonal entries, then it
is positive deﬁnite.
Theorem 14.3 is sometimes called the Gershgorin-
Hadamard theorem.
Since A and A⊤have the same eigenvalues (even for complex matrices)
we also have a version of Theorem 14.3 for the discs of radius
C′
j(A) =
n

i=1
i̸=j
|ai j|,
whose domain G(A⊤) is given by
G(A⊤) =
n

i=1
{z ∈C | |z −ai i| ≤C′
i(A)}.
Figure 14.2 shows G(A⊤) for A =


1 2
3
4 i
6
7 8 1 + i

.
Fig. 14.2
Let A be the 3 × 3 matrix speciﬁed at the end of Deﬁnition 14.5. For this
particular A, we ﬁnd that C′
1(A) = 11, C′
2(A) = 10, and C′
3(A) = 9. The pale blue disk
is |z −1| ≤1, the pink disk is |z −i| ≤10, the ocher disk is |z −1 −i| ≤9, and G(A⊤)
is the union of these three disks.
Thus we get the following:
Theorem 14.4. For any complex n × n matrix A, all the eigenvalues of A
belong to the intersection of the Gershgorin domains G(A) ∩G(A⊤). See
Figure 14.3. Furthermore the following properties hold:

548
Eigenvectors and Eigenvalues
(1) If A is strictly column diagonally dominant, that is
|ai i| >
n

i=1, i̸=j
|ai j|,
for j = 1, . . . , n,
then A is invertible.
(2) If A is strictly column diagonally dominant, and if ai i > 0 for i =
1, . . . , n, then every eigenvalue of A has a strictly positive real part.
Fig. 14.3
Let A be the 3 × 3 matrix speciﬁed at the end of Deﬁnition 14.5. The dusty
rose region is G(A) ∩G(A⊤).
There are reﬁnements of Gershgorin's theorem and eigenvalue location
results involving other domains besides discs; for more on this subject, see
Horn and Johnson [Horn and Johnson (1990)], Sections 6.1 and 6.2.
Remark: Neither strict row diagonal dominance nor strict column diag-
onal dominance are necessary for invertibility. Also, if we relax all strict
inequalities to inequalities, then row diagonal dominance (or column diag-
onal dominance) is not a suﬃcient condition for invertibility.

14.4. Conditioning of Eigenvalue Problems
549
14.4
Conditioning of Eigenvalue Problems
The following n × n matrix
A =










0
1 0
1 0
... ...
1 0
1 0










has the eigenvalue 0 with multiplicity n. However, if we perturb the top
rightmost entry of A by ϵ, it is easy to see that the characteristic polynomial
of the matrix
A(ϵ) =










0
ϵ
1 0
1 0
... ...
1 0
1 0










is Xn −ϵ.
It follows that if n = 40 and ϵ = 10−40, A(10−40) has the
eigenvalues 10−1ek2πi/40 with k = 1, . . . , 40.
Thus, we see that a very
small change (ϵ = 10−40) to the matrix A causes a signiﬁcant change to
the eigenvalues of A (from 0 to 10−1ek2πi/40). Indeed, the relative error
is 10−39. Worse, due to machine precision, since very small numbers are
treated as 0, the error on the computation of eigenvalues (for example, of
the matrix A(10−40)) can be very large.
This phenomenon is similar to the phenomenon discussed in Section 8.5
where we studied the eﬀect of a small perturbation of the coeﬃcients of
a linear system Ax = b on its solution. In Section 8.5, we saw that the
behavior of a linear system under small perturbations is governed by the
condition number cond(A) of the matrix A. In the case of the eigenvalue
problem (ﬁnding the eigenvalues of a matrix), we will see that the condition-
ing of the problem depends on the condition number of the change of basis
matrix P used in reducing the matrix A to its diagonal form D = P −1AP,
rather than on the condition number of A itself. The following proposition
in which we assume that A is diagonalizable and that the matrix norm
∥∥satisﬁes a special condition (satisﬁed by the operator norms ∥∥p for

550
Eigenvectors and Eigenvalues
p = 1, 2, ∞), is due to Bauer and Fike (1960).
Proposition 14.6. Let A ∈Mn(C) be a diagonalizable matrix, P be an
invertible matrix, and D be a diagonal matrix D = diag(λ1, . . . , λn) such
that
A = PDP −1,
and let ∥∥be a matrix norm such that
∥diag(α1, . . . , αn)∥= max
1≤i≤n |αi|,
for every diagonal matrix. Then for every perturbation matrix ∆A, if we
write
Bi = {z ∈C | |z −λi| ≤cond(P) ∥∆A∥},
for every eigenvalue λ of A + ∆A, we have
λ ∈
n
[
k=1
Bk.
Proof. Let λ be any eigenvalue of the matrix A + ∆A. If λ = λj for some
j, then the result is trivial. Thus assume that λ ̸= λj for j = 1, . . . , n. In
this case the matrix D −λI is invertible (since its eigenvalues are λ −λj
for j = 1, . . . , n), and we have
P −1(A + ∆A −λI)P = D −λI + P −1(∆A)P
= (D −λI)(I + (D −λI)−1P −1(∆A)P).
Since λ is an eigenvalue of A + ∆A, the matrix A + ∆A −λI is singular, so
the matrix
I + (D −λI)−1P −1(∆A)P
must also be singular. By Proposition 8.8(2), we have
1 ≤
(D −λI)−1P −1(∆A)P
 ,
and since ∥∥is a matrix norm,
(D −λI)−1P −1(∆A)P
 ≤
(D −λI)−1 P −1 ∥∆A∥∥P∥,
so we have
1 ≤
(D −λI)−1 P −1 ∥∆A∥∥P∥.
Now (D −λI)−1 is a diagonal matrix with entries 1/(λi −λ), so by our
assumption on the norm,
(D −λI)−1 =
1
mini(|λi −λ|).

14.5. Eigenvalues of the Matrix Exponential
551
As a consequence, since there is some index k for which mini(|λi −λ|) =
|λk −λ|, we have
(D −λI)−1 =
1
|λk −λ|,
and we obtain
|λ −λk| ≤
P −1 ∥∆A∥∥P∥= cond(P) ∥∆A∥,
which proves our result.
Proposition 14.6 implies that for any diagonalizable matrix A, if we
deﬁne Γ(A) by
Γ(A) = inf{cond(P) | P −1AP = D},
then for every eigenvalue λ of A + ∆A, we have
λ ∈
n
[
k=1
{z ∈Cn | |z −λk| ≤Γ(A) ∥∆A∥}.
Deﬁnition 14.6. The number Γ(A) = inf{cond(P) | P −1AP = D} is
called the conditioning of A relative to the eigenvalue problem.
If A is a normal matrix, since by Theorem 16.12, A can be diagonal-
ized with respect to a unitary matrix U, and since for the spectral norm
∥U∥2 = 1, we see that Γ(A) = 1. Therefore, normal matrices are very well
conditioned w.r.t. the eigenvalue problem. In fact, for every eigenvalue λ
of A + ∆A (with A normal), we have
λ ∈
n
[
k=1
{z ∈Cn | |z −λk| ≤∥∆A∥2}.
If A and A + ∆A are both symmetric (or Hermitian), there are sharper
results; see Proposition 16.15.
Note that the matrix A(ϵ) from the beginning of the section is not
normal.
14.5
Eigenvalues of the Matrix Exponential
The Schur decomposition yields a characterization of the eigenvalues of the
matrix exponential eA in terms of the eigenvalues of the matrix A. First
we have the following proposition.
Proposition 14.7. Let A and U be (real or complex) matrices and assume
that U is invertible. Then
eUAU −1 = UeAU −1.

552
Eigenvectors and Eigenvalues
Proof. A trivial induction shows that
UApU −1 = (UAU −1)p,
and thus
eUAU −1 =
X
p≥0
(UAU −1)p
p!
=
X
p≥0
UApU −1
p!
= U

X
p≥0
Ap
p!

U −1 = UeAU −1,
as claimed.
Proposition 14.8. Given any complex n×n matrix A, if λ1, . . . , λn are the
eigenvalues of A, then eλ1, . . . , eλn are the eigenvalues of eA. Furthermore,
if u is an eigenvector of A for λi, then u is an eigenvector of eA for eλi.
Proof. By Theorem 14.1, there is an invertible matrix P and an upper
triangular matrix T such that
A = PTP −1.
By Proposition 14.7,
eP T P −1 = PeT P −1.
Note that eT = P
p≥0
T p
p! is upper triangular since T p is upper triangular
for all p ≥0. If λ1, λ2, . . . , λn are the diagonal entries of T, the properties of
matrix multiplication, when combined with an induction on p, imply that
the diagonal entries of T p are λp
1, λp
2, . . . , λp
n. This in turn implies that the
diagonal entries of eT are P
p≥0
λp
i
p! = eλi for 1 ≤i ≤n. Since A and T
are similar matrices, we know that they have the same eigenvalues, namely
the diagonal entries λ1, . . . , λn of T. Since eA = eP T P −1 = PeT P −1, and
eT is upper triangular, we use the same argument to conclude that both
eA and eT have the same eigenvalues, which are the diagonal entries of eT ,
where the diagonal entries of eT are of the form eλ1, . . . , eλn. Now, if u is
an eigenvector of A for the eigenvalue λ, a simple induction shows that u
is an eigenvector of An for the eigenvalue λn, from which is follows that
eAu =

I + A
1! + A2
2! + A3
3! + . . .

u = u + Au + A2
2! u + A3
3! u + . . .
= u + λu + λ2
2! u + λ3
3! u + · · · =

1 + λ + λ2
2! + λ3
3! + . . .

u = eλu,
which shows that u is an eigenvector of eA for eλ.

14.5. Eigenvalues of the Matrix Exponential
553
As a consequence, we obtain the following result.
Proposition 14.9. For every complex (or real) square matrix A, we have
det(eA) = etr(A),
where tr(A) is the trace of A, i.e., the sum a1 1 + · · · + an n of its diagonal
entries.
Proof. The trace of a matrix A is equal to the sum of the eigenvalues of A.
The determinant of a matrix is equal to the product of its eigenvalues, and
if λ1, . . . , λn are the eigenvalues of A, then by Proposition 14.8, eλ1, . . . , eλn
are the eigenvalues of eA, and thus
det
 eA
= eλ1 · · · eλn = eλ1+···+λn = etr(A),
as desired.
If B is a skew symmetric matrix, since tr(B) = 0, we deduce that
det(eB) = e0 = 1. This allows us to obtain the following result. Recall that
the (real) vector space of skew symmetric matrices is denoted by so(n).
Proposition 14.10. For every skew symmetric matrix B ∈so(n), we have
eB ∈SO(n), that is, eB is a rotation.
Proof. By Proposition 8.18, eB is an orthogonal matrix. Since tr(B) = 0,
we deduce that det(eB) = e0 = 1. Therefore, eB ∈SO(n).
Proposition 14.10 shows that the map B 7→eB is a map exp: so(n) →
SO(n). It is not injective, but it can be shown (using one of the spectral
theorems) that it is surjective.
If B is a (real) symmetric matrix, then
(eB)⊤= eB⊤= eB,
so eB is also symmetric. Since the eigenvalues λ1, . . . , λn of B are real, by
Proposition 14.8, since the eigenvalues of eB are eλ1, . . . , eλn and the λi are
real, we have eλi > 0 for i = 1, . . . , n, which implies that eB is symmetric
positive deﬁnite. In fact, it can be shown that for every symmetric positive
deﬁnite matrix A, there is a unique symmetric matrix B such that A = eB;
see Gallier [Gallier (2011b)].

554
Eigenvectors and Eigenvalues
14.6
Summary
The main concepts and results of this chapter are listed below:
• Diagonal matrix.
• Eigenvalues, eigenvectors; the eigenspace associated with an eigen-
value.
• Characteristic polynomial.
• Trace.
• Algebraic and geometric multiplicity.
• Eigenspaces associated with distinct eigenvalues form a direct sum
(Proposition 14.3).
• Reduction of a matrix to an upper-triangular matrix.
• Schur decomposition.
• The Gershgorin's discs can be used to locate the eigenvalues of a com-
plex matrix; see Theorems 14.3 and 14.4.
• The conditioning of eigenvalue problems.
• Eigenvalues of the matrix exponential. The formula det(eA) = etr(A).
14.7
Problems
Problem 14.1. Let A be the following 2 × 2 matrix
A =
1 −1
1 −1

.
(1) Prove that A has the eigenvalue 0 with multiplicity 2 and that
A2 = 0.
(2) Let A be any real 2 × 2 matrix
A =
a b
c d

.
Prove that if bc > 0, then A has two distinct real eigenvalues. Prove that
if a, b, c, d > 0, then there is a positive eigenvector u associated with the
largest of the two eigenvalues of A, which means that if u = (u1, u2), then
u1 > 0 and u2 > 0.
(3) Suppose now that A is any complex 2 × 2 matrix as in (2). Prove
that if A has the eigenvalue 0 with multiplicity 2, then A2 = 0. Prove that
if A is real symmetric, then A = 0.

14.7. Problems
555
Problem 14.2. Let A be any complex n × n matrix. Prove that if A has
the eigenvalue 0 with multiplicity n, then An = 0. Give an example of a
matrix A such that An = 0 but A ̸= 0.
Problem 14.3. Let A be a complex 2 × 2 matrix, and let λ1 and λ2 be
the eigenvalues of A. Prove that if λ1 ̸= λ2, then
eA = λ1eλ2 −λ2eλ1
λ1 −λ2
I + eλ1 −eλ2
λ1 −λ2
A.
Problem 14.4. Let A be the real symmetric 2 × 2 matrix
A =
a b
b c

.
(1) Prove that the eigenvalues of A are real and given by
λ1 = a + c +
p
4b2 + (a −c)2
2
,
λ2 = a + c −
p
4b2 + (a −c)2
2
.
(2) Prove that A has a double eigenvalue (λ1 = λ2 = a) if and only if
b = 0 and a = c; that is, A is a diagonal matrix.
(3) Prove that the eigenvalues of A are nonnegative iﬀb2 ≤ac and
a + c ≥0.
(4) Prove that the eigenvalues of A are positive iﬀb2 < ac, a > 0 and
c > 0.
Problem 14.5. Find the eigenvalues of the matrices
A =
3 0
1 1

,
B =
1 1
0 3

,
C = A + B =
4 1
1 4

.
Check that the eigenvalues of A+B are not equal to the sums of eigenvalues
of A plus eigenvalues of B.
Problem 14.6. Let A be a real symmetric n × n matrix and B be a
real symmetric positive deﬁnite n × n matrix. We would like to solve the
generalized eigenvalue problem: ﬁnd λ ∈R and u ̸= 0 such that
Au = λBu.
(14.1)
(1) Use the Cholseky decomposition B = CC⊤to show that λ and u
are solutions of the generalized eigenvalue problem (14.1) iﬀλ and v are
solutions the (ordinary) eigenvalue problem
C−1A(C⊤)−1v = λv,
with v = C⊤u.
Check that C−1A(C⊤)−1 is symmetric.
(2) Prove that if Au1 = λ1Bu1, Au2 = λ2Bu2, with u1 ̸= 0, u2 ̸= 0 and
λ1 ̸= λ2, then u⊤
1 Bu2 = 0.
(3) Prove that B−1A and C−1A(C⊤)−1 have the same eigenvalues.

556
Eigenvectors and Eigenvalues
Problem 14.7. The sequence of Fibonacci numbers, 0, 1, 1, 2, 3, 5, 8, 13,
21, 34, 55, . . . , is given by the recurrence
Fn+2 = Fn+1 + Fn,
with F0 = 0 and F1 = 1. In matrix form, we can write
Fn+1
Fn

=
1 1
1 0
  Fn
Fn−1

,
n ≥1,
F1
F0

=
1
0

.
(1) Show that
Fn+1
Fn

=
1 1
1 0
n 1
0

.
(2) Prove that the eigenvalues of the matrix
A =
1 1
1 0

are
λ = 1 ±
√
5
2
.
The number
ϕ = 1 +
√
5
2
is called the golden ratio. Show that the eigenvalues of A are ϕ and −ϕ−1.
(3) Prove that A is diagonalized as
A =
1 1
1 0

=
1
√
5
ϕ −ϕ−1
1
1
 ϕ
0
0 −ϕ−1
  1 ϕ−1
−1
ϕ

.
Prove that
Fn+1
Fn

=
1
√
5
ϕ −ϕ−1
1
1
 
ϕn
−(−ϕ−1)n

,
and thus
Fn =
1
√
5(ϕn −(−ϕ−1)n) =
1
√
5
" 
1 +
√
5
2
!n
−
 
1 −
√
5
2
!n#
,
n ≥0.
Problem 14.8. Let A be an n × n matrix. For any subset I of {1, . . . , n},
let AI,I be the matrix obtained from A by ﬁrst selecting the columns whose
indices belong to I, and then the rows whose indices also belong to I. Prove
that
τk(A) =
X
I⊆{1,...,n}
|I|=k
det(AI,I).

14.7. Problems
557
Problem 14.9. (1) Consider the matrix
A =


0 0 −a3
1 0 −a2
0 1 −a1

.
Prove that the characteristic polynomial χA(z) = det(zI −A) of A is
given by
χA(z) = z3 + a1z2 + a2z + a3.
(2) Consider the matrix
A =




0 0 0 −a4
1 0 0 −a3
0 1 0 −a2
0 0 1 −a1



.
Prove that the characteristic polynomial χA(z) = det(zI −A) of A is
given by
χA(z) = z4 + a1z3 + a2z2 + a3z + a4.
(3) Consider the n × n matrix (called a companion matrix)
A =











0 0
0 · · · 0
−an
1 0
0 · · · 0 −an−1
0 1
0 · · · 0 −an−2
... ... ... ... ...
...
0 0
0 ... 0
−a2
0 0
0 · · · 1
−a1











.
Prove that the characteristic polynomial χA(z) = det(zI −A) of A is
given by
χA(z) = zn + a1zn−1 + a2zn−2 + · · · + an−1z + an.
Hint. Use induction.
Explain why ﬁnding the roots of a polynomial (with real or complex
coeﬃcients) and ﬁnding the eigenvalues of a (real or complex) matrix are
equivalent problems, in the sense that if we have a method for solving one
of these problems, then we have a method to solve the other.
Problem 14.10. Let A be a complex n × n matrix. Prove that if A is
invertible and if the eigenvalues of A are (λ1, . . . , λn), then the eigenvalues
of A−1 are (λ−1
1 , . . . , λ−1
n ). Prove that if u is an eigenvector of A for λi,
then u is an eigenvector of A−1 for λ−1
i .

558
Eigenvectors and Eigenvalues
Problem 14.11. Prove that every complex matrix is the limit of a sequence
of diagonalizable matrices that have distinct eigenvalues
Problem 14.12. Consider the following tridiagonal n × n matrices
A =







2 −1 0
−1 2 −1
... ... ...
−1 2 −1
0 −1 2







,
S =







0 1
0
1 0
1
... ... ...
1
0 1
0
1 0







.
Observe that A = 2I−S and show that the eigenvalues of A are λk = 2−µk,
where the µk are the eigenvalues of S.
(2) Using Problem 9.6, prove that the eigenvalues of the matrix A are
given by
λk = 4 sin2

kπ
2(n + 1)

,
k = 1, . . . , n.
Show that A is symmetric positive deﬁnite.
(3) Find the condition number of A with respect to the 2-norm.
(4) Show that an eigenvector (y(k)
1 , . . . , y(k)
n ) associated with the eigen-
value λk is given by
y(k)
j
= sin
 kjπ
n + 1

,
j = 1, . . . , n.
Problem 14.13. Consider the following real tridiagonal symmetric n × n
matrix
A =







c 1
0
1 c
1
... ... ...
1
c 1
0
1 c







.
(1) Using Problem 9.6, prove that the eigenvalues of the matrix A are
given by
λk = c + 2 cos
 kπ
n + 1

,
k = 1, . . . , n.
(2) Find a condition on c so that A is positive deﬁnite. It is satisﬁed by
c = 4?

14.7. Problems
559
Problem 14.14. Let A be an m × n matrix and B be an n × m matrix
(over C).
(1) Prove that
det(Im −AB) = det(In −BA).
Hint. Consider the matrices
X =
Im A
B In

and
Y =
 Im 0
−B In

.
(2) Prove that
λn det(λIm −AB) = λm det(λIn −BA).
Hint. Consider the matrices
X =
λIm A
B
In

and
Y =
 Im
0
−B λIn

.
Deduce that AB and BA have the same nonzero eigenvalues with the same
multiplicity.
Problem 14.15. The purpose of this problem is to prove that the charac-
teristic polynomial of the matrix
A =







1
2
3
4
· · ·
n
2
3
4
5
· · · n + 1
3
4
5
6
· · · n + 2
...
...
...
...
...
n n + 1 n + 2 n + 3 · · · 2n −1







is
PA(λ) = λn−2

λ2 −n2λ −1
12n2(n2 −1)

.
(1) Prove that the characteristic polynomial PA(λ) is given by
PA(λ) = λn−2P(λ),

560
Eigenvectors and Eigenvalues
with
P(λ) =

λ −1
−2
−3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−λ −1 λ −1 −1 −1 · · ·
−1
−1
−1
−1
1
−2
1
0 · · ·
0
0
0
0
0
1
−2 1 · · ·
0
0
0
0
...
...
... ... ...
...
...
...
...
0
0
0
0
...
1
0
0
0
0
0
0
0
...
−2
1
0
0
0
0
0
0 · · ·
1
−2
1
0
0
0
0
0 · · ·
0
1
−2
1

.
(2) Prove that the sum of the roots λ1, λ2 of the (degree two) polynomial
P(λ) is
λ1 + λ2 = n2.
The problem is thus to compute the product λ1λ2 of these roots. Prove
that
λ1λ2 = P(0).
(3) The problem is now to evaluate dn = P(0), where
dn =

−1 −2 −3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−1 −1 −1 −1 · · ·
−1
−1
−1
−1
1 −2 1
0 · · ·
0
0
0
0
0
1 −2 1 · · ·
0
0
0
0
...
...
... ... ...
...
...
...
...
0
0
0
0
...
1
0
0
0
0
0
0
0
...
−2
1
0
0
0
0
0
0 · · ·
1
−2
1
0
0
0
0
0 · · ·
0
1
−2
1


14.7. Problems
561
I suggest the following strategy: cancel out the ﬁrst entry in row 1 and
row 2 by adding a suitable multiple of row 3 to row 1 and row 2, and then
subtract row 2 from row 1.
Do this twice.
You will notice that the ﬁrst two entries on row 1 and the ﬁrst two
entries on row 2 change, but the rest of the matrix looks the same, except
that the dimension is reduced.
This suggests setting up a recurrence involving the entries uk, vk, xk, yk
in the determinant
Dk =

uk xk −3 −4 · · · −n + k −3 −n + k −2 −n + k −1 −n + k
vk yk −1 −1 · · ·
−1
−1
−1
−1
1 −2 1
0 · · ·
0
0
0
0
0
1 −2 1 · · ·
0
0
0
0
...
...
... ... ...
...
...
...
...
0
0
0
0
...
1
0
0
0
0
0
0
0
...
−2
1
0
0
0
0
0
0 · · ·
1
−2
1
0
0
0
0
0 · · ·
0
1
−2
1

,
starting with k = 0, with
u0 = −1,
v0 = −1,
x0 = −2,
y0 = −1,
and ending with k = n −2, so that
dn = Dn−2 =

un−3 xn−3 −3
vn−3 yn−3 −1
1
−2
1

=

un−2 xn−2
vn−2 yn−2
 .
Prove that we have the recurrence relations




uk+1
vk+1
xk+1
yk+1



=




2 −2 1 −1
0
2 0 1
−1 1 0 0
0 −1 0 0








uk
vk
xk
yk



+




0
0
−2
−1



.
These appear to be nasty aﬃne recurrence relations, so we will use the
trick to convert this aﬃne map to a linear map.

562
Eigenvectors and Eigenvalues
(4) Consider the linear map given by






uk+1
vk+1
xk+1
yk+1
1






=






2 −2 1 −1 0
0
2 0 1
0
−1 1 0 0 −2
0 −1 0 0 −1
0
0 0 0
1












uk
vk
xk
yk
1






,
and show that its action on uk, vk, xk, yk is the same as the aﬃne action of
Part (3).
Use Matlab to ﬁnd the eigenvalues of the matrix
T =






2 −2 1 −1 0
0
2 0 1
0
−1 1 0 0 −2
0 −1 0 0 −1
0
0 0 0
1






.
You will be stunned!
Let N be the matrix given by
N = T −I.
Prove that
N 4 = 0.
Use this to prove that
T k = I + kN + 1
2k(k −1)N 2 + 1
6k(k −1)(k −2)N 3,
for all k ≥0.
(5) Prove that






uk
vk
xk
yk
1






= T k






−1
−1
−2
−1
1






=






2 −2 1 −1 0
0
2 0 1
0
−1 1 0 0 −2
0 −1 0 0 −1
0
0 0 0
1






k 





−1
−1
−2
−1
1






,
for k ≥0.
Prove that
T k =










k + 1 −k(k + 1)
k
−k2
1
6(k −1)k(2k −7)
0
k + 1
0
k
−1
2(k −1)k
−k
k2
1 −k (k −1)k −1
3k((k −6)k + 11)
0
−k
0
1 −k
1
2(k −3)k
0
0
0
0
1










,

14.7. Problems
563
and thus that







uk
vk
xk
yk







=







1
6(2k3 + 3k2 −5k −6)
−1
2(k2 + 3k + 2)
1
3(−k3 + k −6)
1
2(k2 + k −2)







,
and that

uk xk
vk yk
 = −1 −7
3k −23
12k2 −2
3k3 −1
12k4.
As a consequence, prove that amazingly
dn = Dn−2 = −1
12n2(n2 −1).
(6) Prove that the characteristic polynomial of A is indeed
PA(λ) = λn−2

λ2 −n2λ −1
12n2(n2 −1)

.
Use the above to show that the two nonzero eigenvalues of A are
λ = n
2
 
n ±
√
3
3
p
4n2 −1
!
.
The negative eigenvalue λ1 can also be expressed as
λ1 = n2 (3 −2
√
3)
6
r
1 −
1
4n2 .
Use this expression to explain the following phenomenon: if we add any
number greater than or equal to (2/25)n2 to every diagonal entry of A we
get an invertible matrix. What about 0.077351n2? Try it!
Problem 14.16. Let A be a symmetric tridiagonal n × n-matrix
A =










b1 c1
c1 b2 c2
c2 b3
c3
...
...
...
cn−2 bn−1 cn−1
cn−1
bn










,

564
Eigenvectors and Eigenvalues
where it is assumed that ci ̸= 0 for all i, 1 ≤i ≤n −1, and let Ak be the
k ×k-submatrix consisting of the ﬁrst k rows and columns of A, 1 ≤k ≤n.
We deﬁne the polynomials Pk(x) as follows: (0 ≤k ≤n).
P0(x) = 1,
P1(x) = b1 −x,
Pk(x) = (bk −x)Pk−1(x) −c2
k−1Pk−2(x),
where 2 ≤k ≤n.
(1) Prove the following properties:
(i) Pk(x) is the characteristic polynomial of Ak, where 1 ≤k ≤n.
(ii) limx→−∞Pk(x) = +∞, where 1 ≤k ≤n.
(iii) If Pk(x) = 0, then Pk−1(x)Pk+1(x) < 0, where 1 ≤k ≤n −1.
(iv) Pk(x) has k distinct real roots that separate the k + 1 roots of
Pk+1(x), where 1 ≤k ≤n −1.
(2) Given any real number µ > 0, for every k, 1 ≤k ≤n, deﬁne the
function sgk(µ) as follows:
sgk(µ) =
 sign of Pk(µ)
if Pk(µ) ̸= 0,
sign of Pk−1(µ)
if Pk(µ) = 0.
We encode the sign of a positive number as +, and the sign of a negative
number as −. Then let E(k, µ) be the ordered list
E(k, µ) = ⟨+, sg1(µ), sg2(µ), . . . , sgk(µ)⟩,
and let N(k, µ) be the number changes of sign between consecutive signs
in E(k, µ).
Prove that sgk(µ) is well deﬁned and that N(k, µ) is the number of roots
λ of Pk(x) such that λ < µ.
Remark: The above can be used to compute the eigenvalues of a (tridi-
agonal) symmetric matrix (the method of Givens-Householder).

Chapter 15
Unit Quaternions and Rotations in
SO(3)
This chapter is devoted to the representation of rotations in SO(3) in terms
of unit quaternions. Since we already deﬁned the unitary groups SU(n),
the quickest way to introduce the unit quaternions is to deﬁne them as the
elements of the group SU(2).
The skew ﬁeld H of quaternions and the group SU(2) of unit quater-
nions are discussed in Section 15.1. In Section 15.2, we deﬁne a homo-
morphism r: SU(2) →SO(3) and prove that its kernel is {−I, I}. We
compute the rotation matrix Rq associated with the rotation rq induced
by a unit quaternion q in Section 15.3. In Section 15.4, we prove that the
homomorphism r: SU(2) →SO(3) is surjective by providing an algorithm
to construct a quaternion from a rotation matrix. In Section 15.5 we deﬁne
the exponential map exp: su(2) →SU(2) where su(2) is the real vector
space of skew-Hermitian 2 × 2 matrices with zero trace. We prove that
exponential map exp: su(2) →SU(2) is surjective and give an algorithm
for ﬁnding a logarithm. We discuss quaternion interpolation and prove the
famous slerp interpolation formula due to Ken Shoemake in Section 15.6.
This formula is used in robotics and computer graphics to deal with inter-
polation problems. In Section 15.7, we prove that there is no "nice" section
s: SO(3) →SU(2) of the homomorphism r: SU(2) →SO(3), in the sense
that any section of r is neither a homomorphism nor continuous.
565

566
Unit Quaternions and Rotations in SO(3)
15.1
The Group SU(2) of Unit Quaternions and the Skew
Field H of Quaternions
Deﬁnition 15.1. The unit quaternions are the elements of the group
SU(2), namely the group of 2 × 2 complex matrices of the form
 α β
−β α

α, β ∈C, αα + ββ = 1.
The quaternions are the elements of the real vector space H = R SU(2).
Let 1, i, j, k be the matrices
1 =
1 0
0 1

,
i =
i 0
0 −i

,
j =
 0 1
−1 0

,
k =
0 i
i 0

,
then H is the set of all matrices of the form
X = a1 + bi + cj + dk,
a, b, c, d ∈R.
Indeed, every matrix in H is of the form
X =
 a + ib
c + id
−(c −id) a −ib

,
a, b, c, d ∈R.
It is easy (but a bit tedious) to verify that the quaternions 1, i, j, k
satisfy the famous identities discovered by Hamilton:
i2 = j2 = k2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Thus, the quaternions are a generalization of the complex numbers, but
there are three square roots of −1 and multiplication is not commutative.
Given any two quaternions X = a1 + bi + cj + dk and Y = a′1 + b′i +
c′j + d′k, Hamilton's famous formula
XY = (aa′ −bb′ −cc′ −dd′)1 + (ab′ + ba′ + cd′ −dc′)i
+ (ac′ + ca′ + db′ −bd′)j + (ad′ + da′ + bc′ −cb′)k
looks mysterious, but it is simply the result of multiplying the two matrices
X =
 a + ib
c + id
−(c −id) a −ib

and
Y =
 a′ + ib′
c′ + id′
−(c′ −id′) a′ −ib′

.

15.2. Representation of Rotations in SO(3) by Quaternions in SU(2)
567
It is worth noting that this formula was discovered independently by
Olinde Rodrigues in 1840, a few years before Hamilton (Veblen and Young
[Veblen and Young (1946)]). However, Rodrigues was working with a dif-
ferent formalism, homogeneous transformations, and he did not discover
the quaternions.
If
X =
 a + ib
c + id
−(c −id) a −ib

,
a, b, c, d ∈R,
it is immediately veriﬁed that
XX∗= X∗X = (a2 + b2 + c2 + d2)1.
Also observe that
X∗=
a −ib −(c + id)
c −id
a + ib

= a1 −bi −cj −dk.
This implies that if X ̸= 0, then X is invertible and its inverse is given
by
X−1 = (a2 + b2 + c2 + d2)−1X∗.
As a consequence, it can be veriﬁed that H is a skew ﬁeld (a noncom-
mutative ﬁeld). It is also a real vector space of dimension 4 with basis
(1, i, j, k); thus as a vector space, H is isomorphic to R4.
Deﬁnition 15.2. A concise notation for the quaternion X deﬁned by α =
a + ib and β = c + id is
X = [a, (b, c, d)].
We call a the scalar part of X and (b, c, d) the vector part of X. With this
notation, X∗= [a, −(b, c, d)], which is often denoted by X. The quaternion
X is called the conjugate of q.
If q is a unit quaternion, then q is the
multiplicative inverse of q.
15.2
Representation of Rotations in SO(3) by Quaternions
in SU(2)
The key to representation of rotations in SO(3) by unit quaternions is a
certain group homomorphism called the adjoint representation of SU(2).
To deﬁne this mapping, ﬁrst we deﬁne the real vector space su(2) of skew
Hermitian matrices.
Deﬁnition 15.3. The (real) vector space su(2) of 2 × 2 skew Hermitian
matrices with zero trace is given by
su(2) =

ix
y + iz
−y + iz −ix


(x, y, z) ∈R3

.

568
Unit Quaternions and Rotations in SO(3)
Observe that for every matrix A ∈su(2), we have A∗= −A, that is, A
is skew Hermitian, and that tr(A) = 0.
Deﬁnition 15.4. The adjoint representation of the group SU(2) is the
group homomorphism
Ad: SU(2) →GL(su(2)) deﬁned such that for every q ∈SU(2), with
q =
 α β
−β α

∈SU(2),
we have
Adq(A) = qAq∗,
A ∈su(2),
where q∗is the inverse of q (since SU(2) is a unitary group) and is given
by
q∗=
α −β
β α

.
One needs to verify that the map Adq is an invertible linear map from
su(2) to itself, and that Ad is a group homomorphism, which is easy to do.
In order to associate a rotation ρq (in SO(3)) to q, we need to embed
R3 into H as the pure quaternions, by
ψ(x, y, z) =

ix
y + iz
−y + iz −ix

,
(x, y, z) ∈R3.
Then q deﬁnes the map ρq (on R3) given by
ρq(x, y, z) = ψ−1(qψ(x, y, z)q∗).
Therefore, modulo the isomorphism ψ, the linear map ρq is the linear
isomorphism Adq. In fact, it turns out that ρq is a rotation (and so is Adq),
which we will prove shortly. So, the representation of rotations in SO(3)
by unit quaternions is just the adjoint representation of SU(2); its image
is a subgroup of GL(su(2)) isomorphic to SO(3).
Technically, it is a bit simpler to embed R3 in the (real) vector spaces
of Hermitian matrices with zero trace,

x
z −iy
z + iy
−x


x, y, z ∈R

.
Since the matrix ψ(x, y, z) is skew-Hermitian, the matrix −iψ(x, y, z) is
Hermitian, and we have
−iψ(x, y, z) =

x
z −iy
z + iy
−x

= xσ3 + yσ2 + zσ1,

15.2. Representation of Rotations in SO(3) by Quaternions in SU(2)
569
where σ1, σ2, σ3 are the Pauli spin matrices
σ1 =
0 1
1 0

,
σ2 =
0 −i
i 0

,
σ3 =
1 0
0 −1

.
Matrices of the form xσ3 + yσ2 + zσ1 are Hermitian matrices with zero
trace.
It is easy to see that every 2 × 2 Hermitian matrix with zero trace must
be of this form. (Observe that (iσ1, iσ2, iσ3) forms a basis of su(2). Also,
i = iσ3, j = iσ2, k = iσ1.)
Now, if A = xσ3 +yσ2 +zσ1 is a Hermitian 2×2 matrix with zero trace,
we have
(qAq∗)∗= qA∗q∗= qAq∗,
so qAq∗is also Hermitian, and
tr(qAq∗) = tr(Aq∗q) = tr(A),
and qAq∗also has zero trace. Therefore, the map A 7→qAq∗preserves the
Hermitian matrices with zero trace. We also have
det(xσ3 + yσ2 + zσ1) = det

x
z −iy
z + iy
−x

= −(x2 + y2 + z2),
and
det(qAq∗) = det(q) det(A) det(q∗) = det(A) = −(x2 + y2 + z2).
We can embed R3 into the space of Hermitian matrices with zero trace
by
ϕ(x, y, z) = xσ3 + yσ2 + zσ1.
Note that
ϕ = −iψ
and
ϕ−1 = iψ−1.
Deﬁnition 15.5. The unit quaternion q ∈SU(2) induces a map rq on R3
by
rq(x, y, z) = ϕ−1(qϕ(x, y, z)q∗) = ϕ−1(q(xσ3 + yσ2 + zσ1)q∗).
The map rq is clearly linear since ϕ is linear.
Proposition 15.1. For every unit quaternion q ∈SU(2), the linear map
rq is orthogonal, that is, rq ∈O(3).

570
Unit Quaternions and Rotations in SO(3)
Proof. Since
−∥(x, y, z)∥2 = −(x2 + y2 + z2) = det(xσ3 + yσ2 + zσ1) = det(ϕ(x, y, z)),
we have
−∥rq(x, y, z)∥2 = det(ϕ(rq(x, y, z))) = det(q(xσ3 + yσ2 + zσ1)q∗)
= det(xσ3 + yσ2 + zσ1) = −
(x, y, z)2 ,
and we deduce that rq is an isometry. Thus, rq ∈O(3).
In fact, rq is a rotation, and we can show this by ﬁnding the ﬁxed points
of rq. Let q be a unit quaternion of the form
q =
 α β
−β α

with α = a + ib, β = c + id, and a2 + b2 + c2 + d2 = 1 (a, b, c, d ∈R).
If b = c = d = 0, then q = I and rq is the identity so we may assume
that (b, c, d) ̸= (0, 0, 0).
Proposition 15.2. If (b, c, d) ̸= (0, 0, 0), then the ﬁxed points of rq are
solutions (x, y, z) of the linear system
−dy + cz = 0
cx −by = 0
dx −bz = 0.
This linear system has the nontrivial solution (b, c, d) and has rank 2.
Therefore, rq has the eigenvalue 1 with multiplicity 1, and rq is a rota-
tion whose axis is determined by (b, c, d).
Proof. We have rq(x, y, z) = (x, y, z) iﬀ
ϕ−1(q(xσ3 + yσ2 + zσ1)q∗) = (x, y, z)
iﬀ
q(xσ3 + yσ2 + zσ1)q∗= ϕ(x, y, z),
and since
ϕ(x, y, z) = xσ3 + yσ2 + zσ1 = A
with
A =

x
z −iy
z + iy
−x

,

15.2. Representation of Rotations in SO(3) by Quaternions in SU(2)
571
we see that rq(x, y, z) = (x, y, z) iﬀ
qAq∗= A
iﬀ
qA = Aq.
We have
qA =
 α β
−β α
 
x
z −iy
z + iy
−x

=
 αx + βz + iβy
αz −iαy −βx
−βx + αz + iαy −βz + iβy −αx

and
Aq =

x
z −iy
z + iy
−x
  α β
−β α

=
αx −βz + iβy βx + αz −iαy
αz + iαy + βx βz + iβy −αx

.
By equating qA and Aq, we get
i(β −β)y + (β + β)z = 0
2βx + i(α −α)y + (α −α)z = 0
2βx + i(α −α)y + (α −α)z = 0
i(β −β)y + (β + β)z = 0.
The ﬁrst and the fourth equation are identical and the third equation is
obtained by conjugating the second, so the above system reduces to
i(β −β)y + (β + β)z = 0
2βx + i(α −α)y + (α −α)z = 0.
Replacing α by a + ib and β by c + id, we get
−dy + cz = 0
cx −by + i(dx −bz) = 0,
which yields the equations
−dy + cz = 0
cx −by = 0
dx −bz = 0.
This linear system has the nontrivial solution (b, c, d) and the matrix of this
system is


0 −d c
c −b 0
d 0 −b

.
Since (b, c, d) ̸= (0, 0, 0), this matrix always has a 2 × 2 submatrix which
is nonsingular, so it has rank 2, and consequently its kernel is the one-
dimensional space spanned by (b, c, d). Therefore, rq has the eigenvalue
1 with multiplicity 1.
If we had det(rq) = −1, then the eigenvalues of
rq would be either (−1, 1, 1) or (−1, eiθ, e−iθ) with θ ̸= k2π (with k ∈Z),
contradicting the fact that 1 is an eigenvalue with multiplicity 1. Therefore,
rq is a rotation; in fact, its axis is determined by (b, c, d).

572
Unit Quaternions and Rotations in SO(3)
In summary, q 7→rq is a map r from SU(2) to SO(3).
Theorem 15.1. The map r: SU(2) →SO(3) is homomorphism whose
kernel is {I, −I}.
Proof. This map is a homomorphism, because if q1, q2 ∈SU(2), then
rq2(rq1(x, y, z)) = ϕ−1(q2ϕ(rq1(x, y, z))q∗
2)
= ϕ−1(q2ϕ(ϕ−1(q1ϕ(x, y, z)q∗
1))q∗
2)
= ϕ−1((q2q1)ϕ(x, y, z)(q2q1)∗)
= rq2q1(x, y, z).
The computation that showed that if (b, c, d) ̸= (0, 0, 0), then rq has the
eigenvalue 1 with multiplicity 1 implies the following: if rq = I3, namely
rq has the eigenvalue 1 with multiplicity 3, then (b, c, d) = (0, 0, 0). But
then a = ±1, and so q = ±I2. Therefore, the kernel of the homomorphism
r: SU(2) →SO(3) is {I, −I}.
Remark: Perhaps the quickest way to show that r maps SU(2) into SO(3)
is to observe that the map r is continuous. Then, since it is known that
SU(2) is connected, its image by r lies in the connected component of I,
namely SO(3).
The map r is surjective, but this is not obvious. We will return to this
point after ﬁnding the matrix representing rq explicitly.
15.3
Matrix Representation of the Rotation rq
Given a unit quaternion q of the form
q =
 α β
−β α

with α = a + ib, β = c + id, and a2 + b2 + c2 + d2 = 1 (a, b, c, d ∈R), to
ﬁnd the matrix representing the rotation rq we need to compute
q(xσ3 + yσ2 + zσ1)q∗=
 α β
−β α
 
x
z −iy
z + iy
−x
 α −β
β α

.
First we have

x
z −iy
z + iy
−x
 α −β
β α

=
xα + zβ −iyβ −xβ + zα −iyα
zα + iyα −xβ −zβ −iyβ −xα

.

15.3. Matrix Representation of the Rotation rq
573
Next, we have
 α β
−β α
 xα + zβ −iyβ −xβ + zα −iyα
zα + iyα −xβ −zβ −iyβ −xα

=
A1 A2
A3 A4

,
with
A1 = (αα −ββ)x + i(αβ −αβ)y + (αβ + αβ)z
A2 = −2αβx −i(α2 + β2)y + (α2 −β2)z
A3 = −2αβx + i(α2 + β
2)y + (α2 −β
2)z
A4 = −(αα −ββ)x −i(αβ −αβ)y −(αβ + αβ)z.
Since α = a + ib and β = c + id, with a, b, c, d ∈R, we have
αα −ββ = a2 + b2 −c2 −d2
i(αβ −αβ) = 2(bc −ad)
αβ + αβ = 2(ac + bd)
−αβ = −ac + bd −i(ad + bc)
−i(α2 + β2) = 2(ab + cd) −i(a2 −b2 + c2 −d2)
α2 −β2 = a2 −b2 −c2 + d2 + i2(ab −cd).
Using the above, we get
(αα −ββ)x + i(αβ −αβ)y + (αβ + αβ)z
= (a2 + b2 −c2 −d2)x + 2(bc −ad)y + 2(ac + bd)z,
and
−2αβx −i(α2 + β2)y + (α2 −β2)z
= 2(−ac + bd)x + 2(ab + cd)y + (a2 −b2 −c2 + d2)z
−i[2(ad + bc)x + (a2 −b2 + c2 −d2)y + 2(−ab + cd)z].
If we write
q(xσ3 + yσ2 + zσ1)q∗=

x′
z′ −iy′
z′ + iy′
−x′

,
we obtain
x′ = (a2 + b2 −c2 −d2)x + 2(bc −ad)y + 2(ac + bd)z
y′ = 2(ad + bc)x + (a2 −b2 + c2 −d2)y + 2(−ab + cd)z
z′ = 2(−ac + bd)x + 2(ab + cd)y + (a2 −b2 −c2 + d2)z.

574
Unit Quaternions and Rotations in SO(3)
In summary, we proved the following result.
Proposition 15.3. The matrix representing rq is
Rq =


a2 + b2 −c2 −d2
2bc −2ad
2ac + 2bd
2bc + 2ad
a2 −b2 + c2 −d2
−2ab + 2cd
−2ac + 2bd
2ab + 2cd
a2 −b2 −c2 + d2

.
Since a2 + b2 + c2 + d2 = 1, this matrix can also be written as
Rq =


2a2 + 2b2 −1
2bc −2ad
2ac + 2bd
2bc + 2ad
2a2 + 2c2 −1 −2ab + 2cd
−2ac + 2bd
2ab + 2cd
2a2 + 2d2 −1

.
The above is the rotation matrix in Euler form induced by the quater-
nion q, which is the matrix corresponding to ρq. This is because
ϕ = −iψ,
ϕ−1 = iψ−1,
so
rq(x, y, z) = ϕ−1(qϕ(x, y, z)q∗) = iψ−1(q(−iψ(x, y, z))q∗)
= ψ−1(qψ(x, y, z)q∗) = ρq(x, y, z),
and so rq = ρq.
We showed that every unit quaternion q ∈SU(2) induces a rotation
rq ∈SO(3), but it is not obvious that every rotation can be represented by
a quaternion. This can shown in various ways.
One way to is use the fact that every rotation in SO(3) is the composi-
tion of two reﬂections, and that every reﬂection σ of R3 can be represented
by a quaternion q, in the sense that
σ(x, y, z) = −ϕ−1(qϕ(x, y, z)q∗).
Note the presence of the negative sign. This is the method used in Gallier
[Gallier (2011b)] (Chapter 9).
15.4
An Algorithm to Find a Quaternion Representing a
Rotation
Theorem 15.2. The homomorphim r: SU(2) →SO(3) is surjective.

15.4. An Algorithm to Find a Quaternion Representing a Rotation
575
Here is an algorithmic method to ﬁnd a unit quaternion q representing
a rotation matrix R, which provides a proof of Theorem 15.2.
Let
q =
 a + ib
c + id
−(c −id) a −ib

,
a2 + b2 + c2 + d2 = 1, a, b, c, d ∈R.
First observe that the trace of Rq is given by
tr(Rq) = 3a2 −b2 −c2 −d2,
but since a2 + b2 + c2 + d2 = 1, we get tr(Rq) = 4a2 −1, so
a2 = tr(Rq) + 1
4
.
If R ∈SO(3) is any rotation matrix and if we write
R =


r11 r12 r13
r21 r22 r23
r31 r32 r33,


we are looking for a unit quaternion q ∈SU(2) such that Rq = R. There-
fore, we must have
a2 = tr(R) + 1
4
.
We also know that
tr(R) = 1 + 2 cos θ,
where θ ∈[0, π] is the angle of the rotation R, so we get
a2 = cos θ + 1
2
= cos2
θ
2

,
which implies that
|a| = cos
θ
2

(0 ≤θ ≤π).
Note that we may assume that θ ∈[0, π], because if π ≤θ ≤2π, then
θ −2π ∈[−π, 0], and then the rotation of angle θ −2π and axis determined
by the vector (b, c, d) is the same as the rotation of angle 2π −θ ∈[0, π]
and axis determined by the vector −(b, c, d). There are two cases.
Case 1. tr(R) ̸= −1, or equivalently θ ̸= π. In this case a ̸= 0. Pick
a =
p
tr(R) + 1
2
.

576
Unit Quaternions and Rotations in SO(3)
Then by equating R −R⊤and Rq −R⊤
q , we get
4ab = r32 −r23
4ac = r13 −r31
4ad = r21 −r12,
which yields
b = r32 −r23
4a
,
c = r13 −r31
4a
,
d = r21 −r12
4a
.
Case 2. tr(R) = −1, or equivalently θ = π. In this case a = 0. By
equating R + R⊤and Rq + R⊤
q , we get
4bc = r21 + r12
4bd = r13 + r31
4cd = r32 + r23.
By equating the diagonal terms of R and Rq, we also get
b2 = 1 + r11
2
c2 = 1 + r22
2
d2 = 1 + r33
2
.
Since q ̸= 0 and a = 0, at least one of b, c, d is nonzero.
If b ̸= 0, let
b =
√1 + r11
√
2
,
and determine c, d using
4bc = r21 + r12
4bd = r13 + r31.
If c ̸= 0, let
c =
√1 + r22
√
2
,
and determine b, d using
4bc = r21 + r12
4cd = r32 + r23.

15.4. An Algorithm to Find a Quaternion Representing a Rotation
577
If d ̸= 0, let
d =
√1 + r33
√
2
,
and determine b, c using
4bd = r13 + r31
4cd = r32 + r23.
It is easy to check that whenever we computed a square root, if we
had chosen a negative sign instead of a positive sign, we would obtain the
quaternion −q. However, both q and −q determine the same rotation rq.
The above discussion involving the cases tr(R) ̸= −1 and tr(R) = −1
is reminiscent of the procedure for ﬁnding a logarithm of a rotation matrix
using the Rodrigues formula (see Section 11.7).
This is not surprising,
because if
B =


0
−u3 u2
u3
0
−u1
−u2 u1
0


and if we write θ =
p
u2
1 + u2
2 + u2
3 (with 0 ≤θ ≤π), then the Rodrigues
formula says that
eB = I + sin θ
θ
B + (1 −cos θ)
θ2
B2,
θ ̸= 0,
with e0 = I. It is easy to check that tr(eB) = 1 + 2 cos θ. Then it is an
easy exercise to check that the quaternion q corresponding to the rotation
R = eB (with B ̸= 0) is given by
q =

cos
θ
2

, sin
θ
2
 u1
θ , u2
θ , u3
θ

.
So the method for ﬁnding the logarithm of a rotation R is essentially the
same as the method for ﬁnding a quaternion deﬁning R.
Remark: Geometrically, the group SU(2) is homeomorphic to the 3-
sphere S3 in R4,
S3 = {(x, y, z, t) ∈R4 | x2 + y2 + z2 + t2 = 1}.
However, since the kernel of the surjective homomorphism r: SU(2) →
SO(3) is {I, −I}, as a topological space, SO(3) is homeomorphic to the
quotient of S3 obtained by identifying antipodal points (x, y, z, t) and
−(x, y, z, t). This quotient space is the (real) projective space RP3, and
it is more complicated than S3. The space S3 is simply-connected, but
RP3 is not.

578
Unit Quaternions and Rotations in SO(3)
15.5
The Exponential Map exp: su(2) →SU(2)
Given any matrix A ∈su(2), with
A =

iu1
u2 + iu3
−u2 + iu3
−iu1

,
it is easy to check that
A2 = −θ2
1 0
0 1

,
with θ =
p
u2
1 + u2
2 + u2
3. Then we have the following formula whose proof
is very similar to the proof of the formula given in Proposition 8.17.
Proposition 15.4. For every matrix A ∈su(2), with
A =

iu1
u2 + iu3
−u2 + iu3
−iu1

,
if we write θ =
p
u2
1 + u2
2 + u2
3, then
eA = cos θI + sin θ
θ
A,
θ ̸= 0,
and e0 = I.
Therefore, by the discussion at the end of the previous section, eA is a
unit quaternion representing the rotation of angle 2θ and axis (u1, u2, u3)
(or I when θ = kπ, k ∈Z). The above formula shows that we may assume
that 0 ≤θ ≤π. Proposition 15.4 shows that the exponential yields a map
exp: su(2) →SU(2). It is an analog of the exponential map exp: so(3) →
SO(3).
Remark: Because so(3) and su(2) are real vector spaces of dimension 3,
they are isomorphic, and it is easy to construct an isomorphism. In fact,
so(3) and su(2) are isomorphic as Lie algebras, which means that there
is a linear isomorphism preserving the the Lie bracket [A, B] = AB −
BA. However, as observed earlier, the groups SU(2) and SO(3) are not
isomorphic.
An equivalent, but often more convenient, formula is obtained by as-
suming that u = (u1, u2, u3) is a unit vector, equivalently det(A) = 1, in
which case A2 = −I, so we have
eθA = cos θI + sin θA.

15.5. The Exponential Map exp: su(2) →SU(2)
579
Using the quaternion notation, this is read as
eθA = [cos θ, sin θ u].
Proposition 15.5. The exponential map exp: su(2) →SU(2) is surjective
Proof. We give an algorithm to ﬁnd the logarithm A ∈su(2) of a unit
quaternion
q =
 α β
−β α

with α = a + bi and β = c + id.
If q = I (i.e. a = 1), then A = 0. If q = −I (i.e. a = −1), then
A = ±π
i 0
0 −i

.
Otherwise, a ̸= ±1 and (b, c, d) ̸= (0, 0, 0), and we are seeking some A =
θB ∈su(2) with det(B) = 1 and 0 < θ < π, such that, by Proposition 15.4,
q = eθB = cos θI + sin θB.
Let
B =

iu1
u2 + iu3
−u2 + iu3
−iu1

,
with u = (u1, u2, u3) a unit vector. We must have
a = cos θ,
eθB −(eθB)∗= q −q∗.
Since 0 < θ < π, we have sin θ ̸= 0, and
2 sin θ

iu1
u2 + iu3
−u2 + iu3
−iu1

=
α −α
2β
−2β α −α

.
Thus, we get
u1 =
1
sin θ b,
u2 + iu3 =
1
sin θ(c + id);
that is,
cos θ = a
(0 < θ < π)
(u1, u2, u3) =
1
sin θ(b, c, d).
Since a2 + b2 + c2 + d2 = 1 and a = cos θ, the vector (b, c, d)/ sin θ is a unit
vector. Furthermore if the quaternion q is of the form q = [cos θ, sin θu]
where u = (u1, u2, u3) is a unit vector (with 0 < θ < π), then
A = θ

iu1
u2 + iu3
−u2 + iu3
−iu1

(15.1)
is a logarithm of q.

580
Unit Quaternions and Rotations in SO(3)
Observe that not only is the exponential map exp: su(2) →SU(2)
surjective, but the above proof shows that it is injective on the open ball
{θB ∈su(2) | det(B) = 1, 0 ≤θ < π}.
Also, unlike the situation where in computing the logarithm of a rotation
matrix R ∈SO(3) we needed to treat the case where tr(R) = −1 (the
angle of the rotation is π) in a special way, computing the logarithm of a
quaternion (other than ±I) does not require any case analysis; no special
case is needed when the angle of rotation is π.
15.6
Quaternion Interpolation ⊛
We are now going to derive a formula for interpolating between two quater-
nions. This formula is due to Ken Shoemake, once a Penn student and
my TA! Since rotations in SO(3) can be deﬁned by quaternions, this has
applications to computer graphics, robotics, and computer vision.
First we observe that multiplication of quaternions can be expressed
in terms of the inner product and the cross-product in R3.
Indeed, if
q1 = [a, u1] and q2 = [a2, u2], it can be veriﬁed that
q1q2 = [a1, u1][a2, u2] = [a1a2 −u1 · u2, a1u2 + a2u1 + u1 × u2].
(15.2)
We will also need the identity
u × (u × v) = (u · v)u −(u · u)v.
Given a quaternion q expressed as q = [cos θ, sin θ u], where u is a unit
vector, we can interpolate between I and q by ﬁnding the logs of I and q,
interpolating in su(2), and then exponentiating. We have
A = log(I) =
0 0
0 0

,
B = log(q) = θ

iu1
u2 + iu3
−u2 + iu3
−iu1

,
and so q = eB. Since SU(2) is a compact Lie group and since the inner
product on su(2) given by
⟨X, Y ⟩= tr(X⊤Y )
is Ad(SU(2))-invariant, it induces a biinvariant Riemannian metric on
SU(2), and the curve
λ 7→eλB,
λ ∈[0, 1]
is a geodesic from I to q in SU(2).
We write qλ = eλB.
Given two
quaternions q1 and q2, because the metric is left invariant, the curve
λ 7→Z(λ) = q1(q−1
1 q2)λ,
λ ∈[0, 1]

15.6. Quaternion Interpolation ⊛
581
is a geodesic from q1 to q2. Remarkably, there is a closed-form formula for
the interpolant Z(λ).
Say q1 = [cos θ, sin θ u] and q2 = [cos ϕ, sin ϕ v], and assume that q1 ̸= q2
and q1 ̸= −q2. First, we compute q−1q2. Since q−1 = [cos θ, −sin θ u], we
have
q−1q2 = [cos θ cos ϕ + sin θ sin ϕ(u · v),
−sin θ cos ϕ u + cos θ sin ϕ v −sin θ sin ϕ(u × v)].
Deﬁne Ωby
cos Ω= cos θ cos ϕ + sin θ sin ϕ(u · v).
(15.3)
Since q1 ̸= q2 and q1 ̸= −q2, we have 0 < Ω< π, so we get
q−1
1 q2 =

cos Ω, sin Ω(−sin θ cos ϕ u + cos θ sin ϕ v −sin θ sin ϕ(u × v)
sin Ω

,
where the term multiplying sin Ωis a unit vector because q1 and q2 are unit
quaternions, so q−1
1 q2 is also a unit quaternion. By (15.1), we have
(q−1
1 q2)λ
=

cos λΩ, sin λΩ(−sin θ cos ϕ u + cos θ sin ϕ v −sin θ sin ϕ(u × v)
sin Ω

.
Next we need to compute q1(q−1
1 q2)λ. The scalar part of this product is
s = cos θ cos λΩ+ sin λΩ
sin Ωsin2 θ cos ϕ(u · u) −sin λΩ
sin Ωsin θ sin ϕ cos θ(u · v)
+ sin λΩ
sin Ωsin2 θ sin ϕ(u · (u × v)).
Since u · (u × v) = 0, the last term is zero, and since u · u = 1 and
sin θ sin ϕ(u · v) = cos Ω−cos θ cos ϕ,
we get
s = cos θ cos λΩ+ sin λΩ
sin Ωsin2 θ cos ϕ −sin λΩ
sin Ωcos θ(cos Ω−cos θ cos ϕ)
= cos θ cos λΩ+ sin λΩ
sin Ω(sin2 θ + cos2 θ) cos ϕ −sin λΩ
sin Ωcos θ cos Ω
= (cos λΩsin Ω−sin λΩcos Ω) cos θ
sin Ω
+ sin λΩ
sin Ωcos ϕ
= sin(1 −λ)Ω
sin Ω
cos θ + sin λΩ
sin Ωcos ϕ.

582
Unit Quaternions and Rotations in SO(3)
The vector part of the product q1(q−1
1 q2)λ is given by
ν = −sin λΩ
sin Ωcos θ sin θ cos ϕ u + sin λΩ
sin Ωcos2 θ sin ϕ v
−sin λΩ
sin Ωcos θ sin θ sin ϕ(u × v) + cos λΩsin θ u
−sin λΩ
sin Ωsin2 θ cos ϕ(u × u) + sin λΩ
sin Ωcos θ sin θ sin ϕ(u × v)
−sin λΩ
sin Ωsin2 θ sin ϕ(u × (u × v)).
We have u × u = 0, the two terms involving u × v cancel out,
u × (u × v) = (u · v)u −(u · u)v,
and u · u = 1, so we get
ν = −sin λΩ
sin Ωcos θ sin θ cos ϕ u + cos λΩsin θ u + sin λΩ
sin Ωcos2 θ sin ϕ v
+ sin λΩ
sin Ωsin2 θ sin ϕ v −sin λΩ
sin Ωsin2 θ sin ϕ(u · v)u.
Using
sin θ sin ϕ(u · v) = cos Ω−cos θ cos ϕ,
we get
ν = −sin λΩ
sin Ωcos θ sin θ cos ϕ u + cos λΩsin θ u + sin λΩ
sin Ωsin ϕ v
−sin λΩ
sin Ωsin θ(cos Ω−cos θ cos ϕ)u
= cos λΩsin θ u + sin λΩ
sin Ωsin ϕ v −sin λΩ
sin Ωsin θ cos Ωu
= (cos λΩsin Ω−sin λΩcos Ω)
sin Ω
sin θ u + sin λΩ
sin Ωsin ϕ v
= sin(1 −λ)Ω
sin Ω
sin θ u + sin λΩ
sin Ωsin ϕ v.
Putting the scalar part and the vector part together, we obtain
q1(q−1
1 q2)λ =
sin(1 −λ)Ω
sin Ω
cos θ + sin λΩ
sin Ωcos ϕ,
sin(1 −λ)Ω
sin Ω
sin θ u + sin λΩ
sin Ωsin ϕ v

,
= sin(1 −λ)Ω
sin Ω
[cos θ, sin θ u] + sin λΩ
sin Ω[cos ϕ, sin ϕ v].
This yields the celebrated slerp interpolation formula
Z(λ) = q1(q−1
1 q2)λ = sin(1 −λ)Ω
sin Ω
q1 + sin λΩ
sin Ωq2,
with
cos Ω= cos θ cos ϕ + sin θ sin ϕ(u · v).

15.7. Nonexistence of a "Nice" Section from SO(3) to SU(2)
583
15.7
Nonexistence of a "Nice" Section from SO(3) to SU(2)
We conclude by discussing the problem of a consistent choice of sign for
the quaternion q representing a rotation R = ρq ∈SO(3). We are looking
for a "nice" section s: SO(3) →SU(2), that is, a function s satisfying the
condition
ρ ◦s = id,
where ρ is the surjective homomorphism ρ: SU(2) →SO(3).
Proposition 15.6. Any section s: SO(3) →SU(2) of ρ is neither a ho-
momorphism nor continuous.
Intuitively, this means that there is no "nice and simple " way to pick
the sign of the quaternion representing a rotation.
The following proof is due to Marcel Berger.
Proof. Let Γ be the subgroup of SU(2) consisting of all quaternions of the
form q = [a, (b, 0, 0)]. Then, using the formula for the rotation matrix Rq
corresponding to q (and the fact that a2 + b2 = 1), we get
Rq =


1
0
0
0 2a2 −1 −2ab
0
2ab
2a2 −1

.
Since a2 + b2 = 1, we may write a = cos θ, b = sin θ, and we see that
Rq =


1
0
0
0 cos 2θ −sin 2θ
0 sin 2θ
cos 2θ

,
a rotation of angle 2θ around the x-axis. Thus, both Γ and its image are
isomorphic to SO(2), which is also isomorphic to U(1) = {w ∈C | |w| = 1}.
By identifying i and i, and identifying Γ and its image to U(1), if we write
w = cos θ + i sin θ ∈Γ, the restriction of the map ρ to Γ is given by
ρ(w) = w2.
We claim that any section s of ρ is not a homomorphism. Consider
the restriction of s to U(1). Then since ρ ◦s = id and ρ(w) = w2, for
−1 ∈ρ(Γ) ≈U(1), we have
−1 = ρ(s(−1)) = (s(−1))2.
On the other hand, if s is a homomorphism, then
(s(−1))2 = s((−1)2) = s(1) = 1,

584
Unit Quaternions and Rotations in SO(3)
contradicting (s(−1))2 = −1.
We also claim that s is not continuous. Assume that s(1) = 1, the case
where s(1) = −1 being analogous. Then s is a bijection inverting ρ on Γ
whose restriction to U(1) must be given by
s(cos θ + i sin θ) = cos(θ/2) + i sin(θ/2),
−π ≤θ < π.
If θ tends to π, that is z = cos θ + i sin θ tends to −1 in the upper-half
plane, then s(z) tends to i, but if θ tends to −π, that is z tends to −1
in the lower-half plane, then s(z) tends to −i, which shows that s is not
continuous.
Another way (due to Jean Dieudonn´e) to prove that a section s of ρ is
not a homomorphism is to prove that any unit quaternion is the product
of two unit pure quaternions. Indeed, if q = [a, u] is a unit quaternion, if
we let q1 = [0, u1], where u1 is any unit vector orthogonal to u, then
q1q = [−u1 · u, au1 + u1 × u] = [0, au1 + u1 × u] = q2
is a nonzero unit pure quaternion. This is because if a ̸= 0 then au1 + u1 ×
u ̸= 0 (since u1 × u is orthogonal to au1 ̸= 0), and if a = 0 then u ̸= 0, so
u1 × u ̸= 0 (since u1 is orthogonal to u). But then, q−1
1
= [0, −u1] is a unit
pure quaternion and we have
q = q−1
1 q2,
a product of two pure unit quaternions.
We also observe that for any two pure quaternions q1, q2, there is some
unit quaternion q such that
q2 = qq1q−1.
This is just a restatement of the fact that the group SO(3) is transi-
tive.
Since the kernel of ρ: SU(2) →SO(3) is {I, −I}, the subgroup
s(SO(3)) would be a normal subgroup of index 2 in SU(2).
Then we
would have a surjective homomorphism η from SU(2) onto the quotient
group SU(2)/s(SO(3)), which is isomorphic to {1, −1}. Now, since any
two pure quaternions are conjugate of each other, η would have a constant
value on the unit pure quaternions. Since k = ij, we would have
η(k) = η(ij) = (η(i))2 = 1.
Consequently, η would map all pure unit quaternions to 1. But since every
unit quaternion is the product of two pure quaternions, η would map every
unit quaternion to 1, contradicting the fact that it is surjective onto {−1, 1}.

15.8. Summary
585
15.8
Summary
The main concepts and results of this chapter are listed below:
• The group SU(2) of unit quaternions.
• The skew ﬁeld H of quaternions.
• Hamilton's identities.
• The (real) vector space su(2) of 2 × 2 skew Hermitian matrices with
zero trace.
• The adjoint representation of SU(2).
• The (real) vector space su(2) of 2 × 2 Hermitian matrices with zero
trace.
• The group homomorphism r: SU(2) →SO(3); Ker (r) = {+I, −I}.
• The matrix representation Rq of the rotation rq induced by a unit
quaternion q.
• Surjectivity of the homomorphism r: SU(2) →SO(3).
• The exponential map exp: su(2) →SU(2).
• Surjectivity of the exponential map exp: su(2) →SU(2).
• Finding a logarithm of a quaternion.
• Quaternion interpolation.
• Shoemake's slerp interpolation formula.
• Sections s: SO(3) →SU(2) of r: SU(2) →SO(3).
15.9
Problems
Problem 15.1. Verify the quaternion identities
i2 = j2 = k2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Problem 15.2. Check that for every quaternion X = a1+bi+cj+dk, we
have
XX∗= X∗X = (a2 + b2 + c2 + d2)1.
Conclude that if X ̸= 0, then X is invertible and its inverse is given by
X−1 = (a2 + b2 + c2 + d2)−1X∗.

586
Unit Quaternions and Rotations in SO(3)
Problem 15.3. Given any two quaternions X = a1 + bi + cj + dk and
Y = a′1 + b′i + c′j + d′k, prove that
XY = (aa′ −bb′ −cc′ −dd′)1 + (ab′ + ba′ + cd′ −dc′)i
+ (ac′ + ca′ + db′ −bd′)j + (ad′ + da′ + bc′ −cb′)k.
Also prove that if X = [a, U] and Y = [a′, U ′], the quaternion product
XY can be expressed as
XY = [aa′ −U · U ′, aU ′ + a′U + U × U ′].
Problem 15.4. Let Ad: SU(2) →GL(su(2)) be the map deﬁned such
that for every q ∈SU(2),
Adq(A) = qAq∗,
A ∈su(2),
where q∗is the inverse of q (since SU(2) is a unitary group). Prove that
the map Adq is an invertible linear map from su(2) to itself and that Ad is
a group homomorphism.
Problem 15.5. Prove that every Hermitian matrix with zero trace is of
the form xσ3 + yσ2 + zσ1, with
σ1 =
0 1
1 0

,
σ2 =
0 −i
i 0

,
σ3 =
1 0
0 −1

.
Check that i = iσ3, j = iσ2, and that k = iσ1.
Problem 15.6. If
B =


0
−u3 u2
u3
0
−u1
−u2 u1
0

,
and if we write θ =
p
u2
1 + u2
2 + u2
3 (with 0 ≤θ ≤π), then the Rodrigues
formula says that
eB = I + sin θ
θ
B + (1 −cos θ)
θ2
B2,
θ ̸= 0,
with e0 = I. Check that tr(eB) = 1 + 2 cos θ. Prove that the quaternion q
corresponding to the rotation R = eB (with B ̸= 0) is given by
q =

cos
θ
2

, sin
θ
2
 u1
θ , u2
θ , u3
θ

.

15.9. Problems
587
Problem 15.7. For every matrix A ∈su(2), with
A =

iu1
u2 + iu3
−u2 + iu3
−iu1

,
prove that if we write θ =
p
u2
1 + u2
2 + u2
3, then
eA = cos θI + sin θ
θ
A,
θ ̸= 0,
and e0 = I. Conclude that eA is a unit quaternion representing the rotation
of angle 2θ and axis (u1, u2, u3) (or I when θ = kπ, k ∈Z).
Problem 15.8. Write a Matlab program implementing the method of Sec-
tion 15.4 for ﬁnding a unit quaternion corresponding to a rotation matrix.
Problem 15.9. Show that there is a very simple method for producing
an orthonormal frame in R4 whose ﬁrst vector is any given nonnull vector
(a, b, c, d).
Problem 15.10. Let i, j, and k, be the unit vectors of coordinates (1, 0, 0),
(0, 1, 0), and (0, 0, 1) in R3.
(1) Describe geometrically the rotations deﬁned by the following quater-
nions:
p = (0, i),
q = (0, j).
Prove that the interpolant Z(λ) = p(p−1q)λ is given by
Z(λ) = (0, cos(λπ/2)i + sin(λπ/2)j) .
Describe geometrically what this rotation is.
(2) Repeat Question (1) with the rotations deﬁned by the quaternions
p =
 
1
2,
√
3
2 i
!
,
q = (0, j).
Prove that the interpolant Z(λ) is given by
Z(λ) =
 
1
2 cos(λπ/2),
√
3
2 cos(λπ/2)i + sin(λπ/2)j
!
.
Describe geometrically what this rotation is.
(3) Repeat Question (1) with the rotations deﬁned by the quaternions
p =
 1
√
2, 1
√
2i

,
q =

0, 1
√
2(i + j)

.
Prove that the interpolant Z(λ) is given by
Z(λ) =
 1
√
2 cos(λπ/3) −1
√
6 sin(λπ/3),
(1/
√
2 cos(λπ/3) + 1/
√
6 sin(λπ/3))i + 2
√
6 sin(λπ/3)j

.

588
Unit Quaternions and Rotations in SO(3)
Problem 15.11. Prove that
w × (u × v) = (w · v)u −(u · w)v.
Conclude that
u × (u × v) = (u · v)u −(u · u)v.

Chapter 16
Spectral Theorems in Euclidean and
Hermitian Spaces
16.1
Introduction
The goal of this chapter is to show that there are nice normal forms for
symmetric matrices, skew-symmetric matrices, orthogonal matrices, and
normal matrices. The spectral theorem for symmetric matrices states that
symmetric matrices have real eigenvalues and that they can be diagonalized
over an orthonormal basis. The spectral theorem for Hermitian matrices
states that Hermitian matrices also have real eigenvalues and that they can
be diagonalized over a complex orthonormal basis. Normal real matrices
can be block diagonalized over an orthonormal basis with blocks having
size at most two and there are reﬁnements of this normal form for skew-
symmetric and orthogonal matrices.
The spectral result for real symmetric matrices can be used to prove two
characterizations of the eigenvalues of a symmetric matrix in terms of the
Rayleigh ratio. The ﬁrst characterization is the Rayleigh-Ritz theorem and
the second one is the Courant-Fischer theorem. Both results are used in
optimization theory and to obtain results about perturbing the eigenvalues
of a symmetric matrix.
In this chapter all vector spaces are ﬁnite-dimensional real or complex
vector spaces.
16.2
Normal Linear Maps: Eigenvalues and Eigenvectors
We begin by studying normal maps, to understand the structure of their
eigenvalues and eigenvectors.
This section and the next three were in-
spired by Lang [Lang (1993)], Artin [Artin (1991)], Mac Lane and Birkhoﬀ
[Mac Lane and Birkhoﬀ(1967)], Berger [Berger (1990a)], and Bertin [Bertin
(1981)].
589

590
Spectral Theorems in Euclidean and Hermitian Spaces
Deﬁnition 16.1. Given a Euclidean or Hermitian space E, a linear map
f : E →E is normal if
f ◦f ∗= f ∗◦f.
A linear map f : E →E is self-adjoint if f = f ∗, skew-self-adjoint if
f = −f ∗, and orthogonal if f ◦f ∗= f ∗◦f = id.
Obviously, a self-adjoint, skew-self-adjoint, or orthogonal linear map is
a normal linear map. Our ﬁrst goal is to show that for every normal linear
map f : E →E, there is an orthonormal basis (w.r.t. ⟨−, −⟩) such that the
matrix of f over this basis has an especially nice form: it is a block diagonal
matrix in which the blocks are either one-dimensional matrices (i.e., single
entries) or two-dimensional matrices of the form
 λ µ
−µ λ

.
This normal form can be further reﬁned if f is self-adjoint, skew-self-
adjoint, or orthogonal. As a ﬁrst step we show that f and f ∗have the same
kernel when f is normal.
Proposition 16.1. Given a Euclidean space E, if f : E →E is a normal
linear map, then Ker f = Ker f ∗.
Proof. First let us prove that
⟨f(u), f(v)⟩= ⟨f ∗(u), f ∗(v)⟩
for all u, v ∈E. Since f ∗is the adjoint of f and f ◦f ∗= f ∗◦f, we have
⟨f(u), f(u)⟩= ⟨u, (f ∗◦f)(u)⟩,
= ⟨u, (f ◦f ∗)(u)⟩,
= ⟨f ∗(u), f ∗(u)⟩.
Since ⟨−, −⟩is positive deﬁnite,
⟨f(u), f(u)⟩= 0
iﬀ
f(u) = 0,
⟨f ∗(u), f ∗(u)⟩= 0
iﬀ
f ∗(u) = 0,
and since
⟨f(u), f(u)⟩= ⟨f ∗(u), f ∗(u)⟩,
we have
f(u) = 0
iﬀ
f ∗(u) = 0.
Consequently, Ker f = Ker f ∗.

16.2. Normal Linear Maps: Eigenvalues and Eigenvectors
591
Assuming again that E is a Hermitian space, observe that Proposi-
tion 16.1 also holds. We deduce the following corollary.
Proposition 16.2. Given a Hermitian space E, for any normal linear map
f : E →E, we have Ker (f) ∩Im(f) = (0).
Proof. Assume v ∈Ker (f) ∩Im(f) = (0), which means that v = f(u) for
some u ∈E, and f(v) = 0. By Proposition 16.1, Ker (f) = Ker (f ∗), so
f(v) = 0 implies that f ∗(v) = 0. Consequently,
0 = ⟨f ∗(v), u⟩
= ⟨v, f(u)⟩
= ⟨v, v⟩,
and thus, v = 0.
We also have the following crucial proposition relating the eigenvalues
of f and f ∗.
Proposition 16.3. Given a Hermitian space E, for any normal linear map
f : E →E, a vector u is an eigenvector of f for the eigenvalue λ (in C) iﬀ
u is an eigenvector of f ∗for the eigenvalue λ.
Proof. First it is immediately veriﬁed that the adjoint of f−λ id is f ∗−λ id.
Furthermore, f −λ id is normal. Indeed,
(f −λ id) ◦(f −λ id)∗= (f −λ id) ◦(f ∗−λ id),
= f ◦f ∗−λf −λf ∗+ λλ id,
= f ∗◦f −λf ∗−λf + λλ id,
= (f ∗−λ id) ◦(f −λ id),
= (f −λ id)∗◦(f −λ id).
Applying Proposition 16.1 to f −λ id, for every nonnull vector u, we see
that
(f −λ id)(u) = 0
iﬀ
(f ∗−λ id)(u) = 0,
which is exactly the statement of the proposition.
The next proposition shows a very important property of normal linear
maps: eigenvectors corresponding to distinct eigenvalues are orthogonal.
Proposition 16.4. Given a Hermitian space E, for any normal linear map
f : E →E, if u and v are eigenvectors of f associated with the eigenvalues
λ and µ (in C) where λ ̸= µ, then ⟨u, v⟩= 0.

592
Spectral Theorems in Euclidean and Hermitian Spaces
Proof. Let us compute ⟨f(u), v⟩in two diﬀerent ways. Since v is an eigen-
vector of f for µ, by Proposition 16.3, v is also an eigenvector of f ∗for µ,
and we have
⟨f(u), v⟩= ⟨λu, v⟩= λ⟨u, v⟩,
and
⟨f(u), v⟩= ⟨u, f ∗(v)⟩= ⟨u, µv⟩= µ⟨u, v⟩,
where the last identity holds because of the semilinearity in the second
argument. Thus
λ⟨u, v⟩= µ⟨u, v⟩,
that is,
(λ −µ)⟨u, v⟩= 0,
which implies that ⟨u, v⟩= 0, since λ ̸= µ.
We can show easily that the eigenvalues of a self-adjoint linear map are
real.
Proposition 16.5. Given a Hermitian space E, all the eigenvalues of any
self-adjoint linear map f : E →E are real.
Proof. Let z (in C) be an eigenvalue of f and let u be an eigenvector for
z. We compute ⟨f(u), u⟩in two diﬀerent ways. We have
⟨f(u), u⟩= ⟨zu, u⟩= z⟨u, u⟩,
and since f = f ∗, we also have
⟨f(u), u⟩= ⟨u, f ∗(u)⟩= ⟨u, f(u)⟩= ⟨u, zu⟩= z⟨u, u⟩.
Thus,
z⟨u, u⟩= z⟨u, u⟩,
which implies that z = z, since u ̸= 0, and z is indeed real.
There is also a version of Proposition 16.5 for a (real) Euclidean space
E and a self-adjoint map f : E →E since every real vector space E can be
embedded into a complex vector space EC, and every linear map f : E →E
can be extended to a linear map fC : EC →EC.
Deﬁnition 16.2. Given a real vector space E, let EC be the structure
E × E under the addition operation
(u1, u2) + (v1, v2) = (u1 + v1, u2 + v2),
and let multiplication by a complex scalar z = x + iy be deﬁned such that
(x + iy) · (u, v) = (xu −yv, yu + xv).
The space EC is called the complexiﬁcation of E.

16.2. Normal Linear Maps: Eigenvalues and Eigenvectors
593
It is easily shown that the structure EC is a complex vector space. It is
also immediate that
(0, v) = i(v, 0),
and thus, identifying E with the subspace of EC consisting of all vectors of
the form (u, 0), we can write
(u, v) = u + iv.
Observe that if (e1, . . . , en) is a basis of E (a real vector space), then
(e1, . . . , en) is also a basis of EC (recall that ei is an abbreviation for (ei, 0)).
A linear map f : E →E is extended to the linear map fC : EC →EC
deﬁned such that
fC(u + iv) = f(u) + if(v).
For any basis (e1, . . . , en) of E, the matrix M(f) representing f
over (e1, . . . , en) is identical to the matrix M(fC) representing fC over
(e1, . . . , en), where we view (e1, . . . , en) as a basis of EC. As a consequence,
det(zI −M(f)) = det(zI −M(fC)), which means that f and fC have the
same characteristic polynomial (which has real coeﬃcients). We know that
every polynomial of degree n with real (or complex) coeﬃcients always
has n complex roots (counted with their multiplicity), and the roots of
det(zI −M(fC)) that are real (if any) are the eigenvalues of f.
Next we need to extend the inner product on E to an inner product on
EC.
The inner product ⟨−, −⟩on a Euclidean space E is extended to the
Hermitian positive deﬁnite form ⟨−, −⟩C on EC as follows:
⟨u1 + iv1, u2 + iv2⟩C = ⟨u1, u2⟩+ ⟨v1, v2⟩+ i(⟨v1, u2⟩−⟨u1, v2⟩).
It is easily veriﬁed that ⟨−, −⟩C is indeed a Hermitian form that is
positive deﬁnite, and it is clear that ⟨−, −⟩C agrees with ⟨−, −⟩on real
vectors. Then given any linear map f : E →E, it is easily veriﬁed that the
map f ∗
C deﬁnedsuch that
f ∗
C(u + iv) = f ∗(u) + if ∗(v)
for all u, v ∈E is the adjoint of fC w.r.t. ⟨−, −⟩C.
Proposition 16.6. Given a Euclidean space E, if f : E →E is any self-
adjoint linear map, then every eigenvalue λ of fC is real and is actually
an eigenvalue of f (which means that there is some real eigenvector u ∈E
such that f(u) = λu). Therefore, all the eigenvalues of f are real.

594
Spectral Theorems in Euclidean and Hermitian Spaces
Proof. Let EC be the complexiﬁcation of E, ⟨−, −⟩C the complexiﬁcation
of the inner product ⟨−, −⟩on E, and fC : EC →EC the complexiﬁcation
of f : E →E. By deﬁnition of fC and ⟨−, −⟩C, if f is self-adjoint, we have
⟨fC(u1 + iv1), u2 + iv2⟩C = ⟨f(u1) + if(v1), u2 + iv2⟩C
= ⟨f(u1), u2⟩+ ⟨f(v1), v2⟩
+ i(⟨u2, f(v1)⟩−⟨f(u1), v2⟩)
= ⟨u1, f(u2)⟩+ ⟨v1, f(v2)⟩
+ i(⟨f(u2), v1⟩−⟨u1, f(v2)⟩)
= ⟨u1 + iv1, f(u2) + if(v2)⟩C
= ⟨u1 + iv1, fC(u2 + iv2)⟩C,
which shows that fC is also self-adjoint with respect to ⟨−, −⟩C.
As we pointed out earlier, f and fC have the same characteristic polyno-
mial det(zI−fC) = det(zI−f), which is a polynomial with real coeﬃcients.
Proposition 16.5 shows that the zeros of det(zI −fC) = det(zI −f) are all
real, and for each real zero λ of det(zI −f), the linear map λid −f is sin-
gular, which means that there is some nonzero u ∈E such that f(u) = λu.
Therefore, all the eigenvalues of f are real.
Proposition 16.7. Given a Hermitian space E, for any linear map
f : E →E, if f is skew-self-adjoint, then f has eigenvalues that are pure
imaginary or zero, and if f is unitary, then f has eigenvalues of absolute
value 1.
Proof. If f is skew-self-adjoint, f ∗= −f, and then by the deﬁnition of the
adjoint map, for any eigenvalue λ and any eigenvector u associated with λ,
we have
λ⟨u, u⟩= ⟨λu, u⟩= ⟨f(u), u⟩= ⟨u, f ∗(u)⟩= ⟨u, −f(u)⟩
= −⟨u, λu⟩= −λ⟨u, u⟩,
and since u ̸= 0 and ⟨−, −⟩is positive deﬁnite, ⟨u, u⟩̸= 0, so
λ = −λ,
which shows that λ = ir for some r ∈R.
If f is unitary, then f is an isometry, so for any eigenvalue λ and any
eigenvector u associated with λ, we have
|λ|2⟨u, u⟩= λλ⟨u, u⟩= ⟨λu, λu⟩= ⟨f(u), f(u)⟩= ⟨u, u⟩,
and since u ̸= 0, we obtain |λ|2 = 1, which implies
|λ| = 1.

16.3. Spectral Theorem for Normal Linear Maps
595
16.3
Spectral Theorem for Normal Linear Maps
Given a Euclidean space E, our next step is to show that for every linear
map f : E →E there is some subspace W of dimension 1 or 2 such that
f(W) ⊆W. When dim(W) = 1, the subspace W is actually an eigenspace
for some real eigenvalue of f. Furthermore, when f is normal, there is a
subspace W of dimension 1 or 2 such that f(W) ⊆W and f ∗(W) ⊆W.
The diﬃculty is that the eigenvalues of f are not necessarily real. One way
to get around this problem is to complexify both the vector space E and
the inner product ⟨−, −⟩as we did in Section 16.2.
Given any subspace W of a Euclidean space E, recall that the orthogonal
complement W ⊥of W is the subspace deﬁned such that
W ⊥= {u ∈E | ⟨u, w⟩= 0, for all w ∈W}.
Recall from Proposition 11.9 that E = W ⊕W ⊥(this can be easily shown,
for example, by constructing an orthonormal basis of E using the Gram-
Schmidt orthonormalization procedure).
The same result also holds for
Hermitian spaces; see Proposition 13.12.
As a warm up for the proof of Theorem 16.2, let us prove that every
self-adjoint map on a Euclidean space can be diagonalized with respect to
an orthonormal basis of eigenvectors.
Theorem 16.1. (Spectral theorem for self-adjoint linear maps on a Eu-
clidean space) Given a Euclidean space E of dimension n, for every self-
adjoint linear map f : E →E, there is an orthonormal basis (e1, . . . , en) of
eigenvectors of f such that the matrix of f w.r.t. this basis is a diagonal
matrix





λ1
. . .
λ2 . . .
...
... ... ...
. . . λn




,
with λi ∈R.
Proof. We proceed by induction on the dimension n of E as follows. If
n = 1, the result is trivial. Assume now that n ≥2. From Proposition 16.6,
all the eigenvalues of f are real, so pick some eigenvalue λ ∈R, and let w
be some eigenvector for λ. By dividing w by its norm, we may assume
that w is a unit vector. Let W be the subspace of dimension 1 spanned by
w. Clearly, f(W) ⊆W. We claim that f(W ⊥) ⊆W ⊥, where W ⊥is the
orthogonal complement of W.

596
Spectral Theorems in Euclidean and Hermitian Spaces
Indeed, for any v ∈W ⊥, that is, if ⟨v, w⟩= 0, because f is self-adjoint
and f(w) = λw, we have
⟨f(v), w⟩= ⟨v, f(w)⟩
= ⟨v, λw⟩
= λ⟨v, w⟩= 0
since ⟨v, w⟩= 0. Therefore,
f(W ⊥) ⊆W ⊥.
Clearly, the restriction of f to W ⊥is self-adjoint, and we conclude by
applying the induction hypothesis to W ⊥(whose dimension is n −1).
We now come back to normal linear maps. One of the key points in the
proof of Theorem 16.1 is that we found a subspace W with the property
that f(W) ⊆W implies that f(W ⊥) ⊆W ⊥. In general, this does not
happen, but normal maps satisfy a stronger property which ensures that
such a subspace exists.
The following proposition provides a condition that will allow us to
show that a normal linear map can be diagonalized. It actually holds for
any linear map. We found the inspiration for this proposition in Berger
[Berger (1990a)].
Proposition 16.8. Given a Hermitian space E, for any linear map
f : E →E and any subspace W of E, if f(W) ⊆W, then f ∗ W ⊥
⊆W ⊥.
Consequently, if f(W) ⊆W and f ∗(W) ⊆W, then f
 W ⊥
⊆W ⊥and
f ∗ W ⊥
⊆W ⊥.
Proof. If u ∈W ⊥, then
⟨w, u⟩= 0
for all w ∈W.
However,
⟨f(w), u⟩= ⟨w, f ∗(u)⟩,
and f(W) ⊆W implies that f(w) ∈W. Since u ∈W ⊥, we get
0 = ⟨f(w), u⟩= ⟨w, f ∗(u)⟩,
which shows that ⟨w, f ∗(u)⟩= 0 for all w ∈W, that is, f ∗(u) ∈W ⊥.
Therefore, we have f ∗(W ⊥) ⊆W ⊥.
We just proved that if f(W) ⊆W, then f ∗ W ⊥
⊆W ⊥. If we also have
f ∗(W) ⊆W, then by applying the above fact to f ∗, we get f ∗∗(W ⊥) ⊆W ⊥,
and since f ∗∗= f, this is just f(W ⊥) ⊆W ⊥, which proves the second
statement of the proposition.

16.3. Spectral Theorem for Normal Linear Maps
597
It is clear that the above proposition also holds for Euclidean spaces.
Although we are ready to prove that for every normal linear map f
(over a Hermitian space) there is an orthonormal basis of eigenvectors (see
Theorem 16.3 below), we now return to real Euclidean spaces.
Proposition 16.9. If f : E →E is a linear map and w = u + iv is an
eigenvector of fC : EC →EC for the eigenvalue z = λ + iµ, where u, v ∈E
and λ, µ ∈R, then
f(u) = λu −µv
and
f(v) = µu + λv.
(16.1)
As a consequence,
fC(u −iv) = f(u) −if(v) = (λ −iµ)(u −iv),
which shows that w = u −iv is an eigenvector of fC for z = λ −iµ.
Proof. Since
fC(u + iv) = f(u) + if(v)
and
fC(u + iv) = (λ + iµ)(u + iv) = λu −µv + i(µu + λv),
we have
f(u) = λu −µv
and
f(v) = µu + λv.
Using this fact, we can prove the following proposition.
Proposition 16.10. Given a Euclidean space E, for any normal linear
map f : E →E, if w = u + iv is an eigenvector of fC associated with the
eigenvalue z = λ + iµ (where u, v ∈E and λ, µ ∈R), if µ ̸= 0 (i.e., z is
not real) then ⟨u, v⟩= 0 and ⟨u, u⟩= ⟨v, v⟩, which implies that u and v are
linearly independent, and if W is the subspace spanned by u and v, then
f(W) = W and f ∗(W) = W. Furthermore, with respect to the (orthogonal)
basis (u, v), the restriction of f to W has the matrix
 λ µ
−µ λ

.
If µ = 0, then λ is a real eigenvalue of f, and either u or v is an eigenvector
of f for λ. If W is the subspace spanned by u if u ̸= 0, or spanned by v ̸= 0
if u = 0, then f(W) ⊆W and f ∗(W) ⊆W.

598
Spectral Theorems in Euclidean and Hermitian Spaces
Proof. Since w = u + iv is an eigenvector of fC, by deﬁnition it is nonnull,
and either u ̸= 0 or v ̸= 0.
Proposition 16.9 implies that u −iv is an
eigenvector of fC for λ−iµ. It is easy to check that fC is normal. However,
if µ ̸= 0, then λ+iµ ̸= λ−iµ, and from Proposition 16.4, the vectors u+iv
and u −iv are orthogonal w.r.t. ⟨−, −⟩C, that is,
⟨u + iv, u −iv⟩C = ⟨u, u⟩−⟨v, v⟩+ 2i⟨u, v⟩= 0.
Thus we get ⟨u, v⟩= 0 and ⟨u, u⟩= ⟨v, v⟩, and since u ̸= 0 or v ̸= 0, u and
v are linearly independent. Since
f(u) = λu −µv
and
f(v) = µu + λv
and since by Proposition 16.3 u + iv is an eigenvector of f ∗
C for λ −iµ, we
have
f ∗(u) = λu + µv
and
f ∗(v) = −µu + λv,
and thus f(W) = W and f ∗(W) = W, where W is the subspace spanned
by u and v.
When µ = 0, we have
f(u) = λu
and
f(v) = λv,
and since u ̸= 0 or v ̸= 0, either u or v is an eigenvector of f for λ. If W is
the subspace spanned by u if u ̸= 0, or spanned by v if u = 0, it is obvious
that f(W) ⊆W and f ∗(W) ⊆W. Note that λ = 0 is possible, and this is
why ⊆cannot be replaced by =.
The beginning of the proof of Proposition 16.10 actually shows that for
every linear map f : E →E there is some subspace W such that f(W) ⊆W,
where W has dimension 1 or 2. In general, it doesn't seem possible to prove
that W ⊥is invariant under f. However, this happens when f is normal.
We can ﬁnally prove our ﬁrst main theorem.
Theorem 16.2. (Main spectral theorem) Given a Euclidean space E of
dimension n, for every normal linear map f : E →E, there is an orthonor-
mal basis (e1, . . . , en) such that the matrix of f w.r.t. this basis is a block
diagonal matrix of the form





A1
. . .
A2 . . .
...
...
...
...
. . . Ap






16.3. Spectral Theorem for Normal Linear Maps
599
such that each block Aj is either a one-dimensional matrix (i.e., a real
scalar) or a two-dimensional matrix of the form
Aj =
λj −µj
µj λj

,
where λj, µj ∈R, with µj > 0.
Proof. We proceed by induction on the dimension n of E as follows. If
n = 1, the result is trivial. Assume now that n ≥2. First, since C is
algebraically closed (i.e., every polynomial has a root in C), the linear map
fC : EC →EC has some eigenvalue z = λ + iµ (where λ, µ ∈R).
Let
w = u + iv be some eigenvector of fC for λ + iµ (where u, v ∈E). We can
now apply Proposition 16.10.
If µ = 0, then either u or v is an eigenvector of f for λ ∈R. Let W
be the subspace of dimension 1 spanned by e1 = u/∥u∥if u ̸= 0, or by
e1 = v/∥v∥otherwise. It is obvious that f(W) ⊆W and f ∗(W) ⊆W. The
orthogonal W ⊥of W has dimension n −1, and by Proposition 16.8, we
have f
 W ⊥
⊆W ⊥. But the restriction of f to W ⊥is also normal, and
we conclude by applying the induction hypothesis to W ⊥.
If µ ̸= 0, then ⟨u, v⟩= 0 and ⟨u, u⟩= ⟨v, v⟩, and if W is the subspace
spanned by u/∥u∥and v/∥v∥, then f(W) = W and f ∗(W) = W. We also
know that the restriction of f to W has the matrix
 λ µ
−µ λ

with respect to the basis (u/∥u∥, v/∥v∥). If µ < 0, we let λ1 = λ, µ1 = −µ,
e1 = u/∥u∥, and e2 = v/∥v∥. If µ > 0, we let λ1 = λ, µ1 = µ, e1 = v/∥v∥,
and e2 = u/∥u∥. In all cases, it is easily veriﬁed that the matrix of the
restriction of f to W w.r.t. the orthonormal basis (e1, e2) is
A1 =
λ1 −µ1
µ1 λ1

,
where λ1, µ1 ∈R, with µ1 > 0. However, W ⊥has dimension n −2, and by
Proposition 16.8, f
 W ⊥
⊆W ⊥. Since the restriction of f to W ⊥is also
normal, we conclude by applying the induction hypothesis to W ⊥.
After this relatively hard work, we can easily obtain some nice normal
forms for the matrices of self-adjoint, skew-self-adjoint, and orthogonal lin-
ear maps. However, for the sake of completeness (and since we have all the
tools to so do), we go back to the case of a Hermitian space and show that

600
Spectral Theorems in Euclidean and Hermitian Spaces
normal linear maps can be diagonalized with respect to an orthonormal
basis. The proof is a slight generalization of the proof of Theorem 16.6.
Theorem 16.3. (Spectral theorem for normal linear maps on a Hermitian
space) Given a Hermitian space E of dimension n, for every normal linear
map f : E →E there is an orthonormal basis (e1, . . . , en) of eigenvectors
of f such that the matrix of f w.r.t. this basis is a diagonal matrix





λ1
. . .
λ2 . . .
...
... ... ...
. . . λn




,
where λj ∈C.
Proof. We proceed by induction on the dimension n of E as follows. If
n = 1, the result is trivial. Assume now that n ≥2. Since C is algebraically
closed (i.e., every polynomial has a root in C), the linear map f : E →E
has some eigenvalue λ ∈C, and let w be some unit eigenvector for λ. Let
W be the subspace of dimension 1 spanned by w. Clearly, f(W) ⊆W. By
Proposition 16.3, w is an eigenvector of f ∗for λ, and thus f ∗(W) ⊆W.
By Proposition 16.8, we also have f(W ⊥) ⊆W ⊥. The restriction of f to
W ⊥is still normal, and we conclude by applying the induction hypothesis
to W ⊥(whose dimension is n −1).
Theorem 16.3 implies that (complex) self-adjoint, skew-self-adjoint, and
orthogonal linear maps can be diagonalized with respect to an orthonormal
basis of eigenvectors.
In this latter case, though, an orthogonal map is
called a unitary map.
Proposition 16.5 also shows that the eigenvalues
of a self-adjoint linear map are real, and Proposition 16.7 shows that the
eigenvalues of a skew self-adjoint map are pure imaginary or zero, and that
the eigenvalues of a unitary map have absolute value 1.
Remark: There is a converse to Theorem 16.3, namely, if there is an or-
thonormal basis (e1, . . . , en) of eigenvectors of f, then f is normal. We
leave the easy proof as an exercise.
In the next section we specialize Theorem 16.2 to self-adjoint, skew-self-
adjoint, and orthogonal linear maps. Due to the additional structure, we
obtain more precise normal forms.

16.4. Self-Adjoint, Skew-Self-Adjoint, and Orthogonal Linear Maps
601
16.4
Self-Adjoint, Skew-Self-Adjoint, and Orthogonal
Linear Maps
We begin with self-adjoint maps.
Theorem 16.4. Given a Euclidean space E of dimension n, for every self-
adjoint linear map f : E →E, there is an orthonormal basis (e1, . . . , en) of
eigenvectors of f such that the matrix of f w.r.t. this basis is a diagonal
matrix





λ1
. . .
λ2 . . .
...
... ... ...
. . . λn




,
where λi ∈R.
Proof. We already proved this; see Theorem 16.1. However, it is instruc-
tive to give a more direct method not involving the complexiﬁcation of
⟨−, −⟩and Proposition 16.5.
Since C is algebraically closed, fC has some eigenvalue λ + iµ, and let
u + iv be some eigenvector of fC for λ + iµ, where λ, µ ∈R and u, v ∈E.
We saw in the proof of Proposition 16.9 that
f(u) = λu −µv
and
f(v) = µu + λv.
Since f = f ∗,
⟨f(u), v⟩= ⟨u, f(v)⟩
for all u, v ∈E. Applying this to
f(u) = λu −µv
and
f(v) = µu + λv,
we get
⟨f(u), v⟩= ⟨λu −µv, v⟩= λ⟨u, v⟩−µ⟨v, v⟩
and
⟨u, f(v)⟩= ⟨u, µu + λv⟩= µ⟨u, u⟩+ λ⟨u, v⟩,
and thus we get
λ⟨u, v⟩−µ⟨v, v⟩= µ⟨u, u⟩+ λ⟨u, v⟩,
that is,
µ(⟨u, u⟩+ ⟨v, v⟩) = 0,

602
Spectral Theorems in Euclidean and Hermitian Spaces
which implies µ = 0, since either u ̸= 0 or v ̸= 0. Therefore, λ is a real
eigenvalue of f.
Now going back to the proof of Theorem 16.2, only the case where µ = 0
applies, and the induction shows that all the blocks are one-dimensional.
Theorem 16.4 implies that if λ1, . . . , λp are the distinct real eigenvalues
of f, and Ei is the eigenspace associated with λi, then
E = E1 ⊕· · · ⊕Ep,
where Ei and Ej are orthogonal for all i ̸= j.
Remark: Another way to prove that a self-adjoint map has a real eigen-
value is to use a little bit of calculus. We learned such a proof from Herman
Gluck. The idea is to consider the real-valued function Φ: E →R deﬁned
such that
Φ(u) = ⟨f(u), u⟩
for every u ∈E. This function is C∞, and if we represent f by a matrix A
over some orthonormal basis, it is easy to compute the gradient vector
∇Φ(X) =
 ∂Φ
∂x1
(X), . . . , ∂Φ
∂xn
(X)

of Φ at X. Indeed, we ﬁnd that
∇Φ(X) = (A + A⊤)X,
where X is a column vector of size n. But since f is self-adjoint, A = A⊤,
and thus
∇Φ(X) = 2AX.
The next step is to ﬁnd the maximum of the function Φ on the sphere
Sn−1 = {(x1, . . . , xn) ∈Rn | x2
1 + · · · + x2
n = 1}.
Since Sn−1 is compact and Φ is continuous, and in fact C∞, Φ takes a
maximum at some X on Sn−1.
But then it is well known that at an
extremum X of Φ we must have
dΦX(Y ) = ⟨∇Φ(X), Y ⟩= 0
for all tangent vectors Y to Sn−1 at X, and so ∇Φ(X) is orthogonal to the
tangent plane at X, which means that
∇Φ(X) = λX

16.4. Self-Adjoint, Skew-Self-Adjoint, and Orthogonal Linear Maps
603
for some λ ∈R. Since ∇Φ(X) = 2AX, we get
2AX = λX,
and thus λ/2 is a real eigenvalue of A (i.e., of f).
Next we consider skew-self-adjoint maps.
Theorem 16.5. Given a Euclidean space E of dimension n, for every skew-
self-adjoint linear map f : E →E there is an orthonormal basis (e1, . . . , en)
such that the matrix of f w.r.t. this basis is a block diagonal matrix of the
form





A1
. . .
A2 . . .
...
...
...
...
. . . Ap





such that each block Aj is either 0 or a two-dimensional matrix of the form
Aj =
 0 −µj
µj
0

,
where µj ∈R, with µj > 0. In particular, the eigenvalues of fC are pure
imaginary of the form ±iµj or 0.
Proof. The case where n = 1 is trivial. As in the proof of Theorem 16.2,
fC has some eigenvalue z = λ + iµ, where λ, µ ∈R. We claim that λ = 0.
First we show that
⟨f(w), w⟩= 0
for all w ∈E. Indeed, since f = −f ∗, we get
⟨f(w), w⟩= ⟨w, f ∗(w)⟩= ⟨w, −f(w)⟩= −⟨w, f(w)⟩= −⟨f(w), w⟩,
since ⟨−, −⟩is symmetric. This implies that
⟨f(w), w⟩= 0.
Applying this to u and v and using the fact that
f(u) = λu −µv
and
f(v) = µu + λv,
we get
0 = ⟨f(u), u⟩= ⟨λu −µv, u⟩= λ⟨u, u⟩−µ⟨u, v⟩

604
Spectral Theorems in Euclidean and Hermitian Spaces
and
0 = ⟨f(v), v⟩= ⟨µu + λv, v⟩= µ⟨u, v⟩+ λ⟨v, v⟩,
from which, by addition, we get
λ(⟨v, v⟩+ ⟨v, v⟩) = 0.
Since u ̸= 0 or v ̸= 0, we have λ = 0.
Then going back to the proof of Theorem 16.2, unless µ = 0, the case
where u and v are orthogonal and span a subspace of dimension 2 applies,
and the induction shows that all the blocks are two-dimensional or reduced
to 0.
Remark: One will note that if f is skew-self-adjoint, then ifC is self-adjoint
w.r.t. ⟨−, −⟩C. By Proposition 16.5, the map ifC has real eigenvalues, which
implies that the eigenvalues of fC are pure imaginary or 0.
Finally we consider orthogonal linear maps.
Theorem 16.6. Given a Euclidean space E of dimension n, for every
orthogonal linear map f : E →E there is an orthonormal basis (e1, . . . , en)
such that the matrix of f w.r.t. this basis is a block diagonal matrix of the
form





A1
. . .
A2 . . .
...
...
...
...
. . . Ap





such that each block Aj is either 1, −1, or a two-dimensional matrix of the
form
Aj =
cos θj −sin θj
sin θj
cos θj

where 0 < θj < π. In particular, the eigenvalues of fC are of the form
cos θj ± i sin θj, 1, or −1.
Proof. The case where n = 1 is trivial. It is immediately veriﬁed that
f ◦f ∗= f ∗◦f = id implies that fC ◦f ∗
C = f ∗
C ◦fC = id, so the map fC is
unitary. By Proposition 16.7, the eigenvalues of fC have absolute value 1.
As a consequence, the eigenvalues of fC are of the form cos θ ± i sin θ, 1, or
−1. The theorem then follows immediately from Theorem 16.2, where the
condition µ > 0 implies that sin θj > 0, and thus, 0 < θj < π.

16.4. Self-Adjoint, Skew-Self-Adjoint, and Orthogonal Linear Maps
605
It is obvious that we can reorder the orthonormal basis of eigenvectors
given by Theorem 16.6, so that the matrix of f w.r.t. this basis is a block
diagonal matrix of the form







A1 . . .
...
...
...
...
. . . Ar
−Iq
. . .
Ip







where each block Aj is a two-dimensional rotation matrix Aj ̸= ±I2 of the
form
Aj =
cos θj −sin θj
sin θj
cos θj

with 0 < θj < π.
The linear map f has an eigenspace E(1, f) = Ker (f −id) of dimen-
sion p for the eigenvalue 1, and an eigenspace E(−1, f) = Ker (f + id) of
dimension q for the eigenvalue −1. If det(f) = +1 (f is a rotation), the
dimension q of E(−1, f) must be even, and the entries in −Iq can be paired
to form two-dimensional blocks, if we wish. In this case, every rotation in
SO(n) has a matrix of the form





A1 . . .
...
...
...
. . . Am
. . .
In−2m





where the ﬁrst m blocks Aj are of the form
Aj =
cos θj −sin θj
sin θj
cos θj

with 0 < θj ≤π.
Theorem 16.6 can be used to prove a version of the Cartan-Dieudonn´e
theorem.
Theorem 16.7. Let E be a Euclidean space of dimension n ≥2. For every
isometry f ∈O(E), if p = dim(E(1, f)) = dim(Ker (f −id)), then f is the
composition of n −p reﬂections, and n −p is minimal.
Proof. From Theorem 16.6 there are r subspaces F1, . . . , Fr, each of di-
mension 2, such that
E = E(1, f) ⊕E(−1, f) ⊕F1 ⊕· · · ⊕Fr,

606
Spectral Theorems in Euclidean and Hermitian Spaces
and all the summands are pairwise orthogonal. Furthermore, the restriction
ri of f to each Fi is a rotation ri ̸= ±id. Each 2D rotation ri can be written
as the composition ri = s′
i ◦si of two reﬂections si and s′
i about lines in Fi
(forming an angle θi/2). We can extend si and s′
i to hyperplane reﬂections
in E by making them the identity on F ⊥
i . Then
s′
r ◦sr ◦· · · ◦s′
1 ◦s1
agrees with f on F1 ⊕· · · ⊕Fr and is the identity on E(1, f) ⊕E(−1, f).
If E(−1, f) has an orthonormal basis of eigenvectors (v1, . . . , vq), letting s′′
j
be the reﬂection about the hyperplane (vj)⊥, it is clear that
s′′
q ◦· · · ◦s′′
1
agrees with f on E(−1, f) and is the identity on E(1, f) ⊕F1 ⊕· · · ⊕Fr.
But then
f = s′′
q ◦· · · ◦s′′
1 ◦s′
r ◦sr ◦· · · ◦s′
1 ◦s1,
the composition of 2r + q = n −p reﬂections.
If
f = st ◦· · · ◦s1,
for t reﬂections si, it is clear that
F =
t\
i=1
E(1, si) ⊆E(1, f),
where E(1, si) is the hyperplane deﬁning the reﬂection si. By the Grass-
mann relation, if we intersect t ≤n hyperplanes, the dimension of their
intersection is at least n −t. Thus, n −t ≤p, that is, t ≥n −p, and n −p
is the smallest number of reﬂections composing f.
As a corollary of Theorem 16.7, we obtain the following fact: If the
dimension n of the Euclidean space E is odd, then every rotation f ∈
SO(E) admits 1 as an eigenvalue.
Proof. The characteristic polynomial det(XI −f) of f has odd degree
n and has real coeﬃcients, so it must have some real root λ.
Since f
is an isometry, its n eigenvalues are of the form, +1, −1, and e±iθ, with
0 < θ < π, so λ = ±1. Now the eigenvalues e±iθ appear in conjugate
pairs, and since n is odd, the number of real eigenvalues of f is odd. This
implies that +1 is an eigenvalue of f, since otherwise −1 would be the
only real eigenvalue of f, and since its multiplicity is odd, we would have
det(f) = −1, contradicting the fact that f is a rotation.

16.5. Normal and Other Special Matrices
607
When n = 3, we obtain the result due to Euler which says that every
3D rotation R has an invariant axis D, and that restricted to the plane
orthogonal to D, it is a 2D rotation.
Furthermore, if (a, b, c) is a unit
vector deﬁning the axis D of the rotation R and if the angle of the rotation
is θ, if B is the skew-symmetric matrix
B =


0 −c b
c
0 −a
−b a
0

,
then the Rodigues formula (Proposition 11.13) states that
R = I + sin θB + (1 −cos θ)B2.
The theorems of this section and of the previous section can be immedi-
ately translated in terms of matrices. The matrix versions of these theorems
is often used in applications so we brieﬂy present them in the section.
16.5
Normal and Other Special Matrices
First we consider real matrices. Recall the following deﬁnitions.
Deﬁnition 16.3. Given a real m × n matrix A, the transpose A⊤of A is
the n × m matrix A⊤= (a⊤
i j) deﬁned such that
a⊤
i j = aj i
for all i, j, 1 ≤i ≤m, 1 ≤j ≤n. A real n × n matrix A is
• normal if
A A⊤= A⊤A,
• symmetric if
A⊤= A,
• skew-symmetric if
A⊤= −A,
• orthogonal if
A A⊤= A⊤A = In.

608
Spectral Theorems in Euclidean and Hermitian Spaces
Recall from Proposition 11.12 that when E is a Euclidean space and
(e1, . . ., en) is an orthonormal basis for E, if A is the matrix of a linear
map f : E →E w.r.t. the basis (e1, . . . , en), then A⊤is the matrix of the
adjoint f ∗of f. Consequently, a normal linear map has a normal matrix,
a self-adjoint linear map has a symmetric matrix, a skew-self-adjoint linear
map has a skew-symmetric matrix, and an orthogonal linear map has an
orthogonal matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P
is the change of basis matrix whose columns are the components of the ui
w.r.t. the basis (e1, . . . , en), then P is orthogonal, and for any linear map
f : E →E, if A is the matrix of f w.r.t. (e1, . . . , en) and B is the matrix of
f w.r.t. (u1, . . . , un), then
B = P ⊤AP.
As a consequence, Theorems 16.2 and 16.4-16.6 can be restated as fol-
lows.
Theorem 16.8. For every normal matrix A there is an orthogonal matrix
P and a block diagonal matrix D such that A = PD P ⊤, where D is of the
form
D =





D1
. . .
D2 . . .
...
...
...
...
. . . Dp





such that each block Dj is either a one-dimensional matrix (i.e., a real
scalar) or a two-dimensional matrix of the form
Dj =
λj −µj
µj λj

,
where λj, µj ∈R, with µj > 0.
Theorem 16.9. For every symmetric matrix A there is an orthogonal ma-
trix P and a diagonal matrix D such that A = PD P ⊤, where D is of the
form
D =





λ1
. . .
λ2 . . .
...
... ... ...
. . . λn




,
where λi ∈R.

16.5. Normal and Other Special Matrices
609
Theorem 16.10. For every skew-symmetric matrix A there is an orthogo-
nal matrix P and a block diagonal matrix D such that A = PD P ⊤, where
D is of the form
D =





D1
. . .
D2 . . .
...
...
...
...
. . . Dp





such that each block Dj is either 0 or a two-dimensional matrix of the form
Dj =
 0 −µj
µj
0

,
where µj ∈R, with µj > 0. In particular, the eigenvalues of A are pure
imaginary of the form ±iµj, or 0.
Theorem 16.11. For every orthogonal matrix A there is an orthogonal
matrix P and a block diagonal matrix D such that A = PD P ⊤, where D
is of the form
D =





D1
. . .
D2 . . .
...
...
...
...
. . . Dp





such that each block Dj is either 1, −1, or a two-dimensional matrix of the
form
Dj =
cos θj −sin θj
sin θj
cos θj

where 0 < θj < π. In particular, the eigenvalues of A are of the form
cos θj ± i sin θj, 1, or −1.
Theorem 16.11 can be used to show that the exponential map
exp: so(n) →SO(n) is surjective; see Gallier [Gallier (2011b)].
We now consider complex matrices.
Deﬁnition 16.4. Given a complex m × n matrix A, the transpose A⊤of
A is the n × m matrix A⊤=
 a⊤
i j

deﬁned such that
a⊤
i j = aj i
for all i, j, 1 ≤i ≤m, 1 ≤j ≤n. The conjugate A of A is the m×n matrix
A = (bi j) deﬁned such that
bi j = ai j

610
Spectral Theorems in Euclidean and Hermitian Spaces
for all i, j, 1 ≤i ≤m, 1 ≤j ≤n. Given an m × n complex matrix A, the
adjoint A∗of A is the matrix deﬁned such that
A∗= (A⊤) = (A)⊤.
A complex n × n matrix A is
• normal if
AA∗= A∗A,
• Hermitian if
A∗= A,
• skew-Hermitian if
A∗= −A,
• unitary if
AA∗= A∗A = In.
Recall from Proposition 13.14 that when E is a Hermitian space and
(e1, . . ., en) is an orthonormal basis for E, if A is the matrix of a linear
map f : E →E w.r.t. the basis (e1, . . . , en), then A∗is the matrix of the
adjoint f ∗of f. Consequently, a normal linear map has a normal matrix,
a self-adjoint linear map has a Hermitian matrix, a skew-self-adjoint linear
map has a skew-Hermitian matrix, and a unitary linear map has a unitary
matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P
is the change of basis matrix whose columns are the components of the
ui w.r.t. the basis (e1, . . . , en), then P is unitary, and for any linear map
f : E →E, if A is the matrix of f w.r.t. (e1, . . . , en) and B is the matrix of
f w.r.t. (u1, . . . , un), then
B = P ∗AP.
Theorem 16.3 and Proposition 16.7 can be restated in terms of matrices
as follows.
Theorem 16.12. For every complex normal matrix A there is a unitary
matrix U and a diagonal matrix D such that A = UDU ∗. Furthermore,
if A is Hermitian, then D is a real matrix; if A is skew-Hermitian, then
the entries in D are pure imaginary or zero; and if A is unitary, then the
entries in D have absolute value 1.

16.6. Rayleigh-Ritz Theorems and Eigenvalue Interlacing
611
16.6
Rayleigh-Ritz Theorems and Eigenvalue Interlacing
A fact that is used frequently in optimization problems is that the eigen-
values of a symmetric matrix are characterized in terms of what is known
as the Rayleigh ratio, deﬁned by
R(A)(x) = x⊤Ax
x⊤x ,
x ∈Rn, x ̸= 0.
The following proposition is often used to prove the correctness of var-
ious optimization or approximation problems (for example PCA; see Sec-
tion 21.4). It is also used to prove Proposition 16.13, which is used to justify
the correctness of a method for graph-drawing (see Chapter 19).
Proposition 16.11. (Rayleigh-Ritz) If A is a symmetric n×n matrix with
eigenvalues λ1 ≤λ2 ≤· · · ≤λn and if (u1, . . . , un) is any orthonormal basis
of eigenvectors of A, where ui is a unit eigenvector associated with λi, then
max
x̸=0
x⊤Ax
x⊤x = λn
(with the maximum attained for x = un), and
max
x̸=0,x∈{un−k+1,...,un}⊥
x⊤Ax
x⊤x = λn−k
(with the maximum attained for x = un−k), where 1 ≤k ≤n −1. Equiva-
lently, if Vk is the subspace spanned by (u1, . . . , uk), then
λk =
max
x̸=0,x∈Vk
x⊤Ax
x⊤x ,
k = 1, . . . , n.
Proof. First observe that
max
x̸=0
x⊤Ax
x⊤x = max
x {x⊤Ax | x⊤x = 1},
and similarly,
max
x̸=0,x∈{un−k+1,...,un}⊥
x⊤Ax
x⊤x
= max
x

x⊤Ax | (x ∈{un−k+1, . . . , un}⊥) ∧(x⊤x = 1)
	
.
Since A is a symmetric matrix, its eigenvalues are real and it can be di-
agonalized with respect to an orthonormal basis of eigenvectors, so let
(u1, . . . , un) be such a basis. If we write
x =
n
X
i=1
xiui,

612
Spectral Theorems in Euclidean and Hermitian Spaces
a simple computation shows that
x⊤Ax =
n
X
i=1
λix2
i .
If x⊤x = 1, then Pn
i=1 x2
i = 1, and since we assumed that λ1 ≤λ2 ≤· · · ≤
λn, we get
x⊤Ax =
n
X
i=1
λix2
i ≤λn
 n
X
i=1
x2
i

= λn.
Thus,
max
x

x⊤Ax | x⊤x = 1
	
≤λn,
and since this maximum is achieved for en = (0, 0, . . . , 1), we conclude that
max
x

x⊤Ax | x⊤x = 1
	
= λn.
Next observe that x ∈{un−k+1, . . . , un}⊥and x⊤x = 1 iﬀxn−k+1 = · · · =
xn = 0 and Pn−k
i=1 x2
i = 1. Consequently, for such an x, we have
x⊤Ax =
n−k
X
i=1
λix2
i ≤λn−k
n−k
X
i=1
x2
i

= λn−k.
Thus,
max
x

x⊤Ax | (x ∈{un−k+1, . . . , un}⊥) ∧(x⊤x = 1)
	
≤λn−k,
and since this maximum is achieved for en−k = (0, . . . , 0, 1, 0, . . . , 0) with a
1 in position n −k, we conclude that
max
x

x⊤Ax | (x ∈{un−k+1, . . . , un}⊥) ∧(x⊤x = 1)
	
= λn−k,
as claimed.
For our purposes we need the version of Proposition 16.11 applying to
min instead of max, whose proof is obtained by a trivial modiﬁcation of the
proof of Proposition 16.11.
Proposition 16.12. (Rayleigh-Ritz) If A is a symmetric n×n matrix with
eigenvalues λ1 ≤λ2 ≤· · · ≤λn and if (u1, . . . , un) is any orthonormal basis
of eigenvectors of A, where ui is a unit eigenvector associated with λi, then
min
x̸=0
x⊤Ax
x⊤x = λ1

16.6. Rayleigh-Ritz Theorems and Eigenvalue Interlacing
613
(with the minimum attained for x = u1), and
min
x̸=0,x∈{u1,...,ui−1}⊥
x⊤Ax
x⊤x = λi
(with the minimum attained for x = ui), where 2 ≤i ≤n. Equivalently, if
Wk = V ⊥
k−1 denotes the subspace spanned by (uk, . . . , un) (with V0 = (0)),
then
λk =
min
x̸=0,x∈Wk
x⊤Ax
x⊤x =
min
x̸=0,x∈V ⊥
k−1
x⊤Ax
x⊤x ,
k = 1, . . . , n.
Propositions 16.11 and 16.12 together are known the Rayleigh-Ritz the-
orem.
As an application of Propositions 16.11 and 16.12, we prove a proposi-
tion which allows us to compare the eigenvalues of two symmetric matrices
A and B = R⊤AR, where R is a rectangular matrix satisfying the equation
R⊤R = I.
First we need a deﬁnition.
Deﬁnition 16.5. Given an n × n symmetric matrix A and an m × m
symmetric B, with m ≤n, if λ1 ≤λ2 ≤· · · ≤λn are the eigenvalues of A
and µ1 ≤µ2 ≤· · · ≤µm are the eigenvalues of B, then we say that the
eigenvalues of B interlace the eigenvalues of A if
λi ≤µi ≤λn−m+i,
i = 1, . . . , m.
For example, if n = 5 and m = 3, we have
λ1 ≤µ1 ≤λ3
λ2 ≤µ2 ≤λ4
λ3 ≤µ3 ≤λ5.
Proposition 16.13. Let A be an n × n symmetric matrix, R be an n × m
matrix such that R⊤R = I (with m ≤n), and let B = R⊤AR (an m × m
matrix). The following properties hold:
(a) The eigenvalues of B interlace the eigenvalues of A.
(b) If λ1 ≤λ2 ≤· · · ≤λn are the eigenvalues of A and µ1 ≤µ2 ≤· · · ≤µm
are the eigenvalues of B, and if λi = µi, then there is an eigenvector
v of B with eigenvalue µi such that Rv is an eigenvector of A with
eigenvalue λi.

614
Spectral Theorems in Euclidean and Hermitian Spaces
Proof. (a) Let (u1, . . . , un) be an orthonormal basis of eigenvectors for A,
and let (v1, . . . , vm) be an orthonormal basis of eigenvectors for B. Let
Uj be the subspace spanned by (u1, . . . , uj) and let Vj be the subspace
spanned by (v1, . . . , vj). For any i, the subspace Vi has dimension i and the
subspace R⊤Ui−1 has dimension at most i −1. Therefore, there is some
nonzero vector v ∈Vi ∩(R⊤Ui−1)⊥, and since
v⊤R⊤uj = (Rv)⊤uj = 0,
j = 1, . . . , i −1,
we have Rv ∈(Ui−1)⊥.
By Proposition 16.12 and using the fact that
R⊤R = I, we have
λi ≤(Rv)⊤ARv
(Rv)⊤Rv
= v⊤Bv
v⊤v .
On the other hand, by Proposition 16.11,
µi =
max
x̸=0,x∈{vi+1,...,vn}⊥
x⊤Bx
x⊤x
=
max
x̸=0,x∈{v1,...,vi}
x⊤Bx
x⊤x ,
so
w⊤Bw
w⊤w
≤µi
for all w ∈Vi,
and since v ∈Vi, we have
λi ≤v⊤Bv
v⊤v
≤µi,
i = 1, . . . , m.
We can apply the same argument to the symmetric matrices −A and −B,
to conclude that
−λn−m+i ≤−µi,
that is,
µi ≤λn−m+i,
i = 1, . . . , m.
Therefore,
λi ≤µi ≤λn−m+i,
i = 1, . . . , m,
as desired.
(b) If λi = µi, then
λi = (Rv)⊤ARv
(Rv)⊤Rv
= v⊤Bv
v⊤v
= µi,
so v must be an eigenvector for B and Rv must be an eigenvector for A,
both for the eigenvalue λi = µi.

16.7. The Courant-Fischer Theorem; Perturbation Results
615
Proposition 16.13 immediately implies the Poincar´e separation theorem.
It can be used in situations, such as in quantum mechanics, where one has
information about the inner products u⊤
i Auj.
Proposition 16.14. (Poincar´e separation theorem) Let A be a n × n sym-
metric (or Hermitian) matrix, let r be some integer with 1 ≤r ≤n, and let
(u1, . . . , ur) be r orthonormal vectors. Let B = (u⊤
i Auj) (an r × r matrix),
let λ1(A) ≤. . . ≤λn(A) be the eigenvalues of A and λ1(B) ≤. . . ≤λr(B)
be the eigenvalues of B; then we have
λk(A) ≤λk(B) ≤λk+n−r(A),
k = 1, . . . , r.
Observe that Proposition 16.13 implies that
λ1 + · · · + λm ≤tr(R⊤AR) ≤λn−m+1 + · · · + λn.
If P1 is the the n × (n −1) matrix obtained from the identity matrix by
dropping its last column, we have P ⊤
1 P1 = I, and the matrix B = P ⊤
1 AP1
is the matrix obtained from A by deleting its last row and its last column.
In this case the interlacing result is
λ1 ≤µ1 ≤λ2 ≤µ2 ≤· · · ≤µn−2 ≤λn−1 ≤µn−1 ≤λn,
a genuine interlacing. We obtain similar results with the matrix Pn−r ob-
tained by dropping the last n−r columns of the identity matrix and setting
B = P ⊤
n−rAPn−r (B is the r × r matrix obtained from A by deleting its
last n−r rows and columns). In this case we have the following interlacing
inequalities known as Cauchy interlacing theorem:
λk ≤µk ≤λk+n−r,
k = 1, . . . , r.
(16.2)
16.7
The Courant-Fischer Theorem; Perturbation Results
Another useful tool to prove eigenvalue equalities is the Courant-Fischer
characterization of the eigenvalues of a symmetric matrix, also known as
the Min-max (and Max-min) theorem.
Theorem 16.13. (Courant-Fischer) Let A be a symmetric n × n matrix
with eigenvalues λ1 ≤λ2 ≤· · · ≤λn. If Vk denotes the set of subspaces of
Rn of dimension k, then
λk =
max
W ∈Vn−k+1
min
x∈W,x̸=0
x⊤Ax
x⊤x
λk = min
W ∈Vk
max
x∈W,x̸=0
x⊤Ax
x⊤x .

616
Spectral Theorems in Euclidean and Hermitian Spaces
Proof. Let us consider the second equality, the proof of the ﬁrst equality
being similar. Let (u1, . . . , un) be any orthonormal basis of eigenvectors of
A, where ui is a unit eigenvector associated with λi. Observe that the space
Vk spanned by (u1, . . . , uk) has dimension k, and by Proposition 16.11, we
have
λk =
max
x̸=0,x∈Vk
x⊤Ax
x⊤x ≥min
W ∈Vk
max
x∈W,x̸=0
x⊤Ax
x⊤x .
Therefore, we need to prove the reverse inequality; that is, we have to show
that
λk ≤
max
x̸=0,x∈W
x⊤Ax
x⊤x ,
for all
W ∈Vk.
Now for any W ∈Vk, if we can prove that W ∩V ⊥
k−1 ̸= (0), then for any
nonzero v ∈W ∩V ⊥
k−1, by Proposition 16.12, we have
λk =
min
x̸=0,x∈V ⊥
k−1
x⊤Ax
x⊤x ≤v⊤Av
v⊤v
≤
max
x∈W,x̸=0
x⊤Ax
x⊤x .
It remains to prove that dim(W∩V ⊥
k−1) ≥1. However, dim(Vk−1) = k−1, so
dim(V ⊥
k−1) = n−k +1, and by hypothesis dim(W) = k. By the Grassmann
relation,
dim(W) + dim(V ⊥
k−1) = dim(W ∩V ⊥
k−1) + dim(W + V ⊥
k−1),
and since dim(W + V ⊥
k−1) ≤dim(Rn) = n, we get
k + n −k + 1 ≤dim(W ∩V ⊥
k−1) + n;
that is, 1 ≤dim(W ∩V ⊥
k−1), as claimed.
The Courant-Fischer theorem yields the following useful result about
perturbing the eigenvalues of a symmetric matrix due to Hermann Weyl.
Proposition 16.15. Given two n × n symmetric matrices A and B = A +
∆A, if α1 ≤α2 ≤· · · ≤αn are the eigenvalues of A and β1 ≤β2 ≤· · · ≤βn
are the eigenvalues of B, then
|αk −βk| ≤ρ(∆A) ≤∥∆A∥2 ,
k = 1, . . . , n.
Proof. Let Vk be deﬁned as in the Courant-Fischer theorem and let Vk be
the subspace spanned by the k eigenvectors associated with λ1, . . . , λk. By

16.7. The Courant-Fischer Theorem; Perturbation Results
617
the Courant-Fischer theorem applied to B, we have
βk = min
W ∈Vk
max
x∈W,x̸=0
x⊤Bx
x⊤x
≤max
x∈Vk
x⊤Bx
x⊤x
= max
x∈Vk
x⊤Ax
x⊤x + x⊤∆Ax
x⊤x

≤max
x∈Vk
x⊤Ax
x⊤x + max
x∈Vk
x⊤∆A x
x⊤x
.
By Proposition 16.11, we have
αk = max
x∈Vk
x⊤Ax
x⊤x ,
so we obtain
βk ≤max
x∈Vk
x⊤Ax
x⊤x + max
x∈Vk
x⊤∆A x
x⊤x
= αk + max
x∈Vk
x⊤∆A x
x⊤x
≤αk + max
x∈Rn
x⊤∆A x
x⊤x
.
Now by Proposition 16.11 and Proposition 8.6, we have
max
x∈Rn
x⊤∆A x
x⊤x
= max
i
λi(∆A) ≤ρ(∆A) ≤∥∆A∥2 ,
where λi(∆A) denotes the ith eigenvalue of ∆A, which implies that
βk ≤αk + ρ(∆A) ≤αk + ∥∆A∥2 .
By exchanging the roles of A and B, we also have
αk ≤βk + ρ(∆A) ≤βk + ∥∆A∥2 ,
and thus,
|αk −βk| ≤ρ(∆A) ≤∥∆A∥2 ,
k = 1, . . . , n,
as claimed.
Proposition 16.15 also holds for Hermitian matrices.
A pretty result of Wielandt and Hoﬀman asserts that
n
X
k=1
(αk −βk)2 ≤∥∆A∥2
F ,

618
Spectral Theorems in Euclidean and Hermitian Spaces
where ∥∥F is the Frobenius norm. However, the proof is signiﬁcantly harder
than the above proof; see Lax [Lax (2007)].
The Courant-Fischer theorem can also be used to prove some famous
inequalities due to Hermann Weyl. These can also be viewed as perturba-
tion results. Given two symmetric (or Hermitian) matrices A and B, let
λi(A), λi(B), and λi(A + B) denote the ith eigenvalue of A, B, and A + B,
respectively, arranged in nondecreasing order.
Proposition 16.16. (Weyl) Given two symmetric (or Hermitian) n × n
matrices A and B, the following inequalities hold: For all i, j, k with 1 ≤
i, j, k ≤n:
(1) If i + j = k + 1, then
λi(A) + λj(B) ≤λk(A + B).
(2) If i + j = k + n, then
λk(A + B) ≤λi(A) + λj(B).
Proof. Observe that the ﬁrst set of inequalities is obtained form the second
set by replacing A by −A and B by −B, so it is enough to prove the second
set of inequalities. By the Courant-Fischer theorem, there is a subspace H
of dimension n −k + 1 such that
λk(A + B) =
min
x∈H,x̸=0
x⊤(A + B)x
x⊤x
.
Similarly, there exists a subspace F of dimension i and a subspace G of
dimension j such that
λi(A) =
max
x∈F,x̸=0
x⊤Ax
x⊤x ,
λj(B) =
max
x∈G,x̸=0
x⊤Bx
x⊤x .
We claim that F ∩G ∩H ̸= (0). To prove this, we use the Grassmann
relation twice. First,
dim(F ∩G ∩H) = dim(F) + dim(G ∩H) −dim(F + (G ∩H))
≥dim(F) + dim(G ∩H) −n,
and second,
dim(G ∩H) = dim(G) + dim(H) −dim(G + H) ≥dim(G) + dim(H) −n,
so
dim(F ∩G ∩H) ≥dim(F) + dim(G) + dim(H) −2n.

16.7. The Courant-Fischer Theorem; Perturbation Results
619
However,
dim(F) + dim(G) + dim(H) = i + j + n −k + 1
and i + j = k + n, so we have
dim(F ∩G ∩H) ≥i + j + n −k + 1 −2n = k + n + n −k + 1 −2n = 1,
which shows that F ∩G∩H ̸= (0). Then for any unit vector z ∈F ∩G∩H
̸= (0), we have
λk(A + B) ≤z⊤(A + B)z,
λi(A) ≥z⊤Az,
λj(B) ≥z⊤Bz,
establishing the desired inequality λk(A + B) ≤λi(A) + λj(B).
In the special case i = j = k, we obtain
λ1(A) + λ1(B) ≤λ1(A + B),
λn(A + B) ≤λn(A) + λn(B).
It follows that λ1 (as a function) is concave, while λn (as a function) is
convex.
If i = 1 and j = k, we obtain
λ1(A) + λk(B) ≤λk(A + B),
and if i = k and j = n, we obtain
λk(A + B) ≤λk(A) + λn(B),
and combining them, we get
λ1(A) + λk(B) ≤λk(A + B) ≤λk(A) + λn(B).
In particular, if B is positive semideﬁnite, since its eigenvalues are non-
negative, we obtain the following inequality known as the monotonicity
theorem for symmetric (or Hermitian) matrices: if A and B are symmetric
(or Hermitian) and B is positive semideﬁnite, then
λk(A) ≤λk(A + B)
k = 1, . . . , n.
The reader is referred to Horn and Johnson [Horn and Johnson (1990)]
(Chapters 4 and 7) for a very complete treatment of matrix inequalities and
interlacing results, and also to Lax [Lax (2007)] and Serre [Serre (2010)].

620
Spectral Theorems in Euclidean and Hermitian Spaces
16.8
Summary
The main concepts and results of this chapter are listed below:
• Normal linear maps, self-adjoint linear maps, skew-self-adjoint linear
maps, and orthogonal linear maps.
• Properties of the eigenvalues and eigenvectors of a normal linear map.
• The complexiﬁcation of a real vector space, of a linear map, and of a
Euclidean inner product.
• The eigenvalues of a self-adjoint map in a Hermitian space are real.
• The eigenvalues of a self-adjoint map in a Euclidean space are real.
• Every self-adjoint linear map on a Euclidean space has an orthonormal
basis of eigenvectors.
• Every normal linear map on a Euclidean space can be block diagonal-
ized (blocks of size at most 2×2) with respect to an orthonormal basis
of eigenvectors.
• Every normal linear map on a Hermitian space can be diagonalized
with respect to an orthonormal basis of eigenvectors.
• The spectral theorems for self-adjoint, skew-self-adjoint, and orthogo-
nal linear maps (on a Euclidean space).
• The spectral theorems for normal, symmetric, skew-symmetric, and
orthogonal (real) matrices.
• The spectral theorems for normal, Hermitian, skew-Hermitian, and uni-
tary (complex) matrices.
• The Rayleigh ratio and the Rayleigh-Ritz theorem.
• Interlacing inequalities and the Cauchy interlacing theorem.
• The Poincar´e separation theorem.
• The Courant-Fischer theorem.
• Inequalities involving perturbations of the eigenvalues of a symmetric
matrix.
• The Weyl inequalities.
16.9
Problems
Problem 16.1. Prove that the structure EC introduced in Deﬁnition 16.2
is indeed a complex vector space.
Problem 16.2. Prove that the formula
⟨u1 + iv1, u2 + iv2⟩C = ⟨u1, u2⟩+ ⟨v1, v2⟩+ i(⟨v1, u2⟩−⟨u1, v2⟩)

16.9. Problems
621
deﬁnes a Hermitian form on EC that is positive deﬁnite and that ⟨−, −⟩C
agrees with ⟨−, −⟩on real vectors.
Problem 16.3. Given any linear map f : E →E, prove the map f ∗
C deﬁned
such that
f ∗
C(u + iv) = f ∗(u) + if ∗(v)
for all u, v ∈E is the adjoint of fC w.r.t. ⟨−, −⟩C.
Problem 16.4. Let A be a real symmetric n×n matrix whose eigenvalues
are nonnegative. Prove that for every p > 0, there is a real symmetric
matrix S whose eigenvalues are nonnegative such that Sp = A.
Problem 16.5. Let A be a real symmetric n×n matrix whose eigenvalues
are positive.
(1) Prove that there is a real symmetric matrix S such that A = eS.
(2) Let S be a real symmetric n×n matrix. Prove that A = eS is a real
symmetric n × n matrix whose eigenvalues are positive.
Problem 16.6. Let A be a complex matrix. Prove that if A can be diag-
onalized with respect to an orthonormal basis, then A is normal.
Problem 16.7. Let f : Cn →Cn be a linear map.
(1) Prove that if f is diagonalizable and if λ1, . . . , λn are the eigenvalues
of f, then λ2
1, . . . , λ2
n are the eigenvalues of f 2, and if λ2
i = λ2
j implies that
λi = λj, then f and f 2 have the same eigenspaces.
(2) Let f and g be two real self-adjoint linear maps f, g: Rn →Rn.
Prove that if f and g have nonnegative eigenvalues (f and g are positive
semideﬁnite) and if f 2 = g2, then f = g.
Problem 16.8. (1) Let so(3) be the space of 3×3 skew symmetric matrices
so(3) =





0 −c b
c
0 −a
−b a
0


 a, b, c ∈R


.
For any matrix
A =


0 −c b
c
0 −a
−b a
0

∈so(3),
if we let θ =
√
a2 + b2 + c2, recall from Section 11.7 (the Rodrigues formula)
that the exponential map exp: so(3) →SO(3) is given by
eA = I3 + sin θ
θ
A + (1 −cos θ)
θ2
A2,
if θ ̸= 0,

622
Spectral Theorems in Euclidean and Hermitian Spaces
with exp(03) = I3.
(2) Prove that eA is an orthogonal matrix of determinant +1, i.e., a
rotation matrix.
(3) Prove that the exponential map exp: so(3) →SO(3) is surjective.
For this proceed as follows: Pick any rotation matrix R ∈SO(3);
(1) The case R = I is trivial.
(2) If R ̸= I and tr(R) ̸= −1, then
exp−1(R) =

θ
2 sin θ(R −RT )
 1 + 2 cos θ = tr(R)

.
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R).
Show that there is a unique skew-symmetric B with corresponding θ
satisfying 0 < θ < π such that eB = R.
(3) If R ̸= I and tr(R) = −1, then prove that the eigenvalues of R are
1, −1, −1, that R = R⊤, and that R2 = I. Prove that the matrix
S = 1
2(R −I)
is a symmetric matrix whose eigenvalues are −1, −1, 0. Thus S can be
diagonalized with respect to an orthogonal matrix Q as
S = Q


−1 0 0
0 −1 0
0
0 0

Q⊤.
Prove that there exists a skew symmetric matrix
U =


0 −d c
d
0 −b
−c b
0


so that
U 2 = S = 1
2(R −I).
Observe that
U 2 =


−(c2 + d2)
bc
bd
bc
−(b2 + d2)
cd
bd
cd
−(b2 + c2)

,
and use this to conclude that if U 2 = S, then b2 + c2 + d2 = 1. Then
show that
exp−1(R) =


(2k + 1)π


0 −d c
d
0 −b
−c b
0

, k ∈Z


,
where (b, c, d) is any unit vector such that for the corresponding skew
symmetric matrix U, we have U 2 = S.

16.9. Problems
623
(4) To ﬁnd a skew symmetric matrix U so that U 2 = S = 1
2(R −I) as
in (3), we can solve the system


b2 −1
bc
bd
bc
c2 −1
cd
bd
cd
d2 −1

= S.
We immediately get b2, c2, d2, and then, since one of b, c, d is nonzero, say
b, if we choose the positive square root of b2, we can determine c and d
from bc and bd.
Implement a computer program in Matlab to solve the above system.
Problem 16.9. It was shown in Proposition 14.10 that the exponential
map is a map exp: so(n) →SO(n), where so(n) is the vector space of real
n×n skew-symmetric matrices. Use the spectral theorem to prove that the
map exp: so(n) →SO(n) is surjective.
Problem 16.10. Let u(n) be the space of (complex) n×n skew-Hermitian
matrices (B∗= −B) and let su(n) be its subspace consisting of skew-
Hermitian matrice with zero trace (tr(B) = 0).
(1) Prove that if B ∈u(n), then eB ∈U(n), and if B ∈su(n), then
eB ∈SU(n).
Thus we have well-deﬁned maps exp: u(n) →U(n) and
exp: su(n) →SU(n).
(2) Prove that the map exp: u(n) →U(n) is surjective.
(3) Prove that the map exp: su(n) →SU(n) is surjective.
Problem 16.11. Recall that a matrix B ∈Mn(R) is skew-symmetric if
B⊤= −B.
Check that the set so(n) of skew-symmetric matrices is a
vector space of dimension n(n −1)/2, and thus is isomorphic to Rn(n−1)/2.
(1) Given a rotation matrix
R =
cos θ −sin θ
sin θ
cos θ

,
where 0 < θ < π, prove that there is a skew symmetric matrix B such that
R = (I −B)(I + B)−1.
(2) Prove that the eigenvalues of a skew-symmetric matrix are either 0
or pure imaginary (that is, of the form iµ for µ ∈R).
Let C : so(n) →Mn(R) be the function (called the Cayley transform of
B) given by
C(B) = (I −B)(I + B)−1.

624
Spectral Theorems in Euclidean and Hermitian Spaces
Prove that if B is skew-symmetric, then I −B and I + B are invertible,
and so C is well-deﬁned. Prove that
(I + B)(I −B) = (I −B)(I + B),
and that
(I + B)(I −B)−1 = (I −B)−1(I + B).
Prove that
(C(B))⊤C(B) = I
and that
det C(B) = +1,
so that C(B) is a rotation matrix. Furthermore, show that C(B) does not
admit −1 as an eigenvalue.
(3) Let SO(n) be the group of n × n rotation matrices. Prove that the
map
C : so(n) →SO(n)
is bijective onto the subset of rotation matrices that do not admit −1 as
an eigenvalue. Show that the inverse of this map is given by
B = (I + R)−1(I −R) = (I −R)(I + R)−1,
where R ∈SO(n) does not admit −1 as an eigenvalue.
Problem 16.12. Please refer back to Problem 3.6. Let λ1, . . . , λn be the
eigenvalues of A (not necessarily distinct). Using Schur's theorem, A is
similar to an upper triangular matrix B, that is, A = PBP −1 with B
upper triangular, and we may assume that the diagonal entries of B in
descending order are λ1, . . . , λn.
(1) If the Eij are listed according to total order given by
(i, j) < (h, k)
iﬀ
i = h and j > k
or i < h.
prove that RB is an upper triangular matrix whose diagonal entries are
(λn, . . . , λ1, . . . , λn, . . . , λ1
|
{z
}
n2
),
and that LB is an upper triangular matrix whose diagonal entries are
(λ1, . . . , λ1
|
{z
}
n
. . . , λn, . . . , λn
|
{z
}
n
).
Hint. Figure out what are RB(Eij) = EijB and LB(Eij) = BEij.
(2) Use the fact that
LA = LP ◦LB ◦L−1
P ,
RA = R−1
P ◦RB ◦RP ,
to express adA = LA −RA in terms of LB −RB, and conclude that the
eigenvalues of adA are λi −λj, for i = 1, . . . , n, and for j = n, . . . , 1.

Chapter 17
Computing Eigenvalues and
Eigenvectors
After the problem of solving a linear system, the problem of computing
the eigenvalues and the eigenvectors of a real or complex matrix is one
of most important problems of numerical linear algebra. Several methods
exist, among which we mention Jacobi, Givens-Householder, divide-and-
conquer, QR iteration, and Rayleigh-Ritz; see Demmel [Demmel (1997)],
Trefethen and Bau [Trefethen and Bau III (1997)], Meyer [Meyer (2000)],
Serre [Serre (2010)], Golub and Van Loan [Golub and Van Loan (1996)], and
Ciarlet [Ciarlet (1989)].
Typically, better performing methods exist for
special kinds of matrices, such as symmetric matrices.
In theory, given an n × n complex matrix A, if we could compute a
Schur form A = UTU ∗, where T is upper triangular and U is unitary, we
would obtain the eigenvalues of A, since they are the diagonal entries in T.
However, this would require ﬁnding the roots of a polynomial, but methods
for doing this are known to be numerically very unstable, so this is not a
practical method.
A common paradigm is to construct a sequence (Pk) of matrices such
that Ak = P −1
k APk converges, in some sense, to a matrix whose eigenvalues
are easily determined. For example, Ak = P −1
k APk could become upper
triangular in the limit. Furthermore, Pk is typically a product of "nice"
matrices, for example, orthogonal matrices.
For general matrices, that is, matrices that are not symmetric, the
QR iteration algorithm, due to Rutishauser, Francis, and Kublanovskaya
in the early 1960s, is one of the most eﬃcient algorithms for computing
eigenvalues. A fascinating account of the history of the QR algorithm is
given in Golub and Uhlig [Golub and Uhlig (2009)]. The QR algorithm
constructs a sequence of matrices (Ak), where Ak+1 is obtained from Ak
by performing a QR-decomposition Ak = QkRk of Ak and then setting
625

626
Computing Eigenvalues and Eigenvectors
Ak+1 = RkQk, the result of swapping Qk and Rk. It is immediately ver-
iﬁed that Ak+1 = Q∗
kAkQk, so Ak and Ak+1 have the same eigenvalues,
which are the eigenvalues of A.
The basic version of this algorithm runs into diﬃculties with matrices
that have several eigenvalues with the same modulus (it may loop or not
"converge" to an upper triangular matrix). There are ways of dealing with
some of these problems, but for ease of exposition, we ﬁrst present a sim-
pliﬁed version of the QR algorithm which we call basic QR algorithm. We
prove a convergence theorem for the basic QR algorithm, under the rather
restrictive hypothesis that the input matrix A is diagonalizable and that
its eigenvalues are nonzero and have distinct moduli. The proof shows that
the part of Ak strictly below the diagonal converges to zero and that the
diagonal entries of Ak converge to the eigenvalues of A.
Since the convergence of the QR method depends crucially only on the
fact that the part of Ak below the diagonal goes to zero, it would be highly
desirable if we could replace A by a similar matrix U ∗AU easily computable
from A and having lots of zero strictly below the diagonal. It turns out that
there is a way to construct a matrix H = U ∗AU which is almost triangular,
except that it may have an extra nonzero diagonal below the main diagonal.
Such matrices called, Hessenberg matrices, are discussed in Section 17.2.
An n × n diagonalizable Hessenberg matrix H having the property that
hi+1i ̸= 0 for i = 1, . . . , n −1 (such a matrix is called unreduced) has the
nice property that its eigenvalues are all distinct. Since every Hessenberg
matrix is a block diagonal matrix of unreduced Hessenberg blocks, it suﬃces
to compute the eigenvalues of unreduced Hessenberg matrices.
There is
a special case of particular interest: symmetric (or Hermitian) positive
deﬁnite tridiagonal matrices. Such matrices must have real positive distinct
eigenvalues, so the QR algorithm converges to a diagonal matrix.
In Section 17.3, we consider techniques for making the basic QR method
practical and more eﬃcient. The ﬁrst step is to convert the original input
matrix A to a similar matrix H in Hessenberg form, and to apply the QR
algorithm to H (actually, to the unreduced blocks of H). The second and
crucial ingredient to speed up convergence is to add shifts.
A shift is the following step: pick some σk, hopefully close to some
eigenvalue of A (in general, λn), QR-factor Ak −σkI as
Ak −σkI = QkRk,
and then form
Ak+1 = RkQk + σkI.

17.1. The Basic QR Algorithm
627
It is easy to see that we still have Ak+1 = Q∗
kAkQk. A judicious choice
of σk can speed up convergence considerably. If H is real and has pairs of
complex conjugate eigenvalues, we can perform a double shift, and it can
be arranged that we work in real arithmetic.
The last step for improving eﬃciency is to compute Ak+1 = Q∗
kAkQk
without even performing a QR-factorization of Ak −σkI. This can be done
when Ak is unreduced Hessenberg. Such a method is called QR iteration
with implicit shifts. There is also a version of QR iteration with implicit
double shifts.
If the dimension of the matrix A is very large, we can ﬁnd approxima-
tions of some of the eigenvalues of A by using a truncated version of the
reduction to Hessenberg form due to Arnoldi in general and to Lanczos
in the symmetric (or Hermitian) tridiagonal case. Arnoldi iteration is dis-
cussed in Section 17.4. If A is an m×m matrix, for n ≪m (n much smaller
than m) the idea is to generate the n × n Hessenberg submatrix Hn of the
full Hessenberg matrix H (such that A = UHU ∗) consisting of its ﬁrst n
rows and n columns; the matrix Un consisting of the ﬁrst n columns of
U is also produced. The Rayleigh-Ritz method consists in computing the
eigenvalues of Hn using the QR- method with shifts. These eigenvalues,
called Ritz values, are approximations of the eigenvalues of A. Typically,
extreme eigenvalues are found ﬁrst.
Arnoldi iteration can also be viewed as a way of computing an orthonor-
mal basis of a Krylov subspace, namely the subspace Kn(A, b) spanned by
(b, Ab, . . . , Anb). We can also use Arnoldi iteration to ﬁnd an approximate
solution of a linear equation Ax = b by minimizing ∥b −Axn∥2 for all xn
is the Krylov space Kn(A, b). This method named GMRES is discussed in
Section 17.5.
The special case where H is a symmetric (or Hermitian) tridiagonal ma-
trix is discussed in Section 17.6. In this case, Arnoldi's algorithm becomes
Lanczos' algorithm. It is much more eﬃcient than Arnoldi iteration.
We close this chapter by discussing two classical methods for computing
a single eigenvector and a single eigenvalue: power iteration and inverse
(power) iteration; see Section 17.7.
17.1
The Basic QR Algorithm
Let A be an n×n matrix which is assumed to be diagonalizable and invert-
ible. The basic QR algorithm makes use of two very simple steps. Starting
with A1 = A, we construct sequences of matrices (Ak), (Qk) (Rk) and (Pk)

628
Computing Eigenvalues and Eigenvectors
as follows:
Factor
A1 = Q1R1
Set
A2 = R1Q1
Factor
A2 = Q2R2
Set
A3 = R2Q2
...
Factor
Ak = QkRk
Set
Ak+1 = RkQk
...
Thus, Ak+1 is obtained from a QR-factorization Ak = QkRk of Ak by
swapping Qk and Rk. Deﬁne Pk by
Pk = Q1Q2 · · · Qk.
Since Ak = QkRk, we have Rk = Q∗
kAk, and since Ak+1 = RkQk, we
obtain
Ak+1 = Q∗
kAkQk.
(17.1)
An obvious induction shows that
Ak+1 = Q∗
k · · · Q∗
1A1Q1 · · · Qk = P ∗
k APk,
that is
Ak+1 = P ∗
k APk.
(17.2)
Therefore, Ak+1 and A are similar, so they have the same eigenvalues.
The basic QR iteration method consists in computing the sequence of
matrices Ak, and in the ideal situation, to expect that Ak "converges" to an
upper triangular matrix, more precisely that the part of Ak below the main
diagonal goes to zero, and the diagonal entries converge to the eigenvalues
of A.
This ideal situation is only achieved in rather special cases. For one
thing, if A is unitary (or orthogonal in the real case), since in the QR
decomposition we have R = I, we get A2 = IQ = Q = A1, so the method
does not make any progress.
Also, if A is a real matrix, since the Ak
are also real, if A has complex eigenvalues, then the part of Ak below the
main diagonal can't go to zero. Generally, the method runs into troubles
whenever A has distinct eigenvalues with the same modulus.

17.1. The Basic QR Algorithm
629
The convergence of the sequence (Ak) is only known under some fairly
restrictive hypotheses. Even under such hypotheses, this is not really gen-
uine convergence. Indeed, it can be shown that the part of Ak below the
main diagonal goes to zero, and the diagonal entries converge to the eigen-
values of A, but the part of Ak above the diagonal may not converge.
However, for the purpose of ﬁnding the eigenvalues of A, this does not
matter.
The following convergence result is proven in Ciarlet [Ciarlet (1989)]
(Chapter 6, Theorem 6.3.10 and Serre [Serre (2010)] (Chapter 13, The-
orem 13.2). It is rarely applicable in practice, except for symmetric (or
Hermitian) positive deﬁnite matrices, as we will see shortly.
Theorem 17.1. Suppose the (complex) n × n matrix A is invertible, di-
agonalizable, and that its eigenvalues λ1, . . . , λn have diﬀerent moduli, so
that
|λ1| > |λ2| > · · · > |λn| > 0.
If A = PΛP −1, where Λ = diag(λ1, . . . , λn), and if P −1 has an LU-
factorization, then the strictly lower-triangular part of Ak converges to zero,
and the diagonal of Ak converges to Λ.
Proof. We reproduce the proof in Ciarlet [Ciarlet (1989)] (Chapter 6, The-
orem 6.3.10). The strategy is to study the asymptotic behavior of the ma-
trices Pk = Q1Q2 · · · Qk. For this, it turns out that we need to consider the
powers Ak.
Step 1. Let Rk = Rk · · · R2R1. We claim that
Ak = (Q1Q2 · · · Qk)(Rk · · · R2R1) = PkRk.
(17.3)
We proceed by induction.
The base case k = 1 is trivial.
For the
induction step, from (17.2), we have
PkAk+1 = APk.
Since Ak+1 = RkQk = Qk+1Rk+1, we have
Pk+1Rk+1 = PkQk+1Rk+1Rk = PkAk+1Rk = APkRk = AAk = Ak+1
establishing the induction step.
Step 2. We will express the matrix Pk as Pk = Q eQkDk, in terms of a
diagonal matrix Dk with unit entries, with Q and eQk unitary.
Let P = QR, a QR-factorization of P (with R an upper triangular
matrix with positive diagonal entries), and P −1 = LU, an LU-factorization
of P −1. Since A = PΛP −1, we have
Ak = PΛkP −1 = QRΛkLU = QR(ΛkLΛ−k)ΛkU.
(17.4)

630
Computing Eigenvalues and Eigenvectors
Here, Λ−k is the diagonal matrix with entries λ−k
i
. The reason for introduc-
ing the matrix ΛkLΛ−k is that its asymptotic behavior is easy to determine.
Indeed, we have
(ΛkLΛ−k)ij =









0
if i < j
1
if i = j
 λi
λj
k
Lij
if i > j.
The hypothesis that |λ1| > |λ2| > · · · > |λn| > 0 implies that
lim
k7→∞ΛkLΛ−k = I.
(17.5)
Note that it is to obtain this limit that we made the hypothesis on the
moduli of the eigenvalues. We can write
ΛkLΛ−k = I + Fk,
with
lim
k7→∞Fk = 0,
and consequently, since R(ΛkLΛ−k) = R(I + Fk) = R + RFkR−1R =
(I + RFkR−1)R, we have
R(ΛkLΛ−k) = (I + RFkR−1)R.
(17.6)
By Proposition 8.8(1), since limk7→∞Fk = 0, and thus limk7→∞RFkR−1 =
0, the matrices I +RFkR−1 are invertible for k large enough. Consequently
for k large enough, we have a QR-factorization
I + RFkR−1 = eQk eRk,
(17.7)
with ( eRk)ii > 0 for i = 1, . . . , n. Since the matrices eQk are unitary, we
have
 eQk

2 = 1, so the sequence ( eQk) is bounded. It follows that it has
a convergent subsequence ( eQℓ) that converges to some matrix eQ, which is
also unitary. Since
eRℓ= ( eQℓ)∗(I + RFℓR−1),
we deduce that the subsequence ( eRℓ) also converges to some matrix eR,
which is also upper triangular with positive diagonal entries. By passing to
the limit (using the subsequences), we get eR = ( eQ)∗, that is,
I = eQ eR.
By the uniqueness of a QR-decomposition (when the diagonal entries of R
are positive), we get
eQ = eR = I.

17.1. The Basic QR Algorithm
631
Since the above reasoning applies to any subsequences of ( eQk) and ( eRk),
by the uniqueness of limits, we conclude that the "full" sequences ( eQk) and
( eRk) converge:
lim
k7→∞
eQk = I,
lim
k7→∞
eRk = I.
Since by (17.4),
Ak = QR(ΛkLΛ−k)ΛkU,
by (17.6),
R(ΛkLΛ−k) = (I + RFkR−1)R,
and by (17.7)
I + RFkR−1 = eQk eRk,
we proved that
Ak = (Q eQk)( eRkRΛkU).
(17.8)
Observe that Q eQk is a unitary matrix, and eRkRΛkU is an upper triangular
matrix, as a product of upper triangular matrices. However, some entries
in Λ may be negative, so we can't claim that eRkRΛkU has positive diagonal
entries. Nevertheless, we have another QR-decomposition of Ak,
Ak = (Q eQk)( eRkRΛkU) = PkRk.
It is easy to prove that there is diagonal matrix Dk with |(Dk)ii| = 1 for
i = 1, . . . , n, such that
Pk = Q eQkDk.
(17.9)
The existence of Dk is consequence of the following fact: If an invertible
matrix B has two QR factorizations B = Q1R1 = Q2R2, then there is a
diagonal matrix D with unit entries such that Q2 = DQ1.
The expression for Pk in (17.9) is that which we were seeking.
Step 3. Asymptotic behavior of the matrices Ak+1 = P ∗
k APk.
Since A = PΛP −1 = QRΛR−1Q−1 and by (17.9), Pk = Q eQkDk, we
get
Ak+1 = D∗
k( eQk)∗Q∗QRΛR−1Q−1Q eQkDk = D∗
k( eQk)∗RΛR−1 eQkDk.
(17.10)
Since limk7→∞eQk = I, we deduce that
lim
k7→∞( eQk)∗RΛR−1 eQk = RΛR−1 =





λ1 ∗· · · ∗
0 λ2 · · · ∗
...
...
...
0
0 · · · λn




,

632
Computing Eigenvalues and Eigenvectors
an upper triangular matrix with the eigenvalues of A on the diagonal. Since
R is upper triangular, the order of the eigenvalues is preserved. If we let
Dk = ( eQk)∗RΛR−1 eQk,
(17.11)
then by (17.10) we have Ak+1 = D∗
kDkDk, and since the matrices Dk are
diagonal matrices, we have
(Ak+1)jj = (D∗
kDkDk)ij = (Dk)ii(Dk)jj(Dk)ij,
which implies that
(Ak+1)ii = (Dk)ii,
i = 1, . . . , n,
(17.12)
since |(Dk)ii| = 1 for i = 1, . . . , n. Since limk7→∞Dk = RΛR−1, we conclude
that the strictly lower-triangular part of Ak+1 converges to zero, and the
diagonal of Ak+1 converges to Λ.
Observe that if the matrix A is real, then the hypothesis that the eigen-
values have distinct moduli implies that the eigenvalues are all real and
simple.
The following Matlab program implements the basic QR-method using
the function qrv4 from Section 11.8.
function T = qreigen(A,m)
T = A;
for k = 1:m
[Q R] = qrv4(T);
T = R*Q;
end
end
Example 17.1. If we run the function qreigen with 100 iterations on the
8 × 8 symmetric matrix
A =













4 1 0 0 0 0 0 0
1 4 1 0 0 0 0 0
0 1 4 1 0 0 0 0
0 0 1 4 1 0 0 0
0 0 0 1 4 1 0 0
0 0 0 0 1 4 1 0
0 0 0 0 0 1 4 1
0 0 0 0 0 0 1 4













,

17.1. The Basic QR Algorithm
633
we ﬁnd the matrix
T =













5.8794 0.0015 0.0000 −0.0000 0.0000 −0.0000 0.0000 −0.0000
0.0015 5.5321 0.0001 0.0000 −0.0000 0.0000 0.0000 0.0000
0
0.0001 5.0000 0.0000 −0.0000 0.0000 0.0000 0.0000
0
0
0.0000 4.3473
0.0000
0.0000 0.0000 0.0000
0
0
0
0.0000
3.6527
0.0000 0.0000 −0.0000
0
0
0
0
0.0000
3.0000 0.0000 −0.0000
0
0
0
0
0
0.0000 2.4679 0.0000
0
0
0
0
0
0
0.0000 2.1.206













.
The diagonal entries match the eigenvalues found by running the Matlab
function eig(A).
If several eigenvalues have the same modulus, then the proof breaks
down, we can no longer claim (17.5), namely that
lim
k7→∞ΛkLΛ−k = I.
If we assume that P −1 has a suitable "block LU-factorization," it can be
shown that the matrices Ak+1 converge to a block upper-triangular matrix,
where each block corresponds to eigenvalues having the same modulus. For
example, if A is a 9 × 9 matrix with eigenvalues λi such that |λ1| = |λ2| =
|λ3| > |λ4| > |λ5| = |λ6| = |λ7| = |λ8| = |λ9|, then Ak converges to a block
diagonal matrix (with three blocks, a 3×3 block, a 1× 1 block, and a 5× 5
block) of the form















⋆⋆⋆∗∗∗∗∗∗
⋆⋆⋆∗∗∗∗∗∗
⋆⋆⋆∗∗∗∗∗∗
0 0 0 ⋆∗∗∗∗∗
0 0 0 0 ⋆⋆⋆⋆⋆
0 0 0 0 ⋆⋆⋆⋆⋆
0 0 0 0 ⋆⋆⋆⋆⋆
0 0 0 0 ⋆⋆⋆⋆⋆
0 0 0 0 ⋆⋆⋆⋆⋆















.
See Ciarlet [Ciarlet (1989)] (Chapter 6 Section 6.3) for more details.
Under the conditions of Theorem 17.1, in particular, if A is a symmetric
(or Hermitian) positive deﬁnite matrix, the eigenvectors of A can be ap-
proximated. However, when A is not a symmetric matrix, since the upper
triangular part of Ak does not necessarily converge, one has to be cautious
that a rigorous justiﬁcation is lacking.

634
Computing Eigenvalues and Eigenvectors
Suppose we apply the QR algorithm to a matrix A satisfying the hy-
potheses of Theorem 17.1. For k large enough, Ak+1 = P ∗
k APk is nearly
upper triangular and the diagonal entries of Ak+1 are all distinct, so we can
consider that they are the eigenvalues of Ak+1, and thus of A. To avoid
too many subscripts, write T for the upper triangular matrix obtained by
setting the entries of the part of Ak+1 below the diagonal to 0. Then we
can ﬁnd the corresponding eigenvectors by solving the linear system
Tv = tiiv,
and since T is upper triangular, this can be done by bottom-up elimi-
nation.
We leave it as an exercise to show that the following vectors
vi = (vi
1, . . . , vi
n) are eigenvectors:
v1 = e1,
and if i = 2, . . . , n, then
vi
j =









0
if i + 1 ≤j ≤n
1
if j = i
−tjj+1vi
j+1 + · · · + tjivi
i
tjj −tii
if i −1 ≥j ≥1.
Then the vectors (Pkv1, . . . , Pkvn) are a basis of (approximate) eigenvectors
for A. In the special case where T is a diagonal matrix, then vi = ei for i =
1, . . . , n and the columns of Pk are an orthonormal basis of (approximate)
eigenvectors for A.
If A is a real matrix whose eigenvalues are not all real, then there is some
complex pair of eigenvalues λ + iµ (with µ ̸= 0), and the QR-algorithm
cannot converge to a matrix whose strictly lower-triangular part is zero.
There is a way to deal with this situation using upper Hessenberg matrices
which will be discussed in the next section.
Since the convergence of the QR method depends crucially only on
the fact that the part of Ak below the diagonal goes to zero, it would be
highly desirable if we could replace A by a similar matrix U ∗AU easily
computable from A having lots of zero strictly below the diagonal.
We
can't expect U ∗AU to be a diagonal matrix (since this would mean that A
was easily diagonalized), but it turns out that there is a way to construct
a matrix H = U ∗AU which is almost triangular, except that it may have
an extra nonzero diagonal below the main diagonal. Such matrices called
Hessenberg matrices are discussed in the next section.

17.2. Hessenberg Matrices
635
17.2
Hessenberg Matrices
Deﬁnition 17.1. An n × n matrix (real or complex) H is an (upper) Hes-
senberg matrix if it is almost triangular, except that it may have an extra
nonzero diagonal below the main diagonal. Technically, hjk = 0 for all
(j, k) such that j −k ≥2.
The 5 × 5 matrix below is an example of a Hessenberg matrix.
H =






∗
∗
∗
∗∗
h21
∗
∗
∗∗
0 h32
∗
∗∗
0
0 h43
∗∗
0
0
0 h54 ∗






.
The following result can be shown.
Theorem 17.2. Every n × n complex or real matrix A is similar to an
upper Hessenberg matrix H, that is, A = UHU ∗for some unitary matrix
U. Furthermore, H can be constructed as a product of Householder matrices
(the deﬁnition is the same as in Section 12.1, except that W is a complex
vector, and that the inner product is the Hermitian inner product on Cn).
If A is a real matrix, then H is an orthogonal matrix (and H is a real
matrix).
Theorem 17.2 and algorithms for converting a matrix to Hessenberg
form are discussed in Trefethen and Bau [Trefethen and Bau III (1997)]
(Lecture 26), Demmel [Demmel (1997)] (Section 4.4.6, in the real case),
Serre [Serre (2010)] (Theorem 13.1), and Meyer [Meyer (2000)] (Exam-
ple 5.7.4, in the real case). The proof of correctness is not diﬃcult and will
be the object of a homework problem.
The following functions written in Matlab implement a function to com-
pute a Hessenberg form of a matrix.
The function house constructs the normalized vector u deﬁning the
Householder reﬂection that zeros all but the ﬁrst entries in a vector x.
function [uu, u] = house(x)
tol = 2*10^(-15);
% tolerance
uu = x;
p = size(x,1);
% computes l^1-norm of x(2:p,1)

636
Computing Eigenvalues and Eigenvectors
n1 = sum(abs(x(2:p,1)));
if n1 <= tol
u = zeros(p,1);
uu = u;
else
l = sqrt(x'*x);
% l^2 norm of x
uu(1) = x(1) + signe(x(1))*l;
u = uu/sqrt(uu'*uu);
end
end
The function signe(z) returns −1 if z < 0, else +1.
The function buildhouse builds a Householder reﬂection from a vector
uu.
function P = buildhouse(v,i)
% This function builds a Householder reflection
%
[I 0 ]
%
[0 PP]
%
from a Householder reflection
%
PP = I - 2uu*uu'
%
where uu = v(i:n)
%
If uu = 0 then P - I
%
n = size(v,1);
if v(i:n) == zeros(n - i + 1,1)
P = eye(n);
else
PP = eye(n - i + 1) - 2*v(i:n)*v(i:n)';
P = [eye(i-1) zeros(i-1, n - i + 1);
zeros(n - i + 1, i - 1) PP];
end
end
The function Hessenberg1 computes an upper Hessenberg matrix H
and an orthogonal matrix Q such that A = Q⊤HQ.
function [H, Q] = Hessenberg1(A)
%
%
This function constructs an upper Hessenberg

17.2. Hessenberg Matrices
637
%
matrix H and an orthogonal matrix Q such that
%
A = Q' H Q
n = size(A,1);
H = A;
Q = eye(n);
for i = 1:n-2
%
H(i+1:n,i)
[~,u] = house(H(i+1:n,i));
%
u
P =
buildhouse(u,1);
Q(i+1:n,i:n) = P*Q(i+1:n,i:n);
H(i+1:n,i:n) = H(i+1:n,i:n) - 2*u*(u')*H(i+1:n,i:n);
H(1:n,i+1:n) = H(1:n,i+1:n) - 2*H(1:n,i+1:n)*u*(u');
end
end
Example 17.2. If
A =




1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7



,
running Hessenberg1 we ﬁnd
H =




1.0000 −5.3852
0
0
−5.3852 15.2069 −1.6893 −0.0000
−0.0000 −1.6893 −0.2069 −0.0000
0
−0.0000 0.0000
0.0000




Q =




1.0000
0
0
0
0
−0.3714 −0.5571 −0.7428
0
0.8339
0.1516 −0.5307
0
0.4082 −0.8165 0.4082



.
An important property of (upper) Hessenberg matrices is that if some
subdiagonal entry Hp+1p = 0, then H is of the form
H =
H11 H12
0
H22

,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p × p
matrix and H22 a (n −p) × (n −p) matrix), and the eigenvalues of H are

638
Computing Eigenvalues and Eigenvectors
the eigenvalues of H11 and H22. For example, in the matrix
H =






∗
∗
∗
∗∗
h21
∗
∗
∗∗
0 h32
∗
∗∗
0
0 h43
∗∗
0
0
0 h54 ∗






,
if h43 = 0, then we have the block matrix
H =






∗
∗∗∗∗
h21
∗∗∗∗
0 h32 ∗∗∗
0
0 0 ∗∗
0
0 0 h54 ∗






.
Then the list of eigenvalues of H is the concatenation of the list of eigen-
values of H11 and the list of the eigenvalues of H22. This is easily seen by
induction on the dimension of the block H11.
More generally, every upper Hessenberg matrix can be written in such
a way that it has diagonal blocks that are Hessenberg blocks whose subdi-
agonal is not zero.
Deﬁnition 17.2.
An upper Hessenberg n × n matrix H is unreduced if
hi+1i ̸= 0 for i = 1, . . . , n−1. A Hessenberg matrix which is not unreduced
is said to be reduced.
The following is an example of an 8 × 8 matrix consisting of three diag-
onal unreduced Hessenberg blocks:
H =













⋆
⋆
⋆
∗
∗
∗
∗
∗
h21
⋆
⋆
∗
∗
∗
∗
∗
0
h32 ⋆
∗
∗
∗
∗
∗
0
0
0
⋆
⋆
⋆
∗
∗
0
0
0 h54
⋆
⋆
∗
∗
0
0
0
0
h65 ⋆
∗
∗
0
0
0
0
0
0
⋆
⋆
0
0
0
0
0
0 h87 ⋆













.
An interesting and important property of unreduced Hessenberg matri-
ces is the following.
Proposition 17.1. Let H be an n × n complex or real unreduced Hessen-
berg matrix. Then every eigenvalue of H is geometrically simple, that is,

17.2. Hessenberg Matrices
639
dim(Eλ) = 1 for every eigenvalue λ, where Eλ is the eigenspace associ-
ated with λ. Furthermore, if H is diagonalizable, then every eigenvalue is
simple, that is, H has n distinct eigenvalues.
Proof. We follow Serre's proof [Serre (2010)] (Proposition 3.26). Let λ be
any eigenvalue of H, let M = λIn −H, and let N be the (n −1) × (n −1)
matrix obtained from M by deleting its ﬁrst row and its last column. Since
H is upper Hessenberg, N is a diagonal matrix with entries −hi+1i ̸=
0, i = 1, . . . , n −1.
Thus N is invertible and has rank n −1.
But a
matrix has rank greater than or equal to the rank of any of its submatrices,
so rank(M) = n −1, since M is singular. By the rank-nullity theorem,
rank(Ker N) = 1, that is, dim(Eλ) = 1, as claimed.
If H is diagonalizable, then the sum of the dimensions of the eigenspaces
is equal to n, which implies that the eigenvalues of H are distinct.
As we said earlier, a case where Theorem 17.1 applies is the case where
A is a symmetric (or Hermitian) positive deﬁnite matrix. This follows from
two facts.
The ﬁrst fact is that if A is Hermitian (or symmetric in the real case),
then it is easy to show that the Hessenberg matrix similar to A is a Hermi-
tian (or symmetric in real case) tridiagonal matrix. The conversion method
is also more eﬃcient. Here is an example of a symmetric tridiagonal matrix
consisting of three unreduced blocks:
H =













α1 β1 0
0
0
0
0
0
β1 α2 β2 0
0
0
0
0
0 β2 α3 0
0
0
0
0
0
0
0 α4 β4 0
0
0
0
0
0 β4 α5 β5 0
0
0
0
0
0 β5 α6 0
0
0
0
0
0
0
0 α7 β7
0
0
0
0
0
0 β7 α8













.
Thus the problem of ﬁnding the eigenvalues of a symmetric (or Hermi-
tian) matrix reduces to the problem of ﬁnding the eigenvalues of a symmet-
ric (resp. Hermitian) tridiagonal matrix, and this can be done much more
eﬃciently.
The second fact is that if H is an upper Hessenberg matrix and if it is
diagonalizable, then there is an invertible matrix P such that H = PΛP −1
with Λ a diagonal matrix consisting of the eigenvalues of H, such that P −1
has an LU-decomposition; see Serre [Serre (2010)] (Theorem 13.3).

640
Computing Eigenvalues and Eigenvectors
As a consequence, since any symmetric (or Hermitian) tridiagonal ma-
trix is a block diagonal matrix of unreduced symmetric (resp. Hermitian)
tridiagonal matrices, by Proposition 17.1, we see that the QR algorithm
applied to a tridiagonal matrix which is symmetric (or Hermitian) positive
deﬁnite converges to a diagonal matrix consisting of its eigenvalues. Let us
record this important fact.
Theorem 17.3. Let H be a symmetric (or Hermitian) positive deﬁnite
tridiagonal matrix. If H is unreduced, then the QR algorithm converges to
a diagonal matrix consisting of the eigenvalues of H.
Since every symmetric (or Hermitian) positive deﬁnite matrix is simi-
lar to tridiagonal symmetric (resp. Hermitian) positive deﬁnite matrix, we
deduce that we have a method for ﬁnding the eigenvalues of a symmetric
(resp. Hermitian) positive deﬁnite matrix (more accurately, to ﬁnd approx-
imations as good as we want for these eigenvalues).
If A is a symmetric (or Hermitian) matrix, since its eigenvalues are real,
for some µ > 0 large enough (pick µ > ρ(A)), A + µI is symmetric (resp.
Hermitan) positive deﬁnite, so we can apply the QR algorithm to an upper
Hessenberg matrix similar to A + µI to ﬁnd its eigenvalues, and then the
eigenvalues of A are obtained by subtracting µ.
The problem of ﬁnding the eigenvalues of a symmetric matrix is dis-
cussed extensively in Parlett [Parlett (1997)], one of the best references on
this topic.
The upper Hessenberg form also yields a way to handle singular matri-
ces. First, checking the proof of Proposition 13.20 that an n × n complex
matrix A (possibly singular) can be factored as A = QR where Q is a uni-
tary matrix which is a product of Householder reﬂections and R is upper
triangular, it is easy to see that if A is upper Hessenberg, then Q is also
upper Hessenberg. If H is an unreduced upper Hessenberg matrix, since
Q is upper Hessenberg and R is upper triangular, we have hi+1i = qi+1irii
for i = 1 . . . , n −1, and since H is unreduced, rii ̸= 0 for i = 1, . . . , n −1.
Consequently H is singular iﬀrnn = 0. Then the matrix RQ is a matrix
whose last row consists of zero's thus we can deﬂate the problem by con-
sidering the (n −1) × (n −1) unreduced Hessenberg matrix obtained by
deleting the last row and the last column. After ﬁnitely many steps (not
larger that the multiplicity of the eigenvalue 0), there remains an invertible
unreduced Hessenberg matrix. As an alternative, see Serre [Serre (2010)]
(Chapter 13, Section 13.3.2).
As is, the QR algorithm, although very simple, is quite ineﬃcient for

17.3. Making the QR Method More Eﬃcient Using Shifts
641
several reasons. In the next section, we indicate how to make the method
more eﬃcient. This involves a lot of work and we only discuss the main
ideas at a high level.
17.3
Making the QR Method More Eﬃcient
Using Shifts
To improve eﬃciency and cope with pairs of complex conjugate eigenvalues
in the case of real matrices, the following steps are taken:
(1) Initially reduce the matrix A to upper Hessenberg form, as A = UHU ∗.
Then apply the QR-algorithm to H (actually, to its unreduced Hessen-
berg blocks). It is easy to see that the matrices Hk produced by the
QR algorithm remain upper Hessenberg.
(2) To accelerate convergence, use shifts, and to deal with pairs of complex
conjugate eigenvalues, use double shifts.
(3) Instead of computing a QR-factorization explicitly while doing a shift,
perform an implicit shift which computes Ak+1 = Q∗
kAkQk without
having to compute a QR-factorization (of Ak −σkI), and similarly in
the case of a double shift. This is the most intricate modiﬁcation of
the basic QR algorithm and we will not discuss it here. This method
is usually referred as bulge chasing. Details about this technique for
real matrices can be found in Demmel [Demmel (1997)] (Section 4.4.8)
and Golub and Van Loan [Golub and Van Loan (1996)] (Section 7.5).
Watkins discusses the QR algorithm with shifts as a bulge chasing
method in the more general case of complex matrices [Watkins (1982,
2008)].
Let us repeat an important remark made in the previous section. If we
start with a matrix H in upper Hessenberg form, if at any stage of the QR
algorithm we ﬁnd that some subdiagonal entry (Hk)p+1p = 0 or is very
small, then Hk is of the form
Hk =
H11 H12
0
H22

,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p × p
matrix and H22 a (n −p) × (n −p) matrix), and the eigenvalues of Hk are

642
Computing Eigenvalues and Eigenvectors
the eigenvalues of H11 and H22. For example, in the matrix
H =






∗
∗
∗
∗∗
h21
∗
∗
∗∗
0 h32
∗
∗∗
0
0 h43
∗∗
0
0
0 h54 ∗






,
if h43 = 0, then we have the block matrix
H =






∗
∗∗∗∗
h21
∗∗∗∗
0 h32 ∗∗∗
0
0 0 ∗∗
0
0 0 h54 ∗






.
Then we can recursively apply the QR algorithm to H11 and H22.
In particular, if (Hk)nn−1 = 0 or is very small, then (Hk)nn is a good
approximation of an eigenvalue, so we can delete the last row and the last
column of Hk and apply the QR algorithm to this submatrix. This process
is called deﬂation.
If (Hk)n−1n−2 = 0 or is very small, then the 2 × 2
"corner block"
(Hk)n−1n−1 (Hk)n−1n
(Hk)nn−1
(Hk)nn

appears, and its eigenvalues can be computed immediately by solving a
quadratic equation. Then we deﬂate Hk by deleting its last two rows and
its last two columns and apply the QR algorithm to this submatrix.
Thus it would seem desirable to modify the basic QR algorithm so
that the above situations arises, and this is what shifts are designed for.
More precisely, under the hypotheses of Theorem 17.1, it can be shown
(see Ciarlet [Ciarlet (1989)], Section 6.3) that the entry (Ak)ij with i > j
converges to 0 as |λi/λj|k converges to 0. Also, if we let ri be deﬁned by
r1 =

λ2
λ1
 ,
ri = max

λi
λi−1
 ,

λi+1
λi


,
2 ≤i ≤n −1,
rn =

λn
λn−1
 ,
then there is a constant C (independent of k) such that
|(Ak)ii −λi| ≤Crk
i ,
1 ≤i ≤n.
In particular, if H is upper Hessenberg, then the entry (Hk)i+1i con-
verges to 0 as |λi+1/λi|k converges to 0. Thus if we pick σk close to λi,
we expect that (Hk −σkI)i+1i converges to 0 as |(λi+1 −σk)/(λi −σk)|k
converges to 0, and this ratio is much smaller than 1 as σk is closer to λi.

17.3. Making the QR Method More Eﬃcient Using Shifts
643
Typically, we apply a shift to accelerate convergence to λn (so i = n −1).
In this case, both (Hk −σkI)nn−1 and |(Hk −σkI)nn −λn| converge to 0
as |(λn −σk)/(λn−1 −σk)|k converges to 0.
A shift is the following modiﬁed QR-steps (switching back to an arbi-
trary matrix A, since the shift technique applies in general). Pick some
σk, hopefully close to some eigenvalue of A (in general, λn), and QR-factor
Ak −σkI as
Ak −σkI = QkRk,
and then form
Ak+1 = RkQk + σkI.
Since
Ak+1 = RkQk + σkI
= Q∗
kQkRkQk + Q∗
kQkσk
= Q∗
k(QkRk + σkI)Qk
= Q∗
kAkQk,
Ak+1 is similar to Ak, as before. If Ak is upper Hessenberg, then it is easy
to see that Ak+1 is also upper Hessenberg.
If A is upper Hessenberg and if σi is exactly equal to an eigenvalue, then
Ak −σkI is singular, and forming the QR-factorization will detect that Rk
has some diagonal entry equal to 0.
Assuming that the QR-algorithm
returns (Rk)nn = 0 (if not, the argument is easily adapted), then the last
row of RkQk is 0, so the last row of Ak+1 = RkQk + σkI ends with σk (all
other entries being zero), so we are in the case where we can deﬂate Ak
(and σk is indeed an eigenvalue).
The question remains, what is a good choice for the shift σk?
Assuming again that H is in upper Hessenberg form, it turns out that
when (Hk)nn−1 is small enough, then a good choice for σk is (Hk)nn. In fact,
the rate of convergence is quadratic, which means roughly that the number
of correct digits doubles at every iteration. The reason is that shifts are
related to another method known as inverse iteration, and such a method
converges very fast. For further explanations about this connection, see
Demmel [Demmel (1997)] (Section 4.4.4) and Trefethen and Bau [Trefethen
and Bau III (1997)] (Lecture 29).
One should still be cautious that the QR method with shifts does not
necessarily converge, and that our convergence proof no longer applies,
because instead of having the identity Ak = PkRk, we have
(A −σkI) · · · (A −σ2I)(A −σ1I) = PkRk.

644
Computing Eigenvalues and Eigenvectors
Of course, the QR algorithm loops immediately when applied to an or-
thogonal matrix A. This is also the case when A is symmetric but not
positive deﬁnite. For example, both the QR algorithm and the QR algo-
rithm with shifts loop on the matrix
A =
0 1
1 0

.
In the case of symmetric matrices, Wilkinson invented a shift which
helps the QR algorithm with shifts to make progress. Again, looking at the
lower corner of Ak, say
B =
an−1 bn−1
bn−1
an

,
the Wilkinson shift picks the eigenvalue of B closer to an. If we let
δ = an−1 −an
2
,
it is easy to see that the eigenvalues of B are given by
λ = an + an−1
2
±
q
δ2 + b2
n−1.
It follows that
λ −an = δ ±
q
δ2 + b2
n−1,
and from this it is easy to see that the eigenvalue closer to an is given by
µ = an −
sign(δ)b2
n−1
(|δ| +
q
δ2 + b2
n−1)
.
If δ = 0, then we pick arbitrarily one of the two eigenvalues. Observe that
the Wilkinson shift applied to the matrix
A =
0 1
1 0

is either +1 or −1, and in one step, deﬂation occurs and the algorithm
terminates successfully.
We now discuss double shifts, which are intended to deal with pairs of
complex conjugate eigenvalues.
Let us assume that A is a real matrix. For any complex number σk with
nonzero imaginary part, a double shift consists of the following steps:
Ak −σkI = QkRk
Ak+1 = RkQk + σkI
Ak+1 −σkI = Qk+1Rk+1
Ak+2 = Rk+1Qk+1 + σkI.

17.3. Making the QR Method More Eﬃcient Using Shifts
645
From the computation made for a single shift, we have Ak+1 = Q∗
kAkQk
and Ak+2 = Q∗
k+1Ak+1Qk+1, so we obtain
Ak+2 = Q∗
k+1Q∗
kAkQkQk+1.
The matrices Qk are complex, so we would expect that the Ak are also
complex, but remarkably we can keep the products QkQk+1 real, and so the
Ak also real. This is highly desirable to avoid complex arithmetic, which is
more expensive.
Observe that since
Qk+1Rk+1 = Ak+1 −σkI = RkQk + (σk −σk)I,
we have
QkQk+1Rk+1Rk = Qk(RkQk + (σk −σk)I)Rk
= QkRkQkRk + (σk −σk)QkRk
= (Ak −σkI)2 + (σk −σk)(Ak −σkI)
= A2
k −2(ℜσk)Ak + |σk|2I.
If we assume by induction that matrix Ak is real (with k = 2ℓ+1, ℓ≥0),
then the matrix S = A2
k −2(ℜσk)Ak +|σk|2I is also real, and since QkQk+1
is unitary and Rk+1Rk is upper triangular, we see that
S = QkQk+1Rk+1Rk
is a QR-factorization of the real matrix S, thus QkQk+1 and Rk+1Rk can
be chosen to be real matrices, in which case (QkQk+1)∗is also real, and
thus
Ak+2 = Q∗
k+1Q∗
kAkQkQk+1 = (QkQk+1)∗AkQkQk+1
is real. Consequently, if A1 = A is real, then A2ℓ+1 is real for all ℓ≥0.
The strategy that consists in picking σk and σk as the complex conjugate
eigenvalues of the corner block
(Hk)n−1n−1 (Hk)n−1n
(Hk)nn−1
(Hk)nn

is called the Francis shift (here we are assuming that A has be reduced to
upper Hessenberg form).
It should be noted that there are matrices for which neither a shift by
(Hk)nn nor the Francis shift works. For instance, the permutation matrix
A =


0 0 1
1 0 0
0 1 0



646
Computing Eigenvalues and Eigenvectors
has eigenvalues ei2π/3, ei4π/3, +1, and neither of the above shifts apply to
the matrix
0 0
1 0

.
However, a shift by 1 does work. There are other kinds of matrices for
which the QR algorithm does not converge. Demmel gives the example of
matrices of the form




0 1 0 0
1 0 h 0
0 −h 0 1
0 0 1 0




where h is small.
Algorithms implementing the QR algorithm with shifts and double
shifts perform "exceptional" shifts every 10 shifts. Despite the fact that
the QR algorithm has been perfected since the 1960's, it is still an open
problem to ﬁnd a shift strategy that ensures convergence of all matrices.
Implicit shifting is based on a result known as the implicit Q theo-
rem. This theorem says that if A is reduced to upper Hessenberg form
as A = UHU ∗and if H is unreduced (hi+1i ̸= 0 for i = 1, . . . , n −1),
then the columns of index 2, . . . , n of U are determined by the ﬁrst col-
umn of U up to sign; see Demmel [Demmel (1997)] (Theorem 4.9) and
Golub and Van Loan [Golub and Van Loan (1996)] (Theorem 7.4.2) for the
proof in the case of real matrices. Actually, the proof is not diﬃcult and
will be the object of a homework exercise. In the case of a single shift,
an implicit shift generates Ak+1 = Q∗
kAkQk without having to compute a
QR-factorization of Ak −σkI. For real matrices, this is done by applying
a sequence of Givens rotations which perform a bulge chasing process (a
Givens rotation is an orthogonal block diagonal matrix consisting of a sin-
gle block which is a 2D rotation, the other diagonal entries being equal to
1). Similarly, in the case of a double shift, Ak+2 = (QkQk+1)∗AkQkQk+1
is generated without having to compute the QR-factorizations of Ak −σkI
and Ak+1 −σkI. Again, (QkQk+1)∗AkQkQk+1 is generated by applying
some simple orthogonal matrices which perform a bulge chasing process.
See Demmel [Demmel (1997)] (Section 4.4.8) and Golub and Van Loan
[Golub and Van Loan (1996)] (Section 7.5) for further explanations regard-
ing implicit shifting involving bulge chasing in the case of real matrices.
Watkins [Watkins (1982, 2008)] discusses bulge chasing in the more general
case of complex matrices.

17.4. Krylov Subspaces; Arnoldi Iteration
647
The Matlab function for ﬁnding the eigenvalues and the eigenvectors
of a matrix A is eig and is called as [U, D] = eig(A). It is implemented
using an optimized version of the QR-algorithm with implicit shifts.
If the dimension of the matrix A is very large, we can ﬁnd approxima-
tions of some of the eigenvalues of A by using a truncated version of the
reduction to Hessenberg form due to Arnoldi in general and to Lanczos in
the symmetric (or Hermitian) tridiagonal case.
17.4
Krylov Subspaces; Arnoldi Iteration
In this section, we denote the dimension of the square real or complex
matrix A by m rather than n, to make it easier for the reader to follow
Trefethen and Bau exposition [Trefethen and Bau III (1997)], which is
particularly lucid.
Suppose that the m × m matrix A has been reduced to the upper Hes-
senberg form H, as A = UHU ∗. For any n ≤m (typically much smaller
than m), consider the (n + 1) × n upper left block
eHn =










h11 h12 h13
· · ·
h1n
h21 h22 h23
· · ·
h2n
0 h32 h33
· · ·
h3n
...
... ...
...
...
0
· · ·
0 hnn−1
hnn
0
· · ·
0
0
hn+1n










of H, and the n × n upper Hessenberg matrix Hn obtained by deleting the
last row of eHn,
Hn =







h11 h12 h13
· · ·
h1n
h21 h22 h23
· · ·
h2n
0 h32 h33
· · ·
h3n
...
... ...
...
...
0
· · ·
0 hnn−1 hnn







.
If we denote by Un the m × n matrix consisting of the ﬁrst n columns of
U, denoted u1, . . . , un, then matrix consisting of the ﬁrst n columns of the
matrix UH = AU can be expressed as
AUn = Un+1 eHn.
(17.13)
It follows that the nth column of this matrix can be expressed as
Aun = h1nu1 + · · · + hnnun + hn+1nun+1.
(17.14)

648
Computing Eigenvalues and Eigenvectors
Since (u1, . . . , un) form an orthonormal basis, we deduce from (17.14) that
⟨uj, Aun⟩= u∗
jAun = hjn,
j = 1, . . . , n.
(17.15)
Equations (17.14) and (17.15) show that Un+1 and eHn can be computed
iteratively using the following algorithm due to Arnoldi, known as Arnoldi
iteration:
Given an arbitrary nonzero vector b ∈Cm, let u1 = b/ ∥b∥;
for n = 1, 2, 3, . . . do
z := Aun;
for j = 1 to n do
hjn := u∗
jz;
z := z −hjnuj
endfor
hn+1n := ∥z∥;
if hn+1n = 0 quit
un+1 = z/hn+1n
When hn+1n = 0, we say that we have a breakdown of the Arnoldi
iteration.
Arnoldi iteration is an algorithm for producing the n × n Hessenberg
submatrix Hn of the full Hessenberg matrix H consisting of its ﬁrst n rows
and n columns (the ﬁrst n columns of U are also produced), not using
Householder matrices.
As long as hj+1j ̸= 0 for j = 1, . . . , n, equation (17.14) shows by an easy
induction that un+1 belong to the span of (b, Ab, . . . , Anb), and obviously
Aun belongs to the span of (u1, . . . , un+1), and thus the following spaces
are identical:
Span(b, Ab, . . . , Anb) = Span(u1, . . . , un+1).
The space Kn(A, b) = Span(b, Ab, . . . , An−1b) is called a Krylov sub-
space. We can view Arnoldi's algorithm as the construction of an orthonor-
mal basis for Kn(A, b). It is a sort of Gram-Schmidt procedure.
Equation (17.14) shows that if Kn is the m × n matrix whose columns
are the vectors (b, Ab, . . . , An−1b), then there is a n × n upper triangular
matrix Rn such that
Kn = UnRn.
(17.16)
The above is called a reduced QR factorization of Kn.

17.4. Krylov Subspaces; Arnoldi Iteration
649
Since (u1, . . . , un) is an orthonormal system, the matrix U ∗
nUn+1 is the
n×(n+1) matrix consisting of the identity matrix In plus an extra column
of 0's, so U ∗
nUn+1 eHn = U ∗
nAUn is obtained by deleting the last row of eHn,
namely Hn, and so
U ∗
nAUn = Hn.
(17.17)
We summarize the above facts in the following proposition.
Proposition 17.2. If Arnoldi iteration run on an m×m matrix A starting
with a nonzero vector b ∈Cm does not have a breakdown at stage n ≤m,
then the following properties hold:
(1) If Kn is the m × n Krylov matrix associated with the vectors
(b, Ab, . . . , An−1b) and if Un is the m × n matrix of orthogonal vec-
tors produced by Arnoldi iteration, then there is a QR-factorization
Kn = UnRn,
for some n × n upper triangular matrix Rn.
(2) The m×n upper Hessenberg matrices Hn produced by Arnoldi iteration
are the projection of A onto the Krylov space Kn(A, b), that is,
Hn = U ∗
nAUn.
(3) The successive iterates are related by the formula
AUn = Un+1 eHn.
Remark: If Arnoldi iteration has a breakdown at stage n, that is, hn+1 =
0, then we found the ﬁrst unreduced block of the Hessenberg matrix H.
It can be shown that the eigenvalues of Hn are eigenvalues of A.
So a
breakdown is actually a good thing. In this case, we can pick some new
nonzero vector un+1 orthogonal to the vectors (u1, . . . , un) as a new starting
vector and run Arnoldi iteration again.
Such a vector exists since the
(n + 1)th column of U works. So repeated application of Arnoldi yields a
full Hessenberg reduction of A. However, this is not what we are after, since
m is very large an we are only interested in a "small" number of eigenvalues
of A.
There is another aspect of Arnoldi iteration, which is that it solves an
optimization problem involving polynomials of degree n. Let Pn denote
the set of (complex) monic polynomials of degree n, that is, polynomials of
the form
p(z) = zn + cn−1zn−1 + · · · + c1z + c0
(ci ∈C).

650
Computing Eigenvalues and Eigenvectors
For any m × m matrix A, we write
p(A) = An + cn−1An−1 + · · · + c1A + c0I.
The following result is proven in Trefethen and Bau [Trefethen and Bau III
(1997)] (Lecture 34, Theorem 34.1).
Theorem 17.4. If Arnoldi iteration run on an m × m matrix A start-
ing with a nonzero vector b does not have a breakdown at stage n ≤m,
then there is a unique polynomial p ∈Pn such that ∥p(A)b∥2 is minimum,
namely the characteristic polynomial det(zI −Hn) of Hn.
Theorem 17.4 can be viewed as the "justiﬁcation" for a method to ﬁnd
some of the eigenvalues of A (say n ≪m of them). Intuitively, the closer
the roots of the characteristic polynomials of Hn are to the eigenvalues of
A, the smaller ∥p(A)b∥2 should be, and conversely. In the extreme case
where m = n, by the Cayley-Hamilton theorem, p(A) = 0 (where p is
the characteristic polynomial of A), so this idea is plausible, but this is far
from constituting a proof (also, b should have nonzero coordinates in all
directions associated with the eigenvalues).
The method known as the Rayleigh-Ritz method is to run Arnoldi iter-
ation on A and some b ̸= 0 chosen at random for n ≪m steps before or
until a breakdown occurs. Then run the QR algorithm with shifts on Hn.
The eigenvalues of the Hessenberg matrix Hn may then be considered as
approximations of the eigenvalues of A. The eigenvalues of Hn are called
Arnoldi estimates or Ritz values. One has to be cautious because Hn is a
truncated version of the full Hessenberg matrix H, so not all of the Ritz
values are necessary close to eigenvalues of A. It has been observed that the
eigenvalues that are found ﬁrst are the extreme eigenvalues of A, namely
those close to the boundary of the spectrum of A plotted in C. So if A
has real eigenvalues, the largest and the smallest eigenvalues appear ﬁrst as
Ritz values. In many problems where eigenvalues occur, the extreme eigen-
values are the one that need to be computed. Similarly, the eigenvectors of
Hn may be considered as approximations of eigenvectors of A.
The Matlab function eigs is based on the computation of Ritz values.
It computes the six eigenvalues of largest magnitude of a matrix A, and the
call is [V, D] = eigs(A). More generally, to get the top k eigenvalues, use
[V, D] = eigs(A, k).
In the absence of rigorous theorems about error estimates, it is hard to
make the above statements more precise; see Trefethen and Bau [Trefethen
and Bau III (1997)] (Lecture 34) for more on this subject.

17.5. GMRES
651
However, if A is a symmetric (or Hermitian) matrix, then Hn is a sym-
metric (resp. Hermitian) tridiagonal matrix and more precise results can be
shown; see Demmel [Demmel (1997)] (Chapter 7, especially Section 7.2).
We will consider the symmetric (and Hermitian) case in the next section,
but ﬁrst we show how Arnoldi iteration can be used to ﬁnd approximations
for the solution of a linear system Ax = b where A is invertible but of very
large dimension m.
17.5
GMRES
Suppose A is an invertible m×m matrix and let b be a nonzero vector in Cm.
Let x0 = A−1b, the unique solution of Ax = b. It is not hard to show that
x0 ∈Kn(A, b) for some n ≤m. In fact, there is a unique monic polynomial
p(z) of minimal degree s ≤m such that p(A)b = 0, so x0 ∈Ks(A, b).
Thus it makes sense to search for a solution of Ax = b in Krylov spaces of
dimension m ≤s. The idea is to ﬁnd an approximation xn ∈Kn(A, b) of
x0 such that rn = b −Axn is minimized, that is, ∥rn∥2 = ∥b −Axn∥2 is
minimized over xn ∈Kn(A, b). This minimization problem can be stated
as
minimize
∥rn∥2 = ∥Axn −b∥2 ,
xn ∈Kn(A, b).
This is a least-squares problem, and we know how to solve it (see Sec-
tion 21.1). The quantity rn is known as the residual and the method which
consists in minimizing ∥rn∥2 is known as GMRES, for generalized minimal
residuals.
Now since (u1, . . . , un) is a basis of Kn(A, b) (since n ≤s, no breakdown
occurs, except for n = s), we may write xn = Uny, so our minimization
problem is
minimize
∥AUny −b∥2 ,
y ∈Cn.
Since by (17.13) of Section 17.4, we have AUn = Un+1 eHn, minimiz-
ing ∥AUny −b∥2 is equivalent to minimizing ∥Un+1 eHny −b∥2 over Cm.
Since Un+1 eHny and b belong to the column space of Un+1, minimizing
∥Un+1 eHny −b∥2 is equivalent to minimizing ∥eHny −U ∗
n+1b∥2. However, by
construction,
U ∗
n+1b = ∥b∥2e1 ∈Cn+1,
so our minimization problem can be stated as
minimize
∥eHny −∥b∥2e1∥2,
y ∈Cn.

652
Computing Eigenvalues and Eigenvectors
The approximate solution of Ax = b is then
xn = Uny.
Starting with u1 = b/ ∥b∥2 and with n = 1, the GMRES method runs
n ≤s Arnoldi iterations to ﬁnd Un and eHn, and then runs a method to
solve the least squares problem
minimize
∥eHny −∥b∥2e1∥2,
y ∈Cn.
When ∥rn∥2 = ∥eHny −∥b∥2e1∥2 is considered small enough, we stop
and the approximate solution of Ax = b is then
xn = Uny.
There are ways of improving eﬃciency of the "naive" version of GM-
RES that we just presented; see Trefethen and Bau [Trefethen and Bau III
(1997)] (Lecture 35). We now consider the case where A is a Hermitian (or
symmetric) matrix.
17.6
The Hermitian Case; Lanczos Iteration
If A is an m × m symmetric or Hermitian matrix, then Arnoldi's method
is simpler and much more eﬃcient. Indeed, in this case, it is easy to see
that the upper Hessenberg matrices Hn are also symmetric (Hermitian
respectively), and thus tridiagonal. Also, the eigenvalues of A and Hn are
real. It is convenient to write
Hn =








α1 β1
β1 α2 β2
β2 α3
...
...
...
βn−1
βn−1
αn








.
The recurrence (17.14) of Section 17.4 becomes the three-term recurrence
Aun = βn−1un−1 + αnun + βnun+1.
(17.18)
We also have αn = u∗
nAUn, so Arnoldi's algorithm become the following
algorithm known as Lanczos' algorithm (or Lanczos iteration).
The in-
ner loop on j from 1 to n has been eliminated and replaced by a single
assignment.
Given an arbitrary nonzero vector b ∈Cm, let u1 = b/ ∥b∥;
for n = 1, 2, 3, . . . do

17.7. Power Methods
653
z := Aun;
αn := u∗
nz;
z := z −βn−1un−1 −αnun
βn := ∥z∥;
if βn = 0 quit
un+1 = z/βn
When βn = 0, we say that we have a breakdown of the Lanczos iteration.
Versions of Proposition 17.2 and Theorem 17.4 apply to Lanczos itera-
tion.
Besides being much more eﬃcient than Arnoldi iteration, Lanczos it-
eration has the advantage that the Rayleigh-Ritz method for ﬁnding some
of the eigenvalues of A as the eigenvalues of the symmetric (respectively
Hermitian) tridiagonal matrix Hn applies, but there are more methods for
ﬁnding the eigenvalues of symmetric (respectively Hermitian) tridiagonal
matrices. Also theorems about error estimates exist. The version of Lanc-
zos iteration given above may run into problems in ﬂoating point arithmetic.
What happens is that the vectors uj may lose the property of being orthog-
onal, so it may be necessary to reorthogonalize them. For more on all this,
see Demmel [Demmel (1997)] (Chapter 7, in particular Sections 7.2-7.4).
The version of GMRES using Lanczos iteration is called MINRES.
We close our brief survey of methods for computing the eigenvalues and
the eigenvectors of a matrix with a quick discussion of two methods known
as power methods.
17.7
Power Methods
Let A be an m × m complex or real matrix. There are two power methods,
both of which yield one eigenvalue and one eigenvector associated with this
vector:
(1) Power iteration.
(2) Inverse (power) iteration.
Power iteration only works if the matrix A has an eigenvalue λ of largest
modulus, which means that if λ1, . . . , λm are the eigenvalues of A, then
|λ1| > |λ2| ≥· · · ≥|λm| ≥0.
In particular, if A is a real matrix, then λ1 must be real (since otherwise
there are two complex conjugate eigenvalues of the same largest modulus).

654
Computing Eigenvalues and Eigenvectors
If the above condition is satisﬁed, then power iteration yields λ1 and some
eigenvector associated with it. The method is simple enough:
Pick some initial unit vector x0 and compute the following sequence
(xk), where
xk+1 =
Axk
∥Axk∥,
k ≥0.
We would expect that (xk) converges to an eigenvector associated with
λ1, but this is not quite correct. The following results are proven in Serre
[Serre (2010)] (Section 13.5.1). First assume that λ1 ̸= 0.
We have
lim
k7→∞
Axk = |λ1|.
If A is a complex matrix which has a unique complex eigenvalue λ1 of
largest modulus, then
v = lim
k7→∞
 λ1
|λ1|
k
xk
is a unit eigenvector of A associated with λ1. If λ1 is real, then
v = lim
k7→∞xk
is a unit eigenvector of A associated with λ1.
Actually some condition
on x0 is needed: x0 must have a nonzero component in the eigenspace E
associated with λ1 (in any direct sum of Cm in which E is a summand).
The eigenvalue λ1 is found as follows. If λ1 is complex, and if vj ̸= 0 is
any nonzero coordinate of v, then
λ1 = lim
k7→∞
(Axk)j
xk
j
.
If λ1 is real, then we can deﬁne the sequence (λ(k)) by
λ(k+1) = (xk+1)∗Axk+1,
k ≥0,
and we have
λ1 = lim
k7→∞λ(k).
Indeed, in this case, since v = limk7→∞xk and v is a unit eigenvector for
λ1, we have
lim
k7→∞λ(k) = lim
k7→∞(xk+1)∗Axk+1 = v∗Av = λ1v∗v = λ1.
Note that since xk+1 is a unit vector, (xk+1)∗Axk+1 is a Rayleigh ratio.

17.7. Power Methods
655
If A is a Hermitian matrix, then the eigenvalues are real and we can say
more about the rate of convergence, which is not great (only linear). For
details, see Trefethen and Bau [Trefethen and Bau III (1997)] (Lecture 27).
If λ1 = 0, then there is some power ℓ< m such that Axℓ= 0.
The inverse iteration method is designed to ﬁnd an eigenvector associ-
ated with an eigenvalue λ of A for which we know a good approximation
µ.
Pick some initial unit vector x0 and compute the following sequences
(wk) and (xk), where wk+1 is the solution of the system
(A −µI)wk+1 = xk
equivalently
wk+1 = (A −µI)−1xk,
k ≥0,
and
xk+1 =
wk+1
∥wk+1∥,
k ≥0.
The following result is proven in Ciarlet [Ciarlet (1989)] (Theo-
rem 6.4.1).
Proposition 17.3. Let A be an m×m diagonalizable (complex or real) ma-
trix with eigenvalues λ1, . . . , λm, and let λ = λℓbe an arbitrary eigenvalue
of A (not necessary simple). For any µ such that
µ ̸= λ
and
|µ −λ| < |µ −λj|
for all j ̸= ℓ,
if x0 does not belong to the subspace spanned by the eigenvectors associated
with the eigenvalues λj with j ̸= ℓ, then
lim
k7→∞
(λ −µ)k
|λ −µ|k

xk = v,
where v is an eigenvector associated with λ. Furthermore, if both λ and µ
are real, we have
lim
k7→∞xk = v
if µ < λ,
lim
k7→∞(−1)kxk = v
if µ > λ.
Also, if we deﬁne the sequence (λ(k)) by
λ(k+1) = (xk+1)∗Axk+1,
then
lim
k7→∞λ(k+1) = λ.

656
Computing Eigenvalues and Eigenvectors
The condition of x0 may seem quite stringent, but in practice, a vector
x0 chosen at random usually satisﬁes it.
If A is a Hermitian matrix, then we can say more. In particular, the
inverse iteration algorithm can be modiﬁed to make use of the newly com-
puted λ(k+1) instead of µ, and an even faster convergence is achieved. Such
a method is called the Rayleigh quotient iteration.
When it converges
(which is for almost all x0), this method eventually achieves cubic con-
vergence, which is remarkable. Essentially, this means that the number of
correct digits is tripled at every iteration. For more details, see Trefethen
and Bau [Trefethen and Bau III (1997)] (Lecture 27) and Demmel [Demmel
(1997)] (Section 5.3.2).
17.8
Summary
The main concepts and results of this chapter are listed below:
• QR iteration, QR algorithm.
• Upper Hessenberg matrices.
• Householder matrix.
• Unreduced and reduced Hessenberg matrices.
• Deﬂation.
• Shift.
• Wilkinson shift.
• Double shift.
• Francis shift.
• Implicit shifting.
• Implicit Q-theorem.
• Arnoldi iteration.
• Breakdown of Arnoldi iteration.
• Krylov subspace.
• Rayleigh-Ritz method.
• Ritz values, Arnoldi estimates.
• Residual.
• GMRES
• Lanczos iteration.
• Power iteration.
• Inverse power iteration.
• Rayleigh ratio.

17.9. Problems
657
17.9
Problems
Problem 17.1. Prove Theorem 17.2; see Problem 12.7.
Problem 17.2. Prove that if a matrix A is Hermitian (or real symmetric),
then any Hessenberg matrix H similar to A is Hermitian tridiagonal (real
symmetric tridiagonal).
Problem 17.3. For any matrix (real or complex) A, if A = QR is a QR-
decomposition of A using Householder reﬂections, prove that if A is upper
Hessenberg then so is Q.
Problem 17.4. Prove that if A is upper Hessenberg, then the matrices Ak
obtained by applying the QR-algorithm are also upper Hessenberg.
Problem 17.5. Prove the implicit Q theorem. This theorem says that if A
is reduced to upper Hessenberg form as A = UHU ∗and if H is unreduced
(hi+1i ̸= 0 for i = 1, . . . , n −1), then the columns of index 2, . . . , n of U are
determined by the ﬁrst column of U up to sign.
Problem 17.6. Read Section 7.5 of Golub and Van Loan [Golub and
Van Loan (1996)] and implement their version of the QR-algorithm with
shifts.
Problem 17.7. If an Arnoldi iteration has a breakdown at stage n, that
is, hn+1 = 0, then we found the ﬁrst unreduced block of the Hessenberg
matrix H. Prove that the eigenvalues of Hn are eigenvalues of A.
Problem 17.8. Prove Theorem 17.4.
Problem 17.9. Implement GRMES and test it on some linear systems.
Problem 17.10. State and prove versions of Proposition 17.2 and Theo-
rem 17.4 for the Lanczos iteration.
Problem 17.11. Prove the results about the power iteration method
stated in Section 17.7.
Problem 17.12. Prove the results about the inverse power iteration
method stated in Section 17.7.
Problem 17.13. Implement and test the power iteration method and the
inverse power iteration method.

658
Computing Eigenvalues and Eigenvectors
Problem 17.14. Read Lecture 27 in Trefethen and Bau [Trefethen and
Bau III (1997)] and implement and test the Rayleigh quotient iteration
method.

Chapter 18
Graphs and Graph Laplacians; Basic
Facts
In this chapter and the next we present some applications of linear algebra
to graph theory. Graphs (undirected and directed) can be deﬁned in terms
of various matrices (incidence and adjacency matrices), and various con-
nectivity properties of graphs are captured by properties of these matrices.
Another very important matrix is associated with a (undirected) graph:
the graph Laplacian. The graph Laplacian is symmetric positive deﬁnite,
and its eigenvalues capture some of the properties of the underlying graph.
This is a key fact that is exploited in graph clustering methods, the most
powerful being the method of normalized cuts due to Shi and Malik [Shi
and Malik (2000)]. This chapter and the next constitute an introduction
to algebraic and spectral graph theory. We do not discuss normalized cuts,
but we discuss graph drawings. Thorough presentations of algebraic graph
theory can be found in Godsil and Royle [Godsil and Royle (2001)] and
Chung [Chung (1997)].
We begin with a review of basic notions of graph theory. Even though
the graph Laplacian is fundamentally associated with an undirected graph,
we review the deﬁnition of both directed and undirected graphs. For both
directed and undirected graphs, we deﬁne the degree matrix D, the inci-
dence matrix B, and the adjacency matrix A. Then we deﬁne a weighted
graph. This is a pair (V, W), where V is a ﬁnite set of nodes and W is a
m×m symmetric matrix with nonnegative entries and zero diagonal entries
(where m = |V |).
For every node vi ∈V , the degree d(vi) (or di) of vi is the sum of the
weights of the edges adjacent to vi:
di = d(vi) =
m
X
j=1
wi j.
659

660
Graphs and Graph Laplacians; Basic Facts
The degree matrix is the diagonal matrix
D = diag(d1, . . . , dm).
Fig. 18.1
Degree of a node.
The notion of degree is illustrated in Figure 18.1. Then we introduce the
(unnormalized) graph Laplacian L of a directed graph G in an "old-fashion"
way, by showing that for any orientation of a graph G,
BB⊤= D −A = L
is an invariant. We also deﬁne the (unnormalized) graph Laplacian L of
a weighted graph G = (V, W) as L = D −W. We show that the notion
of incidence matrix can be generalized to weighted graphs in a simple way.
For any graph Gσ obtained by orienting the underlying graph of a weighted
graph G = (V, W), there is an incidence matrix Bσ such that
Bσ(Bσ)⊤= D −W = L.
We also prove that
x⊤Lx = 1
2
m

i,j=1
wi j(xi −xj)2
for all x ∈Rm.
Consequently, x⊤Lx does not depend on the diagonal entries in W, and if
wi j ≥0 for all i, j ∈{1, . . . , m}, then L is positive semideﬁnite. Then if W
consists of nonnegative entries, the eigenvalues 0 = λ1 ≤λ2 ≤. . . ≤λm of L
are real and nonnegative, and there is an orthonormal basis of eigenvectors
of L. We show that the number of connected components of the graph

Graphs and Graph Laplacians; Basic Facts
661
G = (V, W) is equal to the dimension of the kernel of L, which is also equal
to the dimension of the kernel of the transpose (Bσ)⊤of any incidence
matrix Bσ obtained by orienting the underlying graph of G.
We also deﬁne the normalized graph Laplacians Lsym and Lrw, given by
Lsym = D−1/2LD−1/2 = I −D−1/2WD−1/2
Lrw = D−1L = I −D−1W,
and prove some simple properties relating the eigenvalues and the eigen-
vectors of L, Lsym and Lrw. These normalized graph Laplacians show up
when dealing with normalized cuts.
Next, we turn to graph drawings (Chapter 19). Graph drawing is a very
attractive application of so-called spectral techniques, which is a fancy way
of saying that that eigenvalues and eigenvectors of the graph Laplacian are
used. Furthermore, it turns out that graph clustering using normalized cuts
can be cast as a certain type of graph drawing.
Given an undirected graph G = (V, E), with |V | = m, we would like
to draw G in Rn for n (much) smaller than m. The idea is to assign a
point ρ(vi) in Rn to the vertex vi ∈V , for every vi ∈V , and to draw a
line segment between the points ρ(vi) and ρ(vj). Thus, a graph drawing is
a function ρ: V →Rn.
We deﬁne the matrix of a graph drawing ρ (in Rn) as a m×n matrix R
whose ith row consists of the row vector ρ(vi) corresponding to the point
representing vi in Rn. Typically, we want n < m; in fact n should be much
smaller than m.
Since there are inﬁnitely many graph drawings, it is desirable to have
some criterion to decide which graph is better than another. Inspired by
a physical model in which the edges are springs, it is natural to consider
a representation to be better if it requires the springs to be less extended.
We can formalize this by deﬁning the energy of a drawing R by
E(R) =
X
{vi,vj}∈E
∥ρ(vi) −ρ(vj)∥2 ,
where ρ(vi) is the ith row of R and ∥ρ(vi) −ρ(vj)∥2 is the square of the
Euclidean length of the line segment joining ρ(vi) and ρ(vj).
Then "good drawings" are drawings that minimize the energy function
E. Of course, the trivial representation corresponding to the zero matrix
is optimum, so we need to impose extra constraints to rule out the trivial
solution.

662
Graphs and Graph Laplacians; Basic Facts
We can consider the more general situation where the springs are not
necessarily identical. This can be modeled by a symmetric weight (or stiﬀ-
ness) matrix W = (wij), with wij ≥0. In this case, our energy function
becomes
E(R) =
X
{vi,vj}∈E
wij ∥ρ(vi) −ρ(vj)∥2 .
Following Godsil and Royle [Godsil and Royle (2001)], we prove that
E(R) = tr(R⊤LR),
where
L = D −W,
is the familiar unnormalized Laplacian matrix associated with W, and
where D is the degree matrix associated with W.
It can be shown that there is no loss in generality in assuming that the
columns of R are pairwise orthogonal and that they have unit length. Such
a matrix satisﬁes the equation R⊤R = I and the corresponding drawing is
called an orthogonal drawing. This condition also rules out trivial drawings.
Then we prove the main theorem about graph drawings (Theorem 19.1),
which essentially says that the matrix R of the desired graph drawing is
constituted by the n eigenvectors of L associated with the smallest nonzero
n eigenvalues of L. We give a number examples of graph drawings, many
of which are borrowed or adapted from Spielman [Spielman (2012)].
18.1
Directed Graphs, Undirected Graphs, Incidence
Matrices, Adjacency Matrices, Weighted Graphs
Deﬁnition 18.1. A directed graph is a pair G = (V, E), where V
=
{v1, . . . , vm} is a set of nodes or vertices, and E ⊆V × V is a set of
ordered pairs of distinct nodes (that is, pairs (u, v) ∈V × V with u ̸= v),
called edges. Given any edge e = (u, v), we let s(e) = u be the source of e
and t(e) = v be the target of e.
Remark: Since an edge is a pair (u, v) with u ̸= v, self-loops are not
allowed. Also, there is at most one edge from a node u to a node v. Such
graphs are sometimes called simple graphs.
An example of a directed graph is shown in Figure 18.2.

18.1. Directed Graphs, Undirected Graphs, Weighted Graphs
663
v4
v5
v1
v2
v3
e1
e7
e2
e3
e4
e5
e6
Fig. 18.2
Graph G1.
Deﬁnition 18.2. For every node v ∈V , the degree d(v) of v is the number
of edges leaving or entering v:
d(v) = |{u ∈V | (v, u) ∈E or (u, v) ∈E}|.
We abbreviate d(vi) as di. The degree matrix, D(G), is the diagonal matrix
D(G) = diag(d1, . . . , dm).
For example, for graph G1, we have
D(G1) =






2 0 0 0 0
0 4 0 0 0
0 0 3 0 0
0 0 0 3 0
0 0 0 0 2






.
Unless confusion arises, we write D instead of D(G).
Deﬁnition 18.3. Given a directed graph G = (V, E), for any two nodes
u, v ∈V , a path from u to v is a sequence of nodes (v0, v1, . . . , vk) such that
v0 = u, vk = v, and (vi, vi+1) is an edge in E for all i with 0 ≤i ≤k −1.
The integer k is the length of the path. A path is closed if u = v. The
graph G is strongly connected if for any two distinct nodes u, v ∈V , there
is a path from u to v and there is a path from v to u.
Remark: The terminology walk is often used instead of path, the word
path being reserved to the case where the nodes vi are all distinct, except
that v0 = vk when the path is closed.

664
Graphs and Graph Laplacians; Basic Facts
The binary relation on V ×V deﬁned so that u and v are related iﬀthere
is a path from u to v and there is a path from v to u is an equivalence relation
whose equivalence classes are called the strongly connected components of
G.
Deﬁnition 18.4. Given a directed graph G
=
(V, E), with V
=
{v1, . . . , vm}, if E = {e1, . . . , en}, then the incidence matrix B(G) of G
is the m × n matrix whose entries bi j are given by
bi j =







+1
if s(ej) = vi
−1
if t(ej) = vi
0
otherwise.
Here is the incidence matrix of the graph G1:
B =






1
1
0
0
0
0
0
−1 0 −1 −1 1
0
0
0 −1 1
0
0
0
1
0
0
0
1
0 −1 −1
0
0
0
0 −1 1
0






.
Observe that every column of an incidence matrix contains exactly two
nonzero entries, +1 and −1. Again, unless confusion arises, we write B
instead of B(G).
When a directed graph has m nodes v1, . . . , vm and n edges e1, . . . , en,
a vector x ∈Rm can be viewed as a function x: V →R assigning the value
xi to the node vi. Under this interpretation, Rm is viewed as RV . Similarly,
a vector y ∈Rn can be viewed as a function in RE. This point of view is
often useful. For example, the incidence matrix B can be interpreted as a
linear map from RE to RV , the boundary map, and B⊤can be interpreted
as a linear map from RV to RE, the coboundary map.
Remark: Some authors adopt the opposite convention of sign in deﬁning
the incidence matrix, which means that their incidence matrix is −B.
Undirected graphs are obtained from directed graphs by forgetting the
orientation of the edges.
Deﬁnition 18.5. A graph (or undirected graph) is a pair G = (V, E), where
V = {v1, . . . , vm} is a set of nodes or vertices, and E is a set of two-element
subsets of V (that is, subsets {u, v}, with u, v ∈V and u ̸= v), called edges.

18.1. Directed Graphs, Undirected Graphs, Weighted Graphs
665
Remark: Since an edge is a set {u, v}, we have u ̸= v, so self-loops are not
allowed. Also, for every set of nodes {u, v}, there is at most one edge be-
tween u and v. As in the case of directed graphs, such graphs are sometimes
called simple graphs.
An example of a graph is shown in Figure 18.3.
v4
v5
v1
v2
v3
a
g
b
c
d
e
f
Fig. 18.3
The undirected graph G2.
Deﬁnition 18.6. For every node v ∈V , the degree d(v) of v is the number
of edges incident to v:
d(v) = |{u ∈V | {u, v} ∈E}|.
The degree matrix D(G) (or simply, D) is deﬁned as in Deﬁnition 18.2.
Deﬁnition 18.7. Given a (undirected) graph G = (V, E), for any two
nodes u, v ∈V , a path from u to v is a sequence of nodes (v0, v1, . . . , vk)
such that v0 = u, vk = v, and {vi, vi+1} is an edge in E for all i with
0 ≤i ≤k −1. The integer k is the length of the path. A path is closed
if u = v. The graph G is connected if for any two distinct nodes u, v ∈V ,
there is a path from u to v.
Remark: The terminology walk or chain is often used instead of path, the
word path being reserved to the case where the nodes vi are all distinct,
except that v0 = vk when the path is closed.
The binary relation on V × V deﬁned so that u and v are related iﬀ
there is a path from u to v is an equivalence relation whose equivalence
classes are called the connected components of G.

666
Graphs and Graph Laplacians; Basic Facts
The notion of incidence matrix for an undirected graph is not as useful
as in the case of directed graphs.
Deﬁnition 18.8. Given a graph G = (V, E), with V = {v1, . . . , vm}, if
E = {e1, . . . , en}, then the incidence matrix B(G) of G is the m×n matrix
whose entries bi j are given by
bi j =
(
+1
if ej = {vi, vk} for some k
0
otherwise.
Unlike the case of directed graphs, the entries in the incidence matrix of
a graph (undirected) are nonnegative. We usually write B instead of B(G).
Deﬁnition 18.9. If G = (V, E) is a directed or an undirected graph, given
a node u ∈V , any node v ∈V such that there is an edge (u, v) in the
directed case or {u, v} in the undirected case is called adjacent to u, and
we often use the notation
u ∼v.
Observe that the binary relation ∼is symmetric when G is an undirected
graph, but in general it is not symmetric when G is a directed graph.
The notion of adjacency matrix is basically the same for directed or
undirected graphs.
Deﬁnition 18.10. Given a directed or undirected graph G = (V, E), with
V = {v1, . . . , vm}, the adjacency matrix A(G) of G is the symmetric m×m
matrix (ai j) such that
(1) If G is directed, then
ai j =
(
1
if there is some edge (vi, vj) ∈E or some edge (vj, vi) ∈E
0
otherwise.
(2) Else if G is undirected, then
ai j =
(
1
if there is some edge {vi, vj} ∈E
0
otherwise.
As usual, unless confusion arises, we write A instead of A(G). Here is
the adjacency matrix of both graphs G1 and G2:

18.1. Directed Graphs, Undirected Graphs, Weighted Graphs
667
A =






0 1 1 0 0
1 0 1 1 1
1 1 0 1 0
0 1 1 0 1
0 1 0 1 0






.
If G = (V, E) is an undirected graph, the adjacency matrix A of G can
be viewed as a linear map from RV to RV , such that for all x ∈Rm, we
have
(Ax)i =
X
j∼i
xj;
that is, the value of Ax at vi is the sum of the values of x at the nodes vj
adjacent to vi. The adjacency matrix can be viewed as a diﬀusion operator.
This observation yields a geometric interpretation of what it means for a
vector x ∈Rm to be an eigenvector of A associated with some eigenvalue
λ; we must have
λxi =
X
j∼i
xj,
i = 1, . . . , m,
which means that the the sum of the values of x assigned to the nodes vj
adjacent to vi is equal to λ times the value of x at vi.
Deﬁnition 18.11. Given any undirected graph G = (V, E), an orientation
of G is a function σ: E →V × V assigning a source and a target to every
edge in E, which means that for every edge {u, v} ∈E, either σ({u, v}) =
(u, v) or σ({u, v}) = (v, u). The oriented graph Gσ obtained from G by
applying the orientation σ is the directed graph Gσ = (V, Eσ), with Eσ =
σ(E).
The following result shows how the number of connected components of
an undirected graph is related to the rank of the incidence matrix of any
oriented graph obtained from G.
Proposition 18.1. Let G = (V, E) be any undirected graph with m vertices,
n edges, and c connected components. For any orientation σ of G, if B is
the incidence matrix of the oriented graph Gσ, then c = dim(Ker (B⊤)),
and B has rank m −c.
Furthermore, the nullspace of B⊤has a basis
consisting of indicator vectors of the connected components of G; that is,
vectors (z1, . . . , zm) such that zj = 1 iﬀvj is in the ith component Ki of
G, and zj = 0 otherwise.

668
Graphs and Graph Laplacians; Basic Facts
Proof. (After Godsil and Royle [Godsil and Royle (2001)], Section 8.3.)
The fact that rank(B) = m −c will be proved last.
Let us prove that the kernel of B⊤has dimension c. A vector z ∈Rm
belongs to the kernel of B⊤iﬀB⊤z = 0 iﬀz⊤B = 0.
In view of the
deﬁnition of B, for every edge {vi, vj} of G, the column of B corresponding
to the oriented edge σ({vi, vj}) has zero entries except for a +1 and a −1
in position i and position j or vice-versa, so we have
zi = zj.
An easy induction on the length of the path shows that if there is a path
from vi to vj in G (unoriented), then zi = zj. Therefore, z has a constant
value on any connected component of G. It follows that every vector z ∈
Ker (B⊤) can be written uniquely as a linear combination
z = λ1z1 + · · · + λczc,
where the vector zi corresponds to the ith connected component Ki of G
and is deﬁned such that
zi
j =
(
1
iﬀvj ∈Ki
0
otherwise.
This shows that dim(Ker (B⊤)) = c, and that Ker (B⊤) has a basis con-
sisting of indicator vectors.
Since B⊤is a n × m matrix, we have
m = dim(Ker (B⊤)) + rank(B⊤),
and since we just proved that dim(Ker (B⊤)) = c, we obtain rank(B⊤) =
m −c. Since B and B⊤have the same rank, rank(B) = m −c, as claimed.
Deﬁnition 18.12. Following common practice, we denote by 1 the (col-
umn) vector (of dimension m) whose components are all equal to 1.
Since every column of B contains a single +1 and a single −1, the rows
of B⊤sum to zero, which can be expressed as
B⊤1 = 0.
According to Proposition 18.1, the graph G is connected iﬀB has rank
m −1 iﬀthe nullspace of B⊤is the one-dimensional space spanned by 1.
In many applications, the notion of graph needs to be generalized to
capture the intuitive idea that two nodes u and v are linked with a degree

18.1. Directed Graphs, Undirected Graphs, Weighted Graphs
669
of certainty (or strength). Thus, we assign a nonnegative weight wi j to
an edge {vi, vj}; the smaller wi j is, the weaker is the link (or similarity)
between vi and vj, and the greater wi j is, the stronger is the link (or
similarity) between vi and vj.
Deﬁnition 18.13. A weighted graph is a pair G = (V, W), where V =
{v1, . . . , vm} is a set of nodes or vertices, and W is a symmetric matrix
called the weight matrix, such that wi j ≥0 for all i, j ∈{1, . . . , m}, and
wi i = 0 for i = 1, . . . , m. We say that a set {vi, vj} is an edge iﬀwi j > 0.
The corresponding (undirected) graph (V, E) with E = {{vi, vj} | wi j > 0},
is called the underlying graph of G.
Remark: Since wi i = 0, these graphs have no self-loops. We can think of
the matrix W as a generalized adjacency matrix. The case where wi j ∈
{0, 1} is equivalent to the notion of a graph as in Deﬁnition 18.5.
We can think of the weight wi j of an edge {vi, vj} as a degree of sim-
ilarity (or aﬃnity) in an image, or a cost in a network. An example of a
weighted graph is shown in Figure 18.4. The thickness of an edge corre-
sponds to the magnitude of its weight.
Fig. 18.4
A weighted graph.
Deﬁnition 18.14. Given a weighted graph G = (V, W), for every node

670
Graphs and Graph Laplacians; Basic Facts
vi ∈V , the degree d(vi) of vi is the sum of the weights of the edges adjacent
to vi:
d(vi) =
m
X
j=1
wi j.
Note that in the above sum, only nodes vj such that there is an edge {vi, vj}
have a nonzero contribution. Such nodes are said to be adjacent to vi, and
we write vi ∼vj. The degree matrix D(G) (or simply, D) is deﬁned as
before, namely by D(G) = diag(d(v1), . . . , d(vm)).
The weight matrix W can be viewed as a linear map from RV to itself.
For all x ∈Rm, we have
(Wx)i =
X
j∼i
wijxj;
that is, the value of Wx at vi is the weighted sum of the values of x at the
nodes vj adjacent to vi.
Observe that W1 is the (column) vector (d(v1), . . . , d(vm)) consisting
of the degrees of the nodes of the graph.
We now deﬁne the most important concept of this chapter: the Lapla-
cian matrix of a graph. Actually, as we will see, it comes in several ﬂavors.
18.2
Laplacian Matrices of Graphs
Let us begin with directed graphs, although as we will see, graph Laplacians
are fundamentally associated with undirected graph. The key proposition
below shows how given an undirected graph G, for any orientation σ of
G, Bσ(Bσ)⊤relates to the adjacency matrix A (where Bσ is the incidence
matrix of the directed graph Gσ). We reproduce the proof in Gallier [Gallier
(2011a)] (see also Godsil and Royle [Godsil and Royle (2001)]).
Proposition 18.2. Given any undirected graph G, for any orientation σ of
G, if Bσis the incidence matrix of the directed graph Gσ, A is the adjacency
matrix of Gσ, and D is the degree matrix such that Di i = d(vi), then
Bσ(Bσ)⊤= D −A.
Consequently, L = Bσ(Bσ)⊤is independent of the orientation σ of G, and
D −A is symmetric and positive semideﬁnite; that is, the eigenvalues of
D −A are real and nonnegative.

18.2. Laplacian Matrices of Graphs
671
Proof. The entry Bσ(Bσ)⊤
i j is the inner product of the ith row bσ
i , and
the jth row bσ
j of Bσ. If i = j, then as
bσ
i k =







+1
if s(ek) = vi
−1
if t(ek) = vi
0
otherwise
we see that bσ
i · bσ
i = d(vi). If i ̸= j, then bσ
i · bσ
j ̸= 0 iﬀthere is some
edge ek with s(ek) = vi and t(ek) = vj or vice-versa (which are mutually
exclusive cases, since Gσ arises by orienting an undirected graph), in which
case, bσ
i · bσ
j = −1. Therefore,
Bσ(Bσ)⊤= D −A,
as claimed.
For every x ∈Rm, we have
x⊤Lx = x⊤Bσ(Bσ)⊤x = ((Bσ)⊤x)⊤(Bσ)⊤x =
(Bσ)⊤x
2
2 ≥0,
since the Euclidean norm ∥∥2 is positive (deﬁnite).
Therefore, L =
Bσ(Bσ)⊤is positive semideﬁnite.
It is well-known that a real symmet-
ric matrix is positive semideﬁnite iﬀits eigenvalues are nonnegative.
Deﬁnition 18.15. The matrix L = Bσ(Bσ)⊤= D −A is called the (un-
normalized) graph Laplacian of the graph Gσ. The (unnormalized) graph
Laplacian of an undirected graph G = (V, E) is deﬁned by
L = D −A.
For example, the graph Laplacian of graph G1 is
L =






2 −1 −1 0
0
−1 4 −1 −1 −1
−1 −1 3 −1 0
0 −1 −1 3 −1
0 −1 0 −1 2






.
Observe that each row of L sums to zero (because (Bσ)⊤1 = 0). Con-
sequently, the vector 1 is in the nullspace of L.
Remarks:
(1) With the unoriented version of the incidence matrix (see Deﬁni-
tion 18.8), it can be shown that
BB⊤= D + A.

672
Graphs and Graph Laplacians; Basic Facts
(2) As pointed out by Evangelos Chatzipantazis, Proposition 18.2 in which
the incidence matrix Bσ is replaced by the incidence matrix B of any
arbitrary directed graph G does not hold. The problem is that such
graphs may have both edges (vi, vj) and (vj, vi) between two distinct
nodes vi and vj, and as a consequence, the inner product bi · bj = −2
instead of −1. A simple counterexample is given by the directed graph
with three vertices and four edges whose incidence matrix is given by
B =


1 −1 0 −1
−1 1 −1 0
0
0
1
1

.
We have
BB⊤=


3 −2 −1
−2 3 −1
−1 −1 2

̸=


3 0 0
0 3 0
0 0 2

−


0 1 1
1 0 1
1 1 0

= D −A.
The natural generalization of the notion of graph Laplacian to weighted
graphs is this:
Deﬁnition 18.16. Given any weighted graph G = (V, W) with V
=
{v1, . . . , vm}, the (unnormalized) graph Laplacian L(G) of G is deﬁned by
L(G) = D(G) −W,
where D(G) = diag(d1, . . . , dm) is the degree matrix of G (a diagonal ma-
trix), with
di =
m
X
j=1
wi j.
As usual, unless confusion arises, we write D instead of D(G) and L instead
of L(G).
The graph Laplacian can be interpreted as a linear map from RV to
itself. For all x ∈RV , we have
(Lx)i =
X
j∼i
wij(xi −xj).
It is clear from the equation L = D −W that each row of L sums to 0,
so the vector 1 is the nullspace of L, but it is less obvious that L is positive
semideﬁnite. One way to prove it is to generalize slightly the notion of
incidence matrix.
Deﬁnition 18.17. Given a weighted graph G = (V, W), with V
=
{v1, . . . , vm}, if {e1, . . ., en} are the edges of the underlying graph of G

18.2. Laplacian Matrices of Graphs
673
(recall that {vi, vj} is an edge of this graph iﬀwij > 0), for any oriented
graph Gσ obtained by giving an orientation to the underlying graph of G,
the incidence matrix Bσ of Gσ is the m × n matrix whose entries bi j are
given by
bi j =







+√wij
if s(ej) = vi
−√wij
if t(ej) = vi
0
otherwise.
For example, given the weight matrix
W =




0 3 6 3
3 0 0 3
6 0 0 3
3 3 3 0



,
the incidence matrix B corresponding to the orientation of the underlying
graph of W where an edge (i, j) is oriented positively iﬀi < j is
B =




1.7321
2.4495
1.7321
0
0
−1.7321
0
0
1.7321
0
0
−2.4495
0
0
1.7321
0
0
−1.7321 −1.7321 −1.7321



.
The reader should verify that BB⊤= D −W. This is true in general, see
Proposition 18.3.
It is easy to see that Proposition 18.1 applies to the underlying graph
of G. For any oriented graph Gσ obtained from the underlying graph of
G, the rank of the incidence matrix Bσ is equal to m −c, where c is the
number of connected components of the underlying graph of G, and we
have (Bσ)⊤1 = 0. We also have the following version of Proposition 18.2
whose proof is immediately adapted.
Proposition 18.3. Given any weighted graph G = (V, W) with V
=
{v1, . . . , vm}, if Bσ is the incidence matrix of any oriented graph Gσ ob-
tained from the underlying graph of G and D is the degree matrix of G,
then
Bσ(Bσ)⊤= D −W = L.
Consequently, Bσ(Bσ)⊤is independent of the orientation of the underlying
graph of G and L = D −W is symmetric and positive semideﬁnite; that is,
the eigenvalues of L = D −W are real and nonnegative.

674
Graphs and Graph Laplacians; Basic Facts
Another way to prove that L is positive semideﬁnite is to evaluate the
quadratic form x⊤Lx.
Proposition 18.4. For any m × m symmetric matrix W = (wij), if we
let L = D −W where D is the degree matrix associated with W (that is,
di = Pm
j=1 wij), then we have
x⊤Lx = 1
2
m
X
i,j=1
wi j(xi −xj)2
for all x ∈Rm.
Consequently, x⊤Lx does not depend on the diagonal entries in W, and if
wi j ≥0 for all i, j ∈{1, . . . , m}, then L is positive semideﬁnite.
Proof. We have
x⊤Lx = x⊤Dx −x⊤Wx
=
m
X
i=1
dix2
i −
m
X
i,j=1
wi jxixj
= 1
2


m
X
i=1
dix2
i −2
m
X
i,j=1
wi jxixj +
m
X
i=1
dix2
i


= 1
2
m
X
i,j=1
wi j(xi −xj)2.
Obviously, the quantity on the right-hand side does not depend on the
diagonal entries in W, and if wi j ≥0 for all i, j, then this quantity is
nonnegative.
Proposition 18.4 immediately implies the following facts:
For any
weighted graph G = (V, W),
(1) The eigenvalues 0 = λ1 ≤λ2 ≤. . . ≤λm of L are real and nonnegative,
and there is an orthonormal basis of eigenvectors of L.
(2) The smallest eigenvalue λ1 of L is equal to 0, and 1 is a corresponding
eigenvector.
It turns out that the dimension of the nullspace of L (the eigenspace of
0) is equal to the number of connected components of the underlying graph
of G.
Proposition 18.5. Let G = (V, W) be a weighted graph. The number c
of connected components K1, . . . , Kc of the underlying graph of G is equal

18.3. Normalized Laplacian Matrices of Graphs
675
to the dimension of the nullspace of L, which is equal to the multiplicity
of the eigenvalue 0. Furthermore, the nullspace of L has a basis consist-
ing of indicator vectors of the connected components of G, that is, vectors
(f1, . . . , fm) such that fj = 1 iﬀvj ∈Ki and fj = 0 otherwise.
Proof. Since L = BB⊤for the incidence matrix B associated with any ori-
ented graph obtained from G, and since L and B⊤have the same nullspace,
by Proposition 18.1, the dimension of the nullspace of L is equal to the
number c of connected components of G and the indicator vectors of the
connected components of G form a basis of Ker (L).
Proposition 18.5 implies that if the underlying graph of G is connected,
then the second eigenvalue λ2 of L is strictly positive.
Remarkably, the eigenvalue λ2 contains a lot of information about the
graph G (assuming that G = (V, E) is an undirected graph). This was ﬁrst
discovered by Fiedler in 1973, and for this reason, λ2 is often referred to
as the Fiedler number. For more on the properties of the Fiedler number,
see Godsil and Royle [Godsil and Royle (2001)] (Chapter 13) and Chung
[Chung (1997)]. More generally, the spectrum (0, λ2, . . . , λm) of L contains
a lot of information about the combinatorial structure of the graph G.
Leverage of this information is the object of spectral graph theory.
18.3
Normalized Laplacian Matrices of Graphs
It turns out that normalized variants of the graph Laplacian are needed,
especially in applications to graph clustering. These variants make sense
only if G has no isolated vertices.
Deﬁnition 18.18. Given a weighted graph G = (V, W), a vertex u ∈V is
isolated if it is not incident to any other vertex. This means that every row
of W contains some strictly positive entry.
If G has no isolated vertices, then the degree matrix D contains positive
entries, so it is invertible and D−1/2 makes sense; namely
D−1/2 = diag(d−1/2
1
, . . . , d−1/2
m
),
and similarly for any real exponent α.
Deﬁnition 18.19. Given any weighted directed graph G = (V, W) with
no isolated vertex and with V
= {v1, . . . , vm}, the (normalized) graph

676
Graphs and Graph Laplacians; Basic Facts
Laplacians Lsym and Lrw of G are deﬁned by
Lsym = D−1/2LD−1/2 = I −D−1/2WD−1/2
Lrw = D−1L = I −D−1W.
Observe that the Laplacian Lsym = D−1/2LD−1/2 is a symmetric matrix
(because L and D−1/2 are symmetric) and that
Lrw = D−1/2LsymD1/2.
The reason for the notation Lrw is that this matrix is closely related to a
random walk on the graph G.
Example 18.1. As an example, the matrices Lsym and Lrw associated with
the graph G1 are
Lsym =






1.0000 −0.3536 −0.4082
0
0
−0.3536 1.0000 −0.2887 −0.2887 −0.3536
−0.4082 −0.2887 1.0000 −0.3333
0
0
−0.2887 −0.3333 1.0000 −0.4082
0
−0.3536
0
−0.4082 1.0000






and
Lrw =






1.0000 −0.5000 −0.5000
0
0
−0.2500 1.0000 −0.2500 −0.2500 −0.2500
−0.3333 −0.3333 1.0000 −0.3333
0
0
−0.3333 −0.3333 1.0000 −0.3333
0
−0.5000
0
−0.5000 1.0000






.
Since the unnormalized Laplacian L can be written as L = BB⊤, where
B is the incidence matrix of any oriented graph obtained from the under-
lying graph of G = (V, W), if we let
Bsym = D−1/2B,
we get
Lsym = BsymB⊤
sym.
In particular, for any singular decomposition Bsym = UΣV ⊤of Bsym (with
U an m × m orthogonal matrix, Σ a "diagonal" m × n matrix of singular
values, and V an n × n orthogonal matrix), the eigenvalues of Lsym are
the squares of the top m singular values of Bsym, and the vectors in U
are orthonormal eigenvectors of Lsym with respect to these eigenvalues (the
squares of the top m diagonal entries of Σ). Computing the SVD of Bsym

18.3. Normalized Laplacian Matrices of Graphs
677
generally yields more accurate results than diagonalizing Lsym, especially
when Lsym has eigenvalues with high multiplicity.
There are simple relationships between the eigenvalues and the eigen-
vectors of Lsym, and Lrw.
There is also a simple relationship with the
generalized eigenvalue problem Lx = λDx.
Proposition 18.6. Let G = (V, W) be a weighted graph without isolated
vertices. The graph Laplacians, L, Lsym, and Lrw satisfy the following prop-
erties:
(1) The matrix Lsym is symmetric and positive semideﬁnite. In fact,
x⊤Lsymx = 1
2
m
X
i,j=1
wi j
 
xi
√di
−
xj
p
dj
!2
for all x ∈Rm.
(2) The normalized graph Laplacians Lsym and Lrw have the same spec-
trum (0 = ν1 ≤ν2 ≤. . . ≤νm), and a vector u ̸= 0 is an eigenvector
of Lrw for λ iﬀD1/2u is an eigenvector of Lsym for λ.
(3) The graph Laplacians L and Lsym are symmetric and positive semidef-
inite.
(4) A vector u ̸= 0 is a solution of the generalized eigenvalue problem
Lu = λDu iﬀD1/2u is an eigenvector of Lsym for the eigenvalue λ iﬀ
u is an eigenvector of Lrw for the eigenvalue λ.
(5) The graph Laplacians, L and Lrw have the same nullspace. For any
vector u, we have u ∈Ker (L) iﬀD1/2u ∈Ker (Lsym).
(6) The vector 1 is in the nullspace of Lrw, and D1/21 is in the nullspace
of Lsym.
(7) For every eigenvalue νi of the normalized graph Laplacian Lsym, we
have 0 ≤νi ≤2. Furthermore, νm = 2 iﬀthe underlying graph of G
contains a nontrivial connected bipartite component.
(8) If m ≥2 and if the underlying graph of G is not a complete graph,1
then ν2 ≤1. Furthermore the underlying graph of G is a complete
graph iﬀν2 =
m
m−1.
(9) If m ≥2 and if the underlying graph of G is connected, then ν2 > 0.
(10) If m ≥2 and if the underlying graph of G has no isolated vertices,
then νm ≥
m
m−1.
Proof. (1) We have Lsym = D−1/2LD−1/2, and D−1/2 is a symmet-
ric invertible matrix (since it is an invertible diagonal matrix).
It is a
1Recall that an undirected graph is complete if for any two distinct nodes u, v, there is
an edge {u, v}.

678
Graphs and Graph Laplacians; Basic Facts
well-known fact of linear algebra that if B is an invertible matrix, then
a matrix S is symmetric, positive semideﬁnite iﬀBSB⊤is symmetric,
positive semideﬁnite.
Since L is symmetric, positive semideﬁnite, so is
Lsym = D−1/2LD−1/2. The formula
x⊤Lsymx = 1
2
m
X
i,j=1
wi j
 
xi
√di
−
xj
p
dj
!2
for all x ∈Rm
follows immediately from Proposition 18.4 by replacing x by D−1/2x, and
also shows that Lsym is positive semideﬁnite.
(2) Since
Lrw = D−1/2LsymD1/2,
the matrices Lsym and Lrw are similar, which implies that they have the
same spectrum. In fact, since D1/2 is invertible,
Lrwu = D−1Lu = λu
iﬀ
D−1/2Lu = λD1/2u
iﬀ
D−1/2LD−1/2D1/2u = LsymD1/2u = λD1/2u,
which shows that a vector u ̸= 0 is an eigenvector of Lrw for λ iﬀD1/2u is
an eigenvector of Lsym for λ.
(3) We already know that L and Lsym are positive semideﬁnite.
(4) Since D−1/2 is invertible, we have
Lu = λDu
iﬀ
D−1/2Lu = λD1/2u
iﬀ
D−1/2LD−1/2D1/2u = LsymD1/2u = λD1/2u,
which shows that a vector u ̸= 0 is a solution of the generalized eigenvalue
problem Lu = λDu iﬀD1/2u is an eigenvector of Lsym for the eigenvalue
λ. The second part of the statement follows from (2).
(5) Since D−1 is invertible, we have Lu = 0 iﬀD−1Lu = Lrwu = 0. Sim-
ilarly, since D−1/2 is invertible, we have Lu = 0 iﬀD−1/2LD−1/2D1/2u = 0
iﬀD1/2u ∈Ker (Lsym).
(6) Since L1 = 0, we get Lrw1 = D−1L1 = 0. That D1/21 is in the
nullspace of Lsym follows from (2). Properties (7)-(10) are proven in Chung
[Chung (1997)] (Chapter 1).

18.4. Graph Clustering Using Normalized Cuts
679
The eigenvalues the matrices Lsym and Lrw from Example 18.1 are
0, 7257, 1.1667, 1.5, 1.6076.
On the other hand, the eigenvalues of the unormalized Laplacian for G1 are
0, 1.5858, 3, 4.4142, 5.
Remark: Observe that although the matrices Lsym and Lrw have the same
spectrum, the matrix Lrw is generally not symmetric, whereas Lsym is sym-
metric.
A version of Proposition 18.5 also holds for the graph Laplacians Lsym
and Lrw. This follows easily from the fact that Proposition 18.1 applies to
the underlying graph of a weighted graph. The proof is left as an exercise.
Proposition 18.7. Let G = (V, W) be a weighted graph. The number c of
connected components K1, . . . , Kc of the underlying graph of G is equal to
the dimension of the nullspace of both Lsym and Lrw, which is equal to the
multiplicity of the eigenvalue 0. Furthermore, the nullspace of Lrw has a
basis consisting of indicator vectors of the connected components of G, that
is, vectors (f1, . . . , fm) such that fj = 1 iﬀvj ∈Ki and fj = 0 otherwise.
For Lsym, a basis of the nullpace is obtained by multiplying the above basis
of the nullspace of Lrw by D1/2.
A particularly interesting application of graph Laplacians is graph clus-
tering.
18.4
Graph Clustering Using Normalized Cuts
In order to explain this problem we need some deﬁnitions.
Deﬁnition 18.20. Given any subset of nodes A ⊆V , we deﬁne the volume
vol(A) of A as the sum of the weights of all edges adjacent to nodes in A:
vol(A) =
X
vi∈A
m
X
j=1
wi j.
Given any two subsets A, B ⊆V (not necessarily distinct), we deﬁne
links(A, B) by
links(A, B) =
X
vi∈A,vj∈B
wi j.
The quantity links(A, A) = links(A, A) (where A = V −A denotes the
complement of A in V ) measures how many links escape from A (and A).
We deﬁne the cut of A as
cut(A) = links(A, A).

680
Graphs and Graph Laplacians; Basic Facts
The notion of volume is illustrated in Figure 18.5 and the notions of cut
is illustrated in Figure 18.6.
Fig. 18.5
Volume of a set of nodes.
Fig. 18.6
A cut involving the set of nodes in the center and the nodes on the perimeter.
The above concepts play a crucial role in the theory of normalized cuts.
This beautiful and deeply original method ﬁrst published in Shi and Ma-
lik [Shi and Malik (2000)], has now come to be a "textbook chapter" of
computer vision and machine learning. It was invented by Jianbo Shi and
Jitendra Malik and was the main topic of Shi's dissertation. This method
was extended to K ≥3 clusters by Stella Yu in her dissertation [Yu (2003)]

18.4. Graph Clustering Using Normalized Cuts
681
and is also the subject of Yu and Shi [Yu and Shi (2003)].
Given a set of data, the goal of clustering is to partition the data into
diﬀerent groups according to their similarities. When the data is given in
terms of a similarity graph G, where the weight wi j between two nodes vi
and vj is a measure of similarity of vi and vj, the problem can be stated as
follows: Find a partition (A1, . . . , AK) of the set of nodes V into diﬀerent
groups such that the edges between diﬀerent groups have very low weight
(which indicates that the points in diﬀerent clusters are dissimilar), and the
edges within a group have high weight (which indicates that points within
the same cluster are similar).
The above graph clustering problem can be formalized as an optimiza-
tion problem, using the notion of cut mentioned earlier. If we want to par-
tition V into K clusters, we can do so by ﬁnding a partition (A1, . . . , AK)
that minimizes the quantity
cut(A1, . . . , AK) = 1
2
K
X
i=1
cut(Ai) = 1
2
K
X
i=1
links(Ai, Ai).
For K = 2, the mincut problem is a classical problem that can be solved
eﬃciently, but in practice, it does not yield satisfactory partitions. Indeed,
in many cases, the mincut solution separates one vertex from the rest of
the graph. What we need is to design our cost function in such a way that
it keeps the subsets Ai "reasonably large" (reasonably balanced).
An example of a weighted graph and a partition of its nodes into two
clusters is shown in Figure 18.7.
A way to get around this problem is to normalize the cuts by dividing
by some measure of each subset Ai. A solution using the volume vol(Ai)
of Ai (for K = 2) was proposed and investigated in a seminal paper of Shi
and Malik [Shi and Malik (2000)]. Subsequently, Yu (in her dissertation
[Yu (2003)]) and Yu and Shi [Yu and Shi (2003)] extended the method to
K > 2 clusters. The idea is to minimize the cost function
Ncut(A1, . . . , AK) =
K
X
i=1
links(Ai, Ai)
vol(Ai)
=
K
X
i=1
cut(Ai, Ai)
vol(Ai)
.
The next step is to express our optimization problem in matrix form, and
this can be done in terms of Rayleigh ratios involving the graph Laplacian
in the numerators. This theory is very beautiful, but we do not have the
space to present it here. The interested reader is referred to Gallier [Gallier
(2019)].

682
Graphs and Graph Laplacians; Basic Facts
Fig. 18.7
A weighted graph and its partition into two clusters.
18.5
Summary
The main concepts and results of this chapter are listed below:
• Directed graphs, undirected graphs.
• Incidence matrices, adjacency matrices.
• Weighted graphs.
• Degree matrix.

18.6. Problems
683
• Graph Laplacian (unnormalized).
• Normalized graph Laplacian.
• Spectral graph theory.
• Graph clustering using normalized cuts.
18.6
Problems
Problem 18.1. Find the unnormalized Laplacian of the graph representing
a triangle and of the graph representing a square.
Problem 18.2. Consider the complete graph Km on m ≥2 nodes.
(1) Prove that the normalized Laplacian Lsym of K is
Lsym =







1
−1/(m −1) . . . −1/(m −1) −1/(m −1)
−1/(m −1)
1
. . . −1/(m −1) −1/(m −1)
...
...
...
...
...
−1/(m −1) −1/(m −1) . . .
1
−1/(m −1)
−1/(m −1) −1/(m −1) . . . −1/(m −1)
1







.
(2) Prove that the characteristic polynomial of Lsym is

λ −1
1/(m −1) . . . 1/(m −1) 1/(m −1)
1/(m −1)
λ −1
. . . 1/(m −1) 1/(m −1)
...
...
...
...
...
1/(m −1) 1/(m −1) . . .
λ −1
1/(m −1)
1/(m −1) 1/(m −1) . . . 1/(m −1)
λ −1

= λ

λ −
m
m −1
m−1
.
Hint. First subtract the second column from the ﬁrst, factor λ−m/(m−1),
and then add the ﬁrst row to the second. Repeat this process. You will
end up with the determinant

λ −1/(m −1)
1
1/(m −1)
λ −1
 .
Problem 18.3. Consider the complete bipartite graph Km,n on m+n ≥3
nodes, with edges between each of the ﬁrst m ≥1 nodes to each of the last
n ≥1 nodes. Prove that the eigenvalues of the normalized Laplacian Lsym
of Km,n are 0 with multiplicity m + n −2 and 1 with multiplicity 2.
Problem 18.4. Let G be a graph with a set of nodes V with m ≥2
elements, without isolated nodes, and let Lsym = D−1/2LD−1/2 be its
normalized Laplacian (with L its unnormalized Laplacian).

684
Graphs and Graph Laplacians; Basic Facts
(1) For any y ∈RV , consider the Rayleigh ratio
R = y⊤Lsym y
y⊤y
.
Prove that if x = D−1/2y, then
R =
x⊤Lx
(D1/2x)⊤D1/2x =
X
u∼v
(x(u) −x(v))2
X
v
dvx(v)2
.
(2) Prove that the second eigenvalue ν2 of Lsym is given by
ν2 =
min
1⊤Dx=0,x̸=0
X
u∼v
(x(u) −x(v))2
X
v
dvx(v)2
.
(3) Prove that the largest eigenvalue νm of Lsym is given by
νm = max
x̸=0
X
u∼v
(x(u) −x(v))2
X
v
dvx(v)2
.
Problem 18.5. Let G be a graph with a set of nodes V with m ≥2
elements, without isolated nodes.
If 0 = ν1 ≤ν1 ≤. . . ≤νm are the
eigenvalues of Lsym, prove the following properties:
(1) We have ν1 + ν2 + · · · + νm = m.
(2) We have ν2 ≤m/(m −1), with equality holding iﬀG = Km, the
complete graph on m nodes.
(3) We have νm ≥m/(m −1).
(4) If G is not a complete graph, then ν2 ≤1
Hint. If a and b are nonadjacent nodes, consider the function x given
by
x(v) =







db
if v = a
−da
if v = b
0
if v ̸= a, b,
and use Problem 18.4(2).
(5) Prove that νm ≤2. Prove that νm = 2 iﬀthe underlying graph of G
contains a nontrivial connected bipartite component.
Hint. Use Problem 18.4(3).

18.6. Problems
685
(6) Prove that if G is connected, then ν2 > 0.
Problem 18.6. Let G be a graph with a set of nodes V with m ≥2
elements, without isolated nodes. Let vol(G) = P
v∈V dv and let
x =
P
v dvx(v)
vol(G)
.
Prove that
ν2 = min
x̸=0
X
u∼v
(x(u) −x(v))2
X
v
dv(x(v) −x)2 .
Problem 18.7. Let G be a connected bipartite graph. Prove that if ν is
an eigenvalue of Lsym, then 2 −ν is also an eigenvalue of Lsym.
Problem 18.8. Prove Proposition 18.7.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 19
Spectral Graph Drawing
19.1
Graph Drawing and Energy Minimization
Let G = (V, E) be some undirected graph. It is often desirable to draw
a graph, usually in the plane but possibly in 3D, and it turns out that
the graph Laplacian can be used to design surprisingly good methods. Say
|V | = m. The idea is to assign a point ρ(vi) in Rn to the vertex vi ∈V ,
for every vi ∈V , and to draw a line segment between the points ρ(vi) and
ρ(vj) iﬀthere is an edge {vi, vj}.
Deﬁnition 19.1. Let G = (V, E) be some undirected graph with m ver-
tices. A graph drawing is a function ρ: V →Rn, for some n ≥1. The
matrix of a graph drawing ρ (in Rn) is a m × n matrix R whose ith row
consists of the row vector ρ(vi) corresponding to the point representing vi
in Rn.
For a graph drawing to be useful we want n ≤m; in fact n should be
much smaller than m, typically n = 2 or n = 3.
Deﬁnition 19.2. A graph drawing is balanced iﬀthe sum of the entries of
every column of the matrix of the graph drawing is zero, that is,
1⊤R = 0.
If a graph drawing is not balanced, it can be made balanced by a suitable
translation. We may also assume that the columns of R are linearly inde-
pendent, since any basis of the column space also determines the drawing.
Thus, from now on, we may assume that n ≤m.
Remark: A graph drawing ρ: V →Rn is not required to be injective,
which may result in degenerate drawings where distinct vertices are drawn
687

688
Spectral Graph Drawing
as the same point. For this reason, we prefer not to use the terminology
graph embedding, which is often used in the literature. This is because in
diﬀerential geometry, an embedding always refers to an injective map. The
term graph immersion would be more appropriate.
As explained in Godsil and Royle [Godsil and Royle (2001)], we can
imagine building a physical model of G by connecting adjacent vertices (in
Rn) by identical springs. Then it is natural to consider a representation to
be better if it requires the springs to be less extended. We can formalize
this by deﬁning the energy of a drawing R by
E(R) =
X
{vi,vj}∈E
∥ρ(vi) −ρ(vj)∥2 ,
where ρ(vi) is the ith row of R and ∥ρ(vi) −ρ(vj)∥2 is the square of the
Euclidean length of the line segment joining ρ(vi) and ρ(vj).
Then, "good drawings" are drawings that minimize the energy function
E. Of course, the trivial representation corresponding to the zero matrix
is optimum, so we need to impose extra constraints to rule out the trivial
solution.
We can consider the more general situation where the springs are not
necessarily identical. This can be modeled by a symmetric weight (or stiﬀ-
ness) matrix W = (wij), with wij ≥0. Then our energy function becomes
E(R) =
X
{vi,vj}∈E
wij ∥ρ(vi) −ρ(vj)∥2 .
It turns out that this function can be expressed in terms of the Laplacian
L = D−W. The following proposition is shown in Godsil and Royle [Godsil
and Royle (2001)]. We give a slightly more direct proof.
Proposition 19.1. Let G = (V, W) be a weighted graph, with |V | = m
and W an m × m symmetric matrix, and let R be the matrix of a graph
drawing ρ of G in Rn (a m×n matrix). If L = D −W is the unnormalized
Laplacian matrix associated with W, then
E(R) = tr(R⊤LR).
Proof. Since ρ(vi) is the ith row of R (and ρ(vj) is the jth row of R), if

19.1. Graph Drawing and Energy Minimization
689
we denote the kth column of R by Rk, using Proposition 18.4, we have
E(R) =
X
{vi,vj}∈E
wij ∥ρ(vi) −ρ(vj)∥2
=
n
X
k=1
X
{vi,vj}∈E
wij(Rik −Rjk)2
=
n
X
k=1
1
2
m
X
i,j=1
wij(Rik −Rjk)2
=
n
X
k=1
(Rk)⊤LRk = tr(R⊤LR),
as claimed.
Since the matrix R⊤LR is symmetric, it has real eigenvalues. Actually,
since L is positive semideﬁnite, so is R⊤LR. Then the trace of R⊤LR is
equal to the sum of its positive eigenvalues, and this is the energy E(R) of
the graph drawing.
If R is the matrix of a graph drawing in Rn, then for any n×n invertible
matrix M, the map that assigns ρ(vi)M to vi is another graph drawing of
G, and these two drawings convey the same amount of information. From
this point of view, a graph drawing is determined by the column space of R.
Therefore, it is reasonable to assume that the columns of R are pairwise
orthogonal and that they have unit length.
Such a matrix satisﬁes the
equation R⊤R = I.
Deﬁnition 19.3. If the matrix R of a graph drawing satisﬁes the equation
R⊤R = I, then the corresponding drawing is called an orthogonal graph
drawing.
This above condition also rules out trivial drawings. The following result
tells us how to ﬁnd minimum energy orthogonal balanced graph drawings,
provided the graph is connected. Recall that
L1 = 0,
as we already observed.
Theorem 19.1. Let G = (V, W) be a weighted graph with |V | = m. If
L = D −W is the (unnormalized) Laplacian of G, and if the eigenvalues
of L are 0 = λ1 < λ2 ≤λ3 ≤. . . ≤λm, then the minimal energy of any
balanced orthogonal graph drawing of G in Rn is equal to λ2 + · · · + λn+1

690
Spectral Graph Drawing
(in particular, this implies that n < m). The m × n matrix R consisting
of any unit eigenvectors u2, . . . , un+1 associated with λ2 ≤. . . ≤λn+1
yields a balanced orthogonal graph drawing of minimal energy; it satisﬁes
the condition R⊤R = I.
Proof. We present the proof given in Godsil and Royle [Godsil and Royle
(2001)] (Section 13.4, Theorem 13.4.1). The key point is that the sum of
the n smallest eigenvalues of L is a lower bound for tr(R⊤LR). This can be
shown using a Rayleigh ratio argument; see Proposition 16.13 (the Poincar´e
separation theorem). Then any n eigenvectors (u1, . . . , un) associated with
λ1, . . . , λn achieve this bound. Because the ﬁrst eigenvalue of L is λ1 = 0
and because we are assuming that λ2 > 0, we have u1 = 1/√m. Since the
uj are pairwise orthogonal for i = 2, . . . , n and since ui is orthogonal to
u1 = 1/√m, the entries in ui add up to 0. Consequently, for any ℓwith
2 ≤ℓ≤n, by deleting u1 and using (u2, . . . , uℓ), we obtain a balanced
orthogonal graph drawing in Rℓ−1 with the same energy as the orthogonal
graph drawing in Rℓusing (u1, u2, . . . , uℓ). Conversely, from any balanced
orthogonal drawing in Rℓ−1 using (u2, . . . , uℓ), we obtain an orthogonal
graph drawing in Rℓusing (u1, u2, . . . , uℓ) with the same energy. Therefore,
the minimum energy of a balanced orthogonal graph drawing in Rn is equal
to the minimum energy of an orthogonal graph drawing in Rn+1, and this
minimum is λ2 + · · · + λn+1.
Since 1 spans the nullspace of L, using u1 (which belongs to Ker L) as
one of the vectors in R would have the eﬀect that all points representing
vertices of G would have the same ﬁrst coordinate. This would mean that
the drawing lives in a hyperplane in Rn, which is undesirable, especially
when n = 2, where all vertices would be collinear. This is why we omit the
ﬁrst eigenvector u1.
Observe that for any orthogonal n × n matrix Q, since
tr(R⊤LR) = tr(Q⊤R⊤LRQ),
the matrix RQ also yields a minimum orthogonal graph drawing.
This
amounts to applying the rigid motion Q⊤to the rows of R.
In summary, if λ2 > 0, an automatic method for drawing a graph in R2
is this:
(1) Compute the two smallest nonzero eigenvalues λ2 ≤λ3 of the graph
Laplacian L (it is possible that λ3 = λ2 if λ2 is a multiple eigenvalue);
(2) Compute two unit eigenvectors u2, u3 associated with λ2 and λ3, and
let R = [u2 u3] be the m × 2 matrix having u2 and u3 as columns.

19.2. Examples of Graph Drawings
691
(3) Place vertex vi at the point whose coordinates is the ith row of R, that
is, (Ri1, Ri2).
This method generally gives pleasing results, but beware that there is
no guarantee that distinct nodes are assigned distinct images since R can
have identical rows. This does not seem to happen often in practice.
19.2
Examples of Graph Drawings
We now give a number of examples using Matlab. Some of these are bor-
rowed or adapted from Spielman [Spielman (2012)].
Example 1. Consider the graph with four nodes whose adjacency matrix
is
A =




0 1 1 0
1 0 0 1
1 0 0 1
0 1 1 0



.
We use the following program to compute u2 and u3:
A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0];
D = diag(sum(A));
L = D - A;
[v, e] = eigs(L);
gplot(A, v(:,[3 2]))
hold on;
gplot(A, v(:,[3 2]),'o')
The graph of Example 1 is shown in Figure 19.1. The function eigs(L)
computes the six largest eigenvalues of L in decreasing order, and corre-
sponding eigenvectors. It turns out that λ2 = λ3 = 2 is a double eigenvalue.
Example 2. Consider the graph G2 shown in Figure 18.3 given by the
adjacency matrix
A =






0 1 1 0 0
1 0 1 1 1
1 1 0 1 0
0 1 1 0 1
0 1 0 1 0






.
We use the following program to compute u2 and u3:

692
Spectral Graph Drawing
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Fig. 19.1
Drawing of the graph from Example 1.
A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0];
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on
gplot(A, v(:, [2 3]),'o')
The function eig(L) (with no s at the end) computes the eigenvalues
of L in increasing order. The result of drawing the graph is shown in Fig-
ure 19.2. Note that node v2 is assigned to the point (0, 0), so the diﬀerence
between this drawing and the drawing in Figure 18.3 is that the drawing
of Figure 19.2 is not convex.
Example 3. Consider the ring graph deﬁned by the adjacency matrix A
given in the Matlab program shown below:
A = diag(ones(1, 11),1);
A = A + A';
A(1, 12) = 1; A(12, 1) = 1;
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on
gplot(A, v(:, [2 3]),'o')

19.2. Examples of Graph Drawings
693
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Fig. 19.2
Drawing of the graph from Example 2.
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Fig. 19.3
Drawing of the graph from Example 3.
Observe that we get a very nice ring; see Figure 19.3. Again λ2 = 0.2679
is a double eigenvalue (and so are the next pairs of eigenvalues, except the
last, λ12 = 4).
Example 4. In this example adapted from Spielman, we generate 20
randomly chosen points in the unit square, compute their Delaunay trian-
gulation, then the adjacency matrix of the corresponding graph, and ﬁnally
draw the graph using the second and third eigenvalues of the Laplacian.

694
Spectral Graph Drawing
A = zeros(20,20);
xy = rand(20, 2);
trigs = delaunay(xy(:,1), xy(:,2));
elemtrig = ones(3) - eye(3);
for i = 1:length(trigs),
A(trigs(i,:),trigs(i,:)) = elemtrig;
end
A = double(A >0);
gplot(A,xy)
D = diag(sum(A));
L = D - A;
[v, e] = eigs(L, 3, 'sm');
figure(2)
gplot(A, v(:, [2 1]))
hold on
gplot(A, v(:, [2 1]),'o')
The Delaunay triangulation of the set of 20 points and the drawing of
the corresponding graph are shown in Figure 19.4. The graph drawing on
the right looks nicer than the graph on the left but is is no longer planar.
Example 5. Our last example, also borrowed from Spielman [Spielman
(2012)], corresponds to the skeleton of the "Buckyball," a geodesic dome
invented by the architect Richard Buckminster Fuller (1895-1983). The
Montr´eal Biosph`ere is an example of a geodesic dome designed by Buck-
minster Fuller.
A = full(bucky);
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on;
gplot(A,v(:, [2 3]), 'o')
Figure 19.5 shows a graph drawing of the Buckyball. This picture seems
a bit squashed for two reasons. First, it is really a 3-dimensional graph;
second, λ2 = 0.2434 is a triple eigenvalue. (Actually, the Laplacian of L
has many multiple eigenvalues.) What we should really do is to plot this
graph in R3 using three orthonormal eigenvectors associated with λ2.

19.2. Examples of Graph Drawings
695
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Fig. 19.4
Delaunay triangulation (left) and drawing of the graph from Example 4
(right).
A 3D picture of the graph of the Buckyball is produced by the following
Matlab program, and its image is shown in Figure 19.6. It looks better!
[x, y] = gplot(A, v(:, [2 3]));
[x, z] = gplot(A, v(:, [2 4]));
plot3(x,y,z)

696
Spectral Graph Drawing
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
Fig. 19.5
Drawing of the graph of the Buckyball.
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
−0.4
−0.2
0
0.2
0.4
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
Fig. 19.6
Drawing of the graph of the Buckyball in R3.
19.3
Summary
The main concepts and results of this chapter are listed below:
• Graph drawing.
• Matrix of a graph drawing.

19.3. Summary
697
• Balanced graph drawing.
• Energy E(R) of a graph drawing.
• Orthogonal graph drawing.
• Delaunay triangulation.
• Buckyball.

This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Chapter 20
Singular Value Decomposition and
Polar Form
20.1
Properties of f ∗◦f
In this section we assume that we are dealing with real Euclidean spaces.
Let f : E →E be any linear map. In general, it may not be possible to
diagonalize f. We show that every linear map can be diagonalized if we
are willing to use two orthonormal bases. This is the celebrated singular
value decomposition (SVD). A close cousin of the SVD is the polar form of
a linear map, which shows how a linear map can be decomposed into its
purely rotational component (perhaps with a ﬂip) and its purely stretching
part.
The key observation is that f ∗◦f is self-adjoint since
⟨(f ∗◦f)(u), v⟩= ⟨f(u), f(v)⟩= ⟨u, (f ∗◦f)(v)⟩.
Similarly, f ◦f ∗is self-adjoint.
The fact that f ∗◦f and f ◦f ∗are self-adjoint is very important, because
by Theorem 16.1, it implies that f ∗◦f and f◦f ∗can be diagonalized and that
they have real eigenvalues. In fact, these eigenvalues are all nonnegative as
shown in the following proposition.
Proposition 20.1. The eigenvalues of f ∗◦f and f ◦f ∗are nonnegative.
Proof. If u is an eigenvector of f ∗◦f for the eigenvalue λ, then
⟨(f ∗◦f)(u), u⟩= ⟨f(u), f(u)⟩
and
⟨(f ∗◦f)(u), u⟩= λ⟨u, u⟩,
and thus
λ⟨u, u⟩= ⟨f(u), f(u)⟩,
which implies that λ ≥0, since ⟨−, −⟩is positive deﬁnite. A similar proof
applies to f ◦f ∗.
699

700
Singular Value Decomposition and Polar Form
Thus, the eigenvalues of f ∗◦f are of the form σ2
1, . . . , σ2
r or 0, where
σi > 0, and similarly for f ◦f ∗.
The above considerations also apply to any linear map f : E →F be-
tween two Euclidean spaces (E, ⟨−, −⟩1) and (F, ⟨−, −⟩2). Recall that the
adjoint f ∗: F →E of f is the unique linear map f ∗such that
⟨f(u), v⟩2 = ⟨u, f ∗(v)⟩1,
for all u ∈E and all v ∈F.
Then f ∗◦f and f ◦f ∗are self-adjoint (the proof is the same as in the
previous case), and the eigenvalues of f ∗◦f and f ◦f ∗are nonnegative.
Proof. If λ is an eigenvalue of f ∗◦f and u (̸= 0) is a corresponding
eigenvector, we have
⟨(f ∗◦f)(u), u⟩1 = ⟨f(u), f(u)⟩2,
and also
⟨(f ∗◦f)(u), u⟩1 = λ⟨u, u⟩1,
so
λ⟨u, u⟩1, = ⟨f(u), f(u)⟩2,
which implies that λ ≥0. A similar proof applies to f ◦f ∗.
The situation is even better, since we will show shortly that f ∗◦f and
f ◦f ∗have the same nonzero eigenvalues.
Remark: Given any two linear maps f : E →F and g: F →E, where
dim(E) = n and dim(F) = m, it can be shown that
λm det(λ In −g ◦f) = λn det(λ Im −f ◦g),
and thus g ◦f and f ◦g always have the same nonzero eigenvalues; see
Problem 14.14.
Deﬁnition 20.1. Given any linear map f : E →F, the square roots σi > 0
of the positive eigenvalues of f ∗◦f (and f ◦f ∗) are called the singular values
of f.
Deﬁnition 20.2. A self-adjoint linear map f : E →E whose eigenvalues
are nonnegative is called positive semideﬁnite (or positive), and if f is
also invertible, f is said to be positive deﬁnite. In the latter case, every
eigenvalue of f is strictly positive.

20.1. Properties of f∗◦f
701
If f : E →F is any linear map, we just showed that f ∗◦f and f ◦f ∗are
positive semideﬁnite self-adjoint linear maps. This fact has the remarkable
consequence that every linear map has two important decompositions:
(1) The polar form.
(2) The singular value decomposition (SVD).
The wonderful thing about the singular value decomposition is that
there exist two orthonormal bases (u1, . . . , un) and (v1, . . . , vm) such that,
with respect to these bases, f is a diagonal matrix consisting of the singular
values of f or 0. Thus, in some sense, f can always be diagonalized with
respect to two orthonormal bases. The SVD is also a useful tool for solv-
ing overdetermined linear systems in the least squares sense and for data
analysis, as we show later on.
First we show some useful relationships between the kernels and the
images of f, f ∗, f ∗◦f, and f ◦f ∗. Recall that if f : E →F is a linear
map, the image Im f of f is the subspace f(E) of F, and the rank of f is
the dimension dim(Im f) of its image. Also recall that (Theorem 5.1)
dim (Ker f) + dim (Im f) = dim (E),
and that (Propositions 11.9 and 13.12) for every subspace W of E,
dim (W) + dim (W ⊥) = dim (E).
Proposition 20.2. Given any two Euclidean spaces E and F, where E has
dimension n and F has dimension m, for any linear map f : E →F, we
have
Ker f = Ker (f ∗◦f),
Ker f ∗= Ker (f ◦f ∗),
Ker f = (Im f ∗)⊥,
Ker f ∗= (Im f)⊥,
dim(Im f) = dim(Im f ∗),
and f, f ∗, f ∗◦f, and f ◦f ∗have the same rank.
Proof. To simplify the notation, we will denote the inner products on E
and F by the same symbol ⟨−, −⟩(to avoid subscripts). If f(u) = 0, then
(f ∗◦f)(u) = f ∗(f(u)) = f ∗(0) = 0, and so Ker f ⊆Ker (f ∗◦f).
By
deﬁnition of f ∗, we have
⟨f(u), f(u)⟩= ⟨(f ∗◦f)(u), u⟩

702
Singular Value Decomposition and Polar Form
for all u ∈E. If (f ∗◦f)(u) = 0, since ⟨−, −⟩is positive deﬁnite, we must
have f(u) = 0, and so Ker (f ∗◦f) ⊆Ker f. Therefore,
Ker f = Ker (f ∗◦f).
The proof that Ker f ∗= Ker (f ◦f ∗) is similar.
By deﬁnition of f ∗, we have
⟨f(u), v⟩= ⟨u, f ∗(v)⟩
for all u ∈E and all v ∈F.
(20.1)
This immediately implies that
Ker f = (Im f ∗)⊥
and
Ker f ∗= (Im f)⊥.
Let us explain why Ker f = (Im f ∗)⊥, the proof of the other equation being
similar.
Because the inner product is positive deﬁnite, for every u ∈E, we have
• u ∈Ker f
• iﬀf(u) = 0
• iﬀ⟨f(u), v⟩= 0 for all v,
• by (20.1) iﬀ⟨u, f ∗(v)⟩= 0 for all v,
• iﬀu ∈(Im f ∗)⊥.
Since
dim(Im f) = n −dim(Ker f)
and
dim(Im f ∗) = n −dim((Im f ∗)⊥),
from
Ker f = (Im f ∗)⊥
we also have
dim(Ker f) = dim((Im f ∗)⊥),
from which we obtain
dim(Im f) = dim(Im f ∗).
Since
dim(Ker (f ∗◦f)) + dim(Im (f ∗◦f)) = dim(E),
Ker (f ∗◦f) = Ker f and Ker f = (Im f ∗)⊥, we get
dim((Im f ∗)⊥) + dim(Im (f ∗◦f)) = dim(E).
Since
dim((Im f ∗)⊥) + dim(Im f ∗) = dim(E),
we deduce that
dim(Im f ∗) = dim(Im (f ∗◦f)).
A similar proof shows that
dim(Im f) = dim(Im (f ◦f ∗)).
Consequently, f, f ∗, f ∗◦f, and f ◦f ∗have the same rank.

20.2. Singular Value Decomposition for Square Matrices
703
20.2
Singular Value Decomposition for
Square Matrices
We will now prove that every square matrix has an SVD. Stronger results
can be obtained if we ﬁrst consider the polar form and then derive the SVD
from it (there are uniqueness properties of the polar decomposition). For
our purposes, uniqueness results are not as important so we content our-
selves with existence results, whose proofs are simpler. Readers interested
in a more general treatment are referred to Gallier [Gallier (2011b)].
The early history of the singular value decomposition is described in a
fascinating paper by Stewart [Stewart (1993)]. The SVD is due to Beltrami
and Camille Jordan independently (1873, 1874). Gauss is the grandfather
of all this, for his work on least squares (1809, 1823) (but Legendre also
published a paper on least squares!). Then come Sylvester, Schmidt, and
Hermann Weyl. Sylvester's work was apparently "opaque." He gave a com-
putational method to ﬁnd an SVD. Schmidt's work really has to do with
integral equations and symmetric and asymmetric kernels (1907). Weyl's
work has to do with perturbation theory (1912). Autonne came up with
the polar decomposition (1902, 1915). Eckart and Young extended SVD to
rectangular matrices (1936, 1939).
Theorem 20.1. (Singular value decomposition) For every real n×n matrix
A there are two orthogonal matrices U and V and a diagonal matrix D such
that A = V DU ⊤, where D is of the form
D =





σ1
. . .
σ2 . . .
...
... ... ...
. . . σn




,
where σ1, . . . , σr are the singular values of f, i.e., the (positive) square roots
of the nonzero eigenvalues of A⊤A and A A⊤, and σr+1 = · · · = σn = 0.
The columns of U are eigenvectors of A⊤A, and the columns of V are
eigenvectors of A A⊤.
Proof. Since A⊤A is a symmetric matrix, in fact, a positive semideﬁnite
matrix, there exists an orthogonal matrix U such that
A⊤A = UD2U ⊤,
with D = diag(σ1, . . . , σr, 0, . . . , 0), where σ2
1, . . . , σ2
r are the nonzero eigen-
values of A⊤A, and where r is the rank of A; that is, σ1, . . . , σr are the

704
Singular Value Decomposition and Polar Form
singular values of A. It follows that
U ⊤A⊤AU = (AU)⊤AU = D2,
and if we let fj be the jth column of AU for j = 1, . . . , n, then we have
⟨fi, fj⟩= σ2
i δij,
1 ≤i, j ≤r
and
fj = 0,
r + 1 ≤j ≤n.
If we deﬁne (v1, . . . , vr) by
vj = σ−1
j fj,
1 ≤j ≤r,
then we have
⟨vi, vj⟩= δij,
1 ≤i, j ≤r,
so complete (v1, . . . , vr) into an orthonormal basis (v1, . . . , vr, vr+1, . . . , vn)
(for example, using Gram-Schmidt). Now since fj = σjvj for j = 1 . . . , r,
we have
⟨vi, fj⟩= σj⟨vi, vj⟩= σjδi,j,
1 ≤i ≤n, 1 ≤j ≤r
and since fj = 0 for j = r + 1, . . . , n,
⟨vi, fj⟩= 0
1 ≤i ≤n, r + 1 ≤j ≤n.
If V is the matrix whose columns are v1, . . . , vn, then V is orthogonal and
the above equations prove that
V ⊤AU = D,
which yields A = V DU ⊤, as required.
The equation A = V DU ⊤implies that
A⊤A = UD2U ⊤,
AA⊤= V D2V ⊤,
which shows that A⊤A and AA⊤have the same eigenvalues, that the
columns of U are eigenvectors of A⊤A, and that the columns of V are
eigenvectors of AA⊤.
Example 20.1. Here is a simple example of how to use the proof of The-
orem 20.1 to obtain an SVD decomposition.
Let A =
1 1
0 0

.
Then
A⊤=
1 0
1 0

, A⊤A =
1 1
1 1

, and AA⊤=
2 0
0 0

. A simple calculation
shows that the eigenvalues of A⊤A are 2 and 0, and for the eigenvalue 2,

20.2. Singular Value Decomposition for Square Matrices
705
a unit eigenvector is
1/
√
2
1/
√
2

, while a unit eigenvector for the eigenvalue
0 is
 1/
√
2
−1/
√
2

. Observe that the singular values are σ1 =
√
2 and σ2 = 0.
Furthermore, U =
1/
√
2 1/
√
2
1/
√
2 −1/
√
2

= U ⊤. To determine V , the proof of
Theorem 20.1 tells us to ﬁrst calculate
AU =
√
2 0
0 0

,
and then set
v1 = (1/
√
2)
√
2
0

=
1
0

.
Once v1 is determined, since σ2 = 0, we have the freedom to choose v2
such that (v1, v2) forms an orthonormal basis for R2. Naturally, we chose
v2 =
0
1

and set V =
1 0
0 1

. Of course we could have found V by directly
computing the eigenvalues and eigenvectors for AA⊤. We leave it to the
reader to check that
A = V
√
2 0
0 0

U ⊤.
Theorem 20.1 suggests the following deﬁnition.
Deﬁnition 20.3. A triple (U, D, V ) such that A = V D U ⊤, where U and
V are orthogonal and D is a diagonal matrix whose entries are nonnegative
(it is positive semideﬁnite) is called a singular value decomposition (SVD)
of A.
The Matlab command for computing an SVD A = V DU ⊤of a matrix
A is [V, D, U] = svd(A).
The proof of Theorem 20.1 shows that there are two orthonormal bases
(u1, . . . , un) and (v1, . . . , vn), where (u1, . . . , un) are eigenvectors of A⊤A
and (v1, . . . , vn) are eigenvectors of AA⊤.
Furthermore, (u1, . . . , ur) is
an orthonormal basis of Im A⊤, (ur+1, . . . , un) is an orthonormal basis of
Ker A, (v1, . . . , vr) is an orthonormal basis of Im A, and (vr+1, . . . , vn) is
an orthonormal basis of Ker A⊤.
Using a remark made in Chapter 3, if we denote the columns of U by
u1, . . . , un and the columns of V by v1, . . . , vn, then we can write
A = V D U ⊤= σ1v1u⊤
1 + · · · + σrvru⊤
r .

706
Singular Value Decomposition and Polar Form
As a consequence, if r is a lot smaller than n (we write r ≪n), we see
that A can be reconstructed from U and V using a much smaller number
of elements. This idea will be used to provide "low-rank" approximations
of a matrix. The idea is to keep only the k top singular values for some
suitable k ≪r for which σk+1, . . . σr are very small.
Remarks:
(1) In Strang [Strang (1988)] the matrices U, V, D are denoted by U = Q2,
V = Q1, and D = Σ, and an SVD is written as A = Q1ΣQ⊤
2 . This has
the advantage that Q1 comes before Q2 in A = Q1ΣQ⊤
2 . This has the
disadvantage that A maps the columns of Q2 (eigenvectors of A⊤A) to
multiples of the columns of Q1 (eigenvectors of A A⊤).
(2) Algorithms for actually computing the SVD of a matrix are presented in
Golub and Van Loan [Golub and Van Loan (1996)], Demmel [Demmel
(1997)], and Trefethen and Bau [Trefethen and Bau III (1997)], where
the SVD and its applications are also discussed quite extensively.
(3) If A is a symmetric matrix, then in general, there is no SVD V ΣU ⊤
of A with V = U.
However, if A is positive semideﬁnite, then the
eigenvalues of A are nonnegative, and so the nonzero eigenvalues of A
are equal to the singular values of A and SVDs of A are of the form
A = V ΣV ⊤.
(4) The SVD also applies to complex matrices.
In this case, for every
complex n × n matrix A, there are two unitary matrices U and V and
a diagonal matrix D such that
A = V D U ∗,
where D is a diagonal matrix consisting of real entries σ1, . . . , σn, where
σ1, . . . , σr are the singular values of A, i.e., the positive square roots of
the nonzero eigenvalues of A∗A and A A∗, and σr+1 = . . . = σn = 0.
20.3
Polar Form for Square Matrices
A notion closely related to the SVD is the polar form of a matrix.
Deﬁnition 20.4. A pair (R, S) such that A = RS with R orthogonal and
S symmetric positive semideﬁnite is called a polar decomposition of A.

20.3. Polar Form for Square Matrices
707
Theorem 20.1 implies that for every real n × n matrix A, there is some
orthogonal matrix R and some positive semideﬁnite symmetric matrix S
such that
A = RS.
This is easy to show and we will prove it below. Furthermore, R, S are
unique if A is invertible, but this is harder to prove; see Problem 20.9.
For example, the matrix
A = 1
2




1 1
1
1
1 1 −1 −1
1 −1 1 −1
1 −1 −1 1




is both orthogonal and symmetric, and A = RS with R = A and S = I,
which implies that some of the eigenvalues of A are negative.
Remark: In the complex case, the polar decomposition states that for
every complex n × n matrix A, there is some unitary matrix U and some
positive semideﬁnite Hermitian matrix H such that
A = UH.
It is easy to go from the polar form to the SVD, and conversely.
Given an SVD decomposition A = V D U ⊤, let R = V U ⊤and S =
UD U ⊤. It is clear that R is orthogonal and that S is positive semideﬁnite
symmetric, and
RS = V U ⊤UD U ⊤= V D U ⊤= A.
Example 20.2. Recall from Example 20.1 that A = V DU ⊤where V = I2
and
A =
1 1
0 0

,
U =
 1
√
2
1
√
2
1
√
2 −1
√
2
!
,
D =
√
2 0
0 0

.
Set R = V U ⊤= U and
S = UDU ⊤=
 1
√
2
1
√
2
1
√
2
1
√
2
!
.
Since S =
1
√
2A⊤A, S has eigenvalues
√
2 and 0. We leave it to the reader
to check that A = RS.

708
Singular Value Decomposition and Polar Form
Going the other way, given a polar decomposition A = R1S, where R1 is
orthogonal and S is positive semideﬁnite symmetric, there is an orthogonal
matrix R2 and a positive semideﬁnite diagonal matrix D such that S =
R2D R⊤
2 , and thus
A = R1R2D R⊤
2 = V D U ⊤,
where V = R1R2 and U = R2 are orthogonal.
Example
20.3. Let A
=
1 1
0 0

and A
=
R1S,
where R1
=
1/
√
2 1/
√
2
1/
√
2 −1/
√
2

and S =
1/
√
2 1/
√
2
1/
√
2 1/
√
2

. This is the polar decomposi-
tion of Example 20.2. Observe that
S =
 1
√
2
1
√
2
1
√
2 −1
√
2
! √
2 0
0 0
  1
√
2
1
√
2
1
√
2 −1
√
2
!
= R2DR⊤
2 .
Set U = R2 and V = R1R2 =
1 0
0 1

to obtain the SVD decomposition of
Example 20.1.
The eigenvalues and the singular values of a matrix are typically not
related in any obvious way. For example, the n × n matrix
A =












1 2 0
0 . . . 0 0
0 1 2
0 . . . 0 0
0 0 1
2 . . . 0 0
... ... ... ... ... ... ...
0 0 . . . 0
1 2 0
0 0 . . . 0
0 1 2
0 0 . . . 0
0 0 1












has the eigenvalue 1 with multiplicity n, but its singular values, σ1 ≥· · · ≥
σn, which are the positive square roots of the eigenvalues of the matrix
B = A⊤A with
B =












1 2 0
0 . . . 0 0
2 5 2
0 . . . 0 0
0 2 5
2 . . . 0 0
... ... ... ... ... ... ...
0 0 . . . 2
5 2 0
0 0 . . . 0
2 5 2
0 0 . . . 0
0 2 5













20.4. Singular Value Decomposition for Rectangular Matrices
709
have a wide spread, since
σ1
σn
= cond2(A) ≥2n−1.
If A is a complex n × n matrix, the eigenvalues λ1, . . . , λn and the
singular values
σ1 ≥σ2 ≥· · · ≥σn of A are not unrelated, since
σ2
1 · · · σ2
n = det(A∗A) = | det(A)|2
and
|λ1| · · · |λn| = | det(A)|,
so we have
|λ1| · · · |λn| = σ1 · · · σn.
More generally, Hermann Weyl proved the following remarkable theo-
rem:
Theorem 20.2. (Weyl's inequalities, 1949) For any complex n×n matrix,
A, if λ1, . . . , λn ∈C are the eigenvalues of A and σ1, . . . , σn ∈R+ are the
singular values of A, listed so that |λ1| ≥· · · ≥|λn| and σ1 ≥· · · ≥σn ≥0,
then
|λ1| · · · |λn| = σ1 · · · σn
and
|λ1| · · · |λk| ≤σ1 · · · σk,
for
k = 1, . . . , n −1.
A proof of Theorem 20.2 can be found in Horn and Johnson [Horn and
Johnson (1994)], Chapter 3, Section 3.3, where more inequalities relating
the eigenvalues and the singular values of a matrix are given.
Theorem 20.1 can be easily extended to rectangular m × n matrices,
as we show in the next section. For various versions of the SVD for rect-
angular matrices, see Strang [Strang (1988)] Golub and Van Loan [Golub
and Van Loan (1996)], Demmel [Demmel (1997)], and Trefethen and Bau
[Trefethen and Bau III (1997)].
20.4
Singular Value Decomposition for
Rectangular Matrices
Here is the generalization of Theorem 20.1 to rectangular matrices.
Theorem 20.3. (Singular value decomposition) For every real m×n matrix
A, there are two orthogonal matrices U (n × n) and V (m × m) and a

710
Singular Value Decomposition and Polar Form
diagonal m × n matrix D such that A = V D U ⊤, where D is of the form
D =















σ1
. . .
σ2 . . .
...
... ... ...
. . . σn
0
... . . . 0
...
... ... ...
0
... . . . 0















or D =





σ1
. . .
0 . . . 0
σ2 . . .
0 . . . 0
...
... ...
...
0 ... 0
. . . σm 0 . . . 0




,
where σ1, . . . , σr are the singular values of f, i.e. the (positive) square roots
of the nonzero eigenvalues of A⊤A and A A⊤, and σr+1 = . . . = σp = 0,
where p = min(m, n). The columns of U are eigenvectors of A⊤A, and the
columns of V are eigenvectors of A A⊤.
Proof. As in the proof of Theorem 20.1, since A⊤A is symmetric positive
semideﬁnite, there exists an n × n orthogonal matrix U such that
A⊤A = UΣ2U ⊤,
with Σ = diag(σ1, . . . , σr, 0, . . . , 0), where σ2
1, . . . , σ2
r are the nonzero eigen-
values of A⊤A, and where r is the rank of A. Observe that r ≤min{m, n},
and AU is an m × n matrix. It follows that
U ⊤A⊤AU = (AU)⊤AU = Σ2,
and if we let fj ∈Rm be the jth column of AU for j = 1, . . . , n, then we
have
⟨fi, fj⟩= σ2
i δij,
1 ≤i, j ≤r
and
fj = 0,
r + 1 ≤j ≤n.
If we deﬁne (v1, . . . , vr) by
vj = σ−1
j fj,
1 ≤j ≤r,
then we have
⟨vi, vj⟩= δij,
1 ≤i, j ≤r,
so complete (v1, . . . , vr) into an orthonormal basis (v1, . . . , vr, vr+1, . . . , vm)
(for example, using Gram-Schmidt).

20.4. Singular Value Decomposition for Rectangular Matrices
711
Now since fj = σjvj for j = 1 . . . , r, we have
⟨vi, fj⟩= σj⟨vi, vj⟩= σjδi,j,
1 ≤i ≤m, 1 ≤j ≤r
and since fj = 0 for j = r + 1, . . . , n, we have
⟨vi, fj⟩= 0
1 ≤i ≤m, r + 1 ≤j ≤n.
If V is the matrix whose columns are v1, . . . , vm, then V is an m × m
orthogonal matrix and if m ≥n, we let
D =

Σ
0m−n

=















σ1
. . .
σ2 . . .
...
... ... ...
. . . σn
0
... . . . 0
...
... ... ...
0
... . . . 0















,
else if n ≥m, then we let
D =





σ1
. . .
0 . . . 0
σ2 . . .
0 . . . 0
...
... ...
...
0 ... 0
. . . σm 0 . . . 0




.
In either case, the above equations prove that
V ⊤AU = D,
which yields A = V DU ⊤, as required.
The equation A = V DU ⊤implies that
A⊤A = UD⊤DU ⊤= Udiag(σ2
1, . . . , σ2
r, 0, . . . , 0
| {z }
n−r
)U ⊤
and
AA⊤= V DD⊤V ⊤= V diag(σ2
1, . . . , σ2
r, 0, . . . , 0
| {z }
m−r
)V ⊤,
which shows that A⊤A and AA⊤have the same nonzero eigenvalues, that
the columns of U are eigenvectors of A⊤A, and that the columns of V are
eigenvectors of AA⊤.

712
Singular Value Decomposition and Polar Form
A triple (U, D, V ) such that A = V D U ⊤is called a singular value
decomposition (SVD) of A.
Example 20.4. Let A =


1 1
0 0
0 0

. Then A⊤=
1 0 0
1 0 0

A⊤A =
1 1
1 1

,
and AA⊤=


2 0 0
0 0 0
0 0 0

. The reader should verify that A⊤A = UΣ2U ⊤where
Σ2 =
2 0
0 0

and U = U ⊤=
1/
√
2 1/
√
2
1/
√
2 −1/
√
2

. Since AU =


√
2 0
0 0
0 0

, set
v1 =
1
√
2


√
2
0
0

=


1
0
0

, and complete an orthonormal basis for R3 by
assigning v2 =


0
1
0

, and v3 =


0
0
1

. Thus V = I3, and the reader should
verify that A = V DU ⊤, where D =


√
2 0
0 0
0 0

.
Even though the matrix D is an m × n rectangular matrix, since its
only nonzero entries are on the descending diagonal, we still say that D is
a diagonal matrix.
The Matlab command for computing an SVD A = V DU ⊤of a matrix
A is also [V, D, U] = svd(A).
If we view A as the representation of a linear map f : E →F, where
dim(E) = n and dim(F) = m, the proof of Theorem 20.3 shows that
there are two orthonormal bases (u1, . . ., un) and (v1, . . . , vm) for E and F,
respectively, where (u1, . . . , un) are eigenvectors of f ∗◦f and (v1, . . . , vm)
are eigenvectors of f ◦f ∗. Furthermore, (u1, . . . , ur) is an orthonormal basis
of Im f ∗, (ur+1, . . . , un) is an orthonormal basis of Ker f, (v1, . . . , vr) is an
orthonormal basis of Im f, and (vr+1, . . . , vm) is an orthonormal basis of
Ker f ∗.
The SVD of matrices can be used to deﬁne the pseudo-inverse of a
rectangular matrix; we will do so in Chapter 21.
The reader may also
consult Strang [Strang (1988)], Demmel [Demmel (1997)], Trefethen and
Bau [Trefethen and Bau III (1997)], and Golub and Van Loan [Golub and
Van Loan (1996)].

20.4. Singular Value Decomposition for Rectangular Matrices
713
One of the spectral theorems states that a symmetric matrix can be di-
agonalized by an orthogonal matrix. There are several numerical methods
to compute the eigenvalues of a symmetric matrix A. One method consists
in tridiagonalizing A, which means that there exists some orthogonal matrix
P and some symmetric tridiagonal matrix T such that A = PTP ⊤. In fact,
this can be done using Householder transformations; see Theorem 17.2. It
is then possible to compute the eigenvalues of T using a bisection method
based on Sturm sequences. One can also use Jacobi's method. For details,
see Golub and Van Loan [Golub and Van Loan (1996)], Chapter 8, Dem-
mel [Demmel (1997)], Trefethen and Bau [Trefethen and Bau III (1997)],
Lecture 26, Ciarlet [Ciarlet (1989)], and Chapter 17. Computing the SVD
of a matrix A is more involved. Most methods begin by ﬁnding orthogonal
matrices U and V and a bidiagonal matrix B such that A = V BU ⊤; see
Problem 12.8 and Problem 20.3. This can also be done using Householder
transformations. Observe that B⊤B is symmetric tridiagonal. Thus, in
principle, the previous method to diagonalize a symmetric tridiagonal ma-
trix can be applied. However, it is unwise to compute B⊤B explicitly, and
more subtle methods are used for this last step; the matrix of Problem 20.1
can be used, and see Problem 20.3. Again, see Golub and Van Loan [Golub
and Van Loan (1996)], Chapter 8, Demmel [Demmel (1997)], and Trefethen
and Bau [Trefethen and Bau III (1997)], Lecture 31.
The polar form has applications in continuum mechanics. Indeed, in any
deformation it is important to separate stretching from rotation. This is
exactly what QS achieves. The orthogonal part Q corresponds to rotation
(perhaps with an additional reﬂection), and the symmetric matrix S to
stretching (or compression). The real eigenvalues σ1, . . . , σr of S are the
stretch factors (or compression factors) (see Marsden and Hughes [Marsden
and Hughes (1994)]). The fact that S can be diagonalized by an orthogonal
matrix corresponds to a natural choice of axes, the principal axes.
The SVD has applications to data compression, for instance in image
processing. The idea is to retain only singular values whose magnitudes
are signiﬁcant enough. The SVD can also be used to determine the rank of
a matrix when other methods such as Gaussian elimination produce very
small pivots. One of the main applications of the SVD is the computation
of the pseudo-inverse. Pseudo-inverses are the key to the solution of various
optimization problems, in particular the method of least squares. This topic
is discussed in the next chapter (Chapter 21). Applications of the material
of this chapter can be found in Strang [Strang (1988, 1986)]; Ciarlet [Ciarlet
(1989)]; Golub and Van Loan [Golub and Van Loan (1996)], which contains

714
Singular Value Decomposition and Polar Form
many other references; Demmel [Demmel (1997)]; and Trefethen and Bau
[Trefethen and Bau III (1997)].
20.5
Ky Fan Norms and Schatten Norms
The singular values of a matrix can be used to deﬁne various norms on ma-
trices which have found recent applications in quantum information theory
and in spectral graph theory. Following Horn and Johnson [Horn and John-
son (1994)] (Section 3.4) we can make the following deﬁnitions:
Deﬁnition 20.5. For any matrix A ∈Mm,n(C), let q = min{m, n}, and if
σ1 ≥· · · ≥σq are the singular values of A, for any k with 1 ≤k ≤q, let
Nk(A) = σ1 + · · · + σk,
called the Ky Fan k-norm of A.
More generally, for any p ≥1 and any k with 1 ≤k ≤q, let
Nk;p(A) = (σp
1 + · · · + σp
k)1/p,
called the Ky Fan p-k-norm of A. When k = q, Nq;p is also called the
Schatten p-norm.
Observe that when k = 1, N1(A) = σ1, and the Ky Fan norm N1 is
simply the spectral norm from Chapter 8, which is the subordinate matrix
norm associated with the Euclidean norm. When k = q, the Ky Fan norm
Nq is given by
Nq(A) = σ1 + · · · + σq = tr((A∗A)1/2)
and is called the trace norm or nuclear norm. When p = 2 and k = q, the
Ky Fan Nq;2 norm is given by
Nk;2(A) = (σ2
1 + · · · + σ2
q)1/2 =
p
tr(A∗A) = ∥A∥F ,
which is the Frobenius norm of A.
It can be shown that Nk and Nk;p are unitarily invariant norms, and
that when m = n, they are matrix norms; see Horn and Johnson [Horn and
Johnson (1994)] (Section 3.4, Corollary 3.4.4 and Problem 3).

20.6. Summary
715
20.6
Summary
The main concepts and results of this chapter are listed below:
• For any linear map f : E →E on a Euclidean space E, the maps f ∗◦f
and f ◦f ∗are self-adjoint and positive semideﬁnite.
• The singular values of a linear map.
• Positive semideﬁnite and positive deﬁnite self-adjoint maps.
• Relationships between Im f, Ker f, Im f ∗, and Ker f ∗.
• The singular value decomposition theorem for square matrices (Theo-
rem 20.1).
• The SVD of matrix.
• The polar decomposition of a matrix.
• The Weyl inequalities.
• The singular value decomposition theorem for m × n matrices (Theo-
rem 20.3).
• Ky Fan k-norms, Ky Fan p-k-norms, Schatten p-norms.
20.7
Problems
Problem 20.1. (1) Let A be a real n×n matrix and consider the (2n)×(2n)
real symmetric matrix
S =
 0 A
A⊤0

.
Suppose that A has rank r.
If A = V ΣU ⊤is an SVD for A, with
Σ = diag(σ1, . . . , σn) and σ1 ≥· · · ≥σr > 0, denoting the columns of
U by uk and the columns of V by vk, prove that σk is an eigenvalue of S
with corresponding eigenvector
vk
uk

for k = 1, . . . , n, and that −σk is an
eigenvalue of S with corresponding eigenvector
 vk
−uk

for k = 1, . . . , n.
Hint. We have Auk = σkvk for k = 1, . . . , n. Show that A⊤vk = σkuk
for k = 1, . . . , r, and that A⊤vk = 0 for k = r + 1, . . . , n.
Recall that
Ker (A⊤) = Ker (AA⊤).
(2) Prove that the 2n eigenvectors of S in (1) are pairwise orthogonal.
Check that if A has rank r, then S has rank 2r.
(3) Now assume that A is a real m × n matrix and consider the

716
Singular Value Decomposition and Polar Form
(m + n) × (m + n) real symmetric matrix
S =
 0 A
A⊤0

.
Suppose that A has rank r. If A = V ΣU ⊤is an SVD for A, prove that σk
is an eigenvalue of S with corresponding eigenvector
vk
uk

for k = 1, . . . , r,
and that −σk is an eigenvalue of S with corresponding eigenvector
 vk
−uk

for k = 1, . . . , r.
Find the remaining m + n −2r eigenvectors of S associated with the
eigenvalue 0.
(4) Prove that these m + n eigenvectors of S are pairwise orthogonal.
Problem 20.2. Let A be a real m × n matrix of rank r and let q =
min(m, n).
(1) Consider the (m + n) × (m + n) real symmetric matrix
S =
 0 A
A⊤0

and prove that
Im z−1A
0
In
  zIm −A
−A⊤zIn

=
zIm −z−1AA⊤
0
−A⊤
zIn

.
Use the above equation to prove that
det(zIm+n −S) = tn−m det(t2Im −AA⊤).
(2) Prove that the eigenvalues of S are ±σ1, . . . , ±σq, with |m −n|
additional zeros.
Problem 20.3. Let B be a real bidiagonal matrix of the form
B =








a1 b1
0
· · ·
0
0 a2 b2
...
0
... ... ...
...
...
0 · · ·
0 an−1 bn−1
0
0 · · ·
0
an








.
Let A be the (2n) × (2n) symmetric matrix
A =
 0 B⊤
B
0

,

20.7. Problems
717
and let P be the permutation matrix given by P = [e1, en+1, e2, en+2,
· · · , en, e2n].
(1) Prove that T = P ⊤AP is a symmetric tridiagonal (2n)×(2n) matrix
with zero main diagonal of the form
T =














0 a1 0
0
0
0
· · ·
0
a1 0 b1
0
0
0
· · ·
0
0 b1 0 a2
0
0
· · ·
0
0 0 a2 0
b2
0
· · ·
0
...
...
... ...
...
...
...
...
0 0 0 · · · an−1
0
bn−1 0
0 0 0 · · ·
0
bn−1
0
an
0 0 0 · · ·
0
0
an
0














.
(2) Prove that if xi is a unit eigenvector for an eigenvalue λi of T, then
λi = ±σi where σi is a singular value of B, and that
Pxi =
1
√
2
 ui
±vi

,
where the ui are unit eigenvectors of B⊤B and the vi are unit eigenvectors
of BB⊤.
Problem 20.4. Find the SVD of the matrix
A =


0 2 0
0 0 3
0 0 0

.
Problem 20.5. Let u, v ∈Rn be two nonzero vectors, and let A = uv⊤be
the corresponding rank 1 matrix. Prove that the nonzero singular value of
A is ∥u∥2 ∥v∥2.
Problem 20.6. Let A be a n × n real matrix. Prove that if σ1, . . . , σn are
the singular values of A, then σ3
1, . . . , σ3
n are the singular values of AA⊤A.
Problem 20.7. Let A be a real n × n matrix.
(1) Prove that the largest singular value σ1 of A is given by
σ1 = sup
x̸=0
∥Ax∥2
∥x∥2
,
and that this supremum is achieved at x = u1, the ﬁrst column in U in an
SVD A = V ΣU ⊤.
(2) Extend the above result to real m × n matrices.

718
Singular Value Decomposition and Polar Form
Problem 20.8. Let A be a real m × n matrix. Prove that if B is any
submatrix of A (by keeping M ≤m rows and N ≤n columns of A), then
(σ1)B ≤(σ1)A (where (σ1)A is the largest singular value of A and similarly
for (σ1)B).
Problem 20.9. Let A be a real n × n matrix.
(1) Assume A is invertible. Prove that if A = Q1S1 = Q2S2 are two
polar decompositions of A, then Q1 = Q2 and S1 = S2.
Hint. A⊤A = S2
1 = S2
2, with S1 and S2 symmetric positive deﬁnite. Then
use Problem 16.7.
(2) Now assume that A is singular. Prove that if A = Q1S1 = Q2S2 are
two polar decompositions of A, then S1 = S2, but Q1 may not be equal to
Q2.
Problem 20.10. (1) Let A be any invertible (real) n × n matrix. Prove
that for every SVD, A = V DU ⊤of A, the product V U ⊤is the same (i.e.,
if V1DU ⊤
1 = V2DU ⊤
2 , then V1U ⊤
1 = V2U ⊤
2 ). What does V U ⊤have to do
with the polar form of A?
(2) Given any invertible (real) n × n matrix, A, prove that there is
a unique orthogonal matrix, Q ∈O(n), such that ∥A −Q∥F is minimal
(under the Frobenius norm). In fact, prove that Q = V U ⊤, where A =
V DU ⊤is an SVD of A. Moreover, if det(A) > 0, show that Q ∈SO(n).
What can you say if A is singular (i.e., non-invertible)?
Problem 20.11. (1) Prove that for any n×n matrix A and any orthogonal
matrix Q, we have
max{tr(QA) | Q ∈O(n)} = σ1 + · · · + σn,
where σ1 ≥· · · ≥σn are the singular values of A.
Furthermore, this
maximum is achieved by Q = UV ⊤, where A = V ΣU ⊤is any SVD for A.
(2) By applying the above result with A = Z⊤X and Q = R⊤, deduce
the following result: For any two ﬁxed n×k matrices X and Z, the minimum
of the set
{∥X −ZR∥F | R ∈O(k)}
is achieved by R = V U ⊤for any SVD decomposition V ΣU ⊤= Z⊤X of
Z⊤X.
Remark: The problem of ﬁnding an orthogonal matrix R such that ZR
comes as close as possible to X is called the orthogonal Procrustes problem;
see Strang [Strang (2019)] (Section IV.9) for the history of this problem.

Chapter 21
Applications of SVD and
Pseudo-Inverses
De tous les principes qu'on peut proposer pour cet objet, je pense
qu'il n'en est pas de plus g´en´eral, de plus exact, ni d'une application
plus facile, que celui dont nous avons fait usage dans les recherches
pr´ec´edentes, et qui consiste `a rendre minimum la somme des carr´es des
erreurs. Par ce moyen il s'´etablit entre les erreurs une sorte d'´equilibre
qui, empˆechant les extrˆemes de pr´evaloir, est tr`es propre `as faire con-
naitre l'´etat du syst`eme le plus proche de la v´erit´e.
—Legendre, 1805, Nouvelles M´ethodes pour la d´etermination des Or-
bites des
Com`etes
21.1
Least Squares Problems and the Pseudo-Inverse
This chapter presents several applications of SVD. The ﬁrst one is the
pseudo-inverse, which plays a crucial role in solving linear systems by
the method of least squares. The second application is data compression.
The third application is principal component analysis (PCA), whose pur-
pose is to identify patterns in data and understand the variance-covariance
structure of the data. The fourth application is the best aﬃne approxima-
tion of a set of data, a problem closely related to PCA.
The method of least squares is a way of "solving" an overdetermined
system of linear equations
Ax = b,
i.e., a system in which A is a rectangular m×n matrix with more equations
than unknowns (when m > n). Historically, the method of least squares was
used by Gauss and Legendre to solve problems in astronomy and geodesy.
The method was ﬁrst published by Legendre in 1805 in a paper on methods
719

720
Applications of SVD and Pseudo-Inverses
for determining the orbits of comets. However, Gauss had already used
the method of least squares as early as 1801 to determine the orbit of the
asteroid Ceres, and he published a paper about it in 1810 after the discovery
of the asteroid Pallas. Incidentally, it is in that same paper that Gaussian
elimination using pivots is introduced.
The reason why more equations than unknowns arise in such problems
is that repeated measurements are taken to minimize errors. This produces
an overdetermined and often inconsistent system of linear equations. For
example, Gauss solved a system of eleven equations in six unknowns to
determine the orbit of the asteroid Pallas.
Example 21.1. As a concrete illustration, suppose that we observe the
motion of a small object, assimilated to a point, in the plane. From our
observations, we suspect that this point moves along a straight line, say of
equation y = cx + d. Suppose that we observed the moving point at three
diﬀerent locations (x1, y1), (x2, y2), and (x3, y3). Then we should have
d + cx1 = y1,
d + cx2 = y2,
d + cx3 = y3.
If there were no errors in our measurements, these equations would be
compatible, and c and d would be determined by only two of the equations.
However, in the presence of errors, the system may be inconsistent. Yet we
would like to ﬁnd c and d!
The idea of the method of least squares is to determine (c, d) such that
it minimizes the sum of the squares of the errors, namely,
(d + cx1 −y1)2 + (d + cx2 −y2)2 + (d + cx3 −y3)2.
See Figure 21.1.
In general, for an overdetermined m × n system Ax = b, what Gauss
and Legendre discovered is that there are solutions x minimizing
∥Ax −b∥2
2
(where ∥u∥2
2 = u2
1 +· · ·+u2
n, the square of the Euclidean norm of the vector
u = (u1, . . . , un)), and that these solutions are given by the square n × n
system
A⊤Ax = A⊤b,

21.1. Least Squares Problems and the Pseudo-Inverse
721
y = cx + d
(x , y )
1
1
(x , y )
2
2
(x , y )
3
3
1
(x , cx +d )
1
(x , cx +d )
(x , cx +d )
2
2
3
3
(x , y )
1
1
(x , y )
2
2
(x , y )
3
3
Fig. 21.1
Given three points (x1, y1), (x2, y2), (x3, y3), we want to determine the line
y = cx + d which minimizes the lengths of the dashed vertical lines.
called the normal equations.
Furthermore, when the columns of A are
linearly independent, it turns out that A⊤A is invertible, and so x is unique
and given by
x = (A⊤A)−1A⊤b.
Note that A⊤A is a symmetric matrix, one of the nice features of the normal
equations of a least squares problem. For instance, since the above problem
in matrix form is represented as


1 x1
1 x2
1 x3


d
c

=


y1
y2
y3

,
the normal equations are

3
x1 + x2 + x3
x1 + x2 + x3 x2
1 + x2
2 + x2
3
 d
c

=

y1 + y2 + y3
x1y1 + x2y2 + x3y3

.
In fact, given any real m × n matrix A, there is always a unique x+ of
minimum norm that minimizes ∥Ax −b∥2
2, even when the columns of A are
linearly dependent. How do we prove this, and how do we ﬁnd x+?
Theorem 21.1. Every linear system Ax = b, where A is an m×n matrix,
has a unique least squares solution x+ of smallest norm.

722
Applications of SVD and Pseudo-Inverses
Proof. Geometry oﬀers a nice proof of the existence and uniqueness of
x+. Indeed, we can interpret b as a point in the Euclidean (aﬃne) space
Rm, and the image subspace of A (also called the column space of A) as a
subspace U of Rm (passing through the origin). Then it is clear that
inf
x∈Rn ∥Ax −b∥2
2 = inf
y∈U ∥y −b∥2
2,
with U = Im A, and we claim that x minimizes ∥Ax−b∥2
2 iﬀAx = p, where
p the orthogonal projection of b onto the subspace U.
Recall from Section 12.1 that the orthogonal projection pU : U ⊕U ⊥→
U is the linear map given by
pU(u + v) = u,
with u ∈U and v ∈U ⊥. If we let p = pU(b) ∈U, then for any point y ∈U,
the vectors −→
py = y −p ∈U and −→
bp = p −b ∈U ⊥are orthogonal, which
implies that
∥−→
by∥2
2 = ∥−→
bp∥2
2 + ∥−→
py∥2
2,
where −→
by = y −b. Thus, p is indeed the unique point in U that minimizes
the distance from b to any point in U. See Figure 21.2.
I m 
A = U
b
p
I m 
A = U
b
p
y
Fig. 21.2
Given a 3 × 2 matrix A, U = Im A is the peach plane in R3 and p is the
orthogonal projection of b onto U. Furthermore, given y ∈U, the points b, y, and p are
the vertices of a right triangle.
Thus the problem has been reduced to proving that there is a unique x+
of minimum norm such that Ax+ = p, with p = pU(b) ∈U, the orthogonal

21.1. Least Squares Problems and the Pseudo-Inverse
723
projection of b onto U. We use the fact that
Rn = Ker A ⊕(Ker A)⊥.
Consequently, every x ∈Rn can be written uniquely as x = u + v, where
u ∈Ker A and v ∈(Ker A)⊥, and since u and v are orthogonal,
∥x∥2
2 = ∥u∥2
2 + ∥v∥2
2.
Furthermore, since u ∈Ker A, we have Au = 0, and thus Ax = p iﬀAv = p,
which shows that the solutions of Ax = p for which x has minimum norm
must belong to (Ker A)⊥.
However, the restriction of A to (Ker A)⊥is
injective. This is because if Av1 = Av2, where v1, v2 ∈(Ker A)⊥, then
A(v2 −v2) = 0, which implies v2 −v1 ∈Ker A, and since v1, v2 ∈(Ker A)⊥,
we also have v2 −v1 ∈(Ker A)⊥, and consequently, v2 −v1 = 0. This shows
that there is a unique x+ of minimum norm such that Ax+ = p, and that
x+ must belong to (Ker A)⊥. By our previous reasoning, x+ is the unique
vector of minimum norm minimizing ∥Ax −b∥2
2.
The proof also shows that x minimizes ∥Ax −b∥2
2 iﬀ−→
pb = b −Ax is
orthogonal to U, which can be expressed by saying that b−Ax is orthogonal
to every column of A. However, this is equivalent to
A⊤(b −Ax) = 0,
i.e.,
A⊤Ax = A⊤b.
Finally, it turns out that the minimum norm least squares solution x+ can
be found in terms of the pseudo-inverse A+ of A, which is itself obtained
from any SVD of A.
Deﬁnition 21.1. Given any nonzero m × n matrix A of rank r, if A =
V DU ⊤is an SVD of A such that
D =

Λ
0r,n−r
0m−r,r 0m−r,n−r

,
with
Λ = diag(λ1, . . . , λr)
an r×r diagonal matrix consisting of the nonzero singular values of A, then
if we let D+ be the n × m matrix
D+ =
 Λ−1
0r,m−r
0n−r,r 0n−r,m−r

,
with
Λ−1 = diag(1/λ1, . . . , 1/λr),
the pseudo-inverse of A is deﬁned by
A+ = UD+V ⊤.

724
Applications of SVD and Pseudo-Inverses
If A = 0m,n is the zero matrix, we set A+ = 0n,m. Observe that D+ is
obtained from D by inverting the nonzero diagonal entries of D, leaving all
zeros in place, and then transposing the matrix. For example, given the
matrix
D =




1 0 0 0 0
0 2 0 0 0
0 0 3 0 0
0 0 0 0 0



,
its pseudo-inverse is
D+ =






1 0 0 0
0 1
2 0 0
0 0 1
3 0
0 0 0 0
0 0 0 0






.
The pseudo-inverse of a matrix is also known as the Moore-Penrose pseudo-
inverse.
Actually, it seems that A+ depends on the speciﬁc choice of U and V
in an SVD (U, D, V ) for A, but the next theorem shows that this is not so.
Theorem 21.2. The least squares solution of smallest norm of the linear
system Ax = b, where A is an m × n matrix, is given by
x+ = A+b = UD+V ⊤b.
Proof. First assume that A is a (rectangular) diagonal matrix D, as above.
Then since x minimizes ∥Dx −b∥2
2 iﬀDx is the projection of b onto the
image subspace F of D, it is fairly obvious that x+ = D+b. Otherwise, we
can write
A = V DU ⊤,
where U and V are orthogonal. However, since V is an isometry,
∥Ax −b∥2 = ∥V DU ⊤x −b∥2 = ∥DU ⊤x −V ⊤b∥2.
Letting y = U ⊤x, we have ∥x∥2 = ∥y∥2, since U is an isometry, and since
U is surjective, ∥Ax −b∥2 is minimized iﬀ∥Dy −V ⊤b∥2 is minimized, and
we have shown that the least solution is
y+ = D+V ⊤b.
Since y = U ⊤x, with ∥x∥2 = ∥y∥2, we get
x+ = UD+V ⊤b = A+b.
Thus, the pseudo-inverse provides the optimal solution to the least squares
problem.

21.1. Least Squares Problems and the Pseudo-Inverse
725
By Theorem 21.2 and Theorem 21.1, A+b is uniquely deﬁned by every
b, and thus A+ depends only on A.
The Matlab command for computing the pseudo-inverse B of the matrix
A is
B = pinv(A).
Example 21.2. If A is the rank 2 matrix
A =




1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7




whose eigenvalues are −1.1652, 0, 0, 17.1652, using Matlab we obtain the
SVD A = V DU ⊤with
U =




−0.3147 0.7752
0.2630 −0.4805
−0.4275 0.3424
0.0075
0.8366
−0.5402 −0.0903 −0.8039 −0.2319
−0.6530 −0.5231 0.5334 −0.1243



,
V =




−0.3147 −0.7752 0.5452
0.0520
−0.4275 −0.3424 −0.7658 0.3371
−0.5402 0.0903 −0.1042 −0.8301
−0.6530 0.5231
0.3247
0.4411



,
D =




17.1652
0
0 0
0
1.1652 0 0
0
0
0 0
0
0
0 0



.
Then
D+ =




0.0583
0
0 0
0
0.8583 0 0
0
0
0 0
0
0
0 0



,
and
A+ = UD+V ⊤=




−0.5100 −0.2200 0.0700
0.3600
−0.2200 −0.0900 0.0400
0.1700
0.0700
0.0400
0.0100 −0.0200
0.3600
0.1700 −0.0200 −0.2100



,
which is also the result obtained by calling pinv(A).
If A is an m × n matrix of rank n (and so m ≥n), it is immediately
shown that the QR-decomposition in terms of Householder transformations
applies as follows:

726
Applications of SVD and Pseudo-Inverses
There are n m × m matrices H1, . . . , Hn, Householder matrices or the
identity, and an upper triangular m × n matrix R of rank n such that
A = H1 · · · HnR.
Then because each Hi is an isometry,
∥Ax −b∥2 = ∥Rx −Hn · · · H1b∥2,
and the least squares problem Ax = b is equivalent to the system
Rx = Hn · · · H1b.
Now the system
Rx = Hn · · · H1b
is of the form
 R1
0m−n

x =
c
d

,
where R1 is an invertible n × n matrix (since A has rank n), c ∈Rn, and
d ∈Rm−n, and the least squares solution of smallest norm is
x+ = R−1
1 c.
Since R1 is a triangular matrix, it is very easy to invert R1.
The method of least squares is one of the most eﬀective tools of the
mathematical sciences. There are entire books devoted to it. Readers are
advised to consult Strang [Strang (1988)], Golub and Van Loan [Golub
and Van Loan (1996)], Demmel [Demmel (1997)], and Trefethen and Bau
[Trefethen and Bau III (1997)], where extensions and applications of least
squares (such as weighted least squares and recursive least squares) are
described. Golub and Van Loan [Golub and Van Loan (1996)] also contains
a very extensive bibliography, including a list of books on least squares.
21.2
Properties of the Pseudo-Inverse
We begin this section with a proposition which provides a way to calculate
the pseudo-inverse of an m×n matrix A without ﬁrst determining an SVD
factorization.
Proposition 21.1. When A has full rank, the pseudo-inverse A+ can be
expressed as A+ = (A⊤A)−1A⊤when m ≥n, and as A+ = A⊤(AA⊤)−1
when n ≥m. In the ﬁrst case (m ≥n), observe that A+A = I, so A+ is a
left inverse of A; in the second case (n ≥m), we have AA+ = I, so A+ is
a right inverse of A.

21.2. Properties of the Pseudo-Inverse
727
Proof. If m ≥n and A has full rank n, we have
A = V

Λ
0m−n,n

U ⊤
with Λ an n × n diagonal invertible matrix (with positive entries), so
A+ = U
 Λ−1 0n,m−n

V ⊤.
We ﬁnd that
A⊤A = U
 Λ 0n,m−n

V ⊤V

Λ
0m−n,n

U ⊤= UΛ2U ⊤,
which yields
(A⊤A)−1A⊤= UΛ−2U ⊤U
 Λ 0n,m−n

V ⊤= U
 Λ−1 0n,m−n

V ⊤= A+.
Therefore, if m ≥n and A has full rank n, then
A+ = (A⊤A)−1A⊤.
If n ≥m and A has full rank m, then
A = V
 Λ 0m,n−m

U ⊤
with Λ an m × m diagonal invertible matrix (with positive entries), so
A+ = U
 Λ−1
0n−m,m

V ⊤.
We ﬁnd that
AA⊤= V
 Λ 0m,n−m

U ⊤U

Λ
0n−m,m

V ⊤= V Λ2V ⊤,
which yields
A⊤(AA⊤)−1 = U

Λ
0n−m,m

V ⊤V Λ−2V ⊤= U
 Λ−1
0n−m,m

V ⊤= A+.
Therefore, if n ≥m and A has full rank m, then A+ = A⊤(AA⊤)−1.
For example, if A =


1 2
2 3
0 1

, then A has rank 2 and since m ≥n, A+ =
(A⊤A)−1A⊤where
A+ =
5 8
8 14
−1
A⊤=
7/3 −4/3
4/3 5/6
 1 2 0
2 3 1

=
−1/3 2/3 −4/3
1/3 −1/6 5/6

.

728
Applications of SVD and Pseudo-Inverses
If A =
1 2 3 0
0 1 1 −1

, since A has rank 2 and n ≥m, then A+ = A⊤(AA⊤)−1
where
A+ = A⊤
14 5
5 3
−1
=




1 0
2 1
3 1
0 −1




 3/17 −5/17
−5/17 14/17

=




3/17 −5/17
1/17
4/17
4/17 −1/17
5/17 −14/17



.
Let A = V ΣU ⊤be an SVD for any m × n matrix A. It is easy to check
that both AA+ and A+A are symmetric matrices. In fact,
AA+ = V ΣU ⊤UΣ+V ⊤= V ΣΣ+V ⊤= V
Ir
0
0 0m−r

V ⊤
and
A+A = UΣ+V ⊤V ΣU ⊤= UΣ+ΣU ⊤= U
Ir
0
0 0n−r

U ⊤.
From the above expressions we immediately deduce that
AA+A = A,
A+AA+ = A+,
and that
(AA+)2 = AA+,
(A+A)2 = A+A,
so both AA+ and A+A are orthogonal projections (since they are both
symmetric).
Proposition 21.2. The matrix AA+ is the orthogonal projection onto the
range of A and A+A is the orthogonal projection onto Ker(A)⊥= Im(A⊤),
the range of A⊤.
Proof. Obviously, we have range(AA+) ⊆range(A), and for any y = Ax ∈
range(A), since AA+A = A, we have
AA+y = AA+Ax = Ax = y,
so the image of AA+ is indeed the range of A. It is also clear that Ker(A) ⊆
Ker(A+A), and since AA+A = A, we also have Ker(A+A) ⊆Ker(A), and
so
Ker(A+A) = Ker(A).
Since A+A is symmetric, range(A+A) = range((A+A)⊤) = Ker(A+A)⊥=
Ker(A)⊥, as claimed.

21.2. Properties of the Pseudo-Inverse
729
Proposition 21.3. The set range(A) = range(AA+) consists of all vectors
y ∈Rm such that
V ⊤y =
z
0

,
with z ∈Rr.
Proof. Indeed, if y = Ax, then
V ⊤y = V ⊤Ax = V ⊤V ΣU ⊤x = ΣU ⊤x =
Σr
0
0 0m−r

U ⊤x =
z
0

,
where Σr is the r×r diagonal matrix diag(σ1, . . . , σr). Conversely, if V ⊤y =
( z
0 ), then y = V ( z
0 ), and
AA+y = V
Ir
0
0 0m−r

V ⊤y
= V
Ir
0
0 0m−r

V ⊤V
z
0

= V
Ir
0
0 0m−r
 z
0

= V
z
0

= y,
which shows that y belongs to the range of A.
Similarly, we have the following result.
Proposition 21.4. The set range(A+A) = Ker(A)⊥consists of all vectors
y ∈Rn such that
U ⊤y =
z
0

,
with z ∈Rr.
Proof. If y = A+Au, then
y = A+Au = U
Ir
0
0 0n−r

U ⊤u = U
z
0

,
for some z ∈Rr. Conversely, if U ⊤y = ( z
0 ), then y = U ( z
0 ), and so
A+AU
z
0

= U
Ir
0
0 0n−r

U ⊤U
z
0

= U
Ir
0
0 0n−r
 z
0

= U
z
0

= y,

730
Applications of SVD and Pseudo-Inverses
which shows that y ∈range(A+A).
Analogous results hold for complex matrices, but in this case, V and U
are unitary matrices and AA+ and A+A are Hermitian orthogonal projec-
tions.
If A is a normal matrix, which means that AA⊤= A⊤A, then there
is an intimate relationship between SVD's of A and block diagonalizations
of A. As a consequence, the pseudo-inverse of a normal matrix A can be
obtained directly from a block diagonalization of A.
If A is a (real) normal matrix, then we know from Theorem 16.8 that
A can be block diagonalized with respect to an orthogonal matrix U as
A = UΛU ⊤,
where Λ is the (real) block diagonal matrix
Λ = diag(B1, . . . , Bn),
consisting either of 2 × 2 blocks of the form
Bj =
λj −µj
µj λj

with µj ̸= 0, or of one-dimensional blocks Bk = (λk). Then we have the
following proposition:
Proposition 21.5. For any (real) normal matrix A and any block diag-
onalization A = UΛU ⊤of A as above, the pseudo-inverse of A is given
by
A+ = UΛ+U ⊤,
where Λ+ is the pseudo-inverse of Λ. Furthermore, if
Λ =
Λr 0
0 0

,
where Λr has rank r, then
Λ+ =
Λ−1
r
0
0
0

.
Proof. Assume that B1, . . . , Bp are 2 × 2 blocks and that λ2p+1, . . . , λn
are the scalar entries. We know that the numbers λj ± iµj, and the λ2p+k
are the eigenvalues of A. Let ρ2j−1 = ρ2j =
q
λ2
j + µ2
j =
p
det(Bi) for
j = 1, . . . , p, ρj = |λj| for j = 2p + 1, . . . , r. Multiplying U by a suitable

21.2. Properties of the Pseudo-Inverse
731
permutation matrix, we may assume that the blocks of Λ are ordered so
that ρ1 ≥ρ2 ≥· · · ≥ρr > 0. Then it is easy to see that
AA⊤= A⊤A = UΛU ⊤UΛ⊤U ⊤= UΛΛ⊤U ⊤,
with
ΛΛ⊤= diag(ρ2
1, . . . , ρ2
r, 0, . . . , 0),
so ρ1 ≥ρ2 ≥· · · ≥ρr > 0 are the singular values σ1 ≥σ2 ≥· · · ≥σr > 0
of A. Deﬁne the diagonal matrix
Σ = diag(σ1, . . . , σr, 0, . . . , 0),
where r = rank(A), σ1 ≥· · · ≥σr > 0 and the block diagonal matrix Θ
deﬁned such that the block Bi in Λ is replaced by the block σ−1Bi where
σ =
p
det(Bi), the nonzero scalar λj is replaced λj/|λj|, and a diagonal
zero is replaced by 1. Observe that Θ is an orthogonal matrix and
Λ = ΘΣ.
But then we can write
A = UΛU ⊤= UΘΣU ⊤,
and we if let V = UΘ, since U is orthogonal and Θ is also orthogonal, V is
also orthogonal and A = V ΣU ⊤is an SVD for A. Now we get
A+ = UΣ+V ⊤= UΣ+Θ⊤U ⊤.
However, since Θ is an orthogonal matrix, Θ⊤= Θ−1, and a simple calcu-
lation shows that
Σ+Θ⊤= Σ+Θ−1 = Λ+,
which yields the formula
A+ = UΛ+U ⊤.
Also observe that Λr is invertible and
Λ+ =
Λ−1
r
0
0
0

.
Therefore, the pseudo-inverse of a normal matrix can be computed directly
from any block diagonalization of A, as claimed.

732
Applications of SVD and Pseudo-Inverses
Example 21.3. Consider the following real diagonal form of the normal
matrix
A =




−2.7500 2.1651 −0.8660 0.5000
2.1651 −0.2500 −1.5000 0.8660
0.8660
1.5000
0.7500 −0.4330
−0.5000 −0.8660 −0.4330 0.2500



= UΛU ⊤,
with
U =




cos(π/3)
0
sin(π/3)
0
sin(π/3)
0
−cos(π/3)
0
0
cos(π/6)
0
sin(π/6)
0
−cos(π/6)
0
sin(π/6)



,
Λ =




1 −2 0 0
2 1
0 0
0 0 −4 0
0 0
0 0



.
We obtain
Λ+ =




1/5 2/5
0
0
−2/5 1/5
0
0
0
0 −1/4 0
0
0
0
0



,
and the pseudo-inverse of A is
A+ = UΛ+U ⊤=




−0.1375 0.1949
0.1732 −0.1000
0.1949
0.0875
0.3000 −0.1732
−0.1732 −0.3000 0.1500 −0.0866
0.1000
0.1732 −0.0866 0.0500



,
which agrees with pinv(A).
The following properties, due to Penrose, characterize the pseudo-
inverse of a matrix. We have already proved that the pseudo-inverse satis-
ﬁes these equations. For a proof of the converse, see Kincaid and Cheney
[Kincaid and Cheney (1996)].
Proposition 21.6. Given any m × n matrix A (real or complex), the
pseudo-inverse A+ of A is the unique n × m matrix satisfying the following
properties:
AA+A = A,
A+AA+ = A+,
(AA+)⊤= AA+,
(A+A)⊤= A+A.

21.3. Data Compression and SVD
733
21.3
Data Compression and SVD
Among the many applications of SVD, a very useful one is data compres-
sion, notably for images. In order to make precise the notion of closeness
of matrices, we use the notion of matrix norm. This concept is deﬁned in
Chapter 8, and the reader may want to review it before reading any further.
Given an m × n matrix of rank r, we would like to ﬁnd a best approx-
imation of A by a matrix B of rank k ≤r (actually, k < r) such that
∥A −B∥2 (or ∥A −B∥F ) is minimized. The following proposition is known
as the Eckart-Young theorem.
Proposition 21.7. Let A be an m×n matrix of rank r and let V DU ⊤= A
be an SVD for A. Write ui for the columns of U, vi for the columns of V ,
and σ1 ≥σ2 ≥· · · ≥σp for the singular values of A (p = min(m, n)). Then
a matrix of rank k < r closest to A (in the ∥∥2 norm) is given by
Ak =
k
X
i=1
σiviu⊤
i = V diag(σ1, . . . , σk, 0, . . . , 0)U ⊤
and ∥A −Ak∥2 = σk+1.
Proof. By construction, Ak has rank k, and we have
∥A −Ak∥2 =

p
X
i=k+1
σiviu⊤
i

2
=
V diag(0, . . . , 0, σk+1, . . . , σp)U ⊤
2 = σk+1.
It remains to show that ∥A −B∥2 ≥σk+1 for all rank k matrices B. Let
B be any rank k matrix, so its kernel has dimension n −k. The subspace
Uk+1 spanned by (u1, . . . , uk+1) has dimension k + 1, and because the sum
of the dimensions of the kernel of B and of Uk+1 is (n −k) + k + 1 = n + 1,
these two subspaces must intersect in a subspace of dimension at least 1.
Pick any unit vector h in Ker(B) ∩Uk+1. Then since Bh = 0, and since U
and V are isometries, we have
∥A −B∥2
2 ≥∥(A −B)h∥2
2 = ∥Ah∥2
2 =
V DU ⊤h
2
2 =
DU ⊤h
2
2
≥σ2
k+1
U ⊤h
2
2 = σ2
k+1,
which proves our claim.

734
Applications of SVD and Pseudo-Inverses
Note that Ak can be stored using (m + n)k entries, as opposed to mn
entries. When k ≪m, this is a substantial gain.
Example 21.4. Consider the badly conditioned symmetric matrix
A =




10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10




from Section 8.5. Since A is SPD, we have the SVD
A = UDU ⊤,
with
U =




−0.5286 −0.6149 0.3017 −0.5016
−0.3803 −0.3963 −0.0933 0.8304
−0.5520 0.2716 −0.7603 −0.2086
−0.5209 0.6254
0.5676
0.1237



,
D =




30.2887
0
0
0
0
3.8581
0
0
0
0
0.8431
0
0
0
0
0.0102



.
If we set σ3 = σ4 = 0, we obtain the best rank 2 approximation
A2 = U(:, 1 : 2) ∗D(:, 1 : 2) ∗U(:, 1 : 2)′ =




9.9207 7.0280 8.1923 6.8563
7.0280 4.9857 5.9419 5.0436
8.1923 5.9419 9.5122 9.3641
6.8563 5.0436 9.3641 9.7282



.
A nice example of the use of Proposition 21.7 in image compression is
given in Demmel [Demmel (1997)], Chapter 3, Section 3.2.3, pages 113-115;
see the Matlab demo.
Proposition 21.7 also holds for the Frobenius norm; see Problem 21.4.
An interesting topic that we have not addressed is the actual computa-
tion of an SVD. This is a very interesting but tricky subject. Most methods
reduce the computation of an SVD to the diagonalization of a well-chosen
symmetric matrix which is not A⊤A; see Problem 20.1 and Problem 20.3.
Interested readers should read Section 5.4 of Demmel's excellent book [Dem-
mel (1997)], which contains an overview of most known methods and an
extensive list of references.

21.4. Principal Components Analysis (PCA)
735
21.4
Principal Components Analysis (PCA)
Suppose we have a set of data consisting of n points X1, . . . , Xn, with each
Xi ∈Rd viewed as a row vector.
Think of the Xi's as persons, and if
Xi = (xi 1, . . . , xi d), each xi j is the value of some feature (or attribute) of
that person.
Example 21.5. For example, the Xi's could be mathematicians, d = 2,
and the ﬁrst component, xi 1, of Xi could be the year that Xi was born, and
the second component, xi 2, the length of the beard of Xi in centimeters.
Here is a small data set.
Name
year
length
Carl Friedrich Gauss
1777
0
Camille Jordan
1838
12
Adrien-Marie Legendre
1752
0
Bernhard Riemann
1826
15
David Hilbert
1862
2
Henri Poincar´e
1854
5
Emmy Noether
1882
0
Karl Weierstrass
1815
0
Eugenio Beltrami
1835
2
Hermann Schwarz
1843
20
We usually form the n×d matrix X whose ith row is Xi, with 1 ≤i ≤n.
Then the jth column is denoted by Cj (1 ≤j ≤d). It is sometimes called a
feature vector, but this terminology is far from being universally accepted.
In fact, many people in computer vision call the data points Xi feature
vectors!
The purpose of principal components analysis, for short PCA, is to
identify patterns in data and understand the variance-covariance structure
of the data. This is useful for the following tasks:
(1) Data reduction: Often much of the variability of the data can be ac-
counted for by a smaller number of principal components.
(2) Interpretation: PCA can show relationships that were not previously
suspected.
Given a vector (a sample of measurements) x = (x1, . . . , xn) ∈Rn,

736
Applications of SVD and Pseudo-Inverses
recall that the mean (or average) x of x is given by
x =
Pn
i=1 xi
n
.
We let x −x denote the centered data point
x −x = (x1 −x, . . . , xn −x).
In order to measure the spread of the xi's around the mean, we deﬁne
the sample variance (for short, variance) var(x) (or s2) of the sample x by
var(x) =
Pn
i=1(xi −x)2
n −1
.
Example 21.6. If x = (1, 3, −1), x = 1+3−1
3
= 1, x −x = (0, 2, −2), and
var(x) = 02+22+(−2)2
2
= 4. If y = (1, 2, 3), y = 1+2+3
3
= 2, y−y = (−1, 0, 1),
and var(y) = (−1)2+02+12
2
= 2.
There is a reason for using n−1 instead of n. The above deﬁnition makes
var(x) an unbiased estimator of the variance of the random variable being
sampled. However, we don't need to worry about this. Curious readers will
ﬁnd an explanation of these peculiar deﬁnitions in Epstein [Epstein (2007)]
(Chapter 14, Section 14.5) or in any decent statistics book.
Given two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn), the sample
covariance (for short, covariance) of x and y is given by
cov(x, y) =
Pn
i=1(xi −x)(yi −y)
n −1
.
Example 21.7. If we take x = (1, 3, −1) and y = (0, 2, −2), we know
from Example 21.6 that x −x = (0, 2, −2) and y −y = (−1, 0, 1). Thus,
cov(x, y) = 0(−1)+2(0)+(−2)(1)
2
= −1.
The covariance of x and y measures how x and y vary from the mean
with respect to each other. Obviously, cov(x, y) = cov(y, x) and cov(x, x) =
var(x).
Note that
cov(x, y) = (x −x)⊤(y −y)
n −1
.
We say that x and y are uncorrelated iﬀcov(x, y) = 0.

21.4. Principal Components Analysis (PCA)
737
Finally, given an n × d matrix X of n points Xi, for PCA to be mean-
ingful, it will be necessary to translate the origin to the centroid (or center
of gravity) µ of the Xi's, deﬁned by
µ = 1
n(X1 + · · · + Xn).
Observe that if µ = (µ1, . . . , µd), then µj is the mean of the vector Cj (the
jth column of X).
We let X −µ denote the matrix whose ith row is the centered data
point Xi −µ (1 ≤i ≤n). Then the sample covariance matrix (for short,
covariance matrix) of X is the d × d symmetric matrix
Σ =
1
n −1(X −µ)⊤(X −µ) = (cov(Ci, Cj)).
Example 21.8. Let X =


1 1
3 2
−1 3

, the 3 × 2 matrix whose columns are
the vector x and y of Example 21.6. Then
µ = 1
3 [(1, 1) + (3, 2) + (−1, 3)] = (1, 2),
X −µ =


0 −1
2
0
−2 1

,
and
Σ = 1
2
 0 2 −2
−1 0 1
 

0 −1
2
0
−2 1

=
 4 −1
−1 1

.
Remark: The factor
1
n−1 is irrelevant for our purposes and can be ignored.
Example 21.9. Here is the matrix X −µ in the case of our bearded math-
ematicians: since
µ1 = 1828.4,
µ2 = 5.6,
we get the following centered data set.

738
Applications of SVD and Pseudo-Inverses
Name
year
length
Carl Friedrich Gauss
−51.4
−5.6
Camille Jordan
9.6
6.4
Adrien-Marie Legendre
−76.4
−5.6
Bernhard Riemann
−2.4
9.4
David Hilbert
33.6
−3.6
Henri Poincar´e
25.6
−0.6
Emmy Noether
53.6
−5.6
Karl Weierstrass
13.4
−5.6
Eugenio Beltrami
6.6
−3.6
Hermann Schwarz
14.6
14.4
See Figure 21.3.
Gauss
Jordan
Legendre
Riemann
Hilbert
Poincare
Noether
Weierstrass
Beltrami
Schwarz
Fig. 21.3
The centered data points of Example 21.9.
We can think of the vector Cj as representing the features of X in the di-
rection ej (the jth canonical basis vector in Rd, namely ej = (0, . . . , 1, . . . 0),
with a 1 in the jth position).

21.4. Principal Components Analysis (PCA)
739
If v ∈Rd is a unit vector, we wish to consider the projection of the
data points X1, . . . , Xn onto the line spanned by v. Recall from Euclidean
geometry that if x ∈Rd is any vector and v ∈Rd is a unit vector, the
projection of x onto the line spanned by v is
⟨x, v⟩v.
Thus, with respect to the basis v, the projection of x has coordinate ⟨x, v⟩.
If x is represented by a row vector and v by a column vector, then
⟨x, v⟩= xv.
Therefore, the vector Y ∈Rn consisting of the coordinates of the projec-
tions of X1, . . . , Xn onto the line spanned by v is given by Y = Xv, and
this is the linear combination
Xv = v1C1 + · · · + vdCd
of the columns of X (with v = (v1, . . . , vd)).
Observe that because µj is the mean of the vector Cj (the jth column
of X), we get
Y = Xv = v1µ1 + · · · + vdµd,
and so the centered point Y −Y is given by
Y −Y = v1(C1 −µ1) + · · · + vd(Cd −µd) = (X −µ)v.
Furthermore, if Y = Xv and Z = Xw, then
cov(Y, Z) = ((X −µ)v)⊤(X −µ)w
n −1
= v⊤
1
n −1(X −µ)⊤(X −µ)w
= v⊤Σw,
where Σ is the covariance matrix of X. Since Y −Y has zero mean, we
have
var(Y ) = var(Y −Y ) = v⊤
1
n −1(X −µ)⊤(X −µ)v.
The above suggests that we should move the origin to the centroid µ of the
Xi's and consider the matrix X −µ of the centered data points Xi −µ.
From now on beware that we denote the columns of X −µ by C1, . . . , Cd
and that Y denotes the centered point Y = (X −µ)v = Pd
j=1 vjCj, where
v is a unit vector.

740
Applications of SVD and Pseudo-Inverses
Basic idea of PCA: The principal components of X are uncorrelated
projections Y of the data points X1, . . ., Xn onto some directions v (where
the v's are unit vectors) such that var(Y ) is maximal. This suggests the
following deﬁnition:
Deﬁnition 21.2.
Given an n × d matrix X of data points X1, . . . , Xn, if
µ is the centroid of the Xi's, then a ﬁrst principal component of X (ﬁrst
PC) is a centered point Y1 = (X −µ)v1, the projection of X1, . . . , Xn onto
a direction v1 such that var(Y1) is maximized, where v1 is a unit vector
(recall that Y1 = (X −µ)v1 is a linear combination of the Cj's, the columns
of X −µ).
More generally, if Y1, . . . , Yk are k principal components of X along some
unit vectors v1, . . . , vk, where 1 ≤k < d, a (k +1)th principal component of
X ((k + 1)th PC) is a centered point Yk+1 = (X −µ)vk+1, the projection
of X1, . . . , Xn onto some direction vk+1 such that var(Yk+1) is maximized,
subject to cov(Yh, Yk+1) = 0 for all h with 1 ≤h ≤k, and where vk+1 is a
unit vector (recall that Yh = (X −µ)vh is a linear combination of the Cj's).
The vh are called principal directions.
The following proposition is the key to the main result about PCA. This
result was already proven in Proposition 16.11 except that the eigenvalues
were listed in increasing order. For the reader's convenience we prove it
again.
Proposition 21.8. If A is a symmetric d×d matrix with eigenvalues λ1 ≥
λ2 ≥· · · ≥λd and if (u1, . . . , ud) is any orthonormal basis of eigenvectors
of A, where ui is a unit eigenvector associated with λi, then
max
x̸=0
x⊤Ax
x⊤x = λ1
(with the maximum attained for x = u1) and
max
x̸=0,x∈{u1,...,uk}⊥
x⊤Ax
x⊤x = λk+1
(with the maximum attained for x = uk+1), where 1 ≤k ≤d −1.
Proof. First observe that
max
x̸=0
x⊤Ax
x⊤x = max
x {x⊤Ax | x⊤x = 1},
and similarly,
max
x̸=0,x∈{u1,...,uk}⊥
x⊤Ax
x⊤x = max
x

x⊤Ax | (x ∈{u1, . . . , uk}⊥) ∧(x⊤x = 1)
	
.

21.4. Principal Components Analysis (PCA)
741
Since A is a symmetric matrix, its eigenvalues are real and it can be di-
agonalized with respect to an orthonormal basis of eigenvectors, so let
(u1, . . . , ud) be such a basis. If we write
x =
d
X
i=1
xiui,
a simple computation shows that
x⊤Ax =
d
X
i=1
λix2
i .
If x⊤x = 1, then Pd
i=1 x2
i = 1, and since we assumed that λ1 ≥λ2 ≥· · · ≥
λd, we get
x⊤Ax =
d
X
i=1
λix2
i ≤λ1
 d
X
i=1
x2
i

= λ1.
Thus,
max
x

x⊤Ax | x⊤x = 1
	
≤λ1,
and since this maximum is achieved for e1 = (1, 0, . . . , 0), we conclude that
max
x

x⊤Ax | x⊤x = 1
	
= λ1.
Next observe that x ∈{u1, . . . , uk}⊥and x⊤x = 1 iﬀx1 = · · · = xk = 0
and Pd
i=1 xi = 1. Consequently, for such an x, we have
x⊤Ax =
d
X
i=k+1
λix2
i ≤λk+1

d
X
i=k+1
x2
i

= λk+1.
Thus,
max
x

x⊤Ax | (x ∈{u1, . . . , uk}⊥) ∧(x⊤x = 1)
	
≤λk+1,
and since this maximum is achieved for ek+1 = (0, . . . , 0, 1, 0, . . . , 0) with a
1 in position k + 1, we conclude that
max
x

x⊤Ax | (x ∈{u1, . . . , uk}⊥) ∧(x⊤x = 1)
	
= λk+1,
as claimed.

742
Applications of SVD and Pseudo-Inverses
The quantity
x⊤Ax
x⊤x
is known as the Rayleigh ratio or Rayleigh-Ritz ratio (see Section 16.6) and
Proposition 21.8 is often known as part of the Rayleigh-Ritz theorem.
Proposition 21.8 also holds if A is a Hermitian matrix and if we re-
place x⊤Ax by x∗Ax and x⊤x by x∗x. The proof is unchanged, since a
Hermitian matrix has real eigenvalues and is diagonalized with respect to
an orthonormal basis of eigenvectors (with respect to the Hermitian inner
product).
We then have the following fundamental result showing how the SVD
of X yields the PCs:
Theorem 21.3. (SVD yields PCA) Let X be an n×d matrix of data points
X1, . . . , Xn, and let µ be the centroid of the Xi's. If X −µ = V DU ⊤is
an SVD decomposition of X −µ and if the main diagonal of D consists of
the singular values σ1 ≥σ2 ≥· · · ≥σd, then the centered points Y1, . . . , Yd,
where
Yk = (X −µ)uk = kth column of V D
and uk is the kth column of U, are d principal components of X. Further-
more,
var(Yk) =
σ2
k
n −1
and cov(Yh, Yk) = 0, whenever h ̸= k and 1 ≤k, h ≤d.
Proof. Recall that for any unit vector v, the centered projection of the
points X1, . . . , Xn onto the line of direction v is Y = (X −µ)v and that the
variance of Y is given by
var(Y ) = v⊤
1
n −1(X −µ)⊤(X −µ)v.
Since X −µ = V DU ⊤, we get
var(Y ) = v⊤
1
(n −1)(X −µ)⊤(X −µ)v
= v⊤
1
(n −1)UDV ⊤V DU ⊤v
= v⊤U
1
(n −1)D2U ⊤v.

21.4. Principal Components Analysis (PCA)
743
Similarly, if Y = (X −µ)v and Z = (X −µ)w, then the covariance of Y
and Z is given by
cov(Y, Z) = v⊤U
1
(n −1)D2U ⊤w.
Obviously, U
1
(n−1)D2U ⊤is a symmetric matrix whose eigenvalues are
σ2
1
n−1 ≥· · · ≥
σ2
d
n−1, and the columns of U form an orthonormal basis of
unit eigenvectors.
We proceed by induction on k. For the base case, k = 1, maximizing
var(Y ) is equivalent to maximizing
v⊤U
1
(n −1)D2U ⊤v,
where v is a unit vector. By Proposition 21.8, the maximum of the above
quantity is the largest eigenvalue of U
1
(n−1)D2U ⊤, namely
σ2
1
n−1, and it is
achieved for u1, the ﬁrst column of U. Now we get
Y1 = (X −µ)u1 = V DU ⊤u1,
and since the columns of U form an orthonormal basis, U ⊤u1 = e1 =
(1, 0, . . . , 0), and so Y1 is indeed the ﬁrst column of V D.
By the induction hypothesis, the centered points Y1, . . . , Yk, where Yh =
(X −µ)uh and u1, . . . , uk are the ﬁrst k columns of U, are k principal
components of X. Because
cov(Y, Z) = v⊤U
1
(n −1)D2U ⊤w,
where Y = (X −µ)v and Z = (X −µ)w, the condition cov(Yh, Z) = 0 for
h = 1, . . . , k is equivalent to the fact that w belongs to the orthogonal com-
plement of the subspace spanned by {u1, . . . , uk}, and maximizing var(Z)
subject to cov(Yh, Z) = 0 for h = 1, . . . , k is equivalent to maximizing
w⊤U
1
(n −1)D2U ⊤w,
where w is a unit vector orthogonal to the subspace spanned by
{u1, . . . , uk}.
By Proposition 21.8, the maximum of the above quantity
is the (k +1)th eigenvalue of U
1
(n−1)D2U ⊤, namely
σ2
k+1
n−1 , and it is achieved
for uk+1, the (k + 1)th column of U. Now we get
Yk+1 = (X −µ)uk+1 = V DU ⊤uk+1,
and since the columns of U form an orthonormal basis, U ⊤uk+1 = ek+1,
and Yk+1 is indeed the (k + 1)th column of V D, which completes the proof
of the induction step.

744
Applications of SVD and Pseudo-Inverses
The d columns u1, . . . , ud of U are usually called the principal directions
of X −µ (and X). We note that not only do we have cov(Yh, Yk) = 0
whenever h ̸= k, but the directions u1, . . . , ud along which the data are
projected are mutually orthogonal.
Example 21.10. For the centered data set of our bearded mathematicians
(Example 21.9) we have X −µ = V ΣU ⊤, where Σ has two nonzero singular
values, σ1 = 116.9803, σ2 = 21.7812, and with
U =
0.9995 0.0325
0.0325 −0.9995

,
so
the
principal
directions
are
u1
=
(0.9995, 0.0325)
and
u2
=
(0.0325, −0.9995). Observe that u1 is almost the direction of the x-axis,
and u2 is almost the opposite direction of the y-axis. We also ﬁnd that the
projections Y1 and Y2 along the principal directions are
V D =

















−51.5550
3.9249
9.8031
−6.0843
−76.5417
3.1116
−2.0929 −9.4731
33.4651
4.6912
25.5669
1.4325
53.3894
7.3408
13.2107
6.0330
6.4794
3.8128
15.0607 −13.9174

















,
with
X −µ =

















−51.4000 −5.6000
9.6000
6.4000
−76.4000 −5.6000
−2.4000
9.4000
33.6000 −3.6000
25.6000 −0.6000
53.6000 −5.6000
13.4000 −5.6000
6.6000
−3.6000
14.6000
14.4000

















.
See Figures 21.4, 21.5, and 21.6.
We know from our study of SVD that σ2
1, . . . , σ2
d are the eigenvalues
of the symmetric positive semideﬁnite matrix (X −µ)⊤(X −µ) and that
u1, . . . , ud are corresponding eigenvectors. Numerically, it is preferable to
use SVD on X −µ rather than to compute explicitly (X −µ)⊤(X −µ)
and then diagonalize it.
Indeed, the explicit computation of A⊤A from
a matrix A can be numerically quite unstable, and good SVD algorithms
avoid computing A⊤A explicitly.
In general, since an SVD of X is not unique, the principal directions
u1, . . . , ud are not unique.
This can happen when a data set has some
rotational symmetries, and in such a case, PCA is not a very good method
for analyzing the data set.

21.5. Best Aﬃne Approximation
745
u1
u2
Gauss
Legendre
Riemann
Jordan
Schwarz
Noether
Weierstrass
Hilbert
Poincaire
Beltrami
Fig. 21.4
The centered data points of Example 21.9 and the two principal directions of
Example 21.10.
21.5
Best Aﬃne Approximation
A problem very close to PCA (and based on least squares) is to best approx-
imate a data set of n points X1, . . . , Xn, with Xi ∈Rd, by a p-dimensional
aﬃne subspace A of Rd, with 1 ≤p ≤d −1 (the terminology rank d −p is
also used).
First consider p = d −1. Then A = A1 is an aﬃne hyperplane (in Rd),
and it is given by an equation of the form
a1x1 + · · · + adxd + c = 0.
By best approximation, we mean that (a1, . . . , ad, c) solves the homogeneous
linear system



x1 1 · · · x1 d 1
...
...
...
...
xn 1 · · · xn d 1








a1
...
ad
c




=





0
...
0
0





in the least squares sense, subject to the condition that a = (a1, . . . , ad) is
a unit vector, that is, a⊤a = 1, where Xi = (xi 1, · · · , xi d).

746
Applications of SVD and Pseudo-Inverses
Gauss
Jordan
Schwarz
Poincaire
Legendre
Beltrami
Riemann
Hilbert
Noether
Weierstrass
u1
Fig. 21.5
The ﬁrst principal components of Example 21.10, i.e., the projection of the
centered data points onto the u1 line.
If we form the symmetric matrix
⎛
⎜
⎝
x1 1 · · · x1 d 1
...
...
...
...
xn 1 · · · xn d 1
⎞
⎟
⎠
⊤⎛
⎜
⎝
x1 1 · · · x1 d 1
...
...
...
...
xn 1 · · · xn d 1
⎞
⎟
⎠
involved in the normal equations, we see that the bottom row (and last
column) of that matrix is
nμ1
· · ·
nμd
n,
where nμj = n
i=1 xi j is n times the mean of the column Cj of X.
Therefore, if (a1, . . . , ad, c) is a least squares solution, that is, a solution
of the normal equations, we must have
nμ1a1 + · · · + nμdad + nc = 0,
that is,
a1μ1 + · · · + adμd + c = 0,
which means that the hyperplane A1 must pass through the centroid μ of
the data points X1, . . . , Xn. Then we can rewrite the original system with

21.5. Best Aﬃne Approximation
747
Legendre
Gauss
Riemann
Jordan
Schwarz
Beltrami
Weierstrass
Poincare
Hilbert
Noether
u2
Fig. 21.6
The second principal components of Example 21.10, i.e., the projection of the
centered data points onto the u2 line.
respect to the centered data Xi −μ, ﬁnd that the variable c drops out, get
the system
(X −μ)a = 0,
where a = (a1, . . . , ad).
Thus, we are looking for a unit vector a solving (X −μ)a = 0 in the
least squares sense, that is, some a such that a⊤a = 1 minimizing
a⊤(X −μ)⊤(X −μ)a.
Compute some SVD V DU ⊤of X−μ, where the main diagonal of D consists
of the singular values σ1 ≥σ2 ≥· · · ≥σd of X −μ arranged in descending
order. Then
a⊤(X −μ)⊤(X −μ)a = a⊤UD2U ⊤a,
where D2 = diag(σ2
1, . . . , σ2
d) is a diagonal matrix, so pick a to be the last
column in U (corresponding to the smallest eigenvalue σ2
d of (X −μ)⊤(X −
μ)). This is a solution to our best ﬁt problem.
Therefore, if Ud−1 is the linear hyperplane deﬁned by a, that is,
Ud−1 = {u ∈Rd | ⟨u, a⟩= 0},

748
Applications of SVD and Pseudo-Inverses
where a is the last column in U for some SVD V DU ⊤of X −µ, we have
shown that the aﬃne hyperplane A1 = µ + Ud−1 is a best approximation
of the data set X1, . . . , Xn in the least squares sense.
It is easy to show that this hyperplane A1 = µ + Ud−1 minimizes the
sum of the square distances of each Xi to its orthogonal projection onto
A1. Also, since Ud−1 is the orthogonal complement of a, the last column
of U, we see that Ud−1 is spanned by the ﬁrst d −1 columns of U, that is,
the ﬁrst d −1 principal directions of X −µ.
All this can be generalized to a best (d −k)-dimensional aﬃne subspace
Ak approximating X1, . . . , Xn in the least squares sense (1 ≤k ≤d −1).
Such an aﬃne subspace Ak is cut out by k independent hyperplanes Hi
(with 1 ≤i ≤k), each given by some equation
ai 1x1 + · · · + ai dxd + ci = 0.
If we write ai = (ai 1, · · · , ai d), to say that the Hi are independent means
that a1, . . . , ak are linearly independent.
In fact, we may assume that
a1, . . . , ak form an orthonormal system.
Then ﬁnding a best (d −k)-dimensional aﬃne subspace Ak amounts to
solving the homogeneous linear system



X 1 0 · · · 0 0 0
... ... ... ... ... ... ...
0 0 0 · · · 0 X 1










a1
c1
...
ak
ck







=



0
...
0


,
in the least squares sense, subject to the conditions a⊤
i aj = δi j, for all i, j
with 1 ≤i, j ≤k, where the matrix of the system is a block diagonal matrix
consisting of k diagonal blocks (X, 1), where 1 denotes the column vector
(1, . . . , 1) ∈Rn.
Again it is easy to see that each hyperplane Hi must pass through the
centroid µ of X1, . . . , Xn, and by switching to the centered data Xi −µ we
get the system



X −µ 0 · · ·
0
...
... ...
...
0
0 · · · X −µ






a1
...
ak


=



0
...
0


,
with a⊤
i aj = δi j for all i, j with 1 ≤i, j ≤k.
If V DU ⊤= X −µ is an SVD decomposition, it is easy to see that
a least squares solution of this system is given by the last k columns of

21.5. Best Aﬃne Approximation
749
U, assuming that the main diagonal of D consists of the singular values
σ1 ≥σ2 ≥· · · ≥σd of X −µ arranged in descending order. But now the
(d −k)-dimensional subspace Ud−k cut out by the hyperplanes deﬁned by
a1, . . . , ak is simply the orthogonal complement of (a1, . . . , ak), which is the
subspace spanned by the ﬁrst d −k columns of U.
So the best (d −k)-dimensional aﬃne subpsace Ak approximating
X1, . . . , Xn in the least squares sense is Ak = µ + Ud−k, where Ud−k is
the linear subspace spanned by the ﬁrst d−k principal directions of X −µ,
that is, the ﬁrst d −k columns of U. Consequently, we get the following
interesting interpretation of PCA (actually, principal directions):
Theorem 21.4.
Let X be an n×d matrix of data points X1, . . . , Xn, and
let µ be the centroid of the Xi's. If X−µ = V DU ⊤is an SVD decomposition
of X −µ and if the main diagonal of D consists of the singular values
σ1 ≥σ2 ≥· · · ≥σd, then a best (d −k)-dimensional aﬃne approximation
Ak of X1, . . . , Xn in the least squares sense is given by
Ak = µ + Ud−k,
where Ud−k is the linear subspace spanned by the ﬁrst d −k columns of U,
the ﬁrst d −k principal directions of X −µ (1 ≤k ≤d −1).
Example 21.11. Going back to Example 21.10, a best 1-dimensional aﬃne
approximation A1 is the aﬃne line passing through (µ1, µ2) = (1824.4, 5.6)
of direction u1 = (0.9995, 0.0325).
Example 21.12. Suppose in the data set of Example 21.5 that we add
the month of birth of every mathematician as a feature. We obtain the
following data set.
Name
month
year
length
Carl Friedrich Gauss
4
1777
0
Camille Jordan
1
1838
12
Adrien-Marie Legendre
9
1752
0
Bernhard Riemann
9
1826
15
David Hilbert
1
1862
2
Henri Poincar´e
4
1854
5
Emmy Noether
3
1882
0
Karl Weierstrass
10
1815
0
Eugenio Beltrami
10
1835
2
Hermann Schwarz
1
1843
20

750
Applications of SVD and Pseudo-Inverses
The mean of the ﬁrst column is 5.2, and the centered data set is given
below.
Name
month
year
length
Carl Friedrich Gauss
−1.2
−51.4
−5.6
Camille Jordan
−4.2
9.6
6.4
Adrien-Marie Legendre
3.8
−76.4
−5.6
Bernhard Riemann
3.8
−2.4
9.4
David Hilbert
−4.2
33.6
−3.6
Henri Poincar´e
−1.2
25.6
−0.6
Emmy Noether
−2.2
53.6
−5.6
Karl Weierstrass
4.8
13.4
−5.6
Eugenio Beltrami
4.8
6.6
−3.6
Hermann Schwarz
−4.2
14.6
14.4
Running SVD on this data set we get
U =


0.0394
0.1717 0.9844
−0.9987 0.0390 0.0332
−0.0327 −0.9844 0.1730

,
D =

















117.0706
0
0
0
22.0390
0
0
0
10.1571
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

















,
and
V D =

















51.4683
3.3013
−3.8569
−9.9623 −6.6467 −2.7082
76.6327
3.1845
0.2348
2.2393
−8.6943
5.2872
−33.6038
4.1334
−3.6415
−25.5941
1.3833
−0.4350
−53.4333
7.2258
−1.3547
−13.0100
6.8594
4.2010
−6.2843
4.6254
4.3212
−15.2173 −14.3266 −1.1581

















,

21.5. Best Aﬃne Approximation
751
X −µ =

















−1.2000 −51.4000 −5.6000
−4.2000
9.6000
6.4000
3.8000 −76.4000 −5.6000
3.8000
−2.4000
9.4000
−4.2000 33.6000 −3.6000
−1.2000 25.6000 −0.6000
−2.2000 53.6000 −5.6000
4.8000
13.4000 −5.6000
4.8000
6.6000
−3.6000
−4.2000 14.6000
14.4000

















.
The ﬁrst principal direction u1 = (0.0394, −0.9987, −0.0327) is basically
the opposite of the y-axis, and the most signiﬁcant feature is the year of
birth. The second principal direction u2 = (0.1717, 0.0390, −0.9844) is close
to the opposite of the z-axis, and the second most signiﬁcant feature is the
length of beards. A best aﬃne plane is spanned by the vectors u1 and u2.
There are many applications of PCA to data compression, dimension
reduction, and pattern analysis. The basic idea is that in many cases, given
a data set X1, . . . , Xn, with Xi ∈Rd, only a "small" subset of m < d of
the features is needed to describe the data set accurately.
If u1, . . . , ud are the principal directions of X −µ, then the ﬁrst m
projections of the data (the ﬁrst m principal components, i.e., the ﬁrst m
columns of V D) onto the ﬁrst m principal directions represent the data
without much loss of information. Thus, instead of using the original data
points X1, . . . , Xn, with Xi ∈Rd, we can use their projections onto the ﬁrst
m principal directions Y1, . . . , Ym, where Yi ∈Rm and m < d, obtaining a
compressed version of the original data set.
For example, PCA is used in computer vision for face recognition.
Sirovitch and Kirby (1987) seem to be the ﬁrst to have had the idea of
using PCA to compress facial images. They introduced the term eigen-
picture to refer to the principal directions, ui. However, an explicit face
recognition algorithm was given only later by Turk and Pentland (1991).
They renamed eigenpictures as eigenfaces.
For details on the topic of eigenfaces, see Forsyth and Ponce [Forsyth
and Ponce (2002)] (Chapter 22, Section 22.3.2), where you will also ﬁnd
exact references to Turk and Pentland's papers.
Another interesting application of PCA is to the recognition of hand-
written digits. Such an application is described in Hastie, Tibshirani, and
Friedman, [Hastie et al. (2009)] (Chapter 14, Section 14.5.1).

752
Applications of SVD and Pseudo-Inverses
21.6
Summary
The main concepts and results of this chapter are listed below:
• Least squares problems.
• Existence of a least squares solution of smallest norm (Theorem 21.1).
• The pseudo-inverse A+ of a matrix A.
• The least squares solution of smallest norm is given by the pseudo-
inverse (Theorem 21.2).
• Projection properties of the pseudo-inverse.
• The pseudo-inverse of a normal matrix.
• The Penrose characterization of the pseudo-inverse.
• Data compression and SVD.
• Best approximation of rank < r of a matrix.
• Principal component analysis.
• Review of basic statistical concepts: mean, variance, covariance, co-
variance matrix.
• Centered data, centroid.
• The principal components (PCA).
• The Rayleigh-Ritz theorem (Theorem 21.8).
• The main theorem: SVD yields PCA (Theorem 21.3).
• Best aﬃne approximation.
• SVD yields a best aﬃne approximation (Theorem 21.4).
• Face recognition, eigenfaces.
21.7
Problems
Problem 21.1. Consider the overdetermined system in the single variable
x:
a1x = b1, . . . , amx = bm.
Prove that the least squares solution of smallest norm is given by
x+ = a1b1 + · · · + ambm
a2
1 + · · · + a2m
.
Problem 21.2. Let X be an m × n real matrix. For any strictly positive
constant K > 0, the matrix X⊤X + KIn is invertible. Prove that the limit
of the matrix (X⊤X + KIn)−1X⊤when K goes to zero is equal to the
pseudo-inverse X+ of X.

21.7. Problems
753
Problem 21.3. Use Matlab to ﬁnd the pseudo-inverse of the 8 × 6 matrix
A =













64 2 3 61 60 6
9 55 54 12 13 51
17 47 46 20 21 43
40 26 27 37 36 30
32 34 35 29 28 38
41 23 22 44 45 19
49 15 14 52 53 11
8 58 59 5 4 62













.
Observe that the sums of the columns are all equal to to 256. Let b be
the vector of dimension 6 whose coordinates are all equal to 256. Find the
solution x+ of the system Ax = b.
Problem 21.4. The purpose of this problem is to show that Proposi-
tion 21.7 (the Eckart-Young theorem) also holds for the Frobenius norm.
This problem is adapted from Strang [Strang (2019)], Section I.9.
Suppose the m × n matrix B of rank at most k minimizes ∥A −B∥F .
Start with an SVD of B,
B = V
D 0
0 0

U ⊤,
where D is a diagonal k × k matrix. We can write
A = V
L + E + R F
G
H

U ⊤,
where L is strictly lower triangular in the ﬁrst k rows, E is diagonal, and
R is strictly upper triangular, and let
C = V
L + D + R F
0
0

U ⊤,
which clearly has rank ≤k.
(1) Prove that
∥A −B∥2
F = ∥A −C∥2
F + ∥L∥2
F + ∥R∥2
F + ∥F∥2
F .
Since ∥A −B∥F is minimal, show that L = R = F = 0.
Similarly, show that G = 0.
(2) We have
V ⊤AU =
E 0
0 H

,
V ⊤BU =
D 0
0 0

,
where E is diagonal, so deduce that

754
Applications of SVD and Pseudo-Inverses
(1) D = diag(σ1, . . . , σk).
(2) The singular values of H must be the smallest n −k singular values of
A.
(3) The minimum of ∥A −B∥F must be ∥H∥F = (σ2
k+1 + · · · + σ2
r)1/2.
Problem 21.5. Prove that the closest rank 1 approximation (in ∥∥2) of
the matrix
A =
3 0
4 5

is
A1 = 3
2
1 1
3 3

.
Show that the Eckart-Young theorem fails for the operator norm ∥∥∞
by ﬁnding a rank 1 matrix B such that ∥A −B∥∞< ∥A −A1∥∞.
Problem 21.6. Find a closest rank 1 approximation (in ∥∥2) for the ma-
trices
A =


3 0 0
0 2 0
0 0 1

,
A =
0 3
2 0

,
A =
2 1
1 2

.
Problem 21.7. Find a closest rank 1 approximation (in ∥∥2) for the matrix
A =
cos θ −sin θ
sin θ
cos θ

.
Problem 21.8. Let S be a real symmetric positive deﬁnite matrix and let
S = UΣU ⊤be a diagonalization of S. Prove that the closest rank 1 matrix
(in the L2-norm) to S is u1σ1u⊤
1 , where u1 is the ﬁrst column of U.

Chapter 22
Annihilating Polynomials and the
Primary Decomposition
In this chapter all vector spaces are deﬁned over an arbitrary ﬁeld K.
In Section 6.7 we explained that if f : E →E is a linear map on a K-
vector space E, then for any polynomial p(X) = a0Xd + a1Xd−1 + · · · + ad
with coeﬃcients in the ﬁeld K, we can deﬁne the linear map p(f): E →E
by
p(f) = a0f d + a1f d−1 + · · · + adid,
where f k = f ◦· · · ◦f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f d(u) + a1f d−1(u) + · · · + adu,
for every vector u ∈E. Then we showed that if E is ﬁnite-dimensional
and if χf(X) = det(XI −f) is the characteristic polynomial of f, by the
Cayley-Hamilton theorem, we have
χf(f) = 0.
This fact suggests looking at the set of all polynomials p(X) such that
p(f) = 0.
Such polynomials are called annihilating polynomials of f, the set of all
these polynomials, denoted Ann(f), is called the annihilator of f, and the
Cayley-Hamilton theorem shows that it is nontrivial since it contains a
polynomial of positive degree. It turns out that Ann(f) contains a poly-
nomial mf of smallest degree that generates Ann(f), and this polynomial
divides the characteristic polynomial. Furthermore, the polynomial mf en-
capsulates a lot of information about f, in particular whether f can be
diagonalized. One of the main reasons for this is that a scalar λ ∈K is a
zero of the minimal polynomial mf if and only if λ is an eigenvalue of f.
755

756
Annihilating Polynomials and the Primary Decomposition
The ﬁrst main result is Theorem 22.2 which states that if f : E →E is
a linear map on a ﬁnite-dimensional space E, then f is diagonalizable iﬀ
its minimal polynomial m is of the form
m = (X −λ1) · · · (X −λk),
where λ1, . . . , λk are distinct elements of K.
One of the technical tools used to prove this result is the notion of f-
conductor; see Deﬁnition 22.7. As a corollary of Theorem 22.2 we obtain
results about ﬁnite commuting families of diagonalizable or triangulable
linear maps.
If f : E →E is a linear map and λ ∈K is an eigenvalue of f, recall that
the eigenspace Eλ associated with λ is the kernel of the linear map λid−f.
If all the eigenvalues λ1 . . . , λk of f are in K and if f is diagonalizable, then
E = Eλ1 ⊕· · · ⊕Eλk,
but in general there are not enough eigenvectors to span E. A remedy is to
generalize the notion of eigenvector and look for (nonzero) vectors u (called
generalized eigenvectors) such that
(λid −f)r(u) = 0,
for some r ≥1.
Then it turns out that if the minimal polynomial of f is of the form
m = (X −λ1)r1 · · · (X −λk)rk,
then r = ri does the job for λi; that is, if we let
Wi = Ker (λiid −f)ri,
then
E = W1 ⊕· · · ⊕Wk.
The above facts are parts of the primary decomposition theorem (Theo-
rem 22.4). It is a special case of a more general result involving the factor-
ization of the minimal polynomial m into its irreducible monic factors; see
Theorem 22.3.
Theorem 22.4 implies that every linear map f that has all its eigenval-
ues in K can be written as f = D + N, where D is diagonalizable and N is
nilpotent (which means that N r = 0 for some positive integer r). Further-
more D and N commute and are unique. This is the Jordan decomposition,
Theorem 22.5.

22.1. Basic Properties of Polynomials; Ideals, GCD's
757
The Jordan decomposition suggests taking a closer look at nilpotent
maps. We prove that for any nilpotent linear map f : E →E on a ﬁnite-
dimensional vector space E of dimension n over a ﬁeld K, there is a basis
of E such that the matrix N of f is of the form
N =







0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
... ...
...
...
... ...
0 0 0 · · · 0 νn
0 0 0 · · · 0 0







,
where νi = 1 or νi = 0; see Theorem 22.6. As a corollary we obtain the
Jordan form; which involves matrices of the form
Jr(λ) =








λ 1 0 · · · 0
0 λ 1 · · · 0
... ... ... ... ...
0 0 0 ... 1
0 0 0 · · · λ








,
called Jordan blocks; see Theorem 22.7.
22.1
Basic Properties of Polynomials; Ideals, GCD's
In order to understand the structure of Ann(f), we need to review three
basic properties of polynomials. We refer the reader to Hoﬀman and Kunze
[Kenneth and Ray (1971)], Artin [Artin (1991)], Dummit and Foote [Dum-
mit and Foote (1999)], and Godement [Godement (1958)] for comprehensive
discussions of polynomials and their properties.
We begin by recalling some basic nomenclature. Given a ﬁeld K, any
nonzero polynomial p(X) ∈K[X] has some monomial of highest degree
a0Xn with a0 ̸= 0, and the integer n = deg(p) ≥0 is called the degree of p.
It is convenient to set the degree of the zero polynomial (denoted by 0) to
be
deg(0) = −∞.
A polynomial p(X) such that the coeﬃcient a0 of its monomial of highest
degree is 1 is called a monic polynomial. For example, let K = R. The
polynomial p(X) = 4X7 + 2X5 is of degree 7 but is not monic since a0 =
4. On the other hand, the polynomial p(X) = X3 −3X + 1 is a monic
polynomial of degree 3.
We now discuss three key concepts of polynomial algebra:

758
Annihilating Polynomials and the Primary Decomposition
(1) Ideals
(2) Greatest common divisors and the Bezout identity.
(3) Irreducible polynomials and prime factorization.
Recall the deﬁnition a of ring (see Deﬁnition 2.2).
Deﬁnition 22.1. A ring is a set A equipped with two operations +: A ×
A →A (called addition) and ∗: A × A →A (called multiplication) having
the following properties:
(R1) A is an abelian group w.r.t. +;
(R2) ∗is associative and has an identity element 1 ∈A;
(R3) ∗is distributive w.r.t. +.
The identity element for addition is denoted 0, and the additive inverse
of a ∈A is denoted by −a. More explicitly, the axioms of a ring are the
following equations which hold for all a, b, c ∈A:
a + (b + c) = (a + b) + c
(associativity of +)
(22.1)
a + b = b + a
(commutativity of +)
(22.2)
a + 0 = 0 + a = a
(zero)
(22.3)
a + (−a) = (−a) + a = 0
(additive inverse)
(22.4)
a ∗(b ∗c) = (a ∗b) ∗c
(associativity of ∗)
(22.5)
a ∗1 = 1 ∗a = a
(identity for ∗)
(22.6)
(a + b) ∗c = (a ∗c) + (b ∗c)
(distributivity)
(22.7)
a ∗(b + c) = (a ∗b) + (a ∗c)
(distributivity)
(22.8)
The ring A is commutative if
a ∗b = b ∗a
for all a, b ∈A.
From (22.7) and (22.8), we easily obtain
a ∗0 = 0 ∗a = 0
(22.9)
a ∗(−b) = (−a) ∗b = −(a ∗b).
(22.10)
The ﬁrst crucial notion is that of an ideal.
Deﬁnition 22.2. Given a commutative ring A with unit 1, an ideal of A
is a nonempty subset I of A satisfying the following properties:
(ID1) If a, b ∈I, then b −a ∈I.
(ID2) If a ∈I, then ax ∈I for all x ∈A.

22.1. Basic Properties of Polynomials; Ideals, GCD's
759
An ideal I is a principal ideal if there is some a ∈I, called a generator,
such that
I = {ax | x ∈A}.
In this case we usually write I = aA or I = (a). The ideal I = (0) = {0}
is called the null ideal (or zero ideal).
The following proposition is a fundamental result about polynomials
over a ﬁeld.
Proposition 22.1. If K is a ﬁeld, then every polynomial ideal I ⊆K[X]
is a principal ideal. As a consequence, if I is not the zero ideal, then there
is a unique monic polynomial
p(X) = Xn + a1Xn−1 + · · · + an−1X + an
in I such that I = (p).
Proof. This result is not hard to prove if we recall that polynomials can
divided. Given any two nonzero polynomials f, g ∈K[X], there are unique
polynomials q, r such that
f = qg + r,
and
deg(r) < deg(g).
(22.11)
If I is not the zero ideal, there is some polynomial of smallest degree in I,
and since K is a ﬁeld, by suitable multiplication by a scalar, we can make
sure that this polynomial is monic. Thus, let f be a monic polynomial of
smallest degree in I. By (ID2), it is clear that (f) ⊆I. Now let g ∈I.
Using (22.11), there exist unique q, r ∈K[X] such that
g = qf + r
and
deg(r) < deg(f).
If r ̸= 0, there is some λ ̸= 0 in K such that λr is a monic polynomial, and
since λr = λg −λqf, with f, g ∈I, by (ID1) and (ID2), we have λr ∈I,
where deg(λr) < deg(f) and λr is a monic polynomial, contradicting the
minimality of the degree of f. Thus, r = 0, and g ∈(f). The uniqueness
of the monic polynomial f is left as an exercise.
We will also need to know that the greatest common divisor of polyno-
mials exist. Given any two nonzero polynomials f, g ∈K[X], recall that f
divides g if g = qf for some q ∈K[X].
Deﬁnition 22.3. Given any two nonzero polynomials f, g ∈K[X], a poly-
nomial d ∈K[X] is a greatest common divisor of f and g (for short, a gcd
of f and g) if d divides f and g and whenever h ∈K[X] divides f and g,
then h divides d. We say that f and g are relatively prime if 1 is a gcd of
f and g.

760
Annihilating Polynomials and the Primary Decomposition
Note that f and g are relatively prime iﬀall of their gcd's are constants
(scalars in K), or equivalently, if f, g have no common divisor q of degree
deg(q) ≥1. For example, over R, gcd(X2 −1, X3 + X2 −X −1) = (X −
1)(X+1) since X3+X2−X−1 = (X−1)(X+1)2, while gcd(X3+1, X−1) =
1.
We can characterize gcd's of polynomials as follows.
Proposition 22.2. Let K be a ﬁeld and let f, g ∈K[X] be any two nonzero
polynomials. For every polynomial d ∈K[X], the following properties are
equivalent:
(1) The polynomial d is a gcd of f and g.
(2) The polynomial d divides f and g and there exist u, v ∈K[X] such that
d = uf + vg.
(3) The ideals (f), (g), and (d) satisfy the equation
(d) = (f) + (g).
In addition, d ̸= 0, and d is unique up to multiplication by a nonzero
scalar in K.
As a consequence of Proposition 22.2, two nonzero polynomials f, g ∈
K[X] are relatively prime iﬀthere exist u, v ∈K[X] such that
uf + vg = 1.
The identity
d = uf + vg
of Part (2) of Proposition 22.2 is often called the Bezout identity. For an
example of Bezout's identity, take K = R. Since X3 + 1 and X −1 are
relatively prime, we have
1 = 1/2(X3 + 1) −1/2(X2 + X + 1)(X −1).
An important consequence of the Bezout identity is the following result.
Proposition 22.3. (Euclid's proposition) Let K be a ﬁeld and let f, g, h ∈
K[X] be any nonzero polynomials. If f divides gh and f is relatively prime
to g, then f divides h.
Proposition 22.3 can be generalized to any number of polynomials.
Proposition 22.4. Let K be a ﬁeld and let f, g1, . . . , gm ∈K[X] be some
nonzero polynomials. If f and gi are relatively prime for all i, 1 ≤i ≤m,
then f and g1 · · · gm are relatively prime.

22.1. Basic Properties of Polynomials; Ideals, GCD's
761
Deﬁnition 22.3 is generalized to any ﬁnite number of polynomials as
follows.
Deﬁnition 22.4. Given any nonzero polynomials f1, . . . , fn ∈K[X], where
n ≥2, a polynomial d ∈K[X] is a greatest common divisor of f1, . . . , fn
(for short, a gcd of f1, . . . , fn) if d divides each fi and whenever h ∈K[X]
divides each fi, then h divides d. We say that f1, . . . , fn are relatively prime
if 1 is a gcd of f1, . . . , fn.
It is easily shown that Proposition 22.2 can be generalized to any ﬁnite
number of polynomials.
Proposition 22.5. Let K be a ﬁeld and let f1, . . . , fn ∈K[X] be any
n ≥2 nonzero polynomials. For every polynomial d ∈K[X], the following
properties are equivalent:
(1) The polynomial d is a gcd of f1, . . . , fn.
(2) The polynomial d divides each fi and there exist u1, . . . , un ∈K[X]
such that
d = u1f1 + · · · + unfn.
(3) The ideals (fi), and (d) satisfy the equation
(d) = (f1) + · · · + (fn).
In addition, d ̸= 0, and d is unique up to multiplication by a nonzero
scalar in K.
As a consequence of Proposition 22.5, any n ≥2 nonzero polynomials
f1, . . . , fn ∈K[X] are relatively prime iﬀthere exist u1, . . . , un ∈K[X]
such that
u1f1 + · · · + unfn = 1,
the Bezout identity.
We will also need to know that every nonzero polynomial (over a ﬁeld)
can be factored into irreducible polynomials, which is the generalization of
the prime numbers to polynomials.
Deﬁnition 22.5. Given a ﬁeld K, a polynomial p ∈K[X] irreducible or
indecomposable or prime is if deg(p) ≥1 and if p is not divisible by any
polynomial q ∈K[X] such that 1 ≤deg(q) < deg(p). Equivalently, p is
irreducible if deg(p) ≥1 and if p = q1q2, then either q1 ∈K or q2 ∈K (and
of course, q1 ̸= 0, q2 ̸= 0).

762
Annihilating Polynomials and the Primary Decomposition
Every polynomial aX + b of degree 1 is irreducible. Over the ﬁeld R,
the polynomial X2 + 1 is irreducible (why?), but X3 + 1 is not irreducible,
since
X3 + 1 = (X + 1)(X2 −X + 1).
The polynomial X2 −X + 1 is irreducible over R (why?). It would seem
that X4 + 1 is irreducible over R, but in fact,
X4 + 1 = (X2 −
√
2X + 1)(X2 +
√
2X + 1).
However, in view of the above factorization, X4 + 1 is irreducible over Q.
It can be shown that the irreducible polynomials over R are the polyno-
mials of degree 1 or the polynomials of degree 2 of the form aX2 + bX + c,
for which b2 −4ac < 0 (i.e., those having no real roots). This is not easy
to prove! Over the complex numbers C, the only irreducible polynomials
are those of degree 1. This is a version of a fact often referred to as the
"Fundamental Theorem of Algebra."
Observe that the deﬁnition of irreducibility implies that any ﬁnite num-
ber of distinct irreducible polynomials are relatively prime.
The following fundamental result can be shown
Theorem 22.1. Given any ﬁeld K, for every nonzero polynomial
f = adXd + ad−1Xd−1 + · · · + a0
of degree d = deg(f) ≥1 in K[X], there exists a unique set {⟨p1, k1⟩,
. . . , ⟨pm, km⟩} such that
f = adpk1
1 · · · pkm
m ,
where the pi ∈K[X] are distinct irreducible monic polynomials, the ki are
(not necessarily distinct) integers, and with m ≥1, ki ≥1.
We can now return to minimal polynomials.
22.2
Annihilating Polynomials and the Minimal Polynomial
Given a linear map f : E →E, it is easy to check that the set Ann(f) of
polynomials that annihilate f is an ideal. Furthermore, when E is ﬁnite-
dimensional, the Cayley-Hamilton theorem implies that Ann(f) is not the
zero ideal. Therefore, by Proposition 22.1, there is a unique monic polyno-
mial mf that generates Ann(f).
Deﬁnition 22.6. If f : E →E is a linear map on a ﬁnite-dimensional
vector space E, the unique monic polynomial mf(X) that generates the
ideal Ann(f) of polynomials which annihilate f (the annihilator of f) is
called the minimal polynomial of f.

22.2. Annihilating Polynomials and the Minimal Polynomial
763
The minimal polynomial mf of f is the monic polynomial of smallest
degree that annihilates f. Thus, the minimal polynomial divides the char-
acteristic polynomial χf, and deg(mf) ≥1. For simplicity of notation, we
often write m instead of mf.
If A is any n × n matrix, the set Ann(A) of polynomials that annihilate
A is the set of polynomials
p(X) = a0Xd + a1Xd−1 + · · · + ad−1X + ad
such that
a0Ad + a1Ad−1 + · · · + ad−1A + adI = 0.
It is clear that Ann(A) is a nonzero ideal and its unique monic generator is
called the minimal polynomial of A. We check immediately that if Q is an
invertible matrix, then A and Q−1AQ have the same minimal polynomial.
Also, if A is the matrix of f with respect to some basis, then f and A have
the same minimal polynomial.
The zeros (in K) of the minimal polynomial of f and the eigenvalues of
f (in K) are intimately related.
Proposition 22.6. Let f : E →E be a linear map on some ﬁnite-
dimensional vector space E. Then λ ∈K is a zero of the minimal poly-
nomial mf(X) of f iﬀλ is an eigenvalue of f iﬀλ is a zero of χf(X).
Therefore, the minimal and the characteristic polynomials have the same
zeros (in K), except for multiplicities.
Proof. First assume that m(λ) = 0 (with λ ∈K, and writing m instead
of mf). If so, using polynomial division, m can be factored as
m = (X −λ)q,
with deg(q) < deg(m). Since m is the minimal polynomial, q(f) ̸= 0, so
there is some nonzero vector v ∈E such that u = q(f)(v) ̸= 0. But then,
because m is the minimal polynomial,
0 = m(f)(v)
= (f −λid)(q(f)(v))
= (f −λid)(u),
which shows that λ is an eigenvalue of f.
Conversely, assume that λ ∈K is an eigenvalue of f. This means that
for some u ̸= 0, we have f(u) = λu. Now it is easy to show that
m(f)(u) = m(λ)u,
and since m is the minimal polynomial of f, we have m(f)(u) = 0, so
m(λ)u = 0, and since u ̸= 0, we must have m(λ) = 0.

764
Annihilating Polynomials and the Primary Decomposition
Proposition 22.7. Let f : E →E be a linear map on some ﬁnite-
dimensional vector space E.
If f diagonalizable, then its minimal poly-
nomial is a product of distinct factors of degree 1.
Proof. If we assume that f is diagonalizable, then its eigenvalues are all in
K, and if λ1, . . . , λk are the distinct eigenvalues of f, and then by Proposi-
tion 22.6, the minimal polynomial m of f must be a product of powers of
the polynomials (X −λi). Actually, we claim that
m = (X −λ1) · · · (X −λk).
For this we just have to show that m annihilates f.
However, for any
eigenvector u of f, one of the linear maps f −λiid sends u to 0, so
m(f)(u) = (f −λ1id) ◦· · · ◦(f −λkid)(u) = 0.
Since E is spanned by the eigenvectors of f, we conclude that
m(f) = 0.
It turns out that the converse of Proposition 22.7 is true, but this will take
a little work to establish it.
22.3
Minimal Polynomials of Diagonalizable
Linear Maps
In this section we prove that if the minimal polynomial mf of a linear map
f is of the form
mf = (X −λ1) · · · (X −λk)
for distinct scalars λ1, . . . , λk ∈K, then f is diagonalizable.
This is a
powerful result that has a number of implications. But ﬁrst we need of few
properties of invariant subspaces.
Given a linear map f : E →E, recall that a subspace W of E is invariant
under f if f(u) ∈W for all u ∈W.
For example, if f : R2 →R2 is
f(x, y) = (−x, y), the y-axis is invariant under f.
Proposition 22.8. Let W be a subspace of E invariant under the linear
map f : E →E (where E is ﬁnite-dimensional). Then the minimal poly-
nomial of the restriction f | W of f to W divides the minimal polynomial
of f, and the characteristic polynomial of f | W divides the characteristic
polynomial of f.

22.3. Minimal Polynomials of Diagonalizable Linear Maps
765
Sketch of proof. The key ingredient is that we can pick a basis
(e1, . . . , en) of E in which (e1, . . . , ek) is a basis of W. The matrix of f
over this basis is a block matrix of the form
A =
B C
0 D

,
where B is a k × k matrix, D is an (n −k) × (n −k) matrix, and C is a
k × (n −k) matrix. Then
det(XI −A) = det(XI −B) det(XI −D),
which implies the statement about the characteristic polynomials. Further-
more,
Ai =
Bi Ci
0 Di

,
for some k × (n −k) matrix Ci.
It follows that any polynomial which
annihilates A also annihilates B and D. So the minimal polynomial of B
divides the minimal polynomial of A.
For the next step, there are at least two ways to proceed. We can use an
old-fashion argument using Lagrange interpolants, or we can use a slight
generalization of the notion of annihilator.
We pick the second method
because it illustrates nicely the power of principal ideals.
What we need is the notion of conductor (also called transporter).
Deﬁnition 22.7. Let f : E →E be a linear map on a ﬁnite-dimensional
vector space E, let W be an invariant subspace of f, and let u be any vector
in E. The set Sf(u, W) consisting of all polynomials q ∈K[X] such that
q(f)(u) ∈W is called the f-conductor of u into W.
Observe that the minimal polynomial mf of f always belongs to
Sf(u, W), so this is a nontrivial set.
Also, if W = (0), then Sf(u, (0))
is just the annihilator of f. The crucial property of Sf(u, W) is that it is
an ideal.
Proposition 22.9. If W is an invariant subspace for f, then for each
u ∈E, the f-conductor Sf(u, W) is an ideal in K[X].
We leave the proof as a simple exercise, using the fact that if W invariant
under f, then W is invariant under every polynomial q(f) in Sf(u, W).

766
Annihilating Polynomials and the Primary Decomposition
Since Sf(u, W) is an ideal, it is generated by a unique monic polynomial
q of smallest degree, and because the minimal polynomial mf of f is in
Sf(u, W), the polynomial q divides m.
Deﬁnition 22.8. The unique monic polynomial which generates Sf(u, W)
is called the conductor of u into W.
Example 22.1. For example, suppose f : R2 →R2 where f(x, y) = (x, 0).
Observe that W = {(x, 0) ∈R2} is invariant under f. By representing f
as
1 0
0 0

, we see that mf(X) = χf(X) = X2 −X. Let u = (0, y). Then
Sf(u, W) = (X), and we say X is the conductor of u into W.
Proposition 22.10. Let f : E →E be a linear map on a ﬁnite-dimensional
space E and assume that the minimal polynomial m of f is of the form
m = (X −λ1)r1 · · · (X −λk)rk,
where the eigenvalues λ1, . . . , λk of f belong to K. If W is a proper subspace
of E which is invariant under f, then there is a vector u ∈E with the
following properties:
(a) u /∈W;
(b) (f −λid)(u) ∈W, for some eigenvalue λ of f.
Proof. Observe that (a) and (b) together assert that the conductor of u
into W is a polynomial of the form X −λi. Pick any vector v ∈E not
in W, and let g be the conductor of v into W, i.e. g(f)(v) ∈W. Since g
divides m and v /∈W, the polynomial g is not a constant, and thus it is of
the form
g = (X −λ1)s1 · · · (X −λk)sk,
with at least some si > 0. Choose some index j such that sj > 0. Then
X −λj is a factor of g, so we can write
g = (X −λj)q.
(22.12)
By deﬁnition of g, the vector u = q(f)(v) cannot be in W, since otherwise
g would not be of minimal degree. However, (22.12) implies that
(f −λjid)(u) = (f −λjid)(q(f)(v))
= g(f)(v)
is in W, which concludes the proof.

22.3. Minimal Polynomials of Diagonalizable Linear Maps
767
We can now prove the main result of this section.
Theorem 22.2. Let f : E →E be a linear map on a ﬁnite-dimensional
space E. Then f is diagonalizable iﬀits minimal polynomial m is of the
form
m = (X −λ1) · · · (X −λk),
where λ1, . . . , λk are distinct elements of K.
Proof. We already showed in Proposition 22.7 that if f is diagonalizable,
then its minimal polynomial is of the above form (where λ1, . . . , λk are the
distinct eigenvalues of f).
For the converse, let W be the subspace spanned by all the eigenvectors
of f. If W ̸= E, since W is invariant under f, by Proposition 22.10, there
is some vector u /∈W such that for some λj, we have
(f −λjid)(u) ∈W.
Let v = (f −λjid)(u) ∈W. Since v ∈W, we can write
v = w1 + · · · + wk
where f(wi) = λiwi (either wi = 0 or wi is an eigenvector for λi), and so
for every polynomial h, we have
h(f)(v) = h(λ1)w1 + · · · + h(λk)wk,
which shows that h(f)(v) ∈W for every polynomial h. We can write
m = (X −λj)q
for some polynomial q, and also
q −q(λj) = p(X −λj)
for some polynomial p. We know that p(f)(v) ∈W, and since m is the
minimal polynomial of f, we have
0 = m(f)(u) = (f −λjid)(q(f)(u)),
which implies that q(f)(u) ∈W (either q(f)(u) = 0, or it is an eigenvector
associated with λj). However,
q(f)(u) −q(λj)u = p(f)((f −λjid)(u)) = p(f)(v),
and since p(f)(v) ∈W and q(f)(u) ∈W, we conclude that q(λj)u ∈W.
But, u /∈W, which implies that q(λj) = 0, so λj is a double root of m, a
contradiction. Therefore, we must have W = E.
Remark: Proposition 22.10 can be used to give a quick proof of Theo-
rem 14.1.

768
Annihilating Polynomials and the Primary Decomposition
22.4
Commuting Families of Diagonalizable and
Triangulable Maps
Using Theorem 22.2, we can give a short proof about commuting diagonal-
izable linear maps.
Deﬁnition 22.9. If F is a family of linear maps on a vector space E, we
say that F is a commuting family iﬀf ◦g = g ◦f for all f, g ∈F.
Proposition 22.11. Let F be a commuting family of diagonalizable linear
maps on a vector space E. There exists a basis of E such that every linear
map in F is represented in that basis by a diagonal matrix.
Proof. We proceed by induction on n = dim(E). If n = 1, there is nothing
to prove. If n > 1, there are two cases. If all linear maps in F are of the
form λid for some λ ∈K, then the proposition holds trivially. In the second
case, let f ∈F be some linear map in F which is not a scalar multiple of
the identity. In this case, f has at least two distinct eigenvalues λ1, . . . , λk,
and because f is diagonalizable, E is the direct sum of the corresponding
eigenspaces Eλ1, . . . , Eλk. For every index i, the eigenspace Eλi is invariant
under f and under every other linear map g in F, since for any g ∈F and
any u ∈Eλi, because f and g commute, we have
f(g(u)) = g(f(u)) = g(λiu) = λig(u)
so g(u) ∈Eλi. Let Fi be the family obtained by restricting each f ∈F
to Eλi. By Proposition 22.8, the minimal polynomial of every linear map
f | Eλi in Fi divides the minimal polynomial mf of f, and since f is
diagonalizable, mf is a product of distinct linear factors, so the minimal
polynomial of f | Eλi is also a product of distinct linear factors. By The-
orem 22.2, the linear map f | Eλi is diagonalizable. Since k > 1, we have
dim(Eλi) < dim(E) for i = 1, . . . , k, and by the induction hypothesis, for
each i there is a basis of Eλi over which f | Eλi is represented by a diagonal
matrix. Since the above argument holds for all i, by combining the bases
of the Eλi, we obtain a basis of E such that the matrix of every linear map
f ∈F is represented by a diagonal matrix.
There is also an analogous result for commuting families of linear maps
represented by upper triangular matrices. To prove this we need the fol-
lowing proposition.
Proposition 22.12. Let F be a nonempty commuting family of triangula-
ble linear maps on a ﬁnite-dimensional vector space E. Let W be a proper

22.4. Commuting Families of Diagonalizable and Triangulable Maps
769
subspace of E which is invariant under F. Then there exists a vector u ∈E
such that:
(1) u /∈W.
(2) For every f ∈F, the vector f(u) belongs to the subspace W ⊕Ku
spanned by W and u.
Proof. By renaming the elements of F if necessary, we may assume that
(f1, . . . , fr) is a basis of the subspace of End(E) spanned by F. We prove
by induction on r that there exists some vector u ∈E such that
(1) u /∈W.
(2) (fi −αiid)(u) ∈W for i = 1, . . . , r, for some scalars αi ∈K.
Consider the base case r = 1. Since f1 is triangulable, its eigenvalues
all belong to K since they are the diagonal entries of the triangular matrix
associated with f1 (this is the easy direction of Theorem 14.1), so the
minimal polynomial of f1 is of the form
m = (X −λ1)r1 · · · (X −λk)rk,
where the eigenvalues λ1, . . . , λk of f1 belong to K. We conclude by apply-
ing Proposition 22.10.
Next assume that r ≥2 and that the induction hypothesis holds for
f1, . . . , fr−1. Thus, there is a vector ur−1 ∈E such that
(1) ur−1 /∈W.
(2) (fi −αiid)(ur−1) ∈W for i = 1, . . . , r −1, for some scalars αi ∈K.
Let
Vr−1 = {w ∈E | (fi −αiid)(w) ∈W, i = 1, . . . , r −1}.
Clearly, W ⊆Vr−1 and ur−1 ∈Vr−1.
We claim that Vr−1 is invariant
under F. This is because, for any v ∈Vr−1 and any f ∈F, since f and fi
commute, we have
(fi −αiid)(f(v)) = f((fi −αiid)(v)),
1 ≤i ≤r −1.
Now (fi −αiid)(v) ∈W because v ∈Vr−1, and W is invariant under F, so
f(fi −αiid)(v)) ∈W, that is, (fi −αiid)(f(v)) ∈W.
Consider the restriction gr of fr to Vr−1. The minimal polynomial of
gr divides the minimal polynomial of fr, and since fr is triangulable, just
as we saw for f1, the minimal polynomial of fr is of the form
m = (X −λ1)r1 · · · (X −λk)rk,

770
Annihilating Polynomials and the Primary Decomposition
where the eigenvalues λ1, . . . , λk of fr belong to K, so the minimal polyno-
mial of gr is of the same form. By Proposition 22.10, there is some vector
ur ∈Vr−1 such that
(1) ur /∈W.
(2) (gr −αrid)(ur) ∈W for some scalars αr ∈K.
Now since ur ∈Vr−1, we have (fi −αiid)(ur) ∈W for i = 1, . . . , r −1, so
(fi −αiid)(ur) ∈W for i = 1, . . . , r (since gr is the restriction of fr), which
concludes the proof of the induction step. Finally, since every f ∈F is
the linear combination of (f1, . . . , fr), Condition (2) of the inductive claim
implies Condition (2) of the proposition.
We can now prove the following result.
Proposition 22.13. Let F be a nonempty commuting family of triangu-
lable linear maps on a ﬁnite-dimensional vector space E. There exists a
basis of E such that every linear map in F is represented in that basis by
an upper triangular matrix.
Proof. Let n = dim(E). We construct inductively a basis (u1, . . . , un) of
E such that if Wi is the subspace spanned by (u1 . . . , ui), then for every
f ∈F,
f(ui) = af
1iu1 + · · · + af
iiui,
for some af
ij ∈K; that is, f(ui) belongs to the subspace Wi.
We begin by applying Proposition 22.12 to the subspace W0 = (0) to
get u1 so that for all f ∈F,
f(u1) = αf
1u1.
For the induction step, since Wi invariant under F, we apply Proposi-
tion 22.12 to the subspace Wi, to get ui+1 ∈E such that
(1) ui+1 /∈Wi.
(2) For every f ∈F, the vector f(ui+1) belong to the subspace spanned
by Wi and ui+1.
Condition (1) implies that (u1, . . . , ui, ui+1) is linearly independent, and
Condition (2) means that for every f ∈F,
f(ui+1) = af
1i+1u1 + · · · + af
i+1i+1ui+1,
for some af
i+1j ∈K, establishing the induction step. After n steps, each
f ∈F is represented by an upper triangular matrix.

22.5. The Primary Decomposition Theorem
771
Observe that if F consists of a single linear map f and if the minimal
polynomial of f is of the form
m = (X −λ1)r1 · · · (X −λk)rk,
with all λi ∈K, using Proposition 22.10 instead of Proposition 22.12, the
proof of Proposition 22.13 yields another proof of Theorem 14.1.
22.5
The Primary Decomposition Theorem
If f : E →E is a linear map and λ ∈K is an eigenvalue of f, recall that
the eigenspace Eλ associated with λ is the kernel of the linear map λid−f.
If all the eigenvalues λ1 . . . , λk of f are in K, it may happen that
E = Eλ1 ⊕· · · ⊕Eλk,
but in general there are not enough eigenvectors to span E. What if we
generalize the notion of eigenvector and look for (nonzero) vectors u such
that
(λid −f)r(u) = 0,
for some r ≥1?
It turns out that if the minimal polynomial of f is of the form
m = (X −λ1)r1 · · · (X −λk)rk,
then r = ri does the job for λi; that is, if we let
Wi = Ker (λiid −f)ri,
then
E = W1 ⊕· · · ⊕Wk.
This result is very nice but seems to require that the eigenvalues of f all
belong to K. Actually, it is a special case of a more general result involving
the factorization of the minimal polynomial m into its irreducible monic
factors (see Theorem 22.1),
m = pr1
1 · · · prk
k ,
where the pi are distinct irreducible monic polynomials over K.
Theorem 22.3. (Primary Decomposition Theorem) Let f : E →E be a
linear map on the ﬁnite-dimensional vector space E over the ﬁeld K. Write
the minimal polynomial m of f as
m = pr1
1 · · · prk
k ,
where the pi are distinct irreducible monic polynomials over K, and the ri
are positive integers. Let
Wi = Ker (pri
i (f)),
i = 1, . . . , k.
Then

772
Annihilating Polynomials and the Primary Decomposition
(a) E = W1 ⊕· · · ⊕Wk.
(b) Each Wi is invariant under f.
(c) The minimal polynomial of the restriction f | Wi of f to Wi is pri
i .
Proof. The trick is to construct projections πi using the polynomials prj
j
so that the range of πi is equal to Wi. Let
gi = m/pri
i =
Y
j̸=i
prj
j .
Note that
pri
i gi = m.
Since p1, . . . , pk are irreducible and distinct, they are relatively prime. Then
using Proposition 22.4, it is easy to show that g1, . . . , gk are relatively prime.
Otherwise, some irreducible polynomial p would divide all of g1, . . . , gk, so
by Proposition 22.4 it would be equal to one of the irreducible factors pi.
But that pi is missing from gi, a contradiction.
Therefore, by Proposi-
tion 22.5, there exist some polynomials h1, . . . , hk such that
g1h1 + · · · + gkhk = 1.
Let qi = gihi and let πi = qi(f) = gi(f)hi(f). We have
q1 + · · · + qk = 1,
and since m divides qiqj for i ̸= j, we get
π1 + · · · + πk = id
πiπj = 0,
i ̸= j.
(We implicitly used the fact that if p, q are two polynomials, the linear maps
p(f)◦q(f) and q(f)◦p(f) are the same since p(f) and q(f) are polynomials
in the powers of f, which commute.) Composing the ﬁrst equation with πi
and using the second equation, we get
π2
i = πi.
Therefore, the πi are projections, and E is the direct sum of the images of
the πi. Indeed, every u ∈E can be expressed as
u = π1(u) + · · · + πk(u).
Also, if
π1(u) + · · · + πk(u) = 0,

22.5. The Primary Decomposition Theorem
773
then by applying πi we get
0 = π2
i (u) = πi(u),
i = 1, . . . k.
To ﬁnish proving (a), we need to show that
Wi = Ker (pri
i (f)) = πi(E).
If v ∈πi(E), then v = πi(u) for some u ∈E, so
pri
i (f)(v) = pri
i (f)(πi(u))
= pri
i (f)gi(f)hi(f)(u)
= hi(f)pri
i (f)gi(f)(u)
= hi(f)m(f)(u) = 0,
because m is the minimal polynomial of f. Therefore, v ∈Wi.
Conversely, assume that v ∈Wi = Ker (pri
i (f)). If j ̸= i, then gjhj is
divisible by pri
i , so
gj(f)hj(f)(v) = πj(v) = 0,
j ̸= i.
Then since π1 + · · · + πk = id, we have v = πiv, which shows that v is in
the range of πi. Therefore, Wi = Im(πi), and this ﬁnishes the proof of (a).
If pri
i (f)(u) = 0, then pri
i (f)(f(u)) = f(pri
i (f)(u)) = 0, so (b) holds.
If we write fi = f | Wi, then pri
i (fi) = 0, because pri
i (f) = 0 on Wi (its
kernel). Therefore, the minimal polynomial of fi divides pri
i . Conversely,
let q be any polynomial such that q(fi) = 0 (on Wi). Since m = pri
i gi, the
fact that m(f)(u) = 0 for all u ∈E shows that
pri
i (f)(gi(f)(u)) = 0,
u ∈E,
and thus Im(gi(f)) ⊆Ker (pri
i (f)) = Wi. Consequently, since q(f) is zero
on Wi,
q(f)gi(f) = 0
for all u ∈E.
But then qgi is divisible by the minimal polynomial m = pri
i gi of f, and
since pri
i and gi are relatively prime, by Euclid's proposition, pri
i must divide
q. This ﬁnishes the proof that the minimal polynomial of fi is pri
i , which
is (c).
To best understand the projection constructions of Theorem 22.3, we
provide the following two explicit examples of the primary decomposition
theorem.

774
Annihilating Polynomials and the Primary Decomposition
Example 22.2. First let f : R3 →R3 be deﬁned as f(x, y, z) = (y, −x, z).
In terms of the standard basis f is represented by the 3 × 3 matrix
Xf :=


0 −1 0
1 0 0
0 0 1

.
Then a simple calculation shows that mf(x) = χf(x) = (x2 + 1)(x −1).
Using the notation of the preceding proof set
m = p1p2,
p1 = x2 + 1,
p2 = x −1.
Then
g1 = m
p1
= x −1,
g2 = m
p2
= x2 + 1.
We must ﬁnd h1, h2 ∈R[x] such that g1h1 + g2h2 = 1. In general this is
the hard part of the projection construction. But since we are only working
with two relatively prime polynomials g1, g2, we may apply the Euclidean
algorithm to discover that
−x + 1
2
(x −1) + 1
2(x2 + 1) = 1,
where h1 = −x+1
2
while h2 = 1
2. By deﬁnition
π1 = g1(f)h1(f) = −1
2(Xf −id)(Xf + id) = −1
2(X2
f −id) =


1 0 0
0 1 0
0 0 0

,
and
π2 = g2(f)h2(f) = 1
2(X2
f + id) =


0 0 0
0 0 0
0 0 1

.
Then R3 = W1 ⊕W2, where
W1 = π1(R3) = Ker (p1(Xf)) = Ker (X2
f + id)
= Ker


0 0 0
0 0 0
0 0 1

= {(x, y, 0) ∈R3},
W2 = π2(R3) = Ker (p2(Xf)) = Ker (Xf −id)
= Ker


−1 −1 0
1 −1 0
0
0 0

= {(0, 0, z) ∈R3}.

22.5. The Primary Decomposition Theorem
775
Example 22.3. For our second example of the primary decomposition
theorem let f : R3 →R3 be deﬁned as f(x, y, z) = (y, −x + z, −y), with
standard matrix representation Xf =


0 −1 0
1 0 −1
0 1
0

. A simple calculation
shows that mf(x) = χf(x) = x(x2 + 2). Set
p1 = x2 + 2,
p2 = x,
g1 = mf
p1
= x,
g2 = mf
p2
= x2 + 2.
Since gcd(g1, g2) = 1, we use the Euclidean algorithm to ﬁnd
h1 = −1
2x,
h2 = 1
2,
such that g1h1 + g2h2 = 1. Then
π1 = g1(f)h1(f) = −1
2X2
f =


1
2
0 −1
2
0 1 0
−1
2 0
1
2

,
while
π2 = g2(f)h2(f) = 1
2(X2
f + 2id) =


1
2 0 1
2
0 0 0
1
2 0 1
2

.
Although it is not entirely obvious, π1 and π2 are indeed projections since
π2
1 =


1
2
0 −1
2
0 1 0
−1
2 0
1
2




1
2
0 −1
2
0 1 0
−1
2 0
1
2

=


1
2
0 −1
2
0 1 0
−1
2 0
1
2

= π1,
and
π2
2 =


1
2 0 1
2
0 0 0
1
2 0 1
2




1
2 0 1
2
0 0 0
1
2 0 1
2

=


1
2 0 1
2
0 0 0
1
2 0 1
2

= π2.
Furthermore observe that π1 + π2 = id. The primary decomposition theo-
rem implies that R3 = W1 ⊕W2 where
W1 = π1(R3) = Ker (p1(f)) = Ker (X2 + 2)
= Ker


1 0 1
0 0 0
1 0 1

= span{(0, 1, 0), (1, 0, −1)},
W2 = π2(R3) = Ker (p2(f)) = Ker (X) = span{(1, 0, 1)}.
See Figure 22.1.

776
Annihilating Polynomials and the Primary Decomposition
Fig. 22.1
The direct sum decomposition of R3 = W1 ⊕W2 where W1 is the plane
x + z = 0 and W2 is line t(1, 0, 1). The spanning vectors of W1 are in blue.
If all the eigenvalues of f belong to the ﬁeld K, we obtain the following
result.
Theorem
22.4. (Primary
Decomposition
Theorem,
Version
2)
Let
f : E →E be a linear map on the ﬁnite-dimensional vector space E over
the ﬁeld K. If all the eigenvalues λ1, . . . , λk of f belong to K, write
m = (X −λ1)r1 · · · (X −λk)rk
for the minimal polynomial of f,
χf = (X −λ1)n1 · · · (X −λk)nk
for the characteristic polynomial of f, with 1 ≤ri ≤ni, and let
Wi = Ker (λiid −f)ri,
i = 1, . . . , k.
Then
(a) E = W1 ⊕· · · ⊕Wk.
(b) Each Wi is invariant under f.
(c) dim(Wi) = ni.

22.6. Jordan Decomposition
777
(d) The minimal polynomial of the restriction f | Wi of f to Wi is (X −
λi)ri.
Proof. Parts (a), (b) and (d) have already been proven in Theorem 22.3,
so it remains to prove (c). Since Wi is invariant under f, let fi be the
restriction of f to Wi. The characteristic polynomial χfi of fi divides χ(f),
and since χ(f) has all its roots in K, so does χi(f). By Theorem 14.1, there
is a basis of Wi in which fi is represented by an upper triangular matrix,
and since (λiid −f)ri = 0, the diagonal entries of this matrix are equal to
λi. Consequently,
χfi = (X −λi)dim(Wi),
and since χfi divides χ(f), we conclude hat
dim(Wi) ≤ni,
i = 1, . . . , k.
Because E is the direct sum of the Wi, we have dim(W1)+· · ·+dim(Wk) =
n, and since n1 + · · · + nk = n, we must have
dim(Wi) = ni,
i = 1, . . . , k,
proving (c).
Deﬁnition 22.10. If λ ∈K is an eigenvalue of f, we deﬁne a generalized
eigenvector of f as a nonzero vector u ∈E such that
(λid −f)r(u) = 0,
for some r ≥1.
The index of λ is deﬁned as the smallest r ≥1 such that
Ker (λid −f)r = Ker (λid −f)r+1.
It is clear that Ker (λid −f)i ⊆Ker (λid −f)i+1 for all i ≥1.
By
Theorem 22.4(d), if λ = λi, the index of λi is equal to ri.
22.6
Jordan Decomposition
Recall that a linear map g: E →E is said to be nilpotent if there is some
positive integer r such that gr = 0. Another important consequence of
Theorem 22.4 is that f can be written as the sum of a diagonalizable and
a nilpotent linear map (which commute).
For example f : R2 →R2 be
the R-linear map f(x, y) = (x, x + y) with standard matrix representation
Xf =
1 1
0 1

. A basic calculation shows that mf(x) = χf(x) = (x −1)2.

778
Annihilating Polynomials and the Primary Decomposition
By Theorem 22.2 we know that f is not diagonalizable over R. But since
the eigenvalue λ1 = 1 of f does belong to R, we may use the projection
construction inherent within Theorem 22.4 to write f = D + N, where D
is a diagonalizable linear map and N is a nilpotent linear map. The proof
of Theorem 22.3 implies that
pr1
1 = (x −1)2,
g1 = 1 = h1,
π1 = g1(f)h1(f) = id.
Then
D = λ1π1 = id,
N = f −D = f(x, y) −id(x, y) = (x, x + y) −(x, y) = (0, y),
which is equivalent to the matrix decomposition
Xf =
1 1
0 1

=
1 0
0 1

+
0 1
0 0

.
This example suggests that the diagonal summand of f is related to the
projection constructions associated with the proof of the primary decom-
position theorem. If we write
D = λ1π1 + · · · + λkπk,
where πi is the projection from E onto the subspace Wi deﬁned in the proof
of Theorem 22.3, since
π1 + · · · + πk = id,
we have
f = fπ1 + · · · + fπk,
and so we get
N = f −D = (f −λ1id)π1 + · · · + (f −λkid)πk.
We claim that N = f −D is a nilpotent operator. Since by construction
the πi are polynomials in f, they commute with f, using the properties of
the πi, we get
N r = (f −λ1id)rπ1 + · · · + (f −λkid)rπk.
Therefore, if r = max{ri}, we have (f −λkid)r = 0 for i = 1, . . . , k, which
implies that
N r = 0.
It remains to show that D is diagonalizable. Since N is a polynomial in
f, it commutes with f, and thus with D. From
D = λ1π1 + · · · + λkπk,

22.6. Jordan Decomposition
779
and
π1 + · · · + πk = id,
we see that
D −λiid = λ1π1 + · · · + λkπk −λi(π1 + · · · + πk)
= (λ1 −λi)π1 + · · · + (λi−1 −λi)πi−1 + (λi+1 −λi)πi+1
+ · · · + (λk −λi)πk.
Since the projections πj with j ̸= i vanish on Wi, the above equation implies
that D −λiid vanishes on Wi and that (D −λjid)(Wi) ⊆Wi, and thus that
the minimal polynomial of D is
(X −λ1) · · · (X −λk).
Since the λi are distinct, by Theorem 22.2, the linear map D is diagonaliz-
able.
In summary we have shown that when all the eigenvalues of f belong
to K, there exist a diagonalizable linear map D and a nilpotent linear map
N such that
f = D + N
DN = ND,
and N and D are polynomials in f.
Deﬁnition 22.11.
A decomposition of f as f = D + N as above is called
a Jordan decomposition.
In fact, we can prove more: the maps D and N are uniquely determined
by f.
Theorem 22.5. (Jordan Decomposition) Let f : E →E be a linear map on
the ﬁnite-dimensional vector space E over the ﬁeld K. If all the eigenvalues
λ1, . . . , λk of f belong to K, then there exist a diagonalizable linear map D
and a nilpotent linear map N such that
f = D + N
DN = ND.
Furthermore, D and N are uniquely determined by the above equations and
they are polynomials in f.

780
Annihilating Polynomials and the Primary Decomposition
Proof. We already proved the existence part. Suppose we also have f =
D′ + N ′, with D′N ′ = N ′D′, where D′ is diagonalizable, N ′ is nilpotent,
and both are polynomials in f. We need to prove that D = D′ and N = N ′.
Since D′ and N ′ commute with one another and f = D′ + N ′, we see
that D′ and N ′ commute with f.
Then D′ and N ′ commute with any
polynomial in f; hence they commute with D and N. From
D + N = D′ + N ′,
we get
D −D′ = N ′ −N,
and D, D′, N, N ′ commute with one another. Since D and D′ are both
diagonalizable and commute, by Proposition 22.11, they are simultaneously
diagonalizable, so D −D′ is diagonalizable. Since N and N ′ commute, by
the binomial formula, for any r ≥1,
(N ′ −N)r =
r
X
j=0
(−1)j
r
j

(N ′)r−jN j.
Since both N and N ′ are nilpotent, we have N r1 = 0 and (N ′)r2 = 0,
for some r1, r2 > 0, so for r ≥r1 + r2, the right-hand side of the above
expression is zero, which shows that N ′ −N is nilpotent. (In fact, it is
easy that r1 = r2 = n works.)
It follows that D −D′ = N ′ −N is
both diagonalizable and nilpotent. Clearly, the minimal polynomial of a
nilpotent linear map is of the form Xr for some r > 0 (and r ≤dim(E)).
But D −D′ is diagonalizable, so its minimal polynomial has simple roots,
which means that r = 1. Therefore, the minimal polynomial of D −D′ is
X, which says that D −D′ = 0, and then N = N ′.
If K is an algebraically closed ﬁeld, then Theorem 22.5 holds. This is
the case when K = C. This theorem reduces the study of linear maps (from
E to itself) to the study of nilpotent operators. There is a special normal
form for such operators which is discussed in the next section.
22.7
Nilpotent Linear Maps and Jordan Form
This section is devoted to a normal form for nilpotent maps. We follow
Godement's exposition [Godement (1963)]. Let f : E →E be a nilpotent
linear map on a ﬁnite-dimensional vector space over a ﬁeld K, and assume
that f is not the zero map.
There is a smallest positive integer r ≥1
such f r ̸= 0 and f r+1 = 0.
Clearly, the polynomial Xr+1 annihilates

22.7. Nilpotent Linear Maps and Jordan Form
781
f, and it is the minimal polynomial of f since f r ̸= 0.
It follows that
r + 1 ≤n = dim(E). Let us deﬁne the subspaces Ni by
Ni = Ker (f i),
i ≥0.
Note that N0 = (0), N1 = Ker (f), and Nr+1 = E. Also, it is obvious that
Ni ⊆Ni+1,
i ≥0.
Proposition 22.14. Given a nilpotent linear map f with f r ̸= 0 and
f r+1 = 0 as above, the inclusions in the following sequence are strict:
(0) = N0 ⊂N1 ⊂· · · ⊂Nr ⊂Nr+1 = E.
Proof. We proceed by contradiction. Assume that Ni = Ni+1 for some i
with 0 ≤i ≤r. Since f r+1 = 0, for every u ∈E, we have
0 = f r+1(u) = f i+1(f r−i(u)),
which shows that f r−i(u) ∈Ni+1. Since Ni = Ni+1, we get f r−i(u) ∈Ni,
and thus f r(u) = 0. Since this holds for all u ∈E, we see that f r = 0, a
contradiction.
Proposition 22.15. Given a nilpotent linear map f with f r ̸= 0 and
f r+1 = 0, for any integer i with 1 ≤i ≤r, for any subspace U of E,
if U ∩Ni = (0), then f(U) ∩Ni−1 = (0), and the restriction of f to U is
an isomorphism onto f(U).
Proof. Pick v ∈f(U) ∩Ni−1. We have v = f(u) for some u ∈U and
f i−1(v) = 0, which means that f i(u) = 0. Then u ∈U ∩Ni, so u = 0
since U ∩Ni = (0), and v = f(u) = 0. Therefore, f(U) ∩Ni−1 = (0). The
restriction of f to U is obviously surjective on f(U). Suppose that f(u) = 0
for some u ∈U. Then u ∈U ∩N1 ⊆U ∩Ni = (0) (since i ≥1), so u = 0,
which proves that f is also injective on U.
Proposition 22.16. Given a nilpotent linear map f with f r ̸= 0 and
f r+1 = 0, there exists a sequence of subspace U1, . . . , Ur+1 of E with the
following properties:
(1) Ni = Ni−1 ⊕Ui, for i = 1, . . . , r + 1.
(2) We have f(Ui) ⊆Ui−1, and the restriction of f to Ui is an injection,
for i = 2, . . . , r + 1.
See Figure 22.2.

782
Annihilating Polynomials and the Primary Decomposition
N r
U r+1
f(U r+1)
f
0
E = 
!
!
N r
U r+1
Nr-1
Ur
f(U     )
r+1
f(U   )
r
f
0
Nr-1
=
N r
Ur
f(U   )
r
Nr-2
Ur-1
Nr-1
Ur-1
Nr-2
=
!
Ur-1
f(
)
f
0
Fig. 22.2
A schematic illustration of Ni = Ni−1 ⊕Ui with f(Ui) ⊆Ui−1 for i =
r + 1, r, r −1.
Proof. We proceed inductively, by deﬁning the sequence Ur+1, Ur, . . . , U1.
We pick Ur+1 to be any supplement of Nr in Nr+1 = E, so that
E = Nr+1 = Nr ⊕Ur+1.
Since f r+1 = 0 and Nr = Ker (f r), we have f(Ur+1) ⊆Nr, and by Proposi-
tion 22.15, as Ur+1 ∩Nr = (0), we have f(Ur+1) ∩Nr−1 = (0). As a conse-
quence, we can pick a supplement Ur of Nr−1 in Nr so that f(Ur+1) ⊆Ur.

22.7. Nilpotent Linear Maps and Jordan Form
783
We have
Nr = Nr−1 ⊕Ur
and
f(Ur+1) ⊆Ur.
By Proposition 22.15, f is an injection from Ur+1 to Ur. Assume inductively
that Ur+1, . . . , Ui have been deﬁned for i ≥2 and that they satisfy (1) and
(2). Since
Ni = Ni−1 ⊕Ui,
we have Ui ⊆Ni, so f i−1(f(Ui)) = f i(Ui) = (0), which implies that
f(Ui) ⊆Ni−1.
Also, since Ui ∩Ni−1 = (0), by Proposition 22.15, we
have f(Ui)∩Ni−2 = (0). It follows that there is a supplement Ui−1 of Ni−2
in Ni−1 that contains f(Ui). We have
Ni−1 = Ni−2 ⊕Ui−1
and
f(Ui) ⊆Ui−1.
The fact that f is an injection from Ui into Ui−1 follows from Proposi-
tion 22.15. Therefore, the induction step is proven. The construction stops
when i = 1.
Because N0 = (0) and Nr+1 = E, we see that E is the direct sum of the
Ui:
E = U1 ⊕· · · ⊕Ur+1,
with f(Ui) ⊆Ui−1, and f an injection from Ui to Ui−1, for i = r + 1, . . . , 2.
By a clever choice of bases in the Ui, we obtain the following nice theorem.
Theorem 22.6. For any nilpotent linear map f : E →E on a ﬁnite-
dimensional vector space E of dimension n over a ﬁeld K, there is a basis
of E such that the matrix N of f is of the form
N =







0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
... ...
...
...
... ...
0 0 0 · · · 0 νn
0 0 0 · · · 0 0







,
where νi = 1 or νi = 0.
Proof. First apply Proposition 22.16 to obtain a direct sum E = Lr+1
i=1 Ui.
Then we deﬁne a basis of E inductively as follows. First we choose a basis
er+1
1
, . . . , er+1
nr+1

784
Annihilating Polynomials and the Primary Decomposition
of Ur+1. Next, for i = r + 1, . . . , 2, given the basis
ei
1, . . . , ei
ni
of Ui,
since f
is injective on Ui and f(Ui)
⊆
Ui−1,
the vectors
f(ei
1), . . . , f(ei
ni) are linearly independent, so we deﬁne a basis of Ui−1 by
completing f(ei
1), . . . , f(ei
ni) to a basis in Ui−1:
ei−1
1
, . . . , ei−1
ni , ei−1
ni+1, . . . , ei−1
ni−1
with
ei−1
j
= f(ei
j),
j = 1 . . . , ni.
Since U1 = N1 = Ker (f), we have
f(e1
j) = 0,
j = 1, . . . , n1.
These basis vectors can be arranged as the rows of the following matrix:

















er+1
1
· · · er+1
nr+1
...
...
er
1
· · · er
nr+1 er
nr+1+1 · · · er
nr
...
...
...
...
er−1
1
· · · er−1
nr+1 er−1
nr+1+1 · · · er−1
nr
er−1
nr+1 cdots er−1
nr−1
...
...
...
...
...
...
...
...
...
...
...
...
e1
1
· · · e1
nr+1 e1
nr+1+1 · · · e1
nr e1
nr+1
· · ·
e1
nr−1 · · · · · · e1
n1

















Finally, we deﬁne the basis (e1, . . . , en) by listing each column of the
above matrix from the bottom-up, starting with column one, then column
two, etc. This means that we list the vectors ei
j in the following order:
For j = 1, . . . , nr+1, list e1
j, . . . , er+1
j
;
In general, for i = r, . . . , 1,
for j = ni+1 + 1, . . . , ni, list e1
j, . . . , ei
j.
Then because f(e1
j) = 0 and ei−1
j
= f(ei
j) for i ≥2, either
f(ei) = 0
or
f(ei) = ei−1,
which proves the theorem.

22.7. Nilpotent Linear Maps and Jordan Form
785
As an application of Theorem 22.6, we obtain the Jordan form of a
linear map.
Deﬁnition 22.12. A Jordan block is an r × r matrix Jr(λ), of the form
Jr(λ) =








λ 1 0 · · · 0
0 λ 1 · · · 0
... ... ... ... ...
0 0 0 ... 1
0 0 0 · · · λ








,
where λ ∈K, with J1(λ) = (λ) if r = 1. A Jordan matrix, J, is an n × n
block diagonal matrix of the form
J =



Jr1(λ1) · · ·
0
...
...
...
0
· · · Jrm(λm)


,
where each Jrk(λk) is a Jordan block associated with some λk ∈K, and
with r1 + · · · + rm = n.
To simplify notation, we often write J(λ) for Jr(λ). Here is an example
of a Jordan matrix with four blocks:
J =













λ 1 0 0 0 0 0 0
0 λ 1 0 0 0 0 0
0 0 λ 0 0 0 0 0
0 0 0 λ 1 0 0 0
0 0 0 0 λ 0 0 0
0 0 0 0 0 λ 0 0
0 0 0 0 0 0 µ 1
0 0 0 0 0 0 0 µ













.
Theorem 22.7. (Jordan form) Let E be a vector space of dimension n
over a ﬁeld K and let f : E →E be a linear map. The following properties
are equivalent:
(1) The eigenvalues of f all belong to K (i.e. the roots of the characteristic
polynomial χf all belong to K).
(2) There is a basis of E in which the matrix of f is a Jordan matrix.
Proof. Assume (1). First we apply Theorem 22.4, and we get a direct
sum E = Lk
j=1 Wk, such that the restriction of gi = f −λjid to Wi is

786
Annihilating Polynomials and the Primary Decomposition
nilpotent. By Theorem 22.6, there is a basis of Wi such that the matrix of
the restriction of gi is of the form
Gi =







0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
... ...
...
...
...
...
0 0 0 · · · 0 νni
0 0 0 · · · 0 0







,
where νi = 1 or νi = 0. Furthermore, over any basis, λiid is represented by
the diagonal matrix Di with λi on the diagonal. Then it is clear that we
can split Di + Gi into Jordan blocks by forming a Jordan block for every
uninterrupted chain of 1s. By putting the bases of the Wi together, we
obtain a matrix in Jordan form for f.
Now assume (2).
If f can be represented by a Jordan matrix, it is
obvious that the diagonal entries are the eigenvalues of f, so they all belong
to K.
Observe that Theorem 22.7 applies if K = C. It turns out that there are
uniqueness properties of the Jordan blocks but more machinery is needed
to prove this result.
If a complex n × n matrix A is expressed in terms of its Jordan decom-
position as A = D + N, since D and N commute, by Proposition 8.16, the
exponential of A is given by
eA = eDeN,
and since N is an n × n nilpotent matrix, N n−1 = 0, so we obtain
eA = eD

I + N
1! + N 2
2! + · · · + N n−1
(n −1)!

.
In particular, the above applies if A is a Jordan matrix. This fact can
be used to solve (at least in theory) systems of ﬁrst-order linear diﬀerential
equations. Such systems are of the form
dX
dt = AX,
(22.13)
where A is an n × n matrix and X is an n-dimensional vector of functions
of the parameter t.
It can be shown that the columns of the matrix etA form a basis of
the vector space of solutions of the system of linear diﬀerential equations
(22.13); see Artin [Artin (1991)] (Chapter 4). Furthermore, for any matrix

22.7. Nilpotent Linear Maps and Jordan Form
787
B and any invertible matrix P, if A = PBP −1, then the system (∗) is
equivalent to
P −1 dX
dt = BP −1X,
so if we make the change of variable Y = P −1X, we obtain the system
dY
dt = BY.
(22.14)
Consequently, if B is such that the exponential etB can be easily computed,
we obtain an explicit solution Y of (22.14), and X = PY is an explicit
solution of (22.13). This is the case when B is a Jordan form of A. In this
case, it suﬃces to consider the Jordan blocks of B. Then we have
Jr(λ) = λIr +








0 1 0 · · · 0
0 0 1 · · · 0
...
... ... ... ...
0 0 0 ... 1
0 0 0 · · · 0








= λIr + N,
and the powers N k are easily computed.
For example, if
B =


3 1 0
0 3 1
0 0 3

= 3I3 +


0 1 0
0 0 1
0 0 0


we obtain
tB = t


3 1 0
0 3 1
0 0 3

= 3tI3 +


0 t 0
0 0 t
0 0 0


and so
etB =


e3t 0
0
0 e3t 0
0
0 e3t




1 t (1/2)t2
0 1
t
0 0
1

=


e3t te3t (1/2)t2e3t
0
e3t
te3t
0
0
e3t

.
The columns of etB form a basis of the space of solutions of the system of
linear diﬀerential equations
dY1
dt = 3Y1 + Y2
dY2
dt = 3Y2 + Y3
dY3
dt = 3Y3,

788
Annihilating Polynomials and the Primary Decomposition
in matrix form,




dY1
dt
dY2
dt
dY3
dt



=


3 1 0
0 3 1
0 0 3




Y1
Y2
Y3

.
Explicitly, the general solution of the above system is


Y1
Y2
Y3

= c1


e3t
0
0

+ c2


te3t
e3t
0

+ c3


(1/2)t2e3t
te3t
e3t

,
with c1, c2, c3 ∈R.
Solving systems of ﬁrst-order linear diﬀerential equations is discussed in
Artin [Artin (1991)] and more extensively in Hirsh and Smale [Hirsh and
Smale (1974)].
22.8
Summary
The main concepts and results of this chapter are listed below:
• Ideals, principal ideals, greatest common divisors.
• Monic polynomial, irreducible polynomial, relatively prime poly-
nomials.
• Annihilator of a linear map.
• Minimal polynomial of a linear map.
• Invariant subspace.
• f-conductor of u into W; conductor of u into W.
• Diagonalizable linear maps.
• Commuting families of linear maps.
• Primary decomposition.
• Generalized eigenvectors.
• Nilpotent linear map.
• Normal form of a nilpotent linear map.
• Jordan decomposition.
• Jordan block.
• Jordan matrix.
• Jordan normal form.
• Systems of ﬁrst-order linear diﬀerential equations.

22.9. Problems
789
22.9
Problems
Problem 22.1. Prove that the minimal monic polynomial of Proposi-
tion 22.1 is unique.
Problem 22.2. Given a linear map f : E →E, prove that the set Ann(f)
of polynomials that annihilate f is an ideal.
Problem 22.3. Provide the details of Proposition 22.8.
Problem 22.4. Prove that the f-conductor Sf(u, W) is an ideal in K[X]
(Proposition 22.9).
Problem 22.5. Prove that the polynomials g1, . . . , gk used in the proof of
Theorem 22.3 are relatively prime.
Problem 22.6. Find the minimal polynomial of the matrix
A =


6 −3 −2
4 −1 −2
10 −5 −3

.
Problem 22.7. Find the Jordan decomposition of the matrix
A =


3 1 −1
2 2 −1
2 2 0

.
Problem 22.8. Let f : E →E be a linear map on a ﬁnite-dimensional
vector space. Prove that if f has rank 1, then either f is diagonalizable or
f is nilpotent but not both.
Problem 22.9. Find the Jordan form of the matrix
A =




0 1 0 0
0 0 2 0
0 0 0 3
0 0 0 0



.
Problem 22.10. Let N be a 3 × 3 nilpotent matrix over C. Prove that
the matrix
A = I + (1/2)N −(1/8)N 2 satisﬁes the equation
A2 = I + N.
In other words, A is a square root of I + N.
Generalize the above fact to any n × n nilpotent matrix N over C using
the binomial series for (1 + t)1/2.

790
Annihilating Polynomials and the Primary Decomposition
Problem 22.11. Let K be an algebraically closed ﬁeld (for example, K =
C). Prove that every 4 × 4 matrix is similar to a Jordan matrix of the
following form:




λ1 0
0
0
0 λ2 0
0
0
0 λ3 0
0
0
0 λ4



,




λ 1 0
0
0 λ 0
0
0 0 λ3 0
0 0 0 λ4



,




λ 1 0 0
0 λ 1 0
0 0 λ 0
0 0 0 λ4



,




λ 1 0 0
0 λ 0 0
0 0 µ 1
0 0 0 µ



,




λ 1 0 0
0 λ 1 0
0 0 λ 1
0 0 0 λ



.
Problem 22.12. In this problem the ﬁeld K is of characteristic 0. Consider
an (r × r) Jordan block
Jr(λ) =








λ 1 0 · · · 0
0 λ 1 · · · 0
... ... ... ... ...
0 0 0 ... 1
0 0 0 · · · λ








.
Prove that for any polynomial f(X), we have
f(Jr(λ)) =








f(λ) f1(λ) f2(λ) · · · fr−1(λ)
0
f(λ) f1(λ) · · · fr−2(λ)
...
...
...
...
...
0
0
0
...
f1(λ)
0
0
0
· · ·
f(λ)








,
where
fk(X) = f (k)(X)
k!
,
and f (k)(X) is the kth derivative of f(X).

Bibliography
Andrews, G. E., Askey, R., and Roy, R. (2000). Special Functions, 1st edn. (Cam-
bridge University Press).
Artin, E. (1957). Geometric Algebra, 1st edn. (Wiley Interscience).
Artin, M. (1991). Algebra, 1st edn. (Prentice Hall).
Axler, S. (2004). Linear Algebra Done Right, 2nd edn., Undergraduate Texts in
Mathematics (Springer Verlag).
Berger, M. (1990a). G´eom´etrie 1 (Nathan), english edition: Geometry 1, Univer-
sitext, Springer Verlag.
Berger, M. (1990b). G´eom´etrie 2 (Nathan), english edition: Geometry 2, Univer-
sitext, Springer Verlag.
Bertin, J. (1981). Alg`ebre lin´eaire et g´eom´etrie classique, 1st edn. (Masson).
Bourbaki, N. (1970). Alg`ebre, Chapitres 1-3, El´ements de Math´ematiques (Her-
mann).
Bourbaki, N. (1981a). Alg`ebre, Chapitres 4-7, El´ements de Math´ematiques (Mas-
son).
Bourbaki, N. (1981b). Espaces Vectoriels Topologiques, El´ements de Math´e-
matiques (Masson).
Boyd, S. and Vandenberghe, L. (2004). Convex Optimization, 1st edn. (Cambridge
University Press).
Cagnac, G., Ramis, E., and Commeau, J. (1965). Math´ematiques Sp´eciales,
Vol. 3, G´eom´etrie (Masson).
Chung, F. R. K. (1997). Spectral Graph Theory, Regional Conference Series in
Mathematics, Vol. 92, 1st edn. (AMS).
Ciarlet, P. (1989). Introduction to Numerical Matrix Analysis and Optimization,
1st edn. (Cambridge University Press), french edition: Masson, 1994.
Coxeter, H. (1989). Introduction to Geometry, 2nd edn. (Wiley).
Demmel, J. W. (1997). Applied Numerical Linear Algebra, 1st edn. (SIAM Pub-
lications).
Dieudonn´e, J. (1965). Alg`ebre Lin´eaire et G´eom´etrie El´ementaire, 2nd edn. (Her-
mann).
Dixmier, J. (1984). General Topology, 1st edn., UTM (Springer Verlag).
Dummit, D. S. and Foote, R. M. (1999). Abstract Algebra, 2nd edn. (Wiley).
791

792
Bibliography
Epstein, C. L. (2007). Introduction to the Mathematics of Medical Imaging, 2nd
edn. (SIAM).
Forsyth, D. A. and Ponce, J. (2002). Computer Vision: A Modern Approach, 1st
edn. (Prentice Hall).
Fresnel, J. (1998). M´ethodes Modernes En G´eom´etrie, 1st edn. (Hermann).
Gallier, J. H. (2011a). Discrete Mathematics, 1st edn., Universitext (Springer
Verlag).
Gallier, J. H. (2011b). Geometric Methods and Applications, For Computer Sci-
ence and Engineering, 2nd edn., TAM, Vol. 38 (Springer).
Gallier, J. H. (2019). Spectral Graph Theory of Unsigned and Signed Graphs.
Applications to Graph Clustering: A survey, Tech. rep., University of Penn-
sylvania, http://www.cis.upenn.edu/ jean/spectral-graph-notes.pdf.
Godement, R. (1958). Topologie Alg´ebrique et Th´eorie des Faisceaux, 1st edn.
(Hermann), second Printing, 1998.
Godement, R. (1963). Cours d'Alg`ebre, 1st edn. (Hermann).
Godsil, C. and Royle, G. (2001). Algebraic Graph Theory, 1st edn., GTM No. 207
(Springer Verlag).
Golub, G. H. and Uhlig, F. (2009). The QR algorithm: 50 years later its genesis
by john francis and vera kublanovskaya and subsequent developments, IMA
Journal of Numerical Analysis 29, pp. 467-485.
Golub, G. H. and Van Loan, C. F. (1996). Matrix Computations, 3rd edn. (The
Johns Hopkins University Press).
Hadamard, J. (1947). Le¸cons de G´eom´etrie El´ementaire. I G´eom´etrie Plane, thir-
teenth edn. (Armand Colin).
Hadamard, J. (1949). Le¸cons de G´eom´etrie El´ementaire. II G´eom´etrie dans
l'Espace, eighth edn. (Armand Colin).
Halko, N., Martinsson, P., and Tropp, J. A. (2011). Finding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix de-
compositions, SIAM Review 53(2), pp. 217-288.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical
Learning: Data Mining, Inference, and Prediction, 2nd edn. (Springer).
Hirsh, M. W. and Smale, S. (1974). Diﬀerential Equations, Dynamical Systems
and Linear Algebra, 1st edn. (Academic Press).
Horn, R. A. and Johnson, C. R. (1990). Matrix Analysis, 1st edn. (Cambridge
University Press).
Horn, R. A. and Johnson, C. R. (1994). Topics in Matrix Analysis, 1st edn.
(Cambridge University Press).
Kenneth, H. and Ray, K. (1971). Linear Algebra, 2nd edn. (Prentice Hall).
Kincaid, D. and Cheney, W. (1996). Numerical Analysis, 2nd edn. (Brooks/Cole
Publishing).
Kumpel, P. G. and Thorpe, J. A. (1983). Linear Algebra, 1st edn. (W. B. Saun-
ders).
Lang, S. (1993). Algebra, 3rd edn. (Addison Wesley).
Lang, S. (1996). Real and Functional Analysis, 3rd edn., GTM 142 (Springer
Verlag).
Lang, S. (1997). Undergraduate Analysis, 2nd edn., UTM (Springer Verlag).

Bibliography
793
Lax, P. (2007). Linear Algebra and Its Applications, 2nd edn. (Wiley).
Lebedev, N. N. (1972). Special Functions and Their Applications, 1st edn.
(Dover).
Mac Lane, S. and Birkhoﬀ, G. (1967). Algebra, 1st edn. (Macmillan).
Marsden, J. E. and Hughes, T. J. (1994). Mathematical Foundations of Elasticity,
1st edn. (Dover).
Meyer, C. D. (2000). Matrix Analysis and Applied Linear Algebra, 1st edn.
(SIAM).
O'Rourke, J. (1998). Computational Geometry in C, 2nd edn. (Cambridge Uni-
versity Press).
Parlett, B. N. (1997). The Symmetric Eigenvalue Problem, 1st edn. (SIAM Pub-
lications).
Pedoe, D. (1988). Geometry, A comprehensive Course, 1st edn. (Dover).
Rouch´e, E. and de Comberousse, C. (1900). Trait´e de G´eom´etrie, seventh edn.
(Gauthier-Villars).
Sansone, G. (1991). Orthogonal Functions, 1st edn. (Dover).
Schwartz, L. (1991). Analyse I. Th´eorie des Ensembles et Topologie, Collection
Enseignement des Sciences (Hermann).
Schwartz, L. (1992). Analyse II. Calcul Diﬀ´erentiel et Equations Diﬀ´erentielles,
Collection Enseignement des Sciences (Hermann).
Seberry, J., Wysocki, B. J., and Wysocki, T. A. (2005). On some applications of
Hadamard matrices, Metrika 62, pp. 221-239.
Serre, D. (2010). Matrices, Theory and Applications, 2nd edn., GTM No. 216
(Springer Verlag).
Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation, Transac-
tions on Pattern Analysis and Machine Intelligence 22(8), pp. 888-905.
Snapper, E. and Troyer, R. J. (1989). Metric Aﬃne Geometry, 1st edn. (Dover).
Spielman, D. (2012). Spectral graph theory, in U. Naumannn and O. Schenk
(eds.), Combinatorial Scientiﬁc Computing (CRC Press).
Stewart, G. (1993). On the early history of the singular value decomposition,
SIAM review 35(4), pp. 551-566.
Stollnitz, E. J., DeRose, T. D., and Salesin, D. H. (1996). Wavelets for Computer
Graphics Theory and Applications, 1st edn. (Morgan Kaufmann).
Strang, G. (1986). Introduction to Applied Mathematics, 1st edn. (Wellesley-
Cambridge Press).
Strang, G. (1988). Linear Algebra and its Applications, 3rd edn. (Saunders HBJ).
Strang, G. (2019). Linear Algebra and Learning from Data, 1st edn. (Wellesley-
Cambridge Press).
Strang, G. and Truong, N. (1997). Wavelets and Filter Banks, 2nd edn.
(Wellesley-Cambridge Press).
Tisseron, C. (1994). G´eom´etries aﬃnes, projectives, et euclidiennes, 1st edn. (Her-
mann).
Trefethen, L. and Bau III, D. (1997). Numerical Linear Algebra, 1st edn. (SIAM
Publications).
Tropp, J. A. (2011). Improved analysis of the subsampled Hadamard transform,
Advances in Adaptive Data Analysis 3, pp. 115-126.

794
Bibliography
Van Der Waerden, B. (1973). Algebra, Vol. 1, seventh edn. (Ungar).
van Lint, J. and Wilson, R. (2001). A Course in Combinatorics, 2nd edn. (Cam-
bridge University Press).
Veblen, O. and Young, J. W. (1946). Projective Geometry, Vol. 2, 1st edn. (Ginn).
Watkins, D. S. (1982). Understanding the QR algorithm, SIAM Review 24(4),
pp. 447-440.
Watkins, D. S. (2008). The QR algorithm revisited, SIAM Review 50(1), pp. 133-
145.
Yu, S. X. (2003). Computational Models of Perceptual Organization, Ph.D. thesis,
Carnegie Mellon University, Pittsburgh, PA 15213, USA, dissertation.
Yu, S. X. and Shi, J. (2003). Multiclass spectral clustering, in 9th International
Conference on Computer Vision, Nice, France, October 13-16 (IEEE).

Index
(k + 1)th principal component
of X, 740
3-sphere S3, 577
C0-continuity, 201
C2-continuity, 201
I-indexed family, 26
I-sequence, 27
subfamily, 33
I-sequence, 27
K-vector space, 24
LDU-factorization, 215
LU-factorization, 212, 214
QR algorithm, 627
deﬂation, 642
double shift, 641, 644
Francis shift, 645
implicit Q theorem, 646
implicit shift, 641
bulge chasing, 641
shift, 641, 643
Wilkinson shift, 644
QR-decomposition, 441, 508
Hom(E, F), 60
ℓ2-norm, 10
SO(2), 583
SU(2), 566
adjoint representation, 567, 568
U(1), 583
so(n), 439
su(2), 567
inner product, 580
f-conductor of u into W, 765
k-plane, 46
kth elementary symmetric
polynomial, 535
n-linear form, see multilinear form
n-linear map, see multilinear map
(real) projective space RP3, 577
(upper) Hessenberg matrix, 635
reduced, 638
unreduced, 638
"musical map", 420
abelian group, 19
adjacency matrix, 659, 666
diﬀusion operator, 667
adjoint map, 422, 500
adjoint of f, 422, 424, 501
adjoint of a matrix, 506
adjugate, 177
aﬃne combination, 143
aﬃne frame, 149
aﬃne map, 146, 435
unique linear map, 146
aﬃne space, 147
free vectors, 147
points, 147
translations, 147
algebraic varieties, 379
algebraically closed ﬁeld, 543
alternating multilinear map, 165
annihilating polynomials, 755
795

796
Index
annihilator
linear map, 762
of a polynomial, 755
applications
of Euclidean geometry, 447
Arnoldi iteration, 648
breakdown, 648
Rayleigh-Ritz method, 650
Arnoldi estimates, 650
Ritz values, 650
attribute, 735
automorphism, 61
average, 736
B´ezier curve, 199
control points, 199
B´ezier spline, 201
back-substitution, 204
Banach space, 326
barycentric combination, see aﬃne
combination
basis, 39
dimension, 42, 46
Beltrami, 703
Bernstein polynomials, 40, 89, 199
best (d −k)-dimensional aﬃne
approximation, 748, 749
best aﬃne approximation, 745
best approximation, 745
Bezout's identity, 760, 761
bidual, 61, 369
bijection between E and its dual E∗,
419
bilinear form, see bilinear map
bilinear map, 165, 375
canonical pairing, 375
deﬁnite, 406
positive, 406
symmetric, 165
block diagonalization
of a normal linear map, 598
of a normal matrix, 608
of a skew-self-adjoint linear map,
603
of a skew-symmetric matrix, 609
of an orthogonal linear map, 604
of an orthogonal matrix, 609
canonical
isomorphism, 419
canonical pairing, 375
evaluation at v, 375
Cartan-Dieudonn´e theorem, 605
sharper version, 605
Cauchy determinant, 322
Cauchy sequence
normed vector space, 326
Cauchy-Schwarz inequality, 294, 295,
409, 494
Cayley-Hamilton theorem, 184, 187
center of gravity, 737
centered data point, 736
centroid, 737, 746, 748
chain, see graph path
change of basis matrix, 87, 88
characteristic polynomial, 183, 301,
534
characteristic value, see eigenvalue
characteristic vector, see eigenvector
Chebyshev polynomials, 431
Cholesky factorization, 240, 241
cofactor, 170
column vector, 6, 48, 373
commutative group, see abelian group
commuting family
linear maps, 768
complete normed vector space, see
Banach space
complex number
conjugate, 487
imaginary part, 487
modulus, 487
real part, 487
complex vector space, 24
complexiﬁcation
of a vector space, 592
of an inner product, 593
complexiﬁcation of vector space, 592
computational geometry, 447
condition number, 316, 475
conductor, 766
conjugate

Index
797
of a complex number, 487
of a matrix, 506
continuous
function, 311
linear map, 311
contravariant, 88
Courant-Fishcer theorem, 615
covariance, 736
covariance matrix, 737
covariant, 373
covector, see linear form, see linear
form
Cramer's rules, 182
cross-product, 421
curve interpolation, 199, 201
de Boor control points, 201
data compression, 17, 713, 733
low-rank decomposition, 17
de Boor control points, 201
QR-decomposition, 428, 441, 447,
463, 469, 473, 503, 508
QR-decomposition, in terms of
Householder matrices, 469
degree matrix, 659, 660, 663, 665, 670
degree of a vertex, 660
Delaunay triangulation, 447, 693
Demmel, 734
determinant, 168, 170
Laplace expansion, 170
linear map, 183
determinant of a linear map, 438
determining orbits of asteroids, 720
diagonal matrix, 531
diagonalizable, 539
diagonalizable matrix, 531
diagonalization, 91
of a normal linear map, 600
of a normal matrix, 610
of a self-adjoint linear map, 601
of a symmetric matrix, 608
diagonalize a matrix, 447
diﬀerential equations
system of ﬁrst order, 786
dilation of hyperplane, 268
direction, 268
scale factor, 268
direct graph
strongly connected components,
664
direct product
inclusion map, 130
projection map, 129
vector spaces, 129
direct sum
inclusion map, 133
projection map, 134
vector space, 130
directed graph, 662
closed, 663
path, 663
length, 663
simply connected, 663
source, 662
target, 662
discriminant, 161
dual basis, 64
dual norm, 517, 518
dual space, 61, 369, 517
annihilator, 376
canonical pairing, 375
coordinate form, 63, 369
dual basis, 64, 369, 380, 381
Duality theorem, 380
linear form, 61, 369
orthogonal, 375
duality
in Euclidean spaces, 419
Duality theorem, 380
edge of a graph, 662, 664
eigenfaces, 751
eigenspace, 300, 533
eigenvalue, 91, 300, 301, 532, 591
algebraic multiplicity, 537
Arnoldi iteration, 649
basic QR algorithm, 627
conditioning number, 551
extreme, 650
geometric multiplicity, 537
interlace, 613
spectrum, 301

798
Index
eigenvector, 91, 300, 533, 591
generalized, 756
elementary matrix, 209, 211
endomorphism, 61
Euclid's proposition, 760
Euclidean geometry, 405
Euclidean norm, 10, 288
induced by an inner product, 412
Euclidean space, 597
deﬁnition, 406
Euclidean structure, 406
evaluation at v, 375
face recognition, 751
family, see I-indexed family
feature, 735
vector, 735
Fiedler number, 675
ﬁeld, 23
ﬁnding eigenvalues
inverse iteration method, 655
power iteration, 653
Rayleigh quotient iteration, 656
Rayleigh-Ritz method, 650, 653
ﬁnite support, 418
ﬁrst principal component
of X, 740
ﬂip
transformations, 438, 507
ﬂip about F
deﬁnition, 464
forward-substitution, 205
Fourier analysis, 407
Fourier matrix, 508
free module, 51
free variables, 254
Frobenius norm, 303, 408, 492
from polar form to SVD, 707
from SVD to polar form, 707
Gauss, 448, 719
Gauss-Jordan factorization, 212, 256
Gaussian elimination, 205, 206, 211
complete pivoting, 234
partial pivoting, 233
pivot, 207
pivoting, 207
gcd, see greatest common divisor, see
greatest common divisor
general linear group, 20
vector space, 61
generalized eigenvector, 756, 777
index, 777
geodesic dome, 694
Gershgorin disc, 545
Gershgorin domain, 545
Gershgorin-Hadamard theorem, 547
Givens rotation, 646
gradient, 421
Gram-Schmidt
orthonormalization, 440, 503
orthonormalization procedure, 426
graph
bipartite, 190
connected, 665
connected component, 665
cut, 679
degree of a vertex, 665
directed, 662
edge, 664
edges, 662
isolated vertex, 675
links between vertex subsets, 679
matching, 190
orientation, 667
relationship to directed graph,
667
oriented, 667
path, 665
closed, 665
length, 665
perfect matching, 190
simple, 662, 665
vertex, 664
vertex degree, 663
vertices, 662
volume of set of vertices, 679
weighted, 669
graph clustering, 681
graph clustering method, 659
normalized cut, 659
graph drawing, 661, 687

Index
799
balanced, 687
energy, 661, 688
function, 687
matrix, 661, 687
orthogonal drawing, 662, 689
relationship to graph clustering,
661
weighted energy function, 688
graph embedding, see graph drawing
graph Laplacian, 660
Grassmann's relation, 138
greatest common divisor
polynomial, 759, 761
relatively prime, 759, 761
group, 18
abelian, 19
identity element, 19
H¨older's inequality, 294, 295
Haar basis, 40, 101, 104, 105
Haar matrix, 105
Haar wavelets, 101, 106
Hadamard, 406
Hadamard matrix, 122
Sylvester-Hadamard, 123
Hahn-Banach theorem, 523
Hermite polynomials, 432
Hermitian form
deﬁnition, 488
positive, 490
positive deﬁnite, 490
Hermitian geometry, 487
Hermitian norm, 496
Hermitian reﬂection, 509
Hermitian space, 487
deﬁnition, 490
Hermitian product, 490
Hilbert matrix, 322
Hilbert space, 420, 499
Hilbert's Nullstellensatz, 379
Hilbert-Schmidt norm, see Frobenius
norm
homogenous system, 254
nontrivial solution, 254
Householder matrices, 442, 463
deﬁnition, 467
Householder matrix, 510
hyperplane, 46, 420, 499
hyperplane symmetry
deﬁnition, 464
ideal, 379, 758
null, 759
principal, 759
radical, 379
zero, 759
idempotent function, 135
identity matrix, 11, 50
image
linear map, 54
image Im f of f, 701
image compression, 734
implicit Q theorem, 646, 657
improper
isometry, 438, 507
orthogonal transformation, 438
unitary transformation, 507
incidence matrix, 659, 664, 666
boundary map, 664
coboundary map, 664
weighted graph, 673
inner product, 10, 54, 405
deﬁnition, 406
Euclidean, 295
Gram matrix, 409
Hermitian, 294
weight function, 432
invariant subspace, 764
inverse map, 59
inverse matrix, 50
isometry, 424
isomorphism, 59
isotropic
vector, 420
Jacobi polynomials, 432
Jacobian matrix, 421
Jordan, 703
Jordan block, 785
Jordan blocks, 757
Jordan decomposition, 779
Jordan form, 757, 785

800
Index
Jordan matrix, 785
Kernel
linear map, 54
Kronecker product, 110
Kronecker symbol, 63
Krylov subspace, 648
Ky Fan k-norm, 714
Ky Fan p-k-norm, 714
Laguerre polynomials, 432
Lanczos iteration, 652
Rayleigh-Ritz method, 653
Laplacian
connection to energy function, 688
Fiedler number, 675
normalized Lrw, 676
normalized Lsym, 676
unnormalized, 671
unnormalized weighted graph, 672
lasso, 13
least squares, 713, 719
method, 448
problems, 445
recursive, 726
weighted, 726
least squares solution x+, 721
least-squares
error, 324
least-squares problem
generalized minimal residuals, 651
GMRES method, 651, 652
residual, 651
Legendre, 448, 719
polynomials, 430
length of a line segment, 405
Lie algebra, 578
Lie bracket, 578
line, 46
linear combination, 6, 33
linear equation, 62
linear form, 61, 369
linear isometry, 405, 424, 433, 504
deﬁnition, 433
linear map, 53
automorphism, 61
bounded, 305, 311
continuous, 311
determinant, 183
endomorphism, 61
idempotent, 514
identity map, 53
image, 54
invariant subspace, 132
inverse, 59
involution, 514
isomorphism, 59
Jordan form, 785
matrix representation, 78
nilpotent, 756, 777
nullity, 138
projection, 514
rank, 55
retraction, 141
section, 141
transpose, 389
linear subspace, 36
linear system
condition, 316
ill-conditioned, 316
linear transformation, 9
linearly dependent, 8, 33
linearly independent, 6, 33
liner map
Kernel, 54
Lorentz form, 420
magic square, 265
magic sum, 265
normal, 265
matrix, 7, 48
adjoint, 299, 610
analysis, 447
bidiagonal, 713
block diagonal, 133, 598
change of basis, 87
conjugate, 299, 609
determinant, 168, 170
diagonal, 531
Hermitian, 299, 610
identity, 11, 50
inverse, 12, 50

Index
801
invertible, 12
Jordan, 785
minor, 169, 177
nonsingular, 12, 51
normal, 299, 610
orthogonal, 12, 300, 607
permanent, 189
product, 49
pseudo-inverse, 13
rank, 142
rank normal form, 267
reduced row echelon, 248, 251
similar, 91
singular, 12, 51
skew-Hermitian, 610
skew-symmetric, 607
square, 48
strictly column diagonally
dominant, 233
strictly row diagonally dominant,
234
sum, 48
symmetric, 133, 299, 607
trace, 63, 534
transpose, 299
tridiagonal, 234, 713
unit lower-triangular, 212
unitary, 300, 610
upper triangular, 441, 532, 540
matrix addition, 48
matrix completion, 522
Netﬂix competition, 522
matrix exponential, 329
eigenvalue, 552
eigenvector, 552
skew symmetric matrix, 331, 553
surjectivity exp: su(2) →SU(2),
579
surjectivity exp: so(3) →SO(3),
440
matrix multiplication, 49
matrix norm, 299, 733
Frobenius, 303
spectral, 310
submultiplicativity, 299
matrix norms, 17
matrix of the iterative method, 342
error vector, 342
Gauss-Seidel method, 349
Gauss-Seidel matrix, 349
Jacobi's method, 346
Jacobi's matrix, 346
relaxation method, 350
matrix of relaxation, 350
Ostrowski-Reich theorem, 354
parameter of relaxation, 351
successive overrelaxation, 351
maximal linearly independent family,
41
mean, 736
metric map, 433
metric notions, 405
minimal generating family, 41
minimal polynomial, 755, 762
minimizing ∥Ax −b∥2, 721
Minkowski inequality, 410, 494
Minkowski's inequality, 295
Minkowski's lemma, 523
minor, 169, 177
cofactor, 170
modiﬁed Gram-Schmidt method, 428
module, 51
free, 51
modulus
complex number, 287
monoid, 19
Moore-Penrose pseudo-inverse, 724
motion
planning, 447
mulitset, 27
multilinear form, 165
multilinear map, 164, 165
symmetric, 165
multiresolution signal analysis, 111
nilpotent, 756
linear map, 777
nodes, see vertex
nondegenerate
symmetric bilinear form, 420
norm, 287, 407, 409, 412, 430, 496
1-norm, 288

802
Index
ℓ2-norm, 10
ℓp-norm, 288
dual, 517, 518
equivalent, 297
Euclidean, 10, 288
Frobenius, 408
matrix, 299
nuclear, 521
parallelogram law, 412
quadratic norm, 298
subordinate, 305, 306
sup-norm, 288
triangle inequality, 287
normal
matrix, 730
normal equations, 448, 721
deﬁnition, 721
normal linear map, 424, 589, 597, 600
deﬁnition, 590
normal matrix, 299
normalized cuts, 680
normalized Haar coeﬃcients, 115
normalized Haar transform matrix,
115
normed vector space, 287, 496
1-norm, 288
ℓp-norm, 288
complete, 326
Euclidean norm, 288
norm, 287
sup-norm, 288
triangle inequality, 287
nuclear norm, 521
matrix completion, 522
nullity, 138
nullspace, see Kernel
operator norm, see subordinate norm
L(E; F), 311
seesubordinate norm, 305
optimization problems, 719
orthogonal, 723
basis, 438
complement, 415, 595
family, 415
linear map, 590, 604
reﬂection, 464
spaces, 432
symmetry, 464
transformation
deﬁnition, 433
vectors, 415, 497
orthogonal group, 436
deﬁnition, 438
orthogonal matrix, 12, 300, 438
deﬁnition, 437
orthogonal projection, 728
orthogonal vectors, 10
orthogonal versus orthonormal, 438
orthogonality, 405, 415
and linear independence, 416
orthonormal
basis, 436, 502
family, 415
orthonormal basis
existence, 425
existence, second proof, 426
overdetermined linear system, 719
pairing
bilinear, 386
nondegenerate, 386
parallelepiped, 173
parallelogram, 173
parallelogram law, 412, 497
parallelotope, 173
partial sums, 418
Pauli spin matrices, 569
PCA, 735, 740, 742
permanent, 189
Van der Waerden conjecture, 191
permutation, 19
permutation matrix, 218, 284
permutation on n elements, 159
Cauchy two-line notation, 160
inversion, 163
one-line notation, 160
sign, 163
signature, 163
symmetric group, 160
transposition, 160

Index
803
basic, 161
perpendicular
vectors, 415
piecewise linear function, 105
plane, 46
Poincar´e separation theorem, 615
polar decomposition, 447
of A, 706
polar form, 699
deﬁnition, 706
of a quadratic form, 408
polynomial
degree, 757
greatest common divisor, 759, 761
indecomposable, 761
irreducible, 761
monic, 757
prime, 761
relatively prime, 759, 761
positive
self-adjoint linear map, 700
positive deﬁnite
bilinear form, 406
self-adjoint linear map, 700
positive deﬁnite matrix, 237
positive semideﬁnite
self-adjoint linear map, 700
pre-Hilbert space, 490
Hermitian product, 490
pre-norm, 519
Primary Decomposition Theorem,
771, 776
principal axes, 713
principal components, 735
principal components analysis, 735
principal directions, 18, 740, 744
principal ideal, 759
generator, 759
projection
linear, 463
projection map, 129, 463
proper
isometry, 438
orthogonal transformations, 438
unitary transformations, 507
proper subspace, see eigenspace
proper value, see eigenvalue
proper vector, see eigenvector
pseudo-inverse, 13, 448, 713
deﬁnition, 723
Penrose properties, 732
quadratic form, 489
associated with ϕ, 406
quaternions, 566
conjugate, 567
Hamilton's identities, 566
interpolation formula, 582
multiplication of, 566
pure quaternions, 568
scalar part, 567
unit, 508
vector part, 567
rank
linear map, 55
matrix, 142, 394
of a linear map, 701
rank normal form, 267
Rank-nullity theorem, 136
ratio, 405
Rayleigh ratio, 611
Rayleigh-Ritz
ratio, 742
theorem, 742
Rayleigh-Ritz theorem, 611, 612
real eigenvalues, 423, 447
real vector space, 23
reduced QR factorization, 648
reduced row echelon form, see rref
reduced row echelon matrix, 248, 251
reﬂection, 405
with respect to F and parallel to
G, 463
reﬂection about F
deﬁnition, 464
replacement lemma, 42, 44
ridge regression, 13
Riesz representation theorem, 420
rigid motion, 405, 433
ring, 22
Rodrigues, 567

804
Index
Rodrigues' formula, 439, 577
rotation, 405
deﬁnition, 438
row vector, 6, 48, 373
rref, see reduced row echelon matrix
augmented matrix, 249
pivot, 251
sample, 735
covariance, 736
covariance matrix, 737
mean, 736
variance, 736
scalar product
deﬁnition, 406
Schatten p-norm, 714
Schmidt, 703
Schur complement, 241
Schur norm, see Frobenius norm
Schur's lemma, 542
SDR, see system of distinct
representatives
self-adjoint linear map, 590, 601, 603
deﬁnition, 423
semilinear map, 488
seminorm, 288, 497
sequence, 26
normed vector space, 326
convergent, 326, 339
series
absolutely convergent
rearrangement property, 328
normed vector space, 327
absolutely convergent, 327
convergent, 327
rearrangement, 328
sesquilinear form
deﬁnition, 488
signal compression, 101
compressed signal, 102
reconstruction, 102
signed volume, 173
similar matrix, 91
simple graph, 662, 665
singular decomposition, 13
pseudo-inverse, 13
singular value decomposition, 319,
447, 699, 712
case of a rectangular matrix, 710
deﬁnition, 705
singular value, 319
square matrices, 706
square matrix, 703
singular values, 13
Weyl's inequalities, 709
singular values of f, 700
skew ﬁeld, 567
skew-self-adjoint linear map, 590
skew-symmetric matrix, 133
SOR, see successive overrelaxation
spanning set, 39
special linear group, 20, 183, 438
special orthogonal group, 20
deﬁnition, 438
special unitary group
deﬁnition, 507
spectral graph theory, 675
spectral norm, 310
dual, 521
spectral radius, 301
spectral theorem, 595
spectrum, 301
spectral radius, 301
spline
B´ezier spline, 201
spline curves, 40
splines, 199
square matrix, 48
SRHT, see subsampled randomized
Hadamard transform
subordinate matrix norm, 305, 306
subordinate norm, 517
subsampled randomized Hadamard
transform, 124
subspace, see linear subspace
k-plane, 46
ﬁnitely generated, 39
generators, 39
hyperplane, 46
invariant, 764
line, 46
plane, 46

Index
805
spanning set, 39
sum of vector spaces, 130
SVD, see singular decomposition, see
singular value decomposition, 447,
703, 712, 742, 748
Sylvester, 703
Sylvester's criterion, 240, 245
Sylvester-Hadamard matrix, 123
Walsh function, 124
symmetric bilinear form, 406
symmetric group, 160
symmetric matrix, 133, 423, 447
positive deﬁnite, 237
symmetric multilinear map, 165
symmetry
with respect to F and parallel to
G, 463
with respect to the origin, 465
system of distinct representatives, 191
tensor product of matrices, see
Kronecker product
total derivative, 62, 420
Jacobian matrix, 421
trace, 63, 300, 534
trace norm, see nuclear norm
translation, 143
translation vector, 143
transporter, see conductor
transpose map, 389
transpose of a matrix, 12, 50, 436,
505, 607, 609
transposition, 160
basic, 161
transposition matrix, 209
transvection of hyperplane, 270
direction, 270
triangle inequality, 287, 412
Minkowski's inequality, 295
triangularized matrix, 532
tridiagonal matrix, 234
uncorrelated, 736
undirected graph, 664
unit quaternions, 566
unitary
group, 505
map, 600
matrix, 505
unitary group
deﬁnition, 507
unitary matrix, 300
deﬁnition, 507
unitary space
deﬁnition, 490
unitary transformation, 504
deﬁnition, 504
unreduced Hessenberg matrix, 638
upper triangular matrix, 532
Vandermonde determinant, 175
variance, 736
vector space
basis, 39
component, 47
coordinate, 47
complex, 24
complexiﬁcation, 592
dimension, 42, 46
direct product, 129
direct sum, 130
ﬁeld of scalars, 24
inﬁnite dimension, 46
norm, 287
real, 23
scalar multiplication, 23
sum, 130
vector addition, 23
vectors, 23
vertex
adjacent, 666
vertex of a graph, 662, 664
degree, 663
Voronoi diagram, 447
walk, see directed graph path, see
graph path
Walsh function, 124
wavelets
Haar, 101
weight matrix
isolated vertex, 675

806
Index
weighted graph, 659, 669
adjacent vertex, 670
degree of vertex, 670
edge, 669
underlying graph, 669
weight matrix, 669
Weyl, 703
Weyl's inequalities, 709
zero vector, 6

