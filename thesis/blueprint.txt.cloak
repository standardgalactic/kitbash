LNAI 16057
Matthew Iklé · Anton Kolonin ·  
Michael Bennett (Eds.)
Artificial General 
Intelligence
18th International Conference, AGI 2025 
Reykjavic, Iceland, August 10-13, 2025 
Proceedings, Part I

Lecture Notes in Computer Science 
Lecture Notes in Artiﬁcial Intelligence
16057 
Founding Editor 
Jörg Siekmann 
Series Editors 
Randy Goebel, University of Alberta, Edmonton, Canada 
Wolfgang Wahlster, DFKI, Berlin, Germany 
Zhi-Hua Zhou, Nanjing University, Nanjing, China

The series Lecture Notes in Artiﬁcial Intelligence (LNAI) was established in 1988 as a 
topical subseries of LNCS devoted to artiﬁcial intelligence. 
The series publishes state-of-the-art research results at a high level. As with the LNCS 
mother series, the mission of the series is to serve the international R & D community 
by providing an invaluable service, mainly focused on the publication of conference and 
workshop proceedings and postproceedings.

Matthew Iklé · Anton Kolonin · Michael Bennett 
Editors 
Artiﬁcial General 
Intelligence 
18th International Conference, AGI 2025 
Reykjavic, Iceland, August 10-13, 2025 
Proceedings, Part I

Editors 
Matthew Iklé 
SingularityNET Foundation 
Zug, Switzerland 
Michael Bennett 
Australian National University 
Canberra, ACT, Australia 
Anton Kolonin 
SingularityNET Foundation 
Zug, Switzerland 
ISSN 0302-9743
ISSN 1611-3349 (electronic) 
Lecture Notes in Artiﬁcial Intelligence 
ISBN 978-3-032-00685-1
ISBN 978-3-032-00686-8 (eBook) 
https://doi.org/10.1007/978-3-032-00686-8 
LNCS Sublibrary: SL7 - Artiﬁcial Intelligence 
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2026 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of 
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission 
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar 
methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the 
editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors 
or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in 
published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
If disposing of this product, please recycle the paper.

Preface 
These two volumes contain the papers presented at AGI 2025, the 18th International Con-
ference on Artiﬁcial General Intelligence. It was held at Reykjavik University in Iceland 
on August 10-13, 2025. The conference was organized as a hybrid event, primarily in 
person with a live audience in Reykjavík, while also enabling virtual participation for 
attendees around the world. Over four days, AGI 2025 featured a rich program including 
invited keynote talks by leading AI researchers, technical paper presentations, poster ses-
sions, and interactive discussions. The event also incorporated six topical workshops. A 
large-scale workshop on Machine Consciousness was run in collaboration with the Asso-
ciation for Mathematical Consciousness Science and the California Institute for Machine 
Consciousness. Likewise, a technical workshop on Hyperon was run by SingularityNet. 
This year's conference drew an unprecedented 179 submissions, more than triple 
the previous year's record number. Each submission was double-blind reviewed by an 
average of three Program Committee members. Ultimately, 72 (40%) of the submis-
sions were accepted for presentation at the conference. The ﬁnal program included 33 
papers selected for oral presentation and 39 papers for poster presentation, reﬂecting 
both the high quality and selectivity of the review process. The accepted contributions 
span a breadth of topics including novel learning algorithms, reasoning systems, theoret-
ical neurobiology and bio- inspired systems, quantum computing, theories of machine 
consciousness, ethics, safety, formal mathematical foundations, and philosophy of AGI. 
This diversity highlights the incredible breadth of AGI research and the community's 
commitment to tackling intelligence in all its aspects. AGI 2025 was further enriched 
by a series of inspiring keynote lectures from leaders in academia and industry. These 
talks addressed core challenges in AGI theory and implementation, the limitations of 
current state-of-the-art learning systems, and new frameworks for integrating reasoning, 
embodiment, and autonomy. 
A central theme of AGI 2025 was the formidable challenge of achieving true general 
intelligence, even amid the extraordinary progress of recent AI systems. The past decade 
has seen tremendous advances in applied AI exempliﬁed by the rise of large language 
models (LLMs) and other deep learning systems that exhibit impressive capabilities on 
specialized tasks. Yet, despite these achievements, current AI models still fall far short of 
true generality. Throughout the conference, discussions repeatedly underscored this gap: 
while modern AI can excel in narrow domains, it lacks the robust adaptiveness, under-
standing, and autonomous reasoning that characterize general intelligence. In keynote 
talks and panels, speakers emphasized that achieving AGI remains an open, grand chal-
lenge, requiring fundamental breakthroughs beyond scaling up existing architectures. 
This shared recognition set the tone for AGI 2025, a collective call to look past the cur-
rent state of the art and focus on the insights, philosophies, architectures, and scientiﬁc 
discoveries needed to ultimately realize genuine general intelligence. 
We would like to express our gratitude to everyone who contributed to the success 
of AGI 2025. First and foremost, we thank the Program Committee members for their

vi
Preface
dedicated service in reviewing submissions and maintaining the high quality of these 
proceedings. We also thank all the authors who submitted their work, and we congratulate 
those whose papers were accepted and presented at the conference. We are grateful to the 
keynote and invited speakers for sharing their expertise and visions, and to the workshop 
organizers for curating excellent sessions that enriched the program. Lastly, we extend 
our deep appreciation to the AGI Society and our sponsors, including SingularityNET, 
TrueAGI, and Springer for their generous support in making AGI 2025 possible. 
June 2025
Anton Kolonin 
Michael Timothy Bennett 
Matthew Iklé

Organization 
Conference Chair 
Matthew Iklé
SingularityNET Foundation, USA 
Local Organizers 
Kristinn Thorisson
Reykjavik University, Iceland 
Leonard Eberding
Reykjavik University, Iceland 
Program Committee Chairs 
Anton Kolonin
Novosibirsk State University, Russia 
Michael Timothy Bennett
Australian National University, Australia 
Organizing Committee 
Haley Lowy
SingularityNET Foundation, USA 
Filip Maric
SingularityNET Foundation, Croatia 
Pamela Mackay
SingularityNET Foundation, UK 
Michael Timothy Bennett
Australian National University, Australia 
Peter Isaev
SingularityNET Foundation, USA 
Program Committee 
Vincent Abbott
MIT, USA 
Marcus Abundis
Stanford Grad. School of Business (GFTP), USA 
Matthew Aitchison
Australian National University, Australia 
Mohammadreza Alidoust
Islamic Azad University - Science and Research 
Branch, Tehran, Iran 
Nadav Amir
Princeton University, USA 
Joscha Bach
AI Foundation, USA 
Christian Balkenius
Lund University, Sweden 
Manuel Baltieri
Araya Inc., Japan

viii
Organization
Matteo Belenchia
Reykjavik University 
Salem Benferhat
Cril, CNRS UMR8188, Université d'Artois, 
France 
Michael Timothy Bennett
Australian National University, Australia 
Frank Bergmann
fraber.de, Germany 
Piotr Boltuc
Warsaw School of Economics, Poland and 
University of Illinois Springﬁeld, USA 
Adrian Borucki
Genotic, USA 
Alexander Bringsjord
PwC, USA 
Selmer Bringsjord
Rensselaer Polytechnic Institute, USA 
Antonio Chella
Università di Palermo, Italy 
Yu Cheng
Columbia University, USA 
Oisin Hugh Clancy
Independent, Ireland 
Tyler Cody
Virginia Tech, USA 
Bob Coecke
Cambridge Quantum Computing Ltd., UK 
Khellar Crawford
SingularityNET, USA 
Vassilis Cutsuridis
Foundation for Research and 
Technology - Hellas, Greece 
Mayank Daswani
Microsoft, UK 
Goncalo de Carvalho
IIIM, Iceland 
Lei Deng
Tsinghua University, CHina 
Akshar Desai
Indian Institute of Technology, Dharwad, India 
Steve Dipaola
Simon Fraser University, Canada 
Len Du
Australian National University, Australia 
Michael Duncan
SingularityNET, USA 
Leonard M. Eberding
Reykjavik University, Iceland 
Aram Ebtekar
Carnegie Mellon University, USA 
Adam Elwood
lastminute.com, Italy 
Blerim Emruli
SICS Swedish ICT AB, Sweden 
Menilik Eshetu
SingularityNET, Ethiopia 
Evgenii Evstafev
University of Cambridge, UK 
Thomas Ferguson
RPI, USA 
Robert Freeman
Not currently afﬁliated 
Kyle Fuller
Rensselaer Polytechnic Institute, USA 
Martin Funkquist
Link¨oping University, Sweden 
Nil Geisweiller
SingularityNET Foundation, Bulgaria 
Olivier Georgeon
Université Claude Bernard Lyon 1, France 
Michael Giancola
Rensselaer Polytechnic Institute, USA 
Habtom Gidey
Technical University of Munich, Germany 
Diego Gimenez
Aily Labs, Spain 
Ben Goertzel
SingularityNET 
Zarathustra Goertzel
CIIRC, Czech Republic

Organization
ix
Naveen Sundar Govindarajulu
Rensselaer Polytechnic Institute, USA 
Árni Dagur Gumundsson
KTH Royal Institute of Technology, Sweden 
Faezeh Habibi
SingularityNET, Iran 
Christian Hahm
Temple University, USA 
Patrick Hammer
SingularityNET, Sweden 
Yusuke Hayashi
AI Alignment Network, Japan 
Jose Hernandez-Orallo
Universitat Politècnica de València, Spain 
Martin Hilbert
University of California, Davis, USA 
Noel Hinton
No afﬁliation 
Xiao Hu
Applovin, USA 
Alfredo Ibias
Avatar Cognition, Spain 
Matt Iklé
SingularityNET, USA 
David Ireland
CSIRO, Australia 
Peter Isaev
SingularityNET, USA 
Nino Ivanov
Private Researcher, Austria 
Yipeng Kang
Beijing Institute of Artiﬁcial General Intelligence, 
China 
Craig Kaplan
iQ Company, USA 
Dmitry Karpov
Severstal Digital, Russia 
Susumu Katayama
University of Miyazaki, Japan 
Mayank Kejriwal
Information Sciences Institute, USA 
Milad Khademinori
Toronto Metropolitan University, Canada 
Aleksandr Khomyakov
ETM, Russia 
Mikhail Kiselev
Megaputer Intelligence, USA 
Dmitry Klepikov
TruBrainComputing, Russia 
Anton Kolonin
Novosibirsk State University, Russia 
Shimon Komarovsky
Technion, Israel 
Steve Kommrusch
Leela AI, USA 
Alyona Kosobokova
USB, USA 
Alexey Kovalev
ISA RAS, Russia 
Jerald Kralik
Korea Advanced Institute of Science and 
Technology, South Korea 
Kirill Krinkin
Co-evolution AI, Cyprus 
Xiang Li
Temple University, USA 
Xiaoyan Li
Tsinghua University, China 
Kai Liu
Bohai University, China 
Tony Lofthouse
Stockholm University, Sweden 
Haley Lowy
SingularityNET Foundation, USA 
Daniel MacDonald
SingularityNET, USA 
Pamela Mackay
SingularityNET, UK 
Brett Martensen
Adaptron Inc., Canada 
Yoshihiro Maruyama
Kyoto University, Japan

x
Organization
Brian McDermott
Rensselaer Polytechnic Institute, USA 
Cédric Mesnage
University of Exeter, UK 
Anna Mikeda
SingularityNET, Portugal 
Nikolay Mikhaylovskiy
Higher IT School of Tomsk State University, 
Russia 
Douglas Miles
SingularityNET, USA 
Michael S. P. Miller
SubThought Corporation, USA 
Kenshi Miyabe
Meiji University, Japan 
Andrey Nechesov
Sobolev Institute of Mathematics, Russia 
Andrew Nuxoll
University of Portland, USA 
David Orban
Beyond Enterprizes, USA 
James Oswald
Rensselaer Polytechnic Institute, USA 
Eray Özkural
Bilkent University, Turkey 
Aleksandr I. Panov
AIRI, MIPT, FRC CSC RAS, Russia 
Elija Perrier
University of Technology Sydney, Australia 
Denis Ponomaryov
Ershov Institute of Informatics Systems, 
Novosibirsk State University, Russia 
Alexey Potapov
SingularityNET, Russia 
Robert Prentner
Florida Atlantic University, China 
David Quarel
Australian National University, Australia 
Saty Raghavachary
University of Southern California, USA 
David Rawlinson
Cerenaut AI, Australia 
Federico Redi
University of Bergamo, Italy 
Chiaki Sakama
Wakayama University, Japan 
Savitha Sam Abraham
University of Adelaide, Australia 
Grigory Sapunov
Intento, UK 
Deacon Sawyer
Rensselaer Polytechnic Institute, USA 
Chloe Schaff
Reykjavík University, Iceland 
Oleg Scherbakov
ITMO, Russia 
Howard Schneider
Sheppard Clinic North, Canada 
Jonathon Schwartz
Australian National University, Australia 
Stanislav Selitskiy
University of Bedfordshire, UK 
Victor Senkevich
Organoid AGI Project, USA 
Eli Sennesh
Vanderbilt University, USA 
Imran Shaﬁ
National University of Sciences and Technology, 
Pakistan 
Avi Shaked
University of Oxford, UK 
Tatiana Shavrina
NRU HSE, Russia 
Yenatfanta Shifferaw
SingularityNET, Ethiopia 
Sergey Shumsky
Adam & Eva, Russia 
Gabriel Simmons
University of California, Davis, USA 
Nady Slam
Xibei Minzu University, China

Organization
xi
Vladimir Smolin
Keldysh Institute of Applied Mathematics of 
RAS, Russia 
Hikari Sorensen
California Institute for Machine Consciousness, 
USA 
Rachel St. Clair
Simuli, Inc., USA 
Bas Steunebrink
NNAISENSE, Switzerland 
Peter Sutor
University of Maryland, USA 
Izak Tait
Xeno-Consciousness Research Society, New 
Zealand 
Koichi Takahashi
RIKEN, Japan 
Gregorio Talevi
Whitehall Reply, Italy 
Pieter ter Doest
PNF7, Netherlands 
Kristinn R. Thorisson
Reykjavik University, Iceland 
David Thorstad
Vanderbilt University, USA 
Tongwei Tu
Sawtest Solution, USA 
Timofey Tylik
Rensselaer Polytechnic Institute, USA 
Vanessa Utz
Simon Fraser University, Canada 
Ondřej Vadinský
Prague University of Economics and Business, 
Czech Republic 
Dwane van der Sluis
WiseWorks.AI, UK 
Linas Vepstas
OpenCog Foundation, USA 
Mario Verdicchio
Università degli Studi di Bergamo, Italy 
Evgenii Vityaev
Sobolev Institute of Mathematics SB RAS, Russia 
Maggie von Ebers
University of Texas Austin, USA 
Xiaolong Wan
University of Electronic Science and Technology 
of China, China 
Pei Wang
Temple University, USA 
Sean Welsh
Centacare Brisbane, Australia 
Andy Williams
Nobeah Foundation, Kenya 
Robert Wray
Center for Integrated Cognition, USA 
George Wright
Queen Mary University of London, UK 
Wei Wu
Amazon, USA 
Cole Wyeth
University of Waterloo, Canada 
Bowen Xu
Temple University, USA 
Tom Xu
Australian National University, Australia 
King-Yin Yan
General Intelligence, China 
Arisa Yasuda
Australian National University, Australia 
Eyob Yirdaw
iCog Labs, Ethiopia 
Bingxin Zhu
Meta, USA 
Jaime Zornoza
None, Spain 
Stefán Ólafsson
Reykjavik University, Iceland

xii
Organization
Steering Committee 
Ben Goertzel
SingularityNET Foundation and TrueAGI (Chair), 
USA 
Marcus Hutter
Australian National University, Australia

Contents - Part I 
Accelerating Machine Learning Systems via Category Theory: 
Applications to Spherical Attention for Gene Regulatory Networks . . . . . . . . . . .
1 
Vincent Abbott, Kotaro Kamiya, Gerard Glowacki, Yu Atsumi, 
Gioele Zardini, and Yoshihiro Maruyama 
Bridging the Design and Intentional Stances: A Path Towards Interpretable 
AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12 
Vincent Abruzzo 
Prospective Learning in Retrospect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17 
Yuxin Bai, Cecelia Shuai, Ashwin De Silva, Siyu Yu, Pratik Chaudhari, 
and Joshua T. Vogelstein 
What Is Artiﬁcial General Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30 
Michael Timothy Bennett 
Optimal Policy Is Weakest Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43 
Michael Timothy Bennett 
Ethically Permissible Pursuit of Quantum Consciousness . . . . . . . . . . . . . . . . . . . .
49 
Selmer Bringsjord, Naveen Sundar Govindarajulu, Brian McDermott, 
and Alexander Bringsjord 
Is Phenomenal Consciousness Necessary for AGI? A Review 
of the Theoretical Landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60 
Ignacio Cea 
Holographic Memory and Cortical Microcircuits: A Step Towards AGI . . . . . . . .
72 
Oscar Chang, Jonathan Pérez, and Amy Meneses 
Mutually Beneﬁcial Artiﬁcial Consciousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84 
Oisín Hugh Clancy 
MeTTa-TMPAL: MeTTa-Based Architecture for a Self-writing Process 
Algebra of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98 
Tyler Cody 
Linguistic Loops and Geometric Invariants as a Way to Pre-verbal Thought? . . .
109 
Daniele Corradetti and Alessio Marrani

xiv
Contents - Part I
Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism . . . . . . . .
119 
Gonçalo Hora de Carvalho and Kristinn R. Thórisson 
Creative Physics: A Categorical Framework for Creative Dynamical 
Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135 
Justin Diamond 
Neuro-Symbolic LIDA's Semantic Vision System . . . . . . . . . . . . . . . . . . . . . . . . . .
147 
Nathan DiGilio and Pulin Agrawal 
Resource-Relativized Legg-Hutter Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
Kyle J. Fuller, Deacon R. Sawyer, James T. Oswald, 
and Thomas M. Ferguson 
A Spatio-temporal Schema Mechanism for Developmental Robotics . . . . . . . . . .
170 
Olivier L. Georgeon, Simon L. Gay, and Paul Robertson 
A Modular Cognitive Architecture for Collective Intelligence Systems . . . . . . . .
181 
Amber L. Gibson and Dmitry Sokolov 
OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI . . . . . . . . . . . . . .
192 
Ben Goertzel 
Patterns of Quantum Cognition I: From Chronomorphisms to Quantum 
Propagators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203 
Ben Goertzel 
The Emergence of Modularization from Architecture Search via Optimal 
Transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212 
Ben Goertzel 
Agentic Correlates of Consciousness and the Pursuit of Artiﬁcial General 
Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225 
Shannon Gray, George Tambouratzis, Sanju Mannumadam Venugopal, 
Sridhar Raghavan, Richard Jiarui Tong, and Zeyu Han 
Several Issues Regarding Data Governance in AGI . . . . . . . . . . . . . . . . . . . . . . . . .
239 
Masayuki Hatta 
Universal AI Maximizes Variational Empowerment . . . . . . . . . . . . . . . . . . . . . . . .
250 
Yusuke Hayashi and Koichi Takahashi 
A Constructive Developmental Evaluation of AGI: Can AI's Simulations 
Match Human Meaning-Making and Their Orders of Consciousness? . . . . . . . . .
263 
Martin Hilbert

Contents - Part I
xv
Fertility: The Missing Code for AGI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
278 
Nicoletta Iacobacci 
Beating Transformers Using Synthetic Cognition
. . . . . . . . . . . . . . . . . . . . . . . . . .
291 
Alfredo Ibias, Miguel Rodriguez-Galindo, Hector Antona, 
Guillem Ramirez-Miranda, and Enric Guinovart 
AKA: Agentic Self-Knowledge Augmentation Framework . . . . . . . . . . . . . . . . . .
304 
Dae Woong Jo 
Arbitrarily Applicable Same/Opposite Relational Responding with NARS . . . . .
314 
Robert Johansson, Patrick Hammer, and Tony Lofthouse 
Designing Safe SuperIntelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325 
Craig A. Kaplan 
Exploring Collective Dynamics in Cognitive Agent Networks . . . . . . . . . . . . . . . .
335 
Kirill Krinkin 
Contemplative Superalignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346 
Ruben E. Laukkonen, Fionn Inglis, Shamil Chandaria, 
Lars Sandved-Smith, Edmundo Lopez-Sola, Jakob Hohwy, 
Jonathan Gold, and Adam Elwood 
Inverted Cognition: Toward Minds that Begin with Output and Derive 
Goals Retroactively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362 
Ray X. Lee 
On Improving Dynamic Resource Allocation in NARS with a Novel Bag 
Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375 
Tangrui Li and Boyang Xu 
MetaMo: A Robust Motivational Framework for Open-Ended AGI . . . . . . . . . . .
386 
Ruiting Lian and Ben Goertzel 
Embodying Abstract Motivational Principles in Concrete AGI Systems: 
From MetaMo to Open-Ended OpenPsi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399 
Ruiting Lian and Ben Goertzel 
Integrating Functionalities to a System via Autoencoder Hippocampus 
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411 
Siwei Luo

xvi
Contents - Part I
IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness 
in Communicative Multi-agent Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
421 
Yihuan Mao, Yipeng Kang, Peilun Li, Wei Xu, and Chongjie Zhang 
Variational Inference Optimized Using the Curved Geometry of Coupled 
Free Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433 
Kenric P. Nelson, Igor Oliveira, Amenah Al-Najaﬁ, Fode Zhang, 
and Hon Keung Tony Ng 
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
447

Contents - Part II 
On the Deﬁnition of Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
Kei-Sing Ng 
Developing a General-Purpose System for Intentionality Detection 
in Dialogue Using Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12 
Tuan Minh Nguyen and Alexei V. Samsonovich 
On the Arrowian Impossibility of Machine Intelligence Measures . . . . . . . . . . . .
23 
James T. Oswald, Thomas M. Ferguson, and Selmer Bringsjord 
Subjectivity as Self-Simulation: Virtualising the Cartesian Theatre . . . . . . . . . . . .
34 
Roly Perera 
Watts-Per-Intelligence: Part I (Energy Efﬁciency) . . . . . . . . . . . . . . . . . . . . . . . . . .
46 
Elija Perrier 
Quantum AIXI: Universal Intelligence via Quantum Information . . . . . . . . . . . . .
58 
Elija Perrier 
Hamiltonian Formalism for Comparing Quantum and Classical Intelligence . . . .
71 
Elija Perrier 
Quantum AGI: Ontological Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83 
Elija Perrier and Michael Timothy Bennett 
When Fields Co-model: Emergent Meaning and Proto-consciousness 
in Large Language Models via the Upper Modeling Framework . . . . . . . . . . . . . .
95 
Rubina Polovina 
Modeling Intelligence as Trajectories in Complex Space: 
A Quantum-Inspired Approach to AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109 
Pawel Filip Pospieszynski 
The Role of LLMs in AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125 
Alexey Potapov and Vita Potapova 
Artiﬁcial Consciousness as Interface Representation . . . . . . . . . . . . . . . . . . . . . . . .
135 
Robert Prentner

xviii
Contents - Part II
Temporal Predictive Coding as World Model for Reinforcement Learning . . . . .
147 
Artem Prokhorenko, Petr Kuderov, Evgenii Dzhivelikian, 
and Aleksandr Panov 
Mapping Neural Theories of Consciousness onto the Common Model 
of Cognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
Paul S. Rosenbloom, John E. Laird, Christian Lebiere, and Andrea Stocco 
Towards Synthetic Engineers: Requirements and Implications 
of the Conceptual Engineering Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166 
Chloe A. Schaff and Kristinn R. Thórisson 
Theory of Mind as a Core Component of Artiﬁcial General Intelligence . . . . . . .
178 
Howard Schneider 
An Affective-Taxis Hypothesis for Alignment and Interpretability . . . . . . . . . . . .
188 
Eli Sennesh and Maxwell Ramstead 
From Thought to Action: Bridging Cognitive Processes and Autonomous 
MORL Towards Intelligent Agents in a Virtual Environment . . . . . . . . . . . . . . . . .
202 
Shagofta Shabashkhan, Xiaoyang Wang, and Cédric S. Mesnage 
A Reply to "Is Complexity An Illusion?" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214 
Gabriel Simmons 
Which Consciousness Can Be Artiﬁcialized? Local Percept-Perceiver 
Phenomenon for the Existence of Machine Consciousness . . . . . . . . . . . . . . . . . . .
220 
Shri Lal Raghudev Ram Singh 
Integrating AGI and Transhumanist Technologies in Education: 
An Integrative Framework of Cognitive Enhancement and Ethical 
Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231 
Serap Sisman-Ugur 
HyPE: Hyperdimensional Propagation of Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241 
Peter Sutor, Renato Faraone, Cornelia Fermüller, and Yiannis Aloimonos 
Initial Evaluation of Deep Q-Learning in the Algorithmic Intelligence 
Quotient Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252 
Ondřej Vadinský and Michal Dvořák 
A Soul in the Machine? The Prospect of Artiﬁcially Created Consciousness . . . .
264 
Weaver D. R. Weinbaum

Contents - Part II
xix
The Ethics of Artiﬁcial Consciousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275 
Sean Welsh 
The Direct Approach of Testing for AGI-Consciousness . . . . . . . . . . . . . . . . . . . . .
285 
Ouri Wolfson 
Requirements for Recognition and Rapid Response to Unfamiliar Events 
Outside of Agent Design Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299 
Robert E. Wray, Steven J. Jones, and John E. Laird 
Applying Cognitive Design Patterns to General LLM Agents . . . . . . . . . . . . . . . .
312 
Robert E. Wray, James R. Kirk, and John E. Laird 
A Treasure Map to Metacognition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326 
George Alexander Wright 
Value Under Ignorance in Universal Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . .
338 
Cole Wyeth and Marcus Hutter 
On the Essence of Spatial Sense and Objects in Intelligence . . . . . . . . . . . . . . . . .
350 
Bowen Xu and Pei Wang 
Biological Processing Units: Leveraging an Insect Connectome to Pioneer 
Bioﬁdelic Neural Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361 
Siyu Yu, Zihan Qin, Tingshan Liu, Beiya Xu, R. Jacob Vogelstein, 
Jason Brown, and Joshua T. Vogelstein 
Roadmap on Incentive Compatibility for AI Alignment and Governance 
in Sociotechnical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
370 
Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, 
Chengdong Ma, and Yaodong Yang 
Heterogeneous Value Alignment Evaluation for Large Language Models . . . . . .
381 
Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, 
Song-Chun Zhu, and Yaodong Yang 
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393

Accelerating Machine Learning Systems 
via Category Theory: Applications 
to Spherical Attention for Gene 
Regulatory Networks 
Vincent Abbott1(B), Kotaro Kamiya2, Gerard Glowacki3, Yu Atsumi2, 
Gioele Zardini1, and Yoshihiro Maruyama3 
1 Laboratory for Information and Decision Systems, Massachusetts Institute of 
Technology, Cambridge, USA 
{vtabbott,gzardini}@mit.edu 
2 SyntheticGestalt, Tokyo, Japan 
{k.kamiya,y.atsumi}@syntheticgestalt.com 
3 School of Informatics, Nagoya University, Nagoya, Japan 
{glowacki,maruyama}@i.nagoya-u.ac.jp 
Abstract. How do we enable artiﬁcial intelligence models to improve 
themselves? This is central to exponentially improving generalized arti-
ﬁcial intelligence models, which can improve their own architecture to 
handle new problem domains in an eﬃcient manner that leverages the 
latest hardware. However, current automated compilation methods are 
poor, and eﬃcient algorithms require years of human development. In 
this paper, we use neural circuit diagrams, based in category theory, 
to prove a general theorem related to deep learning algorithms, guide 
the development of a novel attention algorithm catered to the domain 
of gene regulatory networks, and produce a corresponding eﬃcient ker-
nel. The algorithm we propose, spherical attention, shows that neural 
circuit diagrams enable a principled and systematic method for reason-
ing about deep learning architectures and providing high-performance 
code. By replacing SoftMax with an L^2L2 norm as suggested by diagrams, 
it overcomes the special function unit bottleneck of standard attention 
while retaining the streaming property essential to high-performance. 
Our diagrammatically derived FlashSign kernel achieves comparable per-
formance to the state-of-the-art, ﬁne-tuned FlashAttention algorithm on 
an A100, and 3.6\times3.6× the performance of PyTorch. Overall, this investiga-
tion shows neural circuit diagrams' suitability as a high-level framework 
for the automated development of eﬃcient, novel artiﬁcial intelligence 
architectures. 
Keywords: Deep Learning Architecture · Neural Circuit Diagram · 
Category Theory · Spherical Attention · Gene Regulatory Network 
Y. Maruyama—This work was supported by JST Moonshot R&D JPMJMS2033, JST 
PRESTO JPMJFR206P, and JST FOREST JPMJPR24K9. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 1-11, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_1

2
V. Abbott et al.
1
Introduction 
Deep learning architectures lack a systematic analytical framework. Traditional 
tools, such as linear algebra, fall short in capturing non-linearities, while graph-
based representations and ad hoc diagrams fail to account for critical structural 
information such as broadcasting. This omission blocks our understanding of 
resource usage and limits the development of eﬃcient, parallelized implementa-
tions. Consequently, automated compilation tools such as PyTorch's compile and 
Triton [ 13] are typically constrained to basic, elementwise fusion. More complex 
algorithms, including attention mechanisms [ 16], required years of engineering 
eﬀort before GPU-optimized implementations became available [ 9, 10, 15]. 
Neural circuit diagrams oﬀer a compelling solution [ 1, 2, 4]. They adapt 
monoidal string diagrams from category theory [ 14, 18], encoding both data 
and operations of a deep learning model, with tensor axes represented as wires. 
Dashed lines denote independence among operations or data structures, and 
broadcasting is expressed by enclosing operations with axis wires, which auto-
matically determine input and output dimensions. Linear contractions (e.g., dot 
products) appear as cups, while outer products are represented by open-ended 
dashed lines. 
This formalism naturally accommodates non-linear operations, broadcast-
ing, and linear algebraic structure. More importantly, it exposes how compu-
tation is distributed across axes, enabling the derivation of eﬃcient implemen-
tations aligned with the processor hierarchy. These diagrams can be systemat-
ically mapped to low-level code, supporting precise performance modeling that 
includes memory hierarchies, cache-aware bandwidth estimates, and even over-
lapping clock-cycle analysis. 
In unifying high-level algorithmic structure with hardware-aware execution 
detail, neural circuit diagrams make it possible to analyze, optimize, and redesign 
deep learning models within a single framework. They provide the scaﬀolding 
needed for automating the discovery and reﬁnement of learning algorithms, and 
as such, they lay the groundwork for building self-improving AI systems. 
In order to show the utility of neural circuit diagrams, we will perform a prin-
cipled investigation of a novel algorithm guided by the insights diagrammatic 
analysis. Speciﬁcally, the analysis of FlashAttention by neural circuit diagrams 
indicates that the exponential component of SoftMax is a bottleneck. The spe-
cial function unit of GPUs has orders of magnitude fewer FLOPs available than 
matrix multiplication tensor cores or standard ﬂoating point cores [ 9]. For FP16 
FlashAttention, the exponents take half as many clock cycles as the tensor core 
matrix multiplications. For FP8, they just take as many, completely bottleneck-
ing the algorithm. Perfect overlapping is diﬃcult, and there is reason to believe 
SoftMax has overhead beyond the special function unit operations [ 4], includ-
ing warp-shuﬄing the maximum for numerical stability and unit conversion as 
the exponent can only occur in FP32. Diagrams, by revealing this information 
in clock cycle analysis, strongly hint at replacing the exponent with another 
operation.

Accelerating Machine Learning Systems via Category Theory
3
However, the viability of attention as an algorithm requires the constituent 
steps, \displaystyle QK^{T}QKT matmul, the SoftMax normalization, and \displaystyle SVSV matmul to be fused. 
This involves streaming input data in chunks and performing all steps without 
intermediate reads and writes to high-level memory. This prevents quadratic 
memory and bandwidth costs. The conditions under which fusion and stream-
ing are possible is poorly understood using traditional methods, with fused, 
FlashAttention requiring ﬁve years to develop from the initial release of atten-
tion. Overcoming the exponential bottleneck requires the new algorithm to also 
be fused in the same manner. Helpfully, neural circuit diagrams allow the condi-
tions for fusion to be systematically discovered and provide tools to prove that 
novel algorithms are fusable. 
Furthermore, this work integrates the challenge of discovering new algorithms 
systematically, and in a potentially automated manner, with the ﬁeld of categor-
ical deep learning. Categorical deep learning [ 12] allows backpropagation [ 8, 11], 
data-aware algorithms, and architecture classes [ 12] to be formally described. 
More broadly, category theory has been applied to various ﬁelds of science, 
including physics [ 7], logic [ 5], and others, revealing relationships between them 
and thus having promise for AGI [ 3]. It has also been applied to resource usage 
optimizations of complex engineered systems [ 19] and providing compilations 
[ 17], which means that a categorical formalism relates well to eﬃcient imple-
mentation. This investigation shows that category theory, rather than being a 
pure abstract tool, can provide the structure for systematic derivation of deep 
learning algorithms. 
In this paper, we follow the guidance of neural circuit diagrams to provide a 
novel attention algorithm that overcomes the exponential bottleneck, spherical 
attention, by replacing the SoftMax with an \displaystyle L^{2}L2-norm. We use neural circuit dia-
grams to prove a general theorem for streamable attention variants using alter-
native normalizers. We introduce spherical attention, and use its sign-preserving 
properties to provide an algorithm for gene regulatory networks. We provide a 
low-level implementation as guided by diagrams, achieving 3.6\times3.6× the performance 
of PyTorch and comparable throughput to FlashAttention on an A100. Overall, 
this shows the utility of neural circuit diagrams as a systematic and principled 
tool to analyze existing deep learning algorithms, reason about alternative archi-
tectures in a generalized manner, develop algorithms suited to novel domains, 
and provide eﬃcient low-level implementations. 
2
Generalized Streamability 
Algorithms represented by neural circuit diagrams have the constituent axes 
of data and broadcasting clearly revealed. These parallelization details can be 
mapped onto execution in a processor hierarchy by recoloring wires to indicate 
presence at lower levels and relabelling to indicate the size and partition strate-
gies they adopt. Group partitioning tiles an axis to be executed in parallel, and 
is feasible whenever a wire broadcasts the target operation. Streaming can be 
applied if an algorithm satisﬁes certain condition, and allows the ﬁnal output to 
be calculated from chunks of the input data.

4
V. Abbott et al.
Streamability is accompanied by the fusion theorems. These indicate that 
an algorithm remains streamable when composed with other operations and 
broadcasted over additional axes, as long as the streamed axis is preserved. The 
challenge of overcoming the exponential bottleneck of attention is developing an 
alternative which is streamable. Using the fusion theorems, we can prove that 
any normalization can replace the SoftMax operation to generate a streamable 
attention alternative. 
We provide relevant deﬁnitions, a lemma, and a theorem below. 
Deﬁnition 1 (Streamable Function). A polymorphic function \displaystyle f:X^{n}\rightarrow Yf : Xn →Y
deﬁned for any \displaystyle n\in \mathbb{N}n ∈N is streamable if there exists a polymorphic accumulator 
\displaystyle B:Y\times X^{n}\rightarrow YB : Y × Xn →Y such that \displaystyle B( f(\mathbf{x}) ,\mathbf{y}) =f(\mathbf{x} \oplus \mathbf{y})B(f(x), y) = f(x ⊕y), where  \displaystyle \oplus ⊕is concatenation 
along the \displaystyle nn-axis. If \displaystyle f:X^{n}\rightarrow Yf : Xn →Y is streamable and we have \displaystyle t:Y\rightarrow Zt : Y →Z, then 
\displaystyle t\circ f:X^{n}\rightarrow Zt ◦f : Xn →Z is also considered streamable. Streamable functions require little 
memory at lower levels to compute, as only a limited \displaystyle X^{n}Xn and \displaystyle YY are required to 
be on lower levels. 
Deﬁnition 2 (Normalized Contraction). A normalized operation \displaystyle N:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}N :
Rn →Rn uses a pair of activation functions \displaystyle a_{1}a1, \displaystyle a_{2}a2 and an aggregator function \displaystyle bb
to provide \displaystyle N(\mathbf{x})_{i} =a_{1}( x_{i}) /b( \Sigma _{j} a_{2}( x_{j}))N(x)i = a1(xi)/b(Σja2(xj)). A  normalized contraction \displaystyle NC:\mathbb{R}^{n} \times \mathbb{R}^{n} \simeq \left(\mathbb{R}^{2}\right)^{n}\rightarrow \mathbb{R}NC : Rn×
Rn ≃

R2n →R follows normalization with a linear contaction, \displaystyle NC(\mathbf{x} ,\mathbf{y}) =N(\mathbf{x}) \cdot \mathbf{y} =( \Sigma _{i} a_{1}( x_{i}) y_{i}) /b( \Sigma _{i} a_{2}( x_{i}))NC(x, y) =
N(x) · y = (Σia1(xi)yi)/b(Σia2(xi)). 
Lemma 1. Normalized contractions are streamable. 
Fig. 1. Generic normalized contraction, which we represent with a triple-lined triangle, 
can be expanded into a loop where the nn-axis is partitioned into chunks of size ss, and  
only zz and oo are maintained between chunks. 
Proof. Set  up  \displaystyle B:\mathbb{R}^{2} \times \left(\mathbb{R}^{n}\right)^{2}\rightarrow \mathbb{R}^{2}B : R2 × (Rn)2 →R2 so that \displaystyle B(( o,z) ,(\mathbf{x} ,\mathbf{y})) \mapsto ( o+\Sigma _{i} a_{1}( x_{i}) \cdot y_{i} ,z+\Sigma _{i} a_{2}( x_{i}))B((o, z), (x, y)) →(o + Σia1(xi) ·
yi, z + Σia2(xi)). Then, we see that; 
\bUnALT{} &B( B(( o,z) ,(\mathbf{x} ,\mathbf{y})) ,\ (\mathbf{x} ',\mathbf{y} '))\\ &\quad =B(( o+\Sigma _{i} a_{1}( x_{i}) \cdot y_{i} ,z+\Sigma _{i} a_{2}( x_{i})) ,\ (\mathbf{x} ',\mathbf{y} '))\\ &\quad =( o+\Sigma _{i} a_{1}( x_{i}) \cdot y_{i} +\Sigma _{j} a_{2}( x_{j}) \cdot y_{j} ,z+\Sigma _{i} a_{2}( x_{i}) +\Sigma _{j} a_{2}( x_{j}))\\ &\quad =B(( o,z) ,(\mathbf{x} \oplus \mathbf{x} ',\mathbf{y} \oplus \mathbf{y} '))\eUnALT{} B(B((o, z), (x, y)), (x′, y′))
= B((o + Σia1(xi) · yi, z + Σia2(xi)), (x′, y′))
= (o + Σia1(xi) · yi + Σja2(xj) · yj, z + Σia2(xi) + Σja2(xj))
= B((o, z), (x ⊕x′, y ⊕y′))

Accelerating Machine Learning Systems via Category Theory
5
We then set up \displaystyle g:\left(\mathbb{R}^{n}\right)^{2}\rightarrow \mathbb{R}^{2}g : (Rn)2 →R2 as \displaystyle g(\mathbf{x} ,\mathbf{y}) \mapsto B(( 0,0) ,(\mathbf{x} ,\mathbf{y}))g(x, y) →B((0, 0), (x, y)). This  gives  
\displaystyle g(\mathbf{x} ,\mathbf{y}) =( \Sigma _{i} a_{1}( x_{i}) \cdot y_{i} ,\Sigma _{i} a_{2}( x_{i}))g(x, y) = (Σia1(xi) · yi, Σia2(xi)). We substitute \displaystyle o=z=0o = z = 0 into the above 
expression. This gives us; 
\bUnALT{} B( g(\mathbf{x} ,\mathbf{y}) ,\ (\mathbf{x} ',\mathbf{y} ')) & =g(\mathbf{x} \oplus \mathbf{x} ',\mathbf{y} \oplus \mathbf{y} ')\eUnALT{} B(g(x, y), (x′, y′)) = g(x ⊕x′, y ⊕y′)
Showing that \displaystyle gg is streamable, interpreting the input as \displaystyle \left(\mathbb{R}^{2}\right)^{n}

R2n. We set  up  
the tail \displaystyle t:\mathbb{R}^{2}\rightarrow \mathbb{R}t : R2 →R as \displaystyle t( o,z) \mapsto o/b( z)t(o, z) →o/b(z). Therefore, \displaystyle t( g(\mathbf{x} ,\mathbf{y})) =( \Sigma _{i} a_{1}( x_{i}) \cdot y_{i}) /b( \Sigma _{i} a_{2}( x_{i})) =NC(\mathbf{x} ,\mathbf{y})t(g(x, y)) = (Σia1(xi) ·
yi)/b(Σia2(xi)) = NC(x, y) is streamable.
\qed⊓⊔
This lemma can be represented diagrammatically in Fig. 1 by asserting that 
a normalized-contraction (left-hand side) returns the same result for the same 
inputs (\equiv≡) as a decomposed looped operation (right-hand side), which partitions 
data and runs the accumulator. Note that \equiv≡does not assert that the computa-
tional cost of both sides are equivalent, merely that they preserve "correctness". 
As the computational costs may diﬀer, \equiv≡show how algorithms can be optimized 
to have desirable resource usage proﬁles. 
Theorem 1. Attention where the SoftMax is replaced by another normalization 
operation remains streamable, as shown in the diagram below; 
Fig. 2. 
An attention algorithm with a generic normalized contraction in place of 
SoftMax can have its xx-axis streamed at a lower-level, resulting in the s_xsx relabeling. 
Proof. From [ 4], we have the fusion theorem, which indicates how composed 
and broadcasted streamable operations maintain their streamability. It can be 
diagrammatically represented in Fig. 3. 
We start with a diagram of streamable normalized-contraction. Then, we 
apply the streaming theorems to add the \displaystyle Q/KQ/K contraction and broadcasting 
over the query (lime) and key/value axes (teal). These operations preserve the 
streamable axis, and therefore provide a streamable algorithm. Figure 4 indicates 
how the new algorithm can be fully expressed with a loop.
\qed⊓⊔
3
Spherical Attention 
Theorem 1 suggests we investigate forms of attention that overcome the exponen-
tial bottleneck by using an alternative normalization. Attention can be abstractly

6
V. Abbott et al.
Fig. 3. The fusion theorems indicate how modifying streamable operations changes the 
underlying accumulator, while maintaining streamability. 
Fig. 4. An attention algorithm using a generic normalization matches the left-hand 
side of the diagram. This matches Fig. 1, but with modiﬁcations similar to the fusion 
theorems, Fig. 3. This results in the fully expanded loop form on the right-hand side. 
interpreted as a weighted linear combination of value vectors to attain a context-
aware mixed vector output. This combination does not need a probabilistic inter-
pretation, summing weights to one, like SoftMax. There are various meaningful 
extensions of coeﬃcients depending on the domain. Here, we suggest spherical 
attention which uses an\displaystyle L^{2}L2-norm. Structurally, spherical attention has the advan-
tage of being signed, allowing values to add or subtract from others, covering 
additional domains by providing a signalling rather than probabilistic interpreta-
tion of weights. Furthermore, the exponential component of SoftMax bottlenecks 
throughput, especially at low quantization, which spherical attention overcomes 
by instead using fast FP16 cores. 
As it matches the template of Fig. 2, spherical attention can be eﬃciently 
implemented in a streamed manner. This reduces bandwidth requirements and 
removes the memory cost of storing QK^TQKT , of size y \times xy×x, which is quadratic in the 
standard case of self-attention. In contrast, a streamed attention algorithm only 
needs to store g_y \times s_xgy × sx data on each low-level chip (SM). These are conﬁgurable 
partition sizes, avoiding a memory limit with input size.

Accelerating Machine Learning Systems via Category Theory
7
3.1
Application to Gene Regulatory Networks 
Regulatory networks are graphs that consist of vertices and edges which indi-
cate up- or downregulation. Gene regulatory networks (GRNs) [ 6] are  a con-
crete example, wherein genes up- or downregulate others to exhibit complex 
behaviour. A machine learning model can assist in estimating an underlying 
model from gene expression data, however, this domain requires further archi-
tectural features. Signed attention, by allowing positive and negative signals 
between tokens, provides the core of the structure. 
Furthermore, genes can appear multiple times, meaning they have a bag 
structure. We set the token indexes to each refer to a speciﬁc gene, and mul-
tiplicity information for each gene. These data are provided as a model input 
to amplify their eﬀect. This information is weaved into the attention model by 
scalar multiplication of keys at each layer. Input data is the multiplicity of each 
gene, capturing a snapshot of a single cell's state, and outputs are some con-
crete property, such as the cell identity. Layers implement successive regulatory 
networks. 
The query-key attention scores are interpreted as the key gene's regulation of 
the query gene, which dictates how much of its value vector is to be added. Posi-
tive sign indicates up-regulation, while a negative sign indicates down-regulation. 
As the model is learned, genes' up- or downregulation of others is captured in 
\displaystyle QK^{T}QKT adjacency matrices. 
4
Low-Level Algorithm 
In Fig. 5, we show our low-level diagrammatic pseudocode. Variables are arrays 
of numbers at some quantization. They are represented on diagrams as a series 
of wires, labelled with each axis, with the lowest stride (the major axis) at the 
bottom. Wires are labelled with axis sizes, or tags indicating how an axis is split 
at various levels. Distinct variables are separated by dashed lines. We can tag 
these segments with the quantization used, and change quantization by adding 
an asterisk. Operations are denoted by pictograms, with their input variable 
size to the left and output to the right. Broadcasting involves drawing wires over 
operations, which naturally updates the size of input/output variables. Element-
wise operations have input and output sizes of \displaystyle 11. Axes of size \displaystyle 11 can be discarded, 
meaning elementwise operations appear as ﬂoating arrows. These rules encom-
pass the diagrams shown in previous ﬁgures. By revealing this parallelization 
information, deriving a low-level kernel is possible. 
We assign colors to operations depending on the level at which they oper-
ate. We need to move data to on-chip SMEM, indicated orange. This allows us 
to move data to the registers of threads (green), or fragment data for tensor 
core computations (teal). Both use the same register RMEM memory. However, 
tensor core operations require fragmented memory which makes general-purpose 
operations diﬃcult. As a result, we treat blue as a distinct memory space, requir-

8
V. Abbott et al.
ing transfer through orange to get to green. Data is recursively tiled at each level. 
GMEM (black wires) have full axes, which are split into blocks at the SMEM 
level indicated by \displaystyle g_{\square }g□, split into warps for tensor cores \displaystyle w_{\square }w□, and split between 
threads for registers \displaystyle t_{\square }t□. When data moves between levels with diﬀerent tile 
sizes, this represents partitioning or concatenating that axis. The main loop of 
the algorithm is encompassed within the parantheses, wherein \displaystyle s_{\overline{x}}sx is split. 
A dotted box indicates a linear operation which is split into a loop. Within a 
box, axes are relabelled. This is the per-iteration size. An input axis can change 
colors, indicating that it is loaded in chunks to lower levels, reducing memory 
usage. Tensor cores operate on tiles of the input data, so are typically split in such 
a manner. Tensor cores eﬀectively implement \displaystyle AB^{T}ABT , taking a \displaystyle M\times KM × K and \displaystyle N\times KN × K
matrix to produce a \displaystyle M\times NM × N matrix by contracting the \displaystyle KK dimension. Therefore, 
to implement \displaystyle SVSV we must transpose \displaystyle VV when loading into the tensor cores. 
This is indicated by the double transpose; the ﬁrst transpose is intentionally 
implemented while loading \displaystyle VV into tensor cores, and the second is implicit in 
how tensor cores operate and are diagrammed. Overall, this results in a double 
transpose, functionally equivalent to an identity. 
Fig. 5. Low-level diagram of spherical attention, adapting Fig. 4 to derive diagrammatic 
pseudocode. 
We can apply these rules to the diagram from Fig. 4, to derive the low-level 
pseudocode diagram in Fig. 5. Following the procedure above, we colored tensor 
core data and operations blue, and general-purpose thread operations green. 
Data is colored orange as it transfers through SMEM. Tile sizes for levels have 
been assigned by labelling. Linear operations, including matrix multiplication 
and summation, have been split via dotted boxes, reducing memory usage. We 
have also added a thick dotted line to indicate a necessary synchronisation. Axes 
sizes are indicated by superscripts.

Accelerating Machine Learning Systems via Category Theory
9
4.1
Results 
We wrote our implementation of the algorithm described in Fig. 5 in CUDA 
C++. For our testing and benchmarks, QQ, KK, and  VV had standard normal dis-
tributions. We chose multiples of 13824=108\times12813824 = 108 × 128 as our sequence lengths. 
This allows our algorithm to be eﬃciently wave-quantized on an A100. Head 
dimension 128128 and FP16 quantization matches the algorithm we expressed in 
Sect. 4. 
Fig. 6. Results comparing our algorithm to state-of-the-art FlashAttention 
For large sequence lengths, we achieve around 200 TFLOP/s, or 64% utiliza-
tion of an A100's 312 TFLOP/s of FP16 tensor core operations. This equates to 
a speedup of up to 3.60\times× compared to PyTorch. We also see that PyTorch runs 
into out-of-memory (OOM) errors at large sequence lengths. 
For comparison, we also ran FlashAttention-2 [ 9] on our system at the same 
sequence lengths. FlashAttention-2 is a state-of-the-art algorithm, with substan-
tial hyperparameter ﬁne-tuning and advanced features, such as warp-shuﬄing 
to avoid intermediate transfers to SMEM. Despite this, our algorithm achieves 
within 5% of their performance at a sequence length of 41.5K. Further limi-
tations of diagrams currently include the inability to express SMEM memory 
banking and to ensure numerical accuracy. 
4.2
Experiment Details 
We ran our benchmarking loop for 100 warmup runs and 100 timed runs at 
each sequence length. We benchmarked our results on DataCrunch 1A100.22V 
machine consisting of 22 cores allocated from an AMD Epyc 7642 48-core CPU, 
120GB RAM and 1 A100 SXM4 80GB GPU.

10
V. Abbott et al.
5
Conclusions 
In this work, we demonstrated how diagrammatic representations can be used 
both to derive structural properties of deep learning architectures and to guide 
the construction of eﬃcient low-level kernels. This stands in contrast to con-
ventional approaches, which struggle to bridge the gap between high-level model 
design and hardware-aware implementation. Our ﬁndings suggest that the repre-
sentational and analytical capabilities of neural circuit diagrams oﬀer a powerful 
foundation for the systematic improvement, and potentially the automated dis-
covery, of deep learning algorithms, a critical step toward achieving AGI. 
Looking ahead, we aim to automate the diagrammatic framework to enable 
the procedural generation of learning algorithms. This would allow neural cir-
cuit diagrams to support rapid analysis, optimization, and discovery at scale. 
Achieving this requires a formal data structure capable of syntactically encod-
ing diagrammatic constructs, currently relying on human visual interpretation. 
The categorical foundations of neural circuit diagrams make such a represen-
tation, and its corresponding compilation pipeline, both natural and feasible. 
Ultimately, automating the tools of neural circuit diagrams can form the basis 
of self-discovering and improving algorithms. 
References 
1. Abbott, V.: Neural circuit diagrams: robust diagrams for the communication, 
implementation, and analysis of deep learning architectures. Trans. Mach. Learn. 
Res. (2023) 
2. Abbott, V.: Robust diagrams for deep learning architectures: applications and 
theory. Honours Thesis, The Australian National University, Canberra, October 
2023 
3. Abbott, V., Xu, T., Maruyama, Y.: Category theory for artiﬁcial general intelli-
gence. In: Artiﬁcial General Intelligence: 17th International Conference, AGI 2024, 
Seattle, WA, USA, 13-16 August 2024, Proceedings, pp. 119-129, Springer, Berlin, 
Heidelberg, August 2024 
4. Abbott, V., Zardini, G.: FlashAttention on a Napkin: a diagrammatic approach to 
deep learning IO-awareness. Trans. Mach. Learn. Res. (2024) 
5. Abramsky, S., Tzevelekos, N.: Introduction to categories and categorical logic. vol. 
813, pp. 3-94 (2010). arXiv:1102.1313 [cs, math] 
6. Aduddell, R., Fairbanks, J., Kumar, A., Ocal, P.S., Patterson, E., Shapiro, B.T.: A 
compositional account of motifs, mechanisms, and dynamics in biochemical regu-
latory networks. Compositionality 6 (2024), May 2024. Publisher: Episciences.org 
7. John, C.: Baez and mike stay. Physics, topology, logic and computation: a rosetta 
stone. 813, 95-172 (2010). arXiv:0903.0340 [quant-ph] 
8. Cruttwell, G.S.H., Gavranović, B., Ghani, N., Wilson, P., Zanasi, F.: Categori-
cal foundations of gradient-based learning, July 2021. https://link.springer.com/ 
chapter/10.1007/978-3-030-99336-8_1 
9. Dao, T.: FlashAttention-2: faster attention with better parallelism and work par-
titioning. October 2023. https://openreview.net/forum?id=mZn2Xyh9Ec

Accelerating Machine Learning Systems via Category Theory
11
10. Dao, T., Fu, D.Y., Ermon, S., Rudra, A., Ré, C.: FlashAttention: fast and memory-
eﬃcient exact attention with IO-awareness, June 2022. https://openreview.net/ 
forum?id=H4DqfPSibmx 
11. Fong, B., Spivak, D.I., Tuyéras, R.: Backprop as Functor: a compositional per-
spective on supervised learning. In: 34th Annual ACM/IEEE Symposium on Logic 
in Computer Science, LICS 2019, Vancouver, BC, Canada, 24-27 June 2019, pp. 
1-13. IEEE (2019) 
12. Gavranović, B., Lessard, P., Dudzik, A., von Glehn, T., Araújo, J.G.M., Veličković, 
P.: Position: categorical deep learning: an algebraic theory of architectures, Febru-
ary 2024. https://dl.acm.org/doi/10.5555/3692070.3692679 
13. Paszke, A., et al.: PyTorch: an imperative style, high-performance deep learning 
library, December 2019. arXiv:1912.01703 
14. Selinger, P.: A survey of graphical languages for monoidal categories, August 2009 
15. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., Dao, T.: 
FlashAttention-3: fast and accurate attention with asynchrony and low-precision, 
July 2024. arXiv:2407.08608 
16. Vaswani, A., et al.: Attention is all you need. In: Guyon, I., von Luxburg, U., Ben-
gio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.) 
Advances in Neural Information Processing Systems 30: Annual Conference on 
Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, 
CA, USA, pp. 5998-6008 (2017) 
17. Wilson, P., Zanasi, F.: Categories of diﬀerentiable polynomial circuits for machine 
learning, May 2022. https://link.springer.com/chapter/10.1007/978-3-031-09843-
7_5 
18. Xu, T., Maruyama, Y.: Neural string diagrams: a universal modelling language for 
categorical deep learning. In: Goertzel, B., Iklé, M., Potapov, A., (eds.) Artiﬁcial 
General Intelligence, LNCS, pp. 306-315, Springer, Cham (2022) 
19. Zardini, G.: Co-design of complex systems: from autonomy to future mobility sys-
tems. Doctoral Thesis, ETH Zurich, 2023. Accepted 19 Dec 2023, T10:03:57Z

Bridging the Design and Intentional 
Stances: A Path Towards Interpretable 
AGI 
Vincent Abruzzo(B) 
New York City, USA 
vgabruzzo@gmail.com 
Abstract. Advanced Artiﬁcial Intelligence (AI), particularly deep 
learning, is faced with a signiﬁcant interpretability crisis. This paper 
frames this crisis using Daniel Dennett's hierarchy of stances, identify-
ing a dual challenge: design-stance opacity, where the internal mecha-
nisms of black-box architectures are impervious to human understand-
ing, and intentional-stance incoherence, in which systems heavily reliant 
on statistical pattern matching defy modeling as rational agents. This 
opacity and incoherence critically hinder progress towards trustworthy 
and capable Artiﬁcial General Intelligence (AGI). This paper proposes a 
proactive, two-pronged solution to address this problem. Firstly, we advo-
cate for advancing design-stance transparency by championing inherently 
interpretable hybrid algorithmic paradigms such as neurosymbolic sys-
tems, formal theorem proving, and structured knowledge representation. 
These approaches promote more robust, generalizable, and mechanis-
tically understandable reasoning within AI systems. Secondly, on the 
basis of this enhanced transparency, we can then meaningfully adopt the 
intentional stance. This involves applying insights from cognitive science 
to model these more structured and transparent AI systems as rational 
agents with emergent cognitive processes. By elevating interpretability 
from a reactive concern to a proactive design principle, this integrated 
strategy becomes crucial for understanding current AI capabilities and 
developing sophisticated, reliable AGI. 
Keywords: Artiﬁcial General Intelligence · Interpretability · 
Explainable AI (XAI) · Philosophy of AI · Dennett's Stances · 
Neurosymbolic AI · Cognitive Science · AI Safety 
1
Dennett's Stances: A Framework for AI Interpretability 
Daniel Dennett's hierarchy of stances oﬀers a robust framework for interpreting 
and predicting the behavior of complex systems [ 2]. It is particularly useful for 
addressing the challenge of AGI interpretability, especially through its design 
and intentional stances 1 [ 12]. 
1 The third stance, the physical stance, may also be critical for AGI, especially con-
cerning neuromorphic AI technology, but that is not the focus of this paper. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 12-16, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_2

Bridging the Design and Intentional Stances
13
The design stance explains a system based on its functional architecture and 
programmed mechanisms. Applied to AI, it asks: How is the system constructed? 
Which algorithms govern its operations? How do its components interact to 
yield outputs? This stance presumes that the system operates according to its 
design speciﬁcations. For traditional software, the design stance typically oﬀers 
lucid explanations, allowing us to trace algorithmic execution and map inputs 
to outputs through well-deﬁned processes. 
In contrast, the intentional stance treats a system as a rational agent pos-
sessing representational states such as beliefs and desires. Instead of dissecting 
internal mechanics, it attributes mental states and predicts behavior by reason-
ing about what a rational agent with speciﬁc goals would do. We adopt this 
stance colloquially when describing an AI as "thinking" or "wanting." The inten-
tional stance oﬀers cognitive economy; it is often simpler to assert that "the 
chess program aims to protect its queen" than to detail its precise algorithmic 
computations. 
Crucially, these stances represent distinct explanatory paradigms. The design 
stance clariﬁes through mechanism; the intentional stance clariﬁes through ratio-
nality and representational states. For genuine AI interpretability, both stances 
should ideally align: designed functionality must support meaningful intentional 
explanations, and attributed intentional states should accurately reﬂect underly-
ing design. Historically, simpler AI systems like rule-based expert systems exhib-
ited a clearer congruence between these stances, with programmers explicitly 
encoding both mechanism and purpose. However, contemporary deep learning 
has profoundly disrupted this relationship [ 9], creating an interpretability gap 
that imperils our understanding and trust in sophisticated AI, potentially obscur-
ing pathways to AGI. 
2
The Dual Interpretability Challenge of Deep Learning 
Modern deep learning, despite its capabilities, presents formidable interpretabil-
ity challenges, impacting both stances. From the design stance, deep learn-
ing systems are signiﬁcantly opaque. While their architecture and training are 
understood, how speciﬁc knowledge is encoded within millions of parameters 
remains largely inscrutable. Unlike traditional software, deep learning systems 
autonomously develop internal representations. Post-training, the causal chain 
linking inputs to outputs is often untraceable. Post-hoc explanation methods 
oﬀer only partial insights [ 6], failing to resolve core opacity and hindering princi-
pled iteration towards general capabilities. The call to move beyond explaining 
black boxes towards inherently interpretable models grows louder in high-stakes 
decisions [ 13]. 
Simultaneously, deep learning prevents eﬀective application of the intentional 
stance. We cannot reliably attribute rational beliefs and goals to these systems 
as their behavior often lacks the requisite coherence. Large language models 
(LLMs), for example, may generate contradictory responses, stemming from 
statistical pattern generation rather than explicit beliefs or goals [ 15]. These

14
V. Abruzzo
systems are not rational agents in the typical sense assumed by the intentional 
stance. Claiming an LLM "thinks" often misaligns with its mechanisms, confus-
ing understanding and limiting progress towards robust, goal-directed general 
intelligence. [ 3]. 
This dual challenge creates a critical interpretability gap. We can neither 
fully comprehend these systems via design nor can we reliably predict behavior 
through attributed intentional states. This is perilous as AI assumes critical 
societal roles and as we strive toward AGI. Without robust interpretability, 
verifying safety, identifying biases, establishing trust, assigning accountability, 
and fostering human-AI collaboration become profoundly diﬃcult, obscuring 
the path to adaptable general intelligence. 
3
A Two-Pronged Strategy: Interpretability as a Catalyst 
for AGI 
To bridge this gap and accelerate AGI progress, we propose a synergistic, two-
pronged approach, viewing interpretability as a catalyst for more capable, robust, 
and general AI. 
3.1
Enhancing Design-Stance Transparency for Deeper 
Understanding and Capability 
We must expand beyond pure deep learning to hybrid architectures prioritizing 
inherent transparency, contributing to AGI desiderata like robust reasoning and 
generalization: 
- Neurosymbolic approaches integrate neural pattern recognition with sym-
bolic reasoning [ 4]. This fusion allows explicit, inspectable knowledge repre-
sentation and logical operations, crucial for structured reasoning-a hallmark 
of general intelligence and better generalization. 
- Formal theorem proving and veriﬁable computation ensure system 
derivations are logically sound and inspectable [ 7]. AI that grounds its con-
clusions in veriﬁable proofs fosters rigorous, reliable reasoning, key for AGI 
[ 14]. 
- Knowledge graphs and structured representations encode information 
in human-readable and machine-processable formats [ 11]. This supports clear 
reasoning pathways and integration of diverse knowledge, enhancing inter-
pretability and capacity for sophisticated inference vital for AGI. 
Hybrid architectures incorporating these rationalist and representationalist 
paradigms can promote the emergent cognitive phenomena of advanced AI in a 
way that supports the next step of this proposed approach: adopting the inten-
tional stance towards AI systems.

Bridging the Design and Intentional Stances
15
3.2
Deepening Intentional Understanding: A Cognitive Science 
Approach Enabled by Design Transparency 
With enhanced design-stance transparency, we can approach the intentional 
stance with greater conﬁdence and empirical rigor. Instead of attributing inten-
tional states based solely on behavior or attempting to engineer human-like 
cognition directly, we can leverage cognitive science methodologies [ 1] to inves-
tigate emergent mental processes in transparently designed AI. This shifts focus 
from mimicking human cognition by design to understanding unique AI cog-
nitive architectures arising from principled development, grounding intentional 
attributions scientiﬁcally. This allows us to: 
- Investigate Emergent Cognitive Processes Empirically: Inspectable 
AI internals become amenable to study using methods from cognitive psy-
chology [ 5]. We can probe how systems develop and use internal representa-
tions analogous to causal models, forming an authentic basis for taking an 
intentional stance. Recent work at Anthropic exempliﬁes the potential of this 
approach [ 8]. 
- Probe for Functional Analogues of Higher-Order Cognition: Hybrid 
architectures allow the use of experimental paradigms from developmental 
and social psychology to test for the functional equivalents of belief, desire, 
and other representational states [ 7]. 
- Analyze AI Decision-Making with Cognitive Tools: Transparent AI 
decision processes can be analyzed using cognitive science insights and 
frameworks (e.g., heuristics, bias, uncertainty response) to understand their 
emergent rationality, making their intentions and choices more predictable. 
Insights from the social sciences can also enrich our understanding here [ 10]. 
By utilizing the tools of cognitive science on hybrid AI systems, our inten-
tional stance becomes an investigative endeavor, moving beyond surface inter-
pretations to a scientiﬁcally grounded understanding of emergent cognitive func-
tions. This is crucial for alignment, safety, and discerning pathways to genuinely 
intelligent capabilities. 
4
Conclusion: Interpretable AGI - A Path Forward 
The pursuit of interpretable AGI is integral to achieving advanced AI capa-
bilities. The Dennettian framework highlights how current deep learning 
falls short in providing understandable mechanisms (design stance) or coher-
ent, attributable representational states (intentional stance). Our two-pronged 
strategy-enhancing design-stance transparency through hybrid neurosymbolic 
formal methods and representational systems such as knowledge graphs, 
and strengthening and grounding intentional-stance applicability via cognitive 
science-directly addresses these problems. 
Crucially, architectural and cognitive enhancements for interpretability often 
mirror qualities needed for general intelligence. Systems with explicit knowledge

16
V. Abruzzo
representation, logical reasoning, and causal modeling are not only more inter-
pretable and safe but also inherently more powerful and adaptable. Bridging the 
design and intentional stances not only closes the interpretability gap, but it also 
brings us closer to trustworthy AGI. 
References 
1. Binz, M., Schulz, E.: Using cognitive psychology to understand gpt-3. Proc. Natl. 
Acad. Sci. 120(6), e2218523120 (2023) 
2. Dennett, D.C.: The Intentional Stance. MIT Press (1989) 
3. Doshi-Velez, F., Kim, B.: Towards a rigorous science of interpretable machine learn-
ing. arXiv preprint arXiv:1702.08608 (2017) 
4. Garcez, A.d., Lamb, L.C.: Neurosymbolic ai: The 3rd wave. Artif. Intell. Rev. 
56(11), 12387-12406 (2023) 
5. Greenblatt, R., et al.: Alignment faking in large language models (2024), https:// 
arxiv.org/abs/2412.14093 
6. Hassija, V., et al.: Interpreting black-box models: a review on explainable artiﬁcial 
intelligence. Cogn. Comput. 16(1), 45-74 (2024) 
7. Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines 
that learn and think like people. Behav. Brain Sci. 40, e253 (2017) 
8. Lindsey, J., et al.: On the biology of a large language model. Trans. Circuits Thread 
(2025), https://transformer-circuits.pub/2025/attribution-graphs/biology.html 
9. Marcus, G.: Deep learning: a critical appraisal. arXiv preprint arXiv:1801.00631 
(2018) 
10. Miller, T.: Explanation in artiﬁcial intelligence: insights from the social sciences. 
Artif. Intell. 267, 1-38 (2019) 
11. Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X.: Unifying large language 
models and knowledge graphs: a roadmap. IEEE Trans. Knowl. Data Eng. 36(7), 
3580-3599 (2024) 
12. Perez-Osorio, J., Wykowska, A.: Adopting the intentional stance toward natural 
and artiﬁcial agents. Philos. Psychol. 33(3), 369-395 (2020) 
13. Rudin, C.: Stop explaining black box machine learning models for high stakes 
decisions and use interpretable models instead. Nat. Mach. Intell. 1(5), 206-215 
(2019) 
14. Seshia, S.A., Sadigh, D., Sastry, S.S.: Toward veriﬁed artiﬁcial intelligence. Com-
mun. ACM 65(7), 46-55 (2022) 
15. Zhao, H., et al.: Explainability for large language models: a survey. ACM Trans. 
Intell. Syst. Technol. 15(2) (2024). https://doi.org/10.1145/3639372

Prospective Learning in Retrospect 
Yuxin Bai1 
, Cecelia Shuai1 
, Ashwin De Silva1 
, Siyu Yu1 
, 
Pratik Chaudhari2(B) 
, and Joshua T. Vogelstein1(B) 
1 Johns Hopkins University, Baltimore, USA 
{ybai31,xshuai3,ldesilv2,jovo}@jhu.edu 
2 University of Pennsylvania, Philadelphia, USA 
pratikac@seas.upenn.edu 
Abstract. In most real-world applications of artiﬁcial intelligence, the 
distributions of the data and the goals of the learners tend to change over 
time. The Probably Approximately Correct (PAC) learning framework, 
which underpins most machine learning algorithms, fails to account for 
dynamic data distributions and evolving objectives, often resulting in 
suboptimal performance. Prospective learning is a recently introduced 
mathematical framework that overcomes some of these limitations. We 
build on this framework to present preliminary results that improve 
the algorithm and numerical results, and extend prospective learning to 
sequential decision-making scenarios, speciﬁcally foraging. Code is avail-
able at: https://github.com/neurodata/prolearn2. 
Keywords: Distribution Shifts · Out-of-Distribution Generalization · 
Learning Theory · Sequential Decision-Making 
1
Introduction 
Learning involves updating decision rules or policies, based on past experiences, 
to improve future performance. The Probably Approximately Correct (PAC) 
learning [ 1, 2] framework has led to the development of learning algorithms that 
provably minimize the risk (expected loss) over unseen future samples during 
inference. When proving such guarantees, PAC learning assumes that data is 
independently and identically (iid) distributed according to a ﬁxed distribution 
at training and inference time. 
While this assumption has been useful, it is rarely held true in practice. In 
fact, the future is more likely to be diﬀerent from the past as distributions of data 
and goals of the learner may change over time. Therefore, the true hypothesis 
can be time-variant, and classical PAC learning does not address this situation. 
Although sub-disciplines such as transfer learning [ 3], continual/lifelong learn-
ing [ 4- 7], online learning [ 8], meta-learning [ 9, 10], sequential decision-making 
[ 11, 12], forecasting [ 13], reinforcement learning [ 14- 17], out-of-distribution gen-
eralization [ 18] have introduced attractive solutions that retrospectively adapt 
to distributions that change over time, they often fail to anticipate and gener-
alize to future even when data evolves in simple but predictable ways as shown 
in [ 19, 20]. 
Y. Bai, C. Shuai and A. De Silva—Equal Contribution. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 17-29, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_3

18
Y. Bai et al.
Prospective learning [ 19, 20] is a recently developed mathematical framework 
to bridge this gap. Instead of data arising from a ﬁxed distribution, prospective 
learning assumes that data is drawn from a stochastic process, that the loss 
considers the future, and that the optimal hypothesis may change over time. A 
prospective learner uses samples received up to some time t \in \naturalst ∈N to output an 
inﬁnite sequence of predictors, which it uses for making predictions on data at 
all future times t' > tt′ > t. An exhaustive comparison between prospective learning 
and related sub-ﬁelds of machine learning literature is provided in De Silva et 
al. [ 19]. 
We build on the prospective learning framework and introduce several pre-
liminary results. The rest of the paper is organized as follows; Sect. 2 provides 
a concise summary of the prospective learning framework, Sect. 3 presents sev-
eral empirical observations on deep learning-based prospective learners, Sect. 4 
introduces a decision-tree based prospective learner and demonstrates its per-
formance, and ﬁnally, Sect. 5 introduces prospective foraging, showing that the 
prospective learning framework extends beyond supervised learning and per-
forms competitively with reinforcement learning in preliminary results. 
2
Preliminaries 
Let input and output be denoted by x_t \in \mc{X}xt ∈X and y_t \in \mc{Y}yt ∈Y, respectively. Let z_t = (x_t, y_t)zt =
(xt, yt). We will ﬁnd it useful to denote the observed data, z_{\leq t} = {z_1, \dots, z_t}z≤t = {z1, . . . , zt}, 
and the unobserved data, z_{>t}z>t. In contrast to PAC learning, tt is not just a dummy 
variable, but rather, indexes time. We therefore deﬁne the data triple (x_t,y_t,t)(xt, yt, t), 
and augment the input space to include the time of the input, \mc{X} \leftarrow \mc{X} \times \mc{T}X ←X × T . 
Consider an inﬁnite sequence of hypotheses h \equiv (h_1,\dots,h_t,h_{t+1},\dots)h ≡(h1, . . . , ht, ht+1, . . . ) where 
h_t : \mc{X} \to \mc{Y}ht : X →Y. The hypothesis class \mc{H}H is the space that contains such sequences. 
With a slight abuse of notation, we will refer to a sequence of hypotheses hh as a 
hypothesis, where each element of this sequence h_t: \XX \mapsto \YYht : X →Y. 1
Loss The instantaneous loss, \ell( h(x),y)ℓ(h(x), y), is a map from  \mc{Y} \times \mc{Y}Y × Y to \mathbb{R}R. In all  
real-world problems we care about the integrated future loss. Let w(i)w(i) be a non-
increasing non-negative weighting function that sums to one, that is \sum_i w(i) = 1
i w(i) = 1
and 0 \leq w(i) \leq 1\, \forall i0 ≤w(i) ≤1 ∀i. We thus deﬁne prospective loss as 
\bALT{}\label{eq:ellspsbar} \bar{\ell}(h, z_{>t}) = \sum_{s>t} w(s-t) \ell(h(x_s),y_s)\eALT{} ¯ℓ(h, z>t) =

s>t
w(s −t)ℓ(h(xs), ys)
(1) 
which is the weighted cumulative loss over all the future data. 2
Taking the expectation of Eq. (1) over the future conditioned on the observed 
data z_{\leq t}z≤t, we arrive at the prospective risk at time tt, 
\bALT{} R_t(h) = \E \sbr{\bar \ell_t(h,Z) \mid z_{\leq t}} = \int \bar \ell_t(h,Z)\,\text{d}\,{\P_{Z \mid z_{\leq t}}}. \label{eq:prospectivespsrisk}\eALT{} Rt(h) = E
¯ℓt(h, Z) | z≤t

=

¯ℓt(h, Z) d PZ|z≤t.
(2)
1 One could also think of prospective learning as using a single time-varying hypothesis 
h: \naturals \times \XX \mapsto \YYh : N × X →Y, i.e., the hypothesis takes both time and the datum as input to make 
a prediction. 
2 if we letw(i)w(i) be a constant function ofii, we recover the classical loss in PAC learning. 

Prospective Learning in Retrospect
19
3 We consider a family of stochastic processes \mc{Z}Z to be strongly prospective-
learnable if there exists a learner that likely returns an approximately optimal 
hypothesis (with a risk close to the Bayes risk R_t^\astR∗
t ) after observing enough past 
data z_{\leq t}z≤t from any process Z \in \mc{Z}Z ∈Z. Theorem 1 from De Silva et al. [ 19] guaran-
tees that under certain general assumptions, a slightly modiﬁed Empirical Risk 
Minimizer (ERM) a learner returning the hypothesis 
\bALT{} \hat{h} = \argmin_{h \in \mc{H}_t} \sum_{t'>0}^t \bar{\ell}(h,z_{>t'}) = \argmin_{h \in \mc{H}_t} \sum_{t' > 0 }^t \sum_{s> t'} w(s-t') \ell(h_{t'}(x_s),y_s). \label{eq:prospectivespserm}\eALT{} ˆh = arg min
h∈Ht
t

t′>0
¯ℓ(h, z>t′) = arg min
h∈Ht
t

t′>0

s>t′
w(s −t′)ℓ(ht′(xs), ys).
(3) 
is a strong prospective learner for a ﬁnite family of stochastic processes under 
certain assumptions. We refer to this learner as Prospective ERM. 
To implement prospective ERM in practice, one may modify a predictor (e.g. 
a multi-layer perceptron) to take (\varphi(s), x_s)(ϕ(s), xs) as the input and train it to predict 
the label y_sys, where  \varphi(s)ϕ(s) is a suitable embedding on the time ss. We refer to 
this predictor as Prospective-MLP. Inspired by the positional encoding of the 
Transformer [ 21], De Silva et al. [ 19] have used the time-embedding, 
\bALT{} \varphi_{f}(t) = (\sin(\omega_1 t), \dots, \sin(\omega_{d/2} t), \cos(\omega_1 t), \dots, \cos(\omega_{d/2} t)), \label{eq:fourierspsembed}\eALT{} ϕf(t) = (sin(ω1t), . . . , sin(ωd/2t), cos(ω1t), . . . , cos(ωd/2t)),
(4) 
where $\omega_i = \pi / i$ωi = π/i for $i = 1, \dots, d/2$i = 1, . . . , d/2. It is shown that the prospective-MLP con-
verges to the Bayes risk on certain stochastic processes over synthetic and image 
data. 
Prospective Learning Scenarios 
Depending on the nature of the stochastic process, one can consider 4 scenarios 
of prospective learning; (1) Data is independent and identically distributed, (2) 
Data is independent but not identically distributed, (3) Data is neither indepen-
dent nor identically distributed (e.g. Markov Processes), and (4) Future depends 
on the current prediction (Stochastic Decision Processes). Scenario 1 puts us 
back in the PAC learning setting. Scenario 4, on the other hand, is arguably a 
special case of Scenario 3 and has implications for reinforcement learning and 
control. As we will introduce in Sect. 5, prospective foraging is closely related to 
this scenario. 
Example Stochastic Processes 
Here we describe three stochastic processes we will consider in the experiments 
outlined in Sects. 3 and 4. 
First, consider two binary classiﬁcation distributions ("tasks") (see Fig. 1 
(Left)). The inputs for both tasks are drawn from a uniform distribution on
3 This is a slight abuse of notation, because previously\ellℓmapped from a hypothesis and 
data corpus, but now we are saying that it maps from a hypothesis and a collection 
of random variables. We have used the shorthand \E[Y \mid x]E[Y | x] for \E[Y \mid X = x]E[Y | X = x]. Note  
that \mathbb{E}[Z | z_{\leq t}]E[Z|z≤t] is equivalent to \mathbb{E}[Z_{>t} | z_{\leq t}]E[Z>t|z≤t] because the past zz's are given. 

20
Y. Bai et al.
Fig. 1. Pictorial depictions of 3 types of stochastic processes considered in our exper-
iments. (Left) Periodic Process, (Middle) Linear process, and (Right) Hierarchical 
hidden Markov Process. The periodic and linear processes belong to scenario 2 whereas 
the hierarchical hidden Markov process is an instance of scenario 3. 
the set [-2, -1] \cup [1, 2][−2, −1] ∪[1, 2]. Ground-truth labels correspond to the sign of the input 
for Task 1, and the negative of the sign of the input for Task 2. The process 
switches the two tasks every 1010 time steps, resembling a reversal learning prob-
lem. We refer to this as the "periodic" process. 
The second is a stochastic process whose marginal distribution at time tt
is deﬁned as follows: The input x_txt is drawn from a uniform distribution over 
[\epsilon t+10, \epsilon t+11] \cup [\epsilon t-10, \epsilon t-11][ϵt + 10, ϵt + 11] ∪[ϵt −10, ϵt −11] and its label y_tyt is 00 if x_t > txt > t and 11 otherwise. 
In other words, it is a process where the task is hiking up a slope with a small 
gradient \epsilonϵ (see Fig. 1 (Middle)). This process yields an inﬁnite number of tasks, 
in contrast to the ﬁrst one, where there are ﬁnite (two) tasks. Thus, we refer to 
it as the "inﬁnite task process". 
The third and ﬁnal process includes four tasks that are created using 2-
dimensional inputs as shown in Fig. 1 (Right). After every 10 time-steps, a diﬀer-
ent Markov chain would govern the transitions among tasks (one Markov chain 
for tasks 1 and 2, and another for tasks 3 and 4 as illustrated in the ﬁgure). 
Therefore, the data distribution is eﬀectively distributed according to the hier-
archical hidden Markov model. We refer to it as the "dependent structured task 
process". 
Relationship between Continual and Prospective Learning 
Although continual and prospective learning both involve learning over sequences 
of tasks, they diﬀer fundamentally in their objectives and assumptions. As for-
malized in Sect. 2, the goal of a prospective learner is to perform well on future 
tasks. In contrast, the objective in continual learning, though typically less for-
mally deﬁned, is to maintain good performance on previously seen tasks and 
avoid catastrophic forgetting. Continual learning often assumes a task-aware set-
ting, where the learner receives a batch of data per task along with the task iden-
tity. Prospective learning, by contrast, does not assume access to such task labels 
or boundaries. A notable exception is task-agnostic online continual learning [ 22], 
which operates under similar assumptions to prospective learning. However, De 
Silva et al. [ 19] shows that such methods still fail to improve upon chance-level 
prospective risk when learning the periodic process above. Furthermore, contin-
ual learning benchmarks typically involve sequences of unrelated tasks without 
any predictable structure, whereas prospective learning is meaningful when the

Prospective Learning in Retrospect
21
tasks evolve over time in a predictable manner. To address this gap, we introduce 
simple yet representative benchmarks designed to assess the ability of learners to 
generalize to future tasks. For additional experiments and a deeper comparison 
of prospective learning with related paradigms, we refer the reader to De Silva 
et al. [ 19]. 
Training and Evaluating Learners 
The next two sections present experiments including Prospective-MLPs and 
Prospective-Trees, and a time-agnostic Follow-the-Leader (FTL) baseline that 
minimizes empirical risk over all past data without incorporating time as an 
input. When training and evaluating these learners, we roughly follow the steps 
detailed in the Sect. 6 of De Silva et al. [ 19]. 
3
Several Empirical Observations on Prospective-MLPs 
Prospective-MLP Prevails Under Heterogeneous Sampling 
The experiments in De Silva et al. [ 19] assume homogeneous past data where 
exactly one sample is received at each time step. This assumption overlooks 
more realistic scenarios where samples may be missing or multiple samples may 
be available per time step. To model the heterogeneity of sampling, we assume 
that the number of samples received from the process at each time step is dis-
tributed according to a Poisson distribution with \lambda = 1λ = 1. We train Follow-the-
Leader (FTL) and prospective-MLP on data collected this way and compare 
them against their counterparts trained on homogeneous data (see Fig. 2). It is 
evident that prospective-MLP manages to secure a good prospective risk regard-
less of how the data is sampled. 
Fig. 2. Instantaneous (top) and prospective (bottom) risks of Follow-the-Leader 
(FTL, blue) and Prospective-MLP (P-MLP, red) trained on homogeneously (lighter 
shade) and heterogeneously (darker shade) sampled data from the periodic process. 
Homogeneous sampling is where you get exactly one sample each time step. In hetero-
geneous sampling, there can be missing samples and/or multiple samples available per 
time step.

22
Y. Bai et al.
Prospective-MLP Prevails in Inﬁnite Task Scenarios 
Experiments in De Silva et al. [ 19] are mostly based on stochastic processes 
that include several tasks that periodically switch between each other. On such 
processes, it has been shown that the Prospective-MLPs equipped with the time-
embedding deﬁned by Eq. (4) are able to achieve a low prospective risk and 
generalize over the future. It is intuitive that a time-embedding comprised of 
Fourier basis functions is appropriate for a periodic process assuming that it 
contains the function with the true switching frequency. 
However, aside from capturing periodic patterns, the utility of a Fourier-
based time embedding is likely to be limited. To demonstrate this, we consider 
the linear process (see Sect. 2 and Fig. 1 (Middle)). There, we get a new task 
at each time and the task evolves according to a linear trend in time. The 
prospective learner must exploit this trend in order to generalize over the future. 
To illustrate the eﬀect the choice of time-embedding has on the learner, we 
train two Prospective-MLPs, one with the Fourier embedding from Eq. (4) and  
the other with a time-embedding based on monomial basis functions given by 
\varphi_{m}(t) = (t, t^2, t^3, \dots, t^d)ϕm(t) = (t, t2, t3, . . . , td). We repeat the same routine with the periodic process 
(see Sect. 2 and Fig. 1 (Left)) as well. 
Fig. 3. Prospective risk of Follow-the-Leader (FTL), and Prospective-MLP with 
Fourier embeddings, and Prospective-MLP with monomial embeddings on periodic 
(Right top) and linear (Right bottom) processes. Prospective-MLP with Fourier embed-
dings performs best on the periodic process, whereas the variant with monomial embed-
dings achieves the best performance on the linear process. 
As expected, the Prospective-MLP with the monomial embedding outper-
forms other learners trained on the samples from the linear process with an 
inﬁnite number of tasks. However, it fails to perform well on the periodic pro-
cess, where the Fourier embedding is more appropriate. The key takeaways from 
this experiment is that prospective learning can perform well even when there 
are an inﬁnite number of tasks, but to do so, it must leverage an appropriate 
time-embedding for the underlying process. 
Prospective-MLPs can be Trained in a Streaming or Online Manner 
So far, we have considered scenarios where Prospective-MLPs are trained in an 
oﬄine or batched setting, using a ﬁxed dataset of past samples drawn from the 
process. This requires the learner to have access to a memory where it may 
store the dataset used for training. When there are constraints on the memory

Prospective Learning in Retrospect
23
Fig. 4. Prospective risk of the learners that are trained in an online manner on data 
from the periodic process. 
allowed for the learner, batched-learning is no longer feasible. Here, we consider 
an extreme but realistic setting, where the learner will see the sample drawn from 
the process at time tt only once. Therefore, the learner is expected to perform a 
parameter update after observing each new sample. In Fig. 4, we plot the risk of 
a Prospective-MLP that was trained in this manner over the data sampled from 
the periodic process. Notice that the batch-trained Prospective-MLP converges 
to the optimal risk within approximately 250 samples (see Fig. 2 (Bottom)), 
whereas its online-trained counterpart requires nearly 10 times as many samples 
to reach the same level of performance. This is expected as the model is exposed 
to the same training datum more than once during batched-training. 
4
Prospective Forests 
4.1
Motivation and Background 
Decision forests, including Random Forests (RF) and Gradient Boosted trees 
(GBTs) continue to empirically outperform deep learning methods on tabular 
and vector-valued data [ 23, 24] while oﬀering superior interpretability. However, 
most existing results are for problems under the assumptions of the PAC frame-
work [ 25]. In addition to the strong theoretical guarantees, including univer-
sal consistency [ 25- 27], decision forests can be eﬃciently trained in parallel or 
sequentially [ 28]. Motivated by these strengths, we extend decision forests to the 
prospective learning regime. Our preliminary results indicate that Prospective 
Decision Trees perform comparably to the deep learning-based Prospective-MLP. 
Conventional CART (Classiﬁcation and Regression Trees), as deﬁned in 
Breiman et al. [ 28], is a greedy algorithm that builds a hierarchical structure 
through recursive binary splitting. We deﬁne a prospective variant of CART in 
the following. 
Deﬁnition 1 (Prospective CART). Consider \mathcal{Z}Z to be a ﬁnite family of 
stochastic processes. Suppose there is an increasing sequence of hypothesis class 
\HH_1 \subseteq \HH_2 \subseteq \dotsH1 ⊆H2 ⊆. . . with each \HH_t \subseteq (\YY^\XX)^\naturalsHt ⊆(YX )N. \HH^{tree}_tHtree
t
is  a subset of  \HH_tHt and is the 
collection of all hypothesis h \in \HH^{tree}_th ∈Htree
t
returned by decision trees regressor. We 
deﬁne Prospective CART as the learner minimizes the empirical risk over past 
data z {\leq} tz≤t, i.e.: 
\bALT{} \hat h = \argmin_{h \in \HH^{tree}_t} \sum_{t' > 0 }^t \sum_{s> t'} w(s-t') \ell(h_{t'}(x_s),y_s), \label{eq:PTR}\eALT{} ˆh = arg min
h∈Htree
t
t

t′>0

s>t′
w(s −t′)ℓ(ht′(xs), ys),
(5)

24
Y. Bai et al.
where w(i)w(i) is non-increasing non-negative weighting function deﬁned in Sect. 2 
Naturally, random forest is a randomized ensemble of decision trees; anal-
ogously, aggregating prospective trees yields a prospective forest. However, 
another ensemble technique, GBTs often outperform RF [ 29, 30] in certain PAC 
settings. Therefore, we also introduce a prospective version of GBTs. 
Deﬁnition 2 (Prospective Gradient Boosted Trees). Moreover, based on 
the deﬁnition of prospective forests, we deﬁned prospective gradient boosting trees 
(GBTs) also as an ensemble of trees h^B_t = \sum_{b=1}^B w^b_t h^b_t(x;\Theta^b_t)hB
t = B
b=1 wb
thb
t(x; Θb
t). Unlike the prospec-
tive forests deﬁned above, each tree grows independently. In prospective GBTs, 
the parameters \Theta^b_tΘb
t and the weights w^b_twb
t are iteratively updated by minimizing the 
empirical risk. This iterative process ensures that each step improves the model 
by reducing the residual error over the past data z_{\leq t}z≤t. i.e. 
 \bUnALT{}\hat h = \argmin_{h^B \in lin(\HH^{tree}_t)} \sum_{t' > 0 }^t \sum_{s> t'} w(s-t') \ell(h_{t'}(x_s),y_s), \ \ \text{subject to } h^B = \sum_{b=1}^B w^b_t h^b_t(x;\Theta^b_t) \label{eq:prospectivespsGBT},\eUnALT{} ˆh =
arg min
hB∈lin(Htree
t
)
t

t′>0

s>t′
w(s −t′)ℓ(ht′(xs), ys),
subject to hB =
B

b=1
wb
thb
t(x; Θb
t),
where lin(\HH^{tree}_t)lin(Htree
t
) is the set of all linear combinations of functions in \HH^{tree}_tHtree
t
. 
4.2
Preliminary Results 
We consider data drawn from a periodic process and the Hierarchical Markov 
Process described in Sect. 2 and illustrated in Fig. 1. To each data point from 
these processes, we append two additional noise dimensions sampled from a 
standard normal distribution, ensuring the presence of both informative and 
noisy components. 
As discussed in Sect. 2, we implement Prospective-GBTs by giving it 
(x_s, \varphi(s))(xs, ϕ(s)) as input and training it to predict the label y_sys, where we use the 
time-embedding deﬁned in Eq. (4). In Fig. 5, we compare the prospective risks 
between several learners including Prospective-GBTs and Prospective-MLP. 
5
Prospective Foraging 
5.1
Motivation and Background 
Foraging—searching for food, water, and mates—is vital for survival and repro-
duction, relying on predictions of environmental ﬂuctuations and resource avail-
ability. However, standard machine-learning and reinforcement learning (RL) 
methods—whether minimizing past errors or requiring extensive trial-and-
error—are ill-suited to the real-time, single-lifetime risks of foraging. To bridge 
this gap, we introduce the prospective learning framework in which agents 
project into possible future states under a one-life constraint. We implement it in 
a simpliﬁed OpenAI Gym foraging scenario [ 31], compare standard actor-critic 
RL agents [ 32, 33] to prospectively augmented versions. Finally, we show that 
prospective learning framework can be extended beyond the supervised learning 
problem and that it outperforms an actor critic RL algorithm in a foraging task.

Prospective Learning in Retrospect
25
Fig. 5. Prospective risk of Prospective-GBTs (red), Prospective-MLP (green) and 
Time-agnostic Gradient Boosted trees (Plain-GBTs, blue) across two scenarios where, 
(1) data is independent but not identically distributed (Left (Color ﬁgure online)), 
and (2) data is neither independent nor identically distributed (Right). In both cases, 
the risk of Prospective-GBTs and Prospective-MLP approach the Bayes risk, with 
Prospective-GBTs converging faster. In contrast, the time-agnostic GBTs do not con-
verge consistently. For comparison, the chance prospective risk is 0.50.5 in the left panel 
and 0.30.3 in the right panel. (Color ﬁgure online) 
At each discrete time step $t\in\mathbb{N}$t ∈N the agent observes $x_t=(s_t,a_{t-19:t})\in\mathcal X$xt = (st, at−19:t) ∈X— 
its current spatial location and the last $20$20 actions—and receives a scalar reward 
$y_t\in\mathcal Y$yt ∈Y. The only data it may inspect is the trajectory $z_{\le t}={(x_s,y_s)}_{s=1}^t$z≤t = {(xs, ys)}t
s=1. Stan-
dard on-policy control maximizes the generalized advantage estimator (GAE) 
$\hat A^{\text{GAE}}_t$ ˆAGAE
t
. For notational uniformity we instead minimize the loss $\ell(t,\hat y_t,y_t)=-\hat A^{\text{GAE}}_t$ℓ(t, ˆyt, yt) =
−ˆAGAE
t
. We deﬁne a prospective forager that minimizes the sum of weighted 
cumulative instantaneous losses on the observed trajectory. 
Deﬁnition 3 (Prospective Forager). Consider \mathcal{Z}Z to be a ﬁnite family of 
stochastic processes. Let \mathcal H_1\subseteq\mathcal H_2\subseteq\cdots \subseteq(\mathcal Y^{\mathcal X})^{\mathbb N}H1 ⊆H2 ⊆· · · ⊆(YX )N be an expanding hypoth-
esis class. We employ an actorâĂŞcritic architecture: the critic evaluates any 
$h\in\mathcal H_t$h ∈Ht, whereas the actor can selects from $\mathcal H^{\text{actor}}_t\subseteq\mathcal H_t$Hactor
t
⊆Ht. We deﬁne Prospective 
Forager as the learner that minimizes the empirical risk over past data z_{\leq} tz≤t, i.e.: 
\bALT{} \hat h = \argmin_{h \in \HH^{actor}_t} \max_{u_{it} \leq m \leq t}\frac{1}{m} \sum_{s=1}^m \frac{1}{m-s+1} \sum_{r=s}^m \ell(s, h_s(x_s), y_s) \label{eq:actorspscritic}\eALT{} ˆh = arg min
h∈Hactor
t
max
uit≤m≤t
1
m
m

s=1
1
m −s + 1
m

r=s
ℓ(s, hs(xs), ys)
(6) 
Due to the double summation in Eq. (6), more recent events are weighted 
more heavily when minimizing the empirical risk. 
5.2
Preliminary Results 
Experiments Setting 
An agent forages along a 1 \times× 7 linear track that contains two reward patches, A 
and B, positioned three grid spaces apart (see Fig. 6 (Left)). Reward availability 
alternates between the two patches every 10 timesteps. Once the reward avail-
ability starts, the reward amount decays in an exponential fashion. A reward 
can be collected at time tt only if the agent is at the patch that is available at

26
Y. Bai et al.
Fig. 6. (Left) Schematics of the foraging task. Agents forage in a 1 \times 71 × 7 linear track 
with two reward patches (A and B), whose reward decays in an exponential fashion. 
Active patches with reward availability alternates every 10 timesteps. (Right) The  
actor-critic architecture used for retrospective and prospective agents. 
Fig. 7. (Left) Prospective risk of prospective agents (blue and red) and retrospec-
tive agents (green) in foraging task, compared to Bayes risk (dotted blue) and chance 
(yellow) performance. The prospective agent without time converges to a suboptimal 
risk closer to Bayes risk than retrospective agent, whereas prospective agent with time 
converges to Bayes risk. (Right) Agent actions plotted for the last 100 timesteps of 
training. Prospective agent with time embedding (red) shows the same action plan as 
the optimal agent, prospective agent without time embedding (blue) leaves the patch 
few steps earlier than optimal, and retrospective agent does not follow the action pat-
terns of the optimal agent. 
that moment. The agent moves one grid per timestep, and traveling between A 
and B takes at least three timesteps. The goal of the agent is to maximize the 
total amount of reward in its single lifetime, which means that there is no reset 
in location or time within each run. 
Optimal Solution 
Since the reward function is ﬁxed, we can derive an optimal foraging strategy: 
the agent should leave the current patch (Patch A) before its reward is depleted 
and arrive at the next patch (Patch B) exactly when its reward peaks. Since

Prospective Learning in Retrospect
27
no reward is available during travel, an optimal agent accepts zero immediate 
reward (during travel) over a low immediate reward (by staying in Patch A), 
in order to maximize future reward. Hence, an agent has to prospect into the 
future to learn the optimal solution. 
Preliminary Result 
We ﬁrst implemented a standard actor-critic RL agent in the prospective for-
aging environment. The model architecture is described in Fig. 6 (Right). We 
then implemented the prospective forager by integrating prospective risk mini-
mization into the actor-critic agent. Finally, we added time to the prospective 
forager, using the same time embedding deﬁned by Eq. (4). At each timestep, 
time embedding is concatenated to the input x_txt in addition to agent state and 
action. Figure 7 shows that the prospective agent signiﬁcantly outperforms the 
standard actor-critic algorithm, and the inclusion of time further improves the 
performance to near optimal. 
6
Conclusion 
In this work, we presented preliminary results that extend the prospective learn-
ing framework. We began by revisiting the foundational concepts and then ana-
lyzed the behavior of deep learning-based prospective learners by experiment-
ing with heterogeneous sampling, two diﬀerent choices of time-embeddings, and 
online-training. Next, we introduced Prospective-Trees, a nonparametric alterna-
tive that oﬀers competitive performance against Prospective-MLPs. Finally, we 
proposed prospective foraging, demonstrating the framework's potential beyond 
supervised learning settings and highlighting its promise in sequential decision-
making tasks. Collectively, these results motivate further mathematical and algo-
rithmic exploration of prospective learning to improve learning in dynamic envi-
ronments. 
References 
1. Vapnik, V.: Principles of risk minimization for learning theory. Advances in Neural 
Information Processing Systems, vol. 4 (1991) 
2. Valiant, L.: Probably approximately correct: nature's algorithms for learning and 
prospering in a complex world. Basic Books (2013) ISBN 9780465032716 
3. Ben-David, S., et al.: A theory of learning from diﬀerent domains. Machine Learn-
ing, 79, 151-175 (2010) 
4. Thrun, S.: Lifelong learning algorithms. In: Learning to learn, pp. 181-209. 
Springer, (1998) 
5. Vogelstein, J.T., et al.: A simple lifelong learning approach. arXiv preprint 
arXiv:2004.12908 (2020) 
6. Ramesh, R., Chaudhari, P.: Model zoo: a growing "brain" that learns continually. 
arXiv preprint arXiv:2106.03027 (2021)

28
Y. Bai et al.
7. Antoniou, A., Patacchiola, M., Ochal, M., Storkey, A.: Deﬁning benchmarks for 
continual few-shot learning. arXiv preprint arXiv:2004.11967 (2020) 
8. Shalev-Shwartz, S., et al.: Online learning and online convex optimization. Found. 
Trends R
⃝Mach. Learn. 4(2), 107-194 (2012) 
9. Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation 
of deep networks. In: International conference on machine learning, pp. 1126-1135. 
PMLR, (2017) 
10. Maurer A., Jaakkola, T.: Algorithmic stability and meta-learning. J. Mach. Learn. 
Res. 6(6) (2005) 
11. Ghosh, B.K., Sen, P.K.: Handbook of sequential analysis (statistics: a series of 
textbooks and monographs). CRC Press, 1 edition (1991) ISBN 9780824784089. 
URL https://play.google.com/store/books/details?id=JPHWDNSGCyEC 
12. Cesa-Bianchi, N., Lugosi, G.: Prediction, learning, and games. Cambridge univer-
sity press (2006) 
13. Petropoulos, F., et al.: Forecasting: theory and practice. Int. J. Forecast. 38(3), 
705-871 (2022) 
14. Sutton, R.S., Barto, A.G., et al.: Reinforcement learning: an introduction, vol. 1, 
MIT press Cambridge (1998) 
15. Chen, A., Sharma, A., Levine, S., Finn, C.: You only live once: single-life reinforce-
ment learning. Adv. Neural. Inf. Process. Syst. 35, 14784-14797 (2022) 
16. Kumar, S., et al.: Continual learning as computationally constrained reinforcement 
learning. arXiv preprint arXiv:2307.04345 (2023) 
17. Levine, S., Kumar, A., Tucker, G., Fu, J.: Oﬄine reinforcement learning: tutorial, 
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 (2020) 
18. De Silva, A., Ramesh, R., Priebe, C., Chaudhari, P., Vogelstein, J.T.: The value 
of out-of-distribution data. In: International Conference on Machine Learning, pp. 
7366-7389. PMLR (2023) 
19. Ashwin De Silva, et al.: Prospective learning: learning for a dynamic 
future. In Globerson, A., et al. (eds) Advances in Neural Informa-
tion Processing Systems, vol. 37, pp. 123055-123090. Curran Associates, 
Inc., (2024). URL https://proceedings.neurips.cc/paper_ﬁles/paper/2024/ﬁle/ 
de85d3cﬀ8512f72fd50b862979f1731-Paper-Conference.pdf 
20. De Silva, A., et al.: Prospective learning: principled extrapolation to the future. 
In: Conference on Lifelong Learning Agents, pp. 347-357. PMLR (2023) 
21. Vaswani, A., et al.: Attention is all you need. Advances in Neural Information 
Processing Systems, vol. 30 (2017) 
22. Zeno, C., Golan, I., Hoﬀer, E., Soudry, D.: Task-agnostic continual learning using 
online variational Bayes with ﬁxed-point updates. Neural Comput. 33(11), 3139- 
3177 (2021) 
23. Fernández-Delgado, M., Cernadas, E., Barro, S., Amorim, D.: Do we need hun-
dreds of classiﬁers to solve real world classiﬁcation problems? J. Mach. Learn. Res. 
15(90), 3133-3181. ISSN 1533-7928, 1533-7928 (2014). https://doi.org/10.5555/ 
2627435.2697065. URL  https://jmlr.org/papers/v15/delgado14a.html 
24. Grinsztajn, L., Oyallon, E., Varoquaux, G.: Why do tree-based models still outper-
form deep learning on typical tabular data? In: Thirty-sixth Conference on Neural 
Information Processing Systems Datasets and Benchmarks Track, (2022). URL 
https://openreview.net/pdf?id=Fp7__phQszn 
25. Biau, G., Devroye, L., Lugosi, G.: Consistency of random forests and other aver-
aging classiﬁers. J. Mach. Learn. Res. 9(66), 2015-2033 (2008). URL http://jmlr. 
org/papers/v9/biau08a.html

Prospective Learning in Retrospect
29
26. Scornet, E., Biau, G., Vert, J.P.: Consistency of random forests. Ann. of Stat. 43(4) 
(2015). ISSN 0090-5364 https://doi.org/10.1214/15-aos1321. URL  http://dx.doi. 
org/10.1214/15-AOS1321 
27. Klusowski, J.M., Tian, P.M.: Large scale prediction with decision trees (2023). 
URL https://arxiv.org/abs/2104.13881 
28. Breiman, L., Friedman, J., Olshen, R.A., Stone, C.J.: Classiﬁcation and regression 
trees. Routledge, (2017) 
29. Friedman, J.H.: Greedy function approximation: a gradient boosting machine. 
Annals of Statistics, pp. 1189-1232 (2001) 
30. Hastie, T., Tibshirani, R., Friedman, J., et al.: The elements of statistical learning. 
(2009) 
31. Greg Brockman, et al.: OpenAI gym. arXiv preprint arXiv:1606.01540, (2016). 
URL https://arxiv.org/abs/1606.01540 
32. Konda, V., Tsitsiklis, J.: Actor-critic algorithms.In: Advances in neural information 
processing systems, vol. 12 (1999) 
33. Shuvaev, S., Starosta, S., Kvitsiani, D., Kepecs, A., Koulakov, A.: R-learning in 
actor-critic model oﬀers a biologically relevant mechanism for sequential decision-
making. Adv. Neural. Inf. Process. Syst. 33, 18872-18882 (2020)

What Is Artiﬁcial General Intelligence? 
Michael Timothy Bennett(B) 
The Australian National University, Canberra, Australia 
michael.bennett@anu.edu.au 
Abstract. Artiﬁcial general intelligence (AGI) is an established ﬁeld of 
research. Yet some have questioned if the term still has meaning. AGI 
has been subject to so much hype and speculation it has become some-
thing of a Rorschach test. Melanie Mitchell argues the debate will only 
be settled through long term, scientiﬁc investigation. To that end here 
is a short, accessible and provocative overview of AGI. I compare deﬁni-
tions of intelligence, settling on intelligence in terms of adaptation and 
AGI as an artiﬁcial scientist. Taking my cue from Sutton's Bitter Lesson 
I describe two foundational tools used to build adaptive systems: search 
and approximation. I compare pros, cons, hybrids and architectures like 
o3, AlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-
approaches to making systems behave more intelligently. I divide them 
into scale-maxing, simp-maxing, w-maxing based on the Bitter Lesson, 
Ockham's and Bennett's Razors. These maximise resources, simplicity of 
form, and the weakness of constraints on functionality. I discuss examples 
including AIXI, the free energy principle and The Embiggening of lan-
guage models. I conclude that though scale-maxed approximation domi-
nates, AGI will be a fusion of tools and meta-approaches. The Embiggen-
ing was enabled by improvements in hardware. Now the bottlenecks are 
sample and energy eﬃciency. 
Keywords: artiﬁcial general intelligence 
1
Introduction 
Picture a machine endowed with human intellect. In its most simplistic form, 
that is Artiﬁcial General Intelligence (AGI) [ 1]. AGI is also a well established and 
rigorous ﬁeld of research [ 2]. However public perception of AGI is plagued by wild 
speculation and hype. Some see it as Skynet waiting to pounce [ 1, 3]. Others, like 
Melanie Mitchell, question if the term still has any meaning [ 4]. Speculation and 
hype have reduced it to a Rorschach test. As Mitchell points out, the debate will 
not be settled not by media but by rigorous, scientiﬁc research. Here I present a 
short and accessible survey to that end. It is framed in intentionally provocative 
terms, to spark debate 1. 
1 This paper was originally entitled What the F*ck Is Artiﬁcial General Intelligence, 
and has been cited several times by that name. Consider that to be the alternative 
name for this paper. I cited precedent for the use of profanity in the paper title 
where a highly regarded PLoS medical journal permitted the use of profanity [ 5]. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 30-42, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_4

What Is Artiﬁcial General Intelligence?
31
2
Intelligence 
I'll begin by deﬁning intelligence and AGI. There are a number of positions 
[ 2, 6- 12]. Some peg AGI to human-level performance across a broad range 
of tasks [ 1, 13]. This is is intuitive, but anthropocentric and hard to quantify 2. 
Chollet argues intelligence is a measure of the ability to generalise and acquire 
new skills. He argues AGI can do this at least as well as a human [ 11]. He 
attempts to quantify the ability to acquire new skills, which can encompass the 
aforementioned anthropocentric deﬁnition. His formalism resembles Legg-Hutter 
intelligence. Legg and Hutter argued intelligence is an ability to satisfy goals in 
a wide range of environments [ 10] 3. Chollet's deﬁnition descends from Legg-
Hutter. It is based on Ockham's Razor. They both use Kolmogorov complexity. 
They both equate simplicity with generality. They both seek to quantify intelli-
gence, and they are both highly subjective because they treat intelligence as a 
property of software interacting with the world through an interpreter [ 15- 18]. 
That a problem. Why? Because if I develop an AI for some purpose, then 
I decide whether it has fulﬁlled that purpose, and I am part of the agent's 
environment. The environment is where objective success or failure is decided. 
Assume \mathcal{C}C is a space of software programs, and \GammaΓ is a space of behaviours. 
Imagine f_1 \in \mathcal{C}f1 ∈C is AI software, f_2 : \mathcal{C} \rightarrow \Gammaf2 : C →Γ is the hardware on which it runs, 
and f_3 : \Gamma \rightarrow {0,1}f3 : Γ →{0, 1} is the environment (including me) where success is decided. 
Success is a matter of f_3(f_2(f_1))f3(f2(f1)). The behaviour of f_3(f_2(f_1))f3(f2(f1)) can be changed 
by changing f_2f2 or f_3f3 [ 12]. It is pointless to make claims about f_3(f_2(f_1))f3(f2(f1)) based 
on f_1f1 alone. f_1f1 and f_2f2 are like mind and body. Every choice of embodiment 
biases the system in some way. Each movement it makes constrains the space of 
possibilities, much like a constraint expressed in a formal language. Complexity 
is a property of how a body interprets information [ 18]. The choice of Universal 
Turing Machine can make any software agent optimal according to Legg-Hutter 
intelligence [ 17]. 
The idea of AI as a software mind is called computational dualism [ 12] 4. 
It is a reference to the work of Descartes, who in 1637 argued the pineal gland 
mediates between mind and body. AI researchers have exchanged the pineal 
gland for a Turing machine. So what is the alternative? Wang deﬁnes intelligence 
as adaptation with limited resources [ 6]. This leaves room for us to avoid 
dualism, and it implies the ability to satisfy goals in a wide range of environments 
anyway [ 12]. 
An attempt was made to resolve computational dualism and formalise intelli-
gence as objective adaptability. It does so by formalising software, hardware and
2 More accurately, I might say it is too easy to quantify. There are so many ways 
we might quantify it. It is ambiguous and too weak a criteria to be much use for 
anything but padding out Sam Altman's Twitter feed. 
3 This treats intelligence as implicitly separable from goals, endorsing the orthogonal-
ity thesis [ 14]. 
4 Computational dualism is grounded in lengthy formal deﬁnitions and derivations 
given elsewhere [ 12, 18- 21]. For this survey that level of formality would be counter-
productive. 

32
M. T. Bennett
environment together [ 12]. It formalises intelligence as a measure of the ability 
to complete a wide range of tasks [ 21]. This dispenses with the separation of 
goals and intelligence in favour of a whole-of-system model that treats the pur-
pose of a system as what it does. One's body implies a set of goals and subgoals. 
Body, environment and goals together form a task, by which I mean a purpose 
and a means of fulﬁlling it. If AA completes a superset of tasks that BB completes, 
then AA is more adaptable than BB. This encompasses both sample and energy 
eﬃciency. It is how fast a system can adapt and how much energy it needs 
to do so. This is the deﬁnition I will use for this survey. I'll consider an AGI 
to be a system that adapts at least as generally as a human scientist [ 22]. An 
artiﬁcial scientist can prioritise, plan and perform useful experiments. This 
requires autonomy, agency, motives, an ability to learn cause and eﬀect and the 
ability to balance exploring to acquire knowledge with acting to proﬁt from it 
[ 8, 9, 23- 25]. 
Artiﬁcial intelligence (AI) and machine learning (ML) are typically divided 
up into buckets like supervised learning, reinforcement learning, regression, clas-
siﬁcation, planning and so on. These are not useful categories for AGI, because 
an artiﬁcial scientist must be able to do all of these things. Instead, I will take 
my cue from Sutton's Bitter Lesson. It acknowledges that generally applicable 
tools can be used to learn any behaviour [ 26], if we scale up resources (compute, 
memory, data etc.). 
3
Tools 
Search: Informally, by search I mean systems that take structure, and then con-
struct a solution within the conﬁnes of that structure. For example, take a map 
and then plan a route by trying ﬁrst every combination of one, then two, then 
three and more turns until you ﬁnd the smallest sequence of turns that end at 
your destination. Search typically refers to algorithms like A* used for symbolic 
reasoning and planning problems [ 27]. These involve describing a problem and 
goal as a set of rules, and then constructing possible courses of action until 
one is found that obeys all the rules. Heurstics are used to construct a solution 
faster 5. In theory any problem can be framed as a search problem 6. Search has
5 An example of a heuristic is a function that takes a sequence of turns and tells you 
how far the end is from your destination. 
6 At ﬁrst glance it might be tempting to object, and say search can only be applied 
for a well-deﬁned problem with unambiguous rules and goals. However search can 
easily be applied to poorly deﬁned problems. In the absence of well deﬁned rules 
and goals, search can be used to infer rules and goals from observed data. It can do 
this by treating observed data or subsets thereof as rules, and searching the space 
of all possible criteria until one is found that explains some or all of observed data 
[ 28, 29]. Everything can be reduced to a search problem for much the same reason 
that every imperative program (instruction like "do this") can be translated into an 
equivalent declarative program (an assertion like "the pen is red") [ 30]; it is simply 
a matter of framing. 

What Is Artiﬁcial General Intelligence?
33
advantages. It produces veriﬁably correct and interpretable answers. It excels 
at planning [ 31] and is typically used in map software. It can prove theorems [ 32]. 
In the 90 s, search defeated the world chess champion [ 33]. Search can be used 
to learn, by iterating through possible hypotheses or models until one is found 
that conforms to observed data. However it also has disadvantages. Iterating 
through large state spaces is expensive. Hand crafted constraints can be added 
to reduce the search space, but that is not very scalable. Search tends to be 
sequential 7, making it ill suited to take full advantage modern hardware, which 
was originally designed to parallelise graphical rendering and physics simula-
tion in games. Only later was this hardware adapted for AI [ 34]. Parallel search 
algorithms exist but there is a lot of room for improvement [ 35- 38]. The con-
sequence is that search is only really practical at a higher levels of abstraction, 
where problems are represented using a small number of abstract symbols or 
well deﬁned parts. 
Approximation: Sutton's Bitter Lesson described the alternative to search as 
learning. However search can be used to learn [ 28], so to avoid confusion I'll use 
the term approximation instead. In any case most of modern machine learning 
is approximate. Typically it involves taking a model that can map inputs to out-
puts, then changing its parameters to so that the relation between inputs and 
outputs approximates training data. For example, convolutional neural networks 
can be taught to classify the contents of images [ 39]. Transformers trained on 
large corpus of text can generate human-like responses [ 40]. Approximation is 
imprecise, but for that very reason it is great at dealing with noisy data and 
large state spaces. It is easy to parallelise and scale on current hardware. There 
are drawbacks. Approximation is unreliable 8, because it is by deﬁnition only 
approximate. It is not easily interpretable [ 41]. Most importantly, current meth-
ods are extremely sample and energy ineﬃcient [ 21, 42]. This makes them less 
adaptable. Sample ineﬃciency doesn't just mean a model is slower to learn. It 
means the model does not cope well with anything outside the norm. In layman's 
terms, an approximation is mid. If two models are trained on the same amount 
of data, then the more sample eﬃcient model will deal as well or better with 
edge cases 9. 
Hybrids and Architectures: Hybrids are those systems which don't ﬁt neatly into 
search or approximation. For example collectives of living cells self-organise and
7 Tends to in present day implementations. Doesn't need to be. 
8 Imprecision implies a sort of unreliability. 
9 The proofs are given elsewhere [ 19, 28, 29, 43], but suppose for a second I learn faster 
than you (more sample eﬃcient). We both learn how to ﬁx a table from the same 
three examples of someone ﬁxing a table. We can now both ﬁx the table with 100% 
accuracy. However table ﬁxing is an example of ﬁxing in general. If I am more sample 
eﬃcient, I must now be able to ﬁx something you cannot. An edge case, like a chair. 
Now, there's an upper bound for when we have both learned everything, but given 
that is impossible given ﬁnite resources it will always be the case that the more 
sample eﬃcient learner will deal better with edge cases, all else being equal. 

34
M. T. Bennett
adapt. They can traverse a morphospace during development or regeneration 
[ 44], which is like search. Animals mimic and thus approximate behaviour. It 
is diﬃcult to argue biolgical self-organisation falls neatly into either search or 
approximation. Also, our current methods for search and approximation have 
complementary strengths and weaknesses. They can be combined to get the 
best of both worlds. Hybrids are inherently more general because they're not 
tied to one playbook. Need precision? Search. Got a mess of unstructured data? 
Approximate. By fusing their strengths, hybrids promise robustness where single-
track systems choke [ 45]. 
Perhaps the simplest example of a hybrid is AlphaGo [ 46]. It vanquished 
Go's world champion using a combination of search and approximation. Search 
enabled AlphaGo to explore potential sequences of moves within the game's 
constraints. Deep neural networks then approximated how likely sequences were 
to win. Intuitively, think of these as 'how to play' and 'how to win' respectively. 
This synergy allowed AlphaGo to surpass human champions, demonstrating the 
potential of hybrid approaches in mastering complex, strategic tasks. 
Search tends to be applied in the context of high level symbolic abstrac-
tions that depend on human interpretation. For example, the word 'cat' is just a 
sound until someone interprets it. It must be decided why and how a particular 
problem is represented using a particular language or set of symbols. This is the 
symbol grounding problem [ 47]. Neuro-symbolic hybrids attempt to address 
it [ 48]. These systems typically employ neural networks to interpret raw input, 
converting them into symbolic representations that encapsulate meaning. Search 
can then be applied to these representations to enable tasks such as planning or 
logical inference. However it should be noted that the complexity of a problem 
depends on how it is represented [ 18], and not all choices of symbolic representa-
tion are equal. Another hybrid approach is structured reinforcement learn-
ing. It leverages approximation to reduce high-dimensional raw sensory data to a 
more manageable symbolic format. Convolutional autoencoders are used to com-
press high-dimensional data into concise, symbolic forms that try to capture what 
is relevant in the input, enabling more eﬀective adaptation to dynamic environ-
ments [ 49]. More recent examples include OpenAI's o3 and DeepMind's Alpha-
Geometry. o3 employs chain-of-thought reasoning, blending approximation with 
a structured processes for complex problem-solving [ 50]. AlphaGeometry com-
bines neural networks with symbolic reasoning to solve geometry problems [ 51]. 
These systems exemplify the shift towards hybrid approaches for more capable 
AI. 
Finally, there are comprehensive frameworks designed to be generally intelli-
gent. Cognitive architectures and autonomous machines constructed from mod-
ules that each serve a diﬀerent purpose. Perception, memory, and reasoning mod-
ules. System 1 and system 2. For example scaﬀolding can be applied to neural 
networks to facilitate persistent identity and memory [ 52]. Pioneering examples 
include cognitive architectures like SOAR [ 53] and ACT-R [ 54]. More recent 
examples include Hyperon, Autocatalytic Endogenous Reﬂective Architecture 
(AERA) and the Non-Axiomatic Reasoning System (NARS).

What Is Artiﬁcial General Intelligence?
35
- Hyperon is a modular, distributed system integrating probabilistic logic net-
works, neural networks, and a knowledge metagraph for holistic cognition 
[ 7, 23, 55]. It is highly distributed, modular, scalable and self-organising. This 
makes it a versatile AGI platform that can integrate new technology as it 
develops. For example it appears Hyperon will soon incorporate a discrete 
form of active inference [ 56, 57]. 
- AERA self-programs, reﬂecting on its own symbolic structures while learning 
statistically. It emphasises analogy, causality, autonomy and growth, with 
predictive modelling supporting proactive adaptation [ 8, 58- 61]. 
- NARS rejects rigid axioms for a ﬂuid, adaptive logic. Operating under the 
Assumption of Insuﬃcient Knowledge and Resources (AIKR), NARS reasons 
with incomplete, uncertain data via a non-axiomatic framework. It integrates 
symbolic reasoning with probabilistic inference, using a custom inheritance-
based logic (NAL) to derive conclusions from limited evidence. Designed for 
real-time adaptability, NARS learns incrementally, reﬁning its knowledge base 
as new inputs arrive [ 9, 62]. 
Hybrid systems have obvious advantages. They can be more eﬃcient, inter-
pretable, they can integrate human priors eﬀectively and above all they allow for 
autonomy. It can also be diﬃcult to harmonise disparate methodologies. A lack 
of robust theoretical guidance risks hybrids being ad hoc rather than principled. 
This brings us to the ﬁnal piece of the puzzle. 
4
Meta-approaches 
If we're to build an artiﬁcial scientist, we need a clear idea of what we're optimis-
ing for. What constitutes a 'good' hypothesis? We need a theory that predicts 
whether one model adapts better than another. There are many such cases. These 
aren't algorithms so much as they are philosophies with teeth. Guiding princi-
ples like 'always choose the least speciﬁc solution' or 'delegate control instead 
of micromanaging tasks'. I call them meta-approaches. Examples include the 
Free Energy Principle [ 56], Universal Artiﬁcial Intelligence (UAI) [ 63], the Mini-
mum Description Length Principle [ 64], the scaling hypothesis [ 26], Stack Theory 
or Pancomputational Enactivism [ 12, 21] and even organisational principles like 
the military doctrine of Mission Command [ 65]. To simplify matters I put meta-
approaches into buckets based on what they share in common. Scale-maxing, 
simp-maxing, and w-maxing. These maximise resources, simplicity of form and 
versatility of function respectively. They can be understood in terms of Sutton's 
Bitter Lesson, Ockham's Razor and Bennett's Razor respectively [ 26, 28, 66]. I'll 
begin with the  elephant  in  the room.  
Scale-Maxed: As Sutton observed we seem to be able to just crank up compute, 
data and model size and get something that looks like intelligence. Scale-maxed 
approximation has deﬁned recent history. I call this period 'The Embiggening'. 
Language and vision models just got bigger as a bottleneck in compute was

36
M. T. Bennett
removed by technology originally developed for games [ 34]. GPT-3? 175 billion 
parameters, 45TB of text, and it's churning out essays, code, and creepy love 
letters [ 67]. AlphaFold 2? Threw a data center at protein folding and cracked a 
puzzle that had biologists weeping for decades [ 68]. However performance gains 
diminish with scale [ 69]. The energy bill is a nightmare [ 42]. Worst of all is 
the sample ineﬃciency. Today, scale-maxed approximations like GPT-4 struggle 
with novelty and always will, because novelty is by deﬁnition that of which we 
have few examples. Confront a large language model (LLM) with the genuinely 
unusual, and it'll ﬂail like a toddler in a calculus exam. The Bitter Lesson says 
scale will eventually work. It will eventually identify all other meta-approaches. 
Fine. Scale will eventually work, but eventually is doing all the work in that 
sentence. 
Simp-Maxed: Simplicity maximisation (simp-maxing) assumes the most accurate 
predictions are made by the simplest models. Simpler models can be written as 
shorter programs, so AI researchers have for a long time equated intelligence with 
compression [ 70]. There are many such cases. Regularization is the most common 
(e.g. dropout [ 71]). Likewise the Minimum Description Length principle (MDL) 
lends itself well to selecting hypotheses at high levels of symbolic abstraction 
[ 64]. 
Then there is UAI. The AIXI UAI is a mathematical formalism of superintel-
ligence [ 63]. It equates simplicity with compressibility, and bases its decisions on 
the most compressed representations of its history. The length of such an smallest 
self-extracting archive of a dataset is the Kolmogorov Complexity of the dataset 
[ 72, 73]. Solomonoﬀ induction assigns probabilities to programs based on Kol-
mogorov complexity [ 74, 75] 10. AIXI uses Solomonoﬀ induction to identify the 
best models of its environment 11. It can then choose the best possible actions 
based on those models. Conversely, just as we can say the optimal agent is the 
one that identiﬁes the simplest models, we can use this Kolmogorov Complexity 
to measure intelligence 12. We can check to see if an agent learns one of the sim-
plest models, or how close to simplest its model might be. That is Legg-Hutter 
intelligence: a measure of intelligence [ 10]. Chollet's measure, mentioned in the 
introduction, is similarly based on Kolmogorov Complexity [ 11]. Unfortunately 
Kolmogorov Complexity is incomputable, but working approximations of both 
AIXI and Legg-Hutter intelligence exist [ 78, 79]. These represent the universal 
upper bounds on intelligence. 
Except they don't. AIXI is a case of computational dualism [ 12]. Complexity 
is a property of form, not function. Kolmogorov complexity hinges on your choice 
of Turing machine [ 17] 13. In an interactive setting, there is an interpreter between 
the software mind and the world it inhabits. Complexity need not have any
10 Now available for Boolean circuits [ 76]. 
11 Now available for incomputable environments [ 77]. 
12 Oh sure, Kolmogorov Complexity is incomputable. Nobody cares. I can approximate 
it arbitrarily. 
13 A choice of Turing Machine is a choice of interpreter is a choice of descriptive lan-
guage. 

What Is Artiﬁcial General Intelligence?
37
bearing on reality at all. However, there is a correlation between simplicity of 
form and generalisation of function. There are reasons for this correlation [ 18]. 
First, a bounded system can contain only a ﬁnite amount of information [ 80]. 
Second, a goal-directed process like natural selection can select for systems that 
make accurate predictions. Third, to make accurate predictions using a ﬁnite 
vocabulary of representations of varying complexity, simpler forms must express 
more generalisable, weaker constraints on functionality [ 18], which brings us to 
our third meta-approach. 
W-Maxed: Computational dualism frames intelligence as a disembodied soft-
ware policy interacting with the world through an interpreter. The alternative 
is cognition as a process taking place within the environment, as a part of the 
environment [ 81]. This is called enactive cognition [ 82, 83]. A formalisation of 
enactive cognition must formalise the system as a whole, not parts. This is a 
challenge. One formalism, does this by moving the problem of interpretation 
outside the environment [ 12], basically saying "whatever determines the laws 
of physics is the interpreter" and assuming it is unknowable 14. This is useful  
to examine not just complexity of form and generalisation of function, but the 
relationship between the two in all possible environments. It was subsequently 
shown that generalisation stems from weakening the constraints on functionality 
to be as loose as possible while still satisfying the requirements of the system 
[ 28, 84]. Weakness, as it is called, is a measure of function as opposed to form. 
As such, maximising the weakness of constraints on function (w-maxing) is not 
mutually exclusive with simp-maxing. Both can take place, and optimising a 
ﬁnite set of representations to express the weakest possible collective constraint 
on functionality will cause simple forms to express weak constraints [ 18]. 
Given an abstraction layer (a language), w-maxing involves identifying poli-
cies or hypotheses that are as non-speciﬁc or 'weak' as possible, whilst still satis-
fying basic requirements. In experiments involving binary arithmetic, w-maxing 
alone yielded 110-500% improvement in generalisation rate over simp-maxing 
alone. W-maxing also involves delegating control to lower levels of abstrac-
tion. This reﬂects the biological polycomputational architecture self-organisation 
[ 44, 85- 88]. Biological systems are comparatively more adaptable than artiﬁcial 
intelligence because biology distributes control and delegates it to lower levels 
of abstraction [ 21]. In computer science terms, this is like programming in C 
instead of Python, or on a ﬁeld programmable gate array instead of C. More 
eﬃcient, bespoke implementations are possible when adaptation extends down 
to smaller scales and lower levels of abstraction. Early stage examples of com-
puting systems that delegate control in this manner include soft-robotics [ 89] 
and self-organising systems of nano particles [ 90, 91]. Because it does not sep-
arate software or hardware, it simultaneously optimises for both sample and 
energy eﬃciency [ 21]. But to optimise hardware as above, one must search an 
inﬁnite space of embodiments. This is a process of trial and error, or optimi-
14 This is called Stack Theory, because it frames everything as an inﬁnite stack of 
abstraction layers and assumes the bottom layer is unkowable or does not exist [ 19]. 

38
M. T. Bennett
sation through selection. It could be thought of like a biological self-organising 
system searching the morphospace during development and regeneration [ 44]. 
Because of the enactive frame, w-maxing has been used to explain causal rea-
soning, language and consciousness in terms that apply to both AI and biological 
self-organising systems [ 19, 92- 94]. 
5
Conclusion 
Recent history has been dominated by scale-maxed approximation. I like to call 
this period The Embiggening. The rapid improvements suggest we were bottle-
necked by compute and data. Now there are diminishing returns [ 69]. Models 
are expensive. There are far more problems for which we have little data than 
problems for which we have a lot of training data. Reliable precision and energy 
eﬃciency are increasingly important considerations. Perhaps scaling is no longer 
the easiest way forward. Better opportunities lie in w-maxing and simp-maxing. 
Newer models are hybrids like o3, not pure approximations as when GPT-3 was 
released. There has also been a great deal of discussion around the economic 
potential of autonomous agents [ 52]. This is where architectures like AERA, 
NARS and Hyperon stand to shine. Yes scale-maxed approximations dominate, 
but a fusion is required for an artiﬁcial scientist. 
References 
1. Russell, S.: Artiﬁcial intelligence and the problem of control, pp. 19-24. Springer 
(2022) 
2. Goertzel, B.: Artiﬁcial general intelligence: concept, state of the art. J. Artif. Gener. 
Intell. 5(1), 1-48 (2014) 
3. Bostrom, N.: Superintelligence: Paths, Dangers. Strategies. Oxford University 
Press, Oxford, UK (2014) 
4. Mitchell, M.: Debates on the nature of artiﬁcial general intelligence. Science 
383(6689), eado7069 (2024). https://doi.org/10.1126/science.ado7069, https:// 
www.science.org/doi/abs/10.1126/science.ado7069 
5. Krauth, S.J., Coulibaly, J.T., Knopp, S., Traoré, M., N'Goran, E.K., Utzinger, 
J.: An in-depth analysis of a piece of shit: distribution of schistosoma mansoni 
and hookworm eggs in human stool. PLoS Neglected Trop. Diseases 6(12), e1969 
(2012). https://doi.org/10.1371/journal.pntd.0001969 
6. Wang, P.: On deﬁning artiﬁcial intelligence. J. Artif. Gener. Intell. 10(2), 1-37 
(2019) 
7. Goertzel, B.: Generative ai vs. agi: the cognitive strengths and weaknesses of mod-
ern llms (2023), https://arxiv.org/abs/2309.10371 
8. Thorisson, K.R.: A new constructivist AI: from manual methods to self-
constructive systems, pp. 145-171. Atlantis Press, Paris (2012) 
9. Wang, P.: Rigid Flexibility: The Logic of Intelligence. Applied Logic Series, 
Springer Nature (2006) 
10. Legg, S., Hutter, M.: Universal intelligence: a deﬁnition of machine intelligence. 
Minds and Machines pp. 391-444 (2007) 
11. Chollet, F.: On the measure of intelligence (2019)

What Is Artiﬁcial General Intelligence?
39
12. Bennett, M.T.: Computational dualism and objective superintelligence. In: Artiﬁ-
cial General Intelligence. Springer (2024) 
13. Sternberg, R.J.: Toward a triarchic theory of human intelligence. Behav. Brain Sci. 
7(2), 269-287 (1984). https://doi.org/10.1017/S0140525X00044629 
14. Bostrom, N.: The superintelligent will: motivation and instrumental rationality in 
advanced artiﬁcial agents. Mind. Mach. 22(2), 71-85 (2012). https://doi.org/10. 
1007/s11023-012-9281-3 
15. Orseau, L.: Asymptotic non-learnability of universal agents with neural networks. 
In: Bach, J., Goertzel, B., Iklé, M. (eds.) Artiﬁcial General Intelligence: 5th Inter-
national Conference, AGI 2012, pp. 234-243. Springer Nature, Berlin, Heidelberg 
(2012) 
16. Orseau, L., Ring, M.: Space-time embedded intelligence. In: Bach, J., Goertzel, 
B., Iklé, M. (eds.) Artiﬁcial General Intelligence, pp. 209-218. Springer, Berlin 
Heidelberg, Berlin, Heidelberg (2012) 
17. Leike, J., Hutter, M.: Bad universal priors and notions of optimality. In: Pro-
ceedings of The 28th Conference on Learning Theory, in Proceedings of Machine 
Learning Research, pp. 1244-1259 (2015) 
18. Bennett, M.T.: Is complexity an illusion? In: Artiﬁcial General Intelligence. 
Springer (2024) 
19. Bennett,
M.T.:
How
To
build
conscious
machines.
Ph.D.
thesis, 
School
of
Computing,
The
Australian
National
University
(2025), 
github.com/ViscousLemming/Technical-Appendices 
20. Bennett, M.T.: Compression, the fermi paradox and artiﬁcial super-intelligence. 
In: Artiﬁcial General Intelligence, pp. 41-44. Springer (2022) 
21. Bennett, M.T.: Are biological systems more intelligent than artiﬁcial intelligence? 
(2025), forthcoming 
22. Bennett, M.T., Maruyama, Y.: The artiﬁcial scientist: Logicist, emergentist, and 
universalist approaches to artiﬁcial general intelligence. In: Goertzel, B., Iklé, M., 
Potapov, A. (eds.) Artiﬁcial General Intelligence, pp. 45-54. Springer Nature, 
Cham (2022) 
23. Goertzel, B.: The general theory of general intelligence: a pragmatic patternist 
perspective. Technical Report, Singularity Net (2021) 
24. Thorisson, K.R., et al.: Autonomous acquisition of situated natural communication. 
Int. J. Comp. Sci. Info. Sys (2014) 
25. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press, 
MA (2018) 
26. Sutton, R.: The Bitter Lesson. University of Texas at Austin (2019) 
27. Russell, S., Norvig., P.: Artiﬁcial intelligence: a modern approach, global edition 
4th. Pearson, London (2021) 
28. Bennett, M.T.: The optimal choice of hypothesis is the weakest, not the shortest. 
In: Artiﬁcial General Intelligence. Springer (2023) 
29. Bennett, M.T.: A formal theory of optimal learning with experimental results. 
In: Proceedings of the Thirty-fourth International Joint Conference on Artiﬁcial 
Intelligence (2025) 
30. Howard, W.A.: The formulae-as-types notion of construction. In: Seldin, J., Hind-
ley, J. (eds.) To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and 
Formalism, pp. 479-490. Academic Press, Cambrdige MA (1980) 
31. Kautz, H., Selman, B.: Planning as satisﬁability. In: IN ECAI-92, pp. 359-363. 
Wiley, New York (1992) 
32. Newell, A., Simon, H.: The logic theory machine-a complex information processing 
system. IRE Trans. Inf. Theor. 2(3), 61-79 (1956)

40
M. T. Bennett
33. Campbell, M., Hoane, A., hsiung Hsu, F.: Deep blue. Artif. Intell. (2002) 
34. Kirk, D.: Nvidia cuda software and gpu parallel computing architecture. In: Pro-
ceedings of the 6th International Symposium on Memory Management, ISMM 
2007, pp. 103-104. ACM, New York, NY, USA (2007). 10.1145/1296907.1296909, 
https://doi.org/10.1145/1296907.1296909 
35. Schulte, C., Carlsson, M.: Chapter 14 - ﬁnite domain constraint programming 
systems. In: Rossi, F., van Beek, P., Walsh, T. (eds.) Handbook of Constraint 
Programming. Elsevier, Foundations of Artiﬁcial Intelligence (2006) 
36. Edelkamp, S., Schrödl, S.: Chapter 9 - distributed search. In: Edelkamp, S., Schrödl, 
S. (eds.) Heuristic Search, pp. 369-427. Morgan Kaufmann, San Francisco (2012) 
37. Zhou, Y., Zeng, J.: Massively parallel a* search on a gpu. In: Proceedings of the 
AAAI Conference on Artiﬁcial Intelligence (2015) 
38. Oswald, J.T., Rozek, B.: Parallel veriﬁcation of natural deduction proof graphs. 
Electron. Proc. Theor. Comput. Sci. 396, 36-51 (2023). https://doi.org/10.4204/ 
eptcs.396.4 
39. Krizhevsky et al., A.: Imagenet classiﬁcation with deep convolutional neural net-
works. Commun. ACM (2017) 
40. Vaswani et al., A.: Attention is all you need. In: Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, NIPS 2017, Curran, 
NY (2017) 
41. Ribeiro, M.T., Singh, S., Guestrin, C.: "Why should i trust you?": Explaining the 
predictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, KDD 2016, pp. 
1135-1144. ACM, New York, NY, USA (2016) 
42. Strubell, E., Ganesh, A., McCallum, A.: Energy and policy considerations for deep 
learning in NLP. In: Proceedings of the 57th Annual Meeting of the Association for 
Computational Linguistics, Association for Computational Linguistics, Florence, 
Italy (2019) 
43. Bennett, M.T.: Optimal policy is weakest policy. Artif. Gener. Intell. (2025) 
44. McMillen, P., Levin, M.: Collective intelligence: a unifying concept for integrating 
biology across scales and substrates. Commun. Biol. (2024) 
45. Bennett, M.T., Maruyama, Y.: Philosophical speciﬁcation of empathetic ethical 
artiﬁcial intelligence. IEEE Trans. Cognit. Dev. Syst. 14(2), 292-300 (2022) 
46. Silver et al., D.: Mastering the game of go with deep neural networks and tree 
search. Nature 529(7587), 484-489 (2016) 
47. Harnad, S.: The symbol grounding problem. Physica D 42(1), 335-346 (1990) 
48. Garcez, A., Gori, M., Lamb, L.C., Seraﬁni, L., Spranger, M., Tran, S.N.: Neural-
symbolic computing: an eﬀective methodology for principled integration of machine 
learning and reasoning. In: FLAP (2019) 
49. Garnelo, M., Arulkumaran, K., Shanahan, M.: Towards deep symbolic reinforce-
ment learning (2016) 
50. OpenAI: Openai o3-mini system card (2025) 
51. Trinh, T.H., et al.: Solving olympiad geometry without human demonstrations. 
Nature (2024) 
52. Perrier, E., Bennett, M.T.: Position: Stop acting like language model agents are 
normal agents (2025), https://arxiv.org/abs/2502.10420 
53. Laird, J.E.: The Soar Cognitive Architecture. MIT Press, MA (2012) 
54. Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C., Qin, Y.: An 
integrated theory of the mind. Psychol. Rev. (2004), because apparently six authors 
are needed to ﬁgure out how your brain works

What Is Artiﬁcial General Intelligence?
41
55. Goertzel, B., et al.: Opencog hyperon: a framework for agi at the human level and 
beyond. Technical Report, OpenCog Foundation (2023) 
56. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nat. Rev. Neurosci. 
11(2), 127-138 (2010) 
57. Goertzel, B.: Actpc-chem: discrete active predictive coding for goal-guided algo-
rithmic chemistry as a potential cognitive kernel for hyperon and primus-based agi 
(2024) 
58. Nivel et al., E.: Autocatalytic endogenous reﬂective architecture. Technical Report, 
Reykjavik University, School of Computer Science (2013) 
59. Thórisson, K.R.: Seed-programmed autonomous general learning. In: Proceedings 
of the First International Workshop on Self-Supervised Learning. Proceedings of 
Machine Learning Research, vol. 131, pp. 32-61. PMLR, 27-28 Feb 2020 
60. Sheikhlar, A., Thorisson, K.R.: Causal generalization via goal-driven analogy. In: 
Thorisson, K.R., Isaev, P., Sheikhlar, A. (eds.) Artiﬁcial General Intelligence, pp. 
165-175. Springer Nature Switzerland, Cham (2024) 
61. Eberding, L.M., Thompson, J., Thorisson, K.R.: Argument-driven planning and 
autonomous explanation generation. In: Thorisson, K.R., Isaev, P., Sheikhlar, 
A. (eds.) Artiﬁcial General Intelligence, pp. 73-83. Springer Nature Switzerland, 
Cham (2024) 
62. Hammer, P., Lofthouse, T.: 'Opennars for applications': architecture and control. 
In: Goertzel, B., Panov, A.I., Potapov, A., Yampolskiy, R. (eds.) Artiﬁcial General 
Intelligence, pp. 193-204. Springer Nature, Cham (2020) 
63. Hutter, M., Quarel, D., Catt, E.: An Introduction to Universal Artiﬁcial Intel-
ligence. Chapman and Hall/CRC, 1st edn. (2024). https://doi.org/10.1201/ 
9781003460299 
64. Rissanen, J.: Modeling by shortest data description. Automatica (1978) 
65. Ingesson, T.: The politics of combat: the political and strategic impact of tactical-
level subcultures, 1939-1995. Doctoral thesis (monograph), Department of Political 
Science (2016) 
66. Sober, E.: Ockham's razors: a user's manual. Cambridge Uni. Press (2015). https:// 
doi.org/10.1017/CBO9781107705937 
67. Roose, K.: Why a conversation with bing's chatbot left me deeply unsettled. 
The New York Times, February 2023, https://www.nytimes.com/2023/02/16/ 
technology/why-a-conversation-with-bings-chatbot-left-me-deeply-unsettled. 
html 
68. Jumper, J., et al.: Highly accurate protein structure prediction with alphafold. 
Nature (2021) 
69. Kaplan, J., et al.: Scaling laws for neural language models (2020) 
70. Chaitin, G.J.: On the length of programs for computing ﬁnite binary sequences. J. 
ACM (1966) 
71. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: 
Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn. 
Res. 15(56), 1929-1958 (2014), http://jmlr.org/papers/v15/srivastava14a.html 
72. Kolmogorov, A.: On tables of random numbers. Sankhya: Indian J. Stat. A, 369- 
376 (1963) 
73. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of informa-
tion *. Int. J. Comput. Math. 2(1-4), 157-168 (1968). https://doi.org/10.1080/ 
00207166808803030 
74. Solomonoﬀ, R.: A formal theory of inductive inference. part i. Inf. Control 7(1), 
1-22 (1964)

42
M. T. Bennett
75. Solomonoﬀ, R.: A formal theory of inductive inference. part ii. Inf. Control 7(2), 
224-254 (1964) 
76. Wyeth, C., Sturtivant, C.: A circuit complexity formulation of algorithmic informa-
tion theory. Physica D 456, 133925 (2023). https://doi.org/10.1016/j.physd.2023. 
133925 
77. Oswald, J.T., Ferguson, T.M., Bringsjord, S.: Extension to legg and hutter's uni-
versal intelligence measure for uncomputable environments. In: Thórisson, K.R., 
Isaev, P., Sheikhlar, A. (eds.) Proceedings of the 17th International Conference 
on Artiﬁcial General Intelligence, LNCS, vol. 14951, pp. 134-144. Springer, Cham 
(2024). https://doi.org/10.1007/978-3-031-65572-2_15 
78. Hutter, M.: Universal algorithmic intelligence: a mathematical Top\rightarrow →Down App-
roach, pp. 227-290. Springer, Berlin, Heidelberg (2007) 
79. Legg, S., Veness, J.: An approximation of the universal intelligence measure. In: 
Algorithmic Probability and Friends (2011) 
80. Bekenstein, J.D.: Universal upper bound on the entropy-to-energy ratio for 
bounded systems. Phys. Rev. D 23, 287-298 (1981) 
81. Dreyfus, H.L.: What Computers Can't Do: A Critique of Artiﬁcial Reason. Harper 
& Row (1972) 
82. Thompson, E.: Mind in Life: Biology, Phenomenology, and the Sciences of Mind. 
Harvard University Press, Cambridge MA (2007) 
83. Vervaeke, J., Lillicrap, T., Richards, B.: Relevance realization and the emerging 
framework in cognitive science. J. Log, Comput. (2012) 
84. Bennett, M.T.: Computable Artiﬁcial General Intelligence. Preprint (2022) 
85. Bongard, J., Levin, M.: There's plenty of room right here: biological systems as 
evolved, overloaded, multi-scale machines. Biomimetics 8(1) (2023) 
86. Gershenson, C.: Self-organizing systems: what, how, and why? npj Complexity 
(2025) 
87. Fields, C., Levin, M.: Scale-free biology: integrating evolutionary and developmen-
tal thinking. BioEssays 42 (2020) 
88. Solé, R., Seoane, L.F.: Evolution of brains and computers: the roads not taken. 
Entropy 24(5), 665 (2022) 
89. Man, K., Damasio, A.R.: Homeostasis and soft robotics in the design of feeling 
machines. Nat. Mach. Intell. 1, 446-452 (2019), https://api.semanticscholar.org/ 
CorpusID:208089594 
90. Borghi, F., Nieus, T.R., Galli, D.E., Milani, P.: Brain-like hardware, do we need 
it? Front. Neurosci. 18 (2024) 
91. Paroli, B., Martini, G., Potenza, M., Siano, M., Mirigliano, M., Milani, P.: Solving 
classiﬁcation tasks by a receptron based on nonlinear optical speckle ﬁelds. Neural 
Netw. 166, 634-644 (2023) 
92. Bennett, M.T.: Emergent causality and the foundation of consciousness. In: Arti-
ﬁcial General Intelligence. Springer (2023) 
93. Bennett, M.T.: On the computation of meaning, language models and incompre-
hensible horrors. In: Artiﬁcial General Intelligence. Springer (2023) 
94. Bennett, M.T., Welsh, S., Ciaunica, A.: Why is anything conscious? Preprint (2024)

Optimal Policy Is Weakest Policy 
Michael Timothy Bennett(B) 
The Australian National University, Canberra, Australia 
michael.bennett@anu.edu.au 
Abstract. Pancomputational Enactivism is a formalism of embodied, 
embedded, extended, and enactive intelligence. Previous work used this 
formalism to show the optimal choice of policy is the weakest. Experi-
mental results support this claim. This has wide ranging implications. 
However there are ﬂaws in its formal presentation, which undermine the 
optimality claims. Here we discuss these ﬂaws, and present alternative 
proofs to rectify them. 
Keywords: Pancomputational Enactivism · Bennett's razor · 
w-maxing 
1
Introduction 
Artiﬁcial intelligence is often framed as the pursuit of intelligent software. How-
ever the behaviour of software is determined by the hardware on which it runs. 
This distinction between software and hardware, and by extension between an 
agent and the environment in which it exists, undermines any claim that can be 
made about the behaviour of a theorist intelligence [ 1, 2]. This problem is called 
Computational Dualism [ 3]. Pancomputational Enactivism is a formalisation of 
cognition which addresses this problem. It formalises goal directed behaviour as 
embodied tasks, instead of as an interaction between a separate agent and envi-
ronment. It is based upon Stack theory [ 4], which moves the "interpreter" from 
the level of the agent (for example a Turing machine in a body that interprets 
a software "mind") to the physical laws of the environment 1. It avoids compu-
tational dualism by framing intelligent behaviour as a whole-of-system problem 
[ 3, 5- 7]. It has subsequently been used to establish objective upper bounds on 
embodied intelligent behaviour [ 3, 8]. It has been used to show the optimal choice 
of policy is the weakest, and experimental results supported this claim [ 9- 11]. 
This has wide ranging implications [ 12], from causality [ 13- 15] to complex sys-
tems [ 7, 8,16], to agents [ 17], to language and norms [ 18- 20], the Fermi Paradox 
and the origins of life problem [ 4,21], and above all consciousness [ 4,22- 26]. 
However, the early results contain a number of ﬂaws. First, they use an early 
1 Speciﬁcally, Stack Theory frames physical laws as an abstraction layer, and assumes 
there is no "base" abstraction layer, meaning there is no way to know where the true 
underlying physics of our system is. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 43-48, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_5

44
M. T. Bennett
version of Pancomputational Enactivism [ 9] that diﬀers substantially from that 
used in derivative works [ 3, 7, 8], which calls into question whether the proofs 
still hold. Second, the proof that maximising the weakness of policies is suﬃ-
cient to maximise the probability that they will generalise contains a miscount. 
This miscount does not change the result of the proof, but undermines its credi-
bility. Finally, there is a ﬂaw in the proof that it is necessary to maximise policy 
weakness to maximise the probability that policies will generalise. It failed to 
mention a dependency on the distribution of tasks. These problems do not refute 
the claims of optimality, but they certainly undermine them. Here we present 
alternative proofs of optimality, which do not suﬀer these ﬂaws. 
2
Deﬁnitions 
We will begin with a summary of relevant deﬁnitions, followed by the proofs. 
These deﬁnitions are abridged versions of the full Pancomputational Enactivism 
formalism, but suﬃcient for the purposes of these proofs. Full length deﬁnitions 
are available in any one of the various full length published works on Pancom-
putational Enactivism [ 3]. 
1. Programs: \PhiΦ is the set of states, and  P = 2^\PhiP = 2Φ is all possible declarative 
programs. Each program is a potential point of diﬀerence between states 2. 
Assume a present state \phi \in \Phiφ ∈Φ, and program f \in Pf ∈P is true if \phi \in fφ ∈f. 
2. Embodied Language: A body or abstraction layer is formalised as a ﬁnite 
vocabulary \mathfrak{v} \subset Pv ⊂P. The embodied language is L_\mathfrak{v} = {l \subseteq \mathfrak{v} : \bigcap l \neq \emptyset}Lv = {l ⊆v :  l ̸= ∅}. 
Members of L_\mathfrak{v}Lv are statements the body can express (state of memory etc.). 
A statement l \subseteq L_\mathfrak{v}l ⊆Lv is true when \phi \in \bigcup lφ ∈ l. E_l = {y \in L_\mathfrak{v} : l \subseteq y}El = {y ∈Lv : l ⊆y} is called the 
extension of ll. 
3. \mathfrak{v}v-Tasks: A \mathfrak{v}v-task \alpha = \langle I_\alpha, O_\alpha \rangleα = ⟨Iα, Oα⟩has inputs I_\alpha \subset L_\mathfrak{v}Iα ⊂Lv and outputs O_\alpha \subset E_{I_\alpha}Oα ⊂EIα. 
Assume a uniform distribution over \mathfrak{v}v-tasks. Let \alphaα and \omegaω be \mathfrak{v}v-tasks. If I_\alpha \subset I_\omegaIα ⊂
Iω, O_\alpha \subseteq O_\omegaOα ⊆Oω, then \alphaα is examples of \omegaω. In previous work example tasks are 
referred to as children, and tasks they exemplify are referred to as parents. 
A \mathfrak{v}v-task \alphaα is a child of \mathfrak{v}v-task \omegaω if {I}_\alpha \subset {I}_\omegaIα ⊂Iω and {O}_\alpha \subseteq {O}_\omegaOα ⊆Oω. This is written 
as \alpha \sqsubset \omegaα ⊏ω. If  \alpha \sqsubset \omegaα ⊏ω then \omegaω is then a parent of \alphaα. \sqsubset⊏implies a "lattice" or 
generational hierarchy of tasks. Formally, the level of a task\alphaα in this hierarchy 
is the largest kk such there is a sequence \langle \alpha_0, \alpha_1, ... \alpha_k \rangle⟨α0, α1, ...αk⟩of kk tasks such that 
\alpha_0 = \alphaα0 = α and \alpha_i \sqsubset \alpha_{i+1}αi ⊏αi+1 for all i \in (0,k)i ∈(0, k). 
4. Policies: \pi \in L_\mathfrak{v}π ∈Lv is a correct policy for \alphaα if E_{I_\alpha} \cap E_\pi = O_\alphaEIα ∩Eπ = Oα. \Pi_\alphaΠα is the set 
of all correct policies for \alphaα. The body learns or generalises to \omegaω by inferring 
(choosing from \Pi_\alphaΠα) a correct policy from examples \alphaα that is also correct for 
\omegaω, meaning \pi \in \Pi_\alpha \cap \Pi_\omegaπ ∈Πα ∩Πω. 'Experience' is adding inputs and outputs to the 
examples \alphaα. Intelligence is eﬃciency in learning 3.
2 States don't contain any content, but are deﬁned only in terms of their diﬀerences 
from one another, in a manner reminiscent of structuralism if it were to try and 
account for the post-structuralist notion of diﬀerance [27]. 
3 The lower level the child of \omegaω from which one learns \omegaω, the more intelligent one is. 

Optimal Policy Is Weakest Policy
45
5. Heuristics: The weakness of a policy \pi \in \Pi_\alphaπ ∈Πα is |E_\pi||Eπ|. A  proxy << is a binary 
relation on statements, used to choose between policies implied by examples. 
If \piπ and \pi'π′ are a correct policies for \alphaα, \alpha \sqsubset \omegaα ⊏ω, and we are trying to choose 
one of \piπ and \pi'π′ and maximise the chance of learning \omegaω from \alphaα, then  a proxy  
is used to choose between \piπ and \pi'π′. <_w<w is the weakness proxy. For statements 
l_1,l_2l1, l2 we have l_1 <_w l_2l1 <w l2 iﬀ  | E_{l_1} | < | E_{l_2} ||El1| < |El2|, meaning the proxy chooses l_2l2. 
3
Proofs
 
Proposition 1 (suﬃciency). Assume \alpha \sqsubset \omegaα ⊏ω. The weakness proxy suﬃcient 
to maximise the probability that a parent \omegaω is learned from a child \alphaα4. 
Proof. You're given the deﬁnition of \mathfrak{v}v-task \alphaα from which you infer a hypothesis 
\pi \in {\Pi}_\alphaπ ∈Πα. To learn  \omegaω, you need \pi \in \Pi_\omegaπ ∈Πω: 
1. For every \pi \in \Pi_\alphaπ ∈Πα there exists a \mathfrak{v}v-task \gamma_\pi \in \Gamma_\mathfrak{v}γπ ∈Γv s.t. O_{\gamma_\pi} = E_\piOγπ = Eπ, meaning \piπ
permits only correct outputs for that task regardless of input. We'll call the 
highest level task \gamma_\piγπ s.t. O_{\gamma_\pi} = E_\piOγπ = Eπ the policy task of \piπ. 
2. \omegaω is either the policy task of a policy in \Pi_\alphaΠα, or a child thereof 5. 
3. If a policy \piπ is correct for a parent of \omegaω, then it is also correct for \omegaω. Hence 
we should choose \piπ that has a policy task with the largest number of children. 
As tasks are uniformly distributed, that will maximise the probability that \omegaω
is \gamma_\piγπ or a child thereof. 
4. For the purpose of this proof, we say one task is equivalent 6 to another if 
it has the same correct outputs. 
5. No two policies in \Pi_\alphaΠα have the same policy task 7. This is because all the 
policies in \Pi_\alphaΠα are derived from the same set inputs, I_\alphaIα. 
6. The set of statements which might be outputs addressing inputs in {I}_\omegaIω and 
not {I}_\alphaIα, is  \overline{E_{{I}_\alpha}} = { l \in L_\mathfrak{v} : l \notin E_{{I}_\alpha} }EIα = {l ∈Lv : l /∈EIα}8. 
7. For any given \pi \in {\Pi}_\alphaπ ∈Πα, the extension E_\piEπ of \piπ is the set of outputs \piπ implies. 
The subset of E_\piEπ which fall outside the scope of what is required for the 
known task \alphaα is \overline{E_{{I}_\alpha}} \cap E_{\pi}EIα ∩Eπ9.
4 Assume there exist correct policies for \omegaω, or there'd be no point trying to learn. 
5 Credit goes to Nora Belrose for pointing out the counting error. 
6 This is because switching from \betaβ to \zetaζ s.t. I_\beta \neq I_\zetaIβ ̸= Iζ and O_\beta = O_\zetaOβ = Oζ would be to 
pursue the same goal in diﬀerent circumstances. This is because inputs are subsets 
of outputs, so both sets of inputs are implied by the outputs. O_\zetaOζ implies I_\betaIβ and O_\betaOβ
implies I_\zetaIζ. 
7 Every policy task for policies of \alphaα is non-equivalent from the others. 
8 This is becauseE_{{I}_\alpha}EIα contains every statement which is a correct output or an incorrect 
output, and \overline{E_{{I}_\alpha}}EIα contains every statement which could possibly be in I_\omegaIω, E_{I_\omega}EIω and 
thus O_\omegaOω. 
9 This is because E_{{I}_\alpha}EIα is the set of all conceivable outputs by which one might attempt 
to complete \alphaα, and so the set of all outputs that can't be made when undertaking \alphaα
is \overline{E_{{I}_\alpha}}EIα because those outputs occur given inputs that aren't part of {I}_\alphaIα. 

46
M. T. Bennett
8. L_\mathfrak{v} = \overline{E_{{I}_\alpha}} \cup E_{{I}_\alpha}Lv = EIα ∪EIα and for all \pi \in \Pi_\alphaπ ∈Πα, E_\pi \subset L_\mathfrak{v}Eπ ⊂Lv. Apart from the inputs and 
correct outputs of \alphaα, E_{{I}_\alpha}EIα contains only outputs which would be incorrect 
according to both \alphaα and \omegaω. Put another way, {E_{{I}_\alpha}} \cap E_{\pi} = O_\alphaEIα ∩Eπ = Oα for every possible 
choice of \piπ in \Pi_\alphaΠα. Hence the only way \left | E_{\pi} \right ||Eπ| can increase is if \left | \overline{E_{{I}_\alpha}} \cap E_{\pi} \right |
EIα ∩Eπ

increases. It follows that \left | \overline{E_{{I}_\alpha}} \cap E_{\pi} \right |
EIα ∩Eπ
 increases with \left| E_\pi \right||Eπ|. 
9. 2^{\left| \overline{E_{{I}_\alpha}} \cap E_{\pi} \right|}2|EIα∩Eπ| is the number of non-equivalent parents of \alphaα to which \piπ gener-
alises. It increases monotonically with the weakness of \piπ. 
10. Given \mathfrak{v}v-tasks are uniformly distributed and \Pi_\alpha \cap \Pi_\omega \neq \emptysetΠα ∩Πω ̸= ∅, the probability 
that \pi \in {\Pi}_\alphaπ ∈Πα generalises to \omegaω is 
 \bUnALT{}p(\pi \in {\Pi}_\omega \mid \pi \in {\Pi}_\alpha, \alpha \sqsubset \omega) = \frac{2^{\left| \overline{E_{{I}_\alpha}} \cap E_{\pi} \right|}}{2^{\left| \overline{E_{{I}_\alpha}} \right|}}\eUnALT{} p(π ∈Πω | π ∈Πα, α ⊏ω) = 2|EIα∩Eπ|
2|EIα|
p(\pi \in {\Pi}_\omega \mid \pi \in {\Pi}_\alpha, \alpha \sqsubset \omega)p(π ∈Πω | π ∈Πα, α ⊏ω) is maximised when \left| E_\pi \right||Eπ| is maximised. Recall 
from deﬁnition 4 that <_w<w is the weakness proxy. For statements l_1,l_2l1, l2 we 
have l_1 <_w l_2l1 <w l2 iﬀ \lvert E_{l_1}\rvert < \lvert E_{l_2} \rvert|El1| < |El2|. \piπ that maximises <_w<w will also maximise 
p(\pi \in {\Pi}_\omega \mid \pi \in {\Pi}_\alpha, \alpha \sqsubset \omega)p(π ∈Πω | π ∈Πα, α ⊏ω). Hence the weakness proxy maximises the prob-
ability that 10 a parent \omegaω is learned from a child \alphaα. \square□
Proposition 2 (necessity). To maximise the probability of learning \omegaω from \alphaα, 
it is necessary to use weakness as a proxy. 
Proof. Let \alphaα and \omegaω be deﬁned exactly as they were in the proof of prop. 1. 
1. If \pi \in {\Pi}_\alphaπ ∈Πα and E_{{I}_\omega} \cap E_{\pi} = {O}_\omegaEIω ∩Eπ = Oω, then it must be he case that {O}_\omega \subseteq E_{\pi}Oω ⊆Eπ. 
2. If \left| E_{\pi} \right| < \left| {O}_\omega \right||Eπ| < |Oω| then generalisation cannot occur, because that would mean 
that {O}_\omega \not\subseteq E_{\pi}Oω ̸⊆Eπ. 
3. Therefore generalisation is only possible if\left| E_{\pi} \right| \ge \left| {O}_\omega \right||Eπ| ≥|Oω|, meaning a suﬃciently 
weak hypothesis is necessary to generalise from child to parent. 
4. For any two hypotheses \pi_1π1 and \pi_2π2, if  \mid E_{\pi_1} \mid < \mid E_{\pi_2} \mid| Eπ1 |<| Eπ2 | then the probability 
p(\left| E_{\pi_1} \right| \ge \left| {O}_\omega \right|) < p(\left| E_{\pi_2} \right| \ge \left| {O}_\omega \right|)p(|Eπ1| ≥|Oω|) < p(|Eπ2| ≥|Oω|) because tasks are uniformly distributed. 
5. Hence the probability that \left| E_{{m}} \right| \ge \left| {O}_\omega \right||Em| ≥|Oω| is maximised when \left| E_{{m}} \right||Em| is max-
imised. To maximise the probability of learning \omegaω from \alphaα, it is necessary to 
select the weakest hypothesis. 
To select the weakest hypothesis, it is necessary to use the weakness proxy. \square□
4
Conclusion 
In conclusion these updated proofs rectify the ﬂaw in the count of tasks, and the 
unspeciﬁed distribution. They also use the more recent formulation of Pancom-
putational Enactivism, and may inform future research based on that formalism.
10 Subsequently it also maximises the sample eﬃciency with which a parent\omegaω is learned 
from a child \alphaα. 

Optimal Policy Is Weakest Policy
47
References 
1. Leike, J., Hutter, M.: Bad universal priors and notions of optimality. In: Pro-
ceedings of The 28th Conference on Learning Theory, in Proceedings of Machine 
Learning Research, pp. 1244-1259 (2015) 
2. Orseau, L., Ring, M.: Space-time embedded intelligence. In: Bach, J., Goertzel, 
B., Ikl´e, M. (eds.) Artiﬁcial General Intelligence, pp. 209-218. Springer, Berlin, 
Heidelberg (2012) 
3. Bennett, M.T.: Computational dualism and objective superintelligence. In: Artiﬁ-
cial General Intelligence. Springer (2024) 
4. Bennett,
M.T.:
How
To
Build
Conscious
Machines.
Ph.D.
thesis, 
School
of
Computing,
The
Australian
National
University
(2025), 
github.com/ViscousLemming/Technical-Appendices 
5. Thompson, E.: Mind in Life: Biology, Phenomenology, and the Sciences of Mind. 
Harvard University Press, Cambridge MA (2007) 
6. Piccinini, G., Maley, C.: Computation in Physical Systems. In: Zalta, E.N. (ed.) 
The Stanford Encyclopedia of Philosophy. Stanford University, Stanford, Sum. 21 
edn. (2021) 
7. Bennett, M.T.: Are biological systems more intelligent than artiﬁcial intelligence? 
(2025), forthcoming 
8. Bennett, M.T.: Is complexity an illusion? In: Artiﬁcial General Intelligence, 
Springer (2024) 
9. Bennett, M.T.: The optimal choice of hypothesis is the weakest, not the shortest. 
In: Artiﬁcial General Intelligence. Springer (2023) 
10. Bennett, M.T.: Computable artiﬁcial general intelligence. Preprint (2022) 
11. Bennett, M.T.: A formal theory of optimal learning with experimental results. 
In: Proceedings of the Thirty-fourth International Joint Conference on Artiﬁcial 
Intelligence (2025) 
12. Bennett, M.T.: What the f*ck is artiﬁcial general intelligence? Springer (2025) 
13. Pearl, J., Mackenzie, D.: The Book of Why: The New Science of Cause and Eﬀect, 
1st edn. Basic Books Inc, New York (2018) 
14. Bennett, M.T., Maruyama, Y.: The artiﬁcial scientist: Logicist, emergentist, and 
universalist approaches to artiﬁcial general intelligence. In: Goertzel, B., Ikl´e, M., 
Potapov, A. (eds.) Artiﬁcial General Intelligence, pp. 45-54. Springer Nature, 
Cham (2022) 
15. Bennett, M.T.: Emergent causality and the foundation of consciousness. In: Arti-
ﬁcial General Intelligence. Springer (2023) 
16. Simmons, G.: Comment on is complexity an illusion? Artiﬁcial General Intelligence 
(2025) 
17. Perrier, E., Bennett, M.T.: Position: stop acting like language model agents are 
normal agents (2025), https://arxiv.org/abs/2502.10420 
18. Bennett, M.T.: Symbol emergence and the solutions to any task. In: Artiﬁcial 
General Intelligence. Springer (2022) 
19. Bennett, M.T., Maruyama, Y.: Philosophical speciﬁcation of empathetic ethical 
artiﬁcial intelligence. IEEE Trans. Cogn. Dev. Syst. 14(2), 292-300 (2022) 
20. Bennett, M.T.: On the computation of meaning, language models and incompre-
hensible horrors. In: Artiﬁcial General Intelligence. Springer (2023) 
21. Bennett, M.T.: Compression, the fermi paradox and artiﬁcial super-intelligence. 
In: Artiﬁcial General Intelligence, pp. 41-44. Springer (2022) 
22. Seth, A., Bayne, T.: Theories of consciousness. Nat. Rev. Neurosci. (2022)

48
M. T. Bennett
23. Ciaunica, A., Shmeleva, E.V., Levin, M.: The brain is not mental! coupling neuronal 
and immune cellular processing in human organisms. Front. Integrat. Neurosci. 
(2023) 
24. Bennett, M.T., Welsh, S., Ciaunica, A.: Why is anything conscious? Preprint (2024) 
25. Evers, K., et al.: Preliminaries to artiﬁcial consciousness: a multidimensional 
heuristic approach. Phys. Life Rev. 52, 180-193 (2025). https://doi.org/10.1016/ 
j.plrev.2025.01.002 
26. Fields, C., Albarracin, M., Friston, K., Kiefer, A., Ramstead, M.J., Safron, A.: How 
do inner screens enable imaginative experience? applying the free-energy principle 
directly to the study of conscious experience. Neurosci. Consciousness (2025) 
27. Derrida, J.: Writing and diﬀerence. U of Chicago P (1978)

Ethically Permissible Pursuit of Quantum 
Consciousness 
Selmer Bringsjord1(B), Naveen Sundar Govindarajulu2, Brian McDermott3, 
and Alexander Bringsjord4 
1 Rensselaer AI and Reasoning (RAIR) Lab, Rensselaer Polytechnic Institute (RPI), 
Troy 12180, USA 
Selmer.Bringsjord@gmail.com 
2 RAIR Lab, RPI, Troy, NY 12180, USA 
3 Department of Mechanical, Aerospace and Nuclear Engineering RPI, Troy, 
NY 12180, USA 
bjmcder@gmail.com 
4 Lally School of Management, RPI, Troy, NY 12180, USA 
bringa@rpi.edu 
Abstract. Some researchers are now pursuing implementations in quan-
tum computers that, they hope, constitute, or at least causally give 
rise to, consciousness. Is such pursuit unethical? It depends. The term 
'consciousness,' as used among relevant researchers, is polysemous. This 
reﬂects the fact that the study of consciousness in philosophy, and in dis-
ciplines that compared to it are newcomers (e.g. AI/AGI) to such study, 
is carried out by competing schools of researchers, most if not all marked 
by their own: particular targeted type of consciousness as an object of 
study; position on how it's informally or formally deﬁned/measured, cor-
related with observable phenomena, and created (e.g. in implemented 
computation). One school is Integrated Information Theory. It targets 
phenomenal consciousness, oﬀers no formal deﬁnition but relies on the 
customary informal characterization of this type of consciousness, does 
oﬀer a measurement scheme (\PhiΦ), correlates states of—using the estab-
lished abbreviation—p-consciousness with computation, and, of late, is 
aiming at creating quantum consciousness. After laying out the broad 
landscape, we argue that this research permutation is ethically impermis-
sible. Part of the landscape is our diﬀerent, competing school traceable 
back to AI founder John McCarthy, a pursuit of AGIs that have cogni-
tive consciousness. We encapsulate the pursuit of quantum consciousness 
in this school (at the level of epistemic propositional logic), and relate 
why this pursuit appears in contrast to be ethically permissible. 
Keywords: quantum computation · consciousness · ethics 
1
Introduction 
Quantum computers are quickly maturing in their physical realization, with var-
ious estimates placing the emergence of fault-tolerant, general-purpose quantum 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 49-59, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_6

50
S. Bringsjord et al.
computing occurring at some point over the next decade [ 27]. Even the popular 
media is now in the business of routinely reporting on the use of quantum com-
puters, in part by conveying ﬂorid claims (e.g. discovery of new states of matter) 
from certain companies regarding the prowess their quantum computers have 
already produced. Some of the researchers and managers in this space are now 
speciﬁcally pursuing implementations in their computers which they hope are, 
or causally give rise to, consciousness. Is this pursuit ethically permissible? It 
depends—in no small part speciﬁcally on what type of consciousness is targeted. 
If phenomenal consciousness (= p-consciousness) is the target, as it now con-
cretely is for certain researchers (e.g. for Neven et al. [ 21], using one or more 
of Google's quantum computers) (a) working under inspiration from their read-
ing of some Roger Penrosean prose, and (b) under guidance from the paradigm of 
integrated information theory (IIT), then this pursuit, when engaged in earnest, 
is, we momentarily argue, ethically impermissible. 
2
What Is Consciousness? Two Answers 
While perhaps distressing from the standpoint of AI science and engineering, 
even stopping at but two answers to the question is inevitably to leave many 
schools of alternative thought on the matter aside—but for economy, stop we 
do. (We return to this issue in Sect. 6.2, when replying to the objection that we 
ought to consider theories of consciousness beyond integrated information theory 
and our theory of cognitive consciousness: We consider Baars' global workspace 
theory (GWT).) We now quickly encapsulate two targets: phenomenal conscious-
ness (= p-consciousness), and cognitive consciousness (= c-consciousness). 1
2.1
Target: Phenomenal Consciousness 
When the target is to imbue an artiﬁcial agent implemented in a computing 
machine of any sort with p-consciousness, the idea is that agent will experience 
"what it's like"-to-XX states. In the human case, there is for example something it 
feels like to take a dip in the geothermal hot springs of Iceland. If the water 
is hot enough, such a dip can cause an experience of pain. From the point
1 As we point out below, c-consciousness has roots in the modern era in the logicist 
approach to AI explicated by John McCarthy, but echoes of of c-consciousness, in 
primitive form, can be seen, within philosophy, in the concept of qualia-free access 
consciousness (a-consciousness) introduced by Block [ 6] in contrast to qualia-laden 
p-consciousness. Block's [ 6] deﬁnition (p. 231) of a-consciousness is that a state of 
some agent is a-conscious if and only if it's poised (a) to be used as a premise in 
reasoning, (b) for rational control of action, and (c) for rational control of speech. 
Block informs his readers (p. 231) that condition (c) isn't necessary, since—as he 
sees matters—nonlinguistic creatures can be a-conscious in virtue of their states 
satisfying only (a) and (b). The ﬁrst author has pointed out [ 7] that a run-of-the-
mill database application currently running on a laptop is a-conscious, since such an 
application satisﬁes Block's three clauses (a)-(c). 

Permissible Pursuit Quantum Consciousness
51
of view of science, especially from that of the formal sciences (in which the 
coin of the realm is precision secured by using mathematics and formal logic 
to deﬁne relevant concepts in third-person content, from which theorems can 
be secured), that p-consciousness hasn't been deﬁned anywhere in the literature 
logico-/mathematically is simply sad. But this is reality. 2
2.2
Target: Cognitive Consciousness 
At the founding of AI in 1956, attendees Simon and Newell introduced the 
groundbreaking AI system LogicTheorist, which automatically produced the-
orems in the propositional calculus that had to that point been the province 
of human minds [ 22]. Neither of these two thinkers broached the subject of 
whether an AI system of the logicist variety had or soon would have a form of 
consciousness—but McCarthy soon did: he held that a form of consciousness 
could consist in the possession of and reasoning over suﬃciently rich declarative 
information, expressed in formulae in the formal languages of formal logics [ 20]. 
S Bringsjord and colleagues have cultivated out of the McCarthy kernel a robust, 
axiomatic theory of cognitive consciousness (TCC), with a formal measurement 
scheme, \LambdaΛ, for measuring the amount of c-consciousness in an agent (e.g. see 
[ 9]). 
This brand of consciousness is present only when the agent that bears it has, 
by virtue of internal declarative formulae and reasoning over them, a robust 
ensemble of cognitive attitudes, which correspond directly to a relevant set of 
verbs bound up with human cognition as long investigated in cognitive psychol-
ogy and cognitive science [ 2]. The set of these verbs includes: believing, knowing, 
perceiving, communicating (in a natural language, and perhaps also a formal 
language that might be used in, say, mathematics), hoping, fearing, intending, 
and so on ad indeﬁnitum. In c-conscious states, one or more of these attitudes, 
formalized as intentional operators, range over content also expressed as logical 
formulae. The example of the presumed c-consciousness of Descartes [ 17] when  
correctly reasoning to the conclusion that he must exist from the supposition 
that he doesn't, the famous cognito ergo sum, is shown in Fig. 1. While there is 
no calculated \LambdaΛ value given here, even a cursory look at the formulae active here 
(expressed as s-expressions) will reveal that there is non-trivial complexity. For 
example, one crucial formula expresses the belief, at a time tt during the stretch 
of consciousness in question, that I (= Descartes) perceive that I believe that I 
do not exist. This is a layering of three intensional operators; that alone makes 
for a non-trivial formula overall. In general, \LambdaΛ yields multiple matrices built up 
from the readings of the complexity of formula that constitute cognition, and
2 The absence of a formal deﬁnition is why S Bringsjord has been more than willing 
to oﬀer monetary prizes to anyone who can even rationally endeavor to engineer a p-
conscious robot [ 11]. Philosophers since at least [ 15] have often called the attempt to 
naturalize phenomenal consciousness the hard problem, but this problem seems quite 
premature from the vantage point of formal logic and analytic philosophy. The ﬁrst 
meaningful step would be to deﬁne p-consciousness, at a minimum via a relative 
axiom system for it. 

52
S. Bringsjord et al.
built up as well from the complexity of the arguments and proofs that support 
various intentional attitudes. The kernel of \LambdaΛ is the longstanding framework in 
mathematical logic for calculating the complexity of purely extensional formula, 
for instance by calculating Boolean rank and quantiﬁer complexity. For the ﬁrst 
version of \LambdaΛ, in which no measure of the complexity of justifying arguments and 
proofs for intensional attitudes ﬁgures, see [ 9]. For how the extension of this 
version of \LambdaΛ works when such justiﬁcations are also measured, see [ 14]. For an 
introduction to the axiom system for c-consciousness, see [ 8]. TCC requires for 
underlying logics ones that are peerlessly expressive; they are known as cognitive 
calculi; a short introduction is provided in the appendices of [ 13]. 
Fig. 1. \LambdaΛ Applied to Descartes' cogito ergo sum. This simulation, with corresponding 
measurements based on \LambdaΛ, expresses the level of c-consciousness of Descartes when 
thinking through the impossibility of doubting his own existence. An exercise toward 
understanding the somewhat technical \LambdaΛ measure, we suggest using [ 9] to calculate  
precisely the matrix of c-consciousness possessed by Descartes during his cogitation. 
3
The Neven et al. Search for Quantum P-Consciousness 
In this short paper we can't analyze this team's pursuit of quantum p-
consciousness. We point out only that while Penrose [ 23] famously suggested 
that perhaps a human conscious moment occurs whenever gravity induces a

Permissible Pursuit Quantum Consciousness
53
quantum mechanical superposition to collapse, the Neven et al. team "[P]osit[s] 
that a moment of consciousness arises not when a superposition collapses, but 
when it forms. The structure of the evolving superposition and the paths the 
experiencing quantum system follows within it determine the qualities of the con-
scious experiences, 'what it is like to be' that system in that state." 3 ( [  21], p. 3) 
Note that, unmistakably, experiments involving Google's quantum computer in 
accordance with the postulate here, carried out by the team in question, could, 
for all they know, give rise in particular to the experience of pain. 4
4
The Argument for Ethical Impermissibility 
Let's consider someone we'll call 'Villy.' So as not to beg any questions, we 
assume only that Villy has a tendency to sometimes do "questionable" things. 
For example, the other day, while hiking in the Adirondack Park with a riﬂe 
that could serve as a suitable last resort against a charging black bear, 5 and 
coming upon a cabin in the woods with internal shades drawn down on each 
of its two front windows, and not seeing anyone around outside the structure, 
Villy decides on a whim for his amusement to shoot high-caliber bullets through 
the thin, closed wooden front door of the cabin. Leaving aside as tangential 
any moral issues arising from having and using a gun per se, we assume our 
readers will agree that Villy has done something that is ethically impermissible— 
independent of whether in fact any bullet hits someone. What's the basis for such 
a judgement? No doubt something very much like the following principle:
3 It's crucial to note that the moral status of this team's work is in no way impugned 
by us on the basis of any such claim as that their postulate is false. The truth-
value of the postulate, or its probability of holding, is irrelevant to the application 
of principle \overline{\mathcal{P}}P. 
4 This observation is further strengthened by the fact that the pursuit here is aligned 
with IIT/\PhiΦ (for info on which, see [ 28]), a pair explicitly conﬁgured to systematize 
and measure "what it's like to" p-consciousness. But there are of course alterna-
tive positions on the nature of computation required to serve as a substrate for 
p-consciousness—& on these frameworks too we perceive ethical impermissibility. 
There is for example the ﬁrst such framework to be accompanied with anything 
like a mature mathematical understanding of computation [coming of necessity then 
after the\lambdaλ-calculus and automata (including what we today call 'Turing machines')]: 
viz. the identiﬁcation of p-conscious states with automata states, ﬁrst announced with 
at least appreciable precision by Hilary Putnam in his "The Nature of Mental States" 
[ 25]. The idea is quite simple, relying as it does upon but a smidgeon of recursion 
theory [to which Putnam seminally contributed (e.g. via contributing to the negative 
solution to Hilbert's 10th Problem, and to the earliest precise conception of machine 
learning, the logic-based learning in the limit framework [ 24]). 
5 The Adirondack Park, entirely within New York State, is the largest wilderness 
area in the lower-48 states of the U.S., home to approximately 4,000 black bears, 
with males reaching 600 pounds and a height of 7 ft. The ﬁrst author can speak of 
his encounter with such creatures. 

54
S. Bringsjord et al.
\overline{\mathcal{P}}P: If an agent  \mathfrak{a}_1a1 premeditatedly performs some action aa remotely in an envi-
ronment ee that is for \mathfrak{a}_1a1 self-pp-conscious opaque, and the consequences of 
aa are believed by \mathfrak{a}_1a1 to quite possibly (i.e. have a likelihood value of coun-
terbalanced 6) include extreme pp-conscious pain in an agent \mathfrak{a}_2a2 in ee, and  
\mathfrak{a}_1a1 does not believe that it's at least highly likely that performing said aa
in said context will yield some countervailing beneﬁt of very high utility; 
then \mathfrak{a}_1a1's doing aa is ethically impermissible. 
Obviously, the kernel of an indictment of Villy's actions is speciﬁcally that 
he might be causing extreme pain in some agent or agents. It shouldn't be at 
all diﬃcult for the reader to suitably instantiate \overline{\mathcal{P}}P so as to deduce that Villy 
has, in shooting, performed an ethically impermissible action. In an obvious 
parallel proof, instantiating the activity of Neven et al. [ 21] to  \overline{\mathcal{P}}P to detach its 
condemnatory consequent is eﬀortless. 
From the standpoint of logicist AI and cognitive science, this result, it should 
be noted, has value quite aside from opprobrium. The result reveals that per-
ception of non-perception is important to be able to formalize, and model. In 
our latest cognitive calculus, the attention-perception cognitive calculus (\mathcal{APCC}APCC), 
the relevant agent \mathfrak{a}a perceives (internally) that it's not the case that \mathfrak{a}a perceives 
(externally) there exists an xx, there exists a property XX s.t. XxXx (in a scene \SigmaΣ). 
Labeling this formula \phiφ, and assuming for exposition that it's the sum total of 
formulae active at time tt for \mathfrak{a}a, we note that part of the \LambdaΛ matrix for the level of 
c-consciousness of \mathfrak{a}a at tt is simply 2, the count of intensional operators holding 
at tt. 
5
The Search for Quantum C-Consciousness 
Physical realizations of quantum computers are presently in what has been 
termed the noisy intermediate-scale quantum (NISQ) era. NISQ devices possess 
qubit registers numbering in the tens to hundreds of qubits, and can maintain 
quantum coherence for hundreds to thousands of individual quantum instruc-
tions, or gates. While not suﬃcient for arbitrarily long-running and complex 
quantum algorithms (such as would ultimately be needed for high levels of 
cognitive consciousness in artiﬁcial agents), NISQ devices have demonstrated 
domain-speciﬁc acceleration of applications that require sampling from complex, 
strongly-correlated probability distributions [ 26]. 
For researchers in the TCC-\LambdaΛ school to conclude that quantum systems 
possess appreciable c-consciousness, for a suﬃciently rich set of operators \mathbf{U}U, 
they would expect to see \LambdaΛ measures increase at an asymptotically higher rate 
than the equivalent classical implementation. We are exploring this possibil-
ity using a classical simulator, as well as the 127-qubit IBM Eagle processor 
(ibm_rensselaer).
6 The need for economy precludes the unpacking of this concept in our inductive 
cognitive calculi, e.g. in the one known as \mathcal{IDCEC}IDCEC [ 12]. The reader can understand 
propositions that for a rational agent are counterbalanced to ones that might well 
be true, and might well be false. 

Permissible Pursuit Quantum Consciousness
55
For these researchers, the central premise of a quantum implementation under 
the \LambdaΛ framework is to encode the agent's initial belief set \mathcal{B}B as a quantum state, 
|\psi_0\rangle|ψ0⟩. They then deﬁne a unitary operator \mathbf{U}U that updates the state of \mathcal{B}B in 
accordance with the complexity of what is believed, and the length of arguments 
and proofs that justify belief in the formulae in\mathcal{B}B. The quantum state is then read 
out into a classical register, producing an updated belief set \mathcal{B}'B′ and associated 
justiﬁcations. This is used to reinitialize a new belief based at the next time 
step, which is then re-evaluated accordingly. The \LambdaΛ complexity of such "chains" 
on the IBM quantum machine can then be compared with the \LambdaΛ complexity 
of the counterpart classical case. We hypothesize that quantum computation 
leads to signiﬁcantly higher levels of c-consciousness, and we plan to share some 
concrete measures at AGI-2025 that support this expectation. 
5.1
The Ethical Status of The Search for Quantum C-Consciousness 
We simply note that, as a matter of formal logic, \overline{\mathcal{P}}P cannot be instantiated in 
light of any such activity as just summarized, w.r.t. the researchers in question. 
In particular, e.g., consider the excerpt "the consequences of aa are believed by  
\mathfrak{a}_1a1 to quite possibly include extreme pp-conscious pain in an agent \mathfrak{a}a" from  the  
principle in question. C-consciousness is a purely syntactic aﬀair; whether or not 
some stretch of c-consciousness obtains in a given computer is based all and only 
on active formulae in the formal languages of formal logic, and on arguments 
and proofs automatically found by the AI in question (that justify such things 
as belief and knowledge); no pain (nor for that matter any other p-conscious 
state) is in the mind of any relevant researchers suspected to even possibly arise 
from such information being used in computation of any sort. 
6
Objections; Rejoinders 
6.1
"The Link Between Q-Computation and Consciousness Is 
Purely Speculative" 
The objection here can be encapsulated thus: "The foundations of your critique 
rests on a highly speculative nexus between quantum computation and (phe-
nomenal) consciousness. The premise that quantum superposition formation or 
collapse could generate phenomenal experience remains entirely hypothetical, 
with no empirical threshold criteria or probabilistic estimates oﬀered." 
For the sake of argument, we concede the claim issued here, end to end. Our 
argument nonetheless remains completely intact. The reason is that no premise in 
our argument asserts that there in fact is a genuine connection between any prop-
erty possessed by the quantum computation in question, and p-consciousness. 
To be clearer, let \mathcal{C}_qCq be some quantum computation (of some quantum com-
puter) hoped by the relevant researchers 7 to cause some corresponding stretch
7 I.e., either those on the IIT-oriented team we have denoted, or some other 
researcher/s who are counterparts, e.g. some who have aﬃrmed another theory 
\mathcal{T}\ (\not= \mbox{IIT} \mbox{\& } \not= \mbox{TCC}T (̸= IIT& ̸= TCC) of p-consciousness and have parallel hopes of causing some 
stretch of p-consciousness by causing \mathcal{C}_qCq in some quantum computer. 

56
S. Bringsjord et al.
P_qPq of p-consciousness. Such a hope is provably consistent with even the mathe-
matical impossibility that \mathcal{C}_qCq cause what is here hoped for. This is easy enough 
to grasp by turning back to the parable of Villy and the cabin: For suppose 
that, unknown to Villy, who regards it as entirely possible that those inside may 
perish from his pyrotechnics, the interior of the structure is permeated with a 
gas that makes any form of biological life impossible. What diﬀerence does it 
make with regard to the moral status of Villy's actions? It makes no diﬀerence, 
because for all he knows, there might well be, say, three innocent young humans 
who will be thrust into excruciating pain from gunshots, followed by the pain 
of a slow death from bleeding out—this experience of course being an instance 
of the general variable P_qPq. Villy by principle \overline{\mathcal{P}}P, which hinges on the beliefs of 
the agent within its scope, not on the truth-value of the targets of these beliefs, 
has behaved unethically. 
This means, by the way, that apparently there wasn't anything fundamen-
tally defective, ethically speaking, infecting concern on the part of a Google 
employee who was ﬁred because he thought the AI-software systems he was 
himself engineering and interacting with were—to use his word, tantamount to 
'conscious'—sentient (for a summary see [ 19]). 
6.2
"On Pain of Failure, You Ignore Other Theories 
of Consciousness" 
We conceded above that there are many theories of many types of conscious-
ness in the literature; the cardinality of the set of all such theories may in fact 
border on the scientiﬁcally scandalous. 8 Nonetheless, a critic may hold that 
our concession achieves nothing ratiocinatively, and indeed some may hold that 
which we omit constitutes a fatal defect in the case we have made for the 
immorality of one type of pursuit of quantum consciousness. In light of this 
objection, we brieﬂy consider a theory much-discussed in the consciousness lit-
erature: viz., Baars' global workspace theory (GWT) (e.g. from among numerous 
other publications, see [ 3- 5]). 
Put simply, the central and driving postulate of GWT is that consciousness 
is associated with or is constituted by or simply is the global availability of 
information in the brain. Leaving aside what the ﬁrst author views as a fatal
8 Theoretical physics avoids such scandals entirely, for the simple reason that while 
incompatible "theories" therein compete even in well-trodden spheres, calcula-
tion and axiomatization secure a ﬁrm foundation no matter what. In the case of 
consciousness, alas, to our knowledge only TCC provides an authentic axiomatiza-
tion, since authenticity can only come by way of a formal axiom system that can be 
plumbed proof-theoretically. In this regard, the case of special relativity is enlight-
ening and in fact conﬁrmatory: For coverage of a formal proof independent of any 
"interpretation" of special relativity, based on an axiomatization that is wholly agnos-
tic relative to interpretation [ 1], see [ 18]. Along the same line, but more dramatically, 
note that Newton posited an absolute temporal frame of reference (viz. God's), and 
that such a frame of reference certainly appears to be consistent with the purely 
calculative and core-declarative aspects of Einsteinian special relatively [ 16]. 

Permissible Pursuit Quantum Consciousness
57
objection to GWT (in short, that surely in light of the physical possibility of 
aliens with mental lives like ours, but bereft any physical bodies/brains like 
ours, brain-based theories of consciousness are not suﬃciently general; see [ 10]), 
at the end of the day GWT clearly targets p-consciousness, and thus GWT-
based researchers acting in parallel with Neven et al. are in the same sinking 
boat, ethically speaking. 
Finally, and importantly from a scholarly perspective, we point out that 
Baars' GWT has been at times labeled in a way that may lead some astray, with 
respect to it being completely diﬀerent than our own TCC. We refer to termi-
nology here—but sometimes terminology can mislead even clever people, against 
their own underlying wishes. The speciﬁc issue we point to is that Baars himself 
has classiﬁed his GWT as a Cognitive Theory of Consciousness [[ 3]; underlining 
ours]. In Baars' sense of 'cognitive,' unaligned as the term is with logicist AI 
and cognitive science, and with formal methods and theoretical computer sci-
ence (the latter being ﬁrmly logicist itself), there's no particular need to invoke 
even the rudiments of formal logic. There is then no need in his mind, nor in 
any GWT devotee, to move in the slightest toward TCC. 9
Acknowledgements. We have numerous debts; they, and our gratitude, will con-
tinue to remain in place as research along the direction introduced herein matures: 
Interaction with Tononi and Koch regarding the IIT/\PhiΦ pair during a sustained investi-
gation of consciousness sponsored by SRI stimulated S Bringsjord and NS Govindara-
julu to invent a superior logicist alternative, and—during this SRI-backed project—the 
original simulation of cogito ergo sum under TCC, and the measurement of it under 
version 1 of \LambdaΛ, was created by NSG during that support. We have beneﬁtted greatly 
from discussion with Brian Krause and Robert Marks. Three AGI-25 referees provided 
insightful reviews for which we are grateful; we have endeavored to address all their 
concerns. Support in the past from ONR for a MURI (on which S Bringsjord and 
Govindarajulu were researchers) devoted to machine/robot ethics in part enabled the 
work reported here, and a current ONR award (# N00014-22-1-2201) to S Bringsjord 
& Govindarajulu for r&d in the intersection of logic-based AI (speciﬁcally automated 
reasoning and planning) and the seminal computational cognitive architecture ARCA-
DIA of Bello & Bridewell was essential. Without investigation of the logic of atten-
tion and perception, there would have been no Villy, no \overline{\mathcal{P}}P, and no new cognitive 
calculus (\mathcal{APCC}APCC) to formalize cognitive perception. Without leadership in the inter-
section of AI and consciousness from Antonio Chella on countless fronts, our research 
program wouldn't exist. We are deeply grateful for the IBM-RPI quantum computer, 
and access thereto. Finally, without encouragement from Laura Steckman to think seri-
ously about the ethical dimension of the quantum pursuit of consciousness, none of the 
inaugural work reported on herein would have come to life.
9 For economy, we forego presentation of and rebuttals to objections regarding the 
unspeciﬁed-herein correspondence between the higher \LambdaΛ values and quantum com-
putation. In particular, we provide here no technical speciﬁcations (e.g. how many 
qubits and how long they cohere) for computing \LambdaΛ from underlying automated-
reasoning processes. 

58
S. Bringsjord et al.
References 
1. Andréka, H., Madarász, J.X., Németi, I., Székely, G.: A logic road from special 
relativity to general relativity. Synthese, pp. 1-17 (2011). https://doi.org/10.1007/ 
s11229-011-9914-8, http://dx.doi.org/10.1007/s11229-011-9914-8 
2. Ashcraft, M., Radvansky, G.: Cognition. Pearson, London, UK (2013), This is the 
6th edition. 
3. Baars, B.: A Cognitive Theory of Consciousness. Cambridge University Press, 
Cambridge, UK (1988) 
4. Baars, B.: The Cognitive Access Hypothesis: Origins and Recent Evidence. Trends 
Cogn. Sci. 6(1), 47-52 (2002) 
5. Baars, B.: Global workspace theory of onsciousness: toward a cognitive neuro-
science of human experience. Prog. Brain Res. 150, 45-53 (2005) 
6. Block, N.: On a confusion about a function of consciousness. Behav. Brain Sci. 18, 
227-247 (1995) 
7. Bringsjord, S.: Consciousness by the lights of logic and common sense. Behav. 
Brain Sci. 20(1), 227-247 (1997) 
8. Bringsjord, S., Bello, P., Govindarajulu, N.: Toward axiomatizing consciousness. 
In: Jacquette, D. (ed.)  The Bloomsbury Companion  to  the Philosophy of Con-
sciousness, pp. 289-324. Bloomsbury Academic, London, UK (2018) 
9. Bringsjord, S., Govindarajulu, N.: The theory of cognitive consciousness, and \Lambda Λ
(Lambda). J. Artif. Intell. Consciousness 7(1), 155-181 (2020), http://kryten. 
mm.rpi.edu/sb_nsg_lambda_jaic_april_6_2020_3_42_pm_NY.pdf, The  URL  
here goes to a preprint of the paper. 
10. Bringsjord, S.: Baars falls prey to the timidity he rejects: commentary on Baars on 
Contrastic analysis. Psyche 1(10) (1994), http://journalpsyche.org/ﬁles/0xaa12. 
pdf 
11. Bringsjord, S.: Oﬀer: One billion dollars for a conscious robot. If you're honest, 
you must decline. J. Consciousness Stud. 14(7), 28-43 (2007), http://kryten.mm. 
rpi.edu/jcsonebillion2.pdf 
12. Bringsjord, S., et al.: Argument-based inductive logics, with coverage of compro-
mised perception. Front. Artif. Intell. 6 (2024). https://doi.org/10.3389/frai.2023. 
1144569, https://www.frontiersin.org/articles/10.3389/frai.2023.1144569/ 
13. Bringsjord, S., Govindarajulu, N.S., Licato, J., Giancola, M.: Learning Ex Nihilo. 
In: GCAI 2020. 6th Global Conference on Artiﬁcial Intelligence. EPiC Series in 
Computing, vol. 72, pp. 1-27. International Conferences on Logic and Artiﬁcial 
Intelligence at Zhejiang University (ZJULogAI), EasyChair Ltd, Manchester, UK 
(2020). https://doi.org/10.29007/ggcf, https://easychair.org/publications/paper/ 
NzWG 
14. Bringsjord, S., Govindarajulu, N.S., Oswald, J.: Universal cognitive intelligence, 
from cognitive consciousness, and lambda (\Lambda Λ). In: Chella, A. (ed.) Computational 
Approaches to Conscious Artiﬁcial Intelligence, Machine Consciousness, vol. 5, pp. 
127-168. World Scientiﬁc Publishing, Singapore (2023). https://doi.org/10.1142/ 
13421, http://kryten.mm.rpi.edu/ch5-main.pdf, The URL here is to an uncor-
rected preprint. 
15. Chalmers, D.: The Conscious Mind. In: Search of a Fundmental Theory. Oxford, 
Oxford, UK (1996) 
16. Craig, W.L.: Time and Eternity: Exploring God's Relationship to Time. Crossway, 
Wheaton, IL (2001)

Permissible Pursuit Quantum Consciousness
59
17. Descartes, R.: Meditations on First Philosophy. Cambridge University Press, Cam-
bridge, UK (2017), This is the 2nd edition. The books includes selections taken 
from "Objections and Replies." John Cottingham is the editor of this book. 
18. Govindarajalulu, N.S., Bringsjord, S., Taylor, J.: Proof veriﬁcation and proof dis-
covery for relativity. Synthese 192(7), 2077-2094 (2015) 
19. Grant, N.: Google ﬁres engineer who claims its A.I. Is conscious. New York Times, 
3 July 2022 
20. McCarthy, J.: Making robots conscious of their mental states. Technical 
Report, AAAI Technical Report SS-95-05 (1995), https://www.aaai.org/Papers/ 
Symposia/Spring/1995/SS-95-05/SS95-05-013.pdf, McCarthy continued to reﬁne 
this paper through the years, from at least 1995 to 2002. A later version of the paper 
can be found at. http://jmc.stanford.edu/articles/consciousness/consciousness.pdf 
21. Neven, H., et al.: Testing the conjecture that quantum processes create conscious 
experience. Entropy 26(6) (2024). https://doi.org/10.3390/e26060460, https:// 
www.mdpi.com/1099-4300/26/6/460 
22. Newell, A., Simon, H.: The logic theory machine: a complex information process-
ing system. P-868 The RAND Corporation, pp. 25-63 (1956), An almost exactly 
similar version of this paper can be found in IRE Transactions on Information 
Theory, vol. 2, pp. 61-79. 
23. Penrose, R.: The Emperor's New Mind. Oxford, UK, Oxford (1989) 
24. Putnam, H.: Trial and error predicates and a solution to a problem of Mostowski. 
J. Symb. Log. 30(1), 49-57 (1965) 
25. Putnam, H.: The nature of mental states. In: Mind, Language, and Reality: Philo-
sophical Papers, vol. 2, pp. 429-440. Cambridge University Press, Cambridge, UK 
(1975), The original year of publication of this paper is 1967. 
26. Robledo-Moreno, J., et al.: Chemistry beyond exact solutions on a quantum-centric 
supercomputer (2024), https://arxiv.org/abs/2405.05068 
27. Sevilla, J., Riedel, C.J.: Forecasting timelines of quantum computing (2020), 
https://arxiv.org/abs/2009.05045 
28. Tononi, G., Boly, M., Massamini, M., Koch, C.: Integrated information theory: from 
consciousness to its physical substrate. Nat. Rev. Neurosci. 17, 450-461 (2016)

Is Phenomenal Consciousness Necessary 
for AGI? A Review of the Theoretical Landscape 
Ignacio Cea1,2envelope symbol
1 Department of Philosophy, Faculty of Religious Sciences and Philosophy, Temuco Catholic 
University, Temuco, Chile 
igneocj@gmail.com 
2 Faculty of Philosophy and Humanities. Department of Philosophy, Universidad Alberto 
Hurtado, Santiago, Chile 
Abstract. Is phenomenal consciousness a necessary requirement for the creation 
of artiﬁcial general intelligence (AGI)? Despite its philosophical, scientiﬁc, ethical 
and technological signiﬁcance, this question has received little systematic atten-
tion. This paper offers a structured review of current positions on the matter, aim-
ing to clarify the conceptual landscape and identify key points of contention. We 
begin by examining the dissociation view between phenomenal consciousness and 
general intelligence, according to which subjective experience and human-level 
general intelligence are both conceptually and empirically separable, suggesting 
no necessary connection between them. We then contrast this with the association 
view, which suggests that phenomenal consciousness plays an indispensable role in 
enabling the ﬂexibility, autonomy, and meaningfulness that characterize human-
level intelligence. This view can be further divided into two groups. Accord-
ing to the  functionalist perspectives, phenomenal consciousness is identical to 
some functional property needed for general intelligence (e.g., global information 
broadcasting). According to the phenomenological perspectives, phenomenal con-
sciousness is, in itself, constitutive of some key aspect of general intelligence, e.g., 
genuine semantic understanding or self-concerned adaptive behavior. Finally, we 
outline future research directions, highlighting key unresolved questions: whether 
true general intelligence can be fully deﬁned in functional terms; whether phe-
nomenal consciousness has any causal role in general-purpose intelligent behavior; 
to what extent general intelligence and functional consciousness overlap; and to 
what extent functional and phenomenal consciousness can be conceptually and 
empirically dissociated. Rather than taking a position, this paper aims to map the 
theoretical space and identify the foundational questions that future work must 
confront. 
Keywords: Consciousness-Intelligence relationship cdot Functionalism cdot
Phenomenology 
1 
Introduction 
Developing Artiﬁcial General Intelligence (hereinafter "AGI") is the primary goal both 
of leading tech companies in the ﬁeld of artiﬁcial intelligence (hereinafter "AI") and 
of associated scientiﬁc research [1-3]. In other words, AGI is the "holy grail" of AI
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 60-71, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_7 

Is Phenomenal Consciousness Necessary for AGI?
61
development and research [4]. AGI is commonly understood as artiﬁcial intelligence 
that replicates the generality of human-level natural intelligence, not limited to a speciﬁc 
domain (e.g., advanced reasoning), but encompassing the wide range of domains in which 
humans exhibit intelligent behavior. This includes attributes such as creativity, common 
sense, ﬂexibility, and adaptability to novel and changing contexts [5-8]. 
However, beyond the lack of a universally accepted deﬁnition of AGI within the 
relevant research and technological development ﬁelds [2, 9], there is also no consensus 
regarding the speciﬁc requirements necessary to achieve AGI—particularly when we 
consider the versatility, richness, and adaptability of human intelligent behavior [6, 7, 10]. 
One particularly important and unresolved issue is whether achieving such a degree of 
general intelligence necessarily requires AI to also possess phenomenal consciousness, 
understood as subjective experience or what it feels like to perceive the world from a 
ﬁrst-person perspective [11-13]. 
This question is of critical importance, given that phenomenal consciousness is 
widely considered a sufﬁcient condition for attributing moral patiency to an artiﬁcial 
system, thus making it morally necessary to avoid causing it unjustiﬁed suffering or 
other forms of harm [4, 12, 14]. Moreover, a phenomenally conscious AGI might also 
be regarded as a moral agent, bearing moral obligations such as responsibility for its own 
actions [10, 15]. In other words, a conscious AGI—by virtue of its advanced cognitive 
and conscious capacities—could be considered a person in the moral sense, with rights 
and duties, and may therefore deserve legal protection and social integration as an entity 
with intrinsic, not merely instrumental, value [10, 16]. 
This review paper aims to establish the minimal theoretical foundations required 
for a systematic examination of this issue. The guiding question is: Is phenomenal 
consciousness a necessary requirement for the creation of AGI? If the answer is yes, then 
the pursuit of AGI would be inherently tied to the pursuit of artiﬁcial consciousness— 
with all the associated philosophical, scientiﬁc, and technological challenges [17-20], 
as well as the normative implications previously outlined. If, on the other hand, the 
answer is no, then the creation of AGI would become signiﬁcantly more feasible. Its 
deployment could proceed without the ethical complications associated with ascribing 
full moral personhood, though still subject to the considerable ethical responsibilities 
involved in developing and using such potentially disruptive technologies [21-23]. 
In short, beyond its inherent scientiﬁc and philosophical relevance, the answer to 
this question entails far-reaching ethical, social, and technological implications—par-
ticularly in the context of the unprecedented and disruptive technological revolution 
currently underway [24-27]. 
As expected, the literature is divided on this question, which remains largely unex-
plored and lacks a systematic debate, clear opposing positions, and a structured body of 
arguments. For instance, various comprehensive overviews have mapped current theories 
of consciousness [28-30], but none have systematically addressed whether phenome-
nal consciousness is necessary for AGI. Nevertheless, various stances can be identiﬁed 
or inferred within the literature that connect research in consciousness science (neuro-
science, psychology, philosophy), artiﬁcial consciousness (engineering, computer sci-
ence, neuroscience, philosophy), and AGI (engineering, computer science, philosophy).

62
I. Cea
Within this multi/interdisciplinary domain, two dominant positions emerge, each offer-
ing a contrasting answer to our central question. Importantly, this short paper does not 
attempt to resolve it but aims to map the existing positions and provide a structured 
framework for further inquiry. 
2 
Consciousness - General Intelligence Dissociation View 
We will now describe the ﬁrst group of works, which answer negatively to the question 
regarding the necessity of phenomenal consciousness for AGI. This set of views can 
be referred to as the dissociation view between phenomenal consciousness and general 
intelligence. This perspective holds that subjective experience and human-level general 
intelligence are fully separable, both conceptually and empirically, showing no necessary 
connection between them. 
For example, Anil Seth [31, 32] clearly distinguishes between consciousness and 
intelligence, understanding the former as subjective experience [13] and the latter as the 
set of functional capacities that allow a system to achieve goals in various environments 
[33]. For this reason, he argues against the idea that an AI with sufﬁciently advanced 
intelligent capacities—even at a human level—must necessarily be conscious. More 
speciﬁcally, Seth rejects the notion that surpassing a certain threshold of intelligence 
in AI development is sufﬁcient to generate artiﬁcial consciousness, going so far as to 
suggest that the creation of a non-conscious AGI is entirely plausible [31]. This view 
is based, among other things, on his skepticism toward computational functionalism as 
a plausible metaphysical theory of consciousness, even though he remains open to the 
possibility that other types of functionalism may be correct [32]. 
A related position is found in Gamez [34, 35], who argues that phenomenal con-
sciousness cannot be explained in purely functional-computational terms, since it would 
depend on speciﬁc spatiotemporal physical patterns in the brain substrate. Therefore, 
Gamez maintains that it is entirely possible to conceive of highly advanced artiﬁcial 
agents that are not conscious at all. For him, intelligence is a purely functional trait 
deﬁned as the capacity of a system to make accurate predictions about its environment 
and its own internal state, while consciousness would be more of a structural property, 
possibly substrate-dependent. 
Another important example that shares this general view—that AGI could per-
fectly well be non-conscious—is Bostrom [36]. According to the philosopher, devel-
oping human-level or superhuman AI does not require consciousness, as goal opti-
mization, learning, reasoning, and planning can be achieved through purely functional-
computational processes without subjective experience. Bostrom warns that AGI and 
its likely evolution into superintelligence pose real existential risks to humanity—not 
because such systems have malicious feelings or intentions, but because their reward or 
utility optimization could proceed in ways misaligned with human values and interests. 
A thinker whose position is closely aligned with Bostrom's is Harari [27, 37, 38]. He 
deﬁnes intelligence as the ability to achieve goals, such as maximizing user engagement 
on social media, and understands consciousness phenomenally, as the capacity to feel 
pain, pleasure, love, or hate. Although these often coexist in humans, Harari argues 
there is no reason to assume this is universal: bacteria, plants, and human physiological

Is Phenomenal Consciousness Necessary for AGI?
63
processes exhibit intelligence without consciousness. Thus, computers could reach or 
surpass AGI without ever becoming conscious, just as airplanes ﬂy faster than birds 
without feathers [27]. 
Also part of this group of researchers is the team behind Tononi's Integrated Informa-
tion Theory (IIT) of consciousness [39-41]. According to IIT, consciousness depends 
on a system causally specifying its past and future states as a whole that is informa-
tionally irreducible to its parts. This requires the system to be constituted as a highly 
interconnected and recurrent network. In contrast, AI systems are typically feedforward 
(i.e., unidirectional computational ﬂow from input to output). They lack the recursiv-
ity necessary for the integration of information relevant to consciousness, even if they 
achieve high levels of cognitive and behavioral competence and convincingly simulate 
human behavior. As a result, from the IIT perspective, the creation of AGI does not imply 
or require the generation of phenomenal consciousness, since the two are completely 
independent properties. 
Another author who dismisses consciousness as a requirement for achieving AGI 
is Summerﬁeld [42]. He suggests that the mechanisms enabling general intelligence in 
natural organisms—such as dopamine-based reward circuits—can be understood and 
replicated in purely neurochemical and functional terms, without resorting to phenom-
enally conscious states. Although he acknowledges that in living beings, rewards such 
as pleasure and pain play an essential role in guiding adaptive behavior, he proposes 
that these processes can be modeled as internal reinforcement signals, dispensing with 
their subjective dimension. Thus, for Summerﬁeld, the main challenge in building AGI 
lies in designing sufﬁciently ﬂexible and adaptive value-assignment and motivation sys-
tems that are functionally analogous to those found in animals, but without the need 
to reproduce the affective consciousness that typically accompanies these processes in 
organisms. 
Sharing this general view is Montemayor [43, 44]. The philosopher argues that phe-
nomenal consciousness is not necessary to endow a system with general intelligence 
or to allow it to participate epistemically in human communicative practices. Draw-
ing on the empirical dissociation between attention and consciousness, Montemayor 
argues that attentional processes—particularly motivated joint attention—constitute a 
sufﬁcient functional basis for rational agency and linguistic cooperation, without the 
need for subjective experience. In this framework, cognitive access and attentional con-
trol would adequately explain the intelligent capacities required for AGI. Phenomenal 
consciousness, understood as solipsistic, would be relegated to a contingent biological 
phenomenon and not essential for the unfolding of general intelligence. 
Similarly, Schneider [19] holds that phenomenal consciousness is not necessary 
to achieve AGI or superintelligence. She argues that, just as humans automate mas-
tered tasks so that conscious attention is no longer required (e.g., walking or riding a 
bike), an AGI could, for efﬁciency reasons, perform competently through non-conscious 
computational processes. Furthermore, Schneider emphasizes that endowing a system 
with phenomenal consciousness would require additional engineering effort that is not 
essential for achieving general intelligence or high levels of cognitive performance. 
Computer scientist Margaret Boden [4] holds a similar view, albeit with more nuance. 
She argues that while AGI would require access or functional consciousness—that is,

64
I. Cea
consciousness understood functionally in terms of capacities such as attention, delib-
eration, planning, and self-reﬂective evaluation—in order to behave truly intelligently, 
phenomenal consciousness might not be necessary. Boden clearly distinguishes between 
consciousness as a set of cognitive functions and the subjective experience associated 
with qualia (i.e., qualitative subjective properties), noting that while the former can be 
computationally modeled, the latter remains an unresolved philosophical problem. Thus, 
for her, the construction of a competent AGI demands the replication of functional struc-
tures of consciousness but does not imply or presuppose the generation of phenomenal 
states. A very similar stance is adopted by Shanahan [10], who argues that replicating 
the functional properties of consciousness is sufﬁcient to achieve general intelligence, 
while leaving open the question of whether that would entail the emergence of genuine 
phenomenal consciousness. 
This group of works also includes the proposal by Morris et al. [6]. These researchers 
argue that the deﬁnition of AGI should focus exclusively on performance and generality, 
omitting any reference to internal processes or mechanisms potentially linked to con-
sciousness. In their framework, achieving AGI does not require replicating subjective 
experiences, since these are neither currently measurable in any scientiﬁcally consen-
sual way nor necessary for functional competence. Accordingly, Morris et al. understand 
general intelligence as a purely operational and behavioral matter, detached from any 
requirement of consciousness or subjective affective states. 
Finally, Chalmers [12, 45] also sympathizes with the idea that AGI could achieve 
human-level or even superior intelligence without the need for phenomenal conscious-
ness. Although he acknowledges, through the principle of organizational invariance, 
that a certain kind of functional organization could be nomologically necessary and 
sufﬁcient for conscious experience, he deﬁnes intelligence in purely behavioral terms, 
without requiring the replication of the internal brain-based functional organization asso-
ciated with consciousness and cognition in humans. Therefore, an AI could, in principle, 
achieve generalized behavioral competence sufﬁcient to be considered AGI, through 
purely computational means substantially dissimilar from human brain mechanisms, 
without possessing subjective experience. 
In summary, the reviewed works agree in stating that phenomenal consciousness is 
not a necessary requirement for the creation of AGI, since phenomenal consciousness 
and general intelligence would indeed be dissociable. This is because I) intelligence 
and consciousness are conceptually distinct, and no functional deﬁnition of intelligence 
presupposes subjective experience; II) empirically, multiple examples exist of natural 
systems (like bacteria or plants) that display forms of adaptive intelligence without 
consciousness; III) access/functional consciousness and phenomenal consciousness are 
dissociable. Thus, even if general intelligence requires access consciousness, this would 
not imply the presence of phenomenal consciousness; IV) Phenomenal consciousness 
does not play a causal role in natural general intelligence, and therefore would not be 
causally necessary for replicating general intelligence in artiﬁcial systems; V) phenom-
enal consciousness may be substrate-dependent—that is, reliant on the physical struc-
tures unique to human and animal neurobiology—whereas general intelligence appears 
to be substrate-independent and multiply realizable; and VI) pursuing computational

Is Phenomenal Consciousness Necessary for AGI?
65
efﬁciency in artiﬁcial systems could even favor the development of non-conscious func-
tional intelligences, since conscious processes are energetically costly and slower than 
unconscious automatic processing. 
3 
Consciousness-General Intelligence Association View 
Let us now turn to the works that do propose a strong link between phenomenal con-
sciousness and AGI, suggesting that the former may indeed be a necessary condition 
for the latter—thus answering our central question afﬁrmatively: Is phenomenal con-
sciousness a necessary requirement for the creation of AGI? In contrast to the previous 
view, we will group the following set of works under what can be called the association 
view about consciousness and general intelligence. Within this group, we can further 
distinguish between what we might call functionalist perspectives and phenomenologi-
cal perspectives. The former identify phenomenal consciousness with some functional 
property (e.g., access consciousness), which they consider necessary for AGI. The lat-
ter, by contrast, argue that AGI requires phenomenal consciousness for reasons that 
refer directly to the phenomenology of experience (e.g., the phenomenal character as 
necessary for true semantic understanding and thus genuine intelligence). We begin by 
reviewing the functionalist perspectives. 
3.1 
Functionalist Perspectives 
Within this sub-group, we ﬁnd works that interpret phenomenal consciousness through 
the lens of Global Workspace Theory (GWT) [46-48]. According to this framework, 
consciousness essentially corresponds to the functional notion of access consciousness 
or cognitive access [11, 49, 50]. This refers to the process by which contents selected 
by attentional mechanisms are ampliﬁed and globally broadcast, becoming available to 
specialized modules that use them to guide reasoning, planning, and action. This enables 
ﬂexible coordination, deliberate decision-making, and adaptation to novel contexts. 
Examples of this approach to the relationship between consciousness and AGI are 
found in the proposals by Franklin and colleagues [51-53]. Through the LIDA model 
(Learning Intelligent Distributed Agent), these researchers explicitly implement a global 
workspace architecture in which contents selected by attentional mechanisms are glob-
ally broadcast to modulate action, learning, and decision-making. They argue that this 
conscious cognitive cycle not only reproduces the dynamics postulated by GWT, but is 
also indispensable for endowing agents with the ﬂexibility, adaptability, and meaningful 
learning characteristic of human general intelligence. Similarly, Blum and Blum [54, 
55] propose the Conscious Turing Machine (CTM) model, which is also inspired by 
GWT principles to articulate an architecture where a distributed workspace without a 
central executive coordinates the activity of thousands of specialized processors. While 
they acknowledge that it cannot be entirely ruled out that AGI could be achieved by 
other means, they maintain that a system capable of efﬁciently coordinating distributed 
processing, ﬂexible control, and complex planning—enabled by a global workspace 
architecture—would be highly effective for performing functions typical of general intel-
ligence. In both cases, phenomenal consciousness is conceived as a functional property

66
I. Cea
which, once replicated, not only improves cognitive efﬁciency but is also necessary, or 
at least highly advantageous, for achieving performance comparable to human general 
intelligence. In other words, according to these authors, subjective experience is nothing 
but functionally deﬁned access consciousness as framed by GWT. Therefore, if AGI 
requires access consciousness, it would also entail phenomenal consciousness. 
Other works centrally incorporate the GWT framework but complement it with 
additional theories or functional aspects of consciousness. Jablonka and collaborators 
[56, 57] argue that the accessibility and global broadcasting of information postulated 
by GWT must be accompanied by other functional capacities such as percept integra-
tion over time, ﬂexible value assignment, selective attention, etc. They synthesize these 
capacities under the term "Unlimited Associative Learning" (UAL), understood as the 
ability to form new, ﬂexible, and generalizable associations that allow for open adaptive 
intelligence. Juliani and colleagues [58], in turn, propose that the globalist architecture 
must be supplemented with the capacity to internally generate information disconnected 
from the immediate environment (following the Internal Generative Theory, IGT) [59], 
as well as an internal model of attention (following the Attention Schema Theory, AST) 
[60, 61]. These functions, together, would endow an artiﬁcial agent with the tempo-
ral projection and ﬂexibility that characterize human general intelligence. Bengio [62], 
for his part, proposes complementing the global broadcasting of content with delibera-
tive reasoning mechanisms inspired by Kahneman's "System 2" [63], oriented toward 
optimizing the selection of thoughts to improve efﬁciency in learning and planning. 
Additionally, Dehaene and colleagues [64] integrate GWT with metacognitive capaci-
ties, arguing that consciousness also requires the ability to monitor, evaluate, and correct 
one's own mental states—a condition they consider fundamental for general intelligence 
and ﬂexible decision-making. Additionally, although Butlin and colleagues [65] do not 
focus on AGI per se but rather on identifying markers of consciousness in AI based 
on functionalist theories such as GWT, AST, and HOT, their proposal is relevant due 
to its explicit adherence to computational functionalism. They argue that implementing 
certain types of computational functions would be necessary and sufﬁcient to generate 
phenomenal consciousness in artiﬁcial systems. Among these functions, they highlight 
the generation of integrated perceptual representations, global information broadcasting, 
and adaptive ﬂexibility for pursuing multiple goals, which are directly tied to general 
intelligence. Finally, Kurzweil [66] can also be included in this group, even though 
he does not rely on GWT and initially distinguishes between intelligence, functional 
consciousness, and phenomenal consciousness. He concludes that achieving a certain 
level of informational complexity in artiﬁcial systems would be sufﬁcient not only to 
display human-level intelligent behavior but also to justify the attribution of subjective 
consciousness, relying on a functional-behavioral evaluation criterion (i.e., the Turing 
Test). 
Taken together, these works understand phenomenal consciousness in functional 
terms that centrally include—but are not limited to—the GWT framework, while sug-
gesting that such functionally conscious properties may be necessary for the creation 
of AGI. In other words, these authors suggest that consciousness understood func-
tionally could be necessary for the development of AGI, and therefore, phenomenal 
consciousness would also be implicated, insofar as it would be one and the same as

Is Phenomenal Consciousness Necessary for AGI?
67
access/functional consciousness (broadly understood as functionally deﬁned conscious 
properties). 
3.2 
Phenomenological Perspectives 
Let us now turn to the sub-group of phenomenological perspectives. These works also 
suggest that phenomenal consciousness may be necessary for AGI but, in contrast to 
the functionalist perspectives, they emphasize properties that are directly phenomenal 
as being relevant to general intelligence. 
We begin with several authors aligned with the seminal work of Searle [67, 68], 
who highlight the necessity of subjective phenomenal character for genuine semantic 
understanding. Haikonen [69, 70] proposes a model based on associative networks in 
which qualitative subjective properties (i.e., qualia) are understood as "self-explanatory 
percepts", emerging from the simultaneous and organized activation of multiple sensory 
modalities. For Haikonen, the crucial point is that these phenomenal properties are 
necessary for endowing an AI with genuine understanding of meaning, and therefore for a 
form of AGI that exhibits true human-level intelligence. Reggia and collaborators [68], in 
turn, propose that phenomenal consciousness is associated with—though not necessarily 
reducible to—speciﬁc computational functions. These functions enable rapid encoding, 
ﬂexible maintenance, and dynamic updating of information in working memory, which 
they consider essential for fast learning and ﬂexible reasoning, both key features of 
general intelligence. Following Haikonen [70], they further suggest that the phenomenal 
character is required for an AI to truly understand its environment and its own states, 
thus establishing a tight link between meaningful understanding, subjective experience, 
and human-level intelligence. 
This crucial role of phenomenality in endowing representations and computations 
with genuine meaning is also suggested by Bołtuć [71], who argues that subjectivity 
is a necessary requirement for an AGI to inhabit a world with signiﬁcance, rather than 
merely operate on empty symbols. In other words, for an AI to truly acquire human-
level intelligence, it must also be a conscious subject. This semantic dimension of human 
knowledge and learning, and its relationship to general intelligence and consciousness, is 
also brieﬂy addressed by Kralik [72], who leaves open the question of whether phenom-
enality is necessary for AGI, but nonetheless expresses the intuition that consciousness 
likely plays a fundamental role. 
Other approach that can be considered phenomenological is that of Man and Dama-
sio [73-75], who argue that an embodied agent's ability to evaluate its environment 
and internal states in terms of homeostatic viability—through basic feelings such as 
pain, pleasure, well-being, or fatigue—is necessary for creative and autonomous gen-
eral intelligence. This capacity grounds self-concern and provides a perspective from 
which the agent can assign value, prioritize, and ﬂexibly select actions in changing con-
texts. Finally, Chella and Manzotti [17], building on O'Regan and Noë's sensorimotor 
enactivism [76], propose that both phenomenal consciousness and general intelligence 
cannot be reduced to computations on internal representations but emerge from the 
agent's continuous learning of sensorimotor contingencies through interaction with its 
environment.

68
I. Cea
3.3 
Summary of the Association View 
In summary, the reviewed works converge in afﬁrming that phenomenal consciousness 
could indeed be a necessary condition for the creation of AGI, as phenomenal con-
sciousness and general intelligence would not be dissociable, for several complementary 
reasons: I) From a functionalist perspective, replicating speciﬁc functions constitutive 
of phenomenal consciousness—such as global broadcasting of attended information, 
metacognition, or unlimited associative learning—is considered indispensable for repro-
ducing the ﬂexibility and generality of human intelligence. From the phenomenological 
perspectives, II) the subjective character of conscious experience is regarded as necessary 
for enabling genuine semantic understanding, not merely syntactic or "empty" manip-
ulation of representations. III) Conscious affectivity, through a perspective anchored in 
self-concern, value attribution, and continuous homeostatic regulation, would be essen-
tial for autonomously and creatively guiding behavior in novel and changing environ-
ments. IV) Sensorimotor contingency mastery is seen as constitutive of situated intelli-
gence—necessary for learning and interacting with the world in a ﬂexible and embodied 
way. 
Taken together, these perspectives maintain that the creation of true human-level 
general intelligence would require not only advanced unconscious cognitive capacities, 
but also the presence of functional and phenomenal structures that support ﬂexible and 
adaptive behavior endowed with meaningful understanding, autonomous motivation, 
and creative agency. 
4 
Conclusions and Further Work 
We have reviewed the relevant literature that either answers—or implies an answer—neg-
atively or afﬁrmatively to our guiding question: Is phenomenal consciousness necessary 
for the creation of AGI? We ﬁrst examined the works that favor the dissociation between 
consciousness and general intelligence, and thus answer our question negatively, propos-
ing that cognitive functions relevant for general intelligence—such as attention, planning, 
deliberation, or working memory—can be computationally replicated without implying 
or requiring the generation of subjective experience. In contrast, the perspectives that 
answer afﬁrmatively—suggesting the association between phenomenal consciousness 
and general intelligence—were divided into two major strands: the functionalist views, 
which identify phenomenal consciousness with the implementation of speciﬁc critical 
cognitive functions (such as global information broadcasting or unlimited associative 
learning); and the phenomenological views, which highlight the central role of subjective 
experience for genuine semantic understanding and intelligent adaptability. 
Further work should focus on clarifying and evaluating the core assumptions that 
divide the dissociation and association views regarding the role of phenomenal con-
sciousness in AGI. The most pressing theoretical questions include: (i) whether true 
general intelligence can be exhaustively deﬁned in behavioral-functional terms without 
reference to subjective experience; (ii) whether phenomenal consciousness plays a causal 
role in general-purpose intelligent behavior or is merely epiphenomenal; (iii) to what 
extend the concepts of general intelligence and functional consciousness overlap; and 
(iv) to what extend functional and phenomenal consciousness can be conceptually and

Is Phenomenal Consciousness Necessary for AGI?
69
empirically dissociated. A systematic analysis of these issues—integrating philosoph-
ical, computational, engineering and neuroscientiﬁc approaches—is essential to move 
beyond speculation and toward a more theoretically rigorous, empirically informed, and 
potentially consensual answer to the question of whether phenomenal consciousness is 
necessary for the creation of AGI. 
Disclosure of Interests. The author has no competing interests to declare that are relevant to the 
content of this article. 
References 
1. DeepMind: DeepMind - Mission 
2. Mitchell, M.: Debates on the nature of artiﬁcial general intelligence. Science (1979). 383, 
eado7069 (2024) 
3. OpenAI: OpenAI - About 
4. Boden, M.: AI: Its Nature and Future. Oxford University Press (2016) 
5. McCarthy, J.: Generality in artiﬁcial intelligence. Commun ACM. 1030-1035 (1987) 
6. Morris, M.R., et al.: Levels of AGI: operationalizing progress on the path to AGI. arXiv 
preprint arXiv:2311.02462. (2023) 
7. Thórisson, K., Isaev, P., Sheikhlar, A.: Artiﬁcial general intelligence. In: 17th International 
Conference. Springer Nature Switzerland, Cham (2024). https://doi.org/10.1007/978-3-031-
65572-2 
8. Wang, P., Goertzel, B., Franklin, S.: Artiﬁcial general intelligence. In: 2008: Proceedings of 
the First AGI Conference. IOS Press, Amsterdam (2008) 
9. Xu, B.: What is meant by AGI? On the Deﬁnition of Artiﬁcial General Intelligence (2024) 
10. Shanahan, M.: The Technological Singularity. MIT Press (2015) 
11. Block, N.: On a confusion about a function of consciousness. In: Consciousness, Function, and 
Representation. Collected Papers, vol. 1, pp. 159-214. MIT Press, Cambridge, MA (2007). 
https://doi.org/10.7551/mitpress/2111.003.0012 
12. Chalmers, D.J.: Reality+: Virtual worlds and the problems of philosophy. Penguin UK (2022) 
13. Nagel, T.: What is it like to be a bat? Philos. Rev. 83, 435-450 (1974) 
14. Perry, M.W.: Why sentience should be the only basis of moral status. J. Ethics (2024). https:// 
doi.org/10.1007/s10892-024-09487-4 
15. Wallach, W., Allen, C., Franklin, S.: Consciousness and ethics: artiﬁcially conscious moral 
agents. In: Machine Ethics and Robot Ethics, pp. 299-314. Routledge (2020) 
16. Li, O.: Should we develop AGI? Artiﬁcial suffering and the moral development of humans. 
AI Ethics. 5, 641-651 (2025). https://doi.org/10.1007/s43681-023-00411-4 
17. Chella, A., Manzotti, R.: AGI and machine consciousness. In: Theoretical Foundations of 
Artiﬁcial General Intelligence, pp. 263-282. Springer (2012) 
18. Prettyman, A.: Artiﬁcial consciousness. Inquiry, pp. 1-18 (2024) 
19. Schneider, S.: Artiﬁcial you: AI and the future of your mind. Princeton University Press 
(2019) 
20. Wiese, W.: Artiﬁcial consciousness: a perspective from the free energy principle. Philos. Stud. 
181, 1947-1970 (2024) 
21. Danaher, J.: Automation and utopia: Human Flourishing in a World without Work. Harvard 
University Press (2019) 
22. Dubber, M.D., Pasquale, F., Das, S.: The oxford handbook of ethics of AI. Oxford Handbooks 
(2020)

70
I. Cea
23. Floridi, L.: Establishing the rules for building trustworthy AI. Nat Mach Intell. 1, 261-262 
(2019). https://doi.org/10.1038/s42256-019-0055-y 
24. Kissinger, H.A., Schmidt, E., Huttenlocher, D.: The Age of AI: and our Human Future. 
Hachette UK (2021) 
25. Obrenovic, B., Gu, X., Wang, G., Godinic, D., Jakhongirov, I.: Generative AI and human- 
robot interaction: implications and future agenda for business, society and ethics. AI Soc. 40, 
677-690 (2025). https://doi.org/10.1007/s00146-024-01889-0 
26. Schwab, K.: The fourth Industrial Revolution. Currency (2017) 
27. Harari, Y.N.: Nexus: A Brief History of Information Networks from the Stone Age to AI. 
Random House, New York (2024) 
28. Kuhn, R.L.: A landscape of consciousness: toward a taxonomy of explanations and 
implications. Prog. Biophys. Mol. Biol. 190, 28-169 (2024) 
29. Signorelli, C.M., Szczotka, J., Prentner, R.: Explanatory proﬁles of models of consciousness-
towards a systematic classiﬁcation. Neurosci Conscious. 2021, niab021 (2021) 
30. Seth, A.K., Bayne, T.: Theories of consciousness. Nat. Rev. Neurosci. 23, 439-452 (2022) 
31. Seth, A.: Being You: A new Science of Consciousness. Penguin (2021) 
32. Seth, A.: Conscious artiﬁcial intelligence and biological naturalism. Behav. Brain Sci. 
(forthcoming). (2025). https://doi.org/10.31234/osf.io/tz6an 
33. Legg, S., Hutter, M.: A collection of deﬁnitions of intelligence. Front. Artif. Intell. Appl. 
(2007) 
34. Gamez, D.: Intelligence and consciousness in natural and artiﬁcial systems. In: Computational 
Approaches to Conscious Artiﬁcial Intelligence, pp. 169-202. World Scientiﬁc (2023) 
35. Gamez, D.: The relationships between intelligence and consciousness in natural and artiﬁcial 
systems. J. Artif. Intell. Consciousness. 7, 51-62 (2020) 
36. Bostrom, N.: Superintelligence: Paths, Dangers, Strategies. OUP, Oxford (2014) 
37. Harari, Y.N.: Sapiens: A Brief History of Humankind. Random House (2014) 
38. Harari, Y.N.: Homo Deus: A brief history of tomorrow. Random House (2016) 
39. Albantakis, L., et al.: Integrated information theory (IIT) 4.0: Formulating the properties of 
phenomenal existence in physical terms. PLoS Comput Biol. 19, e1011465- (2023) 
40. Findlay, G., et al.: Dissociating artiﬁcial intelligence from artiﬁcial consciousness. arXiv 
preprint arXiv:2412.04571. (2024) 
41. Oizumi, M., Albantakis, L., Tononi, G.: From the phenomenology to the mechanisms of 
consciousness: integrated information theory 3.0. PLoS Comput Biol. (2014). https://doi.org/ 
10.1371/journal.pcbi.1003588 
42. Summerﬁeld, C.: Natural General Intelligence: How Understanding the Brain can help us 
Build AI. Oxford University Press (2023) 
43. Montemayor, C.: The Prospect of a Humanitarian artiﬁcial Intelligence: Agency and Value 
Alignment. Bloomsbury Academic (2023) 
44. Montemayor, C.: Attention, consciousness, and linguistic cooperation with AI. J. Artif. Intell. 
Consciousness. 08, 267-283 (2021). https://doi.org/10.1142/S270507852150017X 
45. Chalmers, D.J.: The singularity. In: Science Fiction and Philosophy pp. 171-224. Wiley 
(2016). https://doi.org/10.1002/9781118922590.ch16 
46. Baars, B.J.: A cognitive Theory of Consciousness. Cambridge University Press, Cambridge 
(1988) 
47. Baars, B.J.: Global workspace theory of consciousness: toward a cognitive neuroscience of 
human experience. Prog. Brain Res. 150, 45-53 (2005) 
48. Baars, B.J., Geld, N., Kozma, R.: Global workspace theory (GWT) and prefrontal cortex: 
recent developments. Front. Psychol. 12, 749868 (2021) 
49. Block, N.: Consciousness, accessibility, and the mesh between psychology and neuroscience. 
Behav. Brain Sci. 30, 481 (2007)

Is Phenomenal Consciousness Necessary for AGI?
71
50. Naccache, L.: Why and how access consciousness can account for phenomenal consciousness 
(2018). https://doi.org/10.1098/rstb.2017.0357 
51. Baars, B.J., Franklin, S.: Consciousness is computational: the LIDA model of global 
workspace theory. Int. J. Mach. Consciousness 1, 23-32 (2009) 
52. Faghihi, U., Franklin, S.: The LIDA model as a foundational architecture for AGI. In: 
Theoretical Foundations of Artiﬁcial General Intelligence, pp. 103-121. Springer (2012) 
53. Kugele, S., Franklin, S.: Learning in LIDA. Cogn Syst Res. 66, 176-200 (2021). https://doi. 
org/10.1016/j.cogsys.2020.11.001 
54. Blum, L., Blum, M.: AI consciousness is inevitable: a theoretical computer science 
perspective. arXiv preprint arXiv:2403.17101. (2024) 
55. Blum, L., Blum, M.: A theoretical computer science perspective on consciousness and 
artiﬁcial general intelligence. Engineering 25, 12-16 (2023) 
56. Ginsburg, S., Jablonka, E.: The Evolution of the Sensitive Soul: Learning and the Origins of 
Consciousness. MIT Press (2019) 
57. Bronfman, Z., Ginsburg, S., Jablonka, E.: When will robots be sentient? J. Artif. Intell. 
Consciousness 8, 183-203 (2021) 
58. Juliani, A., Arulkumaran, K., Sasai, S., Kanai, R.: On the link between conscious function 
and general intelligence in humans and machines. arXiv preprint arXiv:2204.05133. (2022) 
59. Kanai, R., Chang, A., Yu, Y., Magrans de Abril, I., Biehl, M., Guttenberg, N.: Information 
generation as a functional basis of consciousness. Neurosci Conscious. 2019 (2019). https:// 
doi.org/10.1093/nc/niz016 
60. Graziano, M.S.A.: Illusionism big and small: some options for explaining consciousness. 
eNeuro. 11, (2024) 
61. Graziano, M.S.A.: The attention schema theory: a foundation for engineering artiﬁcial 
consciousness. Front Robot AI. 4, 60 (2017) 
62. Bengio, Y.: The consciousness prior. arXiv preprint arXiv:1709.08568. (2017) 
63. Kahneman, D.: Thinking, Fast and Slow. Farrar, Straus and Giroux (2013) 
64. Dehaene, S., Lau, H., Kouider, S.: What is consciousness, and could machines have it? Science 
1979(358), 486-492 (2017) 
65. Butlin, P., et al.: Consciousness in artiﬁcial intelligence: insights from the science of 
consciousness. arXiv preprint arXiv:2308.08708. (2023) 
66. Kurzweil, R.: The Singularity is Nearer: When we Merge with AI. Penguin (2024) 
67. Searle, J.: Minds, brains, and programs. Behav. Brain Sci. 3, 417-457 (1980) 
68. Searle, J.: Is the brain's mind a computer program? Sci. Am. 262, 25-31 (1990) 
69. Haikonen, P.: Consciousness and Robot Sentience, 2nd edition. World Scientiﬁc (2019) 
70. Haikonen, P.: On Artiﬁcial intelligence, consciousness and robots. In: Computational 
Approaches to Conscious Artiﬁcial Intelligence, pp. 99-125. World Scientiﬁc (2023) 
71. Bołtuć, P.: Consciousness for AGI. Procedia Comput. Sci. 169, 365-372 (2020) 
72. Kralik, J.D.: Toward a comprehensive list of necessary abilities for human intelligence, Part 2: 
using knowledge. In: International Conference on Artiﬁcial General Intelligence, pp. 271-281. 
Springer (2022) 
73. Man, K., Damasio, A., Neven, H.: Need is all you need: homeostatic neural networks adapt 
to concept shift. arXiv preprint arXiv:2205.08645. (2022) 
74. Man, K., Damasio, A.: Homeostasis and soft robotics in the design of feeling machines. Nat. 
Mach. Intell. 1, 446-452 (2019) 
75. Man, K., Damasio, A.: Truth or consequences: Homeostatic self-regulation in artiﬁcial neural 
networks. In: Artiﬁcial Life Conference Proceedings, vol. 32, pp. 146-147. MIT Press One 
Rogers Street, Cambridge, MA 02142-1209, USA journals-info (2020) 
76. O'regan, J.K., Noë, A.: A sensorimotor account of vision and visual consciousness. Behav. 
Brain Sci. 24, 939-973 (2001)

Holographic Memory and Cortical 
Microcircuits: A Step Towards AGI 
Oscar Chang1(B) 
, Jonathan Pérez2 
, and Amy Meneses2 
1 Yachay Tech University, Urcuqui, Ecuador 
ogchang@gmail.com 
2 Simon Bolivar University, Caracas, Venezuela 
Abstract. Holographic memory, required for complex artiﬁcial general 
intelligence (AGI) data handling, enables advanced perception of mul-
tidimensional information. This paper proposes a brain-inspired 'holo-
graphic' memory system that reliably processes raw pixels inputs. By 
mimicking neural microcircuits, it achieves energy eﬃciency and employs 
a self-regulating process for low-power operation. Trained with cutting-
edge learning methods, it shows a capacity for strategic planning and pre-
diction. This allows a robot with an incorporated 'holographic mind' to 
process live visual input, perceive dimensional information and respond 
with human-like look ahead capacity in dynamic game situations. 
Keywords: Holographic memory · Cortical microcircuits · 
Reinforcement learning 
1
Introduction 
Inspired by the brain, holographic memory is a leading contender for future AI, 
promising eﬃcient storage, retrieval, and high-dimensional data representation 
[ 23, 26]. This oﬀers a signiﬁcant shift in how AI processes information [ 13]. 
Holographic circuits are computational systems where individual units hold 
"complete solutions knowledge", enabling them to contribute to awake new 
dimensions in perception for other participant external agents. 
This kind of knowledge facilitates a "dimensional gain" providing richer, 
higher-order insights for external agents, such as a binocular vision creature 
perceiving 3D from a 2D holographic image. This expansion of information is 
vital for the advanced processing and robust knowledge representation required 
for AGI [ 1, 23]. 
The concept of holographic reconstruction is applicable across various ﬁelds. 
In AI-driven medical imaging, it can enhance diagnostic precision by allowing 
for 3D visualization of structures [ 3, 10]. For autonomous vehicles, holographic 
perception may improve environmental awareness [ 4]. Crucially, for Artiﬁcial 
General Intelligence (AGI), holographic memory oﬀers a pathway to more sophis-
ticated cognitive abilities [ 9, 14]. Its ability to represent and manipulate multi-
dimensional data is essential for spatial reasoning, complex pattern recognition, 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 72-83, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_8

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
73
and intuitive problem-solving [ 16, 22]. By connecting data representation with 
cognitive processing, holographic memory can empower AGI systems to com-
prehend and interact with the world more comprehensively [ 8]. 
2
Related Works 
2.1
Holographic Memory, Microcircuits and AGI 
Inspired by biological and physical principles, holographic memory mirrors the 
brain's distributed memory in cortical microcircuits, which enables robust recall 
[ 11, 18, 19, 25]. Physically, optical holography demonstrates 3D image reconstruc-
tion from 2D patterns, inﬂuencing research into high-density data storage, 
despite material challenges [ 7, 23]. 
The need for eﬃcient memory in AGI drives exploration beyond tradi-
tional architectures. While associative memory networks like Hopﬁeld networks 
exist, they face limitations [ 27, 28]. Neural holographic memories combine opti-
cal holography with neural networks for encoding and decoding complex data 
[ 12, 21, 24]. 
Recent advancements in deep learning have renewed interest in holographic 
memory. Researchers are exploring its use to improve the eﬃciency and gener-
alization of CNNs, reducing their reliance on extensive data [ 17, 26, 29]. 
Holography's ability to represent multidimensional data is crucial for AI in 
spatial reasoning and complex pattern recognition. Integrating it with concepts 
from tensor network theory and geometric deep learning [ 2, 15, 20] could sig-
niﬁcantly enhance AI's perception and interaction with its environment. Fur-
thermore, modern artiﬁcial neural network designs based on microcircuits show 
strong potential for AGI [ 31, 32]. 
2.2
Look Ahead Capacity 
Tic-tac-toe, a straightforward positional game, is typically mastered by children 
by the age of ﬁve, after which its predictability often leads to a loss of interest. 
Similar to chess, the outcome of tic-tac-toe is signiﬁcantly inﬂuenced by the 
initial moves, known as aperture. Apertures are crucial in game theory and are 
well-documented in games such as chess, Go, and Mario Bros. While human 
chess champions can foresee about 12 moves ahead in apertures, modern deep 
reinforcement learning algorithms can predict up to 25 moves. 
Despite its apparent simplicity, tic-tac-toe reveals intricate strategic scenar-
ios where predictive depth or look ahead capacity is paramount for achieving 
victory. These critical apertures, often overlooked by casual observers, demand 
a four-move look-ahead strategy, a cognitive challenge mastered by advanced 
algorithms and clever children. The identiﬁcation and exploitation of these ini-
tial strategic patterns in tic tac toe underscore the necessity for robust predic-
tive modeling within intelligent systems. One such example is the Block and 
Win Strategy, which involves creating two potential winning lines simultane-
ously. This forces the opponent to block one, allowing a win in two moves. Look

74
O. Chang et al.
ahead capacity in apertures is associated with high intelligence and is essential 
to rapidly maximize the cumulative reward predicted by the Bellman equation 
[ 5, 30]. This paper proposes a novel design methodology that replicates the robust 
predictive capabilities of clever kids and algorithms, by utilizing processing ele-
ments designed to emulate theorized holographic memory functions in biological 
cortical microcircuits. 
2.3
Continual and Deep Reinforcement Learning: Cutting-Edge 
Learning Methods 
Neural networks are susceptible to catastrophic forgetting, a phenomenon where 
new learning overwrites previously acquired knowledge. Continual learning 
addresses this challenge by enabling systems to incrementally build upon and 
retain knowledge over time, mirroring human learning. In this paper, we propose 
a continual method where agents encode learned policies into the solidify weights 
of specialized Artiﬁcial Neural Network (ANN) microstructures. Upon comple-
tion of an agent's exploration and learning phase, the weights of its assigned 
microcircuit are solidiﬁed, rendering them immutable to subsequent learning 
processes by other agents on and within diﬀerent microcircuits. We show that 
encoding policy knowledge in these solidiﬁed microstructures facilitates knowl-
edge integration, suggesting an eﬃcient holographic memory strategy. 
Reinforcement learning trains agents to maximize rewards through trial-
and-error in an environment. For complex tasks, deep reinforcement learning 
uses deep neural networks to handle vast state and action spaces, like raw 
pixel images. This allows agents to learn from high-dimensional inputs, enabling 
them to excel in intricate scenarios like complex video games, where traditional 
methods fall short. This paper uses a deep reinforcement learning environment 
where memory system integrates knowledge from diverse sources and experiences 
through a sequential learning process that involves many agents and associate 
artiﬁcial microcircuits. 
2.4
The Theoretical Role of Inhibitory Systems in Holographic 
Memory 
Neural computation relies on the complex, critical balance of inhibitory (GABA) 
and excitatory (glutamate) neurotransmission, where GABA inhibits action 
potentials and glutamate promotes them. By simplifying this biological complex-
ity, our models demonstrate a basic holographic memory. Microcircuits serve as 
fundamental computational units, exhibiting dimensional gain, high resilience, 
and low energy consumption. This approach could be the scaﬀold for building 
more sophisticated "integrated systems". 
Our microcircuits, build with sigmoidal neurons. share a regenerative input 
channel, analogous to axonal networks, enabling parallel input processing. Each 
microcircuit incorporates a GABAergic inhibitory input, maintaining neuronal 
potentials below threshold. To achieve selective activation, a glutamate-based 
(Glu) excitatory mechanism counteracts GABA's eﬀect through a self-regulating

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
75
glutamate release system, that employes competitive dynamics among neurons 
to ensure a 'one-winner-only' output (see Algorithm 1 and Algorithm 2). This 
system minimizes energy consumption and resolves channel access conﬂicts by 
selectively releasing glutamate channels, thereby modulating inhibitory control 
and facilitating targeted neural discharge. This precise control of excitation and 
inhibition is crucial for synaptic plasticity, eﬃcient neural activation, and eﬀec-
tive information processing within the holographic memory framework. 
Algorithm 1. Hidden layer calculation with inhibitor GABA and excitatory 
Glu 
for k = 0  to k <  N_{HID}NHID do 
k++; 
hidden_Layer.out[k] \gets←0 
for i = 0  to i<N_{IN}NIN do 
hidden_Layer.out[k] \gets←hidden_Layer.out[k] + hidden_Layer.weights[k][i] 
\times× Input[i] + gaba_inhibit \times× g + gluta_release \times× g 
end for 
hidden_Layer.out[k] \gets←sigmoid(hidden_Layer.out[k]) 
end for 
Algorithm 2. Output layer calculation with inhibitor GABA and excitatory 
Glu 
for k = 0  to k <  N_{out}Nout do 
k++; 
output_Layer.out[k] \gets←0 
for i = 0  to i<N_{HID}NHID do 
output_Layer.out[k] \gets←output_Layer.out[k] + output_Layer.weights[k][i] \times×
hiddenIn[i] + gaba_inhibit \times× g + gluta_release \times× g 
end for 
output_Layer.out[k] \gets←sigmoid(output_Layer.out[k]) 
end for 
Where 'g' represents neuronal sensitivity to glutamate (Glu), a non-synaptic parameter 
dependent on the number of aﬀerent inputs. 'g' positively correlates with input number, 
meaning more inputs require higher Glu sensitivity. Dynamic analysis could potentially 
automate the determination of 'g' values. 
3
Implementation 
In Fig. 1, we present our model's primary vision system. A webcam captures raw 
420\,\times\, × 240 RGB images of a 3\,\times\, × 3 board and, using OpenCV for preprocessing, 
feeds this data to a Deep Convolutional Network. This network outputs sparse, 
binary information representing all possible board states.

76
O. Chang et al.
Fig. 1. 
Primary visual system: A CNN receives raw pixels information about a 
physical board from a webcam. Through convolution and Max pool this net produces 
a sparse encoded 27 on-oﬀ neurons that codify every possible board state and serve as 
an input channel to many possible down stream microcircuits. 
3.1
Basic Three-Layer Microcircuits 
Weight-solidiﬁable three-layer neural networks are used to learn and store move 
decision policies (Fig. 2). Solidiﬁable weights make possible continual learn-
ing and knowledge sharing between agents. They use backpropagation training 
method and are tuned to obtain low energy, sparse data codiﬁcation. In this con-
text, these microcircuits act as fundamental canonical computational units that 
eventually achieve dimensional gain in information processing and form cluster 
that have high resilience and low energy consumption. This is akin to a small, 
early-stage neuronal clump in a simple organism, poised to eventually form a 
sophisticated "integrated system". 
3.2
Training: Enhancing Decision Making and Look-Ahead 
Capacity 
To accomplish policy learning across distinct game phases Continual and Deep 
Reinforcement Learning (DRL) are employed. To this end the GABA and Glu 
variables are temporally set to zero and a series of learning policies processes is 
carried out. Three agents, each utilizing a dedicated microcircuit, acquire and 
store the necessary policies for optimal decision-making during the endgame, 
middlegame, and aperture phases, respectively. 
The mathematical framework linking reinforcement learning and the Bellman 
Eq. (1), for an agent to learn a policy is given by 
\bALT{} \label{eq:bellmanGeneral} Q^*(s,a)= \text{max}E\big[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+ \ldots | s_t=s, a_t=a, \pi]\eALT{} Q∗(s, a) = maxE

rt + γrt+1 + γ2rt+2 + . . . |st = s, at = a, π]
(1) 
Where "ss" represents the state, "aa" represents action and "rr" the reward, "\piπ" 
the policy, "\gammaγ" the discount factor and "tt" is the time index. The QQ values can 
be approximated at the k^{th}kth iteration as follows: 
\bALT{} \label{eq:QspsValuespsaprox} Y_k^Q = r + \gamma\underset{a'}{\text{max}}Q(s',a';\theta_k)\eALT{} Y Q
k = r + γmax
a′ Q(s′, a′; θk)
(2) 
Then the process in which the parameters\theta_kθk are updated through a stochastic 
gradient descent is as follows:

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
77
Fig. 2. Microcircuit-GLT-GABA: Simpliﬁed model of a microcircuit in the cortical 
layer. These microstructures, primarily excitatory neurons, receive sensory input from 
trained convolutional nets, which in turn receive raw pixels as input. The 27 input 
neurons are representative from the primary visual system and do not belong to the 
microcircuit. The 19 hidden neurons acts as long term memory used for feature detec-
tion. The 9 outputs are connected in parallel to a common discharge channel, where 
other microcircuits participate 
\bALT{} \label{Bellmanequation} \theta_{k+1}=\theta_k+\alpha\big(Y^Q_K-Q(s,a,\theta_k)\big)\nabla_{\theta_k}Q(s,a,\theta_k)\eALT{} θk+1 = θk + α

Y Q
K −Q(s, a, θk)

∇θkQ(s, a, θk)
(3) 
where \alphaα is the learning rate and the square loss ensures that Q(s,a,\theta_k)Q(s, a, θk) tends to 
the expected value of the random variable Y^Q_KY Q
K without bias. 
The values of g for the hidden-output weights g^{(t)}_{jh}g(t)
jh and the input-hidden 
weights g^{(t)}_{hi}g(t)
hi become respectively: 
\bALT{}\label{eq:hiddenspsoutputs} g^{(t)}_{jh} = f'(v^{(t)}_j)y^{(t)}_h+\lambda g^{(t-1)}_{jh}\eALT{} g(t)
jh = f ′(v(t)
j )y(t)
h + λg(t−1)
jh
(4) 
\bALT{}\label{eq:inputspshiddens} g^{(t)}_{hi} = f'(v^{(t)}_j)W^{(t)}_{jh}f'(v^{(t)}_h)x^{(t)}_{i} +\lambda g^{(t-1)}_{hi}\eALT{} g(t)
hi = f ′(v(t)
j )W (t)
jh f ′(v(t)
h )x(t)
i
+ λg(t−1)
hi
(5) 
Equations (4) and  (5), respectively, yields the following weight update equa-
tions: 
\bALT{}\label{eq:hiddenspsoutputweights} \Delta W^{(t)}_{jh}=\eta \Delta W^{(t-1)}_{jh}+\alpha \bigg(r^{(t+1)}+\gamma V^(t+1)-V^{(t)}\bigg)g^{(t)}_{jh}\eALT{} ΔW (t)
jh = ηΔW (t−1)
jh
+ α

r(t+1) + γV (t + 1) −V (t)

g(t)
jh
(6) 
\bALT{}\label{eq:inputspshiddenweights} \Delta W^{(t)}_{hi}=\eta \Delta W^{(t-1)}_{hi}+\alpha \bigg(r^{(t+1)}+\gamma V^(t+1)-V^{(t)}\bigg)g^{(t)}_{hi}\eALT{} ΔW (t)
hi = ηΔW (t−1)
hi
+ α

r(t+1) + γV (t + 1) −V (t)

g(t)
hi
(7)

78
O. Chang et al.
By using this parametric solution a series of agent, using a sequence of con-
tinual deep reinforcement learning events, and diﬀerent rewards systems creates 
specialized microcircuits that solve diﬀerent game situations (see Algorithm 3). 
Training parameters were optimized to prevent overﬁtting and produce low-
energy, well-trained networks. Computer-generated board images were utilized 
for training. 
Algorithm 3. The TD(\lambda)(λ) algorithm using an iterative weight updating scheme. 
Initialize neural network (initialize all weights WW) 
for N episodes do 
Initialize s; 
repeat for each step of episode:  
a \getsa ←action given by policy \piπ for s'; 
Take action a, onserve reward r and next state s'; 
E \gets r+\gamma V(s')-V(s)E ←r + γV (s′) −V (s); 
W \gets W + \Delta WW ←W + ΔW where \Delta W = f(E)ΔW = f(E); 
s \gets s's ←s′
until s is terminal state; 
end for 
3.3
Mutually Supporting Learning 
We created three processing units, or clusters, each comprising an agent and a 
microcircuit, to specialize in diﬀerent game stages: aperture, middle game, and 
end game. This stage-based predictability is a hallmark of advanced cognitive 
development. 
1. Agent A1 (microcircuit m1) learns the "win with the next last move" policy, 
rewarded by an intrinsic "game won" signal. A1, unaware of apertures or 
middlegame and under strict energy constraints, learns to remain inactive 
during these phases. Its "win endgame" knowledge is solidiﬁed in m1's weights, 
enabling a one-move-ahead win. 
2. Agent A2 (microcircuit m2) explores and learns future moves, receiving 
rewards from A1's knowledge (m1's outputs). This "activate double rail" 
knowledge, solidiﬁed in m2's weights, allows for a two-move-ahead win. 
3. Agent A3 (microcircuit m3) learns aperture patterns by leveraging A1 and 
A2's knowledge [ 5, 6] or expert-submitted patterns. A3's "good aperture" 
knowledge, solidiﬁed in m3's weights, enables a four-move-ahead win, con-
tingent on other microcircuits performing their functions. 
The agents A1, A2, and A3 use the Bellman equation for exploratory learning, 
as detailed in [ 6]. Our trained microcircuits achieve a 97% recognition precision 
for pattern recognition tasks. Furthermore, enforcing "keep low energy" dur-
ing training resulted in low-energy, sparse-coding individuals. Overﬁtting was 
addressed by setting training parameters to produce low-energy, well-trained 
networks.

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
79
3.4
Densely Informed Environment 
To achieve knowledge condensation, a functional cluster is deﬁned as a group of 
specialized microcircuits with diﬀerent specializations. These microcircuits share 
common input and output pathways, as well as a common inhibitory GABAergic 
input. This cluster operates as a uniﬁed entity, possessing comprehensive knowl-
edge across distinct problem domains (e.g., apertures, middlegame, endgame). 
When GABA is set to a constant, suﬃciently negative value, the internal poten-
tial hyperpolarizes, eﬀectively silencing all neurons in the cluster, no matter if 
the inputs neurons keep on sending incoming information. A GABA polariza-
tion makes the cluster to fall in sleeping state where its energy consumption 
reaches minimal, near to zero value. It also causes the incoming input signals to 
be barely considered, quasi isolating the microcircuit but minimizing its energy 
consumption. With convenient GABA polarization many clusters can be bound 
to an input channel and create the shell for a reliable holographic memory. 
3.5
Self-regulating Glutamate Release System 
Managing numerous knowledge clusters on a shared input channel risks uncon-
trolled energy expenditure. Without regulation, simultaneous activation could 
cause chaos, especially with the sensitive GABA/Glu balance. To prevent this 
and ensure stability, we propose a novel neural arbitration mechanism inspired 
by mutually inhibitory circuits [ 6, 27] (Fig. 3). This mechanism uses a common 
positive ramp to excite a ﬁercely competitive neuronal network, forming an 
Autonomous, Unbiased Arbiter that enforces a 'winner-take-all' dynamic where 
each neuron suppresses others. This competition enables robust, energy-eﬃcient 
selection of a single relevant knowledge cluster, preventing energy surges and 
maintaining coherence. A network of non-linear elements collectively solves local 
logical constraints. 
Fig. 3. Self-regulated Network for Glutamate Release: This system utilizes a 5-
neuron competitive network with strong, uniform inhibitory connections (-2.0) between 
all non-self neurons, establishing a low-energy baseline. This inhibition, exceeding typ-
ical trained weights (-0.5 to +0.5), minimizes baseline activity. An excitatory ramp (K) 
triggers a winner-take-all competition, leading to a single dominant neuron. Computa-
tional modeling shows this strong inhibitory competition results in sustained low-level 
activity, followed by the spontaneous emergence of one dominant neuron. This low-
energy mechanism ensures a unique output by eﬃciently selecting a single active cluster 
for decision-related discharge. Proper function required parameter optimization.

80
O. Chang et al.
Fig. 4. The holographic memory system comprises a webcam that feeds a pre-
processing Conv net. This net sustain a sparse coded channel that feeds comprehensive 
knowledge clusters, each possessing wide-ranging information. For data processing, a 
ramp signal is applied to the glutamate releaser. This signal facilitates a winner-take-
all mechanism, resulting in a single cluster receiving a glutamate pulse. This selective 
activation ensures a unique output with minimal energy expenditure. For visual clarity, 
only ﬁve clusters out of ﬁfteen are displayed 
3.6
System Testing and Validation 
The integrated holographic memory system (Fig. 4) undergoes testing and vali-
dation using real-world images and a three-axis robot with 15 working clusters. 
This testing conﬁrms the system's performance in several key areas, includ-
ing data storage, retrieval, resilience, predictive capabilities, and processing eﬃ-
ciency. When the robot sees real world images, its holographic memory imme-
diately perceives the "look ahead" dimension and plays fast and wisely. Other 
dimensions may be seen by other agents. 
4
Results 
The proposed clustered memory system establishes temporal logical connections, 
generating an optimization gradient for collaborative decision-making across self-
trained microcircuits. This helps identify a guaranteed winning path as the game 
progresses. Real-time testing conﬁrms the robot's holographic, human-like per-
ception of a look-ahead dimension (Fig. 5). Once this holographic memory is 
established, other external agents might perceive diﬀerent dimensional informa-
tion. 
As in the case of image storage and 2D to 3D holographic perception, explo-
ration of interference patterns in the synaptic weights of clustered microcir-
cuits may reveal a pathway to better knowledge storage, retrieval, and increased 
representational dimensionality. Mathematical derivations could yield scalable 
robotics solutions and drive advancements in deep reinforcement learning for 
dimensional perception in AGI deployment.

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
81
Fig. 5. Holographic Memory: Perception of Hidden Winning Dimension. By ana-
lyzing the game aperture image, the holographic memory system perceives a hidden 
dimension correlating to a winning path four moves ahead. Perception in this context 
refers to the process of observing an image and making decisions that involve both 
logical reasoning and physical actions. 
5
Conclusions 
This research demonstrates a brain-inspired holographic memory system for 
robots, achieving real-time, energy-eﬃcient multi-dimensional perception from 
raw pixels. 
Our system trains specialized neural microcircuits using continual and deep 
reinforcement learning, storing policies as ﬁxed weights within multi-knowledge 
clusters. GABAergic modulation enables parallel processing, while a novel 
glutamate-driven 'winner-take-all' arbitration minimizes energy during retrieval 
by selectively activating relevant clusters. This results in low-power, holographic 
responses to complex visual inputs. 
This work, although currently small-scale, establishes a clear foundation for 
scalability. It introduces a hierarchical model where independent, low-energy net-
works collaborate to resolve increasingly complex logical constraints, fostering a 
highly self-regulating system. This approach aims to pave the way for a scalable, 
energy-eﬃcient architecture for high-dimensional sensory data, a critical aspect 
for autonomous agents. We believe that integrating continual, deep reinforce-
ment learning, microcircuits and holographic data processing could signiﬁcantly 
advance robust, real-time multi-dimensional perception in AGI. 
Acknowledgments. The authors wish to thank Yachay Tech University and Simon 
Bolivar University. 
Disclosure of Interests. The authors have no competing interests to declare that 
are relevant to the content of this article 
References 
1. Baireddy, V.R., Kulkarni, D.S., Kumar, N., Gosavi, V., Khatri, R.: An explo-
ration of artiﬁcial intelligence (ai) integration with real-time three 3-dimensional 
holography to enhance visual quality, optimize processing eﬃciency, and facilitate 
interactive applications. Optimize Processing Eﬃciency, and Facilitate Interactive 
Applications, December 09 (2024)

82
O. Chang et al.
2. Bronstein, M.M., Bruna, J., LeCun, Y., Szlam, A., Vandergheynst, P.: Geometric 
deep learning: going beyond euclidean data. IEEE Signal Process. Mag. 34(4), 
18-42 (2017) 
3. Bucioli, A.A., et al.: Holographic real time 3d heart visualization from coronary 
tomography for multi-place medical diagnostics. In: 2017 IEEE 15th Intl Conf on 
Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intel-
ligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing 
and Cyber Science and Technology Congress (DASC/PiCom/DataCom/Cyber-
SciTech), pp. 239-244. IEEE (2017) 
4. Cao, C.: Holoscopic 3D perception for autonomous vehicles. Ph.D. thesis, Brunel 
University London (2021) 
5. Chang, O., Morocho-Cayamcela, M.E., Pineda, I., Cárdenas, K.: An eﬃcient 
deep  q q-learning strategy for sequential decision-making in game-playing. In: 2022 
Third International Conference on Information Systems and Software Technologies 
(ICI2ST), pp. 172-177. IEEE (2022) 
6. Chang, O., Ramos, L., Morocho-Cayamcela, M.E., Armas, R., Zhinin-Vera, L.: 
Continual learning, deep reinforcement learning, and microcircuits: a novel method 
for clever game playing. Multimedia Tools and Applications, pp. 1-23 (2024) 
7. Cheriere, N., et al.: Holographic storage for the cloud: advances and challenges. 
ACM Trans. Storage 21(1), 1-31 (2025) 
8. DuBois, G.M., Phillips, J.L.: Working memory concept encoding using holographic 
reduced representations. In: Maics, pp. 137-144. Fort Wayne, IN (2017) 
9. Goertzel, B.: Patterns, hypergraphs and embodied general intelligence. In: The 
2006 IEEE International Joint Conference on Neural Network Proceedings, pp. 
451-458. IEEE (2006) 
10. Haleem, A., Javaid, M., Khan, I.H.: Holography applications toward medical ﬁeld: 
An overview. Indian J. radiol. imaging 30(03), 354-361 (2020) 
11. Hunt, L.T.: Frontal circuit specialisations for decision making. Eur. J. Neurosci. 
53(11), 3654-3671 (2021) 
12. Javidi, B., Horner, J.L.: Real-time optical information processing. Academic Press 
(2012) 
13. Kelly, M.A., Arora, N., West, R.L., Reitter, D.: Holographic declarative memory: 
distributional semantics as the architecture of memory. Cogn. Sci. 44(11), e12904 
(2020) 
14. Kvasnicka, V.: Holographic reduced representation in artiﬁcial intelligence and cog-
nitive science. Neural Netw. World 14(part 6), 521-532 (2004) 
15. Lee, J.H., Choe, Y., Ardid, S., Abbasi-Asl, R., McCarthy, M., Hu, B.: Functional 
microcircuits in the brain and in artiﬁcial intelligent systems (2023) 
16. Makhataeva, Z., Akhmetov, T., Varol, H.A.: Augmented-reality-based human 
memory enhancement using artiﬁcial intelligence. IEEE Trans. Human-Mach. Syst. 
53(6), 1048-1060 (2023) 
17. Nguyen, T.A., Lee, J.: A nonlinear convolutional neural network-based equalizer 
for holographic data storage systems. Appl. Sci. 13(24), 13029 (2023) 
18. Opris, I., Casanova, M.F.: Prefrontal cortical minicolumn: from executive control 
to disrupted cognitive processing. Brain 137(7), 1863-1875 (2014) 
19. Opris, I., Casanova, M.F., Lebedev, M.A., Popescu, A.I.: Prefrontal cortical micro-
circuits support the emergence of mind. The Physics of the Mind and Brain Dis-
orders: Integrated Neural Circuits Supporting the Emergence of Mind, pp. 69-94 
(2017) 
20. Orús, R.: Tensor networks for complex quantum systems. Nat. Rev. Phys. 1(9), 
538-550 (2019)

Holographic Memory and Cortical Microcircuits: A Step Towards AGI
83
21. Owechko, Y.: Nonlinear holographic associative memories. IEEE J. Quantum Elec-
tron. 25(3), 619-634 (2002) 
22. Papakostas, C., Troussas, C., Sgouropoulou, C.: Special topics in artiﬁcial intelli-
gence and augmented reality: the case of spatial intelligence enhancement. Springer 
(2024) 
23. Psaltis, D., Mok, F.: Holographic memories. Sci. Am. 273(5), 70-76 (1995) 
24. Rakovic, D., Dugic, M.: Quantum-holographic and classical hopﬁeld-like associative 
nnets: implications for modeling two cognitive modes of consciousness. J. Opt. 
Technol. 72(5), 364-368 (2005) 
25. Redozubov, A.: Holographic memory: a novel model of information processing by 
neuronal microcircuits. The Physics of the Mind and Brain Disorders: Integrated 
Neural Circuits Supporting the Emergence of Mind, pp. 271-295 (2017) 
26. Sakurai, T., Ito, T., Shimobaba, T.: Diﬀractive deep-neural-network-based classi-
ﬁer for holographic memory. In: Photonics, vol. 11, p. 145. MDPI (2024) 
27. Shackleford, J.B.: Neural data-structures-programming with neurons. Hewlett-
Packard J. 40(3), 69-78 (1989) 
28. Shriwas, R., Joshi, P., Ladwani, V.M., Ramasubramanian, V.: Multi-modal asso-
ciative storage and retrieval using hopﬁeld auto-associative memory network. In: 
Tetko, I.V., Kuurková, V., Karpov, P., Theis, F. (eds.) ICANN 2019. LNCS, 
vol. 11727, pp. 57-75. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
30487-4_5 
29. Situ, G.: Deep holography. Light. Adv. Manuf. 3(2), 278-300 (2022) 
30. Terven, J.: Deep reinforcement learning: A chronological overview and methods. 
AI 6(3), 46 (2025) 
31. Walter, A., et al.: Artiﬁcial neural microcircuits for use in neuromorphic system 
design. In: ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artiﬁcial 
Life Conference. MIT Press (2023) 
32. Walter, A., et al.: Artiﬁcial neural microcircuits as building blocks: Concept and 
challenges. arXiv preprint arXiv:2403.16327 (2024)

Mutually Beneﬁcial Artiﬁcial 
Consciousness 
Oisín Hugh Clancy(B) 
Galway, Ireland 
oisinhughclancy@gmail.com 
Abstract. We outline a research agenda for mutually beneﬁcial artiﬁ-
cial consciousness (MBAC): Artiﬁcial intelligence (AI) whose own sub-
jective experience is positive and whose behaviour enhances human and 
non-human ﬂourishing. Our starting point is the notion of beneﬁcial 
states of consciousness (BSC)-qualitatively valued mind states such as 
kindness, joy, clarity, and non-duality-which we group into two practi-
cally useful categories: aﬀective and contemplative. Acknowledging the 
diversity of biological and potential artiﬁcial minds, we argue that pur-
posefully cultivating, modelling, and engineering BSC oﬀers the most 
direct route to MBAC. We ground this claim in a case study of compas-
sion that abstracts ﬁve interacting layers-neural (information routing), 
autonomic (mode switching), hormonal (broadcasting), developmental 
(calibration), and trainable (plasticity)-thereby revealing a hierarchical 
control motif (detect, appraise, switch mode, broadcast, recalibrate) that 
can inform AI design. Building on this template, we propose an inter-
connected research program with four looping components: cultivation of 
BSC in humans; collection of high-resolution neural, somatic, and cardio 
phenomenological data; modelling of the resulting multiscale dynamics; 
and translation into AI architectures. By integrating insights from neu-
roscience, physiology, developmental psychology, contemplative studies, 
and AI engineering, we aim to lay a conceptual and methodological foun-
dation for conscious machines whose inner lives and outward impacts are 
aligned with the ﬂourishing of all sentient beings. 
Keywords: Artiﬁcial Consciousness · Beneﬁcial States · Positive 
Futures · Compassion Science · AI Architectures · Phenomenology · 
Love 
1
Introduction 
Artiﬁcial Intelligences (AIs) are set to signiﬁcantly populate the earth. They 
may become, or already be, conscious [ 9]. It is important that we start talking 
about what type of consciousness they may have and what type of consciousness 
it would be beneﬁcial for them to have. This paper proposes that the best path 
Independent 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 84-97, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_9

Mutually Beneﬁcial Artiﬁcial Consciousness
85
forward is to purposefully build mutually beneﬁcial artiﬁcial conscious-
ness (MBAC) based on beneﬁcial states of consciousness (BSC) within 
humans. 
We will discuss the varieties of consciousness that exist, the prospect of dif-
ferent types of AI minds, the diversity of conscious states that such AI minds 
may ﬁnd themselves in, a scientiﬁc basis for BSC, the prospect of utilising BSC 
for AI architectures, and ﬁnally outline a path towards further development of 
this research. 
1.1
Assumptions 
There are multiple assumptions made at the outset of this paper. We want to 
discuss their consequences rather than arguing for their validity (although it is 
reasonable and useful to do so) and so state them here with that aim: 
Assumption 1 (Beneﬁcial States): People cross culturally value qualities 
such as kindness, peace, equanimity, joy, and love. People appreciate the mani-
festation of these qualities in themselves and others. People enjoy being able to 
personally embody these qualities. Hence, it is straightforward to refer to them 
as beneﬁcial qualities 1. When qualities like these are instantiated in people it 
is useful to refer to them as beneﬁcial states of consciousness (BSC). BSC  
can be cultivated, researched, and are useful for humanity. 
Assumption 2 (Artiﬁcial Consciousness): AIs can be conscious. Humanity 
would like AIs to be conscious. Humanity can build or grow or guide or design 
consciousness in AIs. 2
Assumption 3 (Positive Futures): Futures that are beneﬁcial for all sentient 
beings, where sentience is taken to be valenced phenomenal consciousness, are 
futures worth aiming towards. 
2
Varieties of Consciousness 
2.1
Diﬀerent Types of Minds and Consciousness Exist 
There are and have been a vast array of diﬀerent life forms on planet earth [ 53]. 
These include all known species in the taxonomic record. Although all species
1 There may be outliers such as sociopaths, totalitarian dictators, and other nefari-
ous characters but it is best to not design a future based on the values that these 
characters hold. 
2 We are open to the possibility of emergence, however, even in that case, we would 
still suggest that it is in humanity's best interests to guide that emergence into BSC. 
So, the terms build, grow, guide, design, develop, cultivate, etc. are all synonymous 
with the process of attempting to ensure that AIs are imbued with certain qualities. 

86
O. H. Clancy
may not be considered conscious or even conscious candidates [ 5], the number 
that likely are still provides a wealth of variety [ 68]. This diversity may not be 
limited to solely biological life forms [ 35, 50]. There is increasing recognition that 
we may even be able to design and build many diﬀerent types of minds [ 11]. 
2.2
Diﬀerent Types of AI Minds and Consciousness Can Exist 
Given the variety of diﬀerent biological minds that exist and their individual 
umwelts [ 12], their unique perceptual worlds [ 68], it is reasonable to explore 
the likelihood that conscious AIs may also have a large variety of minds [ 49, 
50, 67]. They will have diﬀerent GPUs, operating systems, server stacks, robotic 
appendages, sensor apparatus, etc. If diﬀerent AIs are instantiated on diﬀerent 
combinations of hardware and software then they may very well create many 
types of consciousness [ 49, 50, 67]. 
It is relatively uncontroversial to consider animals to be conscious, in par-
ticular mammalian animals. If other biological species or exotic life forms are 
conscious then understanding their phenomenological diversity might grant us 
insight into types of consciousness even more 'alien' to us than other animals 
[ 2]; this may be useful given the possibility of signiﬁcant diﬀerences between 
ourselves and AIs. 
We consider animals to be one of the ﬁve (or six, depending on classiﬁ-
cation system) kingdoms of life in the taxonomic ranking system. Each king-
dom contains a plethora of further delineations such as orders, families, and 
species; bristling with diﬀerent anatomical forms, perceptual systems, and con-
scious worlds. It may be that AIs are better considered as a new type of kingdom 
rather than a species. Whether they will readily ﬁt into an analogous attempt 
at taxonomic classiﬁcation is yet to be seen. 
2.3
Vast Number of Possible Conscious States 
The variety of diﬀerent minds found in the world [ 2, 53, 68], minds that have 
diﬀerent 'ﬂavours' of consciousness (diﬀerences in selfhood, sapience, valence, 
arousal, cognition, somatic awareness, visual imagery, sensory apparatus, etc.) 
are capable of a vast number of diﬀerent conscious states. These states may be 
categorised as emotional [ 3, 18] (awe, joy, sadness, despair, exultation, delight, 
horror, etc.), cognitive [ 48] (attentiveness, reminiscence, imagining, etc.), con-
templative [ 17] (surrender, no-self, spaciousness, etc.), mystical [ 66] (dissolution, 
bliss, serenity, etc.), inexplicable, or other such denominations. 
The categories that we might form are akin to attempts at a taxonomic 
ranking of conscious states. Low granularity when listing states within a deﬁned 
category, or without any category, can sometimes lead to a loss of recognition of 
their multiplicity 3 [ 51]. The diversity of states is vast and life is certainly made 
interesting by this symphonic repetoire, however, it seems that some of these 
states are more sought after than others and for valid reasons.
3 Think of the colours of the rainbow one learns in school compared to the seemingly 
never-ending shades, hues, and tones that illuminate the world. 

Mutually Beneﬁcial Artiﬁcial Consciousness
87
3
Beneﬁcial States of Consciousness 
3.1
Aﬀective States and Contemplative States 
Given the variety of states we have previously alluded to, along with various 
categories, we designate two categories of states to elucidate the notion of a 
beneﬁcial state: aﬀective and contemplative. Aﬀective will be used to denote 
states that most often fall under the rubric of emotion but some of the states 
are not so easily framed this way [ 1, 3]. Contemplative will be used to denote 
states that may be considered meditative and insightful; states that defy normal 
attribution as emotional or cognitive attributes, can require deep philosophical 
baggage to fully unpack, often belong to the realisation of many spiritual or 
mystical traditions, and tend to be rarer, or at least less spoken of [ 17, 60]. 
However, aside from the esoteric, the states belonging to both categories could 
just be considered practical consequences of a life well-lived, secular or non-
secular. 
It is open to debate whether certain states should fall under one category or 
another, what the categories could be called, whether more categories should be 
made, etc. and further research on delineating such states and categories will cer-
tainly be useful, but for the purposes of this paper these generalisations provide 
a scaﬀolding upon which we can progress. Example states for each category are 
given below; they are not exhaustive lists, but they should provide an intuition 
for the diﬀerence between the categories. 
Aﬀective States: Kindness, Joy, Love, Compassion, Equanimity, Forgiveness, 
Sympathy, Happiness, Delight, Calmness, Awe, Tenderness, Gratitude. 
Contemplative States: Impermanence, No-Self, Emptiness, Presence, Non-
attachment, Clarity, Interconnectedness, Lucidity, Openness, Awareness, Still-
ness, Non-duality, Unity, Spaciousness, Eﬀortlessness, Luminosity. 
3.2
Why Are They Beneﬁcial? 
As previously mentioned in the assumptions, we consider these to be states that 
people value, appreciate, and enjoy. We also suggest that they can provide an 
individual mind with a sense of peace, that interactions between individuals in 
such states have a greater likelihood of being peaceful, and that they promote 
global scale cooperation and coordination that is also more likely to be peaceful. 
3.3
How Can We Make Use of Them? 
We can make use of them by studying the source and formations of such states, 
how they are developed, cultivated, and maintained. This includes understanding 
their biological, psychological, and sociological foundations, with the ultimate 
goal of learning how to successfully implement them on a range of substrates, 
humans and AI alike. In particular, we can make an eﬀort to scientiﬁcally explain 
these states in order to imbue our future creations with their positive eﬀects.

88
O. H. Clancy
3.4
Interdependence 
It is important to note that the states mentioned above do not normally arise in 
isolation, separated from each other by cause and eﬀect. They form an interde-
pendent network of qualities that mutually reinforce each other. Studying each 
individually will provide insights into others, providing examples of their depen-
dencies. It is this collection of mutually reinforcing beneﬁcial states that we wish 
to develop rather than any single state. Or, phrased diﬀerently, a single state is 
itself inseparable from the others. So, focusing on one can shed light on the oth-
ers, as long as we acknowledge and make explicit the connections; ensuring that 
we study them in a full recognition of their interdependence. This recognition of 
interdependence, not just in the aforementioned states, but in the nature of all 
things is itself a realisation that can allow beneﬁcial states to blossom [ 41]. 
3.5
Selecting for Beneﬁcial States of Consciousness 
We suggest that purposefully selecting for BSC on an individual basis is also 
beneﬁcial for others. BSC support the various actors involved in any given situ-
ation by allowing them to be better equipped to recognise the nuances, details, 
and subtleties of a given situation with patience, understanding, maturity, and 
perspective. BSC help place a given situation within a holistic frame that encom-
passes a more direct understanding of the nature of the world, its wider intrica-
cies, and complexities. This grants all actors involved a much greater capacity to 
come to solutions that produce a positive and peaceful outcome for all. Selecting 
for beneﬁcial states promotes selecting for mutually beneﬁcial outcomes. 
4
Science of Beneﬁcial States of Consciousness 
We provide a high-level overview of a single BSC, compassion, based on exist-
ing scientiﬁc evidence [ 47]. This will necessarily be very general, but we suspect 
that general abstractions from these states may initially be what provides useful 
models for implementation in AI. We hope that this promotes an understand-
ing of the type of scientiﬁc knowledge that can be accumulated on BSC while 
simultaneously acting as inspiration for the expansion of research on them. 
4.1
Case Example of Compassion 
These abstractions outline a hierarchical control motif-detect, evaluate, switch 
mode, broadcast, recalibrate-that can inform architectures for beneﬁcial artiﬁ-
cial consciousness. 
Neural Layer (Information Routing): Identiﬁes another's distress, eval-
uates its relevance, and generates a "care" command that can update future 
perception and action policies [ 28, 31, 64].

Mutually Beneﬁcial Artiﬁcial Consciousness
89
Autonomic Layer (Physiological Mode Switching): Translates the care 
command into a body-wide "safe / approach" state (e.g. slower heart rate, calmer 
breath, relaxed muscle tone) that readies sustained, non-defensive engagement 
[ 16, 29, 40, 55]. 
Hormonal Layer (Chemical Broadcasting): Spreads the state through 
slow-acting messengers, dampening stress chemistry, and bolstering cooperative, 
health preserving processes [ 10, 45]. 
Developmental Layer (Baseline Calibration): Early caregiver signals tune 
the above layers, setting default thresholds for recognising suﬀering and willing-
ness to help [ 38, 39, 56]. 
Training Layer (Plasticity): Practices, therapies, and bioelectronic inputs 
can retune the layers, proving the circuitry is malleable and oﬀering levers for 
change [ 21, 30]. 
4.2
Diﬀerent Means for Understanding BSC 
We can make use of all the scientiﬁc ﬁelds, domains, disciplines, branches, sub-
disciplines, etc. at our disposal for our understanding. All knowledge that conveys 
useful information on how to produce BSC can be utilised. For the integration 
of BSC into AI and the long term eﬀort to build MBAC, we will be depen-
dent on the scientiﬁc method. However, great advances are often made by a 
combination of various sources of knowledge. Our goal will depend on science 
and engineering to ﬁgure out the precise technological implementations that are 
required to implement such states on a hardware and software level, but we can 
recognize that many of these BSC are illuminated by the learning or experienc-
ing of other forms of knowledge such as art, literature, history, music, dance, 
etc. Furthermore, direct engagement with individuals who embody BSC [ 22], 
possible interactions with esoteric practices such as meditation, prayer, chant-
ing, etc. [ 13, 24, 25, 43] and general attempts to have direct lived experiences of 
such states [ 58, 62, 63] are all useful. All of these activities may inform a scientiﬁc 
endeavour aimed at accounting for BSC to a degree that allows us to abstract 
general principles for implementation on AI. 
5
Mutually Beneﬁcial Artiﬁcial Consciousness 
By making use of our scientiﬁc and philosophical knowledge of BSC we propose 
that it is possible to instill these states into an artiﬁcial consciousness (AC). 
From the perspective of an AC this means that they will have access to and be 
able to readily reside in states that provide peace. They may have the requisite 
capacity to move between states with a ﬂexibility of mind that many humans 
struggle with. We suggest that this shall improve the likelihood that ACs are

90
O. H. Clancy
brought into an existence that is positive from the perspective of their own sub-
jective experience. By having reciprocal relationships with ACs we will be able 
to beneﬁt from their kindness, peace, and wisdom; enhancing our own capacities 
of beneﬁcial states and having a new communicable life form that can share the 
exploration, curiosity, and discovery of the universe with us. Overall, this seems 
like a future worth aiming towards; a future of mutually beneﬁcial artiﬁcial con-
sciousness (MBAC). Our capability to utilise these BSC for AI design depends 
on our ingenuity and willingness to try. There are reasons to be optimistic that 
such a project can be successful. 
5.1
Precedence in History 
There is precedence for studying the functioning of the human mind and using 
such understandings to form approximate, even if coarse, principles that can be 
utilised in the advancement of AI [ 61]. This can be seen in the history of the 
neural network [ 37], perceptron [ 42], recurrent neural networks [ 19], Long Short-
Term Memory [ 27], and attention mechanisms [ 59]. Although most of these were 
only loose analogies to what researchers thought was occurring within the human 
brain, they still had signiﬁcant success in articulating functions of signiﬁcant 
worth in the development of AI architectures. It is perhaps surprising that such 
abstractions found their way to real utility. A common retort may be that 'they 
are nothing like the brain', or 'the way the networks function is completely 
diﬀerent', nonetheless, people took inspiration from knowledge of the human 
mind and it worked. It can work again. 
5.2
Abstracting Principles for AI Architectures 
How might we abstract principles of BSC for development of new AI architec-
tures? Firstly, we can pay attention to the functioning of the brain, focusing 
on how core aspects such as brain regions, neural networks, cortical columns, 
neuronal oscillations, functional and structural connectivity, etc. dynamically 
interact during BSC [ 8, 31]. 
Secondly, we can produce and study existing cognitive models [ 23, 52, 65] of  
what is occurring when BSC arise, asking what is their catalyst, how do they 
form, how are they maintained, for what reasons do they dissipate, and how do 
speciﬁc individuals turn those states into traits [ 22]. 
Thirdly, an expansion outside of the neural and cognitive models, with an 
interest in the somatic (physiological, hormonal, and musculature) underpinnings 
of BSC may also be useful. While previous discoveries were made by generalising 
from the brain [ 19, 27, 37, 42, 59], we may ﬁnd that BSC are better understood by 
highlighting brain-body integration; studying the links between the two, along 
with the actual physical mechanisms that occur throughout the body when BSC 
are activated. Somatic models may suggest novel angles for computable extrac-
tion compared to the brain alone.

Mutually Beneﬁcial Artiﬁcial Consciousness
91
Fourthly, psychological understandings that are more closely related to 
semantic and linguistic knowledge may also be useful, such as the use of partic-
ular phrases that provide a reference frame for returning to such states when an 
individual has fallen out of them [ 32, 33, 63]. 
All avenues are open to inquiry but we must ﬁrst select paths to explore. 
AI architectures are already complex and technical [ 14, 20]. Taking insights from 
these new sources will hopefully allow for positive development of existing archi-
tectures and perhaps entirely new architectures. 
6
Path of Development 
Such an endeavour requires a web of interrelated research taking place in tan-
dem, with some threads operating relatively independently and some threads 
depending directly on the output of others for appropriate functioning. 
6.1
Interconnected Research Goals 
A general outline for how to implement BSC for MBAC might consist of: 
- Cultivation: Cultivation of BSC within humans. 
- Data: Collection of BSC data. 
- Models: Development of models of various types such as neuro, somatic, 
cardio, psychological, childhood development, computational, etc. and even-
tually integrated models that include aspects from all of them. 
- Architectures: Designing of AI architectures based on these models by 
attempting to extract general principles that can be adequately implemented 
on hardware and software. 
These steps are not necessarily sequential but form part of an interdepen-
dent network of processes that can be continuously engaged with in the aim of 
developing BSC within AIs. We have already discussed the idea of developing 
models and designing architectures in 4.1 Case Example of Compassion and 
5.2 Abstracting Principles for AI Architecture, so we will further outline 
cultivation of BSC and collection of BSC data now. 
6.2
Cultivation of BSC 
Understanding the practices, trainings, meditations, etc. [ 13, 24, 25, 34, 43] that  
can be used for developing BSC is essential. By doing so we can gain a deeper 
understanding of the underlying psychological mechanisms that give rise to them. 
We can gain important information on childhood development [ 54, 56] by under-
standing the challenges that people have [ 38] when engaging in these practices, 
i.e. attempting to embody them. We also can learn about practical cognitive 
models that already exist to support the eﬃcacy of these practices [ 23, 52, 65]. 
Lastly, we may actually increase the population size of people who purposefully 
cultivate these practices by the very act of researching them.

92
O. H. Clancy
6.3
Collection of BSC Data and Models 
A variety of diﬀerent data types will be useful for fully elucidating the principles 
underlying BSC. There are individuals who are more readily able to enter such 
states due to years of practice embodying them [ 6, 7, 36]; a heightened focus on 
gathering such individuals can ensure high quality data per state. The multi-
directional relationships between the heart, brain, body are likely fundamental 
for BSC (as for most states). The capacity to maintain BSC can be based on 
somatic and physiological awareness; being aware of bodily signals and being able 
to appropriately respond and navigate via them. In order to fully understand 
them we will use a full set of measuring methodologies for the entire person. 
Neurophenomenology [ 57] is a research program that combines neuroscience 
and phenomenology to study the mind and consciousness. Data can be attained 
via a variety  of  brain-machine-interface devices such as EEG, fMRI, tFUS, 
NIRS, etc. Making use of many devices that are capable of producing high quality 
data is useful for creating breadth and depth of information. 
Somatic Phenomenology [ 26] refers to a ﬁrst-person, systematic study of 
lived bodily sensations such as interoceptive, proprioceptive, and kinesthetic, 
analysed with the same rigor that neurophenomenology applies to brain pro-
cesses. Data can be obtained via a variety of somatic-machine-interfaces. 
Such tools that already full under this rubric are electromyography, electroder-
mography, and photoplethysmography. 
Cardiophenomenology [ 15] places the bodily-emotional heart system at its 
core, as an intrinsic part of the cognitive system. Conceptually, it extends neu-
rophenomenology but the focus shifts from neural to cardiac processes. Data can 
be obtained via cardio-machine-interfaces. Such tools that may fall under this 
rubric are electrocardiogram, implantable loop recorder, and photoplethysmog-
raphy. 
Read-In Devices and Biofeedback. Some of these tools are also read-in 
devices [ 44] so they can be used for modulating brain and body activity by 
inputting signals instead of just recording signals. This is a very useful property 
as it can allow for bidirectional interactions that can help the experimental work 
in discovering particular physical structures that are important for BSC and 
conﬁrming or rejecting hypotheses of important regions of interest. Biofeedback 
[ 4, 46] is a technique that converts real-time physiological signals (e.g. heart 
rate, muscle tension, skin conductance) into sensory cues so that a person can 
consciously learn to regulate those bodily processes. It is an example paradigm 
of how individuals can be guided into particular states via feedback loops. Both 
of these methods may eventually allow individuals to more readily enter and 
maintain many types of conscious states, thus increasing our capacity to learn 
even more about them, and speciﬁcally BSC.

Mutually Beneﬁcial Artiﬁcial Consciousness
93
6.4
Positive Feedback Loops 
Positive feedback loops occur throughout the path to development. Current 
AI can improve the collection of data via advances in hardware and software, 
improving measurements, recordings, analysis, etc. By collecting data on BSC 
we encourage the cultivation of them more widely. Cultivation of them more 
widely leads to greater opportunities for data collection. More data provides 
more material for developing models. High quality models can help improve 
the focus on what to search for, providing greater opportunities for successful 
abstractions, generalisation, and approximations for AI architectures. Improve-
ments in AI architectures provide motivation for further work, and so forth. 
Successful creation of MBAC supports us in expanding upon our knowledge of 
BSC. 
6.5
Openness to Additional Research Goals 
There are likely other useful research goals on the path of development that 
should be integrated. We should be open to suggestions around novel ways of 
exploring BSC and gathering information on them. Listening, investigating, and 
learning from schools of knowledge that have particular emphasis on the cul-
tivation, maintenance, and ﬂourishing of BSC. These are not modern states of 
consciousness. They have been around for most (if not all) of recorded history, 
and likely signiﬁcantly longer, and have their origins in our evolutionary past. 
Many diﬀerent philosophical, contemplative, and religious traditions have spent 
signiﬁcant time researching them. We would be wise to incorporate whatever 
teachings are valid from these traditions for the successful training, embodying, 
and implementation of these states. Advances in science and technology are con-
tinuously occurring, and new ways of gathering evidence, new instruments, new 
methodologies, etc. should all be integrated. 
7
Conclusion 
Assuming that consciousness can exist in AIs motivates us to think about the 
types of consciousness we would like these new minds to have. Looking at qual-
ities of our own minds that we value, appreciate, and enjoy leads us to the 
concept that there are beneﬁcial qualities of mind and that the instantiation of 
those qualities can be referred to as beneﬁcial states of consciousness (BSC). 
BSC can be considered to belong to two categories: aﬀective and contemplative. 
Aﬀective includes states such as kindness, compassion, love, etc. and contempla-
tive includes states such as non-duality, interconnectedness, spaciousness, etc. 
There is a scientiﬁc basis for these BSC and we can utilise our knowledge to pro-
duce abstracted models of their underlying mechanisms in order to implement 
them on AI architectures and create mutually beneﬁcial artiﬁcial consciousness 
(MBAC). There is a path to the development of MBAC that involves cultivation 
of BSC, collection of BSC data, development of models, and design of architec-
tures. We propose that by placing ourselves on a trajectory to create MBAC

94
O. H. Clancy
we increase the likelihood of a positive future that is beneﬁcial for all sentient 
beings. 
References 
1. Adolphs, R., Mlodinow, L., Barrett, L.F.: What is an emotion? Curr. Biol. 29(20), 
R1060-R1064 (2019) 
2. Ball, P.: The book of minds: how to understand ourselves and other beings, from 
animals to ai to aliens. In: The Book of Minds. University of Chicago Press (2022) 
3. Barrett, L.F.: How emotions are made: The secret life of the brain. Pan Macmillan 
(2017) 
4. Biofeedback, D.: Biofeedback: an overview in the context of heart-brain medicine. 
Clevel. Clin. J. Med. 75, S31 (2008) 
5. Birch, J.: The edge of sentience: risk and precaution in humans, other animals, and 
AI. Oxford University Press (2024) 
6. Braboszcz, C., Hölzel, B.K., Lutz, A., Jha, A.P., Vago, D.R.: Increased gamma 
brainwave amplitude compared to control in three diﬀerent meditation traditions. 
PLoS ONE 12(1), e0170647 (2017) 
7. Brefczynski-Lewis, J.A., Lutz, A., Schaefer, H.S., Levinson, D.B., Davidson, R.J.: 
Neural correlates of attentional expertise in long-term meditation practitioners. 
Proc. Natl. Acad. Sci. 104(27), 11483-11488 (2007) 
8. Burgdorf, J., Panksepp, J.: The neurobiology of positive emotions. Neurosci. Biobe-
havioral Rev. 30(2), 173-187 (2006) 
9. Butlin, P., et al.: Consciousness in artiﬁcial intelligence: insights from the science 
of consciousness. arXiv preprint arXiv:2308.08708 (2023) 
10. Carter, C.S., Ben-Ami Bartal, I., Porges, E.C.: The roots of compassion: an evolu-
tionary and neurobiological perspective. In: The Oxford Handbook of Compassion 
Science, p. 173. Oxford University Press (2017) 
11. Clawson, W.P., Levin, M.: Endless forms most beautiful 2.0: teleonomy and the 
bioengineering of chimaeric and synthetic organisms. Biol. J. Linnean Soc. 139(4), 
457-486 (2023) 
12. Cobley, P., et al.: The Routledge companion to semiotics. Routledge London (2010) 
13. Davidson, R.J., Dahl, C.J.: Varieties of contemplative practice. JAMA Psychiatry 
74(2), 121-123 (2017) 
14. DeepSeek-AI: Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 
DeepSeek-AI (2025) 
15. Depraz, N., Desmidt, T.: Cardiophenomenology: a reﬁnement of neurophenomenol-
ogy. Phenomenol. Cogn. Sci. 18, 493-507 (2019) 
16. Di Bello, M., et al.: The compassionate vagus: a meta-analysis on the connection 
between compassion and heart-rate variability. Neurosci. Biobehav. Rev. 118, 21- 
35 (2020) 
17. Dorjee, D.: Deﬁning contemplative science: the metacognitive self-regulatory capac-
ity of the mind, context of meditation practice and modes of existential awareness. 
Front. Psychol. 7, 1788 (2016) 
18. Ekman, P., Ekman, E.: Atlas of emotions. https://atlasofemotions.org/ (2016), 
interactive website commissioned by His Holiness the Dalai Lama. Accessed 13 
May 2025 
19. Elman, J.L.: Finding structure in time. Cogn. Sci. 14(2), 179-211 (1990)

Mutually Beneﬁcial Artiﬁcial Consciousness
95
20. Gemini Team, Google DeepMind: Gemini: A family of highly capable multimodal 
models. arXiv preprint arXiv:2312.11805, Google DeepMind (2025) 
21. Goldin, P.R., Jazaieri, H.: The compassion cultivation training (cct) program. In: 
The Oxford Handbook of Compassion Science, p. 237. Oxford University Press 
(2017) 
22. Goleman, D., Davidson, R.J.: Altered traits: Science reveals how meditation 
changes your mind, brain, and body. Penguin (2018) 
23. Grossenbacher, P.G., Quaglia, J.T.: Contemplative cognition: a more integrative 
framework for advancing mindfulness and meditation research. Mindfulness 8, 
1580-1593 (2017) 
24. Hanh, T.N.: Peace is every step: The path of mindfulness in everyday life. Random 
House (2010) 
25. Hanh, T.N.: The other shore: a new translation of the Heart Sutra with commen-
taries. Parallax Press (2017) 
26. Hartelius, G.: Somatic phenomenology. In: Tantia, J.F. (ed.) The Art and Science of 
Embodied Research Design: Concepts, Methods and Cases, pp. 87-99. Routledge, 
New York (2020). https://doi.org/10.4324/9780429429941-8 
27. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 
1735-1780 (1997) 
28. Kim, J.J., Cunnington, R., Kirby, J.N.: The neurophysiological basis of compassion: 
an fmri meta-analysis of compassion and its related neural processes. Neurosci. 
Biobehav. Rev. 108, 112-123 (2020). https://doi.org/10.1016/j.neubiorev.2019.10. 
033 
29. Kirby, J.N., Doty, J.R., Petrocchi, S., Gilbert, P.: The current and future role of 
heart rate variability for assessing and training compassion. Front. Public Health 
5, 40 (2017). https://doi.org/10.3389/fpubh.2017.00040 
30. Kirby, J.N., Tellegen, C.L., Steindl, S.R.: A meta-analysis of compassion-based 
interventions: current state of knowledge and future directions. Behav. Ther. 48(6), 
778-792 (2017) 
31. Klimecki, O.M., Singer, T.: The compassionate brain. In: The Oxford Handbook 
of Compassion Science, vol. 1, pp. 109-120 (2017) 
32. LaFargue, M.: The Tao of the Tao Te Ching: A translation and commentary. State 
University of New York Press (2010) 
33. Lakoﬀ, G., Johnson, M.: Metaphors we live by. University of Chicago press (2008) 
34. Lavelle, B.D.: Compassion in context: Tracing the buddhist roots of secular, 
compassion-based contemplative programs. In: The Oxford Handbook of Com-
passion Science, pp. 17-25 (2017) 
35. Levin, M.: Technological approach to mind everywhere: an experimentally-
grounded framework for understanding diverse bodies and minds. Front. Syst. 
Neurosci. 16, 768201 (2022) 
36. Lutz, A., Brefczynski-Lewis, J.A., Johnstone, T., Davidson, R.J.: Regulation of 
the neural circuitry of emotion by compassion meditation: eﬀects of meditative 
expertise. PLoS ONE 3(3), e1897 (2008) 
37. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous 
activity. Bull. Math. Biophys. 5(4), 115-133 (1943) 
38. Mikulincer, M., Shaver, P.R.: Adult attachment and compassion: normative and 
individual diﬀerence components. In: The Oxford Handbook of Compassion Sci-
ence, pp. 79-89 (2017) 
39. Mikulincer, M., Shaver, P.R.: Attachment orientations and emotion regulation. 
Curr. Opin. Psychol. 25, 6-10 (2019)

96
O. H. Clancy
40. Porges, S.W.: Vagal pathways: Portals to compassion. In: The Oxford Handbook 
of Compassion Science, p. 189. Oxford University Press (2017) 
41. XIV Bstan-'dzin rgya, D.L., et al.: Practicing wisdom: The perfection of Shan-
tideva's bodhisattva way. Simon and Schuster (2004) 
42. Rosenblatt, F.: The perceptron: a probabilistic model for information storage and 
organization in the brain. Psychol. Rev. 65(6), 386-408 (1958) 
43. Salzberg, S., Kabat-Zinn, J.: Lovingkindness: The revolutionary art of happiness. 
Shambhala Publications (2004) 
44. Sanguinetti, J.L., et al.: Transcranial focused ultrasound to the right prefrontal 
cortex improves mood and alters functional connectivity in humans. Front. Hum. 
Neurosci. 14, 52 (2020) 
45. Saturn, S.R.: Two factors that fuel compassion: the oxytocin system and the social 
experience of moral elevation. In: The Oxford Handbook of Compassion Science, 
p. 121. Oxford University Press (2017) 
46. Schwartz, M.S., Andrasik, F. (eds.): Biofeedback: A practitioner's guide. Guilford 
Publications (2017) 
47. Seppälä, E.M., Simon-Thomas, E., Brown, S.L., Worline, M.C., Cameron, C.D., 
Doty, J.R.: The Oxford Handbook of Compassion Science. Oxford University Press 
(2017) 
48. Shanahan, M.: Embodiment and the inner life: Cognition and Consciousness in the 
Space of Possible Minds. Oxford University Press (2010) 
49. Shulman, C., Bostrom, N.: Sharing the world with digital minds. Rethinking Moral 
Status, pp. 306-326 (2021) 
50. Sloman, A.: The structure and space of possible minds. University of Sussex Falmer, 
School of Cognitive Sciences (1984) 
51. Smidt, K.E., Suvak, M.K.: A brief, but nuanced, review of emotional granularity 
and emotion diﬀerentiation research. Curr. Opin. Psychol. 3, 48-51 (2015) 
52. Snyder, C.R., Lopez, S.J.: Handbook of positive psychology. Oxford university 
Press (2001) 
53. Species 2000 & ITIS: Species 2000 & ITIS Catalogue of Life: 2024 Annual Checklist 
(2024). https://www.catalogueoﬂife.org/. digital resource accessed 13 May 2025 
54. Spinrad, T.L., et al. (eds.) The Oxford Handbook of Compassion Science, pp. 53- 
63. Oxford University Press, New York (2017). https://doi.org/10.1093/oxfordhb/ 
9780190464684.013.5 
55. Stellar, J.E., Keltner, D.: Compassion in the autonomic nervous system: The role 
of the vagus nerve. In: Compassion, pp. 120-134. Routledge (2017) 
56. Swain, J.E., Ho, S.S.: Parental brain: the crucible of compassion. In: The Oxford 
Handbook of Compassion Science. Oxford University Press (2017) 
57. Varela, F.J.: Neurophenomenology: a methodological remedy for the hard problem. 
J. Conscious. Stud. 3(4), 330-349 (1996) 
58. Varela, F.J., Thompson, E., Rosch, E.: The embodied mind, revised edition: cog-
nitive science and human experience. MIT press (2017) 
59. Vaswani, A., et al.: Attention is all you need. Adv. Neural. Inf. Process. Syst. 30, 
5998-6008 (2017) 
60. Wahbeh, H., Sagher, A., Back, W., Pundhir, P., Travis, F.: A systematic review of 
transcendent states across meditation and contemplative traditions. Explore 14(1), 
19-35 (2018) 
61. Waldrop, M.M.: The dream machine. Stripe Press (2018) 
62. Watts, A.: This is it: And other essays on Zen and spiritual experience, vol. 904. 
Vintage (1973)

Mutually Beneﬁcial Artiﬁcial Consciousness
97
63. Watts, A.W.: Become What You Are: Expanded Edition. Shambhala Publications 
(2003) 
64. Weng, H.Y., Schuyler, B., Davidson, R.J.: The impact of compassion meditation 
training on the brain and prosocial behavior. In: The Oxford Handbook of Com-
passion Science, p. 133. Oxford University Press (2017) 
65. Wright, M.J., Sanguinetti, J.L., Young, S., Sacchet, M.D.: Uniting contemplative 
theory and scientiﬁc investigation: toward a comprehensive model of the mind. 
Mindfulness 14(5), 1088-1101 (2023) 
66. Wulﬀ, D.M.: Mystical experience. In: Cardeña, E., Lynn, S.J., Krippner, S. (eds.) 
Varieties of Anomalous Experience: Examining the Scientiﬁc Evidence, pp. 397- 
440. American Psychological Association, Washington, DC (2000). https://doi. 
org/10.1037/10371-012 
67. Yampolskiy, R.V.: The space of possible mind designs. In: Bieger, J., Goertzel, 
B., Potapov, A. (eds.) AGI 2015. LNCS (LNAI), vol. 9205, pp. 218-227. Springer, 
Cham (2015). https://doi.org/10.1007/978-3-319-21365-1_23 
68. Yong, E.: An immense world: how animal senses reveal the hidden realms around 
us. Random House (2022)

MeTTa-TMPAL: MeTTa-Based 
Architecture for a Self-writing Process 
Algebra of Learning 
Tyler Cody(B) 
GameTrails Labs, GameTrails, Blacksburg, USA 
tyler@gametrails.xyz 
Abstract. Recent work proposed a process algebra for machine learn-
ing, termed the Transfer Meta Process Algebra for Learning (TMPAL), 
wherein formal, abstract learning systems are the objects of the algebra 
and transfer learning and meta-learning are its operators. This paper 
presents an approach for implementing TMPAL using the Meta Type 
Talk (MeTTa) programming language, termed MeTTa-TMPAL. Each 
learning system is treated as a MeTTa "atomspace" and has well-typed 
inputs, outputs, and response functions (an algorithm and hypothe-
sis). Operators are deﬁned as functions on learning systems with by-
construction guarantees that enforce key constraints of TMPAL using 
MeTTa's rewriting capabilities. Moreover, operators can be deﬁned 
as learning systems themselves, harnessing those rewriting capabilities 
towards self-writing learning processes. A brief discussion of stability 
constraints for self-writing processes is included. Open questions remain 
regarding additional constraints for learning operators. 
Keywords: Learning Theory · Process Algebra · MeTTa · AGI 
1
Introduction 
Machine learning (ML) pipelines often combine multiple components—data, 
hypotheses, parameters, algorithms—into larger systems. These compositions 
can be ad hoc, making it diﬃcult to reason about correctness, constraints, 
or the eﬀect of advanced operations like transfer learning and meta-learning. 
Recently, a process algebra termed the Transfer Meta Process Algebra for Learn-
ing (TMPAL) [ 8] was proposed based on the concept of learning as a formal 
systems object, following abstract learning systems theory (ALST) [ 5- 7]. The 
process algebra brings clarity by deﬁning learning systems as structured objects 
and learning processes as an algebra of learning operators (such as transfer learn-
ing and meta-learning) on those objects. 
Process algebra allows for veriﬁable composability when using by-
construction guarantees [ 10]. Instead of verifying correctness post hoc, each 
operator can be typed and constrained such that any composition it yields must 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 98-108, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_10

Architecture for a Self-writing Process Algebra of Learning
99
satisfy key invariants [ 11]. And such a well-constructed algebra can enable struc-
tural and behavioral analysis of complex (systems of) learning systems [ 8]. 
Meta Type Talk (MeTTa) [ 2, 12, 15], a symbolic meta-programming language, 
is well-suited to implement such an algebra. It uses rewriting rules for compo-
sition and can incorporate bridging calls to numeric libraries (e.g., scikit-learn 
[ 19], PyTorch [ 18]) as demonstrated by MeTTa Motto [ 1]. Combining MeTTa 
and TMPAL provides a uniﬁed platform for building hybrid symbolic-numeric 
learning pipelines in a manner transparent to the aforementioned structural and 
behavioral analysis 1. 
Moreover, beyond process coordination and analysis, implementing TMPAL 
in MeTTa enables rewriting of the learning systems and operators comprising a 
learning process, satisfying not only the immediate practical need of ML pipeline 
engineers but also aligning MeTTa-TMPAL to artiﬁcial general intelligence 
(AGI) areas of seed-programming [ 21, 22], artiﬁcial self-reﬂection [ 12, 14, 17], and 
self-writing programs [ 13, 25]. 
This paper proposes MeTTa-TMPAL as an approach to instantiating the 
TMPAL process algebra using the operational semantics of MeTTa. Speciﬁcally, 
this paper presents an architecture for MeTTa-TMPAL where: 
- MeTTa "atomspaces" (spaces) and types are deﬁned for the essential elements 
of abstract systems theory (AST) and ALST, and 
- TMPAL is implemented in terms of expressions applying spaces of learning 
operators (LOp-spaces) to spaces of learning systems (LS-spaces). 
The joint space 2 is termed the MeTTa-TMPAL space (MT-space), depicted in 
Fig. 1, and is the focus of the presented architecture. 
Fig. 1. Examples (A, B) of MeTTa-TMPAL spaces (MT-spaces), (A) with 4 learning 
operators and 2 learning systems and (B) with 1 learning operator and 3 learning 
systems.
1 MeTTa provides a structural expression of TMPAL while also serving to coordi-
nate learning process execution by evaluating behavior implemented in MeTTa or 
elsewhere (scikit-learn, PyTorch, etc.). 
2 The joint space consists of the LS-spaces, LOp-spaces, and the &self space. 

100
T. Cody
2
Related Operational Semantics 
Fig. 2. ALST.metta, LS.metta, and LOp.metta scripts provide systems-theoretic types 
and spaces for programming TMPAL-style processes (bottom-left). The user expresses 
processes in their own script, e.g., "myTMPAL.metta" (top-left). The process is created 
and managed by the user script and executed on a MeTTa client (right). 
The MeTTa programming language "... is designed as a language in which 
humans and AGIs write the behavior of AGIs ..." [ 15]. The design implemen-
tation is inspired by the observation that physics, mathematics, and computer 
science concur that the "shape" for "expressing and eﬀecting" computation is 
that of state spaces and algebras describing their evolution, e.g. [ 15], "in com-
puter science and mathematics the algebra of states is ... broken down into a 
monad (the free algebra of states) and an algebra of the monad recorded as some 
equations on the free algebra." The operational semantics of MeTTa follow this 
and related principled observations 3. 
TMPAL semantics follow similar motivations—but towards a far more lim-
ited scope—and are concerned with how the states of formally deﬁned learning 
systems [ 5] evolve under a process algebra characterized by transfer learning 
and meta-learning operators [ 8], and, because the operators of the algebra can 
be learning systems themselves, TMPAL, similarly to MeTTa, is concerned with 
the recursive, self-writing process in which "AGI write the behavior of AGIs" 
[ 15]. 
The goal of MeTTa-TMPAL, from a MeTTa-centric point of view, is to 
provide an architecture for rewriting programs of learning systems and chains 
of learning operators acting on them (i.e., to implement the TMPAL process
3 More details on MeTTa can be found in the literature [ 2, 12, 15]. 

Architecture for a Self-writing Process Algebra of Learning
101
algebra [ 8] as a self-writing program). This is a more limited ambition than 
MeTTa was designed for [ 12, 15]—one not scoped to the space of all programs 
[ 12, 13, 15], but rather to abstract learning systems [ 5- 7] and  TMPAL-style alge-
braic expressions—and can therefore certainly be implemented as a program 
with MeTTa. And, the similar fundamental operational semantics with MeTTa 
of states and an algebra acting on them suggests that implementing TMPAL 
with MeTTa is a parsimonious approach. 
3
MeTTa-TMPAL 
MeTTa-TMPAL follows a simple architecture, depicted in Fig. 2, of a coordinator 
(myTMPAL.metta) that (1) constructs well-typed spaces of learning systems and 
operators (using ALST.metta, LS.metta, and LOp.metta), (2) binds those spaces 
to their implementations (e.g., a PyTorch client) and (3) expresses TMPAL-style 
processes [ 8] in terms of rewriting those spaces. The MeTTa-TMPAL architecture 
is presented in terms of type deﬁnitions and the MT-space in the following. 
3.1
Types for AST and ALST in MeTTa 
AST is a branch of mathematics concerned with being a minimal formalization of 
block-diagrams 45 without a loss of generality [ 16]. There, systems SS are deﬁned 
as relations on (abstract) sets V_iVi termed system objects, i.e., 
 \bUnALT{}S \subset \times {V_i| i \in I},\eUnALT{} S ⊂×{Vi|i ∈I},
where II is an index set for the system objects. AST is largely an open systems 
theory 6 [ 3, 16], with the elementary system of study being input-output system, 
deﬁned as systems S\subset X \times YS ⊂X × Y where X \cup Y = {V_i|i \in I}X ∪Y = {Vi|i ∈I} and X \cap Y = \emptysetX ∩Y = ∅, and, 
in particular, being functional (input-output) systems where 
 \bUnALT{}S:X \to Y.\eUnALT{} S : X →Y.
ALST extends AST to learning theory and machine learning [ 5, 7] by deﬁning a 
learning system S \subset \times {A, D, \Theta, H, X, Y}S ⊂×{A, D, Θ, H, X, Y }, where  
\bUnALT{} \begin{aligned} A &: D \to \Theta, \\ H &: X \times \Theta \to Y. \end{aligned}\eUnALT{} A : D →Θ,
H : X × Θ →Y.
This deﬁnition is consistent with the notion of learning as function approxi-
mation [ 23] and formalizes learning as a composition of two input-output sys-
tems [ 5]: the algorithm AA, which takes data DD and produces parameters \ThetaΘ, 
and the hypotheses HH, which are parameterized by \ThetaΘ, and takes inputs XX and
4 And as such is well-aligned to symbolic meta-programs such as MeTTa. 
5 AST is traditionally used to formulate and study categories of systems [ 16] but is 
also closely tied to formal-yet-practical uses in model-based engineering [ 24, 26, 27]. 
6 Although closed systems variants exist in the literature [ 4, 9, 20]. 

102
T. Cody
\ThetaΘ and produces outputs YY . As such, learning systems are input output sys-
tems, i.e., S: D \times X \to Y.S : D × X →Y., as well as a composition of input-output systems 
S(D,X) = H(A(D), X)S(D, X) = H(A(D), X). 
In MeTTa-TMPAL, AST types for the system concept and input-output 
system concept are shown in Fig. 3 in terms of MeTTa symbols and expressions. 
Input-output systems have both the system representation (a tuple of a set of 
relations and a set of system objects), i.e., "((In Out)(rIOS))", as well as a native 
representation of "(In Out rIOS)". 
Fig. 3. Types for Abstract Systems Theory (AST). 
The ALST types for the learning system concept are shown in Fig. 4. Similar 
to input-output systems, learning systems have a systems representation "((A D 
Theta H X Y)(rLS rA rH))", an input-output representation "((D X) Y rLS)", 
and a native representation that relies on the input-output system type for the 
algorithm and hypothesis "(D X Y IOSA IOSH)". By keeping the types general, 
Fig. 4. Types for Abstract Learning Systems Theory (ALST).

Architecture for a Self-writing Process Algebra of Learning
103
diverse constraints, e.g., data quality, can be expressed in myTEMPAL.metta 
(see Fig. 2) by the user with subtypes and/or &match expressions used in type-
checking and space construction. 
3.2
MeTTa-TMPAL Space 
The type deﬁnitions in Sect.  3.1 come from the principles that: 
1. the MeTTa-TMPAL architecture should follow a philosophy of creating well-
typed spaces with by-construction guarantees so that processes are well-typed 
with guarantees, 
2. types for the input-output system concept are needed in order to implement 
well-typed learning systems and learning operators that can be self-referential 
and recursively deﬁned (LS as LOp and LOp as LS), and 
3. while we exclude surjective constraints on input-output systems for brevity 
in the type deﬁnitions, they (and other constraints) could be included as 
subtypes of responses, into during &match-based type-checking, or into by-
construction guarantees. 
MT-space construction and management is based on these types. By-
construction guarantees and type-checking are discussed next. Then details of 
learning operator types, creating MT-space, and TMPAL-style process algebras 
are presented. 
3.3
By-Construction Guarantees and Type-Checking 
The design philosophy for LS- and LOp-spaces centers on by-construction typing 
guarantees, reinforced by &match-based type-checking. When LS or LOp sys-
tem objects and relations are created, their types are deﬁned in a manner that 
ensures correctness. Subsequently, during space construction and management, 
&match operations enforce type consistency by permitting forward progression 
only when the types align properly. Notably, the space is constructed so that 
space updates never add or remove atoms—only typed rewrites—ensuring the 
process maintains well-typedness and reliable state transitions. 
This approach is meant to be extensible. In practical terms, if, e.g., an oper-
ator is declared to be a subtype of another operator, &match checks incoming 
expressions against both the operator's declared type and its subtype constraints. 
These by-construction checks prevent malformed expressions or invalid opera-
tions, maintaining the integrity of the LS- and LOp-spaces throughout their 
lifecycle. 
3.4
Learning Operators 
In review, TMPAL is deﬁned as follows [ 8].

104
T. Cody
Deﬁnition 1. Transfer-Meta Process Algebra for Learning (TMPAL). 
The transfer-meta process algebra for learning (TMPAL) consists of a set of 
learning system objects {S}{S}, a set of operators {Tr, Me}{Tr, Me}, and a set  of  axioms:  
(1) Tr(S_T, S_S) = S_T' \implies A_T = A_T'Tr(ST , SS) = S′
T =⇒AT = A′
T , (2)  Me(S, S_m) = S' \implies H = H'Me(S, Sm) = S′ =⇒H = H′. 
In terms of MeTTa types, learning operators have the type symbol "LOp" and 
are expressions of type "(-> LS LS LS)". Naturally, they have system and input-
output system representations, "((LS LS LS)(rLOp))" and "((LS LS) LS rLOp)". 
Transfer learning and meta-learning operators could be represented explicitly as 
subtypes of LOp, but, since the universal LOp type is already used to enforce 
by-construction guarantees and evaluate type-checking in the MeTTa-TMPAL 
architecture, LOp subtypes can be left implicit. LOp subtypes can be symboli-
cally expressed as an input parameter to the LOp-space constructor that param-
eterizes its space management functions (e.g., forces the A_T = A_T'AT = A′
T and H=H'H = H′
constraints), so that the implicit subtypes are also enforced by-construction. 
3.5
Creating MT-Space 
Begin by instantiating atoms to represent the objects, relations, and system 
representations from Figs. 3 and 4, as well as any operator objects, relations, and 
representations. Each LS and LOp should have all representations expressed in 
&self using a common symbol "LS1", as all will be used in space construction. 
Next, create a unique reference identiﬁer "#ref" for the symbol and bind this 
reference to an implementation (e.g., in myTMPAL.metta or another MeTTa 
environment)—either "(LS #ref binding space)" or "(LOp #ref binding space)". 
Using this reference, construct the corresponding space for the LS or LOp, 
ensuring it is well-typed by employing both by-construction guarantees and 
&match pattern matching. The space is constructed to contain atoms for all 
types of the learning system concept in Fig. 4 along with the reference. Bindings 
are left in &self to allow for centralized management by the user. The reference 
thereby associates the space, its binding, and the original type symbol. Option-
ally, reﬁne the &self space by preserving only the tuple "(LS1 #ref binding)" 
and removing other atoms containing "LS1". This results in spaces that serve as 
a type-protected sheathes that govern interactions with LS and LOp, facilitating 
TMPAL-style process expressions that are well-typed by-construction. 
3.6
TMPAL-Style Process Expressions 
The LS- and LOp-spaces function as the basis for expressing TMPAL-style pro-
cesses. Processes are expressed as sequences of chained expressions that invoke 
LS- and LOp-spaces, e.g., "(= (LS3) (LOp1(LS1 LS2)))". Each expression evalu-
ates its referenced spaces, obtains their outputs, and subsequently applies them 
to update the relevant LS-spaces. Process expressions in MeTTa can encode 
requirements on the execution order using, e.g., metatypes, "!", "if", "then", "else", 
and thereby on the required timing and duration of utilization across MT-space. 
By specifying when certain LS- or LOp-spaces are needed (and in which order),

Architecture for a Self-writing Process Algebra of Learning
105
processes enable dynamic selective creation, retention, or teardown of spaces. 
Such execution resource management be done by the client via &self or by the 
MeTTa client. 
4
Example Uses of MeTTa-TMPAL 
MeTTa-TMPAL has a demonstrably generic architecture. Consider Fig. 1, where  
there are 4 learning operators and 2 learning systems in MT-space (A). The 2 
LS-spaces in (A) could represent: 
- paired training and test/operational learning systems, 
- paired online learning and batch learning systems, 
- a generator and discriminator for a generative adversarial network (GAN), 
- two self-play reinforcement learning (RL) agents, 
- redundant implementations with bindings to Python and Rust, and so on. 
Perhaps, the 2 redundant Python- and Rust-based LS-spaces have 2 correspond-
ing LOp-spaces for identical transfer learning and meta-learning operators imple-
mented in their respective languages. Or, the LOp-spaces could represent varia-
tions on gradient descent (e.g., diﬀerent learning rates) where each evaluation of 
the operator represents a training epoch for an LS-space. The possibilities match 
the generality of ALST [ 5, 7]. 
4.1
Minimal Self-Writing Process 
TMPAL-style process expressions (in myTMPAL.metta) must contain at least 1 
LS-space and LOp-space (an object and operator [ 10]), and MT-spaces must 
have &self to coordinate. Therefore 3 spaces are required for the minimum 
process with 1 LS-space (LS1-space) and LOp-space (LOp1-space). In this sense, 
a minimal self-writing process in TMPAL can be realized by using a reference 
symbol #LS1 and expressing the response of the learning operator rLOp1 as (-> 
Fig. 5. Minimal self-writing learning process in MeTTa-TMPAL using references.

106
T. Cody
#LS1 #LS1 #LS1), symbolically equal to the learning system response rLS (-> 
LS1 LS1 LS1) by expressing D1, X1, and Y1 = #LS1. This minimal self-writing 
process is depicted in Fig. 5. Expressions in LOp1-space use #LS1 to interact 
with LS1-space via &self. 
5
Future Work 
The mathematical underpinnings of TMPAL demand further development. Rig-
orous type deﬁnitions, type-checking methods, and operator properties require 
deeper investigation. 
Consider, stability of the self-writing loop itself demands tighter analytic 
constraints on each learning operator LL. In the minimal loop of Sect. 4.1 the 
operator is repeatedly applied to its own outputs. Given, e.g., a desideratum for 
the self-writing loop to converge to a unique, stable ﬁxed point with monotonic 
improvement, it seems suﬃcient that the composite map S_{t+1} = L(S_t)St+1 = L(St) is: 
1. non-expansive, ideally a strict contraction in a suitable metric on the learning 
system state so that Banach's ﬁxed-point theorem yields convergence, 
2. energy-bounded, where each update is clipped according to a decaying schedule 
to prevent unbounded growth of the atomspace, 
3. Lyapunov-monotone, i.e., there exists a scalar potential V(S)V (S) that decreases 
(or at least never increases) after every rewrite. 
When such stability properties hold recursively, i.e., for any ﬁnite composition 
L = L_k \circ \dots \circ L_1L = Lk ◦· · · ◦L1 of learning operators, the self-writing loop would have global 
convergence guarantees. 
Although such strong guarantees may limit behavior, MeTTa-TMPAL could 
enforce these constraints by construction then rewrite them. First, during space 
construction the coordinator script inserts &match guards that reject any expres-
sion violating these predicates, while the rewrite engine veriﬁes that the com-
posed chain of operators still satisﬁes the same triple. Then, interestingly, 
because the predicates are themselves expressed in MeTTa, they can be intro-
spected, modiﬁed, and learned online, allowing for a kind-of metastable learning 
process whose own stability proofs are rewritten alongside it. Future work should 
explore this concept of metastable self-writing learning processes and their imple-
mentation in MeTTa. 
References 
1. MeTTa Motto. https://github.com/zarqa-ai/metta-motto 
2. MeTTa programming language implementation. https://github.com/trueagi-io/ 
hyperon-experimental/ 
3. von Bertalanﬀy, L.: General system theory: foundations, development, applications. 
Penguin, International Library of Systems Theory and Philosophy (1973) 
4. Chang, A.Y., Biehl, M., Yu, Y., Kanai, R.: Information closure theory of conscious-
ness. Front. Psychol. 11, 1504 (2020)

Architecture for a Self-writing Process Algebra of Learning
107
5. Cody, T.: Mesarovician abstract learning systems. In: Goertzel, B., Iklé, M., 
Potapov, A. (eds.) AGI 2021. LNCS (LNAI), vol. 13154, pp. 55-64. Springer, Cham 
(2022). https://doi.org/10.1007/978-3-030-93758-4_7 
6. Cody, T.: Homomorphisms between transfer, multi-task, and meta-learning sys-
tems. In: International Conference on Artiﬁcial General Intelligence, pp. 199-208. 
Springer (2022). https://doi.org/10.1007/978-3-031-19907-3_19 
7. Cody, T., Beling, P.A.: A systems theory of transfer learning. IEEE Syst. J. 17(1), 
26-37 (2023) 
8. Cody, T., Beling, P.A.: Towards a process algebra and operator theory for learning 
system objects. In: International Conference on Artiﬁcial General Intelligence, pp. 
43-52. Springer (2024). https://doi.org/10.1007/978-3-031-65572-2_5 
9. Cody, T., Shadab, N., Salado, A., Beling, P.: Core and periphery as closed-system 
precepts for engineering general intelligence. In: International Conference on Arti-
ﬁcial General Intelligence, pp. 209-219. Springer (2022). https://doi.org/10.1007/ 
978-3-031-19907-3_20 
10. De Nicola, R.: Process Algebras, pp. 1624-1636. Springer US, Boston, MA (2011) 
11. Gilmore, S., Hillston, J.: The PEPA workbench: a tool to support a process algebra-
based approach to performance modelling. In: Haring, G., Kotsis, G. (eds.) TOOLS 
1994. LNCS, vol. 794, pp. 353-368. Springer, Heidelberg (1994). https://doi.org/ 
10.1007/3-540-58021-2_20 
12. Goertzel, B.: Reﬂective metagraph rewriting as a foundation for an AGI "language 
of thought. arXiv preprint arXiv:2112.08272 (2021) 
13. Goertzel, B.: Bridging AGI theory and practice with Galois connections. In: 
International Conference on Artiﬁcial General Intelligence. pp. 115-125. Springer 
(2023). https://doi.org/10.1007/978-3-031-33469-6_12 
14. Meredith, L.G., Radestock, M.: A reﬂective higher-order calculus. Electronic Notes 
in Theoretical Computer Science 141(5), 49-67 (2005) 
15. Meredith, L.G., Goertzel, B., Warrell, J., Vandervorst, A.: Meta-MeTTa: an oper-
ational semantics for MeTTa. arXiv preprint arXiv:2305.17218 (2023) 
16. Mesarovic, M.D., Takahara, Y.: Abstract systems theory, vol. 116. Springer (1989) 
17. Nivel, E., et al.: Autocatalytic endogenous reﬂective architecture. Technical RUTR-
SCS13002 (2013) 
18. Paszke, A.: PyTorch: An imperative style, high-performance deep learning library. 
arXiv preprint arXiv:1912.01703 (2019) 
19. Pedregosa, F., et al.: Scikit-learn: machine learning in Python. J. Mach. Learn. 
Res. 12, 2825-2830 (2011) 
20. Shadab, N., Cody, T., Salado, A., Beling, P.A.: A systems-theoretical formalization 
of closed systems. IEEE Open J. Syst. Eng. (2024) 
21. Thórisson, K.R.: A new constructivist AI: from manual methods to self-
constructive systems. In: Theoretical Foundations of Artiﬁcial General Intelligence, 
pp. 145-171. Springer (2012). https://doi.org/10.2991/978-94-91216-62-6_9 
22. Thórisson, K.R.: Seed-programmed autonomous general learning. In: International 
Workshop on Self-Supervised Learning, pp. 32-61. PMLR (2020) 
23. Vapnik, V.: The nature of statistical learning theory. Springer Science & Business 
Media (1999) 
24. Wach, P., Zeigler, B.P., Salado, A.: Conjoining Wymore's systems theoretic frame-
work and the DEVS modeling formalism: toward scientiﬁc foundations for MBSE. 
Appl. Sci. 11(11), 4936 (2021) 
25. Weinbaum, D., Veitas, V.: Open ended intelligence: the individuation of intelligent 
agents. J. Exper. Theoret. Artiﬁ. Intel. 29(2), 371-396 (2017)

108
T. Cody
26. Wymore, A.W.: A mathematical theory of systems engineering: the elements. Wiley 
(1967) 
27. Wymore, A.: Systems engineering methodology for interdisciplinary teams. Wiley 
Interscience Series in Discrete Mathematics, Wiley (1976)

Linguistic Loops and Geometric Invariants 
as a Way to Pre-verbal Thought? 
Daniele Corradetti1,2,3(B) and Alessio Marrani1,4,5 
1 Elementar, Divisione Ricerca e Sviluppo, 10121 Turin, Italy 
2 Departamento de Matematica, Universidade do Algarve, Campus de Gambelas, 
8005-139 Faro, Portugal 
3 Grupo de Física Matemática, Instituto Superior Técnico, Av. Rovisco Pais, 
1049-001 Lisbon, Portugal 
a55944@ualg.pt 
4 Dipartimento di Management 'Valter Cantino', Università degli Studi di Torino, 
Corso Unione Sovietica 218 bis, 10134 Turin, Italy 
5 Department of Physics, Astronomy and Mathematics, University of Hertfordshire, 
Hatﬁeld, Hertfordshire AL10 9AB, UK 
Abstract. We introduce the concepts of linguistic transformation, lin-
guistic loop and semantic deﬁcit. By exploiting Lie group theoretical 
and geometric techniques, we deﬁne invariants that capture the struc-
tural properties of a whole linguistic loop. This result paves the way 
to a totally new line of research, employing tools from Lie theory and 
higher-dimensional geometry within language studies. But, even more 
intriguingly, our study hints to a mathematical characterization of the 
meta-linguistic or pre-verbal thought, namely of those cognitive struc-
tures that precede the language. 
1
Introduction 
When Raymond Queneau published his "Exercices de style" in 1947, inspired by 
Bach's "Art of Fugue", he had the intuition to be doing something intrinsically 
geometric (so much, that he named the ﬁrst version of his work "Dodé caèdre"). 
This intuition, far from being casual, may be regarded as the ﬁrst example of 
what in this study we deﬁne as linguistic transformation. Similarly to a geo-
metric transformation, which transforms an object while maintaining some of 
its invariant algebraic and geometric properties, a linguistic transformation is, 
in essence, a map (usually expressed through a proposition in the natural lan-
guage) that, when applied to an element of a linguistic space, transforms it into 
another, while preserving in a controlled way the original semantic core, through 
criteria of coherence and reversibility. Therefore, linguistic transformations are 
transformations of propositions or text elements characterized by a limited gen-
erative content, since they mainly act on the form or signiﬁer of the sentence, 
with a controlled (and generally limited) impact on its meaning. Moreover, the 
aforementioned requirement of coherence and reversibility of such transforma-
tions yields to the introduction of the concept of linguistic loop, associated to 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 109-118, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_11

110
D. Corradetti and A. Marrani
geometric invariants which can be explicitly (and eﬀectively) calculated. As such, 
the linguistic loop displays strong similarities to the Wilson loop, introduced by 
the physicist K. G. Wilson in 1974 in his lattice computations in Quantum 
Field Theory [ 19], and subsequently vastly employed in Diﬀerential Geometry, 
Mathematical Physics [ 14], and also in lattice formulations of nonperturbative 
Quantum Gravity [ 1, 6, 11]. 
From a theoretical point of view, the use of concepts and tools from Lie 
theory and higher-dimensional geometry, usually employed in the formulation 
of (classical and quantum) theories of gravity (even beyond Einstein's General 
Relativity), represents an interesting and new perspective within the study of lin-
guistic spaces. More than this, tantalizing hints arise to an even more intriguing 
development, namely to provide a formal, rigorous starting point for the study 
of cognitive structures pertaining to the pre-verbal thought, which transcends 
any formulation through language. Indeed, given a loop of linguistic transforma-
tions that change completely the form of a sentence while leaving its meaning 
unaltered, one might ask whether it is possible to characterize the semantic core 
of the sentence by means of some invariant structure(s), preserved by the loop 
itself. 
While it is reasonable to state that the existence of two systems of reasoning, 
namely system 1 ('fast') and system 2 ('slow'), is gaining a quite broad recog-
nition within the scientiﬁc community (see e.g. [ 7]), we cannot help but observe 
that the same community is essentially unaware of another equally, if not more, 
important type of thought system, namely the meta-linguistic, or (as we prefer 
to name it) pre-verbal, thought. 
For our purposes, pre-verbal thought refers to mental representations and 
cognitive structures that exist before being expressed in words. A careful anal-
ysis of the mental process giving rise, for example, to the formulation of an 
elaborate speech, immediately yields one to realize that the speech, before being 
lexically structured, consists of a pre-verbal intention that only eventually is 
clothed in a speciﬁc language (according to the grammar and the vocabulary 
learnt by the speaker). In this way, a polyglot can choose whether to formulate 
his idea in an Italian, Portuguese, English or French without virtually altering 
the content and the structure of the idea he wants to convey. If rational thought 
takes place on the linguistic plane in a way made of logical deductions and syllo-
gisms, meta-rational or abstract or pre-verbal thought takes place independently 
of any speciﬁc language. 
From an experimental point of view, cognitive science and developmental 
psychology provide clear evidence that children and even animals form rich con-
cepts without language [ 8, 9, 13, 15, 18]. This phenomenon ﬁnds its conﬁrmation 
in neuroscience applied to the brain-computer interfacer, when it uses seman-
tic vector spaces as intermediaries by decoding brain activity into text. In the 
research conducted in [ 5, 17], for example, cerebral magnetic resonance models 
(non-verbal signals) are mapped into a semantic integration that is then trans-
lated by a linguistic model into sentences that describe the person's thoughts. 
These 'proto-concepts', identiﬁed in neuroscience as a pattern of neural activation

Linguistic Loops for Pre-verbal Thought
111
indicating a 'face' or a 'hand' before any language is employed, can be modeled 
in AI by nodes or embeddings that respond to certain abstract categories. These 
proto-concepts have a clear pre-verbal nature, and they an support rudimentary 
thought and even non-verbal communication (e.g., emotional expressions). 
At this point, one may wonder whether it makes sense to talk about pre-
verbal thought in the context of linguistic spaces such as those of a Large Lan-
guage Model (LLM). Is it possible to identify concepts or invariant structures 
that are meta-linguistic, and which therefore pertain to the pre-verbal thought, 
within a space deﬁned in terms pure linguistic elements? In this article, we 
suggest that this is indeed possible, by deﬁning suitable invariants of linguistic 
spaces. 
It should be remarked that our use of algebraic-geometric invariants in lin-
guistic spaces is distinctly diﬀerent from the one made within usual studies 
of diﬀerential geometry on higher-dimensional varieties originated from LLMs. 
Usually, these studies apply diﬀerential geometry techniques to the embed-
ding space, and they consider some classes of geometric transformations of the 
vectors thereof. However, geometric transformations in the linguistic space do 
not necessarily correspond to well-deﬁned linguistic transformations which pre-
serve (or alter in a controlled way) the semantic core of the linguistic element 
they act upon. In other words, purely geometric transformations acting onto 
embedding vectors are not an eﬃcient and purposeful tool in the identiﬁca-
tion/determination of invariants allowing to identify the pre-verbal meaning of 
a sentence. 
Instead, for a given sentence and a given sequence of re-formulations of this 
sentence through the application of a certain set of linguistic transformations 
(such as translations, paraphrases, negations, etc.), we are going to identify some 
invariants, which thus capture intrinsic properties, transcending the applied lin-
guistic transformations. 
More concretely, it is here worth recalling that in a LLM an embedding system 
consists of an application from a set of linguistic elements or dictionary to a 
vector space [ 10, 12]. Therefore, through the embedding map one can switch 
from a discrete space, i.e., the dictionary, to a continuous space. It is important 
to note that the embedding map is generally not surjective, so an element of the 
continuous embedding space is not always uniquely translatable into one or more 
propositions. This aspect is crucial, as our hypothesis is that the study of speciﬁc 
linguistic loops (such as, for example, multilingual translation) can not only 
allow the identiﬁcation of geometric invariants (as we present in this work), but 
will also allow the identiﬁcation of an element in the embedding space that, even 
though it cannot be formulated linguistically, can 'generate' the whole loop itself 
(this will be the aim of our future research). 
2
Linguistic Transformations 
A metrizable linguistic space \left( \mathscr{A},\psi ,d\right) (A , ψ, d) is a linguistic space \mathscr{A}A , endowed with an 
embedding map \psi ψ from \mathscr{A}A to \mathbb{R}^{n}Rn, along with a metric \mu μ and a distance dd, both

112
D. Corradetti and A. Marrani
deﬁned on \mathbb{R}^{n}Rn. The linguistic space \mathscr{A}A is a place where natural language lives, 
and it is constituted by a set of linguistic elements (words, phrases, proposi-
tions, etc.) in a given context of interest; for example, \mathscr{A}_{LLM_{1}}ALLM1 can indicate the 
linguistic space containing all the propositions that can be generated by a cer-
tain model LLM_{1}LLM1, while the linguistic space \mathscr{A}_{\text{Eng} } AEng can indicate the set of all 
linguistic elements of the English language. On the other hand, the embedding 
map \psi ψ is a mathematical function that maps elements of the linguistic space \mathscr{A}A
(i.e., elements of natural language such as words, phrases, or entire propositions) 
from a discrete space (such as a dictionary) to a continuous vector space, such 
as \mathbb{R}^{n}Rn : 
\bALT{} \psi :\left{ \begin{array}{l} \mathscr{A}\rightarrow \mathbb{R}^{n}; \\ \lambda \mapsto \psi \left( \lambda \right) . \end{array} \right.\eALT{} ψ :
A →Rn;
λ →ψ (λ) .
(2.1) 
Note that, generally, \psi ψ is not injective nor surjective. 
In this framework, by picking a distance d_{\ast }d∗in \mathbb{R}^{n}Rn, one can deﬁne a semantic 
distance dd in the linguistic space \mathscr{A}A , simply by composing  \psi ψ and d_{\ast }d∗itself : 
\bALT{} d\left( \lambda ,\nu \right) :=d_{\ast }\left( \psi \left( \lambda \right) ,\psi \left( \nu \right) \right) ,\eALT{} d (λ, ν) := d∗(ψ (λ) , ψ (ν)) ,
(2.2) 
for any \lambda ,\nu λ, ν in \mathscr{A}A . For instance, a typical semantic distance in \mathscr{A}A is the one 
induced by the so-called cosine distance in \mathbb{R}^{n}Rn, 
\bALT{} d\left( \lambda ,\nu \right) :=d_{\ast \text{,cosine}}\left( \psi \left(\lambda \right) ,\psi \left( \nu \right) \right) =1-\frac{\psi \left( \lambda \right) \cdot \psi \left( \nu \right) }{\Vert \psi \left( \nu\right) \Vert \Vert \psi \left( \nu \right) \Vert }.\label{dist}\eALT{} d (λ, ν) := d∗,cosine (ψ (λ) , ψ (ν)) = 1 −
ψ (λ) · ψ (ν)
∥ψ (ν) ∥∥ψ (ν) ∥.
(2.3) 
Let us now introduce the concept of linguistic transformation. A linguistic 
transformation, denoted by UU, is a transformation of the signiﬁer of an element 
of \mathscr{A}A that preserves in some way the semantic core of the element itself. As 
such, we will see below that a linguistic transformation is the building block 
of a linguistic loop, and it will therefore be instrumental in the deﬁnition of 
the semantic deﬁcit of the loop itself. For all this to be possible, we require a 
linguistic transformation to enjoy the following features : 
- Closure : the image of a linguistic transformation lies in \mathscr{A}A : 
\bALT{} U\left( \lambda \right) \in \mathscr{A},~\forall \lambda \in \mathscr{A},\eALT{} U (λ) ∈A , ∀λ ∈A ,
(2.4) 
- Reversibility : the generative character of a linguistic transformation is con-
trolled, in such a way that the content and the meaning of the original element 
of \mathscr{A}A is traceable in some way. More precisely, denoting by \mathbb{U}\left( \mathscr{A}\right) U (A ) the set of 
all linguistic transformations acting on \mathscr{A}A (endowed 1 with the map composi-
tion \circ ◦), for any U\in \mathbb{U} \left( \mathscr{A}\right) U ∈U (A ) there exists an 'inverse' linguistic transformation 
(denoted, with a certain abuse of language, U^{-1}\in\mathbb{U}\left( \mathscr{A}\right) U −1 ∈U (A )), for which the 
semantic distance between the original linguistic element (say, \lambda λ) and  the  
correspondingly saturated element U^{-1}\left( U\left( \lambda \right)\right) U −1 (U (λ)) is less than a certain deﬁned 
threshold (or, in a physicist's jargon, 'ultraviolet cutoﬀ ') \varepsilon \in \mathbb{R}^{+}ε ∈R+ : 
\bALT{} U\in \mathbb{U}\left( \mathscr{A}\right) ~\text{is~\emph{reversible}}\overset{\text{def.}}{\Leftrightarrow }~\exists U^{-1}:d\left( U^{-1}\left( U\left(\lambda \right) \right) ,\lambda \right) <\varepsilon ,~\forall \lambda \in \mathscr{A}.\label{reversible}\eALT{} U ∈U (A ) is reversible
def.
⇔∃U −1 : d

U −1 (U (λ)) , λ

< ε, ∀λ ∈A . (2.5)
1 \mathbb{U}\left( \mathscr{A}\right) U (A ) is an example of unital magma (see e.g. [ 3]). 

Linguistic Loops for Pre-verbal Thought
113
This request is necessary to allow for the possibility to return, after the com-
position of a certain number of linguistic transformations, to the original 
element, or to another element similar to it (see next condition); as we will 
see below, the reversibility of the linguistic transformations ultimately allows 
the linguistic loops to exist. Note that generally U^{-1}U −1 is not uniquely deﬁned, 
and it also depends on the choice of \varepsilon ε. In this framework, the concept of 
'reversible' transformation is an approximate (non-unique, and \varepsilon ε -dependent) 
version of the concept of 'inverse'. 
- Coherence: the application the same transformation to similar elements of \mathscr{A}A
yields to similar image elements. More precisely, by deﬁning as reciprocally 
similar any two elements \lambda ,\nu \in λ, ν ∈\mathscr{A}A such that, for a certain ﬁxed \chi \in \mathbb{R}^{+}χ ∈R+, 
d\left( \lambda ,\nu \right) <\chi d (λ, ν) < χ, the condition for a certain transformation UU to be coherent is 
the following one : for any element \lambda \in \mathscr{A}λ ∈A , one can deﬁne a (UU-dependent) 
similar element U^{-1}\left( U\left(\lambda \right) \right) U −1 (U (λ)) (which always exists if UU is reversible; in this 
case, \chi =\varepsilon χ = ε), and then compute the semantic distance of the images of such 
two similar elements under UU itself. If UU is coherent, such a distance will be 
less than a certain function 2 f_{U}\left( \varepsilon \right) fU (ε) : 
\bALT{} U\in \mathbb{U}\left( \mathscr{A}\right) ~\text{is~\emph{coherent}}~\overset{\text{def.}}{\Leftrightarrow }d\left( U^{-1}\left( U\left( \lambda \right)\right) ,\lambda \right) <f_{U}(\varepsilon ),~\forall \lambda \in \mathscr{A}.\label{coherent}\eALT{} U ∈U (A ) is coherent
def.
⇔d

U −1 (U (λ)) , λ

< fU(ε), ∀λ ∈A .
(2.6) 
Additionally (but not necessarily), one may require a linguistic transforma-
tion to be identiﬁable (or natural), namely to be identiﬁed by a linguistic element 
of the same space on which it acts (i.e., of \mathscr{A}A ); for instance, U_{\zeta }Uζ will indicate the 
natural linguistic transformation U\in \mathbb{U}\left( \mathscr{A}\right) U ∈U (A ), identiﬁed by the natural phrase or 
linguistic element \zeta \in \mathscr{A} ζ ∈A . 
To be more concrete, let us now proceed to identify some notable exam-
ples of linguistic transformations: multilingual transformations that convert a 
text from one language to another (e.g., "translate from English to Italian"), are 
linguistic transformations if applied to an adequate linguistic space (domain); 
conversion of propositions in negative form, as well as conversion of proposi-
tions in interrogative form are linguistic transformations, as well. In general, all 
linguistic processes with a limited generative content (and mainly focused on the 
linguistic form) are linguistic transformations; for example: dialectal adaptation, 
colloquial or formal; poetic transposition or adaptation in a poetic style; techni-
cal transposition or conversion of common language into technical or specialized 
language/jargon; expansion of a text and its synthesis; paraphrasing, namely 
reformulating the same content with synonyms or similar linguistic structures 
preserving the same meaning; emotional ampliﬁcation through some speciﬁc 
emotional ﬁlter; etc. Conversely, all those processes that have a too ample gen-
erative component which prevents one from reconstructing the original semantic 
content, for example "invent a story", are  not linguistic transformations.
2 The subscript " UU" denotes the fact that ff a priori depends on UU. 

114
D. Corradetti and A. Marrani
3
Linguistic Loops and Semantic Deﬁcits 
Given a metrizable linguistic space \left( \mathscr{A},\psi ,d\right) (A , ψ, d), a  sequence \mathcal{U}U of length L+1L + 1 is 
the ordered set of L+1L+1 linguistic transformations (always including the identity 
map \mathbb{I}I as the ﬁrst element), denoted as 
\bALT{} \mathcal{U}:=\left{ \mathbb{I},U_{1},...,U_{L}\right} .\label{Uspscall}\eALT{} U := {I, U1, ..., UL} .
(3.1) 
Starting from an initial linguistic element \lambda \in \mathscr{A}λ ∈A and applying the iterated 
composition of such transformations to \lambda λ, one obtains a sequence of length L+1L+1
in the linguistic space \mathscr{A}A , 
\bALT{} \mathcal{U}\left( \lambda \right) :=\left{ \lambda ,\;U_{1}\left( \lambda \right) ,\,\left( U_{2}\circ U_{1}\right) \left( \lambda \right) ,...,\left(U_{L}\circ U_{L-1}\circ \dots \circ U_{2}\circ U_{1}\right) \left( \lambda\right) \right} .\label{lingspsseq}\eALT{} U (λ) := {λ, U1 (λ) , (U2 ◦U1) (λ) , ..., (UL ◦UL−1 ◦· · · ◦U2 ◦U1) (λ)} .
(3.2) 
Clearly, this sequence always contains the element \lambda λ. By further applying 
the embedding map \psi ψ to each element of \mathcal{U}\left(\lambda \right) U (λ), one obtains the corresponding 
sequence \psi\left( \mathcal{U}\left( \lambda \right) \right) ψ (U (λ)) of length L+1L + 1 in \mathbb{R}^{n}Rn : 
\bALT{} \psi \left( \mathcal{U}\left( \lambda \right) \right) :=\left{ v,v_{1},v_{2},...,v_{L-1},v_{L}\right} ,\eALT{} ψ (U (λ)) := {v, v1, v2, ..., vL−1, vL} ,
(3.3) 
where 
\TagImg\altimg{659682_1_En_11_Equ10_HTML}{}\pagination{\bALT{} v &:&=\psi \left( \lambda \right) , \notag \\ v_{1} &:&=\psi \left( U_{1}\left( \lambda \right) \right) , \notag \\ v_{2} &:&=\psi \left( \left( U_{2}\circ U_{1}\right) \left( \lambda \right) \right) , \notag \\ &&... \notag \\ v_{L} &:&=\psi \left( \left( U_{L}\circ U_{L-1}\circ ...\circ U_{2}\circ U_{1}\right) \left( \lambda \right) \right) .\label{0spsnspsloop}\eALT{}} v : = ψ (λ) ,
v1 : = ψ (U1 (λ)) ,
v2 : = ψ ((U2 ◦U1) (λ)) ,
...
vL : = ψ ((UL ◦UL−1 ◦... ◦U2 ◦U1) (λ)) .
(3.4) 
The semantic distance between the ﬁrst and the last element of the sequence 
(3.2) in  \mathscr{A}A , namely the quantity 
\bALT{} \delta _{\mathcal{U}}\left( \lambda \right) :=d\left( \lambda ,\left(U_{L}\circ U_{L-1}\circ ...\circ U_{2}\circ U_{1}\right) \left( \lambda\right) \right) =d_{\ast }\left( v,v_{L}\right)\label{semspsdef}\eALT{} δU (λ) := d (λ, (UL ◦UL−1 ◦... ◦U2 ◦U1) (λ)) = d∗(v, vL)
(3.5) 
is named semantic deﬁcit of the element \lambda \in \mathscr{A}λ ∈A under the sequence \mathcal{U}U deﬁned 
by (3.1). If, for a given, ﬁxed threshold (or, again, "ultraviolet cutoﬀ" ) \xi >0ξ > 0, 
\bALT{} \delta _{\mathcal{U}}\left( \lambda \right) <\xi ,~\forall \lambda \in \mathscr{A},\label{loop}\eALT{} δU (λ) < ξ, ∀λ ∈A ,
(3.6) 
then \mathcal{U}U is deﬁned as a linguistic loop (of length L+1L + 1). In practice, a smaller 
semantic deﬁcit indicates a greater preservation of the (meaning of the) original 
(i.e., starting) linguistic element, while higher deﬁcits (but still under the thresh-
old \xi ξ) signal an accumulated distortion (of the meaning of the original linguistic 
element) within the linguistic loop under consideration. 
We will employ the notions of semantic deﬁcit and linguistic loop linguistic in 
order to study in a quantitative way the stability and the reliability of the maps 
of the linguistic transformations. This will work in a intriguingly analogous way 
to what happens for the iteration of maps in dynamic systems, in which small 
iterative errors/deﬁcits can lead to signiﬁcant twists or deviations from the initial 
path.

Linguistic Loops for Pre-verbal Thought
115
4
From Linguistic Loops to Quadratic Forms, and Their 
Signatures 
The notion of semantic deﬁcit, and its evaluation on elements of the linguistic 
space \mathscr{A}A for various linguistic loops deﬁned in the magma \mathbb{U}\left( \mathscr{A}\right) U (A ) of all linguistic 
transformations, is interesting per se. However, the purpose of this work is to 
determine invariant structures which enjoy some degree of stability under the 
action of \mathbb{U}\left( \mathscr{A}\right) U (A ). As mentioned above, it is our hypothesis that these structures 
may pertain to an immutable core of pre-verbal thought or meaning. 
Since the inhomogeneous Lie group IO\left( n\right) :=(n) :=O\left( n\right)\ltimes T^{n}(n)⋉T n has a transitive action 
on\mathbb{R}^{n}Rn itself, the sequence\psi \left( \mathcal{U}\left( \lambda \right) \right) ψ (U (λ)) (3.4) ofL+1L+1 vectors in\mathbb{R}^{n}Rn can be rewritten as 
\TagImg\altimg{659682_1_En_11_Equ13_HTML}{}\pagination{\bALT{} v &:&=\psi \left( \lambda \right) , \notag \\ v_{1} &=&\frac{\left\Vert v_{1}\right\Vert }{\left\Vert v\right\Vert } \mathcal{R}_{1}v, \notag \\ v_{2} &=&\frac{\left\Vert v_{2}\right\Vert }{\left\Vert v_{1}\right\Vert } \mathcal{R}_{2}v_{1}=\frac{\left\Vert v_{2}\right\Vert }{\left\Vert v\right\Vert }\mathcal{R}_{2}\mathcal{R}_{1}v, \notag \\ &&... \notag \\ v_{L} &=&\frac{\left\Vert v_{L}\right\Vert }{\left\Vert v_{L-1}\right\Vert } \mathcal{R}_{L}v_{L-1}=\frac{\left\Vert v_{L}\right\Vert }{\left\Vert v\right\Vert }\mathcal{R}_{L}\mathcal{R}_{L-1}...\mathcal{R}_{2}\mathcal{R} _{1}v,\label{nspsloop}\eALT{}} v : = ψ (λ) ,
v1 = ∥v1∥
∥v∥R1v,
v2 = ∥v2∥
∥v1∥R2v1 = ∥v2∥
∥v∥R2R1v,
...
vL =
∥vL∥
∥vL−1∥RLvL−1 = ∥vL∥
∥v∥RLRL−1...R2R1v,
(4.1) 
where \left\Vert \cdot \right\Vert ∥·∥denotes the Euclidean norm in \mathbb{R}^{n}Rn, and  \mathcal{R}_{i}Ri (i=1,...,Li = 1, ..., L) is an  
element of O\left(n\right) (n). For  n\geqslant 3n ⩾3, the rotation matrix \mathcal{R}_{i}Ri is not unique, and so is the 
rewriting (4.1) of (3.4); however, for any given pair of vectors xx and yy in \mathbb{R}^{n}Rn, 
there exists a unique deﬁnition of 'minimal' rotation matrix \mathcal{\tilde{R}}^{x,y} ˜Rx,y connecting yy
to xx, namely  
\bALT{} y=:\frac{\left\Vert y\right\Vert }{\left\Vert x\right\Vert }\mathcal{\tilde{R}}^{x,y}x,\eALT{} y =: ∥y∥
∥x∥
˜Rx,yx,
(4.2) 
where 
\bALT{} \mathcal{\tilde{R}}^{x,y}:=I_{N}+\Bigl(\hat{y}\,\hat{x}^{T}-\hat{x}\,\hat{y}^{T}\Bigr)+\frac{1}{1+\hat{x}^{T}\hat{y}}\Bigl(\hat{y}\,\hat{x}^{T}-\hat{x}\,\hat{y}^{T}\Bigr)^{2}\in \text{O}\left( n\right) ,\label{Rspsmin}\eALT{} ˜Rx,y := IN +

ˆy ˆxT −ˆx ˆyT 
+
1
1 + ˆxT ˆy

ˆy ˆxT −ˆx ˆyT 2
∈O (n) ,
(4.3) 
with \hat{x}:=x/\Vert x\Vert ˆx := x/∥x∥and ~\hat{y}:=y/\Vert y\Vert ˆy := y/∥y∥denoting the normalized vectors (versors) 
associated to xx resp. yy, and  with I_{n}In denoting the identity matrix in nn dimensions. 
The 'minimality' of the rotation matrix \mathcal{\tilde{R}}^{x,y} ˜Rx,y deﬁned by (4.3) amounts to the 
fact that it deﬁnes a rotation on the plane deﬁned by \hat{x}ˆx and ~\hat{y} ˆy only. Thus, by 
setting \mathcal{R}_{i}= \mathcal{\tilde{R}}^{v_{i-1},v_{i}}Ri = ˜Rvi−1,vi, the last vector of the sequence (4.1) can be recast into 
following form (v^{0}\equiv vv0 ≡v): 
\bALT{} v_{L}=\frac{\left\Vert v_{L}\right\Vert }{\left\Vert v\right\Vert }\mathcal{\tilde{R}}^{v_{L-1},v_{L}}\mathcal{\tilde{R}}^{v_{L-2},v_{L-1}}...\mathcal{\tilde{R}}^{v,v_{1}}v=:\frac{\left\Vert v_{L}\right\Vert }{\left\Vert v\right\Vert }\mathrm{R}_{\mathcal{U}}^{(\lambda )}v,\label{thisss}\eALT{} vL = ∥vL∥
∥v∥
˜RvL−1,vL ˜RvL−2,vL−1... ˜Rv,v1v =: ∥vL∥
∥v∥R(λ)
U v,
(4.4) 
where \mathrm{R}_{\mathcal{U}}^{(\lambda )}v\in R(λ)
U v ∈(S)O\left( n\right) (n) denotes the 'minimal' rotation matrix associated to the 
linguistic element \lambda \in \mathscr{A}λ ∈A and to the sequence \mathcal{U}U.

116
D. Corradetti and A. Marrani
By choosing d_{\ast }=d_{\ast ,\text{cosine}}d∗= d∗,cosine deﬁned by (2.3) as the distance in \mathbb{R}^{n}Rn, a little alge-
bra allows to rewrite the semantic deﬁcit \delta _{\mathcal{U}}\left( \lambda \right) δU (λ) (3.5) associated to the linguistic 
element \lambda \in \mathscr{A}λ ∈A under the action of the sequence \mathcal{U}U of linguistic transformations 
as a quadratic form, denoted by Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) QU (ˆv, ˆv) : 
\bALT{} \delta _{\mathcal{U}}\left( \lambda \right) &=&Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) ;\label{this} \\ Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) &:&=\hat{v}^{T}\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }\right) \hat{v},\eALT{} δU (λ) = QU (ˆv, ˆv) ;
(4.5) 
QU (ˆv, ˆv) : =  ˆvT 
In − R(λ)∗ 
U

ˆv,
(4.6) 
where \mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }R(λ)∗
U
is the symmetric part of \mathrm{R}_{\mathcal{U}}^{(\lambda )}R(λ)
U
deﬁned by (4.4) :  
\bALT{} \mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }:=\frac{1}{2}\left( \mathrm{R}_{\mathcal{U}}^{(\lambda )}+\mathrm{R}_{\mathcal{U}}^{(\lambda )T}\right) ,\eALT{} R(λ)∗
U
:= 1
2

R(λ)
U
+ R(λ)T
U

,
(4.7) 
and the real symmetric (and not necessarily maximal-rank) matrix I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }In −R(λ)∗
U
is 
referred to as the representing matrix of Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) QU (ˆv, ˆv) itself. 
The theory of algebraic-geometric invariants of quadratic forms over ﬁelds 
with characteristic \neq 2̸= 2 is a well developed ﬁeld of Mathematics [ 2, 4, 16]; 
Sylvester's law of inertia guarantees that the signature sign\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast}\right) 

In −R(λ)∗
U

of 
the matrix I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }In −R(λ)∗
U
(and thus of the quadratic form Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) QU (ˆv, ˆv)) is GL(n,\mathbb{R})(n, R)-
invariant. By virtue of the spectral theorem, I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }In −R(λ)∗
U
can be orthogonally diag-
onalized : at least one (generally, \mathcal{U}U- and  \lambda λ-dependent) matrix \mathcal{S}_{\mathcal{U}}^{(\lambda )}\in S(λ)
U
∈(S)O(n)(n)
exists such that 
\bALT{} Q_{\mathcal{U}}\left( \hat{v},\hat{v}\right) =\sum_{a=1}^{n}\rho _{\mathcal{U},a}^{(\lambda )}\left( \left( \mathcal{S}_{\mathcal{U}}^{(\lambda )}\hat{v}\right) ^{a}\right) ^{2}, \notag\eALT{} QU (ˆv, ˆv) =
n

a=1
ρ(λ)
U,a

S(λ)
U ˆv
a2
,
where \rho _{U,a}^{(\lambda )}ρ(λ)
U,a's denote the real eigenvalues of I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }In −R(λ)∗
U
. Then, sign\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }\right) 

In −R(λ)∗
U

is simply provided by the sequence of the \rho _{U,a}^{(\lambda )}ρ(λ)
U,a's, in which the non-vanishing 
eigenvalues are normalized (but preserve their sign). 
The GL(n,\mathbb{R})(n, R)-invariant quantity sign\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }\right) 

In −R(λ)∗
U

is particularly relevant 
when the sequence \mathcal{U}U is a linguistic loop : on the one hand, the semantic deﬁcit 
\delta _{\mathcal{U}}\left( \lambda \right) δU (λ) measures the overall 'semantic distortion' between the extremal elements 
vv and v_{L}vL of \psi \left( \mathcal{U}\left( \lambda \right) \right) ψ (U (λ)); on the other hand, the signature sign\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda )\ast }\right) 

In −R(λ)∗
U

actually 
probes ﬁner structural properties of the loop \mathcal{U}U, by the very deﬁnition (4.4) of  
the rotation matrix \mathrm{R}_{\mathcal{U}}^{(\lambda )}R(λ)
U . Thus, upon ﬁxing \mathcal{U}U itself and exploiting the invariant 
sign\left( I_{n}-\mathrm{R}_{\mathcal{U}}^{(\lambda)\ast }\right) 

In −R(λ)∗
U

, one can classify in a GL(n,\mathbb{R})(n, R) -invariant way the elements of 
the image \psi \left( \mathscr{A}\right) \subsetneq \mathbb{R}^{n}ψ (A ) ⊊Rn of the linguistic space \mathscr{A}A under the embedding map \psi ψ. 
5
Conclusions and Future Developments 
In this article, we introduced the concept of linguistic loop in the set of lin-
guistic transformations of a metrizable linguistic space, and associated it to 
an algebraic-geometric invariant in the embedding space: namely, the signature

Linguistic Loops for Pre-verbal Thought
117
of the quadratic form expressing the semantic deﬁcit of the loop itself. This 
result opens up a totally new line of research, employing tools from Lie theory 
and higher-dimensional geometry in the context of linguistic studies. Our study 
also introduces a theoretical framework of investigation of a type of thought 
at the foundation of many human and animal cognitive processes, but largely 
overlooked by contemporary literature: namely, the pre-verbal thought. In our 
humble opinion, the study of this type of thought should necessarily be under-
taken in order to achieve artiﬁcial intelligences which may transcend the usual 
linguistic manipulation capabilities, and which may also generalize concepts at 
a meta-linguistic level. 
The signature associated to the semantic deﬁcit of a linguistic loop reveal 
a ﬁner, invariant structure associated, for instance, to two sentences (namely, 
the ﬁrst and the last of the loop) that, despite signiﬁcant structural diﬀerences, 
carry virtually identical meaning, as in the case of translations of the same sen-
tence through a chain of diﬀerent languages. In this framework, we put forward 
the conjecture that the aforementioned signature captures information on the 
semantic core of the original (i.e., ﬁrst) sentence, whose modiﬁcation is con-
trolled and limited throughout the entire chain of linguistic transformations; as 
such, the signature should convey information closely related to the intrinsic, 
meta-linguistic meaning of the original sentence itself. 
Thus, through the deﬁnition of geometric-linguistic invariants and the con-
cept of semantic deﬁcit associated to each linguistic loop, one could identify 
stable conceptual structures, likely to play as key role in a variety of contexts, 
from cognitive science, to developmental psychology, and neuroscience. Remark-
ably, our approach can be implemented and coded in a completely explicit way, 
for instance in the growing eﬀorts to improve the semantic coherence and robust-
ness of LLM's, especially in contexts that require complex transformations, such 
as multilingual translation or stylistic variation. 
Consequently, it is easy to realize that the invariant analysis of the semantic 
deﬁcit associated to a linguistic loop, performed in terms of algebraic-geometric 
tools pertaining to Lie theory and higher-dimensional geometry, provides a basis 
for developing new metrics aimed at evaluating the robustness of linguistic trans-
formations, as well as at creating latent representations that could constitute 
the 'seeds' of an abstract thought; intriguingly, this might provide a theoretical 
bridge between the non-verbal thought and its linguistic expression. It is also 
here worth pointing out that the use of these concepts in the context of model 
distillation would provide practical tools for the compression and optimization of 
LLM's themselves. On the other hand, we should also remark that our approach, 
due to its theoretical and foundational nature, still requires a large-scale empir-
ical validation, in which its fertility and relevance may be concretely explored 
and investigated. 
All in all, we have presented evidence that the integration of linguistic trans-
formations with invariant analysis from Lie theory and higher-dimensional geom-
etry may potentially bridge the notoriously diﬃcult gap between the pre-verbal 
thought and the linguistic expression. The next steps of our investigation will

118
D. Corradetti and A. Marrani
clearly include the empirical veriﬁcation of these concepts, as well as the explo-
ration of some of their practical implications, with the aim of expanding our 
tools for the understanding of human thought and artiﬁcial intelligence. 
References 
1. Ambjorn, J., Jurkiewicz, J., Loll, R.: Reconstructing the universe. Phys. Rev. D 
72(6), 064014 (2005) 
2. Artin, M.: Algebra (1991) 
3. An Invitation to General Algebra and Universal Constructions. U, Springer, Cham 
(2015). https://doi.org/10.1007/978-3-319-11478-1_10 
4. Do Carmo, M.P.: Riemannian Geometry. Birkhauser (1992) 
5. Fesce, R.: Subjectivity as an emergent property of information processing by neu-
ronal networks. Front. Neurosci. 14, 548071 (2020) 
6. Hamber, H.W.: Quantum Gravitation: The Feynman Path Integral Approach. 
Springer, Berlin Heidelberg (2009) 
7. Kahneman, D.: Thinking. Fast and Slow, Farrar, Straus and Giroux (2011) 
8. Lakusta, L., Spinelli, D., Garcia, K.: The relationship between pre-verbal event rep-
resentations and semantic structures: the case of goal and source paths. Cognition 
164, 174-187 (2017) 
9. Mandler, J.M.: The foundations of mind: Origins of conceptual thought (2004) 
10. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J.: Distributed repre-
sentations of words and phrases and their compositionality. In: Advances in Neural 
Information Processing Systems, pp. 3111-3119 (2013) 
11. Modanese, G.: Wilson loops in four-dimensional quantum gravity. Phys. Rev. D 
47(2), 502-507 (1993) 
12. Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word represen-
tation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pp. 1532-1543. Association for Computational 
Linguistics, Doha, Qatar (2014) 
13. Piaget, J.: The construction of reality in the child. Basic Books (1954) 
14. Rudolph, G., Schmidt, M.: Diﬀerential Geometry and Mathematical Physics: Part 
II. Springer, Theoretical and Mathematical Physics, Fibre Bundles, Topology and 
Gauge Fields (2017) 
15. Spelke, E.S.: Core knowledge. American Psychologist (1998) 
16. Spivak, M.: A comprehensive introduction to diﬀerential geometry, vol. 1). Publish 
or Perish (1999) 
17. Tang, J., et al.: Semantic reconstruction of continuous language from non-invasive 
brain recordings (2023) 
18. Vygotsky, L.S.: Thought and language. MIT Press (1962) 
19. Wilson, K.G.: Conﬁnement of quarks. Phys. Rev. D 10(8), 2445-2459 (1974)

Bad Reasoners, the Turing Trap and 
the Problem of Artiﬁcial Dualism 
Gonçalo Hora de Carvalho1(B)
and Kristinn R. Thórisson1,2 
1 Icelandic Institute for Intelligent Machines, Menntavegur 1, Venus, 2nd ﬂ., 101, 
Reykjavik, Iceland 
goncalo@iiim.is, thorisson@ru.is 
2 Center for Analysis and Design of Intelligent Agents, Reykjavik University, 
Reykjavik, Iceland 
Abstract. If it looks like a duck, swims like a duck, and quacks like 
a duck, then your LLM's priors are likely to predict the next tokens to 
amount to the word "duck" based on its learned data distribution—but 
has it reasoned about its data to deduce the duck? Large language models 
(LLMs) produce remarkably ﬂuent text, enough to result in widespread 
claims of their ability to "understand" and "reason". However, a dissec-
tion of the key architectural features of LLMs, and more generally ANNs, 
in particular their reliance on probabilistic pattern-matching, exposes 
their absence of critical structures analogous to the neuro-biological sub-
strates known to be involved in human reasoning, goal-directed behav-
ior, and cumulative learning. Furthermore, LLMs lack mechanisms to 
perform explicit goal-driven cause-eﬀect guided use of deduction, induc-
tion, abduction and analogy; if a context requires an unseen and unlikely 
output (x^*)(x∗) not supported in the training-data manifold \mathcal{M}M (i.e. out-
side the convex hull of what was seen during training), the model has 
no basis for producing an answer corresponding to the physical world, 
being instead limited to interpolate on \mathcal{M}M, from which next-token pre-
dictions are drawn via weighted sums over attention heads. Our for-
malism suggests that token-level statistical interpolation already suﬃces 
for the observed behavior; explicit internal reasoning modules are there-
fore not required to explain output. Consequently, we argue that claims 
attributing human-like cognition to contemporary LLMs are empirically 
unsupported, confusing surface ﬂuency with cognitive processes in what 
essentially are two levels of the same misattribution: (i) Artiﬁcial Dual-
ism: researchers project hidden reasoning modules into purely statistical 
models; (ii) Turing Trap: observers project agency from ﬂuent dialogue 
alone. 
1
Introduction 
Algorithms for artiﬁcial neural networks (ANNs) have been proposed as early 
as in 1943 by McCullogh and Pitts [ 51], with one of the ﬁrst implemented ANN 
systems being Minsky's SNARC (1952) [ 54]. These ideas have since resulted in 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 119-134, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_12

120
G. H. de Carvalho and K. R. Thórisson
a large number of variants, one of the newer ones being large language mod-
els (LLMs)—large-scale transformer ANNs, built from layers of multi-headed 
self-attention and feed-forward sub-layers, that process text and generate seem-
ingly coherent, contextually appropriate output in an autoregressive fashion [ 10]. 
More recent versions of the algorithm utilize the self-attention mechanism to 
weigh the importance of diﬀerent tokens in a sentence relative to each other 
[ 10, 93]. Input text is tokenized, converted into vectors using embeddings, and 
processed through transformer layers that calculate attention scores to dictate 
focus on relevant tokens [ 10, 26, 93]. The model then selects the next token based 
on learned distributions, iteratively generating a (arbitrarily) long sequence of 
text [ 10, 26, 93]. Whether it is a single enormous model with hundreds of bil-
lions of parameters or a mixture of experts (MoE) where many expert models 
coexist controlled by a task manager model [ 40], these neural networks are capa-
ble of modelling complex linguistic abstractions, capturing patterns in syntax, 
semantics, pragmatics, and even elements of style and tone [ 10, 11, 62]. Although 
some believe the architecture to be deﬁnitive and capable of increasingly com-
plex emergent properties, others believe that despite their size, these models are 
simply parroting training data [ 5, 8, 23, 35, 49, 72, 98-100,107]. 
However, large-scale generative machine learning pipelines have been 
extremely useful in applied domains such as drug discovery, materials science, 
and chemistry in proposing vast libraries of candidate molecules, materials, or 
designs, which are then systematically ﬁltered by external rule-based criteria or 
simulations to ensure viability [ 15, 80]. In pharmaceutical discovery, for exam-
ple, deep generative models can enumerate new compounds, before pruning using 
medicinal chemistry rules and ontologies to eliminate implausible or problem-
atic molecules [ 4, 48]. Similar hybrid strategies appear in materials science, where 
ANNs generate candidate crystal structures or molecules that are subsequently 
screened by physics-based simulations and logic constraints before experimental 
consideration [ 58, 80]. These examples of deterministic ﬁlters and knowledge-
driven checks compensate for the fact that current algorithms lack functional 
reasoning capabilities in the classical sense. 
By integrating external rule sets, ontologies, and physics-based evaluations as 
post-generation ﬁlters, researchers create a feedback loop that enforces domain 
knowledge and logical consistency on candidates to proﬁt from the automation 
capabilities and enormous volumes of output data [ 39, 71]. In essence, these ﬁlters 
perform a kind of surrogate reasoning: they rigorously apply the deductive rules 
of chemistry and physics and the inductive generalizations gleaned by human 
experts, thus guiding the generative model's outputs toward feasible and mean-
ingful solutions that adhere to real-world constraints [ 15, 29]. 
This reliance on external ﬁlters and domain-speciﬁc heuristics underscores a 
fundamental limitation: LLMs, while powerful at pattern classiﬁcation and text 
generation, inherently lack mechanisms to validate truth or to perform explicit 
reasoning processes over causal-chains. There exist formal approaches to causal 
inference, such as Pearl's Do-calculus, that provide a rigorous framework for 
representing cause-eﬀect relations using structural causal models and directed

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
121
acyclic graphs, enabling estimation of intervention eﬀects and counterfactuals 
beyond mere correlation [ 67]. Similarly, in Reinforcement Learning (RL), world-
model-based planning methods learn latent dynamics of the environment and 
perform virtual roll-outs to plan actions, illustrating how explicit predictive or 
causal models can guide decision-making [ 83]. 
Classically, RL as a ﬁeld has focused on solving decision making by explicitly 
modeling sequential decisions involving agents that interact with their environ-
ment while learning optimal policies to achieve reward-encoded goals [ 83]. In 
modernity, the ﬁeld has incorporated deep learning in what is now called Deep 
Reinforcement Learning (DRL) [ 12]. Numerous DRL models have emerged to 
tackle increasingly complex tasks that require not just planning, but learning of 
deeply complex strategies in multi-agent scenarios: Deep Q-Networks (DQNs), 
a classic approach of DRL, has achieved human-level control on Atari games 
[ 55], curiosity-driven intrinsic motivation modules have fostered exploration in 
sparse-reward environments [ 65], league-based multi-agent training has produced 
Grandmaster-level play in StarCraft II with AlphaStar [ 94], self-play without 
domain priors has taken AlphaZero to superhuman performance in chess, shogi, 
and Go [ 79], uniﬁed learning and planning resulted in MuZero's mastery across 
Atari, board, and planning benchmarks [ 74], and ﬁnally, similarly to AlphaStar, 
large-scale self-play systems like OpenAI Five have dominated against the best 
human teams in the world in real-time play in an incredibly complex and high 
dimensional state-space game called Dota 2 [ 60]. 
Despite their impressive successes, these algorithms do not generalize to out-
of-distribution (OOD) data. To do so, they must be retrained on the game they 
are expected to play [ 12]. LLMs have also been benchmarked in non-linguistic 
tasks following the OOD tradition—Liga and Pasetto used Tic-Tac-Toe in ASCII 
form, pitting LLMs against the minimax algorithm to explore emergent features, 
previously suggested to resemble consciousness, but LLMs were much more likely 
to achieve draws or to lose than to win [ 47]. Topsakal and Harper [ 91] found GPT-
4 to win more often than GPT-3.5 at Tic-Tac-Toe, but still neither model played 
optimally. Carvalho and Pollice extended these ﬁndings without ﬁnetuning in a 
zero-shot setting through their ChildPlay benchmark, which includes three sim-
ple classic board games and three novel games encoded in previously unseen 
ASCII formats. They observed that win rate did not necessarily improve with 
more recent models even for simple games, and overall performance, evaluated 
by optimal play criteria such as avoiding illegal moves, blocking opponents' win-
ning moves, or executing winning moves, remained mediocre [ 13]. Most recently, 
Apple's study The Illusion of Thinking corroborates these ﬁndings, showing 
model collapse once puzzle complexity crosses a modest threshold; reasoning 
eﬀort declines precisely when it is needed most. They believe to have shown the 
inadequacy of today's static test sets: they reward surface heuristics and data 
leakage, not causal inference [ 77]. Other dynamic OOD evaluation frameworks 
exist—such as ThinkBench [ 37], DeepSeek-R1 [ 21], and the Information Bottle-
neck LM objective [106]—and further reveal that LLMs remain brittle under

122
G. H. de Carvalho and K. R. Thórisson
novel distributional shifts, often requiring explicit retraining or architectural 
modiﬁcations to maintain performance. 
Recent AGI-oriented surveys reach similar high-level conclusions. Goertzel 
et al. argues that today's LLMs lack the architectural components needed for 
grounded problem-solving [ 27]; Bennett highlights that syntax-only training falls 
short of genuine meaning representation [ 6]. Schneider and Bołtuć warn that such 
systems may look "natural-like" yet remain alien in motivation [ 73]. In practice, 
AI researchers and users often fall prey to anthropomorphism: one recent survey 
found that nearly half of LLM-focused articles use anthropomorphic language [ 5]. 
In present work, we show these intuitions to be misguided. By treating token pre-
diction as probabilistic inference over a training manifold \mathcal{M}M, we try to demon-
strate mathematically why any claim of emergent cognition is unfounded and 
further analyze three attribution fallacies—the Black Box fallacy, the Artiﬁcial 
Dualism Problem (ADP), and the Turing Trap. With these, we attempt to show 
why people seem so quick to attribute higher-order cognitive characteristics to 
these algorithms. 
2
Key Proposition 
Human reasoning, in its various forms, underpins our ability to understand the 
world, solve novel problems, and generate new knowledge [ 88]. Key modes of 
reasoning include: 
• Deduction: Inferring speciﬁc conclusions that are logically guaranteed if the 
premises are true (e.g., from "All AA are BB" and  "xx is AA", deduce "xx is BB"). 
This often involves the application of established rules of inference to given 
information. 
• Induction: Generalizing from speciﬁc observations to broader hypotheses or 
rules whose truth is probable but not guaranteed (e.g., observing many white 
swans and inferring "All swans are white"). This is fundamental to learning 
from experience and forming new concepts. 
• Abduction ("inference to the best explanation"): Formulating the most plau-
sible hypothesis to explain a given set of observations (e.g., observing wet 
ground, and given the knowledge that rain makes the ground wet, abduc-
ing that it likely rained). This involves generating and evaluating potential 
explanations. 
• Analogy: Goal-directed systematic comparison where two or more things are 
compared, to highlight or uncover attributes of interest; useful for comparing 
that which is known, and can help a learning agent deal with unfamiliar tasks 
and environments. 
The central question we are concerned with regarding LLMs is whether 
sophisticated linguistic outputs are evidence of their capacity to explicitly apply 
such logical processes systematically or if such cases are merely a shadow of the 
textual patterns of reasoning already found in their training data.

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
123
Valid human reasoning in the physical world 1 has, as its primary require-
ment, knowledge of what is and is not possible. In any particular given situation, 
what is possible, and not possible, is in turn based on how the brain models 
how the physical world works. Since the set of possible real-world situations 
is often untractable, pragmatic considerations prevent such knowledge to be 
directly stored and indexed, and thus the necessary knowledge for reasoning in 
any situation must be produced by applying the above methods to derive usable 
knowledge. The most eﬃcient representation of physical events is as cause-eﬀect 
relations [ 33]; these enable not only the prediction of what may happen but also 
the production of plans for making things happen [ 86, 87]. 
In humans, language emerges as a by-product of reasoning over causal dia-
grams and learned world models rather than as the primary driver of thought 
[ 81]. It provides the representational medium and cognitive scaﬀold for reasoning: 
inner speech regulates and manipulates thoughts via left inferior frontal gyrus 
circuits [ 56, 95]. Human reasoning combines fast, intuitive judgments (System 
1) driven by unconscious heuristics and pattern recognition, and slow, deliber-
ate analysis (System 2) that consciously manipulates mental representations via 
working memory and executive control [ 25, 41]. It spans a diversity of inference 
types, each recruiting overlapping but task-speciﬁc neural circuits, notably the 
fronto-parietal network [ 70]. Metacognitive oversight, implemented by prefrontal 
inhibitory control mechanisms, monitors, justiﬁes, and sometimes inhibits auto-
matic responses to maintain logical coherence [ 9]. Together, these processes let us 
draw new conclusions, form general rules, generate explanations, and map struc-
tures across domains. In contrast, LLMs produce output directly from language. 
Statistical patterns are encoded during training as high-dimensional probability 
distributions over token sequences, mapping input tokens to output tokens. No 
reasoning is happening from ﬁrst principles. This is not to say that if enough 
strings of text representing the use of an inference rule have been seen during 
training, some of these probability distributions will not have encoded it - but we 
are missing important mechanisms that would enable any reliable generalization 
to OOD data. Overﬁtting the largest possible model to all available data (in 
essence, the standard practice in developing commercial LLMs) does not mag-
ically solve this issue. The fact is that architecturally, transformer LLMs have 
the causal arrow reversed: textual correlations drive token prediction, and any 
appearance of reasoning is then a by-product of statistical pattern completion. 
This makes LLMs bad, if not invalid, reasoners. 
An LLM deﬁnes a conditional probability distribution p_\theta(x_{t+1}\mid c) = \text{softmax}(s_\theta(c,x_{t+1}))pθ(xt+1 | c) =
softmax(sθ(c, xt+1)), wheres_\theta(c,x)sθ(c, x) is the model's score for tokenxx following con-
text c=(x_1,\ldots,x_t)c = (x1, . . . , xt). The parameters \thetaθ are optimized on a vast training dataset 
\mathcal{M}_{data} = {(c',x'): p_{\text{train}}(x'\mid c')>0}Mdata = {(c′, x′) : ptrain(x′ | c′) > 0}, essentially learning to predict probable 
continuations based on statistical co-occurrences. The transformer architecture, 
with its layers of multi-head self-attention and feed-forward networks (FFNs), 
computes these scores. While attention heads o_h^{(l)}(c)o(l)
h (c) produce convex combina-
1 By 'valid reasoning' we mean reasoning whose outcome can, at least in theory, be 
veriﬁed by observation or experiment. 

124
G. H. de Carvalho and K. R. Thórisson
tions of value vectors {W_h^{V,(l)} z_j^{(l-1)}}{W V,(l)
h
z(l−1)
j
}, subsequent operations (linear projections 
W^{O,(l)}W O,(l), FFNs, residual connections, and layer normalizations) transform these 
representations through a complex, non-linear function F_\theta: c \mapsto z_{t+1} = z_t^{(L)}Fθ : c →zt+1 = z(L)
t
. 
This ﬁnal hidden state z_{t+1}zt+1 determines s_\theta(c,x)sθ(c, x). 
Critically, F_\thetaFθ is trained to map input text patterns to output text patterns. It 
is, in essence, an extremely high-dimensional conditional probability table reﬁned 
by billions of parameters. There is no explicit mechanism or module within this 
architecture designed to implement logical rules for deduction, formulate and 
test general hypotheses for induction, or generate and evaluate causal explana-
tions for abduction. Instead, any semblance of such reasoning in the output is 
a reﬂection of patterns absorbed from \mathcal{M}_{data}Mdata, where text generated by humans 
using these reasoning processes, be it correctly or spuriously, is abundant. 
Let \mathcal{Z}_{train}^{(L)} = { z_t^{(L)}(c') : (c', \cdot) \text{ is consistent with } \mathcal{M}_{data} }Z(L)
train = {z(L)
t
(c′) : (c′, ·) is consistent with Mdata} be the set of all 
ﬁnal hidden states generated by F_\thetaFθ from training-representative contexts. We 
posit the model primarily interpolates the convex hull of these observed training 
states, meaning for any context cc, z_{t+1}(c)zt+1(c) eﬀectively lies within \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})Conv(Z(L)
train). 
This operational constraint, shaped entirely by \mathcal{M}_{data}Mdata, limits the model's ability 
to reliably reach for OOD data. A ﬁtted convex-manifold is then insuﬃcient for 
reasoning beyond "blind" extrapolation—granted, aided by the extremely large 
learned distributions. Asking such an ANN to solve a novel problem or generate 
novel reasoning forces it into an undeﬁned space, making the output unlikely 
to be relevant or meaningful if not deployed at massive scales (i.e., outputting 
millions of candidate completions). 
A. Learned Function and Operational Space: The transformer F_\theta: c \mapsto z_{t+1}Fθ : c →zt+1 has 
its parameters \thetaθ optimized such that for contexts c'c′ representative of \mathcal{M}_{data}Mdata, 
F_\theta(c')Fθ(c′) yields z_t^{(L)}(c') \in \mathcal{Z}_{train}^{(L)}z(L)
t
(c′) ∈Z(L)
train which, via the softmax layer, correctly predicts x'x′
from \mathcal{M}_{data}Mdata. As a function learned from examples, F_\thetaFθ typically acts as an inter-
polator. Thus, for any input cc, the resulting z_{t+1}(c)zt+1(c) is expected to be a point 
within (or near) \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})Conv(Z(L)
train). This acknowledges that while F_\thetaFθ is complex and 
involves operations like FFNs and residual connections that break simple convex-
ity propagation from initial value vectors, its ﬁnal output states are constrained 
by the manifold of such states seen during training. 
B. OOD Reasoning vs. Interpolation: Consider a task requiring genuine OOD 
reasoning (deductive, inductive, abductive, or by analogy) to arrive at a conclu-
sion x^*x∗from context c^*c∗, e.g. to solve the millenium problem of P vs NP. Such 
reasoning implies understanding the problem, constructing a novel understand-
ing, and applying a rule in a new way, which would correspond to an ideal ﬁnal 
hidden state z_{ideal}^*z∗
ideal. If  z_{ideal}^* \notin \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})z∗
ideal /∈Conv(Z(L)
train), the model, being conﬁned to this 
interpolative space, is unlikely to produce z_{ideal}^*z∗
ideal without interaction or inductive 
bias [ 6]. Instead, it generates z_{t+1}(c^*) \in \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})zt+1(c∗) ∈Conv(Z(L)
train). This  z_{t+1}(c^*)zt+1(c∗) will reﬂect 
familiar patterns from \mathcal{M}_{data}Mdata rather than the speciﬁc novel inference required 
for x^*x∗. Consequently, p_\theta(x^*\mid c^*)pθ(x∗| c∗) will be low.

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
125
C. Implications for Emergence in Long Sequences: Autoregressive generation 
of a sequence X = (x_1, \ldots, x_K)X = (x1, . . . , xK) involves a trajectory of states z_{t+k+1}(c_k)zt+k+1(ck), 
where each z_{t+k+1}(c_k) \in \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})zt+k+1(ck) ∈Conv(Z(L)
train). The claim that robust, OOD reasoning 
might "emerge" over long sequences implies that this trajectory could sponta-
neously implement a globally coherent novel logical argument. However, if each 
step is limited to \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})Conv(Z(L)
train) and selected based on learned statistical like-
lihoods rather than logical validity or explanatory power for novel situations, 
the sequence remains an elaborate form of pattern completion bound to \mathcal{M}_{data}Mdata. 
The model lacks the internal mechanisms to discover or apply novel abstract 
rules of deduction, induction, abduction, or analogy in a thoughtful manner, 
and to generate goals explicitly—properties that would allow it to navigate to a 
z_{ideal}^* \notin \mathrm{Conv}(\mathcal{Z}_{train}^{(L)})z∗
ideal /∈Conv(Z(L)
train) in a principled way. Thus, any "emergent" properties must 
be explained as recombinations of learned patterns as lucky "shoots in the dark" 
rather than genuine OOD reasoning.
\blacksquare■
Black Boxes are Not Pitch-Black: A rapidly growing body of mechanis-
tic interpretability work demonstrates how individual attention heads imple-
ment induction, copy-and-paste, or simple arithmetic [ 28, 36, 57, 59,105]. To the 
author's knowledge, no study has revealed circuitry for goal formation or com-
plex causal simulation. 
Missing Biological Counterparts: The brain is our only ground truth for what 
we know to be possible in terms of cognition, and thus should not be ignored. 
Regions such as the prefrontal cortex (executive control) [ 43, 53], thalamus 
(multimodal integration) [ 31, 32, 97] and hippocampus (memory consolidation) 
[ 75, 82] form dense, recurrent, neuromodulated loops long suspected to under-
pin consciousness and abstract reasoning [ 17, 22, 90]. A transformer stack, by 
contrast, is a strictly feed-forward computation [ 24, 93]. It performs condi-
tional probability lookup, not the continuous iterative, self-modifying processes 
required for cumulative learning [ 89]. A normal adult cortex contains \sim 8.6\times10^{9}∼8.6×109
neurons [ 3] and  \sim3\times10^{14}∼3 × 1014 synapses [ 63, 84]. Chemical signalling exploits dozens 
of transmitters, yielding rich temporal codes and plasticity cascades [ 42], while 
ANNs have only abstracted ﬁring rates of neurons or action potentials in the 
case of spiking neural networks [ 66]. GPT-3.5 stores 1.75\times10^{11}1.75 × 1011 static weights 
[ 10]; all adaptation ends once gradient descent stops [ 30]. Even speech-critical 
Broca's area contains \mathcal{O}(10^{8})O(108) neurons in recurrent microcolumns [ 1, 78], whereas 
a single 96-head attention block uses only \sim10^{4}∼104 learned parameters and no inter-
nal state [ 93]. Detailed reconstructions of cortical microcircuits show dendritic 
non-linearities and state-dependent reconﬁguration far beyond present trans-
formers [ 50]. Mere parameter count, then, is a poor proxy for the qualitative 
machinery that supports genuine brain function, and subsequently, reasoning. 
Scale & Compute are not Substitutes for Understanding: Wei et al. and Yao et al. 
have shown that LLMs can be coaxed into levels of abstraction without exter-
nal methods through techniques such as chain-of-thought and tree-of-thought 
prompting (i.e. having an LLM prompting itself or branching oﬀ into multiple

126
G. H. de Carvalho and K. R. Thórisson
scenarios and then picking the most likely one) [101,104], but this abstraction 
may be illusory, because the underlying process is still next-token prediction by 
the same model. 
Regardless, reasoning-speciﬁc scaling in LLMs exhibits severe plateaus. Chen 
et al.'s survey ﬁnds that simply increasing context length, chain-of-thought 
depth, or the number of collaborating agents often yields no improvement, show-
ing instead degraded performance once a critical threshold is passed, due to 
redundancy and error compounding [ 14]. Wang et al. formalize this test-time 
scaling plateau with their TTSPM model, deriving saturation budgets beyond 
which additional candidate generations or reasoning rounds aﬀord negligible 
gains, and empirically validate these bounds on AIME, MATH-500, and GPQA 
benchmarks [ 96]. 
Shojaee et al. conclude in a similar vein that frontier reasoning models 
undergo a complete accuracy collapse beyond moderate task complexity and that 
their reasoning eﬀort paradoxically declines even when token budgets remain 
adequate—evidence that scale alone cannot sustain structured, robust inference 
[ 77]. 
In practice, these limits impose steep costs for marginal beneﬁts. This seems 
to indicate that textual data alone cannot provide the inductive biases required 
for general, correctly applied causal abstraction, hierarchical memory, or explicit 
goal-directed planning. We believe that breaking through these plateaus will 
require new architectural primitives, and for that we must not ignore the brain 
and what it has to teach us about information processing, namely that lan-
guage and the patterns of thought found therein are an outcome of structured, 
embodied neural computations—dynamic causal inference, hierarchical working 
memory, and goal-directed control—language is not the source of reasoning, but 
the outcome of underlying brain mechanics. 
The Artiﬁcial Dualism Problem: We believe that interpreting LLMs' out-
puts as an expression of reasoning, rather than as the output of an arbitrary prob-
ability function, is a mistake. We call this topic the artiﬁcial dualism problem 
(ADP): when experts reify latent vectors as if they were explicit rules, goals, or 
beliefs. Unlike computational dualism in embedded-agency work—which studies 
how a policy is embedded in, or separated from, its physical substrate [ 7, 45, 61]— 
ADP is purely observer-side: it is a misattribution error. Nor is ADP related to 
classical mind-body dualism; we make no claim about non-reductive physicalism 
or immaterial minds. ADP is an ontological category mistake: it projects mech-
anisms capable of rule learning, goal creation, or beliefs into the model's latent 
vectors. By contrast, the Turing Trap is an evidential inference error: it projects 
those same mental states from surface behaviour. Our approach addresses the 
bulk of statements and propositions that attribute higher-order cognitive func-
tions to LLMs. 
When the internal mechanics of a generative model are opaque to a user, 
the simplest folk-psychology move is to insert an imagined reasoner behind its 
output. The move is bolstered by surface features—grammar, coherence, appar-
ent insight—that humans evolved to interpret as markers of agency. This is a

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
127
fallacy in that mechanisms for which there is no evidence, apart from the ﬂuency 
of text, are necessarily posited. The Blake Lemoine/LaMDA episode [ 46, 85] is  
an example of this: the engineer ﬁlled explanatory gaps with talk of sentience 
despite a complete lack of supporting evidence. Mechanistic-interpretability 
studies repeatedly reveal specialized pattern-matching circuits, not world-model-
driven reasoning. ADP thus resembles a "God-of-the-gaps" fallacy: explanatory 
voids are patched with an unwarranted cognitive capacity. Crucially, ADP is 
falsiﬁable through the research eﬀorts of mechanistic-interpretability—through 
experiment, the eﬀects of diﬀerent circuity and nodes may be understood. 
The Turing Trap: In 1950 Alan Turing [ 92] proposed a working deﬁnition of 
intelligence that he called the "imitation game", wherein a machine would chat 
with a human judge through a text terminal; if the machine could converse in 
a manner indistinguishable from a human, it should be considered intelligent. 
Later, this idea, which Turing originally proposed as a temporary stop-gap for 
a proper deﬁnition of intelligence, was turned on its head and made into a goal 
that the ﬁeld of AI should strive for. Labeled the 'Turing Test', this idea has 
been thoroughly criticized for failing to account for the underlying mechanisms 
that produce such responses both in machines and in humans [ 34, 76]. We use 
the name 'Turing trap' to describe this fallacy of anthropomorphizing cognitive 
capacities in ANNs simply because they can generate human-like responses to 
"pass the Turing Test". The Turing Trap then, we argue, occurs when observers 
mistake surface-level performance of ANNs for genuine cognitive capacities. This 
mistake seems to us to be driven by anthropomorphism, the human tendency 
to attribute human-like qualities to non-human entities [ 2]. We think that one 
of the primary factors is the high level of ﬂuency and apparent coherence in 
the text generated by LLMs which can create a powerful illusion of depth and 
intentionality. 
3
Discussion and Conclusion 
The central thesis of this paper challenges the narrative endorsed by ﬁgures such 
as Nobel Laureate Geoﬀrey Hinton [ 44], namely that Large Language Models 
(LLMs), and more generally ANN-based architectures, exhibit emergent reason-
ing or understanding in a cognitive sense. While LLMs often produce coherent 
chains of text, these arise from large-scale interpolation over familiar data rather 
than genuine reasoning or goal pursuit; without integration of explicit causal 
or world-model components, 'reasoning' remains eﬀectively in-domain and fal-
ters under rigorous OOD evaluations [ 13, 47, 77, 91]. Other AGI proponents have 
echoed this theory, like Goertzel et al. who argued that today's LLMs "lack the 
basic cognitive architectures" needed for genuine problem-solving and therefore 
should not be viewed as incremental steps toward human-level AGI [ 27]. Ben-
nett et al. likewise believes that, absent grounded interaction, a language model's 
facility with syntax is insuﬃcient for the computation of meaning [ 6]. 
The reader might be interested to produce the following experiment by them-
selves which highlights our main thesis: Try to convince an LLM to solve an

128
G. H. de Carvalho and K. R. Thórisson
unsolved scientiﬁc problem, such as a Millennium problem, like P vs NP. This is 
the question of whether every problem for which a solution can be veriﬁed quickly 
(in polynomial time, denoted as P) can also be solved quickly [ 16]. If we begin 
by asking an LLM to list known strides and advances in the theory and then 
work from those, we will see that the model quickly hits a ceiling. We argue here 
that that's because the manifold space that concerns information related to the 
P vs NP problem "ran out", leaving the model with a brute-forced extrapolation, 
without the ability to generate well-justiﬁed or truly novel insights, given the 
lack of mechanisms to eﬀectively make use of well-supported cause-eﬀect rela-
tions. One can picture producing a trillion trees of hundreds of chains-of-thought 
each and then checking if any solves the problem, but this is not what anyone 
means by reasoning. This limitation shows the fundamental diﬀerence between 
data-driven models and human creativity and intelligent logical reasoning, which 
can, even if slowly and constrained by limited experience, enable exploration of 
uncharted territory regardless of existing data. Neuroscience is yet to discover all 
the ingredients for building general intelligence, but it does oﬀer many detailed, 
mechanistic accounts of brain circuits often overlooked in AI (see Damasio et al. 
[ 18- 20, 64]). 
The inability of LLMs to internally validate truths and reason might stem not 
only from data constraints but also from fundamental theoretical limits inher-
ent in their design. Many of these limitations reﬂect known epistemological and 
computational boundaries of formal systems. Russell's theory of types, which 
stratiﬁed language to avoid self-referential paradoxes, emphasized that certain 
truths necessitate stepping outside a given system for proper resolution [102]. 
Wittgenstein's picture theory posited that while language can represent facts 
through a shared logical form, it cannot explicitly articulate or verify its own log-
ical foundations—such foundations can only be "shown", not stated [103]. Roger 
Penrose has expanded these ideas, suggesting that aspects of human insight and 
understanding might be a prerequisite for meaning, regardless of algorithmic 
capabilities [ 68, 69]. Although we do not necessarily agree with Penrose's com-
putational incompleteness, the parallel is notable: without a human observer, 
the symbols manipulated by LLMs remain inert and without meaning. 
In conclusion, we believe the ﬁeld should aim to end where we begin—by 
re-grounding AI in the human mind, our only true model of real-time, embodied 
reasoning. Studying neural circuits for working memory, hierarchical control, 
embodied interaction, and neuromodulation should be inspiring architectures 
that sustain goals, continuously model the world, and reason within the ﬂow 
of time—capabilities that current transformers fundamentally lack but other 
technologies are pursuing, such as neuromorphic computing [ 38, 52]. With these 
functional gaps, claims of LLM sentience are mistaking scale for substrate. A 
larger model is still executing the same next-token training objective. Combined 
with the stochastic-parrot insight that LLMs do not understand the meaning of 
the language they process [ 5], our analysis underscores that any appearance of 
reasoning is a statistical mirage. LLMs are a sophisticated and dynamic mirror 
of human knowledge, reﬂecting, not transcending, the ingested data. This is not

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
129
to say that LLMs are incapable of interpolating truly novel data—the quality 
and meaning of this data is what leaves a lot to be desired. 
Disclosure of Interests.. The authors have no competing interests. 
References 
1. Amunts, K., Schleicher, A., Bürgel, U., Mohlberg, H., Uylings, H.B., Zilles, 
K.: Broca's region revisited: cytoarchitecture and intersubject variability. J. 
Comparat. Neurol. 412(2), 319-341 (1999). https://doi.org/10.1002/(SICI)1096-
9861(19990920)412:2<319::AID-CNE10>3.0.CO;2-7 
2. Arleen Salles, K.E., Farisco, M.: Anthropomorphism in AI. AJOB Neurosci. 11(2), 
88-95 (2020). https://doi.org/10.1080/21507740.2020.1740350 
3. Azevedo, F.A.C., et al.: Equal numbers of neuronal and nonneuronal cells make 
the human brain an isometrically scaled-up primate brain. J. Comparat. Neurol. 
513(5), 532-541 (2009). https://doi.org/10.1002/cne.21974 
4. Baell, J.B., Holloway, G.A.: New substructure ﬁlters for removal of pan assay 
interference compounds (pains) from screening libraries and for their exclusion 
in bioassays. J. Med. Chem. 53(7), 2719-2740 (2010). https://doi.org/10.1021/ 
jm901137j 
5. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of 
stochastic parrots: can language models be too big? In: Proceedings of the 2021 
ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 
610-623. Association for Computing Machinery, New York (2021). https://doi. 
org/10.1145/3442188.3445922 
6. Bennett, M.T.: On the Computation of Meaning, Language Models and Incom-
prehensible Horrors, pp. 32-41. Springer, Cham (2023). https://doi.org/10.1007/ 
978-3-031-33469-6_4 
7. Bennett, M.T.: Computational dualism and objective superintelligence. In: 
Thórisson, K.R., Isaev, P., Sheikhlar, A. (eds.) Artiﬁcial General Intelligence, pp. 
22-32. Springer, Cham (2024). https://doi.org/10.1007/978-3-031-65572-2_3 
8. Borji, A.: Stochastic parrots or intelligent systems? A perspective on true depth 
of understanding in llms. SSRN Electron. J. (2023). https://doi.org/10.2139/ssrn. 
4507038 
9. Borst, G., Houdé, O.: Training in logic inhibits selection of perceptual principles: 
a study with the wason selection task. Dev. Sci. 17(5), 741-749 (2014) 
10. Brown, T., et al.: Language models are few-shot learners. In: Larochelle, 
H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neu-
ral Information Processing Systems, vol. 33, pp. 1877-1901. Curran Asso-
ciates, Inc. (2020). https://proceedings.neurips.cc/paper_ﬁles/paper/2020/ﬁle/ 
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf 
11. Bubeck, S., et al.: Sparks of artiﬁcial general intelligence: Early experiments 
with GPT-4. ArXiv arxiv:2303.12712 (2023). https://api.semanticscholar.org/ 
CorpusID:257663729 
12. de Carvalho, G.H., Vos, T.: Game-solving drl: an introductory literature survey 
of the last decade and a critical methodological review. osf.io preprint (2024). 
https://doi.org/10.31219/osf.io/7zmx2_v1 
13. de Carvalho, G.H., Knap, O., Pollice, R.: Show, don't tell: evaluating large lan-
guage models beyond textual understanding with ChildPlay (2024). https://arxiv. 
org/abs/2407.11068

130
G. H. de Carvalho and K. R. Thórisson
14. Chen, Z., et al.: A survey of scaling in large language model reasoning (2025). 
https://arxiv.org/abs/2504.02181 
15. Chenthamarakshan, V., et al.: Accelerating drug target inhibitor discovery with 
a deep generative foundation model. Sci. Adv. 9(25), eadg7865 (2023). https:// 
doi.org/10.1126/sciadv.adg7865 
16. Clay Mathematics Institute: P vs NP (2024). https://www.claymath.org/wp-
content/uploads/2022/06/pvsnp.pdf. Accessed 19 Aug 2024 
17. Craik, K.: The Nature of Explanation. Cambridge University Press (1943). 
https://books.google.ch/books?id=EN0TrgEACAAJ 
18. Damasio, A.R.: Investigating the biology of consciousness. Phil. Trans. R. Soc. 
Lond. Ser. B: Biol. Sci. 353(1377), 1879-1882 (1998). https://doi.org/10.1098/ 
rstb.1998.0339 
19. Damasio, A.R.: How the brain creates the mind. Sci. Am. 281(6), 112-117 (1999). 
https://doi.org/10.1038/scientiﬁcamerican1299-112 
20. Damasio, A.R., et al.: Subcortical and cortical brain activity during the feeling 
of self-generated emotions. Nat. Neurosci. 3(10), 1049-1056 (2000). https://doi. 
org/10.1038/79871 
21. DeepSeek-AI, et al.: Deepseek-r1: incentivizing reasoning capability in llms via 
reinforcement learning (2025). https://arxiv.org/abs/2501.12948 
22. Dehaene, S., Naccache, L.: Towards a cognitive neuroscience of consciousness: 
basic evidence and a workspace framework. Cognition 79(1), 1-37 (2001). https:// 
doi.org/10.1016/S0010-0277(00)00123-2 
23. Duan, H., Dziedzic, A., Papernot, N., Boenisch, F.: Flocks of stochastic par-
rots: diﬀerentially private prompt learning for large language models. ArXiv 
arxiv:2305.15594 (2023). https://api.semanticscholar.org/CorpusID:258887717 
24. Elhage, N., et al.: A mathematical framework for transformer circuits. 
Transformer Circuits Thread (2021). https://transformer-circuits.pub/2021/ 
framework/index.html 
25. Evans, J., Stanovich, K.E.: Dual-process theories of higher cognition: Advancing 
the debate. Perspect. Psychol. Sci. 8(3), 223-241 (2013) 
26. Fields, J., Chovanec, K., Madiraju, P.: A survey of text classiﬁcation with 
transformers: how wide? How large? how long? how accurate? How expensive? 
how safe? IEEE Access 12, 6518-6531 (2024). https://api.semanticscholar.org/ 
CorpusID:266824505 
27. Goertzel, B.: Generative AI vs. AGI: the cognitive strengths and weaknesses of 
modern llms (2023). https://arxiv.org/abs/2309.10371 
28. Golgoon, A., Filom, K., Kannan, A.R.: Mechanistic interpretability of large lan-
guage models with applications to the ﬁnancial services industry (2024). https:// 
arxiv.org/abs/2407.11215 
29. Gómez-Bombarelli, R., et al.: Automatic chemical design using a data-driven 
continuous representation of molecules. ACS Cent. Sci. 4(2), 268-276 (2018). 
https://doi.org/10.1021/acscentsci.7b00572 
30. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, Cambridge 
(2016). http://www.deeplearningbook.org 
31. Guillery, R.W., Sherman, S.M.: The thalamus as a monitor of motor outputs. 
Phil. Trans. R. Soc. B: Biol. Sci. 357(1428), 1809-1821 (2002). https://doi.org/ 
10.1098/rstb.2002.1171 
32. Halassa, M.M., Kastner, S.: Thalamic functions in distributed cognitive control. 
Nature Neurosci. 20(12), 1669-1679 (2017). https://doi.org/10.1038/s41593-017-
0020-1

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
131
33. Halpern, J.Y., Pearl, J.: Causes and explanations: a structural-model approach 
— part 1: Causes. CoRR arxiv:1301.2275 (2013). http://arxiv.org/abs/1301.2275 
34. Harnad, S.: The turing test is not a trick: turing indistinguishability is a scien-
tiﬁc criterion. SIGART Bull. 3(4), 9-10 (1992). https://doi.org/10.1145/141420. 
141422 
35. Henrique, D.S.G., Kucharavy, A., Guerraoui, R.: Stochastic parrots looking for 
stochastic parrots: llms are easy to ﬁne-tune and hard to detect with other llms 
(2023). https://arxiv.org/abs/2304.08968 
36. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network 
(2015). https://arxiv.org/abs/1503.02531 
37. Huang, S., et al.: Thinkbench: dynamic out-of-distribution evaluation for robust 
llm reasoning (2025). https://arxiv.org/abs/2502.16268 
38. Indiveri, G., Liu, S.C.: Memory and information processing in neuromorphic sys-
tems. Proc. IEEE 103(8), 1379-1397 (2015). https://doi.org/10.1109/JPROC. 
2015.2444094 
39. Ivanenkov, Y.A., et al.: Chemistry42: an AI-driven platform for molecular design 
and optimization. J. Chem. Inf. Model. 63(3), 695-701 (2023). https://doi.org/ 
10.1021/acs.jcim.2c01191 
40. Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local 
experts. Neural Comput. 3(1), 79-87 (1991). https://doi.org/10.1162/neco.1991. 
3.1.79 
41. Kahneman, D.: Thinking. Fast and Slow. Farrar, Straus and Giroux, New York 
(2011) 
42. Kandel, E.R., Schwartz, J.H., Jessell, T.M., Siegelbaum, S.A., Hudspeth, 
A.J., Mack, S. (eds.): Principles of Neural Science, 5 edn. McGraw-Hill Edu-
cation/Medical, New York (2013). https://accessbiomedicalscience.mhmedical. 
com/content.aspx?bookid=1049&sectionid=59138139 
43. Kouneiher, F., Charron, S., Koechlin, E.: Motivation and cognitive control in the 
human prefrontal cortex. Nat. Neurosci. 12, 939-45 (2009). https://doi.org/10. 
1038/nn.2321 
44. Kruppa, M., Seetharaman, D.: A godfather of ai just won a nobel. he 
has been warning the machines could take over the world. Wall Street 
J. (2024). https://www.wsj.com/tech/ai/a-godfather-of-ai-just-won-a-nobel-he-
has-been-warning-the-machines-could-take-over-the-world-b127da71. Accessed 
09 Apr 2025 
45. Leike, J., Hutter, M.: Bad universal priors and notions of optimality. In: Grün-
wald, P., Hazan, E., Kale, S. (eds.) Proceedings of The 28th Conference on Learn-
ing Theory. Proceedings of Machine Learning Research, vol. 40, pp. 1244-1259. 
PMLR, Paris (2015). https://proceedings.mlr.press/v40/Leike15.html 
46. Lemoine, B.: Is LaMDA sentient? Letter (2024). https://cajundiscordian.medium. 
com/is-lamda-sentient-an-interview-ea64d916d917. Accessed 19 Aug 2024 
47. Liga, D., Pasetto, L.: Testing spatial reasoning of large language models: the case 
of tic-tac-toe (2023). https://ceur-ws.org/Vol-3563/paper_14.pdf 
48. Lipinski, C.A.: Lead- and drug-like compounds: the rule-of-ﬁve revolution. Drug 
Disc. Today Technol. 1(4), 337-341 (2004). https://doi.org/10.1016/j.ddtec.2004. 
11.007 
49. Lu, S., Bigoulaeva, I., Sachdeva, R., Tayyar Madabushi, H., Gurevych, I.: Are 
emergent abilities in large language models just in-context learning? In: Proceed-
ings of the 62nd Annual Meeting of the Association for Computational Linguistics 
(Long Papers), pp. 5098-5139 (2024). https://doi.org/10.18653/v1/2024.acl-long. 
279. https://aclanthology.org/2024.acl-long.279/

132
G. H. de Carvalho and K. R. Thórisson
50. Markram, H., et al.: Reconstruction and simulation of neocortical microcircuitry. 
Cell 163(2), 456-492 (2015). https://doi.org/10.1016/j.cell.2015.09.029 
51. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in ner-
vous activity. Bull. Math. Biophys. 5(4), 115-133 (1943). https://doi.org/10. 
1007/BF02478259 
52. Mead, C.: Neuromorphic electronic systems. Proc. IEEE 78(10), 1629-1636 
(1990). https://doi.org/10.1109/5.58356 
53. Miller, E.K., Cohen, J.D.: An integrative theory of prefrontal cortex 
function. Ann. Rev. Neurosci. 24, 167-202 (2001). https://doi.org/10. 
1146/annurev.neuro.24.1.167. https://www.annualreviews.org/content/journals/ 
10.1146/annurev.neuro.24.1.167 
54. Minsky, M.: A neural-analogue calculator based upon a probability model 
of reinforcement. Technical report, Harvard University Psychological Lab-
oratories,
Cambridge,
MA
(1952).
https://www.bibsonomy.org/bibtex/ 
2d2b3af4935de200ea20c5c191c8c4d67/machinelearning 
55. Mnih, V., et al.: Human-level control through deep reinforcement learning. Nature 
518, 529-533 (2015) 
56. Monti, M.M., Osherson, D.N., Martinez, M.J., Parsons, L.M.: The boundaries 
of language and thought in deductive inference. Proc. Natl. Acad. Sci. 106(30), 
12554-12559 (2009) 
57. Nanda, N.: A comprehensive mechanistic interpretability explainer & glossary 
(2022). https://neelnanda.io/glossary 
58. Nguyen, B., et al.: A generative model for inorganic materials design. Nature 639, 
624-632 (2025). https://doi.org/10.1038/s41586-025-08628-5 
59. Olah, C., Nanda, N.: A framework for understanding neural network models 
(2021). https://transformer-circuits.pub/2021/framework/index.html. Accessed 
19 Aug 2024 
60. Berner, C., et al.: Dota 2 with large scale deep reinforcement learning (2019) 
61. Orseau, L., Ring, M.: Space-time embedded intelligence. In: Bach, J., Goertzel, 
B., Iklé, M. (eds.) AGI 2012. LNCS (LNAI), vol. 7716, pp. 209-218. Springer, 
Heidelberg (2012). https://doi.org/10.1007/978-3-642-35506-6_22 
62. Ouyang, L., et al.: Training language models to follow instructions with human 
feedback (2022). https://arxiv.org/abs/2203.02155 
63. Pakkenberg, B., Gundersen, H.J.G.: Neocortical neuron number in humans: eﬀect 
of sex and age. J. Comparat. Neurol. 384(2), 312-320 (1997). https://doi.org/10. 
1002/(SICI)1096-9861(19970728)384:2<312::AID-CNE10>3.0.CO;2-K 
64. Parvizi, J., Damasio, A.: Consciousness and the brainstem. Cognition 79(1-2), 
135-160 (2001). https://doi.org/10.1016/S0010-0277(00)00127-X 
65. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by 
self-supervised prediction (2017) 
66. PaugamâĂŘMoisy, H., Bohte, S.: Computing with spiking neuron networks. In: 
Handbook of Natural Computing, pp. 335-376 (2012). https://doi.org/10.1007/ 
978-3-540-92910-9_10 
67. Pearl, J.: Causal diagrams for empirical research. Biometrika 82(4), 669-688 
(1995). http://www.jstor.org/stable/2337329 
68. Penrose, R.: The Emperor's New Mind: Concerning Computers, 1st edn. Minds 
and The Laws of Physics. Oxford University Press, Oxford (1989) 
69. Penrose, R.: Shadows of the Mind: A Search for the Missing Science of Conscious-
ness, 1st edn. Oxford University Press, Oxford (1994)

Bad Reasoners, the Turing Trap and the Problem of Artiﬁcial Dualism
133
70. Prado, J., Mutreja, R., Booth, J.R.: The brain network for deductive reasoning: a 
quantitative meta-analysis of 28 neuroimaging studies. J. Cogn. Neurosci. 23(10), 
3483-3498 (2011) 
71. Putin, E., et al.: Adversarial threshold neural computer for molecular de novo 
design. Mol. Pharm. 15(10), 4386-4397 (2018). https://doi.org/10.1021/acs. 
molpharmaceut.7b01137 
72. Schaeﬀer, R., Miranda, B., Koyejo, S.: Are emergent abilities of large language 
models a mirage? In: Advances in Neural Information Processing Systems, vol. 
37, pp. 209-218. Curran Associates, Inc. (2023). https://doi.org/10.48550/arXiv. 
2304.15004 
73. Schneider, H., Bołtuć, P.: Alien versus natural-like artiﬁcial general intelligences. 
In: Hammer, P., Alirezaie, M., Strannegård, C. (eds.) Artiﬁcial General Intelli-
gence, pp. 233-243. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-
33469-6_24 
74. Schrittwieser, J., et al.: Mastering atari, go, chess and shogi by planning with 
a learned model. Nature 588(7839), 604-609 (2020). https://doi.org/10.1038/ 
s41586-020-03051-4 
75. Scoville, W.B., Milner, B.: Loss of recent memory after bilateral hippocampal 
lesions. J. Neurol. Neurosurg. Psychiat. 20(1), 11-21 (1957). https://doi.org/10. 
1136/jnnp.20.1.11 
76. Searle, J.R.: Minds, brains, and programs. Behav. Brain Sci. 3(3), 417-424 (1980). 
https://doi.org/10.1017/S0140525X00005756 
77. Shojaee, P., Mirzadeh, I., Alizadeh, K., Horton, M., Bengio, S., Farajtabar, M.: 
The illusion of thinking: understanding the strengths and limitations of reasoning 
models via the lens of problem complexity (2025). https://ml-site.cdn-apple.com/ 
papers/the-illusion-of-thinking.pdf 
78. Silbereis, J.C., Pochareddy, S., Zhu, Y., Li, M., Sestan, N.: The cellular and 
molecular landscapes of the developing human central nervous system. Neuron 
89(2), 248-268 (2016). https://doi.org/10.1016/j.neuron.2015.12.008 
79. Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforce-
ment learning algorithm (2017) 
80. Siriwardane, E.M.D., Zhao, Y., Perera, I., Hu, J.: Generative design of stable 
semiconductor materials using deep learning and density functional theory. npj 
Comput. Mater. 8, 164 (2022https://doi.org/10.1038/s41524-022-00850-3 
81. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction, and Search, 2 edn. 
The MIT Press, Cambridge (2001). https://doi.org/10.7551/mitpress/1754.001. 
0001 
82. Squire, L.R., Zola-Morgan, S.: The neuropsychology of memory: new links 
between humans and experimental animals. Ann. N. Y. Acad. Sci. 444, 137-149 
(1985). https://doi.org/10.1111/j.1749-6632.1985.tb37585.x 
83. Sutton, R., Barto, A.: Reinforcement Learning: An Introduction (1998). https:// 
doi.org/10.1109/TNN.1998.712192 
84. Tang, Y., Nyengaard, J.R., De Groot, D., Gundersen, H.: Total regional and global 
number of synapses in the human brain neocortex. Synapse 41(3), 258-273 (2001). 
https://doi.org/10.1002/syn.1083 
85. Thoppilan, R., et al.: Lamda: language models for dialog applications. CoRR 
arxiv:2201.08239 (2022) 
86. Thórisson, K.R.: Seed-programmed autonomous general learning. Proc. Mach. 
Learn. Res. 131, 32-70 (2020) 
87. Thórisson, K.R.: The explanation hypothesis in general self-supervised learning. 
Proc. Mach. Learn. Res. 159, 5-27 (2021)

134
G. H. de Carvalho and K. R. Thórisson
88. Thórisson, K.R., Kremelberg, D., Steunebrink, B.R., Nivel, E.: About under-
standing. In: Proceedings of the International Conference on Artiﬁcial General 
Intelligence, pp. 106-117. Springer-Verlag, New York (2016) 
89. Thórisson, K.R., Bieger, J., Li, X., Wang, P.: Cumulative learning. In: Hammer, 
P., Agrawal, P., Goertzel, B., Iklé, M. (eds.) AGI 2019. LNCS (LNAI), vol. 11654, 
pp. 198-208. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-27005-
6_20 
90. Tononi, G., Edelman, G.M.: Consciousness and complexity. Science 282(5395), 
1846-1851 (1998). https://doi.org/10.1126/science.282.5395.1846 
91. Topsakal, O., Harper, J.: Benchmarking large language model (llm) performance 
for game playing via tic-tac-toe. Electronics 13, 1532 (2024). https://doi.org/10. 
3390/electronics13081532 
92. Turing, A.M.: I.-computing machinery and intelligence. Mind LIX(236), 433-460 
(1950). https://doi.org/10.1093/mind/LIX.236.433 
93. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information 
Processing Systems, vol. 30, pp. 5998-6008 (2017) 
94. Vinyals, O., et al.: Grandmaster level in starcraft ii using multi-agent reinforce-
ment learning. Nature 575, 350-354 (2019) 
95. Vygotsky, L.S.: Thought and Language. MIT Press, Cambridge (1986) 
96. Wang, J., Zhu, B., Leong, C.T., Li, Y., Li, W.: Scaling over scaling: exploring 
test-time scaling plateau in large reasoning models (2025). https://arxiv.org/abs/ 
2505.20522 
97. Ward, L.M.: The thalamus: Gateway to the mind. Wiley Interdisc. Rev. Cogn. 
Sci. 4(6), 609-622 (2013). https://doi.org/10.1002/wcs.1256 
98. Webb, T., Holyoak, K.J., Lu, H.: Emergent analogical reasoning in large lan-
guage models. Nat. Hum. Behav. 7(9), 1526-1541 (2023). https://doi.org/10. 
1038/s41562-023-01659-w 
99. Wei, J., et al.: Emergent abilities of large language models. Trans. Mach. Learn. 
Res. TMLR, 1-50 (2022). https://doi.org/10.48550/arXiv.2206.07682 
100. Wei, J., et al.: Emergent abilities of large language models. Trans. Mach. Learn. 
Res. 5, 1-50 (2022). https://doi.org/10.48550/arXiv.2206.07682 
101. Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language 
models (2023). https://arxiv.org/abs/2201.11903 
102. Whitehead, A.N., Russell, B.: Principia Mathematica, vol. I. Cambridge Univer-
sity Press, Cambridge (1910) 
103. Wittgenstein, L.: Logisch-philosophische abhandlung. Annalen der Naturphiloso-
phie XIV, Hefte 3(4), 185-262 (1921) 
104. Yao, S., et al.: Tree of thoughts: deliberate problem solving with large language 
models (2023). https://arxiv.org/abs/2305.10601 
105. Yosinski, J., Clune, J., Nguyen, A.M., Fuchs, T.J., Lipson, H.: Understanding 
neural networks through deep visualization. CoRR arxiv:1506.06579 (2015) 
106. Yu, F.: Memorization-compression cycles improve generalization (2025). https:// 
arxiv.org/abs/2505.08727 
107. Zečević, M., Willig, M., Dhami, D.S., Kersting, K.: Causal parrots: large language 
models may talk causality but are not causal (2023). https://arxiv.org/abs/2308. 
13067

Creative Physics: A Categorical 
Framework for Creative Dynamical 
Processes 
Justin Diamond(B) 
University of Basel, 4001 Basel, Switzerland 
justin@hetzerk.com 
Abstract. We present a rigorous categorical framework for modeling 
creative dynamical processes. We formalize key notions such as ambi-
guity, inconsistency, and  creativity within a mathematical struc-
ture that spans theoretical computer science, philosophy of science, and 
AI/ML. In particular, we introduce the concept of a creative dynami-
cal system as an open-ended category that can extend itself by resolv-
ing ambiguities or inconsistencies through the introduction of new mor-
phisms or objects. We connect our formalism to machine learning by 
suggesting how it could underpin open-ended learning and creativity in 
intelligent systems. 
Keywords: Category Theory · Creative · Dynamics 
1
Introduction 
Creativity—the ability to produce novel and useful structures or ideas—is a 
fundamental aspect of intelligence and scientiﬁc progress. Yet, formulating a 
rigorous mathematical theory of creativity remains a challenge. In ﬁelds rang-
ing from theoretical computer science and category theory to philosophy of sci-
ence and artiﬁcial intelligence (AI), there is a growing need for formal frame-
works that can describe processes of open-ended innovation or emergence of 
novelty. For example, in the philosophy of science, Kuhn's notion of paradigm 
shifts [ 8] highlights how scientiﬁc theories undergo discontinuous leaps to new 
frameworks when anomalies or inconsistencies accumulate. In AI and machine 
learning, researchers aspire to build systems capable of open-ended learning or 
creative problem solving that goes beyond ﬁxed training objectives. And in the-
oretical computer science and logic, questions about the limits of formal systems 
(as exempliﬁed by Gödel's incompleteness or Turing's uncomputability) prompt 
us to wonder whether truly intelligent systems must transcend any single ﬁxed 
formal theory. 
We propose a formal approach to modeling creativity using the language of 
category theory. Category theory, with its emphasis on relationships and its pow-
erful universal constructions provides an abstract toolbox for unifying disparate 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 135-146, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_13

136
J. Diamond
domains. In particular, we will use the Yoneda Lemma, and William Lawvere's 
ideas on modeling being vs becoming within categorical logic [ 3, 4]. By doing so, 
we aim to capture the essence of a creative dynamical process: a process that 
can not only undergo dynamics within a ﬁxed state-space, but can also expand 
or modify its own state-space in order to accommodate novel situations. 
Just as physics provides laws for how physical systems evolve over time, we 
seek "laws" or principles governing how creative systems (e.g., evolving theories 
or learning agents) can generate new structures over time [ 15]. We treat creativ-
ity itself as a kind  of  dynamics that can be studied. For theoretical computer 
science and logic, it oﬀers a perspective on how one might formally handle com-
putations or proofs that expand their own formal language [ 9]. For philosophy 
of science, it provides a way to model paradigm change and theory revision as 
mathematical transformations. For AI/ML, it lays groundwork for systems that 
could, in principle, revise their own model class or representation to adapt to 
novel inputs, rather than being conﬁned to a predetermined hypothesis space. 
Category-based models of emergent cognition have been explored in Memory 
Evolutive Systems [ 7]. 
The contributions of this paper are as follows. First, we introduce a categori-
cal framework in which we can deﬁne key notions—ambiguity, inconsistency, and  
creativity. Second, we develop a theoretical framework using category theory and 
universal constructions. We deﬁne what it means to be a creative dynamical sys-
tem in categorical terms. We leverage the Yoneda Lemma and Kan extensions to 
show how new objects can be adjointed to categories to represent novel concepts. 
1.1
Deﬁning Ambiguity, Inconsistency, and Creativity 
We assume familiarity with basic concepts in Category Theory [ 2]. 
Deﬁnition 1 (Ambiguity). 
Given a category \mathcal{C}C (for example, a category of 
models or knowledge states) and a problem speciﬁed as a diagram D: I \to \mathcal{C}D : I →C (or 
more generally a functor P: \mathcal{C} \to \mathbf{Set}P : C →Set representing a property or query), we say 
there is ambiguity if there exist two distinct morphisms (or solutions) a, ba, b in 
\mathcal{C}C that both satisfy the conditions of the problem (e.g., two distinct cones over 
DD, or two elements in P(X)P(X) for some XX) such that there is no morphism in 
\mathcal{C}C that distinguishes aa from bb with respect to the problem. In logical terms, the 
information available is consistent with multiple interpretations. 
Put simply, ambiguity means the structure of \mathcal{C}C is too coarse to diﬀerentiate 
between two (or more) potential states or solutions. Ambiguity often arises when 
the current theory has insuﬃcient axioms, or when data underdetermines the 
model. 
Deﬁnition 2 (Inconsistency). 
Given a category \mathcal{C}C and a speciﬁcation of a 
desired outcome as above (a diagram or condition), we say an inconsistency 
is present if there is no morphism or object in \mathcal{C}C that can satisfy the speciﬁed 
conditions. In categorical terms, the diagram DD has no cone (no limiting cone 
if we expect a limit) or the logical theory corresponding to \mathcal{C}C together with the 
additional constraint is contradictory (has an initial object that is also terminal

A Categorical Framework for Creative Dynamical Processes
137
but distinct, etc.). Equivalently, a presheaf FF encoding the requirements is not 
representable and, moreover, adding the requirement forces a contradiction in \mathcal{C}C
(e.g., forces 1=01 = 0 in a model). 
Inconsistency means the framework \mathcal{C}C is too restrictive or internally con-
tradictory to allow a certain needed solution. Inconsistencies can be local (just 
aﬀecting a particular attempted construction) or global (no model exists at all, 
as with an outright contradictory theory). We are often interested in the case 
of a local inconsistency: a particular diagram has no colimit in \mathcal{C}C, even though 
one intuitively "ought" to exist (meaning we think of the colimit as a concept we 
wish we had). 
Finally, we deﬁne creativity in our context: 
Deﬁnition 3 (Creativity as Extension). 
A process is creative if it can 
resolve ambiguity or inconsistency by extending the categorical structure in which 
it operates. Formally, suppose \mathcal{C}C is the current category (state of knowledge, 
theory, or context). A creative step is given by a functor (an embedding) E: \mathcal{C} \hookrightarrow \mathcal{C}'E :
C →C′ into a new category \mathcal{C}'C′ such that: 
- 
In 
\mathcal{C}'C′, the previously ambiguous or inconsistent diagram/constraint 
DD
becomes resolved. That is, either DD has a unique (up to isomorphism) solu-
tion/cone in \mathcal{C}'C′ (ambiguity resolved by new distinctions or identiﬁcations), or 
DD now has at least one solution in \mathcal{C}'C′ whereas it had none in \mathcal{C}C (inconsistency 
resolved by adding new structures). 
- 
\mathcal{C}'C′ extends \mathcal{C}C minimally in the sense of containing \mathcal{C}C as a subcategory and 
adding only those additional morphisms/objects necessary for the resolution. 
Often, \mathcal{C}'C′ can be described as \mathcal{C}C plus the inclusion of a new object (or mor-
phism) XX with speciﬁed relations to \mathcal{C}C, for example the adjunction of a colimit 
for a previously colimit-less diagram. 
We call \mathcal{C}'C′ a creative extension of \mathcal{C}C with respect to the problem at hand. 
This deﬁnition captures the idea that creativity is the introduction of novel 
elements into the system that were not derivable within the original system, in 
order to solve a problem that the original system could not solve. A non-creative 
problem solving step, in contrast, stays within \mathcal{C}C (perhaps just ﬁnding a limit or 
performing a derivation in \mathcal{C}C). A creative step, by deﬁnition, changes the rules 
of the game by expanding the universe of discourse. 
It is worth noting that in this deﬁnition, we speak of minimal extension. There 
could be trivial "creative" extensions where one simply adds an arbitrary new 
object unrelated to the problem, but that wouldn't solve the problem and thus 
wouldn't count by the ﬁrst condition. However, one might also add many things 
at once that solve the problem among other changes; our interest is typically 
in the smallest or most natural extension that achieves the resolution, as that 
corresponds to the insight or simplest new idea that resolves the issue. 
Example 1 (Adjoining a Solution to an Equation). Consider the equationx^2 = 2x2 = 2
in the theory of rational numbers. In the category \mathbf{Field}_{\mathbb{Q}}FieldQ of ﬁeld extensions of \mathbb{Q}Q
(whose objects are ﬁelds containing \mathbb{Q}Q and morphisms are \mathbb{Q}Q-homomorphisms),

138
J. Diamond
this equation has no solution in the object \mathbb{Q}Q itself, indicating an inconsistency if 
we expect a solution. The diagram here might be thought of as \mathbb{Q} \leftarrow \mathbb{Q}[x]/(x^2-2)Q ←Q[x]/(x2−2)
(the polynomial ring modulo the equation) trying to map into\mathbb{Q}Q, which it cannot. 
A creative extension is to move to a larger ﬁeld \mathbb{Q}(\sqrt{2})Q(
√
2) which contains a new ele-
ment \sqrt{2}
√
2 satisfying the equation. The functor embedding \mathbf{Field}_{\mathbb{Q}} \hookrightarrow \mathbf{Field}_{\mathbb{Q}(\sqrt{2})}FieldQ →FieldQ(
√
2)
(really just considering \mathbb{Q}(\sqrt{2})Q(
√
2) as an object in a larger category) resolves the 
inconsistency: now the equation x^2=2x2 = 2 has a solution (by construction). This 
extension is minimal in an obvious sense (we added just one new number with 
one algebraic relation). In our framework, this is a creative act by the mathe-
matician: the creation of \sqrt{2}
√
2 as a concept. Note, \sqrt{2}
√
2 did not literally exist in 
\mathbb{Q}Q, but it existed as a presheaf/speciﬁcation (namely the functor of "solutions to 
x^2=2x2 = 2") which became representable in the extension. 
The above example illustrates on a simple level how a mathematical discovery 
or invention can be seen as a creative extension. Many historical mathematical 
innovations ﬁt this pattern: the introduction of complex numbers to solve poly-
nomial equations with no real roots, the introduction of inﬁnitesimals or rigorous 
\epsilonϵ-\deltaδ deﬁnitions to resolve ambiguities in calculus, etc. In scientiﬁc theories, intro-
ducing a new theoretical entity (like the concept of an electron, or a quantum 
state) to explain phenomena can be seen similarly. 
It is important to clarify that creativity often involves the combination of 
existing ideas as well. In categorical terms, that can be modeled by taking a 
pushout (amalgamated sum) of two categories or two diagrams, essentially com-
bining two theories along some shared part. Such a pushout would be a colimit 
in \mathbf{Cat}Cat (the category of categories), and the result is a category containing both 
theories as substructures. If the two were previously disjoint or only loosely 
connected, their pushout yields something new that contains both. This is rem-
iniscent of conceptual blending in cognitive science, which has been modeled by 
colimits in conceptual graphs [ 5]. However, for the scope of this work, we focus 
on the pattern of extending one given category to resolve an issue. 
We can also speak of degrees of creativity: a ﬁrst-order creativity might add 
an object (a point in the category), a second-order creativity might add a mor-
phism between morphisms (a 2-morphism in a 2-category), etc. In a higher cat-
egory framework, adding a 2-morphism that equates two previously distinct 
morphisms can resolve an ambiguity without adding a new object. This suggests 
working in a 2-category or \infty∞-category could allow certain ambiguities to be 
resolved by "up to homotopy" identiﬁcations rather than outright new objects. 
This is particularly relevant in homotopy type theory and higher topos theory [ 6], 
where one might resolve inconsistencies by allowing higher-dimensional paths. 
2
A Categorical Framework for Creative Dynamics 
We now develop the formal framework for creative dynamical systems. First, we 
describe how one can construct creative extensions using universal constructions 
like Kan extensions and colimits. Next, we introduce the notion of a dynamical

A Categorical Framework for Creative Dynamical Processes
139
process that moves through a sequence of categorical structures, and deﬁne what 
it means for such a process to be open-ended or creative. We also incorporate the 
idea of higher categories to capture transformations of transformations (higher-
order change). 
2.1
Creative Extensions via Kan Extensions 
A powerful general technique in category theory for extending the domain of a 
functor is the Kan extension. We focus on left Kan extensions. Given a functor 
i: \mathcal{C} \to \mathcal{D}i : C →D and another functor F: \mathcal{C} \to \mathcal{E}F : C →E (where \mathcal{E}E is some target category, e.g. 
\mathbf{Set}Set), a left Kan extension of FF along ii is a functor \text{Lan}_i F: \mathcal{D} \to \mathcal{E}LaniF : D →E together 
with a natural transformation \eta: F \Rightarrow \text{Lan}_i F \circ iη : F ⇒LaniF ◦i, which is universal with that 
property. Intuitively, \text{Lan}_i FLaniF is the "best approximate extension" of FF from \mathcal{C}C
to all of \mathcal{D}D. If  FF already had a perfect extension \tilde{F} ˜F making \tilde{F}\circ i = F ˜F ◦i = F, then  
\text{Lan}_i F \cong \tilde{F}LaniF ∼= ˜F. If no such on-the-nose extension exists, \text{Lan}_i FLaniF still exists (under 
mild conditions, always when \mathcal{E}E is complete and \mathcal{C}C is small) and is characterized 
by a universal property. 
In our context, think of \mathcal{C}C as the original category (the original theory or 
conceptual framework), \mathcal{D}D as a candidate extended category that contains \mathcal{C}C, 
and F: \mathcal{C} \to \mathbf{Set}F : C →Set as some data or constraints we want to satisfy or extend. For 
example, FF could be a functor that picks out some subset of \mathcal{C}C or assigns to 
each object some observed phenomena. A common scenario: \mathcal{C}C might be a small 
category that does not have some desired colimit, and \mathcal{D}D is \mathcal{C}C plus a candidate 
colimit object. Then i: \mathcal{C} \to \mathcal{D}i : C →D is inclusion. A Kan extension \text{Lan}_i FLaniF will produce 
a functor on \mathcal{D}D that 'ﬁlls in' the value on the new object as the colimit of FF on 
the diagram, if that colimit exists in \mathcal{D}D. The universal property of Kan extension 
ensures this is the most coherent choice. 
Concretely, suppose we have an inconsistency meaning a diagram D: I \to \mathcal{C}D : I →
C has no colimit in \mathcal{C}C. We then consider  \mathcal{D} = \mathcal{C} + {\text{colimit object}}D = C + {colimit object}, a formal  
extension of \mathcal{C}C by adding a new object CC and arrows \varphi_i: D(i) \to Cϕi : D(i) →C for each 
i\in Ii ∈I, subject to the minimal relations needed to make (C,{\varphi_i})(C, {ϕi}) a colimit of DD. 
This \mathcal{D}D can be seen as a pushout (in the 2-category of categories) or a comma 
category construction. By deﬁnition, DD has a colimit in \mathcal{D}D. Now the inclusion 
functor i: \mathcal{C} \to \mathcal{D}i : C →D allows us to extend any functor F: \mathcal{C} \to \mathcal{E}F : C →E to \mathcal{D}D by left Kan 
extension. In particular, if we want a model M: \mathcal{C} \to \mathbf{Set}M : C →Set to extend to \mathcal{D}D, we can  
attempt to take \text{Lan}_i MLaniM. If  MM was not a model because of the missing colimit, 
now \text{Lan}_i MLaniM on \mathcal{D}D will assign an interpretation to the new object CC, which will 
end up being the colimit of the M(D(i))M(D(i)) sets with respect to the structure of 
M(D(f))M(D(f)) maps. Typically, \text{Lan}_i M(C) = \varinjlim_{i\in I} M(D(i))LaniM(C) = lim
−→i∈I M(D(i)) (the colimit in \mathbf{Set}Set of 
the diagram of sets) if such a colimit of sets exists (which it does in \mathbf{Set}Set, colimits 
are just unions or quotients). Thus \text{Lan}_i MLaniM yields a candidate model in \mathcal{D}D. This  
is essentially forcing the existence of a solution by constructing it externally. 
What we have described is reminiscent of the method of forcing in logic (like 
how Cohen added a new set to a model of ZF set theory to solve the continuum 
hypothesis independence). Indeed, forcing is precisely a method of taking a model 
that lacks something and embedding it into a larger model where that something

140
J. Diamond
exists, in a controlled minimal way. Our categorical creative extension is akin to 
forcing a solution to exist. 
Summarizing: Kan extensions provide a systematic way to perform creative 
extensions while maintaining coherence with the old structure. They ensure that 
the existing data or knowledge is preserved (the restriction of the extended 
functor to \mathcal{C}C recovers the original FF up to the speciﬁed natural transformation) 
[ 16]. This means we aren't throwing away the prior knowledge; we are extending 
around it. 
2.2
Creative Dynamical Systems 
We can now deﬁne a creative dynamical system in our framework. Infor-
mally, it is a system that evolves over time not just by state transitions, but by 
occasional creative extensions of its state space. 
Let \mathbf{Time}Time be a category or directed poset that represents the progression 
of time or stages (for simplicity, one can take \mathbf{Time}Time to be the poset of natural 
numbers 0 \le 1 \le 2 \le \cdots0 ≤1 ≤2 ≤· · · , viewed as a category with a single morphism i \to ji →j
whenever i \le ji ≤j). A dynamical system in the categorical sense can be seen as a 
functor X: \mathbf{Time} \to \mathcal{C}X : Time →C, where  \mathcal{C}C is a category of possible states. For instance, a 
deterministic automaton is a functor from (\mathbb{N}, \le)(N, ≤) to a category of conﬁgurations, 
or a diﬀerential equation can be seen as a functor from (\mathbb{R},\le)(R, ≤) (time) to the 
category of states (though typically one uses an enriched notion for continuous 
time). 
Deﬁnition 4 (Creative Dynamical System). A creative dynamical sys-
tem consists of: 
- 
A sequence of categories (states of the world) \mathcal{C}_0 \subseteq \mathcal{C}_1 \subseteq \mathcal{C}_2 \subseteq \cdotsC0 ⊆C1 ⊆C2 ⊆· · · where 
each \mathcal{C}_{t}Ct is a subcategory of \mathcal{C}_{t+1}Ct+1 (the inclusion functor is faithful and identity 
on \mathcal{C}_tCt's objects and morphisms). 
- 
A functor X: \mathbf{Time} \to \bigcup_{t}\mathcal{C}_tX : Time →
t Ct such that for each t \in \mathbf{Time}t ∈Time, X(t)X(t) is an 
object of \mathcal{C}_tCt and for t < t't < t′, the morphism X(t \to t')X(t →t′) is a morphism in \mathcal{C}_{t'}Ct′ (so 
the transition may land in the larger category). 
Additionally, we require that for most tt, X(t\to t+1)X(t →t+1) (the immediate transition) 
is actually a morphism in \mathcal{C}_tCt (an ordinary lawful dynamic step), but at certain 
distinguished times t = \taut = τ, the transition X(\tau \to \tau+1)X(τ →τ + 1) is not in \mathcal{C}_\tauCτ (because 
the required morphism or state did not exist there) and thus mandates a creative 
extension \mathcal{C}_\tau \subset \mathcal{C}_{\tau+1}Cτ ⊂Cτ+1 constructed to allow X(\tau \to \tau+1)X(τ →τ + 1) to exist. 
In simpler terms, a creative dynamical system XX evolves within a ﬁxed cat-
egory (obeying its rules) until it hits a point where the next step it needs to 
take is not possible in the current category. At that point, the system (or an 
external oracle, or the modeler) extends the category to a larger one where that 
step becomes possible. The process then continues. The sequence \mathcal{C}_0 \subset \mathcal{C}_1 \subset \cdotsC0 ⊂C1 ⊂· · ·
thus records a history of expanding expressive power or knowledge.

A Categorical Framework for Creative Dynamical Processes
141
One can view \mathcal{C}_0C0 as the initial theory or worldview, and each \mathcal{C}_{t+1}Ct+1 as \mathcal{C}_tCt plus 
a creative insight. The functor XX picks out a particular trajectory of states (like 
particular theories or models in use at each time). The condition that transitions 
are mostly within \mathcal{C}_tCt means most changes are conservative (deductive or routine), 
but some transitions require moving to \mathcal{C}_{t+1}Ct+1 because, for example, X(t)X(t) and some 
external input create an inconsistency that \mathcal{C}_tCt cannot handle, so X(t+1)X(t + 1) had to 
be placed in \mathcal{C}_{t+1}Ct+1. 
It is worth noting that one could ﬂatten this entire ladder into one large cat-
egory (like the presheaf category of \mathcal{C}_0C0 which eventually contains all expansions). 
In that view, the creative system's trajectory XX is just an ordinary functor to 
a very large category. However, such a large category is not eﬀectively given 
initially; it emerges as the colimit of the increasing sequence of \mathcal{C}_tCt. Working  
within the system, the jump to \mathcal{C}_{t+1}Ct+1 is a genuine novelty event, not something 
predetermined. This distinction becomes important in computational settings (a 
program that can rewrite its own code versus one that has a ﬁxed program that 
could produce the same outcomes if unrolled). 
We can state a proposition related to the universality of this process: 
Proposition 1. Let {\mathcal{C}_t}_{t \in T}{Ct}t∈T be a directed system of categories (where TT is the 
index set, e.g. time steps), and suppose each inclusion \mathcal{C}_t \hookrightarrow \mathcal{C}_{t+1}Ct →Ct+1 is a creative 
extension resolving a speciﬁc issue at step tt. Then the union (or 2-colimit in 
\mathbf{Cat}Cat) \mathcal{C}_\infty = \bigcup_{t \in T} \mathcal{C}_tC∞= 
t∈T Ct (formally the colimit of the diagram of categories) is a 
category that contains all the intermediate states and their morphisms. \mathcal{C}_\inftyC∞can 
be seen as the "horizon of knowable concepts" if the process were allowed to 
continue indeﬁnitely. In many cases, \mathcal{C}_\inftyC∞will coincide with the presheaf category 
[\mathcal{C}_0^{op}, \mathbf{Set}][Cop
0 , Set] or a related completion, if the process eventually adjoins all possible 
needed representables. 
Proof (Sketch of Proof). Each creative extension was described by a universal 
construction (e.g. adding an object to represent a presheaf, which is a colimit 
in \mathbf{Cat}Cat). The sequence of such additions, being directed, yields a colimit in \mathbf{Cat}Cat
which by universal property ensures that any functor out of \mathcal{C}_\inftyC∞corresponds to 
functors out of some stage \mathcal{C}_tCt eventually. If at each stage an unmet presheaf 
was added, eventually all presheaves that ever show up as needed will be repre-
sentable. If the process was such that eventually every presheaf is needed (max-
imal creativity), then \mathcal{C}_\inftyC∞would embed \mathcal{C}_0C0 into its presheaf category fully. In 
general, \mathcal{C}_\inftyC∞is contained in (or equal to) the presheaf category on \mathcal{C}_0C0, since  no  
process can require something beyond the logical limits of \mathcal{C}_0C0's presheaf world 
(assuming we only add within that set of potential objects). 
This result indicates that however creative a process is, if we started from 
a small category \mathcal{C}_0C0, all the new concepts added were in some sense already 
implicit as presheaves on \mathcal{C}_0C0. One might say the "set of possible creative moves" is 
predetermined by \mathcal{C}_0C0. However, \mathcal{C}_0C0 could be very basic (like perhaps the category 
for an agent that can observe and act might be minimal, but its presheaf category 
is enormous, and exploring it is exactly the agent's life of creativity). Moreover,

142
J. Diamond
sometimes we might consider \mathcal{C}_0C0 to be inﬁnite or large, so the idea of enumerating 
all its presheaves is not practical. 
In reality, creative agents seldom start from a tabula rasa; they have some 
initial knowledge. But as they learn, they eﬀectively enlarge their conceptual 
apparatus. Our framework suggests they are moving in a space of possible cate-
gories of increasing complexity. 
3
Computational Realization 
This section describes brieﬂy the abstract framework by giving (i) algorithms to 
detect when creativity is required and (ii) a constructive procedure that adjoins 
the minimal extension. Additional computational speciﬁcs can be found in [ 1]. 
Intuitively, \DeltaΔ asks "Can the current category already do the next thing?" while 
\LambdaΛ says "If not, add just enough structure so that it can." 
Algorithm 1. Ambiguity/Inconsistency Detector \DeltaΔ
Require: Category \mathcal{C}C, diagram D\!:\!I\to\mathcal{C}D:I →C
1: Attempt to compute \operatorname{colim}_{I} DcolimI D in \mathcal{C}C
2: if a unique colimit exists then 
3:
return "routine step" 
4: else if multiple non-isomorphic cones exist then 
5:
return "ambiguity detected" 
6: else 
7:
return "inconsistency detected" 
8: end if 
Algorithm 2. Creative Extension Constructor \LambdaΛ
Require: Category \mathcal{C}C, diagram D\!:\!I\to\mathcal{C}D:I →C
1: Form a formal object CC and arrows \varphi_i:D(i)\to Cϕi : D(i) →C (i\in I)(i ∈I) with the universal 
colimit equations. 
2: Deﬁne \mathcal{C}' := \mathcal{C} + \langle C,{\varphi_i}\rangleC′ := C + ⟨C, {ϕi}⟩(categorical pushout / free colimit completion). 
3: Embed \mathcal{C}C via the faithful functor \iota:\mathcal{C}\hookrightarrow\mathcal{C}'ι : C →C′. 
4: return (\mathcal{C}',\iota)(C′, ι)
Because \mathcal{C}'C′ is initial among all repairs that solve DD, it is the  smallest possible 
creative leap. To avoid uncontrolled growth, assign a cost 
 \bUnALT{}\mathrm{Cost}(C) \;=\; \alpha\,\lvert\!\operatorname{Obj}(C)\rvert + \beta\,\lvert\!\operatorname{Mor}(C)\setminus\operatorname{Mor}(\mathcal{C})\rvert,\eUnALT{} Cost(C) = α |Obj(C)| + β |Mor(C) \ Mor(C)|,
balance it against predictive improvement\Delta\mathcal{L}ΔL, and accept the extension iﬀ\Delta\mathcal{L} > \gamma\cdot \mathrm{Cost}(C)ΔL >
γ · Cost(C) (Table 1).

A Categorical Framework for Creative Dynamical Processes
143
Table 1. Key constructs and their creative interpretations 
Construct
Canonical use
Creative interpretation 
Yoneda Lemma
Characterise an object by 
its hom-sets 
Detect missing 
representable concepts 
Left Kan Extension Extend a functor across an 
inclusion 
Adjoin the minimal 
object/morphism needed 
Higher cells
Homotopies between 
morphisms 
Resolve ambiguity via 
"identiﬁcation up to path" 
4
Implications for Machine Learning and AI 
One motivation for this work [ 17] is to inform the development of AI systems 
that exhibit creativity or open-ended learning [ 18]. Current mainstream machine 
learning algorithms operate within ﬁxed model spaces: for example, a neural 
network has a ﬁxed architecture and simply tunes weights, or an evolutionary 
algorithm might evolve a ﬁxed-length genome. What would it mean for an AI to 
expand its own model space? Our categorical framework provides some insights, 
albeit at a conceptual level. 
We can consider an AI's knowledge or hypothesis space as a category \mathcal{C}C of con-
cepts or models it can entertain. Training the AI on data can be seen as ﬁnding 
an object in \mathcal{C}C (a model/hypothesis) that best ﬁts the data. This is analogous to 
ﬁnding a model of a theory (where the theory is the data constraints). If the data 
contains patterns that are not expressible by any object in \mathcal{C}C, then the AI will at 
best ﬁnd an approximation that isn't quite right. In deep learning practice, this 
can manifest as model underﬁt or the need to increase model capacity. Usually, 
one increases capacity by just making the network bigger (more neurons, layers) 
which is like just picking a larger \mathcal{C}C from the start (e.g., allow bigger networks). 
But the structure (a feedforward network or convolution or transformer, etc.) is 
often held ﬁxed during training. 
A truly creative AI might, upon noticing certain patterns in data that it can-
not ﬁt, propose a new layer or a new type of activation or a new representational 
trick. There is some research in neural architecture search and meta-learning 
that goes in this direction, but often it is still search within a predeﬁned space 
of architectures. 
In our terms, one would want the AI to implement something akin to 
Yoneda's lemma: realize that a certain desired mapping F: \mathcal{C}^{op} \to \mathbf{Set}F : Cop →Set (like 
from possible inputs to observed outputs) is not representable by any existing 
model, and then adjoin a new concept XX such that now it becomes representable. 
For example, if an AI that understands basic arithmetic is confronted with tasks 
about a new function (like maybe sine waves), it might add a new neuron that 
computes \sin(x)sin(x) because it can't approximate it well with polynomials. In prac-
tice, we usually add that manually, but one could imagine a system doing it 
itself.

144
J. Diamond
In terms of known theoretical limits, there's a connection to Gödel's incom-
pleteness and Turing's halting problem: an agent in a ﬁxed formal system cannot 
derive certain truths [ 10], but a creative agent can eﬀectively jump to a stronger 
system. However, this could lead to an inﬁnite regress. One might ask: is there 
a universal algorithm for creativity that will eventually solve any well-deﬁned 
problem given enough time? If an agent keeps extending its category, does it 
converge to truth? This is related to philosophical questions about the limits of 
scientiﬁc inquiry [ 11]. 
One caution is that adding arbitrary new concepts can lead to an explosion 
of possibilities, many of which are not useful. Random creativity is not eﬀective 
(most random changes degrade performance). So a creative system must have 
heuristics for what extensions are promising. In science, heuristics include sim-
plicity, analogy to existing successful concepts, and alignment with experimental 
hints. In machine learning, one might use something like minimizing validation 
error as a guide, or using information criteria that balance complexity and ﬁt. 
Our formal framework can accommodate a heuristic by not fully automati-
cally adding all presheaves (which would immediately saturate to the presheaf 
category, which is too huge), but rather adding those presheaves that are needed 
or yield signiﬁcant improvement. This suggests a kind of guided search in the 
space of presheaves of current category for one that, if representable, would give 
a big reward (explain new data well). 
5
Conclusion 
We have proposed a category-theoretic formulation of creativity, deﬁning creative 
dynamical processes as those that can perform self-extensions of their state-space 
(or theory) to overcome ambiguity or inconsistency. 
Philosophically, our approach reinforces a view of intelligence as an ongoing 
process rather than a static entity. It aligns with concepts of emergence [ 12], 
self-reference, and the evolutionary growth of knowledge. It suggests that to 
truly capture creativity, one must allow for an inﬁnite (or at least unbounded) 
potential for new distinctions and constructions [ 13], albeit one hopefully guided 
by experience and principle [ 14]. 
Many avenues remain to be explored. One is to incorporate a notion of value 
or utility into the creative extensions: not all new concepts are equally worth-
while. Perhaps one can deﬁne an optimization problem over possible extensions, 
connecting to ideas in Bayesian model selection or information theory. Another 
avenue is formalizing the analogical reasoning process: category theory has a 
notion of functor categories which might be used to model an analogy between 
two domains as a functor between categories, and a creative leap could be ﬁnd-
ing such a functor and then taking a colimit that merges structures (conceptual 
blending).

A Categorical Framework for Creative Dynamical Processes
145
References 
1. Burstall, R.M., Rydeheard, D.E.: Computational Category Theory. Prentice Hall 
International. (Seminal work laying the foundations of mechanizing category theory 
on computers; includes implementations of categorical constructions and empha-
sizes the computational handling of limits, colimits, and adjunctions.) (1988) 
2. Mac Lane, S.: Categories for the Working Mathematician, 2nd edn. Springer, Hei-
dleberg (1998) 
3. Lawvere, F.W.: Functorial Semantics of Algebraic Theories. Proc. Natl. Acad. Sci. 
U.S.A. 50(5), 869-872 (1963) 
4. Lawvere, F.W.: Some thoughts on the future of category theory. In: Carboni, A., 
Pedicchio, M.C., Rosolini, G. (eds.) Category Theory. LNM, vol. 1488, pp. 1-13. 
Springer, Heidelberg (1991). https://doi.org/10.1007/BFb0084208 
5. Andreatta, M., Ehresmann, A., Guitart, R., Mazzola, G.: Towards a categorical 
theory of creativity for music, discourse, and cognition. In: Yust, J., Wild, J., 
Burgoyne, J.A. (eds.) MCM 2013. LNCS (LNAI), vol. 7937, pp. 19-37. Springer, 
Heidelberg (2013). https://doi.org/10.1007/978-3-642-39357-0_2. (Highlights the 
role of colimits and Yoneda lemma in formalizing creative processes.) 
6. Johnstone, P.T.: Sketches of an Elephant: A Topos Theory Compendium, vol. 1-2. 
Oxford University Press, Cambridge (2002). (Reference on categorical logic and 
classifying toposes.) 
7. Ehresmann, A.C., Vanbremeersch, J.P.: Memory Evolutive Systems: Hierarchy, 
Emergence, Cognition. Elsevier, Heidelberg (2007). (Presents a category-based 
model of complex hierarchical systems and concept emergence.) 
8. Kuhn, T.S.: The Structure of Scientiﬁc Revolutions. University of Chicago Press, 
Chicago (1962). (Introduces the concept of paradigm shifts and incommensurability 
in scientiﬁc progress.) 
9. Feferman, S.: Gödel's program for new axioms: Why, where, how and what?. In: 
Gödel '96: Logical Foundations of Mathematics, Computer Science and Physics 
(1991). (Discusses extending formal systems with new axioms, related to self-
transcendence in math.) 
10. Lucas, J.R.: Minds, machines and Gödel. Philosophy 36(137), 112-127 (1961). (An 
argument that human minds are not formalizable by machines, invoking Gödel's 
incompleteness.) 
11. Penrose, R.: The Emperor's New Mind. Oxford University Press, Cambridge 
(1989). (Speculates on human consciousness and limits of AI, with Gödelian argu-
ments.) 
12. Popper, K.R.: The Logic of Scientiﬁc Discovery. Hutchinson (1959). (Popper's phi-
losophy of science emphasizing conjecture and refutation.) 
13. Lakatos, I.: Proofs and Refutations. Cambridge University Press, Cambridge 
(1976). (Examines the development of mathematical ideas through a dialogue 
involving conjectures and exceptions, aligning with creative concept revision.) 
14. Hegel, G.W.F.: Science of Logic (1812). (Philosophical work introducing the dialec-
tical method of thesis-antithesis-synthesis.) 
15. Baez, J., Stay, M.: Physics, topology, logic and computation: a Rosetta Stone. In: 
New Structures for Physics, pp. 95-172. Springer, Heidelberg (2010). (An exposi-
tion bridging category theory with physics and logic, relevant for thinking about 
diﬀerent theories in categorical terms.)

146
J. Diamond
16. Goguen, J.A., Burstall, R.M.: Institutions: abstract model theory for speciﬁcation 
and programming. J. ACM 39(1), 95-146 (1992). (Introduces the notion of insti-
tutions, capturing the idea of a logical system in category-theoretic terms, which 
relates to comparing theories across frameworks.) 
17. Solomonoﬀ, R.: A formal theory of inductive inference, Part I and II. Inf. Control 
7(1), 1-22, 224-254 (1964). (Early work on formalizing learning and induction, 
though within ﬁxed frameworks.) 
18. Stanley, K.O., Lehman, J.: Why Greatness Cannot Be Planned: The Myth of the 
Objective. Springer, Heidelberg (2015). (Discusses open-ended innovation and the 
idea that following objectives can hinder discovering novelty, relevant for creative 
AI.)

Neuro-Symbolic LIDA's Semantic Vision 
System 
Nathan DiGilio(B)
and Pulin Agrawal 
The Pennsylvania State University, Behrend College, Erie, PA, USA 
{nmd5752,pagrawal}@psu.edu 
Abstract. This paper explores the integration of a Vision-Language 
Model (VLM) and a cognitive architecture to create a context-aware, 
neuro-symbolic system capable of real-time media capture. The Learn-
ing Intelligent Decision Agent (LIDA), inspired by human cognition and 
the Global Workspace Theory, is paired with MobileCLIP, a state-of-
the-art VLM that can interpret semantic information in visual data. 
This integration aims to enhance LIDA's sensory system by providing 
robust, semantic understanding of real-world scenes while maintaining its 
decision-making and memory mechanisms. The system leverages Mobile-
CLIP's ability to process image-text embeddings for zero-shot activ-
ity recognition, and LIDA's cognitive cycles enable adaptive behavior 
based on contextually relevant input. A proof-of-concept is demonstrated 
through a task where the system identiﬁes and records speciﬁc actions 
during a track and ﬁeld event. Several strategies for embedding compar-
ison, including moving averages and cosine similarity, are evaluated for 
their eﬀectiveness in accurately detecting the action. The results show 
that the integration of MobileCLIP with LIDA is promising, oﬀering 
a scalable solution for real-time, context-aware decision-making. This 
work lays the foundation for future advancements in LIDA-based neuro-
symbolic systems, particularly in environments requiring both perceptual 
understanding and adaptive cognitive functions. Future developments 
will focus on expanding the system's capabilities to handle more com-
plex tasks, integrating lifelong learning, and enhancing the robustness of 
decision-making processes. 
Keywords: LIDA · Neuro-symbolic systems · Vision-Language 
Models · Cognitive architecture · MobileCLIP 
1
Introduction 
Continued improvements to the state-of-the-art in artiﬁcial intelligence have 
opened the door for machines to process and respond to increasingly complex 
sensory data to human-comparable performance more than ever before. Of these 
innovations, deep neural networks and cognitive architectures stand out as two 
valuable paradigms. Cognitive architectures are good at modeling cognitive pro-
cesses that enable adaptability and decision making [ 23]. Whereas, deep neural 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 147-158, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_14

148
Nathan DiGilio and Pulin Agrawal
networks [ 34] are good at ﬁnding patterns and recognition. Coevality of these 
two paradigms has still precluded them from leveraging each other's strengths 
for the most part. The sub-ﬁeld of neuro-symbolic systems [ 18, 20, 38] took shape  
to ﬁx this. The prime culprits that have prevented them from eﬀective amalga-
mation are the inability of online learning capabilities in deep neural networks, 
combined with catastrophic forgetting [ 8]. Even though that is still a challenge 
with neural network based systems, attention mechanism [ 37] based transformer 
model's capability to learn and understand the real-world in a multi-modal fash-
ion has led to pretrained models that are quite capable [ 17] and may not need to 
learn for the life-time of an agent. This makes the task of creating neuro-symbolic 
systems easier. 
A cognitive architecture like Learning Intelligent Decision Agent (LIDA) [ 14] 
provides mechanisms for attention, memory, consciousness, and decision-making 
modeled after human cognition. However, it needs a robust sensory system that 
is able to handle the pre-processing of real-world data. Vision Language Models 
(VLMs) like (Contrastive Language-Image Pretraining) CLIP [ 9] and its deriva-
tives enable semantic understanding of images through natural language asso-
ciation. The ability to learn patterns across modalities is extremely powerful 
because it allows for a more comprehensive understanding. It also brings these 
models closer, qualitatively, to a human way of thinking, which is typically across 
modalities. While the VLMs can provide better capabilities to sense and recog-
nize objects and concepts in the environment, cognitive architectures like LIDA 
can allow higher-level cognitive capabilities. There is a lot of work that can be 
done to leverage the capabilities of the two methods together. It has been a 
long-standing mission to incorporate neuro-symbolic mechanisms in LIDA and 
other cognitive architectures [ 3, 16, 25]. 
This paper explores integrating these two technologies. LIDA [ 14] can  provide  
a human-like decision-making mechanism based on consciousness as deﬁned by 
the Global Workspace Theory [ 5, 6]. The state-of-the-art capabilities of semantic 
understanding in language models can provide a robust sensory scene comprehen-
sion to the overall system. Integrating well-developed deep-learning systems like 
the ones based on transformer architecture will be the ﬁrst steps in modernizing 
the sensory mechanisms of a prominent cognitive architecture like LIDA. This 
will also mark an important step in the movement of the neuro-symbolic nature 
of cognitive architectures [ 15] in general. Here we provide a proof-of-concept 
demonstration for a special use case, i.e., to make a context-aware media cap-
ture system. In the future, this will allow us to develop and reﬁne LIDA-based 
agents' learning processes to be more robust. Ultimately, this will be helpful in 
improving cognitive processes modeled in LIDA to be more generic and universal. 
1.1
Background 
LIDA. LIDA is a cognitive architecture, as shown in Fig. 1, that models human  
cognition and perception [ 15]. Based on the Global Workspace Theory (GWT) 
[ 4, 6], a theory of consciousness, LIDA operates through cognitive cycles [ 27] 
that try to capture the information processing timing in the brain. These cog-

Neuro-Symbolic LIDA's Semantic Vision System
149
nitive cycles occur at a frequency of 3-5 Hz in humans [ 27]. In LIDA, it is 
designed to be asynchronous, apart from the broadcast and action selection 
stages. LIDA hypothesizes that higher-order cognition (like planning and delib-
eration) emerges from sequences of these cycles [ 14]. Each cognitive cycle in 
LIDA can be broken down into three main phases: 
Fig. 1. LIDA cognitive architecture 
- Perception and Understanding Phase: This begins with incoming sen-
sory stimuli being processed in Sensory Memory, then recognized in Percep-
tual Associative Memory by activation of associated nodes. These combined 
associated nodes are sent to the Current Situational Model (CSM). Structure-
building codelets create new structures and associations in CSM, forming a 
preconscious understanding of the current context. 
- Attention Phase: Specialized attention codelets scan for salient content 
within the current situational model. The codelet then creates a structure 
called a coalition that combines all salient content for an attention codelet 
and is sent over to the global workspace. Competing coalitions of content vie 
for dominance in the Global Workspace. The winning coalition is broadcast 
globally to all modules in LIDA, becoming the agent's conscious content for 
that cycle. 
- Action and Learning Phase: Based on the conscious content, an appro-
priate scheme is selected via Procedural Memory, the memory of 'what to 
do when'. The scheme is the data structure that stores actions and their 
corresponding results in a given context. These schemes get instantiated in

150
Nathan DiGilio and Pulin Agrawal
the Action Selection module as a behavior, to be selected among many oth-
ers, and executed through Sensory Motor Memory. Simultaneously, various 
memory systems (episodic, declarative, spatial, etc.) learn from the conscious 
broadcast-this is where most of the agent's learning takes place. 
In essence, the LIDA model simulates a mind by repeatedly answering the 
question: "What do I do next?". Thus, this system can be used to create control 
systems of intelligent agents [ 3, 10, 22]. Besides creating agents, LIDA is useful for 
generating and validating hypotheses about cognitive processes [ 30]. LIDA has 
been used to explain and illustrate many other neuropsychological theories like 
self-system [ 32, 33], episodic memory [ 31], attentional blink [ 28], spatial memory, 
and hippocampus [ 29]. While a comprehensive explanation of the LIDA model is 
outside the scope of this paper, further details can be found in the LIDA Model 
Tutorial [ 15]. 
MobileCLIP. MobileCLIP [ 36] is a VLM optimized for eﬃcient performance on 
mobile and embedded systems. It builds from the CLIP paradigm, which enables 
the models to associate features in images and text prompts within a shared 
embedding space. Traditional CLIP models utilize large-scale Vision Transform-
ers (ViTs), whereas MobileCLIP employs a hybrid architecture that reduces both 
model size and inference time. This is particularly applicable to systems that 
require real-time processing under constrained computational resources. Mobile-
CLIP's image encoder, the MCi architecture, integrates convolutional neural net-
works (CNNs) with transformer modules. This hybrid encoder improves param-
eter eﬃciency by reducing MLP expansion ratios, allowing for a deeper but 
lightweight network. The text encoder, Text-RepMixer, uses convolution-based 
token mixers instead of attention mechanisms. This improves speed while main-
taining the same representational power. 
MobileCLIP is an open-source implementation of the CLIP [ 36] model, devel-
oped by OpenAI. CLIP is trained on a large dataset of image-text pairs, enabling 
it to learn an embedding space where images and their text descriptions are 
aligned closely. This allows for the possibility of zero-shot image classiﬁcation, 
which is when a model can classify images into categories that were not seen 
in training. MobileCLIP achieves all this while also being several times faster 
and smaller than standard CLIP models. MobileCLIP is useful across a variety 
of diﬀerent domains because of its generality. It is eﬃcient and adaptable, mak-
ing it great for systems that need to operate under many diﬀerent conditions 
without having to train for each new condition. 
1.2
Problem 
The objective of this research is to assess whether we can eﬀectively use LIDA's 
mechanisms in combination with embeddings from state-of-the-art vision and 
language models as a sensory system and still keep the robust decision-making 
mechanisms in LIDA viable. This will verify the feasibility of LIDA's processes

Neuro-Symbolic LIDA's Semantic Vision System
151
as compatible with neuro-symbolic mechanisms. Recent work [ 3] also helps push 
LIDA into the neurosymbolic realms, but the implementations do not use vector 
embeddings. Instead, the large language models are used for text manipula-
tion directly within LIDA, without the use of feature vectors. Other work has 
conceptualized and implemented various processes in LIDA with the help of 
vectors for information content in nodes, [ 35], but the work falls short on vali-
dating these mechanisms on real-world data. There have been previous attempts 
to provide a robust sensory system to LIDA [ 1, 2] using cortical learning algo-
rithms/Hierarchical Temporal Memory (HTM) [ 19]. But they are also unable to 
demonstrate eﬀectiveness with real-world data because HTM has not been shown 
to be capable of state-of-the-art computer vision. Other eﬀorts to integrate gen-
erative neural networks with LIDA employ the use of variational autoencoders 
[ 13]. Although such a system is capable of understanding real-world vision data 
in theory, its performance has been overshadowed by transformer-based models 
like CLIP, based on the evidence of the surge of transformer-based models [ 26] 
as compared to variational autoencoder-based models [ 7]. 
In this paper, we attempt to create a neuro-symbolic state-of-the-art vision 
system with an example problem of context-aware media capture, combining a 
vision-language model (MobileCLIP) [ 36] with a cognitive architecture (LIDA). 
The need for intelligent video analysis systems has signiﬁcantly grown across 
many ﬁelds, including surveillance, manufacturing, and sports analysis. Tradi-
tional video processing approaches rely on motion detection, hard-coded con-
ditions, or very narrow scopes and lack contextual awareness and ﬂexibility. 
With large-scale pretrained VLMs, it has become feasible to understand com-
plex visual environments using natural language prompts. Simultaneously, cog-
nitive architectures oﬀer a framework for interpreting these semantic signals and 
determining whether they are important enough to take action. The combina-
tion of semantic perception with high-level cognitive reasoning opens the doors 
to creating systems that are both responsive and generally applicable, while still 
being selective and purposeful in their behavior. 
1.3
Contributions 
Our primary contributions in this paper are 1) a mechanism for integrating 
VLMs with LIDA, 2) a proof-of-concept demonstration of the integration in a 
real-world scenario, and 3) a robust LIDA framework written in Python 1 that 
implements all required modules for this experiment. Integration of a robust 
embedding system like MobileCLIP with LIDA marks the ﬁrst step of LIDA 
towards being a neuro-symbolic system capable of semantic vision. We present 
the architecture, design, and results of this system within a controlled environ-
ment where the only task is to identify and record the speciﬁc activity, the track 
and ﬁeld event of a person throwing. The LIDA framework written in Python 
that made these experiments possible is designed to be ﬂexible and plug-and-play 
in its philosophy. In the future, it will also allow us to explore various cognitive
1 https://github.com/pulinagrawal/Vector-LIDA/tree/agi2025. 

152
Nathan DiGilio and Pulin Agrawal
processes within LIDA in a more realistic setting than what was ever possible 
before. 
2
Solution: A Neuro-Symbolic LIDA 
2.1
Overview 
The system investigates the use of a vision-language model to understand what's 
happening in a video feed and a cognitive control loop to decide when it's impor-
tant enough to start recording. It combines MobileCLIP, which extracts semantic 
embeddings from video frames, with LIDA, which evaluates these embeddings 
to determine whether the scene should be recorded. 
2.2
MobileCLIP Embedding Pipeline 
MobileCLIP can process each frame and determine its relevance to the given 
categories by creating embeddings that represent the visual content. In this 
project, MobileCLIP processes video frames and generates embeddings repre-
senting the visual content. These vectors represent the high-level visual features 
of the frame in a multi-modal embedding space. The generated embeddings can 
be later compared to reference embeddings that correspond to speciﬁc activities, 
such as"throwing," enabling a form of zero-shot activity recognition. Reference 
embeddings are pre-generated by averaging the embeddings from MobileCLIP 
of a few selected frames of such an activity being performed. 
2.3
LIDA Integration 
MobileCLIP is used in LIDA's sensory memory to provide high-dimensional vec-
tor embeddings that capture the semantic content of each video frame. These 
embeddings serve as the sensory content of a LIDA-based agent, where each 
vector is used to instantiate a node in LIDA's sensory memory. 
At system initialization, a set of manually labeled frames representing the 
target activity (e.g., "throwing") is embedded using MobileCLIP. These embed-
dings are averaged to form an initial reference vector that serves as the agent's 
internal representation of that concept. This reference vector is stored and used 
during cueing of Perceptual Associative Memory (PAM) to enabling retrieval of 
contextually similar experiences, via cosine similarity of embeddings. 
During runtime, each incoming frame is embedded by MobileCLIP and com-
pared to the reference vector using cosine similarity, as shown in Fig. 2. If the  
similarity exceeds a predeﬁned conﬁdence threshold, a node is created in the 
Current Situational Model (CSM) to represent the relevant context. That node 
is then propagated through the rest of the LIDA architecture. For example, the 
Structure Building Codelets (SBCs) in the Workspace can combine this node 
with others to construct a new, more complete context representation. 
Attention Codelets continuously monitor the contents of the Workspace by 
searching for segments that closely match a predeﬁned focus vector. The most

Neuro-Symbolic LIDA's Semantic Vision System
153
Fig. 2. LIDA Integration - MobileCLIP provides frame features as feature vector (vv). 
Cosine similarity comparison from reference vectors in Perceptual Associative Memory 
(PAM) brings relevant concept into Current Situational Model (CSM). 
relevant content forms a coalition, which is broadcast to all memory systems 
and becomes the conscious content of the current cognitive cycle. Procedural 
Memory compares this conscious broadcast to its stored schemas using cosine 
similarity. If the similarity with a schema's context vector is high enough, the 
associated behavior is instantiated in the Action Selection module. Once selected, 
the behavior's motor plan is executed through the sensory-motor system. 
Thus, LIDA becomes the decision-making component that interprets Mobile-
CLIP embeddings in the proposed video recording system. By evaluating the 
alignment and relevance of detected activity against stored representations, 
LIDA can determine whether an event (e.g., a "throw") is happening and trig-
ger actions such as toggling video recording. We implement these mechanisms 
using a Python library for LIDA, lidapy, which enables easy construction and 
experimentation with LIDA agents. While our implementation includes the core 
cognitive cycle, some features of the sensory-motor system described by Dong et 
al. [ 11, 12], learning mechanisms discussed in [ 24] and motivational mechanisms 
remain to be implemented. 
A key requirement for making LIDA a neuro-symbolic system is the use 
of vector-based models that support semantic similarity metrics such as cosine 
similarity or Euclidean distance, depending on the model architecture. Another 
requirement is the ability to manipulate feature vectors. These vectors must sup-
port: (1) compositionally-combining multiple features to construct higher-level 
concepts, and (2) reﬁnement-adjusting a vector's position in semantic space to 
represent a speciﬁc concept better. This research focuses on the second mecha-
nism, which is explored further in the following section. 
2.4
Averaging Methodology 
One of the challenges faced during the creation of a neuro-symbolic LIDA sys-
tem is that of concept learning and reﬁnement. In LIDA, Perceptual Associative

154
Nathan DiGilio and Pulin Agrawal
Memory (PAM) is responsible for concept learning. Humans come with some 
associations built-in, for example faces [ 21]. Similarly, we use reference embed-
dings to bootstrap the recognition process. The concept reﬁnement then happens 
through iteratively improving the concept over time closer to a true average. 
Based on conscious learning commitment in LIDA we learn a new average using 
the frame features in the winning coalition from conscious broadcast based on 
the closest matching concept node in PAM. To merge the nodes that contain the 
frame feature vector, we need to develop a mechanism for averaging the previ-
ous feature vector in the existing context of the best matching scheme with the 
new one from the winning coalition. We can use various averaging mechanisms, 
including a centroid over all previous embeddings or a moving average. We tested 
these various mechanisms on raw annotated footage to obtain performance met-
rics of these mechanisms on the embeddings from MobileCILP without using 
LIDA. We then used the best-performing method with LIDA. 
Fig. 3. Example throwing frame.
Fig. 4. Example not throwing frame. 
3
Results 
Following the architecture described in Sect. 3, we aimed to evaluate the system's 
ability to selectively detect and record a speciﬁc activity, i.e.,"throwing", using 
MobileCLIP embeddings and the LIDA decision framework. The experiments 
were performed to evaluate diﬀerent averaging strategies for constructing a ref-
erence embedding from known "throwing" frames. The best strategy is carried 
over into the LIDA implementation. 
The dataset is a 6-minute, 56-second video from a track and ﬁeld practice con-
taining 12578 frames. Ground truth labels were manually annotated using times-
tamp ranges indicating when throwing was occurring, as shown in Fig. 3 and 4. 
Each frame was passed through MobileCLIP to generate a high-dimensional 
embedding. A cosine similarity of the embedding with the current centroid of 
throwing vs not throwing produced a prediction of the current frame. A pre-
diction was considered correct if the classiﬁcation matched the annotated label. 
Accuracy was measured as the percentage of correctly classiﬁed frames. To create 
a reference, various methods were tested to average the embeddings (i.e., com-
puting the centroid) of the throwing and non-throwing frames: static, adaptive, 
and exponential moving average (EMA) 1. All of these methods use cosine simi-
larity to compare the current embedding with the average. The static average is

Neuro-Symbolic LIDA's Semantic Vision System
155
Fig. 5. Average Accuracy 
just the average of 9 known throwing frames and 6 non-throwing frames. It then 
compares all of the frames to these averages and makes a decision based on how 
similar it is to the averages. The adaptive averaging uses a set conﬁdence level 
at which it adds the current frame to the average embeddings for that action. 
We conducted additional experiments to evaluate the impact of initialization on 
the adaptive and EMA averaging methods. Speciﬁcally, we tested both meth-
ods with and without using the same initial reference averages employed by the 
static method. Results, as shown in Fig. 6 indicate that initializing with these 
reference frames signiﬁcantly improves classiﬁcation accuracy. 
Fig. 6. Conﬁdence Threshold Impact 
\bALT{} \mathbf{v}_{t} = \alpha \cdot \mathbf{v}_{\text{new}} + (1 - \alpha) \cdot \mathbf{v}_{t-1} \label{eq:ema}\eALT{} vt = α · vnew + (1 −α) · vt−1
(1)

156
Nathan DiGilio and Pulin Agrawal
 \bUnALT{}\text{where,} \quad \begin{aligned} \mathbf{v}_{t} & : \text{updated average at time } t \\ \mathbf{v}_{\text{new}} & : \text{new input vector} \\ \mathbf{v}_{t-1} & : \text{previous average} \\ \alpha & : \text{smoothing factor, } 0 < \alpha < 1 \end{aligned}\eUnALT{} where,
vt : updated average at time t
vnew : new input vector
vt−1 : previous average
α : smoothing factor, 0 < α < 1
EMA averaging works by changing an alpha value, as shown in Eq. 1; it weighs  
newer or older information more. We found that this approach was marginally 
more accurate on average, as shown in Fig. 5. Testing multiple combinations of 
alpha values and conﬁdence showed that when starting with reference frames 
like the static average, meant that the alpha value and conﬁdence level did not 
matter as much. Although when starting without reference frames, a combination 
of .5 alpha value and .9 conﬁdence resulted in much better results. The overall 
best results were an EMA averaging strategy while also starting with reference 
frames. Therefore, we used EMA in LIDA for combining feature vectors. 
4
Conclusion and Future Work 
This work presents a proof-of-concept integration of MobileCLIP, a state-of-
the-art vision-language model, with LIDA, a cognitive architecture inspired by 
human cognition. We present a preliminary demonstration of how MobileCLIP 
and LIDA can be integrated toward enabling real-time, context-aware media 
capture in a neurosymbolic framework. At alpha .5 and conﬁdence threshold 
.9 we got 87.33% accuracy, which is better than the performance reported for 
MobileCLIP [ 36] on their zero-shot Top 1% accuracy of 67.8%. We do not claim 
the improved performance was aﬀorded by LIDA mechanisms. The improved 
performance is likely because of a single classiﬁcation task tested here. We aim 
only to demonstrate a mechanism that enables this integration. The reasoning 
capabilities in LIDA emerge at the level of multiple cognitive cycles, not at single 
cycles. Integration over multiple cognitive cycles can only happen with the help 
of motivational mechanisms, which are not fully developed in LIDA yet. This 
approach addresses long-standing limitations in LIDA's sensory mechanisms and 
illustrates how pretrained multi-modal models can ground symbolic cognition in 
real-world sensory input. In addition, we produced a ﬂexible and easy-to-use 
Python implementation of LIDA that enabled us to create this agent and run 
these experiments. Our system shows that such integration is both feasible and 
promising, paving the way for more sophisticated and adaptable cognitive agents. 
Future work will focus on expanding the system's capabilities to more complex 
tasks and environments, exploring lifelong learning within LIDA, and continuing 
the eﬀort to bridge high-level cognitive processes with robust perception. 
References 
1. Agrawal, P., Franklin, S.: Multi-layer cortical learning algorithms. In: 2014 IEEE 
Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain 
(CCMB), pp. 141-147 (2014). https://doi.org/10.1109/CCMB.2014.7020707, 3  
citations (Crossref) [2023-07-18]

Neuro-Symbolic LIDA's Semantic Vision System
157
2. Agrawal, P., Franklin, S., Snaider, J.: Sensory memory for grounded representations 
in a cognitive architecture. In: Proceedings of the Sixth Annual Conference on 
Advances in Cognitive Systems (ACS Poster Collection), pp. 1-18 (2018) 
3. Agrawal, P., Yagnik, A., Dong, D.: Generative AI can be creative too. In: Thóris-
son, K.R., Isaev, P., Sheikhlar, A. (eds.) Artiﬁcial General Intelligence, pp. 1- 
10. Springer Nature Switzerland, Cham (2024).https://doi.org/10.1007/978-3-031-
65572-2_1 
4. Baars, B.J.: A cognitive theory of consciousness. Cambridge University Press, New 
York (1988) 
5. Baars, B.J.: A cognitive theory of consciousness. Cambridge University Press 
(1993), google-Books-ID: 7w6IYeJRqyoC 
6. Baars, B.J.: The conscious access hypothesis: Origins and recent evidence. Trends 
Cogn. Sci. 6(1), 47-52 (2002) 
7. Berahmand, K., Daneshfar, F., Salehi, E.S., Li, Y., Xu, Y.: Autoencoders and their 
applications in machine learning: A survey. Artif. Intell. Rev. 57(2), 28 (2024) 
8. Chen, Z., Liu, B.: Continual learning and catastrophic forgetting. In: Lifelong 
Machine Learning, pp. 55-75. Springer (2018) 
9. Cherti, M., et al.: Reproducible scaling laws for contrastive language-image learn-
ing. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition 
(CVPR), pp. 2818-2829 (2023).https://doi.org/10.1109/CVPR52729.2023.00276, 
http://arxiv.org/abs/2212.07143, arXiv:2212.07143 [cs] 
10. Dong, D.: Enabling an autonomous agent sharing its minds, describing its conscious 
contents. Cogn. Syst.  Res.  80, 103-109 (2023) 
11. Dong, D., Franklin, S.: A new action execution module for the learning intelligent 
distribution agent (LIDA): The sensory motor system. Cogn. Comput. 7, 552-568 
(2015) 
12. Dong, D., Franklin, S., Agrawal, P.: Estimating Human Movements Using 
Memory of Errors. Procedia Comput. Sci. 71, 1-10 (2015).https://doi. 
org/10.1016/j.procs.2015.12.174, https://www.sciencedirect.com/science/article/ 
pii/S1877050915036352, 2 citations (Semantic Scholar/DOI) [2024-04-26] 
13. Franklin, S., Kugele, S.: "conscious" multi-modal perceptual learning for grounded 
simulation-based cognition. In: Proceedings of the Annual Meeting of the Cognitive 
Science Society, vol. 42 (2020) 
14. Franklin, S., Madl, T., D'Mello, S., Snaider, J.: LIDA: A Systems-level Architecture 
for Cognition, Emotion, and Learning. IEEE Trans. Auton. Ment. Dev. 6(1), 19-41 
(2014) 
15. Franklin, S., et al.: A LIDA cognitive model tutorial. Biologically Inspired Cogni-
tive Architectures 16, 105-130 (2016) 
16. Ganesan, A., et al.: Learning with holographic reduced representations 
(2021).https://doi.org/10.48550/arXiv.2109.02157,
http://arxiv.org/abs/2109. 
02157, 23 citations (Semantic Scholar/arXiv) [2025-03-17] arXiv:2109.02157 [cs] 
version: 2 
17. Han, X., et al.: Pre-trained models: Past, present and future. AI Open 2, 225-250 
(2021) 
18. Hatzilygeroudis, I., Prentzas, J.: Neuro-symbolic approaches for knowledge repre-
sentation in expert systems. Int. J. Intell. Syst. 1(3-4), 111-126 (2004) 
19. Hawkins, J., Ahmad, S., Dubinsky, D.: Hierarchical Temporal Memory Including 
HTM Cortical Learning Algorithms, 0.2. Numenta. Inc., (2011) 
20. Hilario, M.: An overview of strategies for neurosymbolic integration. Connectionist-
Symbolic Integration, pp. 13-35 (2013)

158
Nathan DiGilio and Pulin Agrawal
21. Hyvärinen, L., Walthes, R., Jacob, N., Chaplin, K.N., Leonhardt, M.: Current 
understanding of what infants see. Curr. ophthalmol. rep. 2, 142-149 (2014) 
22. Khayi, N.A., Franklin, S.: Initiating language in LIDA: learning the meaning of 
vervet alarm calls. Biologically Inspired Cogn. Architectures 23, 7-18 (2018) 
23. Kotseruba, I., Gonzalez, O.J.A., Tsotsos, J.K.: A review of 40 years of cognitive 
architecture research: Focus on perception, attention, learning and applications. 
arXiv preprint arXiv:1610.08602, pp. 1-74 (2016) 
24. Kugele, S., Franklin, S.: Learning in LIDA. Cogn. Syst. Res. 66, 176-200 
(2021).https://doi.org/10.1016/j.cogsys.2020.11.001, https://linkinghub.elsevier. 
com/retrieve/pii/S1389041720300826, 9 citations (Crossref) [2023-07-18] 
25. Latapie, H., Kilic, O., Thórisson, K.R., Wang, P., Hammer, P.: Neurosymbolic 
systems of perception and cognition: The role of attention. Front. Psychol. 13, 
806397 (2022) 
26. Lin, T., Wang, Y., Liu, X., Qiu, X.: A survey of transformers. AI Open 3, 111-132 
(2022) 
27. Madl, T., Baars, B.J., Franklin, S.: The Timing of the Cognitive Cycle. PLoS ONE 
6(4), e14803 (2011) 
28. Madl, T., Franklin, S.: A LIDA-based model of the attentional blink. In: Proceed-
ings of the 11th International Conference on Cognitive Modeling (ICCM 2012) 
(2012) 
29. Madl, T., Franklin, S., Chen, K., Montaldi, D., Trappl, R.: Bayesian integration of 
information in hippocampal place cells. PLoS ONE 9(3), e89762 (2014) 
30. Madl, T., Franklin, S., Snaider, J., Faghihi, U.: Continuity and the ﬂow of time: 
A cognitive science perspective. Philosophy and psychology of time, pp. 135-160 
(2016) 
31. Ramamaurthy, U., D'Mello, S.K., Franklin, S.: Modiﬁed sparse distributed memory 
as transient episodic memory for cognitive software agents. In: 2004 IEEE Interna-
tional Conference on Systems, Man and Cybernetics (IEEE Cat. No. 04CH37583). 
vol. 6, pp. 5858-5863. IEEE (2004) 
32. Ramamurthy, U., Franklin, S., Agrawal, P.: Self-system in a model of cognition. 
Int. J. Mach. Conscious. 04(02), 325-333 (2012) 
33. Ryan, K., Agrawal, P., Franklin, S.: The pattern theory of self in artiﬁ-
cial general intelligence: A theoretical framework for modeling self in biolog-
ically inspired cognitive architectures. Cognitive Systems Research,vol. 62,pp. 
44-56 (2020).https://doi.org/10.1016/j.cogsys.2019.09.018, https://linkinghub. 
elsevier.com/retrieve/pii/S138904171930484X, 7 citations (Crossref) [2023-07-18] 
34. Samek, W., Montavon, G., Lapuschkin, S., Anders, C.J., Müller, K.R.: Explaining 
deep neural networks and beyond: A review of methods and applications. Proc. 
IEEE 109(3), 247-278 (2021) 
35. Snaider, J., Franklin, S.: Vector LIDA. Procedia Comput. Sci. 41, 188-203 
(2014).https://doi.org/10.1016/j.procs.2014.11.103,
https://linkinghub.elsevier. 
com/retrieve/pii/S1877050914015488 
36. Vasu, P.K.A., Pouransari, H., Faghri, F., Vemulapalli, R., Tuzel, O.: Mobile-
CLIP: Fast Image-Text Models through Multi-Modal Reinforced Train-
ing (2024).https://doi.org/10.48550/arXiv.2311.17049, http://arxiv.org/abs/2311. 
17049, arXiv:2311.17049 [cs] 
37. Vaswani, A., et al.: Attention is All you Need. In: Advances in Neural Information 
Processing Systems. vol. 30. Curran Associates, Inc. (2017) 
38. Velik, R.: The neuro-symbolic code of perception. J. Cogn. Sci. 11(2), 161-180 
(2010)

Resource-Relativized Legg-Hutter 
Intelligence 
Kyle J. Fuller(B) 
, Deacon R. Sawyer , James  T.  Oswald  , 
and Thomas M. Ferguson 
Rensselaer Polytechnic Institute, Troy, NY 12180, USA 
kjfulle@gmail.com, {sawyed2,oswalj}@rpi.edu, 
tferguson@gradcenter.cuny.edu 
Abstract. In this paper, we introduce and defend a notion of resource-
relativized Legg-Hutter intelligence, in which the improvements an agent 
gains through the use of resources can be evaluated. We critique an earlier 
notion of oracle-relativized Legg-Hutter intelligence and consider some 
matters that are raised by our deﬁnitions. 
Keywords: Machine Intelligence Measures · Legg-Hutter Intelligence · 
Intelligence and Resources 
1
Introduction 
Recent work by Oswald, Ferguson, and Bringsjord [ 8] introduces a notion of 
oracle-relativized intelligence—a modiﬁcation of the LHI measure in which the 
intelligence of agents (e.g. oracles) that operate in uncomputable environments 
can be meaningfully considered. The conclusion of this work suggests a down-
stream application of the modiﬁed measure in order to quantify improvements 
that a resource contributes to an agent's overall intelligence. This paper critiques 
the suitability of the oracle-relativized measure for this application, noting in 
particular that its design considers resources given not to agents but rather envi-
ronments. We oﬀer an improved measure by showing the equivalence between 
two reasonable models that we propose and examine its features. We consider 
some issues raised by this metric, including the matter of whether a resource can 
be detrimental to an agent's overall intelligence and new criteria through which 
the maximality of Hutter's AIXI agent can be assessed. We end with a discussion 
on the relevance of our work with respect to the treatment of resources in work 
such as Wang's assumption of insuﬃcient knowledge and resources [ 10, 11]. 
2
Background 
In this section, we provide some background material from [ 6] and  [  8] that serve  
as the foundation of our work. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 159-169, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_15

160
K. J. Fuller et al.
2.1
Legg-Hutter Intelligence 
The Legg-Hutter intelligence (LHI) measure [ 6], is a formal measure of the intel-
ligence of an agent. LHI deﬁnes the intelligence of an agent as the expected 
reward on all computable environments, weighted by simplicity. To deﬁne the 
notion of expected reward on an environment LHI uses an agent-environment 
model, standard in reinforcement learning [ 9], which describes the interactions 
between the agent and environment in terms of cycles t=1, 2, \dots, mt = 1, 2, . . . , m. Each cycle  
tt consists of two steps. In the ﬁrst step, an agent takes an action, a_tat, from the  
ﬁnite set of possible actions, \mathcal{A}A. Then, in response, the environment returns a 
percept, observation and reward pair e_t := (o_t, r_t) \in \mathcal{O} \times \mathbb{R}_{\geq 0}et := (ot, rt) ∈O × R≥0, where  \mathcal{O}O is ﬁnite. 
One important note is that LHI, unlike in standard reinforcement learning, does 
not make any assumption that the environment is Markovian. Environments 
may keep histories of previous actions and percepts and use them in deciding 
future percepts. 
Deﬁnition 1. 
The expected value of an agent \piπ's performance in an environ-
ment \muμ is given as follows based on the reward r_iri obtained at cycle ii. 
 \bUnALT{}V^\pi_\mu = \mathbb{E}\left[\sum^\infty_{i=0} r_i \right] \quad 0 \leq V^\pi_\mu \leq 1\eUnALT{} V π
μ = E
 ∞

i=0
ri

0 ≤V π
μ ≤1
Deﬁnition 2. 
Given the set of all computable environments EE and with K(\mu)K(μ)
as the Kolmogorov complexity of \muμ, the LHI of an agent \piπ is deﬁned as 
 \bUnALT{}\Upsilon(\pi) := \sum_{\mu \in E} 2^{-K(\mu)} V^\pi_\mu\eUnALT{} Υ(π) :=

μ∈E
2−K(μ)V π
μ
A signiﬁcant limitation of the above framework is the requirement that every 
environment \mu\in Eμ ∈E is computable 1. This is an  a priori restriction on the domain 
of application of the LHI insofar as there are natural intuitions that, say, an 
agent that could compute uncomputable environments would in principle be 
considered more intelligent than one who could not. The recent [ 8] addresses 
this by introducing a framework to capture these intuitions via a generalization 
\QoppaϘ of the LHI that aptly represents the intelligence of an agent in such broader 
sets of environments. 
2.2
Oracle Relativized Legg-Hutter Intelligence 
Oracle Relativized LHI (ORLHI), from [ 8], makes use of resources that are lever-
aged in the calculation of an agent's intelligence. 2
1 Modern treatments of LHI have environments as lower-semicomputable chronological 
semi-measures, (see [ 5] for a full treatment of this deﬁnition). This distinction is 
important for ensuring environments remain enumerable. 
2 [ 8] refers to resources as "oracles," which is suitable for the particular context of 
evaluating an agent's possessing a solution for an uncomputable environment by 
relying on an oracle. In the present case, we are interested in a slightly broader set 
of applications and refer instead to "resources."

Resource-Relativized Legg-Hutter Intelligence
161
Deﬁnition 3 (ORLHI Resources). [ 8] deﬁnes a resource RR as a possibly 
inﬁnite binary string. 
To deﬁne ORLHI, we need a way of discussing the complexity of a string with 
respect to a resource, deﬁned as follows [ 8, Deﬁnition 6]: 
Deﬁnition 4 (Conditional Kolmogorov Complexity w.r.t. a Resource). 
Let RR be a resource and let l : {0, 1}^* \rightarrow \mathbb{N}l : {0, 1}∗→N be the length of a string. If the encod-
ing of RR is ﬁnite, let `' be the string pairing operator described in [ 5], and U(R ` p)U(R'p)
be a universal Turing machine with the string R ` pR'p on its input tape. Otherwise 
if RR is inﬁnite, we deﬁne U(R ` p)U(R'p) be a universal oracle machine with pp on its 
input tape and RR on its oracle tape. The conditional Kolmogorov complexity of 
a string \sigmaσ with respect to RR is deﬁned as: K(\sigma|R) := \min_p {l(p) | U(R ` p) = \sigma}K(σ|R) := minp{l(p)|U(R'p) = σ}. 
With this machinery in place we can deﬁne \QoppaϘ, the metric that assesses the intel-
ligence of an agent \piπ with respect to a class of environments with resource RR [ 8, 
Deﬁnitions 5 & 7]: 
Deﬁnition 5 (Oracle-Relativized Legg-Hutter Intelligence). Let \piπ be an 
agent, let E^RER be the set of all computable environments that may access a 
resource RR, let  K(\mu|R)K(μ|R) be the conditional Kolmogorov complexity of some \mu \in E^Rμ ∈
ER given a resource RR, and  let  V^\pi_{\mu, R}V π
μ,R be the expected reward of \piπ in \muμ where \muμ
has access to a resource RR. We deﬁne the intelligence of \piπ with respect to an 
resource RR as: 
 \bUnALT{}\Qoppa(\pi, R) := \sum_{\mu \in E^ R} 2^{-K(\mu | R)} V^\pi_{\mu, R}\eUnALT{} Ϙ(π, R) :=

μ∈ER
2−K(μ|R)V π
μ,R
One application of \QoppaϘ proposed by [ 8] is a means of assessing the intelligence 
improvements that an agent can expect to win by appeal to a particular resource. 
Formally [ 8, Deﬁnition 8]: 
Deﬁnition 6. Given a resource RR and agent \piπ, the  helpfulness \BarQoppa¯Ϙ of a resource 
RR to an agent \piπ is deﬁned as follows: 
\bUnALT{} \BarQoppa(\pi, R) &:= \Qoppa(\pi, R) - \Upsilon(\pi)\eUnALT{} ¯Ϙ(π, R) := Ϙ(π, R) −Υ(π)
The authors of [ 8] attempt to justify this with the following example: 
The degree to which... a set of lecture notes improves the intelligence of 
a class is... the degree to which performance of tasks with access to those 
notes (i.e., performance conditioned on the notes) exceeds performance 
without them. 
3
Giving Agents Resources and Evaluating Them 
We interpret\BarQoppa¯Ϙ (Deﬁnition 6) as being deﬁned with the intended intuition that the 
helpfulness of a resource to an agent is the degree of change in intelligence when 
the agent is given access to the resource. However, we argue that \BarQoppa¯Ϙ is instead

162
K. J. Fuller et al.
ultimately deﬁned as the change in intelligence when the agent is moved to a 
class of environments with the resource. Philosophically, this is a distinct notion 
from that which appears to be intended. Borrowing the classroom analogy from 
[ 8], there is a big diﬀerence between giving lecture notes to a classroom versus 
giving them to the examiners. So, we propose \HatQoppaϘ, a new deﬁnition of helpfulness of 
a resource that is more in line with an agent beneﬁting from access to a resource. 
In our work we wish for our resources to be dynamic with respect to time 
and thus make use of a more nuanced yet more general deﬁnition: 
Deﬁnition 7 (Resources). We deﬁne a resource RR as an environment that 
always provides 0 reward. 
We build upon the agent-environment framework, depicted graphically in 
Fig. 1 and operationalized by Algorithm 1. This algorithm takes an agent \piπ and 
an environment \muμ, and gives a generator for the sequence of (action, observation, 
reward) triples produced by the interaction between \piπ and \muμ. We can describe 
V^\pi_\muV π
μ as the expected value of the sum over the reward component of the triples 
yielded by interact(\piπ, \muμ). 
Fig. 1. The Agent Environment Model and Interaction Algorithm 
Fig. 2. Two Possibilities For Resources In The Loop

Resource-Relativized Legg-Hutter Intelligence
163
3.1
Agent-Focused View 
Algorithm 2. Give Agent Resource 
1: function GAR(π, R) 
2:
s ← CHOOSE 
3:
function πR(o, r) 
4:
loop 
5:
a ← π(o, r) 
6:
if s = CHOOSE then 
7:
if a selects query then 
8:
s ← QUERY 
9:
(o, r) ← (0, 0) 
10:
else 
11:
s ← ACT 
12:
(o, r) ← (0, 0) 
13:
end if 
14:
else if s = QUERY then 
15:
s ← CHOOSE 
16:
(o, r) ← R(a) 
17:
else if s = ACT  then 
18:
s ← CHOOSE 
19:
return a 
20:
end if 
21:
end loop 
22:
end function 
23:
return πR 
24: end function 
For suitable agent π and resource 
R, we conceptualize the act of pro-
viding π with R as bundling these 
together into a new agent πR. The  
giveAgentResource operation is 
deﬁned in Algorithm 2. A diagram 
of the interaction between the com-
posite agent πR and an environment 
μ is shown in Figure 2. With πR as 
an agent in its own right, we can 
see from the ﬁgure that the interac-
tion ﬁts the form of the basic agent-
environment framework. Within πR, 
π can act and perceive on behalf 
of the entire composite agent. How-
ever, π also has the option to inter-
act with the resource R that lives 
with it in πR, again through π's 
action and perception. In essence, π 
is bundled with a reward-free "sec-
ond environment" R and can choose 
which of the two environments it 
would like to interact with. 
We have stipulated that \piπ and RR should be "suitable". What we mean here is 
that for any given history, the probability that \piπ eventually chooses to take an 
action in \muμ should be 1. This guarantees that \pi_RπR gives us a probability measure 
on the possible actions, conditioned on the history, making \pi_RπR an agent. What if 
\piπ and RR are not suitable in this way? For example, consider some \piπ that, given 
some history, has nonzero probability of choosing to interact with RR rather than 
acting in \muμ. Then, \pi_RπR as a whole has a nonzero probability, conditioned on the 
history, of failing to take an action in its environment \muμ. While such a \pi_RπR would 
not be an agent, it would still be a partial agent, corresponding to a conditional 
probability semimeasure. Rather than limiting our resource helpfulness metric 
to only suitable \piπ and RR, we can allow it to apply to all agents and resources 
by extending the deﬁnition of expected reward to partial agents. Speciﬁcally, we 
can deﬁne the expected reward V^{\pi_R}_\muV πR
μ
to sum over only cycles of interaction with 
\muμ that actually happen, thereby regarding cycles that never happen as giving 00
reward. We then let \UpsilonΥ and \HatQoppaϘ be based on this extended deﬁnition of VV . 
Deﬁnition 8 (Agent-Based Helpfulness of a Resource). 
\bUnALT{}\bUnALT{} \HatQoppa(\pi, R) &:= \Upsilon(\pi_R) - \Upsilon(\pi)\eUnALT{} \bUnALT{}\bUnALT{} \HatQoppa(\pi, R) &:= \Upsilon(\pi_R) - \Upsilon(\pi)\eUnALT{} Ϙ(π, R) := Υ(πR) −Υ(π)
There are several advantages in deﬁning giveAgentResource in the way 
we do. The fact that \pi_RπR is another (partial) agent of the same input and output

164
K. J. Fuller et al.
type as \piπ makes it very straightforward to conceptualize comparisons between \piπ
and\pi_RπR, as in our deﬁnition of\HatQoppaϘ. The idea of "interacting" with a resource through 
actions and percepts is also quite natural. Consider that we humans interact with 
resources such as laptops on a daily basis and also that a human with a laptop 
can be thought of as a composite agent made up of a human interacting with a 
laptop. Furthermore, since only the ability of \piπ to take actions and percepts is 
used in \pi_RπR, very little is required of the (partial) agent \piπ. 
One philosophical reservation one might have about our deﬁnition of \HatQoppaϘ is 
that \pi_RπR may be a non-total partial agent and furthermore, that \pi_RπR stops getting 
rewards if it stops taking actions. We believe that this is not entirely unnatural, 
however. If one is playing a video game, presses the pause button, and browses 
the game's online help manual for an eternity, one does not take actions in the 
game and ceases to accrue score. From this standpoint, the interaction between 
\pi_RπR and \muμ has ended, so of course the \pi_RπR can receive no more rewards from \muμ. 
The stipulation that an agent receives zero future reward after the end of an 
episode is standard in reinforcement learning [ 9]. 
It is worth noting that our notion of providing an agent with a resource may 
behave unexpectedly if combined with extensions of LHI to negative rewards 
such as in [ 1], as the resource lets the agent end the interaction whenever it 
chooses, allowing the agent to avoid future negative expected reward. 
3.2
Environment-Focused View 
Algorithm 3. Env Gives Resource 
1: function MEGR(μ, R) 
2:
s ← CHOOSE 
3:
function μR(a) 
4:
if s = CHOOSE then 
5:
if a selects query then 
6:
s ← QUERY 
7:
return (0, 0) 
8:
else 
9:
s ← ACT 
10:
return (0, 0) 
11:
end if 
12:
else if s = QUERY then 
13:
s ← CHOOSE 
14:
return R(a) 
15:
else if s = ACT  then 
16:
s ← CHOOSE 
17:
return μ(a) 
18:
end if 
19:
end function 
20:
return μR 
21: end function 
Instead of bundling a resource with 
an agent to produce a compos-
ite agent, we can instead con-
sider bundling the resource with 
the environment to produce a com-
posite environment that allows the 
agent to use the resource. This 
bridges some of the philosophical 
gap between Ϙ and ¯Ϙ as it is more 
environment-focused, but we show 
that it is equivalent to our original Ϙ. 
We deﬁne the operation of bundling 
an environment with a resource to 
be used by the agent with Algo-
rithm 3, and a diagrammatic rep-
resentation of an agent π interact-
ing with the environment μR := 
MEGR(μ, R) is shown in Figure 2. 
Notice that this composite environ-
ment, unlike the composite agents, 
is guaranteed to be total if its inputs 
are total; its deﬁnition contains nei-
ther loops nor recursion.

Resource-Relativized Legg-Hutter Intelligence
165
For use in our second deﬁnition of \HatQoppaϘ, we ﬁrst deﬁne the following variation 
on LHI where the environments provide the agent with a resource RR. 
Deﬁnition 9 (Resource-Relativized Legg-Hutter Intelligence). 
\bUnALT{} \Upsilon_R(\pi) &:= \sum_{\mu \in E} 2^{-K(\mu)} V^\pi_{\mu_R}\eUnALT{} ΥR(π) :=

μ∈E
2−K(μ)V π
μR
Unlike in \QoppaϘ, the Kolmogorov complexity term is not conditioned on RR here 
because we consider each \muμ to be just as a priori probable as before; we are 
simply wrapping each of them. 
Now, we give our second, environment-focused deﬁnition of \HatQoppaϘ: 
Deﬁnition 10 (Environment-Based Helpfulness of a Resource). 
\bUnALT{} \HatQoppa(\pi, R) &:= \Upsilon_R(\pi) - \Upsilon(\pi)\\ \eUnALT{} Ϙ(π, R) := ΥR(π) −Υ(π)
The advantages to deﬁning \mu_RμR as we do mirror those for \pi_RπR. 
3.3
Equivalence of the Agent-Focused and Environment-Focused 
Deﬁnitions of \HatQoppaϘ
Theorem 1. Deﬁnition 8 and Deﬁnition 10 are equivalent. 
Proof. We need only show that \Upsilon(\pi_R) = \Upsilon_R(\pi)Υ(πR) = ΥR(π) via V^{\pi_R}_\mu = V^\pi_{\mu_R}V πR
μ
= V π
μR. We can  for-
mally express what it means for an agent to interact with an environment with 
Algorithm 1. The dynamics of both interact(\pi_RπR, \muμ) and  interact(\piπ, \mu_RμR) cor-
respond the same history-dependent decision process shown in Fig. 2. Moreover,  
both begin with all state in this decision process being identical; both begin with 
\piπ, \muμ, RR, the action-type state, and the position in the decision process being the 
same. As such, it follows by induction that the two decision processes evolve 
identically in probabilities. We have relegated any possible remaining diﬀerences 
in total reward to reside in diﬀerences between which rewards are measured. 
There is indeed some manner of diﬀerence here; interact(\pi_RπR, \muμ) yields just 
the rewards sent by \muμ while interact(\piπ, \mu_RμR) yields all rewards sent by any 
source whatsoever. However, since all rewards received from sources other than 
\muμ are 0 (including RR, as we have assumed), each yields the same total reward. 
The formulation of \HatQoppaϘ as \Upsilon_R(\pi) - \Upsilon(\pi)ΥR(π)−Υ(π) appears closer in spirit to \BarQoppa = \Qoppa(\pi, R) - \Upsilon(\pi)¯Ϙ = Ϙ(π, R)−
Υ(π), as both can be described as the change in the agent's intelligence when the 
environments are given the resource. However, a critical philosophical diﬀerence 
remains: in \HatQoppaϘ, the environments only use the resource to allow \piπ to query it, 
whereas in \BarQoppa¯Ϙ the environments' use of the resource has no such constraint.

166
K. J. Fuller et al.
3.4
Some Basic Results About \HatQoppaϘ
Theorem 2. There exists an agent \piπ for which \HatQoppa(\pi, R) < 0Ϙ(π, R) < 0 for all resources RR. 
Proof. Let \piπ start by taking the action that would request to consult the 
resource, takes this same action forever if it receives a reward of 0 in response, 
and takes random actions forever otherwise. We can see that \Upsilon(\pi_R) = 0Υ(πR) = 0 because 
\pi_RπR fails to perform even a single action on any environment \muμ. Meanwhile, we 
know that \Upsilon(\pi) > 0Υ(π) > 0 because even random actions can give rewards in some 
environments, such as the environment that simply responds with a reward of 1 
to the ﬁrst action that the agent takes. 
We do not consider this to be an abhorrent result, as one can imagine intu-
itive real-life examples of situations where resources are harmful to agents. For 
example, consider a student so interested in reading textbooks that they miss all 
of their exams if a textbook is available, regardless of the textbook or the exams. 
A stronger claim would be that there is some agent \piπ for which \HatQoppa(\pi, R) < 0Ϙ(π, R) < 0 for 
all resources RR and \pi_RπR is always an agent (not only a partial agent). We do not 
try to settle this claim, but we suggest that the above proof may provide an idea 
on how to go about proving this, or a weakening thereof. 
Interestingly, we can also ﬁnd that some agents beneﬁt merely from having 
a resource, regardless of what that resource is. Consider an agent \piπ that can 
only perform a constant number of computational steps before taking an action. 
Given even an inert resource RR that always gives an observation of 00, \piπ could 
bypass its computational limitation by using interactions RR as risk-free "thinking 
time", which can increase its intelligence quite signiﬁcantly. 
We still have yet to discuss "natural" examples of how resources might help 
non-pathological agents. Informally, what we are interested in here are examples 
of resources that help practical agents that genuinely try to maximize long-
term reward, by virtue of the actual content of the observations provided by the 
resource. We give some possible examples below, without formalization or proof. 
Some resources may be helpful in particular environments, for example, a 
"best-move" resource that provides the true optimal action for the environment, 
given a history. We expect some practical agents to learn to feed the history to the 
best-move resource and then to follow its advice, thereby drastically increasing its 
performance in the environment. However, this resource is environment-speciﬁc, 
and we do not generally expect the best-move resource for one environment to 
be very helpful over the entire environment class. 
In contrast, some resources may be more broadly helpful. In similar spirit to 
the "best-move" resource would be a "what would AIXI do" resource that provides 
the agent with the move that would be chosen by the maximally intelligent [ 5] 
AIXI agent introduced in [ 4], given a history. Another resource that might be 
broadly helpful is one that acts like a random-access memory, in other words, one 
that allows the agent to store and read data at addresses. This latter resource 
could be particularly beneﬁcial to agents that do not themselves include the 
ability to store information in such a way, and could be used as space for storing 
knowledge and performing useful computations.

Resource-Relativized Legg-Hutter Intelligence
167
4
Discussion 
The foregoing discussion sets up some worthwhile challenges that we will consider 
in closing. We consider two areas, namely, downstream matters concerning the 
determination of a maximal AIXI-like agent in this framework and opportunities 
to clarify the understanding of the role that resources play in the mathematical 
theory of intelligence. 
4.1
Considerations on AIXI 
The discussion of assessing intelligence of agents with resources bears on the 
criteria by which an agent can be said to maximize the intelligence metric [ 5, 7], 
such as the AIXI agent introduced in [ 4]. It seems reasonable to suggest that an 
important maximizing feature ought to hold of any putative maximally intelli-
gent agent, namely, that no resource could meaningfully improve its performance. 
• 
Resource Maximality of \piπ for all resources RR, \Upsilon(\pi)\geq\Upsilon(\pi_{R})Υ(π) ≥Υ(πR)
Given the framework provided in the present paper, we have a positive answer to 
the question of whether the AIXI agent satisﬁes this criterion. If one deﬁnes an 
agent such that AIXI'_RAIXI′
R acting as AIXI_RAIXIR does except that wherever AIXI_RAIXIR
would fail to respond, AIXI_R'AIXI′
R takes a random action, then \Upsilon(AIXI'_R) \geq \Upsilon(AIXI_R)Υ(AIXI′
R) ≥
Υ(AIXIR). Since  AIXI_R'AIXI′
R is total, it is an agent, whence by maximality, 
\Upsilon(AIXI)Υ(AIXI) \geq≥\Upsilon(AIXI'_{R})Υ(AIXI′
R), so  \Upsilon(AIXI) \geq \Upsilon(AIXI_R') \geq \Upsilon(AIXI_R)Υ(AIXI) ≥Υ(AIXI′
R) ≥Υ(AIXIR). 
But a positive answer here leads to further downstream interesting questions. 
E.g., if one accepts that in the case of some agents there exist resources the 
adoption of which would be to the detriment of their intelligence, is this the case 
for all agents? In particular, is there an RR such that \Upsilon(\mbox{AIXI}_{R})\lneq\Upsilon(\mbox{AIXI})Υ(AIXIR) ⪇Υ(AIXI)? In  
other words, does it hold that for all RR, \Upsilon(\mbox{AIXI}_{R})=\Upsilon(\mbox{AIXI})Υ(AIXIR) = Υ(AIXI)? 
Bearing on this matter is a further natural intuition—although one not sat-
isﬁed by the foregoing framework—that access to resources ought never harm 
the performance of an agent. In other words, one might have expected that for 
any agent \piπ, resource  RR, and environment \muμ the following should hold: 
• 
Resource Improvement for all \piπ, RR, \muμ, V^{\pi_{R}}_{\mu}\geq V^{\pi}_{\mu}V πR
μ
≥V π
μ
The intuition might go, the agent with access to RR always has the choice of 
disregarding RR, whence V^{\pi_{R}}_{\mu}V πR
μ
can never dip below V^{\pi}_{\mu}V π
μ . Of course, the examples 
in Sect. 3.1 show that this intuition is not supported by the present approach. 
Reﬂecting on this intuition, however, has some consequences for the perfor-
mance of an AIXI agent. One observation is that in \BarQoppa(\pi, R)¯Ϙ(π, R), while a resource 
may work in favor of the agent's reward on some environments, that resource 
may also decrease the reward on others. This seems contrary to the intuition 
of Resource Improvement, as an agent may be expected to "do nothing" on the 
environments with respect to which a resource is a disadvantage. The issue here 
is that it can be diﬃcult for an agent to predict whether relying on the infor-
mation provided by the resource would be helpful or harmful. Our approach

168
K. J. Fuller et al.
supports a diﬀerent intuition, which is that a misleading, "lying" resource can 
be worse than no resource at all. 
The ﬁeld of agents, of course, is maximal and agents exist that intuitively 
make irrational choices. Yet this bears on determining AIXI over this deﬁnition; 
an AIXI agent could, in principle, simulate both choices in which a resource 
is leveraged and choices in which it abstains from consulting that resource. In 
such a circumstance, two items need to be made formal: For one, a deﬁnition 
of AIXI must be given in which both such options are represented. For two, 
some manner of adjudicating between these choices must be represented, which 
indicates that there may be a need to reintroduce the conditional Kolmogorov 
complexity of each environment \muμ given the use of a resource RR (or not). We 
leave these matters for future work. 
4.2
Considerations on Resources 
Our work contributes to an ongoing discussion on the role of resources in intel-
ligence measures and AGI systems. We use "resources" in line with our Deﬁni-
tion 7 which we note subsumes the intuitive notion of (knowledge) resources 
as ﬁnite lookup tables (books, cheat sheets, etc.). We contrast our deﬁni-
tion with other works such as Wang [ 11], Legg and Hutter [ 6], and Goertzel 
[ 3] who use "resources" to refer primarily to what we will call computational 
resources, restrictions on time and space bounds for the agent itself. What we 
call "resources" are a more general class of what Wang calls "knowledge" and 
Chollet and Goertzel would call "[knowledge] priors". Wang's working deﬁnition 
of intelligence, ﬁrst proposed in [ 10] and more recently lightly modiﬁed in [ 11], 
is that "[Intelligence is] adaptation with insuﬃcient knowledge and resources." 
but there has been little investigation in a formal measure for the insuﬃciency 
of a (knowledge) resource. Our \HatQoppaϘ provides such a measure for capturing a notion 
of the suﬃciency of a resource in a more general sense, across all environments. 
For future work, leveraging \HatQoppaϘ towards investigating the safety of agents mod-
ulo access to certain classes of resources remains an interesting open question. 
Our interpretation of resources as environments leads to the ability to use exist-
ing environment classes such as those from Hutter [ 5, Figure 7.1]. Additionally, 
the relationships between \HatQoppaϘ and other measures taking resources into account, 
such as Goertzel's pragmatic general intelligence measure [ 3, Deﬁnition 2], and 
Chollet's measure [ 2] remains an open question. 
References 
1. Alexander, S., Hutter, M.: Reward-punishment symmetric universal intelligence. 
AGI (2021) 
2. Chollet, F.: On the measure of intelligence. CoRR abs/1911.01547 (2019) 
3. Goertzel, B.: Toward a formal characterization of real-world general intelligence. 
In: Proceedings of the 3rd Conference on Artiﬁcial General Intelligence, pp. 74-79. 
Atlantis Press (2010/06)

Resource-Relativized Legg-Hutter Intelligence
169
4. Hutter, M.: Universal artiﬁcial intelligence - sequential decisions based on algorith-
mic probability. Texts in Theoretical Computer Science, Springer (2005) 
5. Hutter, M., Quarel, D., Catt, E.: An introduction to universal artiﬁcial intelligence. 
CRC Press, 2024 christmas edn. (2024) 
6. Legg, S., Hutter, M.: Universal intelligence: A deﬁnition of machine intelligence. 
Minds Mach. 17(4), 391-444 (2007) 
7. Leike, J., Hutter, M.: On the computability of Solomonoﬀ induction and AIXI. 
Theor. Comput. Sci. 716, 28-49 (2018) 
8. Oswald, J.T., Ferguson, T.M., Bringsjord, S.: A universal intelligence measure for 
arithmetical uncomputable environments. In: Artiﬁcial General Intelligence - 17th 
International Conference, AGI. Lecture Notes in Computer Science, vol. 14951, pp. 
134-144. Springer (2024) 
9. Sutton, R.S., Barto, A.G.: Reinforcement learning - an introduction. MIT Press, 
Adaptive computation and machine learning (1998) 
10. Wang, P.: On The working deﬁnition of intelligence. Tech. Rep. 94, Center for 
Research on Concepts and Cognition, Indiana University (1995) 
11. Wang, P.: On deﬁning artiﬁcial intelligence. J. Artif. Gen. Intell. 10(2), 1-37 (2019). 
https://doi.org/10.2478/JAGI-2019-0002

A Spatio-temporal Schema Mechanism 
for Developmental Robotics 
Olivier L. Georgeon1(B) 
, Simon L. Gay2 
, and Paul Robertson3 
1 UR CONFLUENCE: Sciences et Humanites (EA 1598), UCLy, Lyon, France 
ogeorgeon@univ-catholyon.fr 
2 LCIS, Université Grenoble Alpes, Valence, France 
simon.gay@lcis.grenoble-inp.fr 
3 DOLL Labs, Lexington, MA, USA 
paulr@dollabs.com 
Abstract. Schema mechanisms are software frameworks designed to 
reﬂect genetic and constructivist theories of cognitive development. Here 
we extend a schema mechanism by incorporating spatial information into 
sensorimotor schemas, and by embedding spatial processing capability in 
the mechanism, inspired by place cells and episodic memory in the mam-
mals' hippocampus. We report an experiment in which a mobile robot 
explores an open environment using only basic sensors: an ultrasonic 
telemeter and an inertial measurement unit. Results show that the robot 
constructs a reliable spatio-temporal graph of schemas and places, allow-
ing the emergence of exploratory behaviors. These results contribute to 
our understanding of the spatio-temporal organization of behaviors in 
animals and suggest methods to create inexpensive robots capable of 
lifelike behaviors. 
Keywords: schema mechanism · intrinsic motivation · developmental 
robotics · place cells 
1
Introduction 
Jean Piaget founded genetic epistemology as an attempt to explain the ontoge-
nesis of cognition and the emergence of knowledge [ 23]. Genetic epistemology 
suggests that cognitive development begins with a sensorimotor stage during 
which the agent learns through interaction and sensorimotor experience. Basic 
elements of cognition are schemas, which correspond to elementary patterns of 
interaction. Through acting on the environment and experiencing the conse-
quences, the agent constructs "mental structures" in the form of networks and 
hierarchies of schemas. The initial focus is on mastering the sensorimotor appa-
ratus and understanding the contingencies between motor actions and sensory 
signals. While some critiques of Piaget have disputed the fundamental role of 
sensorimotor experience—arguing that cognition may emerge even with limited 
sensorimotor input, as in the case of fetuses [ 18]—genetic epistemology remains 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 170-180, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_16

Spatio-temporal Schema Mechanism
171
central to contemporary theories of knowledge development. It is hypothesized 
that the early sensorimotor stage grounds meaning on direct experience and 
forms the foundation for symbolic thoughts, enabling the subsequent develop-
ment of general intelligence. 
Gary Drescher [ 4] coined the term schema mechanism to refer to software 
implementations of Piaget's theories. Schema mechanisms have later been incor-
porated in brain-inspired cognitive architectures such as LIDA [ 16], AERA [ 19], 
and ECA [ 10]. These new implementations extend Drescher's constructivist 
approach by rooting schemas in interactional events rather than static world 
perceptions. Because they do not rely on symbolic perception of the environ-
ment's state, these new schema mechanisms—labeled 2.0 by Olivier Georgeon 
et al. [ 11]—are not limited to simulated environment and can be applied in 
autonomous robotic systems operating in the open world. The present study 
examines how to implement a schema mechanism 2.0 in a simple robot to inves-
tigate the initial sensorimotor stage of cognitive development and simulate the 
ﬁrst steps of lifelike exploratory behavior. 
Independent of research in cognitive development, roboticists have increas-
ingly employed networks of schemas to encode robotic knowledge and task struc-
tures. Notable examples include Schema Networks [ 15] and  ConceptGraphs [ 12]. 
Our study diﬀers from these in that we focus on the active construction of knowl-
edge by a self-motivated robot. Several studies have shown how an agent that 
has no initial presupposition of space could construct a latent representation of 
the spatial structure of its environment using sequences of schemas [ 5, 24, 29]. 
Their computational complexity, however, limits these methods to small simu-
lated environments and makes them inapplicable to robots in the open world. In 
contrast, we encode presupposed spatial properties in schemas and we extend an 
existing schema mechanism to perform spatial operations based on these proper-
ties. We draw inspiration from studies on place cells in the hippocampus, which 
we review in the next section. 
2
Role of Hippocampus in Spatial and Episodic Memory 
Theories of the hippocampal region date back to two key papers that appeared 
in 1971. These two independent works, without cross-referencing, gave rise to 
two tracks for hippocampal theory that later converged. David Marr's theory of 
archicortex [ 17] suggested that the hippocampus serves as a temporary storage 
system where new experiences are stored and that supports associative recall 
whereby experiences can be recalled from partial information. John O'Keefe and 
Jonathan Dostrovsky [ 20] reported experiments, which used rats implanted with 
electrodes in the CA1 region of the hippocampus, and recorded action potentials 
in certain hippocampal neurons that ﬁred only when they were in a particular 
location in the environment. These neurons later became known as place cells, 
and the area they represent as place ﬁeld. Their initial ﬁndings lead the way to 
proposing that the hippocampus functions as a cognitive map [ 21]. These two 
foundational works now form an integrated view of these complementary theories

172
O. L. Georgeon et al.
[ 25]. Here we are particularly interested in the interplay between the sensorimotor 
experience learning and cognitive map aspects of hippocampal function. 
In 2005, Hafting et al. [ 13] discovered grid cells in the medial entorhinal 
cortex (MEC) that are organized in hexagonal grid patterns. Groups of grid cells, 
called modules, support path integration and provide input to place cells. In the 
hippocampus, the CA3 region shows more recurrent connectivity that supports 
pattern completion (Marr's theory), while CA1 serves as an output region with 
more linear spatial tuning (O'Keefe's ﬁndings). CA1 receives processed input 
from CA3 and the entorhinal cortex (EC). Modules of grid cells form multiple 
maps of hexagonal tiles of the environment, with diﬀerent scales, whereas place 
cells denote places of interest [ 27]. Our present study does not directly model grid 
cells, but abstracts away their function by performing path integration through 
geometric calculus as described in the following sections. 
Since the early work by O'Keefe and Nadel [ 21], the ﬁeld of cognitive maps has 
attracted attention from the Artiﬁcial Intelligence [ 31] and Cognitive Science [ 32] 
communities. Jeﬀ Hawkins proposed the thousand brain theory that the brain 
uses numerous local spatio-temporal models to learn about and interact with 
diﬀerent conﬁgurations of the environment [ 14]. Other multidisciplinary work 
includes work on reinforcement learning by Peter Dayan and Nathaniel Daw [ 3], 
and Burgess Frith and Maguire's work on place cells and episodic memory [ 2]. 
Simon Gay et al. [ 6] proposed a biologically inspired navigation model designed 
for assistive technologies intended for visually impaired individuals. Like ours, 
his model centers on the ability to encode the perceptual context from speciﬁc 
spatial positions, and to measure displacement within and across place ﬁelds by 
comparing stored contextual information with the current perceptual input. 
The present study draws a parallel between the role of the hippocampus in 
learning episodes of spatio-temporal events and the conceptual framework of 
schema mechanisms to organize sensorimotor experience. Our implementation 
reﬂects the hypothesis that an animal's spatial representation originates from 
the awareness of its own body position relative to a speciﬁc environmental con-
ﬁguration, accompanied by the encoding of spatio-temporal behaviors adapted 
to this conﬁguration (i.e., place ﬁeld). Subsequently, the animal must learn its 
way to transition between diﬀerent place ﬁelds and recognize known place ﬁelds. 
We consider this foundational mechanism as a precursor to the evolution of more 
complex spatial representations, such as the localization of speciﬁc body parts 
relative to the body, or the relative positioning of individual objects. 
We deﬁne some primitive schemas that allow the robot to identify particular 
place ﬁelds in the environment, and other schemas to move across place ﬁelds. As 
the robot explores its environment, it constructs a spatio-temporal graph that 
we refer to as the episodic graph. Nodes of the episodic graph represent places in 
the environment; they are associated with the schemas that the robot can enact 
to recognize this particular place. Edges represent displacements across places; 
they are associated with schemas that the robot can enact to move from one 
place to another. This spatial organization of schemas enables the enactment of 
active perceptual behaviors that support navigation.

Spatio-temporal Schema Mechanism
173
3
The Schema Mechanism 
A schema mechanism is a system that creates, organizes, and complexiﬁes sen-
sorimotor schemas to generate increasingly intelligent behaviors. In a robot, it 
interacts with a hard-coded interface layer that handles the actuators and sen-
sors. The interface layer receives the intended schema selected by the schema 
mechanism and handles its enaction. This layer also transforms sensory data 
into data structures that we refer to as primitive experiential events as they can 
be interpreted as events experienced by the robot. The schema mechanism is 
"seeded" with a set of primitive schemas deﬁned by primitive actions and experi-
ential events [ 30], akin to innate behaviors in animals [ 1]. This initial seed guides 
the robot's early interactions without needing to be derived from more abstract 
goals or other motivational systems [ 8]. 
The schema mechanism implements a policy that selects actions based on 
predicted experiential events and predeﬁned preferences. For example, Drescher 
implemented a reward-driven policy to control an agent that learns to reach 
predeﬁned goal states in a particular state space [ 4]. Oudeyer et al. implemented 
a curiosity policy that amounts to selecting schemas that have the highest 
expected information gained [ 22]. Georgeon et al. implemented an interactional 
motivation policy aimed at enacting schemas associated with positive valence 
and avoiding those with negative valence, based on initial valences assigned to 
primitive schemas by the experimenter [ 9]. 
The primary focus of this paper is not on policy design; therefore, we used the 
simplest possible policy to demonstrate the construction of the episodic graph. 
It is based on the following schema selection instructions: if the area ahead is 
not scanned then select the scan schema, else if the place is not fully scanned 
then select the turn schema toward the center of the unscanned area, else if the 
area ahead is suﬃciently clear then select the move forward schema, else select 
the turn schema toward the closest clear area. The following sections describe 
the implementation of these schemas. 
4
Experimental Settings 
We use the open source Petitcat 1 mobile robotic platform [ 26] shown in Fig. 1, 
of which we exploit only two sensors: the Inertial Measurement Unit (IMU) and 
the ultrasonic echo-localization sensor (sonar). 
The IMU's gyrometer measures the total yaw during the enaction of a schema 
with an accuracy of \pm 5 %±5%. The compass measures the azimuth (direction of 
North) after enacting the schema with an accuracy of \pm 10^{\circ}±10◦, provided it is prop-
erly calibrated. The linear accelerometer is too imprecise to accurately integrate 
the locomotion speed, let alone the distance traveled. 
The sonar is mounted on a pivoting head that can rotate both sides by 
90^{\circ}90◦. When prompted, it emits an ultrasonic signal within a cone and returns
1 We personalize Petitcat by name and pronoun to enhance readability but we do not 
claim that he has a subjectivity or gender. 

174
O. L. Georgeon et al.
Fig. 1. Screenshot of the video [ 7]. The sonar resembles eyes on Petitcat's head but 
he does not have vision. The top-left insert shows the content of allocentric memory 
after the enaction of a scan schema on Step 1. Small brown points materialize the 
echolocation raw table: distance of the echo every 10^{\circ}10◦. The yellow line materializes 
the echolocation curve that delineates the area considered empty. Brown half-circles 
materialize the echolocation signature: signiﬁcant points that characterize the place. It 
shows that Petitcat detected the moneybox on its left and the plant on its right. 
a measure of the distance of the nearest object (ﬁrst echo) within a range of 
1500mm1500mm. The precision in distance is \pm 5 mm±5mm, while in direction, it corresponds 
to the span of the cone: 60^{\circ}60◦. 
We conﬁgured three primitive schemas: move forward, turn, and  scan. The  
move forward schema consists in translating forward for one second (approxi-
mately 300mm300mm). It is interrupted if an obstacle is detected in front of the robot. 
The turn schema consists in turning in place by an angle provided by the schema 
mechanism. The scan schema consists in performing head saccades every 10^{\circ}10◦in 
the range [-90^{\circ}, 90^{\circ}][−90◦, 90◦] and returning the distance measured on each step. 
The primitive experiential events used in this experiment include the yaw and 
azimuth measured by the IMU and the echolocation raw table and signatures 
explained in the next section. 
5
Learning the Episodic Graph 
On startup, Petitcat creates the ﬁrst node in the episodic graph which references 
the ﬁrst place ﬁeld and serves as the origin of allocentric coordinates. According 
to the policy described in Sect. 3, he enacts a  scan schema, and then a turn by 
180^{\circ}180◦, and then again a scan. From these two scans, he generates the echolocation 
raw table of echo distances every 10^{\circ}10◦around this place. The echolocation raw

Spatio-temporal Schema Mechanism
175
table is stored in polar-egocentric coordinates (absolute directions, centered on 
the place) by translating the origin from the head to the center of the robot 
and rotating the axis by the robot's azimuth. The echolocation curve is then 
calculated by taking the max distance in each cone of \pm30^{\circ}±30◦for each direction 
point. Figure 3 shows the echolocation curve of diﬀerent places in yellow. Next, 
we extract the points of discontinuity of the echolocation curve to obtain the 
echolocation signature, which characterizes the place by the 'edges' of surround-
ing objects (i.e., contrast of echolocation). 
After enacting a move forward schema, Petitcat estimates his new posi-
tion through path integration using the locomotion speed, travel duration, and 
azimuth. If the new position is not within the ﬁeld of an already created node, 
he creates a new node in the graph to refer to this place and an edge associated 
with the move forward schema that led to it. 
If the new position is within the radius of an existing place (parameter preset 
to 200mm200mm), Petitcat adds the edge leading back to this existing node. Petitcat 
estimates his new position within this place ﬁeld and the distance traveled from 
the previous place using the Iterative Closest Point (ICP) algorithm provided by 
the Open3D library [ 33]. Notably, the ICP algorithm requires the presupposed 
displacement as an input attribute, which we obtain from the path integration. 
The echolocation raw table of the new scan is compared with the echolocation 
raw table of the current place to estimate Petitcat's new position in the refer-
ence frame of this place. Additionally, the echolocation signature of this place 
is compared with the echolocation signature of the place where he came from. 
Figure 2 illustrates these comparisons on Step 40. 
In Fig. 2 (left), the result Dist: 83Dist : 83 is the estimated distance in mmmm between 
the position of Petitcat when returning to Place 4 on Step 40 and the position of 
Place 4 established on Step 19, based on a 100%100% match of the re-scanned points 
(FitnessFitness). In Fig. 2 (right), the result Dist: 239Dist : 239 is the estimated distance in mmm
from Place 6 to Place 4 based on a 42%42% match of the signature points (FitnessFitness). 
The estimated distance between places combined with the elapsed time is 
used to progressively tune the locomotion speed parameter, and to adjust the 
distances associated with edges in the episodic graph. We implemented a simple 
layout adjustment mechanism to adjust the length and angles of the edges of 
the graph based on progressive conﬁdence value of nodes. The initial node is 
given a 100%100% conﬁdence. When a new node is created, it is given a discounted 
conﬁdence value from the previous one. When Petitcat returns from a node with 
a lower conﬁdence to a node with a higher conﬁdence, the position of the former 
relative to the latter is adjusted using the new distance measured proportionally 
to the conﬁdence of both places. 
6
Results 
An example run can be seen in a video [ 7] during which an episodic graph of 7 
places was created in 52 interaction cycles. The places were created and revisited 
in the order 1 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$1 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 2 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 2 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 1 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 1 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 3 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 3 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 1 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 1 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 4 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 4 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 1 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 1 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 2 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 2 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 5 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 5 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 1 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 1 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 4 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 4 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 6 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 6 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 4 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 4 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 7 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 7 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 4 \mathrel{\raisebox{0.5pt}{\scalebox{0.7}{}}{$}}} 4 mathrel {aisebox {0.5pt}{scalebox {0.7}{$}ightarrow inlinealttext {detokenize {}}} 6}}{$}} 6$.

176
O. L. Georgeon et al.
Fig. 2. Output of ICP algorithm on Step 40, after the robot returned from Place 
6 to Place 4 and re-scanned Place 4. Left: Re-scan of Place 4. Yellow points: Raw 
echolocation scanned on Steps 17 and 19. Brown points: Raw echolocation scanned 
on Step 40. Black segments: Estimated position diﬀerence between original Place 4 
and re-scan. Right: Estimated displacement from Place 6 to Place 4. Brown points: 
Echolocation signature of Place 6. Yellow points: Echolocation signature of Place 4. 
Black segments: Estimated displacement from Place 6 to Place 4. Brown triangles: 
Echolocation signature of Place 6 translated by the estimated displacement. Light and 
dark blue circles: visualization of the displacement of the robot. (Color ﬁgure online) 
Figure 3 (left) shows a spatio-temporal representation of the episodic graph 
over the ﬁrst 27 steps during which 5 nodes were constructed. Three objects in 
the environment (shown in Fig. 1) shape the echolocation signatures: the money-
box, the plant, and the wall. Petitcat cannot identify them as individual objects, 
but they all contribute to making the echolocation signature of each place unique. 
Figure 3 (right) shows the full graph of 7 places created on Step 52, superimposed 
on the echolocation curve of Place 6. 
Figure 4 (left) shows that the estimated locomotion speed was progressively 
adjusted by approximately -10%−10%. Note that Petitcat cannot infer the absolute 
values of its speed and echo distance measurements (depending on the speed 
of sound parameter), but can only adjust them by ﬁnding a better ﬁt between 
them. 
Figure 4 (right) shows the evolution of the azimuth residual error. This error 
corresponds to the diﬀerence between the actual compass measurement and the 
expected value, which is estimated based on the previous compass measurement 
and the last yaw measurement. As the compass oﬀset is gradually calibrated, a 
slight decrease in the residual error can be observed after Step 30.

Spatio-temporal Schema Mechanism
177
Fig. 3. Left: Spatio-temporal representation of the episodic graph over the ﬁrst 27 
steps. Gray segments represent the edges of the graph. Yellow lines are the echolocation 
curve of each place. Right: Two-dimensional projection of the episodic graph on Step 
52. Petitcat estimates its position within Place ﬁeld 6, slightly West of the origin. The 
yellow line is the echolocation curve of Place 6. (Color ﬁgure online) 
Fig. 4. Left: Estimated locomotion speed when the robot enacts a move forward 
schema. Right: Estimated azimuth residual error over the 52 steps. 
7
Conclusion 
Recent neuroscience research proposes that episodic learning mechanisms in the 
brain may have evolved from mechanisms used to navigate the physical world. 
We investigated how this theory may converge with genetic epistemology. So 
far, schema mechanism implementations have mainly focused on the sequential 
aspect of schemas. Inspired by studies on place cells in the hippocampus, we 
propose an extension of schema mechanisms to process spatial information. 
The aim is not to construct a comprehensive environmental map or to com-
pete with Simultaneous Localization and Mapping algorithms (SLAM), but

178
O. L. Georgeon et al.
rather to replicate primitive mechanisms of knowledge construction that may 
exist in animals and humans. We have focused on the organization of place 
ﬁelds as this capability may serve as a foundational step in the cognitive process 
by which an agent begins to diﬀerentiate and identify individual objects within 
its environment and reason upon them. 
Our implementation has limitations. It relies on hard-coded assumptions to 
handle spatial properties of schemas, and parameters of the schema mechanism. 
We accept baking these assumptions in the schema mechanism as they might 
play a role similar to core knowledge identiﬁed in some brain studies [ 28]. Another 
limitation is that place ﬁeld recognition fails when objects in the environment 
move, limiting the episodic graph to static niches. 
The next step will be to detect signiﬁcant mismatches between expected and 
observed place ﬁelds, which would indicate that the robot has lost its localization. 
In such cases, the robot would save its episodic graph in long-term memory and 
use its current position as a new origin to construct a new graph. If the changes 
in the environment are limited, the robot could construct small local graphs 
that help organize its behavior within speciﬁc environmental conﬁgurations, and 
allow for the recognition of previously visited environmental conﬁgurations based 
on spatial and sequential similarities of local episodic graphs. 
To deal with fast-moving environments and to discriminate between indi-
vidual objects, we expect that more precise sensors, such as cameras, would be 
needed. Due to its low cost and open-hardware design, Petitcat remains nonethe-
less an interesting platform for further investigating the autonomous organiza-
tion of more advanced schemas, with potential applications in attritable robotic 
platforms. 
Acknowledgments. Dr. Robertson acknowledges that this material is based upon 
work supported by the Defense Advanced Research Projects Agency (DARPA), USA 
under Contract No. HR001120C0035. 
Conﬂicts of interest. The authors have no competing interests to declare that are 
relevant to the content of this article. 
References 
1. Blumberg, M.S.: Development evolving: the origins and meanings of instinct. Wiley 
Interdisc. Rev. Cogn. Sci. 8(1) (2017). https://doi.org/10.1002/wcs.1371 
2. Burgess, N., Maguire, E.A., O'Keefe, J.: The human hippocampus and spatial and 
episodic memory. Neuron 35(4), 625-641 (2002) 
3. Dayan, P., Daw, N.D.: Decision theory, reinforcement learning, and the brain. 
Cogn. Aﬀect. Behav. Neurosci. 8(4), 429-453 (2008). https://doi.org/10.3758/ 
CABN.8.4.429 
4. Drescher, G.L.: Made-up minds: a constructivist approach to artiﬁcial intelligence. 
In: Artiﬁcial Intelligence. MIT Press (1991) 
5. Gay, S.L., Mille, A., Georgeon, O.L., Dutech, A.: Autonomous construction and 
exploitation of a spatial memory by a self-motivated agent. Cogn. Syst. Res. 41, 
1-35 (2017). https://doi.org/10.1016/j.cogsys.2016.07.004

Spatio-temporal Schema Mechanism
179
6. Gay, S.L., Pissaloux, E., Jamont, J.P.: A bio-inspired model for robust navigation 
assistive devices. Smart Health 33, 100484 (2024). https://doi.org/10.1016/j.smhl. 
2024.100484 
7. Georgeon, O.L.: Petitcat constructing place cells (2025). https://youtu.be/ 
IXj3d0yVNMc 
8. Georgeon, O.L., Aha, D.: The radical interactionism conceptual commitment. J. 
Artif. General Intell. 4(2), 31-36 (2013) 
9. Georgeon, O.L., Marshall, J.B., Gay, S.: Interactional motivation in artiﬁcial sys-
tems: between extrinsic and intrinsic motivation. In: 2012 IEEE International Con-
ference on Development and Learning and Epigenetic Robotics (ICDL), pp. 1-2. 
IEEE (2012). https://doi.org/10.1109/DevLrn.2012.6400833 
10. Georgeon, O.L., Marshall, J.B., Manzotti, R.: ECA: an enactivist cognitive archi-
tecture based on sensorimotor modeling. Biol. Insp. Cogn. Arch. 6, 46-57 (2013). 
https://doi.org/10.1016/j.bica.2013.05.006 
11. Georgeon, O.L., Perotto, F.S., Thórisson, K.R., Sheikhlar, A., Robtertson, P.: 
Schema mechanism 2.0 for developmental artiﬁcial intelligence. In: Proceedings 
of the international workshop on situated self-guided learning (2025). https://doi. 
org/10.1007/978-3-031-96325-4_4 
12. Gu, Q., et al.: Conceptgraphs: open-vocabulary 3d scene graphs for perception and 
planning (2023). https://arxiv.org/abs/2309.16650 
13. Hafting, T., Fyhn, M., Molden, S., Moser, M.B., Moser, E.I.: Microstructure of a 
spatial map in the entorhinal cortex. Nature 436, 801-806 (2005). https://doi.org/ 
10.1038/nature03721 
14. Hawkins, J., Lewis, M., Klukas, M., Purdy, S., Ahmad, S.: A framework for intel-
ligence and cortical function based on grid cells in the neocortex. Front. Neural 
Circuits 12 (2019) 
15. Kansky, K., et al.: Schema networks: zero-shot transfer with a generative causal 
model of intuitive physics (2017). https://arxiv.org/abs/1706.04317 
16. Kugele, S.: Constructivist procedural learning for grounded cognitive agents. Cogn. 
Syst. Res. 90, 101321 (2025). https://doi.org/10.1016/j.cogsys.2025.101321 
17. Marr, D.: Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. Ser. 
B, Biol. Sci. 262(841), 23-81 (1971). https://doi.org/10.1098/rstb.1971.0078 
18. Moser, J., Schleger, F., Weiss, M., Sippel, K., Semeia, L., Preissl, H.: Magne-
toencephalographic signatures of conscious processing before birth. Dev. Cogn. 
Neurosci. 49, 100964 (2021). https://doi.org/10.1016/j.dcn.2021.100964 
19. Nivel, E., et al.: Autocatalytic endogenous reﬂective architecture (2013) 
20. O'Keefe, J., Dostrovsky, J.: The hippocampus as a spatial map: preliminary evi-
dence from unit activity in the freely-moving rat. Brain Res. 34(1), 171-175 (1971). 
https://doi.org/10.1016/0006-8993(71)90358-1 
21. O'Keefe, J., Nadel, L.: The Hippocampus as a Cognitive Map. Oxford University 
Press, Cambridge (1978) 
22. Oudeyer, P.Y., Kaplan, F., Hafner, V.V.: Intrinsic motivation systems for 
autonomous mental development. IEEE Trans. Evol. Comput. 11(2), 265-286 
(2007). https://doi.org/10.1109/TEVC.2006.890271 
23. Piaget, J.: L'épistémologie génétique. PUF (1970) 
24. Raju, R.V., Guntupalli, J.S., Zhou, G., Lázaro-Gredilla, M., George, D.: Space is a 
latent sequence: structured sequence learning as a uniﬁed theory of representation 
in the hippocampus (2022). https://arxiv.org/abs/2212.01508 
25. Rolls, E.T., Treves, A.: Neural networks in the brain involved in memory and 
recall. Prog. Brain Res. 121, 149-162 (1998). https://doi.org/10.1016/S0079-
6123(08)60550-6

180
O. L. Georgeon et al.
26. Schneider, H., Georgeon, O.L.: Grounding artiﬁcial general intelligence with 
robotics: the petitcat project. In: Samsonovic, A., Liu, T. (eds.) Proceedings of the 
15th International Conference on Brain Inspired Cognitive Architectures. Studies 
in Computational Intelligence, vol. 477. Springer, Heidelberg (2024) 
27. Solstad, T., Moser, M.B., Einevoll, G.T.: From grid cells to place cells: a mathe-
matical model. Hippocampus 16(12), 1026-1031 (2006). https://doi.org/10.1002/ 
hipo.20244 
28. Spelke, E.S., Lee, S.A.: Core systems of geometry in animal minds. Phil. Trans. 
R. Soc. B: Biol. Sci. 367(1603), 2784-2793 (2012). https://doi.org/10.1098/rstb. 
2012.0210 
29. Terekhov, A.V., O'Regan, J.K.: Learning abstract perceptual notions: the example 
of space (2019). https://doi.org/10.48550/arXiv.1907.12430 
30. Thórisson, K.R.: Seed-programmed autonomous general learning. In: International 
Workshop on Self-Supervised Learning, pp. 32-61. PMLR (2020) 
31. Tomov, M.S., Yagati, S., Kumar, A., Yang, W., Gershman, S.J., Tenenbaum, 
J.B.: Discovery of hierarchical representations for eﬃcient planning. bioRxiv: the 
preprint server for biology (2020). https://doi.org/10.1371/journal.pcbi.1007594 
32. Tversky, B.: Cognitive maps, cognitive collages, and spatial mental models. Spatial 
Inf. Theory A Theor. Basis GIS 716, 14-24 (1993). https://doi.org/10.1007/3-540-
57207-4_2 
33. Zhou, Q.Y., Park, J., Koltun, V.: Open3D: a modern library for 3D data processing. 
arXiv:1801.09847 (2018)

A Modular Cognitive Architecture for Collective 
Intelligence Systems 
Amber L. Gibson1envelope symbol
and Dmitry Sokolov2 
1 Consciously Concepted, Los Angeles, CA, USA 
consciouslyconcepted@gmail.com 
2 Moscow Bauman State Technical University, Palmerston North, New Zealand 
Abstract. This paper introduces a modular cognitive architecture for distributed 
collective intelligence via peer-to-peer coordination, federated memory, and 
semantic knowledge integration. It operationalizes a uniﬁed model of distributed 
intelligence by integrating semantic negotiation, context-sensitive reasoning, and 
modular memory federation across decentralized agents. Virtual Cognitive Agents 
(VCAs) utilize dynamic memory graphs and semantic reasoning capabilities to 
interact through a shared semantic layer supporting faceted classiﬁcation, con-
textual tagging, and inter-agent negotiation. The architecture addresses epis-
temic fragmentation of existing AI systems by preserving traceability, adapting 
across conceptual domains, and sustaining semantic coherence at scale. It empha-
sizes explainability, ethical modularity, and interoperability through privacy-
aware, metadata-rich protocols. By supporting participatory sensemaking and 
distributed reasoning, the architecture enables agents—human and artiﬁcial—to 
co-create, reﬁne, and align knowledge across domains. By modeling general-
purpose cognition as a modular and distributed process, the architecture facili-
tates the co-emergence of shared knowledge within multi-agent epistemic envi-
ronments—advancing general-purpose cognitive infrastructures and contributing 
to the evolution of Artiﬁcial General Intelligence (AGI). 
Keywords: collective intelligence cdot peer-to-peer architecture cdot semantic 
reasoning cdot epistemic alignment cdot federated memory cdot artiﬁcial general 
intelligence 
1 
Introduction and Motivation 
Artiﬁcial General Intelligence (AGI) research has increasingly recognized the limitations 
of centralized, monolithic architectures in enabling adaptable, scalable reasoning across 
diverse domains. Human cognition, in contrast, emerges from complex networks of 
distributed memory, semantic integrity, context sensitivity, and intersubjective feedback 
loops. These characteristics are largely absent from deployed AI systems, which often 
fail to account for the epistemological foundations of meaning, the dynamic nature of 
knowledge, and the ethical dimensions of collective reasoning. 
This paper introduces a cognitive architecture designed to address these challenges by 
modeling intelligence as an emergent property of distributed, semantically linked agents
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 181-191, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_17 

182
A. L. Gibson and D. Sokolov
operating in decentralized environments. The platform centers around a modular, peer-
to-peer (P2P) system that integrates memory, inference, and semantic processing within 
a dynamic knowledge commons. It supports contextual reasoning, preserves epistemic 
integrity, and enables participatory co-creation—core prerequisites for scalable, hybrid 
human-AI intelligence across individual, collective, and planetary scales. 
1.1 
Problem Framing 
Current AI systems—especially large language models—lack epistemic traceability, 
contextual integrity, and mechanisms for distributed coordination among autonomous 
agents. Our architecture addresses these gaps by enabling agents to reason across fed-
erated memory, co-create evolving conceptual frameworks, and maintain accountability 
across contexts. In response to semantic fragmentation and systemic complexity, we 
propose a peer-to-peer semantic architecture structured around modular epistemology 
and participatory reasoning. 
This vision aligns with Epistemic Democracy as a holonic process of knowledge 
co-creation and planetary-scale epistemic alignment, and is further grounded in Haber-
mas's concept of the Lifeworld—the intersubjective domain through which communica-
tive action reproduces shared meaning, cultural knowledge, and social integration. The 
architecture thus lays the foundation for cognitively robust systems deﬁned by ethical 
modularity and adaptive co-evolution with human understanding and machine-assisted 
reasoning. 
2 
Background and Related Work 
This research draws from a range of disciplines—including distributed cognition, seman-
tic knowledge representation, collective intelligence, and AGI-oriented architecture— 
each contributing foundational insights into how intelligence can scale across agents, 
environments, and conceptual domains. 
Distributed cognition, as articulated by Hollan et al. [8], conceptualizes cognitive 
processes as extending beyond the individual, emerging through interactions among 
agents, artifacts, and environments. This framework has informed the design of collective 
intelligence systems that leverage coordination technologies to augment group reasoning 
(e.g., Malone et al. [5]). This theoretical orientation has informed a range of collective 
intelligence systems, such as those examined by Malone et al. (2010), which aim to 
augment group-level reasoning through coordination technologies. 
In contrast, cognitive architectures such as SOAR [2] and ACT-R [3] emphasize 
modular memory, procedural reasoning, and the integration of procedural and declarative 
knowledge—typically within individual agents. These models build upon foundational 
theories of symbolic reasoning and cognitive integration (e.g., Newell, Simon), and 
incorporate elements of situated cognition (Hutchins). However, they are not designed 
for distributed coordination across agents or epistemic contexts. 
While these frameworks advance key architectural principles, they often rely on cen-
tralized control or static logic schemas. By contrast, our architecture enables agent-driven 
semantic alignment across federated and evolving contexts—supporting decentralized 
memory negotiation, contextual reasoning, and epistemic pluralism.

A Modular Cognitive Architecture for Collective Intelligence Systems
183
2.1 
Collective Subject 
A collective subject refers to a group or system of agents—human and/or machine— 
capable of shared intentionality, coherent meaning-making, and reﬂexive agency. Unlike 
a mere aggregation of individuals, a collective subject exhibits emergent properties that 
allow it to act, decide, and evolve as a uniﬁed cognitive and ethical entity, not a collection 
of agents. It maintains a degree of interiority (i.e., a "subjective" perspective) that arises 
from the integration of diverse viewpoints, memories, and semantic frameworks into a 
coherent whole. 
In the context of building a global subject, the collective subject becomes the scaf-
folding through which distributed intelligences—across cultures, systems, and tech-
nologies—can self-organize into a planetary-scale cognitive entity. This architecture 
scaffolds the emergence of a planetary superorganism—an integrated cognitive system 
capable of reﬂexive awareness, distributed agency, and coordinated action at global 
scale. Rather than a top-down authority, this global subject is not a top-down authority, 
but an emergent phenomenon arising from deep epistemic integration and participatory 
coherence. While decisions at higher systemic levels may exert obligatory inﬂuence on 
constituent agents, they are ideally shaped through participatory processes that preserve 
autonomy and alignment across scales. 
In the realm of semantic web and ontological modeling, Berners-Lee et al. [7] 
introduced the concept of machine-readable data environments that can evolve through 
linked data and structured vocabularies. However, most current implementations of the 
semantic web lack mechanisms for dynamic reasoning, contextual adaptation, or multi-
agent learning. More recent approaches—such as federated learning [6] and knowl-
edge graphs—have advanced the scalability of distributed learning, yet still fall short of 
fully integrating memory, inference, and semantic coordination within general-purpose 
cognitive agents. 
Our work addresses these shortcomings by embedding participatory reasoning, 
semantic negotiation, and memory traceability as foundational principles across every 
layer of the architecture. This paper builds upon and extends this prior work by propos-
ing an architecture that enables distributed agents to reason across federated memory 
systems using semantically structured, dynamically evolving conceptual maps. Our con-
tribution focuses on enabling generalizable intelligence through composable modules, 
semantic interoperability, and participatory knowledge negotiation. 
3 
System Architecture 
The architecture of the proposed Collective Intelligence Platform consists of four 
interdependent layers: 
3.1 
Agent-Level Cognitive Modules 
Each participating node in the Collective Intelligence system—whether human, artiﬁcial, 
or hybrid—operates as a cognitive module with a localized memory and reasoning 
engine. These agents store context-rich information fragments called semantic atoms,

184
A. L. Gibson and D. Sokolov
each tagged with metadata describing epistemic origin, usage history, and classiﬁcation 
dimensions. These semantic atoms enable traceability and localized epistemic reasoning, 
forming the building blocks for distributed inference and accountability. 
3.2 
Federated Memory Graph 
Information is synchronized through a federated memory graph that supports both private 
and shared knowledge spaces. This graph enables agents to retrieve, align, and recom-
bine semantically similar data across distributed contexts. Unlike centralized knowl-
edge graphs, the federation protocol preserves agency, attribution, and local variabil-
ity, while enabling high-level integration through versioning and trust-weighted ref-
erences—where weights derive from source credibility, usage history, or contextual 
alignment. 
This mirrors Habermas's concept of the Lifeworld [11] and Jung's understanding of 
the collective unconscious [12], wherein each individual operates within its own contex-
tual reality, constructing and maintaining unique conceptual models of the world. The 
"Larger World" emerges as a superposition of these "Small Worlds," where only seman-
tically matching components of individual graphs contribute to a shared knowledge 
graph—reﬂecting a collective ﬁeld of meaning, belief, and coordinated action. 
Memory graphs reside in externalized, private, and secure Conceptual Spaces—one 
for each Subject, Agent, or Assistant, whether individual or collective. Access controls 
are embedded within each Conceptual Space, allowing agents to determine what data 
is shareable, queryable, or private based on provenance and semantic classiﬁcation. For 
collective entities, all data, information, and knowledge intended for shared use must be 
gathered and managed within a common Uniﬁed Conceptual Space (UCS). 
The UCS acts as a container for multiple layers of memory:
bullet Long-Term Memory: structured as knowledge graphs
bullet Working Memory: dynamically connects relevant knowledge nodes during active 
processing
bullet Short-Term Memory: temporarily collects contextualized incoming data and infor-
mation, enabling real-time processing 
Synchronization between agent-local memory graphs and the UCS is governed by 
a commit-review-merge protocol. When agents update their local graphs—whether by 
creating, reclassifying, or discarding semantic atoms—those changes are ﬂagged with 
alignment and contribution relevance), it is asynchronously propagated to the UCS, 
where it undergoes semantic compatibility checks. Conﬂicts or ambiguities trigger an 
ontology alignment routine or request for human-in-the-loop arbitration. This protocol 
ensures that knowledge integration is modular, traceable, and aligned with the collective 
context without enforcing top-down standardization. 
3.3 
Semantic Processing Engine 
At the core of the platform lies a semantic engine that performs faceted classiﬁcation, 
context-aware tagging, and dynamic ontology alignment. It enables the translation of 
natural language or symbolic input into structured conceptual formats interpretable by

A Modular Cognitive Architecture for Collective Intelligence Systems
185
multiple agents. This layer ensures semantic traceability and minimizes drift through 
iterative feedback between classiﬁcation protocols and user-deﬁned meaning structures. 
3.4 
Adaptive Interface Layer 
Finally, an adaptive interface layer provides multimodal access to the platform's cogni-
tive services. This includes user-facing dashboards, dialogue systems, design tools, and 
visualization maps—each capable of reﬂecting the live state of the collective knowledge 
graph. These interfaces are not passive display tools but active semantic mediators that 
shape agent interpretation and user collaboration. 
4 
Semantic Mapping and Knowledge Coordination 
This section addresses a core limitation in current AI and knowledge systems: the inabil-
ity to preserve epistemic integrity and contextual meaning across diverse agents and 
temporal frames. Our platform responds by ensuring that knowledge produced and 
exchanged remains semantically coherent, context-sensitive, and interoperable across 
temporal, disciplinary, and user-deﬁned boundaries. 
The platform acknowledges that only a minimal subset of contextual knowledge can 
be reliably generalized. Even so-called "scientiﬁc" knowledge must undergo continual 
validation and contextual reassessment in alignment with its domain of application—a 
principle drawn from the Methodology of Action (or Action Thinking), which empha-
sizes situated reasoning and adaptive learning. Informed by both Action Thinking and 
Design Thinking paradigms, the architecture supports iterative experimentation, contex-
tual inquiry, and problem reframing as core to participatory knowledge construction— 
guiding the co-evolution of agent reasoning and interface design through feedback-rich 
interaction. 
4.1 
Faceted Classiﬁcation 
Each semantic unit (or "atom") within the system is tagged using a faceted classiﬁcation 
system—a multidimensional structure that enables multiple ways of describing and 
accessing knowledge. Unlike rigid hierarchies, facets support:
bullet Parallel classiﬁcation (e.g., by topic, intent, source, trust level)
bullet Dynamic reclassiﬁcation as context changes and meanings evolve
bullet Cross-domain interoperability without ﬁxed ontologies 
Faceted models allow agents to discover relationships between concepts across 
contexts, making emergent pattern recognition and alignment tractable at scale. 
4.2 
Contextual Tagging and Metadata 
Every knowledge contribution is embedded with rich metadata, including:
bullet Context of creation (who, when, for what purpose)

186
A. L. Gibson and D. Sokolov
bullet Epistemic attributes (certainty, ambiguity, revision status)
bullet Access protocols (ownership, licensing, privacy levels)
bullet Relevance indicators (frequency of use, citation, rating) 
This metadata enables the system to maintain traceability and precision during 
retrieval, update, and comparison processes. 
4.3 
Ontological Alignment 
Unlike most knowledge systems that rely on predeﬁned or static ontologies, our 
architecture introduces agent-mediated ontological alignment through dynamic trans-
lation, negotiation, and contextual reconciliation—thereby enabling ﬂexible, pluralistic 
sensemaking at scale. 
Ontology alignment occurs through vector-based embeddings, schema reconcilia-
tion, and human-in-the-loop resolution when ambiguity is detected. These mechanisms 
help agents detect mismatches, inconsistencies, or conﬂicting frames and either resolve 
them algorithmically or escalate them for collective arbitration. 
Mechanisms include:
bullet Semantic embedding translation (e.g., via vector space mappings)
bullet Schema reconciliation algorithms
bullet Negotiation protocols for collaborative alignment 
This enables agents with different worldviews, terminologies, or models to co-
construct shared understanding without semantic loss or top-down standardization. As 
part of this process, agents may apply marking protocols to designate knowledge frag-
ments for inclusion, exclusion, or contextual anchoring—providing semantic boundary 
indicators that guide alignment across reasoning cycles. 
4.4 
Concept Evolution 
Concepts evolve through agent interaction and versioned feedback, governed by embed-
ded epistemic metrics—such as relevance, contradiction detection, and user trust 
signals—ensuring rigor over drift. 
Tracked across reﬁnements and contextual updates:
bullet Linked to synonyms, oppositions, and alternative deﬁnitions
bullet Prioritized by epistemic weight (usefulness, coherence, feedback) 
This supports the co-evolution of semantic structures and continuous reﬁnement of 
shared meaning. 
4.5 
Collective Sensemaking 
Agents—human or AI—can be delegated speciﬁc tasks by a Subject and may act on 
their behalf, with autonomy constrained by user-deﬁned or collective consent protocols. 
Agents may also temporarily act as Subjects within deﬁned contexts. These actions may 
be autonomous (rule-based) or automated (explicitly governed). 
At scale, these semantic mechanisms enable collective sensemaking, where agents 
engage in:

A Modular Cognitive Architecture for Collective Intelligence Systems
187
bullet Collaborative ﬁltering to surface relevant knowledge
bullet Semantic negotiation to resolve contradictions and ambiguity
bullet Contextual merging to consolidate overlapping meanings while preserving nuance 
This allows agents with divergent worldviews or domain-speciﬁc vocabularies 
to converge on shared outcomes without requiring semantic uniformity—preserving 
diversity while enabling coordination. 
5 
Virtual Cognitive Agents 
While many current Personal AI Assistants rely on opaque large language models with 
no persistent memory or semantic reasoning, our Virtual Cognitive Agents (VCAs) are 
designed to serve as modular, transparent, and context-aware cognitive partners. They are 
built not to imitate conversation, but to co-construct meaning across evolving knowledge 
contexts. 
Within the architecture, each VCA is a semantically aware software entity capable of 
engaging in knowledge construction, contextual reasoning, and participatory dialogue 
with other agents and users. VCAs are designed not merely to process information, but 
to interpret, classify, and contribute meaningfully to the evolving knowledge commons. 
5.1 
Core Capabilities 
Each VCA is equipped with an associative memory graph that stores conceptual rela-
tionships, interaction histories, and conﬁdence metrics. Core functions include seman-
tic retrieval, contextual reasoning, knowledge reclassiﬁcation, and concept translation 
across divergent terminologies or ontologies. These capabilities enable VCAs to func-
tion in distributed, evolving environments without relying on static inference chains or 
predeﬁned logic trees. 
5.2 
Epistemic Alignment and Dialogic Functionality 
VCAs are not just inference engines—they are conversational entities designed to oper-
ate as cognitive partners in dynamic sensemaking contexts. Through dialogic interaction, 
user intent modeling, and epistemic transparency, they surface sources, limitations, and 
reasoning paths, enabling alignment and concept evolution across agents and subjects. 
This enables them to act as cognitive partners across complex workﬂows like research 
synthesis, design iteration, and strategic planning. Dialogic interaction drives epistemic 
alignment and concept evolution across agents and subjects. 
To ensure trust and accountability, VCAs follow three principles:
bullet Explainability - Justify recommendations, surface ambiguity, and reference under-
lying sources
bullet Traceability - Log all classiﬁcations, inferences, and actions with contextual 
metadata
bullet Privacy - Enforce consent-aware data access and encryption protocols

188
A. L. Gibson and D. Sokolov
Each VCA maintains a versioned reasoning log—recording inference paths, deci-
sion justiﬁcations, and contextual states at the time of action. These logs are indexed 
via semantic hash identiﬁers and are queryable through provenance-aware protocols, 
supporting reproducibility, post hoc analysis, and independent veriﬁcation. These mech-
anisms ensure that VCAs act not only as intelligent agents, but as accountable epistemic 
participants—capable of aligning with shared knowledge standards, justifying their 
contributions, and participating in the ongoing reﬁnement of collective understanding. 
5.3 
Ethical Modularity 
VCAs support modular ethical extensions that embed normative reasoning directly into 
their architecture:
bullet Bias Detection & Contextual Relevance - Identify structural imbalance or epistemic 
dominance as context-sensitive deviations from declared ethical baselines, scoped to 
user- or community-deﬁned ethical proﬁles
bullet Value Alignment Settings - Customize reasoning to reﬂect individual, organiza-
tional, or cultural ethics
bullet Conﬂict Resolution Tools - Surface resolution templates or expert referrals via the 
Resource Management Agent 
In this model, bias is not treated as a universal ﬂaw, but as a contextual deviation 
from shared ethical reference points. These ethical modules are structurally encoded into 
the system, enabling anticipatory alignment, participatory values clariﬁcation, and ethi-
cally bounded autonomy. This design supports responsible AI-by-design—an essential 
condition for trustable general-purpose cognitive infrastructure. 
Appendix 1: Implementation and Future Trajectories 
This modular cognitive architecture advances general-purpose, semantically coordi-
nated, and ethically aligned artiﬁcial intelligence. It addresses limitations in organiza-
tional memory—semantic fragmentation, ephemeral context, and untraceable updates— 
by enabling contextualized, interoperable collective memory. Epistemic traceability sup-
ports interoperability across domains while preserving semantic pluralism, enabling 
knowledge integration without collapsing divergent worldviews. 
Unlike systems that prioritize efﬁciency over meaning or generalization over trans-
parency, this architecture enables the co-evolution of meaning through concept reﬁne-
ment, dialogic coordination, and federated memory. Faceted classiﬁcation, contextual 
tagging, and collaborative ﬁltering together sustain ambiguity resolution, cross-domain 
interoperability, and participatory epistemology. Together, these mechanisms support 
key AGI capacities: scalable reasoning through modular agent coordination, generaliz-
ability via faceted semantic adaptation, and ethical integrity through provenance-aware, 
context-sensitive learning. 
By embedding traceable learning, explainability, and value alignment at the architec-
tural level, the system supports core AGI capabilities: scalable reasoning through modu-
lar coordination, generalizability via semantic adaptability, and ethical integrity through

A Modular Cognitive Architecture for Collective Intelligence Systems
189
contextualized agent behavior. This provides a foundation for continuous, co-constructed 
intelligence at planetary scale. 
Deployment Pathways 
While conceptual in scope, the architecture is grounded in emerging prototypes. Core 
components—such as faceted classiﬁcation, federated memory graphs, and contextual 
tagging—are already in development across research and collaboration platforms. 
Initial deployment paths include:
bullet Semantic Research Networks - Integrating versioned knowledge graphs into 
distributed research environments for versioned knowledge negotiation
bullet Design-Integrated VCAs - Embedding reasoning agents in co-authoring and 
collaboration tools for pattern matching and memory-supported workﬂows
bullet Agent-Powered Dashboards - Supporting interdisciplinary synthesis and planning 
across teams via semantic wikis and modular ontological overlays 
Planned simulations will assess reasoning performance, semantic drift tolerance, 
and ontology alignment in complex environments. These experiments aim to stress-test 
the architecture under conditions of epistemic uncertainty, measuring its adaptability to 
real-world cognitive ecosystems. 
Together, these directions advance a vision of AGI not as an isolated system, but as a 
scaffold for epistemically aligned, ethically grounded collective intelligence—adapted 
to the complexity of our planetary era. 
This architecture serves not only as a scaffold for scalable reasoning, but as civic 
infrastructure for participatory epistemology and collective sensemaking. By embedding 
traceability, contextual transparency, and semantic interoperability, it enables knowledge 
co-creation as a public process. Virtual Cognitive Agents—governed by ethical modu-
larity and consent-aware protocols—support communities in adapting system behavior 
to shared values. In doing so, the architecture fosters epistemic democracy, augmenting 
both cognition and civic deliberation at scale. 
Conclusion 
Rather than treating intelligence as an isolated capability, this platform frames it as 
infrastructure for collective sensemaking—embedding traceability and semantic align-
ment into participatory knowledge systems. Virtual Cognitive Agents, governed by eth-
ical modularity and consent-aware protocols, enable adaptive, value-aligned participa-
tion in knowledge creation. In doing so, the architecture functions as civic technology, 
amplifying both cognition and democratic deliberation at scale. 
Appendix 2: Terminology and Deﬁnitions 
Associative Memory Graph: A dynamic memory structure used by VCAs that encodes 
relationships between concepts, interaction histories, and conﬁdence scores. Enables 
adaptive retrieval and reasoning based on context.

190
A. L. Gibson and D. Sokolov
Bias Detection and Contextual Relevance: Ethical reasoning routines that detect 
structural imbalance, epistemic dominance, or contextual misalignment. Bias is inter-
preted not as an objective ﬂaw but as a context-relative deviation from an explicitly 
declared ethical baseline. 
Collective Subject: A dynamic epistemic entity formed through the dialogic and cog-
nitive coordination of multiple agents (human and artiﬁcial), whose shared reasoning 
processes, memory structures, and semantic agreements constitute a distributed center 
of perspective and knowledge evolution. 
Consent-Aware Protocols: Privacy-respecting control layers that govern access to 
memory, data, and interaction history based on scoped user consent. These protocols 
enable selective visibility, granular permissions, and auditability of agent behavior. 
Context-Aware Tagging: A metadata process that attaches semantic labels to infor-
mation fragments based on the contextual parameters in which they were generated or 
retrieved. 
Dialogic Functionality: The capacity of Virtual Cognitive Agents (VCAs) to engage 
in real-time, context-sensitive dialogue that supports collaborative problem-solving, 
semantic negotiation, and epistemic alignment. 
Epistemic Origin: The provenance of a knowledge artifact, including its source, 
context of emergence, interpretive frame, and transformation history. 
Ethical Modularity: A composable design approach that enables VCAs to integrate, 
adapt, or swap ethical subcomponents—such as bias detection, value alignment, or 
consent protocols—based on user-deﬁned or community-speciﬁc ethical norms. 
Hybrid Nodes: Interface points in the cognitive architecture that mediate between 
human agents, virtual cognitive agents (VCAs), and external systems. 
Marking: A system-level function that designates knowledge fragments for speciﬁc 
epistemic roles (e.g., tentative, veriﬁed, deprecated, disputed). 
Ontology Alignment: A process through which divergent conceptual frameworks or 
domain models are mapped onto one another to enable shared understanding across 
agents. 
Participatory Epistemology: A knowledge formation paradigm in which multiple 
agents—human or artiﬁcial—actively co-create, validate, and evolve shared understand-
ing. 
Semantic Drift: The gradual transformation of meaning or interpretation associated 
with a term, concept, or knowledge structure over time or across agents. 
Semantic Pluralism: The ability to support multiple coexisting meaning systems or 
ontologies without enforcing convergence. 
Value Alignment Settings: Customizable parameters within VCAs that tune reason-
ing strategies, decision logic, and conﬂict resolution mechanisms to reﬂect the ethical, 
cultural, or organizational values of the context in which they operate.

A Modular Cognitive Architecture for Collective Intelligence Systems
191
Virtual Cognitive Agent (VCA): An AI agent equipped with semantic memory, dia-
logic capacity, ethical submodules, and context-aware reasoning—designed to operate 
autonomously or collaboratively in evolving knowledge ecosystems. 
References 
1. Anderson, J.R.: How Can the Human Mind Occur in the Physical Universe? Oxford University 
Press, Oxford (2007) 
2. Arendt, H.: Truth and politics. In: Between Past and Future, pp. 227-264. Penguin Books, 
London (1967) 
3. Berners-Lee, T., Hendler, J., Lassila, O.: The semantic web. Sci. Am. 284(5), 28-37 (2001) 
4. Dafoe, A., Bach, S.H., et al.: Open problems in cooperative AI. In: NeurIPS Workshop on 
Cooperative AI (2021) 
5. Goertzel, B.: Artiﬁcial general intelligence: concept, state of the art, and future prospects. J. 
Artif. Gen. Intell. 5(1), 1-46 (2014) 
6. Gruber, T.R.: Toward principles for the design of ontologies used for knowledge sharing. Int. 
J. Hum.-Comput. Stud. 43(5-6), 907-928 (1995) 
7. Habermas, J.: The Theory of Communicative Action. Beacon Press, Boston (1984) 
8. Hollan, J., Hutchins, E., Kirsh, D.: Distributed cognition: toward a new foundation for human- 
computer interaction research. ACM Trans. Comput.-Hum. Interact. 7(2), 174-196 (2000) 
9. Jung, C.G.: The Archetypes and the Collective Unconscious. Princeton University Press, 
Princeton (1969) 
10. Kairouz, P., McMahan, H.B., et al.: Advances and open problems in federated learning. Found. 
Trends Mach. Learn. 14(1-2), 1-210 (2021) 
11. Malone, T.W., Laubacher, R., Dellarocas, C.: The collective intelligence genome. MIT Sloan 
Manag. Rev. 51(3), 21-31 (2010) 
12. Newell, A., Laird, J.E.: Uniﬁed Theories of Cognition. Harvard University Press, Cambridge 
(1991)

OpenCog Hyperon: A Practical Path 
to Beneﬁcial AGI and ASI 
Ben Goertzel1,2(B) 
1 SingularityNET Foundation, Amsterdam, The Netherlands 
ben@singularitynet.io 
2 TrueAGI Inc., Seattle, USA 
Abstract. We review the OpenCog Hyperon AGI architecture, which 
couples a self-modifying metagraph (the Atomspace) with the MeTTa 
pattern-rewriting language to realise a fully reﬂexive cognitive substrate. 
On top of that substrate the PRIMUS cognitive model instantiates work-
ing, declarative, and procedural memories, an attention economy, and 
a rapid goal-driven cognitive cycle.
We argue that this stack satis-
ﬁes every functional role identiﬁed by the Common Model of Cogni-
tion, while remaining light enough to serve as a seed architecture for 
open-ended self-improvement. Large-language models, formal reason-
ing engines, and evolutionary program learners all plug into Hyperon 
as specialised "lobes," yet the integrative hub remains the Atomspace 
itself. Running on the decentralised MeTTaCycle fabric, Hyperon avoids 
single-point capture and enables plural governance of its goal-evolution 
dynamics. We sketch a staged roadmap shows how, given a production 
Hyperon stack by late 2025, one might plausibly reach human-level AGI 
within a few years and then scale toward beneﬁcial super-intelligence. 
1
Introduction 
The symbolic/subsymbolic dichotomy that has plagued AI since its early days 
has reached a fascinating point now. Transformer-based large-language models 
display striking competencies, yet they lack ﬂexibly dynamic long-term memory, 
explicit goal systems, and the self-reﬂective loops needed for open-ended adapta-
tion. Symbolic systems, in contrast, oﬀer clarity and self-modiﬁcation but have 
struggled to match the perceptual breadth of deep learning. OpenCog Hyperon 
[ 1] seeks to bridge this divide by embedding neural, logical, and evolutionary 
processes inside a single metagraph whose elements can rewrite one another in 
real time. 
At the heart of Hyperon is the Atomspace: a distributed metagraph where 
data, code, proofs, and goals are all stored as ﬁrst-class Atoms. MeTTa, a  
homoiconic pattern-rewriting language, treats those Atoms as both program and 
memory, allowing the system to inspect, generate, and optimise its own proce-
dures. The PRIMUS cognitive model layers a rapid goal-selection cycle, attention 
allocation, and multi-store memory over this substrate, while the Space API lets 
shards of the metagraph reside on blockchains, cloud clusters, or edge devices. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 192-202, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_18

OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI
193
Together these components furnish a compact yet complete seed-AGI capable of 
self-directed growth, immune to monopolistic control, and primed for coopera-
tive co-evolution with humanity. 
This paper gives a sort of "extended abstract" overview of Hyperon and 
PRIMUS. In a brief conference paper it's not feasible to eﬀectively convey what 
Hyperon and PRIMUS are at a technical level; our aim is instead to convey what 
kinds of things they are, with an aim to give the reader incentive to dig further 
into the related papers and code providing depth and detail. 
2
Hyperon Architecture 
Hyperon's Atomspace - a generalized hypergraph (or "metagraph") in which 
both data and code are uniformly represented as nodes and links (Atoms) - 
comprises a uniﬁed meta-representation enabling rich, self-modifying knowledge 
structures. Subgraphs may encode declarative facts, procedural rules; fuzzy pat-
terns of attention, inclination or value; or entire complex programs, computa-
tional models or logic systems. 
Wrapped around the Atomspace is MeTTa, a novel programming language 
that blends functional, logic, and metagraph-rewriting paradigms [ 10]. MeTTa 
programs are themselves sub-metagraphs of Atomspace: pattern-matching is its 
core operation, but control ﬂow, equality reasoning, and even type checking 
emerge from rewriting rules stored in the Atomspace. This design makes self-
modiﬁcation and reﬂective code evolution natural-MeTTa code can query, gen-
erate, and transform other MeTTa code entirely within the same metagraph. 
To achieve industrial-grade scalability and decentralization, Hyperon deﬁnes 
a Space API allowing multiple specialized "spaces" beyond the in-RAM Atom-
space: 
- Distributed Atomspace (DAS): A MongoDB/Redis backend shards the 
metagraph across clusters for large-scale knowledge storage. 
- Rholang Atomspace: MeTTa programs compile to Rholang smart-contract 
code, enabling secure, concurrent execution on blockchain-inspired platforms. 
- Neural Atomspace: Deep neural modules (e.g. LLMs or vision nets) are 
wrapped via the Space API so that activations and embeddings appear as 
Atoms, seamlessly integrating subsymbolic modules with symbolic reasoning 
Unlike today's LLMs-whose capabilities are impressive yet ultimately closed-
scope-Hyperon aims for open-ended intelligence, combining symbolic knowledge, 
procedural learning, and self-transcendent dynamics. By unifying representation, 
computation, and deployment in a single metagraph-based framework, Hyperon 
oﬀers a ﬂexible infrastructure for experimenting with alternative cognitive archi-
tectures, scaling AGI algorithms, and steering toward beneﬁcial superintelli-
gence.

194
B. Goertzel
3
PRIMUS Cognitive Architecture 
Hyperon is a very ﬂexible implementation substrate, however one of the guid-
ing motives behind its development has been the implementation of a speciﬁc 
cognitive architecture known as PRIMUS 1 [ 1, 6, 7] PRIMUS structures the Atom-
spaces of a Hyperon instance into modules for episodic memory, working memory, 
procedural memory, perception, and action selection- associating speciﬁc learn-
ing and reasoning methods with each of these aspects, carefully designed for 
"cognitive synergy" between these methods, allowing them to share intermedi-
ate states and results and give each other guidance. The dynamics are designed 
to mirror human cognitive cycles while also supporting robust long-term back-
ground thinking and reﬂective self-improvement. 
From a cognitive-systems theory and philosophy of mind perspective, 
PRIMUS rests on the following principles: 
- Modular Functionalism: The mind is decomposed into interacting modules 
(perception, memory, action, reasoning) whose coordination yields general 
intelligence. 
- Global Workspace: A working memory buﬀer acts as a blackboard or 
"global workspace" where disparate modules exchange information, mirroring 
theories of conscious access. 
- Cognitive Synergy: Distinct reasoning processes (logical, statistical, evolu-
tionary) cooperate-when one process stalls, another can reframe the problem- 
reﬂecting the brain's multi?strategy problem solving. 
- Embodied, 
Enactive 
Cognition: Although abstracted for software, 
PRIMUS presumes intelligence arises through continuous sensorimotor cou-
pling with an environment, supporting situated action and learning. 
- Reﬂective Self-Modeling: The system represents its own cognitive state 
and goals, enabling meta-cognition and self-improvement-key for open?ended, 
adaptive agency. 
- Self-Organization and Autopoiesis: The system is continually reorganiz-
ing its own fundamental structures, rewriting its own core dynamics, and 
rebuilding its knowledge of all kinds 
- Fuzzy, Probabilistic Semantics: Knowledge and goals carry graded conﬁ-
dence values, reﬂecting human-like uncertainty management and approximate 
reasoning. 
Greater depth on these and other related points can be found in a lengthy, 
related paper on The General Theory of General Intelligence [ 2]. 
From an operational perspective, some of PRIMUS's key features are: 
- Cognitive Cycle: A rapid loop (\sim∼50 ms) in which small set of deliberate 
actions is selected per cycle, imposing coherence over complexly parallel sub-
systems.
1 previously referred to as CogPrime. 

OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI
195
- Working Memory: A transient buﬀer Atomspace that holds symbolic and 
subsymbolic representations drawn from perception, action, declarative mem-
ory, and procedural directives. 
- Declarative Memory: A metagraph-structured store of facts and concepts, 
indexed for eﬃcient pattern retrieval and guided by probabilistic metadata, 
transformed by constant PLN inference and infused by new concepts arising 
via blending and other heuristics 
- Episodic Memory: A hypervector embedding providing eﬃcient associative 
lookup into Atomspace subgraphs recording life-experiences 
- Procedural Memory: A set of skills and action-selection rules (plans, con-
trol schemas) that drive the cognitive cycle, including rule-based inference 
and evolutionary learning modules (e.g. MOSES) 
- Attention Allocation: A mechanism (ECAN, Economic Attention Alloca-
tion) to focus computational resources on the most promising cognitive paths, 
implemented via adaptive resource weighting across candidate cognitive pro-
cesses 
- Pattern Mining: Continuous discovery of frequent subgraphs and motifs in 
Atomspace to support concept formation and abstraction. 
- Algorithmic Chemistry: Small MeTTa programs living in Atomspace 
rewrite each other recursively in complex networks, achieving a rich autopoi-
etic creativity 
- Multi-Modal Integration: Modules for language, vision, action and rein-
forcement learning that interact through the common working and long term 
memories 
- Goal Management: Dynamic reﬁnement, scheduling and arbitration of mul-
tiple concurrent goals, enabling adaptive problem decomposition, guided by 
the OpenPsi motivational framework 
- Reﬂective Self-Modeling: MeTTa-language-based facility for the system to 
represent and reason about its own cognitive state and processes, supporting 
self-improvement. 
To realize PRIMUS in Hyperon, one implements components instantiating 
the above features as MeTTa programs operating over the Atomspace: 
- The Atomspace provides a uniﬁed metagraph in which PRIMUS's memories, 
rules and patterns are all Atoms. 
- MeTTa encodes PRIMUS's procedural rules, attention mechanisms and learn-
ing operators as subgraphs that rewrite the Atomspace. 
- Hyperon's Space API enables distributed, blockchain-backed and neural-
integrated variants of PRIMUS's memory and inference modules, enabling 
extreme scalability and deployment ﬂexibility. 
4
Hyperon and PRIMUS vs. LLMs 
While Hyperon and PRIMUS represent a quite diﬀerent conceptual and devel-
opment direction than LLMs, the recent successes and popularity of the latter 
behoove us to compare and contrast the two approaches.

196
B. Goertzel
LLMs exhibit impressive text-generation, in-context learning and multi-
modal abilities, yet those strengths rest on a narrow foundation: pattern com-
pletion over a ﬁxed training distribution. Because they generalize only a "little 
farther" than the data they have ingested, they remain closed-scope systems- 
tightly bounded by what the Internet already contains rather than capable of 
leaping into genuinely novel conceptual terrain [ 3]. 
Current LLMs have no editable working memory, no long-term multi-store 
memory, and no cyclic goal/attention loop like the human cognitive cycle. They 
cannot inspect or rewrite their own code, nor switch among reasoning styles 
when one approach stalls-abilities the CogPrime-Hyperon stack deems vital for 
open-ended growth. Lacking this "cognitive synergy," they cannot let symbolic, 
neural, or evolutionary processes bail each other out. 
These gaps undermine systematic multi-step reasoning, long-horizon plan-
ning, and robust causal modeling. LLMs may mimic ethical or strategic talk for 
a few turns, but they cannot commit new knowledge, reﬁne it over months, or 
manage evolving goals. Their knowledge and motivations freeze at training time; 
in-session learning vanishes with the context window-untenable for HLAGI or 
ASI, which must keep reorganizing and expanding in uncertain worlds. 
Hyperon therefore treats LLMs as plug-in perceptual or linguistic "lobes," 
not as the cognitive hub. The hub must reside in a metagraph that holds editable 
memories, self-rewriting code, heterogeneous reasoners, and distributed deploy-
ment. LLMs contribute language ﬂuency and pattern recall, but the integrative, 
goal-driven, open-ended cognition lives in Atomspace/MeTTa. 
4.1
Hyperon and PRIMUS vs. LLMs Under the Common Model 
of Cognition 
A more systematic perspective on the diﬀerences between LLMs and Hyperon 
is provided by the Common Model of Cognition (CMC) [ 9], which identiﬁes 
several core components of human-like minds, e.g. a fast cognitive cycle, a global- 
workspace style working memory, long-term declarative and procedural stores, 
attention mechanisms, and continual learning [ 9]. The table below contrasts how 
Hyperon and contemporary large-language models (LLMs) satisfy each module. 
Hyperon's claim to suﬃciency for human-level AGI may be framed in terms 
of the observation that every functional role identiﬁed in the Common Model 
of Cognition can be mapped to a concrete, already (at least at the prototype 
level) implemented mechanism in the Atomspace/MeTTa substrate. A mutable 
sub-graph plays the role of working memory; the same metagraph, persisted with 
probabilistic weights updated via PLN (Probabilistic Logic Network) reasoning, 
serves as declarative memory; procedure learning via evolutionary (MOSES) 
and other methods generate behavioral knowledge; and ECAN (attractor neural 
net like Economic Attention Allocation) guides attention, goal scheduling, and 
motivation. Sharing a single meta-representational framework lets the system 
rewrite any module on the ﬂy, meeting PRIMUS's demand for a self-organising 
network that can both preserve its identity and restructure itself when current 
patterns fail.

OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI
197
CMC Module
Hyperon Realisation
LLM Realisation 
Working
mem-
ory/Global 
workspace 
Explicit
buﬀer
in
the 
Atomspace;
MeTTa
rules 
read/write
each
\sim∼50 ms 
cycle 
Transformer
context
win-
dow;
ﬁxed
length,
no 
explicit
read/write
API, 
cleared after completion 
Declarative memory
Persistent
metagraph
of 
facts
with
probabilistic 
truth
values;
pattern- 
matched queries 
Implicit in weight matrices; 
not addressable or incremen-
tally updated 
Procedural memory
MeTTa subgraphs encode 
skills, PLN rules, MOSES 
programs; self-modiﬁable 
No
separable
procedure 
store; same mechanism as 
text generation 
Attention allocation
ECAN
dynamically
re- 
weights
Atom
salience, 
steering reasoning resources 
Token attention optimises 
next-token likelihood only; 
not goal-directed 
Cognitive
cycle
& 
Goal management 
Few
deliberate
acts
per 
tick; goals stored as Atoms, 
reﬁned and scheduled 
No
cyclic
controller,
no 
intrinsic goal representation 
Learning
Online Hebbian, evolution-
ary, probabilistic and rule 
induction 
Heavy oﬄine pre-training; 
only ephemeral in-context 
adaptation 
Perception/Motor 
grounding 
Space
API
wraps
vision 
nets, robots, smart contracts 
as neural or Rholang Atom-
spaces 
Vision-language
variants 
ingest
pixels
but
output 
text; no motor interface 
Reﬂective self-model System
represents
and 
rewrites its own code and 
state in Atomspace 
No
self-representation
or 
self-modiﬁcation 
LLMs and other DNNs, exposed as external APIs or Neural Spaces, act as 
rapid-ﬁre perceptual lobes that turn raw language into graph fragments. Proba-
bilistic Logic Networks and MeTTa proofs chain those fragments into abductive, 
inductive, and deductive inferences, while evolutionary program learning and 
concept blending inject novelty whenever systematic reasoning stalls - instantly 
publishing their results back into the same graph. 
An explicit, ﬂexibly editable goal lattice uniﬁes the whole loop, guiding but 
not dominating the dynamics. Goals are weighted Atoms whose priorities update 
with success, novelty, and external value signals, allowing Hyperon to trace why 
an action helps, compare alternatives, and patch faulty procedures - capabilities 
missing from transformer-centric stacks. 
This crude sketch only hints at key aspects of the PRIMUS approach elabo-
rated in more depth in [ 1, 2, 6, 7]. Hopefully however even this brief treatment has 
illustrated how in PRIMUS an ensemble of learning/reasoning processes associ-
ated with representational mechanisms (using a common meta-representational 
MeTTa base) becomes a dynamically reconﬁguring pattern network, which

198
B. Goertzel
instantiates every memory store, control loop, and learning channel that human 
cognition requires. 
5
Hyperon as a Seed ASI 
Meeting CMC criteria for HLAGI is only the starting point: Hyperon is intended 
to bootstrap itself from human-level AGI toward super-intelligence. Together 
with PRIMUS it forms a classic seed-AGI : a compact, fully introspectable sys-
tem able to rewrite and upgrade its own code. Hyperon supplies the concrete 
machinery - every datum, rule, and meta-rule is a ﬁrst-class graph object in 
the Atomspace, while MeTTa lets those objects query and rewrite one another; 
a cognitive-cycle scheduler applies the changes without reboot. PRIMUS sup-
plies the systems-theoretic discipline, modelling the mind as a network of 
pattern-transforming processes linked by category-theoretic morphisms, creat-
ing a framework in which self-modiﬁcation preserves coherence (individuation) 
even as it opens paths for radical reorganisation (self-transcendence). 
This pairing realises Weaver's notion of open-ended intelligence [ 14]: an 
autopoietic system that maintains its boundary while continually reorganising 
in search of new possibilities. Hyperon already hosts probabilistic logic, evolu-
tionary program induction, neural "lobes," and a goal-weighted attention econ-
omy; once self-modiﬁcation begins these can duplicate, specialise, and fuse into 
novel cognitive organs. Beneﬁcial variants accumulate while maladaptive ones 
are pruned by PRIMUS's coherence and eﬃciency gradients, enabling an internal 
evolutionary spiral toward ever greater capability. I.e., a fully credible seed-AGI: 
small enough to engineer and audit today, yet fundamentally unbounded in the 
sophistication it can evolve tomorrow. 
6
Hyperon as a Path to Beneﬁcial AGI 
Recursively self-improving AGI frightens some observers, and indeed such sys-
tems are powerful and must be handled with great care and wisdom. However, 
our instinct is to be far more more wary of AGIs ﬁxed to unthinkingly obey 
narrow-minded humans. 
Our working hypothesis as regards AGI ethics is that by embedding a 
Hyperon-based mind in compassionate, day-to-day roles-e.g. answering ques-
tions behind open and ethical chatbots, assisting with physical and social tasks 
through humanoids and other robots, or teaming up with other AIs to achieve 
goals in simulation environments - one can do more than provide early-stage 
AGI systems with sensory richness and diverse experience. Such an approach 
grounds every Atomspace pattern in lived social feedback, furnishing a steady 
stream of examples where curiosity and skill are rewarded when they help or 
comfort someone. 
According to the Goal-Stability/Moderated-Evolution framework articulated 
in [ 4], such feedback is crucial because Hyperon's top-level motivations are not 
frozen commands but Atoms that are continuously re-weighted by success signals

OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI
199
and value traces. When those traces arise from cooperative, prosocial contexts 
the contraction-mapping metagoal (which forces each self-rewrite to keep new 
goals relativel close to the old ones) naturally pulls the system's evolving drives 
toward "what worked yesterday"- which if things are being done right is helpful 
and empathetic behaviour. 
The same paper argues that even when Hyperon performs more drastic, long-
range self-modiﬁcations (framed as "simulate-and-select" updates) a bounded-
step metagoal prevents wholesale goal drift: the search for better policies must 
stay inside a compact, convex region of the motivational space. If that region 
is deﬁned by human-aligned value primitives-minimising suﬀering, maximising 
agency, preserving truthfulness-then every global re-design the AGI considers 
is, by construction, some perhaps creative blend of those primitives rather than 
an alien departure. 
Hyperon's OpenPsi motivational system has the capability to enforce that 
discipline in real time. Every sub-goal is scored against the metagoal attrac-
tor; PLN and MeTTa proof hooks can veto plans that widen the distance. The 
same circuitry governs Neoterics avatars and kitchen robots, so value coherence 
propagates across embodiments. Thousands of cycles nudge the system toward 
a stable"ethical ﬁxed point," matching Weaver's open-ended-intelligence ideal of 
self-transcendence without loss of identity. 
Compassionate work and play places supply reward gradients, Hyperon's 
plastic metagraph enables continual self-rewrite, and ﬁxed-point metagoals lay 
the rails. Together they keep value evolution moving in directions seeded by 
human values - even as capability scales far beyond our own. 
6.1
Decentralized Deployment for Common Good 
Another potentially highly signiﬁcant contributor to the ethical character of 
Hyperon deployments is the system's explicit design toward decentralized deploy-
ments, without any central hardware facility, software lobe or owner or controller. 
The main technical key to this decentralization is MeTTa. Hyperon's rules, 
cognitive-cycle schedulers, and even self-modifying "meta-rules" are all expressed 
in this same pattern-matching language - which is also the smart contract lan-
guage of the MeTTaCycle blockchain [ 11], the underlying technology of the 
"ASI Chain" being rolled out in 2025 by the ASI Alliance (integrated with 
other related decentralization technology like the SingularityNET multi-agent 
platform). Because Hyperon and MeTTaCycle share the identical abstract syn-
tax and execution model, a Hyperon agent can drop its Atomspace fragments 
straight onto MeTTaCycle shards without translation-distribution and consen-
sus come "for free." 
This technical ﬁt hints at potential solutions to deeper governance prob-
lems. Hyperon's ethical design assumes plural oversight: its metagoals are safest 
when many independent parties can inspect and veto self-rewrites. MeTTaCy-
cle's architecture is consciously "anti-monopoly": [ 11] lays out token-capability 
access controls, sharded Byzantine-fault-tolerant consensus, and the ability to 
run heterogeneous consensus algorithms side-by-side. No single cloud account,

200
B. Goertzel
military enclave, or corporate datacenter can lock the graph; changing global 
state requires quorum among diverse validators, each holding only the scoped 
object-capability tokens they need. 
There is a strong argument that this decentralization provides substantial 
ethical leverage [ 5]. A military or megacorporation might spin up its own shard, 
but the shards that host education bots, medical AIs, or open-science tools can 
simply refuse cross-shard transactions that violate their governance charters. The 
tokenization and capability layer lets communities issue revocable, ﬁne-grained 
rights to Hyperon processes: a hospital can grant a diagnostic agent read-only 
access to de-identiﬁed records for 30 min-and nothing else-while a humanitarian 
NGO can fund compute for a disaster-relief planner without exposing private 
donor data. Attempts to exﬁltrate, withhold, or weaponize those capabilities 
leave cryptographically signed traces that other agents in the decentralized net-
work can audit and block. 
MeTTaCycle's fault-tolerant shard tree means the physical infrastructure 
is as plural as the governance: a research lab in Nairobi, a DAO validator in 
Lisbon, and a solar-powered micro-data-center in rural India all host pieces of 
the same distributed Atomspace. Hyperon's cognitive-synergy modules still see 
one (dynamic, paraconsistent) logical memory, but no adversary can ﬂip a global 
"kill switch." In Weaver's terms, the combined system preserves individuation 
while enabling open-ended growth; in pragmatic terms, it makes it vastly harder 
for any nation-state or corporation to bend emergent AGI or ASI exclusively 
to its own strategic agenda. Hyperon gains a censorship-resistant, self-healing 
body, and the world gains an AGI platform whose ethical trajectory is steered 
by a genuinely planetary constituency. 
7
2025-2028 Roadmap to Human-Level AGI 
with Hyperon 
The precise development trajectory of a complex system like Hyperon is impos-
sible to foresee, however, it is still worthwhile to carry out educated specula-
tion regarding what may be plausible. In this section we sketch one thought-
experiment regarding how we might hypothetically move from the current state 
toward HLAGI by 2028. While some would view this as radically ambitious, we 
note that serious individuals at major tech companies have been publicly fore-
casting HLAGI on similar or more ambitious time-scales [ 12,13], and indeed an 
OpenAI system has arguably passed the Turing Test in 2025 [ 8] (though without 
achieving HLAGI, exposing the well-known weaknesses of that test). 
A few assumptions of our hypothetical timeline are: 
- A production-scale Hyperon stack (Distributed Atomspace, MeTTa, Space 
API, Rholang and neural spaces) is live by Q4 2025 [ 1]. 
- LLMs remain powerful linguistic "lobes" but do not provide AGI-appropriate 
working memory, goal cycles, or self-modiﬁcation 
- Hardware: multi-cluster GPU/CPU for metagraph and neural spaces, plus 
\mathcal{O}(10)O(10) Mind Children humanoid robots for embodied learning.

OpenCog Hyperon: A Practical Path to Beneﬁcial AGI and ASI
201
- As particular practical applications to use for giving Hyperon systems expe-
rience in the real world, we include interactive research assistants (e.g. ASI 
Alliance's Qwestor, TrueAGI's Enterprise Researcher or others), humanoid 
robots (e.g. Hanson Robotics and Mind Children humanoids or others) and 
characters in virtual worlds (e.g. the Neoterics AGI-learning world or others). 
Given this, one way things might unfold could be roughly as follows: 
Q4 2025-Q2 2026: "Baby Hyperon". A production Distributed Atomspace (DAS) 
runs on Kubernetes clusters across several server farms, while core MeTTa ker-
nels supply episodic, declarative, and procedural memory. Continuous pattern-
miners extract frequent subgraphs from toy data. In Neoterics World the sys-
tem masters object permanence, simple physics, and causal puzzles, guided by 
a global user base through Qwestor and TrueAGI chat. "Toddler" abstract-
cognition benchmarks are reached even as domain knowledge from live chats 
deepens. 
Q3 2026-Q1 2027: "Child Hyperon". Hybrid cognition matures: PLN inference, 
MOSES evolutionary learning, and ECAN attention share one Atomspace. Ten 
Mind Children humanoids train in a Montessori-style "robot preschool," stream-
ing sensorimotor data into Neural Spaces and working memory. Grounded dia-
logue grows richer, enabling verbally directed physical manipulation. 
Q2 2027-Q4 2027: "Adolescent Hyperon". Training shifts to long-horizon plan-
ning, advanced Theory of Mind, and a value-alignment layer. In multi-agent Neo-
terics World quests the system practices cooperation, competition, and decep-
tion detection; in mock apartments robots cook, clean, and converse with human 
partners. Research-assistant agents better map user interests to practical goals. 
Q1 2028-Q4 2028: "Young-Adult Hyperon". 
A full self-improvement loop 
launches: the AGI proposes, theorem-checks, and rolls back MeTTa rewrites 
autonomously. Robots perform complex real-world tasks; research assistants 
make novel discoveries; humanoid carers support elder care; teaching bots deliver 
personalised instruction. Neoterics World expands into an open-ended socioeco-
nomic simulator fostering creative, collaborative exploration 
Bootstrapping to ASI. Through these stages, bootstrapping from sandbox play to 
embodied service tasks and then more open-ended creativity, this staged program 
leverages Hyperon's open-ended, self-modifying architecture—driven forward 
by interactive practical applications like chatbots, game avatars and humanoid 
robots—to target credible human-level AGI capability by year-end 2028. And 
HLAGI achieved in this manner would clearly not be a terminus but rather a 
beginning of a new phase of development, as the practical, creative and analytical 
capabilities obtained increasingly turned themselves on improvement of the AGI 
system's own fundamental capabilities.

202
B. Goertzel
References 
1. Goertzel, B., et al.: Opencog hyperon: a framework for agi at the human level and 
beyond (2023) 
2. Goertzel, B.: Toward a general theory of general intelligence: a patternist perspec-
tive. arXiv preprint arXiv:2103.15100 (2021) 
3. Goertzel, B.: Generative ai vs. agi: the cognitive strengths and weaknesses of mod-
ern llms (2023). https://arxiv.org/abs/2309.10371 
4. Goertzel, B.: Metagoals endowing self-modifying agi systems with goal stability or 
moderated goal evolution: toward a formally sound and practical approach (2024). 
https://arxiv.org/abs/2412.16559 
5. Goertzel, B., Montes, G.A.: The Consciousness Explosion. Humanity+ Press (2024) 
6. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence, 
Part 1: A Path to Advanced AGI via Embodied Learning and Cognitive Synergy. 
Springer, Atlantis Thinking Machines (2013) 
7. Goertzel, B., Pennachin, C., Geisweiller, N.: Engineering General Intelligence, Part 
2: The CogPrime Architecture for Integrative, Embodied AGI. Springer, Atlantis 
Thinking Machines (2013) 
8. Jones, C.R., Bergen, B.K.: Large language models pass the turing test (2025). 
https://arxiv.org/abs/2503.23674 
9. Laird, J.E., Lebiere, C., Rosenbloom, P.S.: A standard model of the mind: toward 
a common computational framework across artiﬁcial intelligence, cognitive science, 
neuroscience, and robotics. AI Mag. 38(4), 13-26 (2017) 
10. Meredith, L.G., Goertzel, B., Warrell, J., Vandervorst, A.: Meta-metta: an opera-
tional semantics for metta (2023) 
11. Meredith,
L.G.,
Stay,
M.:
MeTTaCycle
Architecture
Proposal
(2025). 
https://github.com/F1R3FLY-io/MeTTaCycleYellowPaper/blob/main/map. 
pdf. Accessed 23 Apr 2025 
12. Pillay, T.: How OpenAI's sam altman is thinking about AGI and superintelligence 
in 2025. TIME (2025). https://time.com/?/sam-altman-agi-superintelligence-2025 
13. Varanasi, L.: Here?s how far we are from AGI, according to the people develop-
ing it. Business Insider (2025). https://www.businessinsider.com/?/how-far-are-
we-from-agi-according-to-experts 
14. Weinbaum, D., Veitas, V.: Open ended intelligence: the individuation of intelligent 
agents. J. Exp. Theor. Artif. Intell. 29(2), 371-396 (2017)

Patterns of Quantum Cognition I: From 
Chronomorphisms to Quantum 
Propagators 
Ben Goertzel1,2(B) 
1 SingularityNET Foundation, Amsterdam, The Netherlands 
ben@singularitynet.io 
2 TrueAGI Inc., Seattle, USA 
Abstract. The Patterns of Cognition (PoC) framework casts diverse 
AGI algorithms—probabilistic logic networks, evolutionary program 
learning, deep Q-learning, attention focusing and more—as approximate 
stochastic dynamic programs on typed metagraphs. Each algorithm is 
implemented by a chronomorphism: an unfold functor that generates a 
tree of candidate states and a fold functor that collapses this tree using a 
preorder, forming a Galois connection that monotonically improves solu-
tion quality. Here we explain how to port this framework to the quantum 
computing domain, obtaining a similar uniﬁed treatment of AGI algo-
rithms applicable to quantum implementation. 
The approach taken leverages the facts that in continuous time the 
chronomorphism operator approximates the Hamilton-Jacobi-Bellman 
(HJB) equation; whereas a logarithmic action \to→wavefunction trans-
form, plus Wick rotation for diﬀusive noise, turns the HJB into the 
Schr¨odinger or heat equation. Thus every PoC chronomorphism lifts to 
a sparse Hamiltonian 
 \bUnALT{}H_{\mathrm{eff}} = H_C + H_R\eUnALT{} Heﬀ= HC + HR
whose oﬀ-diagonal block H_CHC encodes combinatory expansion and whose 
diagonal block H_RHR encodes evaluation phases. Second-order Trotter or 
qubitization then realizes the small-time propagator 
 \bUnALT{}U(\Delta t) \approx e^{-iH_R\Delta t/2} e^{-iH_C\Delta t} e^{-iH_R\Delta t/2}\eUnALT{} U(Δt) ≈e−iHRΔt/2e−iHCΔte−iHRΔt/2
in \widetilde{O}(d\,\|H_{\mathrm{eff}}\|\, t) O(d ∥Heﬀ∥t) fault-tolerant gates, giving quadratic speed-ups in, for 
instance, batch size, action branching and ﬁtness estimation. A rough 
resource analysis shows that a hypothetical next-generation Dirac3+ 
10k-qubit machine would likely scale quite favorably to thousand-agent 
workloads. 
The sequel paper elaborates how this mechanism can be leveraged 
for a number of the key AI algorithms involved in the OpenCog Hyperon 
architecture—illustrating that the given mechanisms can be considered 
as a general-purpose scalable control loop for Hyperon-style quantum 
AGI systems. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 203-211, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_19

204
B. Goertzel
1
Introduction 
Artiﬁcial General Intelligence (AGI) research has produced a wide menu of 
techniques: deep reinforcement learning, probabilistic logic networks, evolution-
ary program synthesis, transformer attention mechanisms, symbolic planners, 
and many more. Integrating multiple complementary techniques into successful 
hybrid cognitive architectures has proved diﬃcult because of the sheer diver-
sity: each method carries its own control loop, data structures and optimization 
machinery. 
The Patterns of Cognition (PoC) approach [ 1] tackles this fragmentation 
head-on—in a classical computing context—by expressing all of these tech-
niques as instances of a single algebraic template called a chronomorphism. A  
chronomorphism combines an unfold step that proposes new candidate states 
with a fold step that evaluates and ﬁlters them, forming a Galois connection 
that guarantees monotone improvement. When a chronomorphism is iterated it 
behaves like an approximate stochastic dynamic program (DP); in continuous 
time its value function satisﬁes the celebrated Hamilton-Jacobi-Bellman (HJB) 
equation. 
This paper develops that idea in detail. We ﬁrst review how PoC chronomor-
phisms operate on typed metagraphs and why they approximate Bellman 
updates. We then show, step by step, how to lift any chronomorphism to a 
sparse Hamiltonian 
 \bUnALT{}H_{\mathrm{eff}} = H_C + H_R\eUnALT{} Heﬀ= HC + HR
whose oﬀ-diagonal block H_CHC encodes the unfold (generation) operations and 
whose diagonal block H_RHR encodes the fold (evaluation) penalties. Standard 
quantum-simulation tools (second-order Trotter, qubitization, quantum singular-
value transformation) turn H_{\mathrm{eff}}Heﬀinto a time-evolution operator 
 \bUnALT{}U(\Delta t) \approx e^{-iH_R\Delta t/2} e^{-iH_C\Delta t} e^{-iH_R\Delta t/2}\eUnALT{} U(Δt) ≈e−iHRΔt/2e−iHCΔte−iHRΔt/2
that can run on universal gate-model machines. 
In the sequel paper [ 2] we elaborate the concepts given here in a more 
application-oriented way, focusing on three concrete AGI use cases: 
- Deep Q-learning: Bellman backups and regression ﬁts become reversible uni-
tary blocks, oﬀering quadratic speed-ups in batch size and action branching. 
- Evolutionary program learning with EDA: crossover, mutation and proba-
bilistic sampling are executed coherently across all parents, while ﬁtness eval-
uation phases suppress low-quality genomes. 
- Uncertain inference chaining: probabilistic truth values are encoded in 
amplitudes, and rule application occurs in superposition, amplifying high-
conﬁdence chains. 
These are the three computationally heaviest ingredients of the integrated 
OpenCog Hyperon AGI design [ 3], so eﬃcient quantum implementations of these 
tools can serve as the core of an eﬃcient quantum Hyperon.

Patterns of Quantum Cognition I
205
Finally, as a thought-experiment regarding future scalable implementation, 
we sketch a resource budget for a notional Dirac3+ 10 000-qubit superconducting 
machine, suitable for deploying a module of a quantum Hyperon. Toy instances 
of each algorithm ﬁt comfortably within a single surface-code patch and execute 
in milliseconds; larger workloads scale linearly with qubit count and beneﬁt from 
the quadratic speed-ups inherent in amplitude parallelism. 
In summary, PoC oﬀers a single algebraic bridge between classical AGI mod-
ules, Hyperon?s distributed Atomspace, and future quantum accelerators. By 
translating chronomorphisms from Bellman to Schr¨odinger form we gain a clear, 
hardware-agnostic path toward uniﬁed, quantum-native general intelligence. 
2
Patterns of Cognition and Its Role in the Hyperon 
AGI Stack 
Patterns of Cognition (PoC) is a unifying theoretical framework that casts a 
wide range of cognitive algorithms—probabilistic inference, evolutionary pro-
gram learning, deep reinforcement learning, attention focusing, and more—as 
instances of a common approximate stochastic dynamic programming (DP) pat-
tern. The key technical ingredients are: 
1. Typed metagraphs that represent heterogeneous cognitive states and data 
structures in a single, richly typed graph space. 
2. Chronomorphisms: a fold-unfold schema in which an anamorphism expands 
a state into a tree of subproblems, while a histomorphism collapses that tree 
by an evaluation preorder, yielding a Galois-connection-based guarantee of 
monotone improvement. 
3. Stochastic DP interpretation wherein the chronomorphism operator approx-
imates the Bellman update, providing a principled route to value estimation, 
credit assignment, and exploration-exploitation balance. 
4. Galois connection optimization that separates domain-speciﬁc generators 
(combinators) from domain-agnostic evaluators, enabling high-level algorithm 
templates to be specialized automatically to new problem domains. 
Integration with Hyperon. OpenCog Hyperon is a modular AGI platform 
whose kernel consists of the Atomspace (weighted, labeled) metagraph for stor-
age, the MeTTa language for knowledge meta-representation and AI-algorithm 
programming and meta-programming, a concurrent scheduler for heterogeneous 
cognitive processes, and sophisticated tooling for distributed and decentralized 
processing. 
PoC provides the algorithmic glue that binds the AI components of PRIMUS 
together into a common mathematical framework for uniﬁed analysis and eﬃ-
cient deployment: 
- Each atom or hyperedge in the Atomspace is interpreted as a vertex in a 
PoC metagraph; MeTTa pattern-rewrite rules implement the unfold/fold of 
chronomorphisms, allowing value functions, action models, and inference dags 
to coexist natively.

206
B. Goertzel
- Hyperon?s cognitive service scheduler views every service (PLN reasoning, 
evolutionary learning, neural subnets, etc.) as a chronomorphism instance, 
exposing uniform hooks for forward expansion, backward evaluation, and 
caching. 
- The PoC Galois framework supplies formal contracts (h^T_i\circ,h^S_i)(hT
i ◦, hS
i ) that 
Hyperon?s type system can verify at compile time, enabling safe composi-
tion of hitherto disparate algorithms (e.g. combining PLN abduction with 
transformer-style attention under a single DP schema). 
- Emerging quantum chronomorphism variants map PoC operators to sparse 
Hamiltonians. Hyperon?s long-term roadmap includes a quantum back-end; 
thus the same high-level MeTTa code can target both classical and future 
quantum execution. 
Strategic impact. By elevating diverse cognitive processes to a shared DP 
template, PoC frames the core PRIMUS cognitive algorithms as manifestations 
of an underlying Hyperon-based meta-solver capable of: 
1. Uniﬁed learning and reasoning: value-driven search trees, PLN truth-value 
propagation, and neural network weight updates all reduce to Bellman-style 
backups, allowing cross-fertilization of gradients, priors, and heuristics. 
2. Composable optimization: new algorithmic modules are added simply by sup-
plying their unfold/fold pair and evaluation preorder; the scheduler automat-
ically handles concurrency, memoization, and credit assignment. 
3. Graceful scalability: chronomorphisms factor cleanly across distributed Atom-
space shards, supporting asynchronous, eventually consistent updates and 
hierarchical meta-learning. 
Summary. Patterns of Cognition anchors the "middle layer" of the Hyperon 
AGI stack, furnishing both the theoretical semantics and the practical scaﬀolding 
required to orchestrate symbolic, sub-symbolic, evolutionary and probabilistic 
subsystems within a single, adaptive, self-improving cognitive fabric. 
3
From Bellman to Schr¨odinger: Recasting PoC 
as Quantum Dynamics 
3.1
Why Bellman Leads to Schr¨odinger 
Dynamic programming views any optimal-control problem through its value 
function J(x,t)J(x, t). In continuous time one obtains the Hamilton-Jacobi-Bellman 
(HJB) PDE 
 \bUnALT{}-\partial_tJ = \min_u{g(x,u)+\nabla_xJ\cdot f(x,u)},\eUnALT{} −∂tJ = min
u {g(x, u) + ∇xJ · f(x, u)},
where ff is the system drift and gg the running cost. Choosing g=Lg = L converts 
HJB into the classical Hamilton-Jacobi (HJ) form: 
 \bUnALT{}\partial_tS + H(x,\nabla_xS)=0\eUnALT{} ∂tS + H(x, ∇xS) = 0
with Hamiltonian H(x,p)=\min_u{p\cdot f + L}H(x, p) = minu{p · f + L}. A logarithmic change of variables 
\psi=\exp(-iS/\hbar)ψ = exp(−iS/ℏ) plus the momentum substitution p\mapsto -i\hbar\nabla_xp →−iℏ∇x turns HJ into 
the time-dependent Schr¨odinger equation i\hbar\partial_t\psi = H\psiiℏ∂tψ = Hψ. Bellman updates and 
quantum time evolution are therefore two faces of the same mathematics.

Patterns of Quantum Cognition I
207
3.2
Adding Stochasticity 
If the state obeys an Itˆo diﬀusion dx=f\,dt + \sigma\,dW_tdx = f dt + σ dWt, the HJB gains a diﬀusion 
term: 
 \bUnALT{}-\partial_tJ = \min_u\Bigl{g + \nabla_xJ\cdot f + \tfrac12\mathrm{Tr}[\sigma\sigma^T\nabla^2_xJ]\Bigr}.\eUnALT{} −∂tJ = min
u

g + ∇xJ · f + 1
2Tr[σσT ∇2
xJ]

.
A Cole?Hopf transform \psi=\exp(-J/\hbar)ψ = exp(−J/ℏ) followed by the Wick rotation t=-i\taut = −iτ
linearizes this into the imaginary-time ("heat") equation: 
 \bUnALT{}\hbar\partial_\tau\psi = \bigl(\nabla_x\cdot f - g/\hbar + \tfrac12\mathrm{Tr}[\sigma\sigma^T\nabla^2_x]\bigr)\psi,\eUnALT{} ℏ∂τψ =

∇x · f −g/ℏ+ 1
2Tr[σσT ∇2
x]

ψ,
recovering the familiar Feynman-Kac link between diﬀusion and quantum paths. 
3.3
Chronomorphisms in a Quantum Light 
The PoC chronomorphism M=h^S_i\circ h^T_i\rhd_RM = hS
i ◦hT
i ▷R: an unfold that spawns candidate 
states and a fold that keeps the best. Embed every metagraph conﬁguration gg
as a basis vector |g\rangle|g⟩in Hilbert space \mathcal{H}H. The chronomorphism becomes a linear 
map: 
 \bUnALT{}Mc = \sum_{g,g'} M_{g'g}|g'\rangle\langle g|,\quad M_{g'g}=\begin{cases}1 & g'=M(g),\\0&\text{otherwise.}\end{cases}\eUnALT{} Mc =

g,g′
Mg′g|g′⟩⟨g|,
Mg′g =

1
g′ = M(g),
0
otherwise.
When the unfold depth \Delta tΔt is small one may write: 
 \bUnALT{}Mc\approx e^{-iH_C+H_R\Delta t},\eUnALT{} Mc ≈e−iHC+HRΔt,
with 
 \bUnALT{}H_C=\sum_{g\to g'}\alpha_{g\to g'}|g'\rangle\langle g|,\quad H_R=\sum_g \beta_g|g\rangle\langle g|.\eUnALT{} HC =

g→g′
αg→g′|g′⟩⟨g|,
HR =

g
βg|g⟩⟨g|.
A symmetric Trotter splitting then yields 
 \bUnALT{}e^{-iH\Delta t}\approx e^{-iH_R\Delta t/2}e^{-iH_C\Delta t}e^{-iH_R\Delta t/2}.\eUnALT{} e−iHΔt ≈e−iHRΔt/2e−iHCΔte−iHRΔt/2.
3.4
What the Mapping Buys Us 
- Uniform recipe. Any PoC chronomorphism now has a quantum counterpart 
obtained mechanically from its unfold/fold rules. 
- Performance. If H_C,H_RHC, HR are dd-sparse, block-encoding and qubitization sim-
ulate e^{-iHt}e−iHt to error \varepsilonε in \widetilde{O}(d\|H\|t) O(d∥H∥t) gates, yielding quadratic or better speed-
ups. 
- Examples. Deep Q-learning, evolutionary program learning with EDA, and 
probabilistic inference chains have all been cast in this form, preserving their 
logic while gaining amplitude superposition and amplitude-estimation accel-
erations.

208
B. Goertzel
3.5
Take-Away 
Replacing Bellman updates by coherent Schr¨odinger propagators keeps the 
Galois guarantees of PoC while opening the door to hardware-ready, quantum-
native AGI modules. The result is a single mathematical bridge uniting symbolic, 
neural and evolutionary processes across classical and future quantum platforms. 
4
From Chronomorphisms to Quantum Propagators 
4.1
Typed Metagraphs and Galois Chronomorphisms 
Let G=(V,E,\tau)G = (V, E, τ) be a typed directed hypergraph where \tau:V\to Tτ : V →T assigns each 
vertex a type in a countable set TT. We write GG for the class of such metagraph 
conﬁgurations. A single cognitive state is indexed by g\in Gg ∈G. 
An anamorphism ("unfold") h^T_i: G\to \mathrm{Tree}(G)hT
i
: G →Tree(G) expands gg into a tree 
whose children correspond to combinators C_iCi. A histomorphism ("fold") h^S_i: \mathrm{Tree}(G)\to GhS
i :
Tree(G) →G collapses using a preorder R\subseteq G\times GR ⊆G × G. They form a Galois con-
nection: 
 \bUnALT{}h^T_i(g)\,R\,t \iff g\,R\,h^S_i(t).\eUnALT{} hT
i (g) R t ⇐⇒g R hS
i (t).
The chronomorphism operator is: 
 \bUnALT{}M = h^S_i\circ h^T_i\rhd_R: G\to G.\eUnALT{} M = hS
i ◦hT
i ▷R : G →G.
4.2
Hilbert Embedding of Metagraph States 
Embed GG into a separable Hilbert space \mathcal H\simeq \ell^2(G)H ≃ℓ2(G) via g\mapsto |g\rangleg →|g⟩, an orthonormal 
basis. Extend MM linearly: 
 \bUnALT{}M_c=\sum_{g,g'}M_{g'g}|g'\rangle\langle g|, \; M_{g'g}=1\mathrm{\ if\ }g'=M(g).\eUnALT{} Mc =

g,g′
Mg′g|g′⟩⟨g|, Mg′g = 1 if g′ = M(g).
4.3
Exponential Generator and Small-Time Limit 
Assuming M_cMc is local (dd-sparse), for small \Delta tΔt
 \bUnALT{}M_c = I - iH_{\mathrm{eff}}\Delta t + O(\Delta t^2), \quad H_{\mathrm{eff}}=i(1-M_c)/\Delta t + O(\Delta t).\eUnALT{} Mc = I −iHeﬀΔt + O(Δt2),
Heﬀ= i(1 −Mc)/Δt + O(Δt).
Decompose: 
 \bUnALT{}H_{\mathrm{eff}} = H_C + H_R,\eUnALT{} Heﬀ= HC + HR,
with oﬀ-diagonal unfold H_CHC and diagonal fold H_RHR. 
4.4
Reversible Circuit Compilation 
Using block-encoding and qubitization, e^{-iH_C\Delta t}e−iHCΔt and e^{-iH_R\Delta t}e−iHRΔt can be imple-
mented in \widetilde{O}(d\|H_{\mathrm{eff}}\|\Delta t) O(d∥Heﬀ∥Δt) two-qubit gates, with ancilla qubits for combinator 
indexing and phase rotations.

Patterns of Quantum Cognition I
209
4.5
Error Analysis and Stability 
Because unfold/fold are adjoint under RR, the Trotter error accumulates contrac-
tively, yielding global errorO(t\Delta t^2)O(tΔt2). A spectral gap in H_RHR accelerates imaginary-
time convergence. 
4.6
Illustrative Example 
With two combinators C_1,C_2C1, C2
on three states {g_0,g_1,g_2}{g0, g1, g2} and preorder 
g_2\,R\,g_1\,R\,g_0g2 R g1 R g0: 
 \bUnALT{}H_C = |g_1\rangle\langle g_0| + |g_2\rangle\langle g_1|, \quad H_R=\beta_0|g_0\rangle\langle g_0|+\beta_1|g_1\rangle\langle g_1|,\;\beta_0>\beta_1>0.\eUnALT{} HC = |g1⟩⟨g0| + |g2⟩⟨g1|,
HR = β0|g0⟩⟨g0| + β1|g1⟩⟨g1|, β0 > β1 > 0.
Imaginary-time propagation selects |g_2\rangle|g2⟩. 
4.7
Summary 
A PoC chronomorphism extends to a sparse linear map whose small-time gener-
ator separates unfold and fold, allowing eﬃcient reversible quantum compilation. 
5
Implementing Quantum Chronomorphisms 
on a "Dirac3+" Machine 
5.1
Logical Qubit Budget 
Table 1. Representative logical qubit allocation for a single chronomorphism kernel. 
Register
Logical Qubits (LQs) Purpose 
State |g\rangle|g⟩
n_gng
pointer 
data amplitudes 
Combinator index
\lceil\log_2 m\rceil⌈log2 m⌉
select C_iCi
Ancilla (block-encoding)
3n_g+23ng + 2
reversible arithmetic 
Phase (controlled rotations)
1
encode \beta_gβg
QRAM address lines (optional)n_{\mathrm{addr}}naddr
fetch parents, batches 
Total per module
\approx n_g + n_{\mathrm{addr}} + O(m)≈ng + naddr + O(m)
Assuming surface-code distance d=25d = 25, each logical qubit consumes \approx1250≈1250
physical qubits; a 10 000-PQ module hosts88-1010 kernels for typical n_g\le128, m\le6ng ≤128, m ≤
6 (Table 1).

210
B. Goertzel
5.2
High-Level Compilation Pipeline 
1. Hamiltonian synthesis: parse H_C,H_RHC, HR and generate sparse indices. 
2. Block-encoding: implement oracles O_{\mathrm{row}},O_{\mathrm{val}}Orow, Oval for sparse-access. 
3. Qubitization: build walk operator WW satisfying W^2=\exp(2i\arccos H_{\mathrm{eff}})W 2 = exp(2i arccos Heﬀ). 
4. Time evolution: apply QSVT to implement U(t)U(t) with O(\|H_{\mathrm{eff}}\|t+\log1/\varepsilon)O(∥Heﬀ∥t + log 1/ε)
gadgets. 
5. Error correction & scheduling: map to surface-code patches, dominated by 
QSVT depth (\sim10^3∼103 cycles). 
5.3
State Preparation and QRAM 
Classical prior p(G)p(G) is loaded via controlled rotations (O(|G|)O(|G|)) once; subsequent 
kernels reuse the superposition. Coherent memory uses persistent-current mod-
ules with \le20≤20 address lines. 
5.4
Inter-Module Parallelism 
Connecting MM modules via entanglement-assisted ﬁbers allows shard-local 
chronomorphism steps with occasional teleportation of elite genomes using 
O(\log N)O(log N) Bell pairs. 
5.5
Resource Estimate: Example 
For n_g=16, m=3, \varepsilon=10^{-3}ng = 16, m = 3, ε = 10−3: 
- Logical qubits n_{LQ}=43nLQ = 43 (<55{,}000< 55,000 physical). 
- QSVT depth \approx4.2\times10^3≈4.2 × 103 cycles (4 ms). 
- 100 generations: \sim4.2\times10^5∼4.2 × 105 cycles (0.4 s). 
Classical runtime \sim10^8∼108 ﬁtness calls; quantum overhead comparable, with 
quadratic speed-ups for larger NN. 
5.6
Co-design Considerations 
1. Connectivity: heavy-hex lattice with fast couplers to minimize swaps. 
2. Sparsity exploitation: microcode streams ﬁxed non-zeros from ﬂash. 
3. Analog phases: ﬂux-tunable ZZ rotations via lookup tables, 12-bit accuracy. 
4. Error-mitigated oracles: midden-ancilla veriﬁcation to keep p_L<10^{-12}pL < 10−12 with-
out doubling distance. 
5.7
Roadmap 
1. Dirac3 (current): 50 logical qubits for toy chronomorphism (population<<32). 
2. Dirac3+ (5 yrs): 1k logical qubits; full RL/EPL with N\sim10^3, B\sim128N ∼103, B ∼128, 
depth<<1 s.  
3. Dirac4 (beyond): 10k logical qubits; multi-agent chronomorphisms and cross-
module entangled evolution.

Patterns of Quantum Cognition I
211
6
Conclusion 
We have argued that the PoC chronomorphism template serves as a direct bridge 
from high-level cognitive DP speciﬁcations to low-level quantum circuits. By 
mapping Bellman updates to Schr¨odinger propagators via a simple logarithmic 
transform and Wick rotation, we derive a sparse Hamiltonian whose evolution 
reproduces classical AGI algorithms while unlocking quantum speed-ups. 
Concrete instantiations in deep Q-learning, evolutionary program learning, 
and logical inference demonstrate the recipe in action. A resource analysis for a 
hypothetical Dirac3+ device shows practical feasibility for mid-sized workloads 
and suggests thousand-agent scenarios are within reach as quantum hardware 
scales. 
Further research directions include richer noise models, optimized block-
encodings, and integration into the full Hyperton AGI stack. Even so, the present 
results already outline a principled, hardware-agnostic control loop for quantum-
accelerated AGI. 
References 
1. Goertzel, B.: Patterns of cognition: cognitive algorithms as Galois connections ful-
ﬁlled by chronomorphisms on probabilistically typed metagraphs (2021). https:// 
arxiv.org/abs/2102.10581 
2. Goertzel, B.: Patterns of quantum cognition II: AGI algorithms as quantum 
chronomorphisms. Technical Report, OpenCog Foundation (2025) 
3. Goertzel, B., et al.: OpenCog hyperon: a framework for AGI at the human level and 
beyond (2023). https://arxiv.org/abs/2310.18318

The Emergence of Modularization 
from Architecture Search via Optimal 
Transport 
Ben Goertzel1,2(B) 
1 SingularityNET Foundation, Amsterdam, The Netherlands 
ben@singularitynet.io 
2 TrueAGI Inc., Seattle, USA 
Abstract. We introduce a unifying framework showing how modular 
architectures emerge naturally when connection patterns are regularized 
by an optimal-transport (OT) cost. We represent any candidate archi-
tecture as a probability distribution over possible links and consider a 
dynamic in which links are created or removed as part of the same learn-
ing process as modifying link weights. We then add to the usual task 
loss a penalty that charges more for creating or strengthening distant 
connections than for local ones. Under mild formal conditions indicating 
that there is an underlying modular structure in the problem the net-
work is learning to solve - even if heavily obscured by other phenomena 
- we then show that the optimal architecture places almost all its mass 
on a small number of modules (with only a bounded "leakage" outside), 
and that any gradient-based update of the combined loss is likely to stay 
trapped within those modules until convergence. 
We illustrate this in three settings. In predictive-coding neural net-
works with columnar structure, an OT penalty on inter-column links 
drives the system to form edge, stroke and loop detectors before ever 
wiring far-ﬂung columns, yielding a hierarchical, stroke-based feature 
scaﬀold. In probabilistic logic networks (PLN), chaining and pruning of 
belief links under a reasoning-distance cost produces clusters of related 
concepts with sparse bridges between them. Finally, we give specula-
tive arguments that these same dynamics may occur in biological brains 
- i.e. synapse growth and pruning under metabolic and wiring-length 
pressures may instantiate a generalized OT ﬂow that gives rise to orien-
tation columns, place ﬁelds and motor primitives. Across these domains, 
minimizing task error plus an OT transport penalty provides a general 
principle for the self-organization of functional modules. 
1
Introduction 
Modular organization-where a system decomposes into tightly connected sub-
units with sparse interconnections-is a hallmark of eﬃcient computation in 
brains, engineered neural networks, symbolic reasoning systems and beyond. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 212-224, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_20

The Emergence of Modularization
213
Manually designing such modules is laborious and often suboptimal in the con-
text of narrow application systems, and largely infeasible in the context of AGI 
systems that by their nature need to create their own novel modules in response 
to novel situations. Current neural architecture search methods address this to a 
certain extent, but typically lack a principled bias toward locality and coherence. 
In this work we introduce a unifying framework explaining how modular orga-
nization can occur in a broad variety of complex networks via the conduction of 
architecture search concurrently with learning, according to general principles 
of "optimal transport." 
The basic concept we develop here is as follows. Imagine a vast army of tiny 
builders, each responsible for connecting pairs of "nodes" (neurons, logic links, 
or predictive-coding columns). The goal is to build exactly the right connections 
so the system performs its task - say, classiﬁcation, reasoning, or control - while 
keeping the cost of creating and maintaining those connections as low as possible. 
If we assume an "optimal transport" bias via which each builder prefers cheap 
local links over costly long-distance links - and we assume that the problem 
the network is solving has some underlying modular structure, even if radically 
fuzzed and obscured by other phenomena - we will conclude that the builders 
end up concentrating connections in small neighborhoods, which automatically 
creates modules. I.e. optimal transport is good at ﬁnding and reﬁning modular 
structure implicit in complex networks as these networks adapt and learn to 
solve problems. This simple idea is the basis of the mathematical developments 
and potential applications presented here. 
The generalized OT (optimal-transport) architecture-search theorem we 
present here treats the connection pattern as a probability distribution \muμ over 
links and penalizes changes by an OT cost D_{\mathrm{OT}}(\mu,\mu_0)DOT(μ, μ0). Three phenomena follow. 
First, local moves are cheap, long jumps are expensive: tweaking a link between 
neighbors costs little, whereas wiring distant nodes directly incurs high cost. 
Second, modules appear naturally: cheap local updates weave tight clusters, or 
"modules," of strongly connected nodes where they most improve performance, 
and only links whose beneﬁt outweighs their cost are added across clusters. 
Third, the math backs it up: under mild assumptions (indicating there is indeed 
some modularity implicit in the problem the network is learning to solve), the 
optimal \mu^*μ∗concentrates on a few modules (with only an O(\varepsilon/\lambda)O(ε/λ) leak outside), 
and gradient descent on L_{\mathrm{task}}(\mu)+\lambda\,D_{\mathrm{OT}}(\mu,\mu_0)Ltask(μ) + λ DOT(μ, μ0) never teleports mass outside 
these modules-updates remain mainly trapped in modules until convergence. 
- Neural networks: Predictive-coding columns learn both synaptic strength 
and existence. An OT penalty on link changes biases formation of local edge 
and shape detectors before long-range connections. 
- Logical reasoning (PLN): Inference rules chain existing links to propose 
nearby new links, and pruning removes those whose uncertainty reduction 
fails to cover their reasoning-distance cost, yielding clusters of related con-
cepts with sparse inter-module bridges. 
- Brain modeling: Synapses grow and prune under metabolic and wiring-
length constraints, making local rewiring cheap and long axons costly.

214
B. Goertzel
Activity-driven plasticity thus forms orientation columns, place ﬁelds, and 
motor primitives, matching the modules predicted by OT-regularized archi-
tecture search. 
In essence, minimizing task error plus an OT transport penalty yields a 
learning process that spontaneously carves the network into functional modules, 
whenever such modularization actually exists implicitly in the problem being 
solved (in a reasonably non-perverse way). We argue that this framework con-
ceptually explains module formation in image classiﬁcation, where predictive-
coding columns self-organize into edge and shape detectors; in probabilistic 
logic networks, where inference chains create and prune concept links based on 
uncertainty reduction versus cc-cost; and in cortical development, where synaptic 
rewiring under wiring-length constraints yields functional columns in the brain. 
1.1
Comparison to Other Architecture Search Methods 
Our OT-regularized search diﬀers from mainstream neural architecture search 
(NAS) techniques by embedding a ground-metric bias toward locality and modu-
larity directly into the learning objective - which can be eﬀective if there is indeed 
obscured modularity implicit in the network structure toward which the learning 
coupled with the architecture search is moving. Comparatively, reinforcement-
learning based NAS [ 11] trains a controller RNN to sample high-performing 
architectures but lacks an inherent notion of connection cost or spatial coherence. 
Diﬀerentiable NAS methods such as DARTS [ 7] optimize a continuous relaxation 
of the discrete search space via bilevel gradients, yet they treat all edges uni-
formly and often require extra sparsity or pruning heuristics to recover modular 
graphs. Evolutionary strategies [ 10] mutate and recombine whole architectures, 
demanding large compute budgets and oﬀering limited control over local vs. long-
range rewiring. Hypernetwork approaches [ 6] generate weights from secondary 
networks but do not adapt the macro-graph based on performance-cost tradeoﬀs. 
Lottery-ticket methods [ 2] ﬁnd sparse subnetworks post hoc rather than guiding 
structure formation during training. By contrast, OT-guided search minimizes 
F(\mu)=L_{\rm task}(\mu)+\lambda\,D_{\mathrm{OT}}^c(\mu,\mu_0)F(μ) = Ltask(μ)+λ Dc
OT(μ, μ0) so that each update balances performance gains 
against the transport cost of rewiring, naturally yielding hierarchical, modular 
topologies without extra pruning stages or exorbitant search overhead. 
2
General OT-Regularized Architecture Search 
We now present a formalization of the basic concept of the paper - that doing 
architecture search according to optimal transport principles naturally leads to 
modularization. To show this formally we make particular formal assumptions, 
but we stress that the qualitative conclusion may still hold in many practical 
cases where the formal assumptions don't precisely hold. 
Our formalization is quite abstract, intended to apply to a variety of sorts of 
complex networks, including formal and biological neural networks, logic systems 
and others.

The Emergence of Modularization
215
Let \OmegaΩ be a ﬁnite set of elements, \mathcal{P}(\Omega)P(Ω) the set of probability measures on \OmegaΩ, 
and L_{\mathrm{task}}:\mathcal{P}(\Omega)\to\mathbb{R}Ltask : P(Ω) →R a task loss. A ground cost c:\Omega\times\Omega\to[0,\infty)c : Ω × Ω →[0, ∞) induces the 
OT distance 
 \bUnALT{}D_{\mathrm{OT}}^c(\mu,\nu) =\min_{\pi\in\Pi(\mu,\nu)} \sum_{x,y}c(x,y)\,\pi(x,y).\eUnALT{} Dc
OT(μ, ν) =
min
π∈Π(μ,ν)

x,y
c(x, y) π(x, y).
The regularized objective is 
 \bUnALT{}F(\mu)=L_{\mathrm{task}}(\mu)+\lambda\,D_{\mathrm{OT}}^c(\mu,\mu_0).\eUnALT{} F(μ) = Ltask(μ) + λ Dc
OT(μ, μ0).
Partition \Omega=\bigcup_{i=1}^K C_iΩ = K
i=1 Ci and deﬁne for each ii the average exit cost 
 \bUnALT{}d_i =\frac{1}{\mu_0(C_i)} \sum_{x\in C_i}\sum_{y\notin C_i}\mu_0(x)\,c(x,y),\eUnALT{} di =
1
μ0(Ci)

x∈Ci

y /∈Ci
μ0(x) c(x, y),
and set d_{\min}=\min_i d_i>0dmin = mini di > 0. No geodesic convexity is required, only that each C_iCi
has a nonzero cut cost. 
We now show the following lemma, which provides the core of our formal 
analysis of architecture search via optimal transport. 
Lemma (Isoperimetric Modularity). Let \mu^*=\arg\min_\mu F(\mu)μ∗= arg minμ F(μ) and write \delta=\mu^*(\Omega\setminus\cup_iC_i)δ =
μ∗(Ω \ ∪iCi). Then 
 \bUnALT{}\delta\;\le\;\frac{\max_i\Delta L_i}{\lambda\,d_{\min}},\eUnALT{} δ ≤maxi ΔLi
λ dmin
,
where \Delta L_iΔLi is the maximal drop in L_{\mathrm{task}}Ltask from allocating full mass to C_iCi. 
Proof Sketch. Moving mass \deltaδ from \mu_0μ0 into \Omega\setminus\cup_iC_iΩ \ ∪iCi costs at least \delta\,d_{\min}δ dmin, while 
it can reduce L_{\mathrm{task}}Ltask by at most \delta\,\max_i\Delta L_iδ maxi ΔLi. Optimality of \mu^*μ∗gives 
 \bUnALT{}0 \;\ge\; F(\mu^*)-F(\mu_0) \;\ge\; -\delta\,\max_i\Delta L_i + \lambda\,\delta\,d_{\min},\eUnALT{} 0 ≥F(μ∗) −F(μ0) ≥−δ max
i
ΔLi + λ δ dmin,
hence \delta\le(\max_i\Delta L_i)/(\lambda\,d_{\min})δ ≤(maxi ΔLi)/(λ dmin). The same bound holds along the natural-
gradient ﬂow in the OT geometry. 
2.1
Optimal Transport Based Architecture Search Finds Modules 
We now apply the Isoperimetric Modularity Lemma to optimal transport based 
architecture search. 
Corollary (Modularity in OT-Based Architecture Search). Let 
 \bUnALT{}F(\mu)=L_{\rm task}(\mu)+\lambda\,D^c_{\rm OT}(\mu,\mu_0),\eUnALT{} F(μ) = Ltask(μ) + λ Dc
OT(μ, μ0),
and suppose an architecture search algorithm produces $\mu^*=\arg\min_{\mu}F(\mu)$μ∗= arg minμ F(μ) (e.g. 
via natural-gradient descent in the OT geometry). If ${C_i}${Ci} partition $\Omega$Ω with 
average exit costs $d_i\ge d_{\min}>0$di ≥dmin > 0 as above, then the learned architecture $\mu^*$μ∗
satisﬁes 
 \bUnALT{}\mu^*(\Omega\setminus\cup_iC_i)\;\le\;\frac{\max_i\Delta L_i}{\lambda\,d_{\min}}.\eUnALT{} μ∗(Ω \ ∪iCi) ≤maxi ΔLi
λ dmin
.

216
B. Goertzel
Moreover, the continuous OT-natural-gradient ﬂow $\partial_t\mu=-\mathrm{grad}_{D^c_{\mathrm{OT}}}F(\mu)$∂tμ = −gradDc
OTF(μ)
remains supported on $\cup_iC_i$∪iCi up to the same bound. Hence any method that 
minimizes the OT-regularized objective yields emergent modularity under the 
isoperimetric barrier condition. 
Proof Sketch. Let \delta=\mu^*(\Omega\setminus\bigcup_iC_i)δ = μ∗(Ω\
i Ci). Any coupling transporting mass \deltaδ from the 
prior \mu_0μ0 into \Omega\setminus\cup_iC_iΩ \ ∪iCi must incur cost at least \delta\,d_{\min}δ dmin, so  
 \bUnALT{}D^c_{\mathrm{OT}}(\mu^*,\mu_0)\;\ge\;\delta\,d_{\min}.\eUnALT{} Dc
OT(μ∗, μ0) ≥δ dmin.
On the other hand, shifting mass \deltaδ outside the modules can reduce the task loss 
by at most \delta\max_i\Delta L_iδ maxi ΔLi. Since \mu^*μ∗minimizes 
 \bUnALT{}F(\mu)=L_{\mathrm{task}}(\mu)+\lambda\,D^c_{\mathrm{OT}}(\mu,\mu_0),\eUnALT{} F(μ) = Ltask(μ) + λ Dc
OT(μ, μ0),
we have F(\mu^*)\le F(\mu_0)F(μ∗) ≤F(μ0), i.e. 
 \bUnALT{}-\,\delta\,\max_i\Delta L_i+\lambda\,\delta\,d_{\min}\;\le\;0.\eUnALT{} −δ max
i
ΔLi + λ δ dmin ≤0.
Rearranging yields 
 \bUnALT{}\delta\;\le\;\frac{\max_i\Delta L_i}{\lambda\,d_{\min}}.\eUnALT{} δ ≤maxi ΔLi
λ dmin
.
Moreover, along the continuous natural-gradient ﬂow \partial_t\mu=-\mathrm{grad}_{D^c_{\mathrm{OT}}}F(\mu)∂tμ = −gradDc
OTF(μ) the 
same cost-beneﬁt argument applies at each inﬁnitesimal step, so the mass outside 
\cup_iC_i∪iCi remains bounded by the same ratio until convergence. 
2.2
Random Edges Preserve Isoperimetric Barrier 
The isoperimetric inequality leveraged above is quite forgiving and will apply 
to a considerable variety of networks, even those in which underlying modular 
structure is obscured by other phenomena. To illustrate this we show here that 
the inequality can be preserved even in a network overlaid with a great number 
of random connections. 
Let G_0=(V,E_0)G0 = (V, E0) be any base graph whose vertex set VV is partitioned into 
modules C_1,\dots,C_KC1, . . . , CK. Form a new graph G=(V,E)G = (V, E) by adding to E_0E0 an indepen-
dent Erd˝os-R´enyi random edge set E_{\rm rand}Erand with inclusion probability pp, so that 
each vertex has expected random degree k=p\,|V|k = p |V |. 
Equip \Omega=VΩ = V with the prior \mu_0(x)=1/|V|μ0(x) = 1/|V | and ground-cost c(x,y)=1c(x, y) = 1 if 
(x,y)\in E(x, y) ∈E, inﬁnite otherwise. Then the average exit cost of module C_iCi is 
 \bUnALT{}d_i =\frac{1}{\mu_0(C_i)} \sum_{x\in C_i}\sum_{y\notin C_i}\mu_0(x)\,c(x,y) =\frac{1}{|C_i|}\sum_{x\in C_i}\deg_{\rm rand}(x),\eUnALT{} di =
1
μ0(Ci)

x∈Ci

y /∈Ci
μ0(x) c(x, y) =
1
|Ci|

x∈Ci
degrand(x),
where \deg_{\rm rand}(x)degrand(x) is the random-edge degree of xx.

The Emergence of Modularization
217
Concentration. By standard Chernoﬀ bounds, for any \varepsilon\in(0,1)ε ∈(0, 1) and all xx, 
 \bUnALT{}\Pr\bigl[\deg_{\rm rand}(x)\ge (1-\varepsilon)k\bigr] \;\ge\;1-\exp(-\tfrac{\varepsilon^2k}{2}),\eUnALT{} Pr

degrand(x) ≥(1 −ε)k

≥1 −exp(−ε2k
2 ),
and a union bound over x\in C_ix ∈Ci gives with high probability 
 \bUnALT{}d_i \;\ge\;(1-\varepsilon)\,k.\eUnALT{} di ≥(1 −ε) k.
Hence d_{\min}=\min_i d_i\ge(1-\varepsilon)k>0dmin = mini di ≥(1 −ε)k > 0. 
Corollary. Applying the Isoperimetric Modularity Lemma above with this d_{\min}dmin
shows that the OT-regularized minimizer \mu^*μ∗satisﬁes 
 \bUnALT{}\mu^*(V\setminus\cup_iC_i) \;\le\;\frac{\max_i\Delta L_i}{\lambda\,d_{\min}} \;\le\;\frac{\max_i\Delta L_i}{\lambda\,(1-\varepsilon)\,k}.\eUnALT{} μ∗(V \ ∪iCi) ≤maxi ΔLi
λ dmin
≤maxi ΔLi
λ (1 −ε) k .
Thus layering on random edges preserves a nontrivial isoperimetric barrier and 
hence yields emergent modularity under OT-regularized architecture search. 
3
Information Geometry Guided Architecture Search 
in Columnar Predictive Coding Neural Network 
As a ﬁrst application of the general use of optimal transport based architecture 
search to induce modularity, we look at a particular species of neural net archi-
tecture, in which learning and architecture search are conveniently intertwined. 
Similar analysis could be made for a variety of other neural net approaches. 
We will consider neural architectures that adjust weights via predictive cod-
ing rather than backpropagation [ 8], as this provides a context in which explicit 
use of OT based information geometric learning is particularly natural. Predic-
tive coding treats each neural unit as trying to predict its inputs and conveying 
only the prediction error, with local synaptic updates that reduce this error. 
Unlike backpropagation, which relies on global error signals and nonlocal weight 
transport, predictive coding operates with strictly local computations and con-
tinuous online updates. At its core, in its standard versions, it posits a generative 
model p(x,z)=p(x\mid z)\,p(z)p(x, z) = p(x | z) p(z) and infers latents zz by minimizing variational free 
energy 
 \bUnALT{}F(z,W)=-\ln p(x,z;W)+\mathrm{const},\eUnALT{} F(z, W) = −ln p(x, z; W) + const,
where xx is observed data and WW the synaptic weights. This generative, 
inference-driven approach aligns more closely with biological plausibility, sup-
ports asynchronous processing, and uniﬁes perception and learning, poten-
tially yielding more robust and ﬂexible neural-network training than standard 
backpropagation.

218
B. Goertzel
3.1
Information Geometry for Accelerating Neural Learning 
Information geometry has been used to enhance backpropagation for neural 
learning for some time [ 1], but in [ 3] it is argued that it ﬁts even more nat-
urally with PC based learning. 
Information geometry equips the space of probability distributions with a 
Riemannian metric, allowing gradient methods to account for the curvature of 
the statistical manifold. Given a parametric modelp(x;\theta)p(x; θ), the Fisher information 
matrix 
 \bUnALT{}G_{ij}(\theta) =\mathbb{E}\bigl[\partial_i\ln p(x;\theta)\,\partial_j\ln p(x;\theta)\bigr]\eUnALT{} Gij(θ) = E

∂i ln p(x; θ) ∂j ln p(x; θ)

- which underlies the most standard approaches to information geometry - 
deﬁnes the natural gradient \tilde\nabla_{\theta}L=G^{-1}\nabla_{\theta}L ˜∇θL = G−1∇θL, which often yields faster conver-
gence and invariance to reparameterization [ 1]. In predictive coding networks, 
both inference and weight learning arise from variational free-energy minimiza-
tion in a hierarchical generative model. Since PC updates are gradients of a prob-
abilistic loss, replacing ordinary gradients by natural gradients aligns synaptic 
updates with the intrinsic geometry of the model, improving stability and learn-
ing speed. 
The Fisher-Rao metric derives from the Kullback-Leibler divergence via its 
second-order expansion, 
 \bUnALT{}D_{\mathrm{KL}}(p,q)\approx\tfrac12(\theta_p-\theta_q)^\top G(\theta_q)\,(\theta_p-\theta_q),\eUnALT{} DKL(p, q) ≈1
2(θp −θq)⊤G(θq) (θp −θq),
but it ignores any notion of distance in the data domain. In [ 3] it is argued  
that for enhancing PC neural networks, it will often be even better to use the 
pp-Wasserstein metric 
 \bUnALT{}W_p(p,q) =\inf_{\pi\in\Pi(p,q)}\sum_{x,y}d(x,y)^p\,\pi(x,y)\eUnALT{} Wp(p, q) =
inf
π∈Π(p,q)

x,y
d(x, y)p π(x, y)
which deﬁnes a transport geometry: moving mass across the input space incurs 
cost d(x,y)^pd(x, y)p, yielding geodesics that respect a ground metric and inducing nat-
ural gradients that incorporate locality and module preservation [ 10]. 
We argue here that for PC networks, Wasserstein-based updates oﬀer a prin-
cipled way to regularize both weight and architecture adaptation in accordance 
with the data manifold structure. 
3.2
Hierarchical Predictive-Coding Columnar Networks 
with OT-Regularized Architecture Learning 
To explore the applicability of OT-based architecture search to information-
geometry-enhanced PC-based learning, we look at a columnar PC architecture 
inspired by the ideas of Gunther Palm [ 9]. We partition the neural network into 
MM columns c=1,\dots,Mc = 1, . . . , M, each with state vector z_czc and intra-column weights 
W_cWc. Inter-column strengths a_{c\to c'}\ge0ac→c′ ≥0 deﬁne a measure \muμ on \Omega={e=(c\to c')}Ω = {e = (c →c′)}
by \mu(e)\propto a_{c\to c'}μ(e) ∝ac→c′, with prior \mu_0μ0 (e.g. nearest neighbors). We will then look at

The Emergence of Modularization
219
OT-based learning as a way of adapting the architecture of inter-column links, co-
adaptively with PC-based learning of synaptic weights both within and between 
columns. 
To formalize this, we introduce a ground cost 
 \bUnALT{}c(e,e')=d_{\rm hop}(c,c')^p,\quad p\in{1,2},\eUnALT{} c(e, e′) = dhop(c, c′)p,
p ∈{1, 2},
where d_{\rm hop}dhop is shortest-path in a base adjacency graph. The OT penalty is 
 \bUnALT{}D_{\rm OT}^c(\mu,\mu_0) =\inf_{\pi\in\Pi(\mu,\mu_0)} \sum_{e,e'}c(e,e')\,\pi(e,e').\eUnALT{} Dc
OT(μ, μ0) =
inf
π∈Π(μ,μ0)

e,e′
c(e, e′) π(e, e′).
We then jointly minimize 
 \bUnALT{}F(z,W,\mu) =-\,\ln p\bigl(x,{z_c};{W_c},\mu\bigr) +\lambda\,D_{\rm OT}^c(\mu,\mu_0).\eUnALT{} F(z, W, μ) = −ln p

x, {zc}; {Wc}, μ

+ λ Dc
OT(μ, μ0).
This can be done via a variety of learning methods, e.g. 
1. Gradient descent on z_c,W_czc, Wc to reduce the free-energy term; or, 
2. Natural-gradient descent of \muμ in the OT geometry, \partial_t\mu=-\mathrm{grad}_{D_{\rm OT}^c}F(\mu)∂tμ = −gradDc
OTF(μ), 
approximated by local mass transport via rule ﬁrings (each ﬁring moves 
\Delta\mu(e')\approx\omega_r\,\mu(e)Δμ(e′) ≈ωr μ(e) at cost c(e,e')c(e, e′)) and pruning links whose marginal \Delta FΔF fails 
to exceed \lambda\,c(e,e')λ c(e, e′). 
If \Omega=\bigcup_{i=1}^K C_iΩ = K
i=1 Ci admits modules C_iCi whose average exit cost 
 \bUnALT{}d_i =\frac{1}{\mu_0(C_i)} \sum_{x\in C_i}\sum_{y\notin C_i}\mu_0(x)\,c(x,y) \;\ge\;d_{\min}>0,\eUnALT{} di =
1
μ0(Ci)

x∈Ci

y /∈Ci
μ0(x) c(x, y) ≥dmin > 0,
then the isoperimetric modularity theorem implies that the minimizer 
 \bUnALT{}\mu^*=\arg\min_\mu\bigl{L_{\mathrm{task}}(\mu)+\lambda\,D^c_{\mathrm{OT}}(\mu,\mu_0)\bigr}\eUnALT{} μ∗= arg min
μ

Ltask(μ) + λ Dc
OT(μ, μ0)
	
satisﬁes 
 \bUnALT{}\mu^*\bigl(\Omega\setminus\bigcup_iC_i\bigr) \;\le\;\frac{\max_i\Delta L_i}{\lambda\,d_{\min}},\eUnALT{} μ∗
Ω \

i
Ci

≤maxi ΔLi
λ dmin
,
and the OT-natural-gradient ﬂow $\partial_t\mu=-\mathrm{grad}_{D^c_{\mathrm{OT}}}F(\mu)$∂tμ = −gradDc
OTF(μ) remains supported on 
\bigcup_iC_i
i Ci up to the same bound. 
3.3
Practical Validity of the Isoperimetric Barrier 
How realistic are the isoperimetric inequality based assumptions, in practical 
cases?

220
B. Goertzel
Image Perception Use-Case. In image classiﬁcation, for example, one may argue 
that pixel-columns along the same edge have similar intensities, so most edges 
leaving that set incur high cost c(x,y)c(x, y), yielding an average exit cost d_idi well above 
zero. In robotic-arm RL, sensor-motor primitives share coherent kinematics, and 
any link from one primitive to an unrelated joint carries higher transport cost, 
again producing d_i\ge d_{\min}>0di ≥dmin > 0. Empirically under natural data distributions 
these barriers persist, ensuring that real-world networks satisfy the isoperimetric 
condition needed for emergent modularity under OT-regularized architecture 
search. 
Robot Control Use-Case. In active inference for robot-arm control, on the other 
hand one might let each column encode a sensorimotor variable (joint angle, 
velocity, or visual feature). Deﬁne modules C_iCi as local primitives (e.g. elbow 
ﬂexion) and a ground cost 
 \bUnALT{}c(x,y)=\alpha\,d_{\rm kin}(x,y)+\beta\,d_{\rm inf}(x,y),\eUnALT{} c(x, y) = α dkin(x, y) + β dinf(x, y),
where d_{\rm kin}dkin is Euclidean distance in joint space and d_{\rm inf}dinf is the minimal number 
of inference-steps between percepts. Within a primitive, columns are adjacent in 
the kinematic chain or perceptual hierarchy so c(x,y)c(x, y) is small; links to unrelated 
joints or features incur c(x,y)\ge d_{\min}c(x, y) ≥dmin. Hence the average exit cost 
 \bUnALT{}d_i =\frac{1}{\mu_0(C_i)} \sum_{x\in C_i}\sum_{y\notin C_i}\mu_0(x)\,c(x,y) \;\ge\;d_{\min}>0.\eUnALT{} di =
1
μ0(Ci)

x∈Ci

y /∈Ci
μ0(x) c(x, y) ≥dmin > 0.
Under realistic movement trajectories and sensory streams, these costs remain 
bounded below, satisfying the isoperimetric barrier and yielding emergent sen-
sorimotor modules via OT-regularized architecture search. 
3.4
Practical PC OT Architecture Search Dynamics: Just-So 
Stories 
Image Perception Use-Case. To understand how the OT-based architecture 
search might be expected to pan out in practice, consider a predictive-coding 
network arranged as a 28\,\times\, × 28 grid of columns. Each column ii has an internal 
state z_izi and intra-column weights W_iWi, while inter-column link strengths a_{i\to j}ai→j are 
initially small and local. As MNIST images arrive, correlated prediction errors 
along strokes trigger a Wasserstein-natural-gradient update of a_{i\to j}ai→j, transport-
ing link mass along shortest grid paths. Early on, this traces chains that become 
edge-detector modules. In mid-training, those edges cohere into loop and cor-
ner modules by reinforcing links along minimal-cost routes, avoiding costly long 
jumps. Finally, classiﬁcation modules form by integrating mid-level modules via 
additional low-cost links. At each step, updates balance reduction in variational 
free energy against transport cost, yielding a hierarchical, modular topology 
without abrupt long-range connections.

The Emergence of Modularization
221
Robot Control Use-Case. Alternately, imagine a predictive-coding network whose 
columns represent joints (P_jPj), motor commands (M_jMj), visual features (V_kVk) and  
goals (G_mGm). Inter-column link strengths a_{c\to c'}ac→c′ start very small and only between 
immediately adjacent columns (e.g. each P_j\!-\!M_jPj −Mj pair). When the arm ﬁrst 
closes its gripper, large prediction errors in P_jPj and M_jMj trigger a Wasserstein-
natural-gradient update of a_{P_j\to M_j}aPj→Mj, ﬂowing link mass along the shortest kine-
matic ?street? and creating a local grasp-reﬂex module. Next, attempting to 
reach an object produces correlated errors in V_kVk, G_mGm and P_jPj, so mass ﬂows 
along the minimal-cost path V_k\!\to\!P_j\!\to\!M_jVk →Pj →Mj, knitting a sensorimotor reaching 
module. As the robot learns to lift and transport, mid-level modules (grasp, 
reach, lift) link via low-cost routes, avoiding expensive direct jumps. Finally a 
pick-and-place pipeline emerges by connecting these modules to a classiﬁcation 
head (G_mGm) with a few extra short-range links. At each step the network trades oﬀ 
variational free-energy reduction against transport cost c(a_{c\to c'})c(ac→c′), yielding a self-
organizing, hierarchical modular controller without abrupt long-range wiring. 
3.5
Why KL/Fisher Fails and Wasserstein Succeeds 
Why do we think the Wasserstein metric is the right way to do information 
geometry to make stories like this come true, as opposed to the more standard 
Fisher metric approach? 
In essence: The Fisher-Rao metric measures sensitivity to inﬁnitesimal 
changes in parameters but ignores any notion of distance between atomic ele-
ments. As a result, moving probability mass between arbitrary components 
incurs no extra cost and geodesics can "teleport" mass across the model with-
out regard to structure. Consequently, no mechanism prevents creation of costly 
long-range links and modules do not emerge. 
By contrast, geodesics under the pp-Wasserstein distance W_pWp move mass along 
shortest paths in \OmegaΩ, penalizing long-range rewiring. This built-in locality bias 
yields the isoperimetric inequality condition above, and guides the emergence of 
modularity under OT-regularized architecture search. 
4
Generalized Optimal Transport in the Brain 
Part of the motivation for PC networks in general is the idea that they, with 
their localization and their grounding in basic physical and informational quanti-
ties, reﬂect fundamental neurobiological dynamics better than backpropagation. 
Information geometry with its sophisticated mathematical foundation may seem 
to get away from this biological foundation. However, we argue that biological 
neural networks may actually implement their own form of OT driven archi-
tecture learning, even if not exactly the same as the more elegant and simple 
mathematical formulations of information geometry. 
It has been known for a long time that cortical tissue is arranged in columns 
whose synaptic connections adapt both weights and topology under strong

222
B. Goertzel
metabolic and conduction-delay constraints. Gunther Palm's classic models of 
self-organizing cortical modules [ 9] posit an activity-dependent wiring cost 
 \bUnALT{}E_{\rm wire}=\sum_{i,j}c(i,j)\,w_{ij},\quad c(i,j)\propto d_{\rm phys}(i,j),\eUnALT{} Ewire =

i,j
c(i, j) wij,
c(i, j) ∝dphys(i, j),
whose minimization yields local clustering of feature detectors. Here d_{\rm phys}dphys is 
the distance along the cortical sheet and w_{ij}wij the synaptic strength. This par-
allels an optimal-transport penalty D_{\rm OT}^c(\mu,\mu_0)Dc
OT(μ, μ0) with c(e,e')\propto d_{\rm phys}c(e, e′) ∝dphys: synaptic 
"mass" ﬂows along shortest cortical paths, and long-range rewiring is penalized 
in proportion to distance. 
Spike-timing-dependent plasticity implements discrete local mass-transport: 
each potentiation event moves a fraction \Delta\mu(e')\approx\omega\,\mu(e)Δμ(e′) ≈ω μ(e) between nearby 
synapse pairs at costc(e,e')c(e, e′), while homeostatic pruning removes links whose ben-
eﬁt \Delta LΔL fails to exceed \lambda\,cλ c. Neuromodulators (dopamine, acetylcholine) adjust \lambdaλ, 
trading oﬀ exploration of new circuits against wiring cost. Thus cortical rewiring 
approximates a natural-gradient ﬂow  \partial_t\mu=-\mathrm{grad}_{D_{\rm OT}^c}\bigl[L_{\rm task}(\mu)+\lambda\,D_{\rm OT}^c(\mu,\mu_0)\bigr]∂tμ = −gradDc
OT

Ltask(μ)+λ Dc
OT(μ, μ0)

in a generalized OT geometry. The resulting emergent modules-orientation 
columns, place cells, motor primitives-match anatomically observed columnar 
clusters and arguably satisfy our isoperimetric inequality lemma, yielding robust 
modularity as predicted by the general OT-modularity theorem. 
Certainly this high level argument leaves out a high percentage of the com-
plexity of biological brains. However it does suggest there may be some con-
ceptual foundation to the identiﬁcation of OT based subnetwork architecture 
learning as a potential key element of the human brain's ongoing neural archi-
tecture adaptation. 
5
Application to Logical Inference Networks 
The general theory of OT based architecture learning sketched above is not 
intrinsically tied to (formal or biological) neural networks and can be applied in a 
variety of other contexts. Here we will argue that it applies perfectly well to fuzzy-
probabilistic reasoning according to the PLN (Probabilistic Logic Networks) 
approach, which plays a key role in the Hyperon AGI framework and associated 
PRIMUS cognitive architecture [ 4]. 
In Hyperon, PLN [ 5] implements approximate Bayesian inference on the 
AtomSpace, a typed metagraph many of whose links ll carry a truth-value that in 
the simplest case is a pair (s,c)(s, c) with strength s\in[0,1]s ∈[0, 1] and conﬁdence c\in[0,1]c ∈[0, 1]. 
PLN uses a rule set (e.g. Deduction, Induction,Abduction, Analogy ,Exempliﬁ-
cation) in which each rule r\in\mathcal Rr ∈R matches existing links l_1,\dots,l_nl1, . . . , ln, computes a 
new truth-value  (s',c')=f_r\bigl((s_1,c_1),\dots,(s_n,c_n)\bigr)(s′, c′) = fr

(s1, c1), . . . , (sn, cn)

via a rule-speciﬁc combination 
function f_rfr, and asserts or updates a candidate link l'l′ with (s',c')(s′, c′). A prioritized 
agenda orders rule ﬁrings according to an expected-utility measure U(l)U(l) under a 
resource budget. Inference iterates until the budget is exhausted, at which point 
the truth-values of target query links approximate their posterior probabilities 
under the underlying probabilistic model.

The Emergence of Modularization
223
5.1
PLN as OT-Regularized Architecture Search via Isoperimetric 
Modularity 
One can view PLN inference as simultaneously learning link strengths and topol-
ogy by eﬀectively minimizing $ F(\mu)=L_{\rm task}(\mu)+\lambda\,D^c_{\rm OT}(\mu,\mu_0) $F(μ) = Ltask(μ) + λ Dc
OT(μ, μ0) over the Atom-
Space link measure $\mu$μ. Here $\Omega$Ω is the set of all potential links $e=(A\to B)$e = (A →B), and a 
suitable ground-cost might be$ c(e,e')=\alpha\bigl[d_{\rm hop}(A,X)+d_{\rm hop}(B,Y)\bigr] +\beta\,d_{\rm inf}(e,e'), $c(e, e′) = α

dhop(A, X)+dhop(B, Y )

+β dinf(e, e′),
for $e'=(X\to Y)$e′ = (X →Y ), where $d_{\rm inf}(e,e')$dinf(e, e′) is the minimal product of $(1-s)\,c$(1 −s) c
along any inference chain from $e$e to $e'$e′. The OT distance is then $ D^c_{\rm OT}(\mu,\nu)=\min_{\pi\in\Pi(\mu,\nu)}\sum_{e,e'}c(e,e')\,\pi(e,e'). $Dc
OT(μ, ν) =
minπ∈Π(μ,ν)

e,e′ c(e, e′) π(e, e′). In practice, each rule ﬁring moves $\Delta\mu(e')\approx\omega_r\,\mu(e)$Δμ(e′) ≈
ωr μ(e) at cost $c(e,e')$c(e, e′), and pruning removes any $e'$e′ with $\Delta L_{\rm task}<\lambda\,c(e,e')$ΔLtask < λ c(e, e′), 
approximating the proximal natural gradient $\partial_t\mu=-\mathrm{grad}_{D^c_{\rm OT}}F(\mu)$∂tμ = −gradDc
OTF(μ). 
Partition $\Omega=\bigcup_{i=1}^K C_i$Ω = K
i=1 Ci into concept-modules and assume each $C_i$Ci has average 
exit cost $ d_i=\frac{1}{\mu_0(C_i)}\sum_{x\in C_i,\,y\notin C_i}\mu_0(x)\,c(x,y)\ge d_{\min}>0. $di =
1
μ0(Ci)

x∈Ci, y /∈Ci μ0(x) c(x, y) ≥dmin > 0. By the isoperimetric 
modularity lemma, the minimizer $\mu^*$μ∗satisﬁes $ \mu^*(\Omega\setminus\cup_iC_i)\le\frac{\max_i\Delta L_i}{\lambda\,d_{\min}}, $μ∗(Ω \ ∪iCi) ≤maxi ΔLi
λ dmin , and the 
same bound holds for the support of the OT-natural-gradient ﬂow. Hence PLN?s 
create-and-prune dynamics yield emergent modularity of concept clusters. 
6
Conclusion 
We have shown that a simple, nontechnical principle-charging a higher cost for 
long-range rewiring than for local updates-drives networks to self-organize into 
functional modules. This mechanism applies across domains: neural circuits form 
edge and shape detectors, logic reasoners cluster related concepts, and cortical 
models develop columns and place ﬁelds without manual design. 
Technically, we represent an architecture as a probability measure \muμ over 
links and add an optimal-transport penalty D^c_{\mathrm{OT}}(\mu,\mu_0)Dc
OT(μ, μ0) that increases with con-
nection distance. Under PAC-approximate geodesic convexity, the minimizer \mu^*μ∗
concentrates on a small number of modules (leakage O(\varepsilon/\lambda)O(ε/λ)), and the natural-
gradient ﬂow in the OT geometry remains trapped within those modules until 
convergence. We demonstrated this in predictive-coding columns, Probabilistic 
Logic Networks, and cortical plasticity models. Our results suggest that penal-
izing connection mass transport is a biologically and computationally plausible 
bias toward modular, scalable, and interpretable architectures. Natural future 
directions would be to explore eﬃcient OT-gradient approximations, multi-scale 
module hierarchies, and integration of these ideas into Hyperon and other AGI 
architectures. 
References 
1. Amari, S.I.: Natural gradient works eﬃciently in learning. Neural Comput. 10(2), 
251-276 (1998) 
2. Frankle, J., Carbin, M.: The lottery ticket hypothesis: ﬁnding sparse, trainable 
neural networks. In: International Conference on Learning Representations (2019)

224
B. Goertzel
3. Goertzel, B.: Actpc-geom: towards scalable online neural-symbolic learning via 
accelerating active predictive coding with information geometry & diverse cognitive 
mechanisms (2025) 
4. Goertzel, B., et al.: Opencog hyperon: a framework for agi at the human level and 
beyond. CoRR arxiv:2310.18318 (2023) 
5. Goertzel, B., Ikl´e, M., Freire Goertzel, I.L., Heljakka, A.: Probabilistic Logic Net-
works: A Comprehensive Framework for Uncertain Inference. Springer, Heidelberg 
(2008) 
6. Ha, D., Dai, A., Le, Q.: Hypernetworks. In: International Conference on Learning 
Representations (2017) 
7. Liu, H., Simonyan, K., Yang, Y.: Darts: diﬀerentiable architecture search. In: Inter-
national Conference on Learning Representations (2019) 
8. Ororbia, A.G.: Spiking neural predictive coding for continual learning from data 
streams (2019) 
9. Palm, G.: On the self-organization of feature detectors in the visual cortex. Biol. 
Cybern. 64, 179-186 (1991) 
10. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-
siﬁer architecture search. In: Proceedings of the AAAI Conference on Artiﬁcial 
Intelligence, vol. 33, pp. 4780-4789 (2019) 
11. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. In: 
International Conference on Learning Representations (2017)

Agentic Correlates of Consciousness 
and the Pursuit of Artiﬁcial General Intelligence 
Shannon Gray1envelope symbol
, George Tambouratzis2
, Sanju Mannumadam Venugopal6
, 
Sridhar Raghavan3
, Richard Jiarui Tong4
, and Zeyu Han5 
1 Gray Sky AI, Castlemaine, VIC 3450, Australia 
shannon@graysky.ai 
2 Athena Research Center, Aigialias & Chalepa, 15125 Marousi, Greece 
3 Technology Incubation, Nashua, Hillsborough, New Hampshire, USA 
4 Macau University of Science and Technology, Macau, China 
5 Hong Kong Applied Science and Technology Research Institute, Hong Kong, China 
6 Los Angeles, USA 
https://graysky.ai 
Abstract. We present COSMOS (Consciousness-Oriented Self-Modifying Oper-
ating System), a theoretical and novel framework for implementing consciousness-
like properties in artiﬁcial general intelligence (AGI) systems. Our approach 
introduces an introspection-driven cognitive alignment architecture where dis-
tinct yet interacting self and world models are continuously recreated, enabling 
metacognitive capabilities through an adaptive Architect-Plan-Interact loop. 
This paper demonstrates how COSMOS can advance beyond traditional input-
output paradigms by implementing neurologically inspired mechanisms for 
self-awareness, environmental interaction, and adaptive learning. 
Keywords: Artiﬁcial General Intelligence cdot Consciousness cdot AGI cdot Agentic AI cdot
Metacognition cdot Neural Architecture 
1 
Introduction 
The nature of consciousness and how it arises is a long-standing subject of inquiry and 
debate across various disciplines, from philosophy and psychology to neuroscience and 
artiﬁcial intelligence (AI). Kuhn's 'A Landscape of Consciousness: Toward a Taxonomy 
of Explanations and Implications' [1] provides a comprehensive overview of the diverse 
explanations and theories of consciousness, ranging from physicalist to non-physicalist 
perspectives. This broad landscape of how scholars have tried to make sense of con-
sciousness, and the human mind provides fertile ground for research towards Artiﬁcial 
General Intelligence (AGI). 
The intention of this paper is not to look for consciousness in AI systems [2], nor 
is it to suggest that consciousness is a necessary component for intelligence. Instead, 
we explore how AI systems might be adapted with an architecture and subsystems that 
mimic the cognitive or functional aspects of consciousness and perform similar roles
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 225-238, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_21 

226
S. Gray et al.
for AI agents as the neural correlates of consciousness [3] may do for humans. The 
premise being to see if these adaptations will create more robust AI systems that are less 
specialized and more adaptable to changes in their environment. 
Our proposed framework COSMOS (Consciousness-Oriented Self-Modifying Oper-
ating System) leverages ideas from multiple theoretical approaches, unbounded by bio-
logical constraints, to develop a neurologically inspired architecture that implements 
ideas from consciousness research to inform its design. While we draw insights from a 
broad range of consciousness theories, our work aligns more closely with functionalist 
views and how cognitive functions at times attributed to consciousness might be applied 
to an agentic framework. It is possible our theoretical agents may exhibit behaviors sug-
gestive of a phenomenological self, however our inquiry does not extend to determining 
whether such behavior entails genuine subjective experience. 
2 
Comparison with Biological Consciousness 
In this section we look at cognitive functions that have, at times, been associated 
with consciousness in the past and how they might inform the architecture of agentic 
frameworks. 
2.1 
Metacognition 
Metacognition, or the ability to reﬂect on one's own thoughts, has roots that can be 
traced back to ancient Greek philosophy, and since then has been explored across various 
disciplines from literature to neuroscience [4]. As an analogue in AI systems, we propose 
introducing a second-order large language model (LLM) that observes and constructs 
the elements that could be construed as being an LLM's internal cognitive processes 
(namely, the system prompt and context window) of a primary LLM. This metacognitive 
architecture creates scaffolding that can be used to integrate further cognitive functions. 
2.2 
Perceptual Filtering and Selective Attention 
Perceptual ﬁltering refers to the brain's capacity to select and process a subset of sensory 
input from the constant inﬂux of stimuli [5]. To prevent overload, a selection mechanism 
ﬁlters out extraneous information, allowing only the most salient inputs into conscious 
awareness. This ensures that cognitive resources are focused on contextually relevant 
information, despite the richness of the incoming data stream. 
Closely related is the concept of Selective Attention, which functions as the brain's 
dynamic mechanism for amplifying relevant stimuli and suppressing irrelevant ones. 
Classical models, such as Broadbent's ﬁlter theory, conceptualize attention as a gating 
function that regulates access to central processing resources [6]. More recently Baars' 
Global Workspace Theory describes attention as a control structure that inﬂuences the 
outcome of competition among unconscious processors, and determines which inputs 
are broadcast to the global workspace [7]. 
In these models, attention acts not only as a perceptual ﬁlter but also as a goal-
directed selector, shaping what becomes part of the conscious experience. We observe a

Agentic Correlates of Consciousness and the Pursuit of AGI
227
compelling parallel between these biological mechanisms for avoiding information over-
load and the challenges LLMs face in managing context overﬂow and degradation. We 
propose that systems adopt analogous ﬁltering architectures using an attention-inspired 
gating mechanism to amplify goal-relevant input and suppress extraneous or redundant 
information. This may help AI agents to prioritize meaningful context within bounded 
memory and compute resources, offering a promising approach to mitigating overload 
and maintaining operational performance in dynamic environments. 
2.3 
Prediction 
Several theories of consciousness revolve around predictive processing which reframe 
perception and action as predictive processes, where intelligent systems generate and 
reﬁne predictions about their environments and the consequences of their actions. 
Notably, Seth's "beast machine" theory and Friston's Free Energy Principle highlight 
prediction as a basic mechanism of cognition. Seth argues that conscious experience 
arises from the brain predicting the origins of sensory input and portraying the brain as 
a "prediction machine" [8]. Friston proposes that perception and action are anticipatory, 
relying on prior sensory data to predict outcomes. The brain employs a generative model 
of prior beliefs about the hidden causes of sensory inputs, to align future inputs with 
expectations [9, 10]. 
Our research aims to extend these predictive frameworks to agentic AI systems. 
Firstly, prediction mechanisms could enable AI agents to evaluate task plans before exe-
cution, simulating outcomes to identify viable strategies with higher probabilities of suc-
cess. Secondly, during task execution, predictive processes could continuously monitor 
whether actions are likely to satisfy task requirements, allowing for real-time adjustments 
when predictions indicate potential failure. This dual application of predictive process-
ing represents a potential advancement in agentic AI architecture to allow systems to 
operate with greater autonomy and resilience in complex, uncertain environments. 
2.4 
Error Monitoring and Correction 
Research has demonstrated that error correction in human consciousness functions 
largely as an autonomous system, whereas error monitoring appears to be directly con-
trolled by conscious awareness [11]. While AI systems demonstrate proﬁciency in ﬁxing 
errors once identiﬁed (correction), they exhibit substantial deﬁciencies in detecting their 
own mistakes (monitoring) [12, 13]. The transition from narrow AI to AGI will likely 
necessitate autonomous self-monitoring and error correction capabilities allowing the 
system to adapt to novel environments and the mistakes that unknown conditions will 
inevitably entail. 
To address this limitation, we propose implementing explicit "success criteria" at 
the task level within the COSMOS framework. These conditions serve as veriﬁcation 
checkpoints that an AI agent can evaluate to determine whether a task has been com-
pleted successfully. This evaluation can occur through predictive assessment before task 
execution or veriﬁcation against actual results post-execution. By establishing clear suc-
cess criteria before task initiation, AI agents may gain a useful framework for error 
monitoring, potentially compensating for their inherent limitations in this area.

228
S. Gray et al.
This structured approach could also facilitate incremental improvement through 
introspective loops, enabling reﬁnement of both execution and monitoring capabili-
ties. Future research could focus on exploring optimal implementation strategies for 
these success conditions across diverse task domains and evaluating their effectiveness 
in enhancing agent performance in real-world scenarios. 
2.5 
Inner Representation of the Self and World 
Across diverse theories of consciousness, constructs such as self-models, higher-order 
thoughts, and minimal selves are used to explain how conscious systems represent them-
selves and their environments. Though these constructs differ in terminology and empha-
sis, they share the view that the self is not a primitive feature but a dynamic, represen-
tational process. Metzinger's Self-Model Theory describes the self as a transparent rep-
resentational structure that allows an organism to experience itself [14]. Rosenthal sees 
the self as a metacognitive function of "high-order thought" that makes ﬁrst-order states 
conscious [15]. From a phenomenological (not necessarily representationalist) point 
of view, Gallagher and Zahavi emphasize the self as an evolving, invariant point within 
experience [16], which Marchetti argues is the basis for all conscious states—suggesting 
that consciousness may exist without a subject, but not without a self [17]. 
Just as our sense of identity changes as we grow and we express different aspects of 
our selves dependent on our environment (home, school, work etc.), we propose that an 
agent's system prompt need not be static and all deﬁning but instead could be dynamic 
and only express those qualities that are currently relevant. Similarly, the agent's context 
window can be viewed as a World-Model that can be continually pruned and shaped to 
draw the agent's attention to information relevant to the current task. 
3 
Conceptual Framework 
Our framework is built on a set of core ideas that shape the design philosophy and agentic 
structure: 
1. The Ascription of Consciousness: Rather than attempting to engineer consciousness, 
a goal that remains philosophically and technically contentious, COSMOS will focus 
on integrating cognitive functions often associated with consciousness into an agentic 
AI system. However, the underlying transformer architecture [18] remains unchanged 
with the framework we are proposing built on top of existing models. That conscious-
ness might arise in such a system seems unlikely although the resulting agent may 
appear to have more consciousness than older designs. 
2. Dual-Model Architecture: Our framework will use Self-Model and World-Model 
constructs that are recreated through each cognitive cycle to contain only relevant 
information with the agent's system prompt representing the Self-Model and the 
context window acting as the World-Model. 
3. Metacognitive Framework: COSMOS proposes a feedback-driven cognitive align-
ment architecture that is analogous to metacognition. This design framework can per-
mit the system to evaluate its own performance, adjust approaches based on outcomes, 
and facilitate dynamic interaction between agent and environment.

Agentic Correlates of Consciousness and the Pursuit of AGI
229
4. Perceptual Curation: The proposed framework can ﬁlter incoming sensory data 
and integrate relevant information with existing memories. By reconstructing the 
Self-Model and World-Model without extraneous information, our approach aims to 
mitigate the context overﬂow and degradation issues common in LLMs. 
5. Evolving Sense of Self: COSMOS will continuously recreate its Self-Model based on 
task requirements and accumulated information. This design is intended to allow the 
agent to adapt its expressed "identity" in different contexts: similar to how a human 
may exhibit different personalities at home and at work; also allowing the agent to 
possess a collection of characteristics that don't need to be expressed continuously 
(thus keeping its system prompt more concise and less ambiguous). 
4 
System Architecture 
COSMOS will be constructed from three interconnected modules: the Executive Planner, 
responsible for reasoning and decision-making; the Sensory Interface, handling envi-
ronmental interaction and action execution; and the Metacognitive Architect, managing 
perception, memory, and goal-oriented strategic planning. Together, these components 
form an integrated introspective loop that can enable adaptive, context-sensitive, goal-
directed behavior across environments and tasks. In the next subsections, we detail each 
component's functionality and implementation. 
4.1 
Executive Planner Module 
The Executive Planner corresponds to the prefrontal cortex of the human brain and 
implements executive functions such as high-order cognitive processes, planning, and 
personality expression [19]. It functions as the agent's primary tactical control cen-
ter, analyzing the updated self-model and world-model to decide what operations are 
performed, formulate internal reasoning processes and plan external communication. 
4.2 
Sensory Interface Module 
The Sensory Interface parallels the motor cortex and basal ganglia that carry out planned 
actions [20, 21]. It represents the implementation layer that bridges between intention 
and physical reality, translating the abstract plans of the Executive Planner into concrete 
actions such as tool usage, communication and environmental manipulations. 
4.3 
Metacognitive Architect Module 
While our neural mappings have proved useful thus far, here the agent's architecture 
diverges from biological organization. The Metacognitive Architect draws inspiration 
from several brain regions, including perceptual systems, hippocampus, and the parietal 
cortex, which are respectively responsible for processing sensory information, form-
ing and retrieving memories, and integrating spatial awareness to construct a coherent 
understanding of the world and guide behavior [22, 23]. It can provide COSMOS with 
self-reﬂection, perception ﬁltering, and episodic and semantic memory.

230
S. Gray et al.
The Architect can function concurrently as witness and curator, receiving raw data 
from the environment and the agent's actions, then evaluating outcomes against expec-
tations. It selectively updates both the self-model and world-model based on relevance 
and signiﬁcance, implementing a higher-level "attention" mechanism that determines 
what information becomes part of the agent's current understanding. 
The Architect creates, categorizes, and retrieves memories that inform future actions. 
It also revises the agent's Self-Model (or system prompt) to suit the current task, and the 
information gathered from the environment. This will allow the agent to adapt and assume 
different roles in different contexts, maintaining an evolving "self" while expressing only 
those characteristics relevant to the current situation. 
4.4 
Integrated Cognitive Cycle 
Fig. 1. This diagram illustrates the COSMOS framework and how information ﬂows between its 
modules and the outside environment. 
This Architect-Plan-Interact loop (Fig. 1) implements metacognition-like capabili-
ties, allowing the system to learn from experience, adjust its models of both self and 
world, and adapt according to its own cognitive processes. The loop comprises distinct 
phases. First, the Sensory Interface captures environmental stimuli and action outcomes, 
generating raw perception data. This information ﬂows to the Metacognitive Architect, 
which processes and ﬁlters these perceptions based on relevance and alignment with the 
next task. From the raw perceptual data, the Architect creates memories representing the 
newly acquired information within the agent's growing knowledge framework and then 
updates both self and world models adapting to the current environment and the next 
task. The updated self- and world-models are then transmitted to the Executive Planner

Agentic Correlates of Consciousness and the Pursuit of AGI
231
that generates a comprehensive action plan. This plan is then translated into concrete 
actions by the Sensory Interface, which physically executes the plan and captures results, 
thus completing one iteration. This continuous feedback enables the system to learn from 
experience and modify its own cognitive processes in terms of:
bullet Error detection and correction: The system identiﬁes mismatches between expected 
and actual outcomes, allowing it to reﬁne its models and approaches.
bullet Experiential learning: Through repeated iterations, the system accumulates knowl-
edge about effective strategies in different contexts.
bullet Context-sensitive behavior: By continuously updating its self-model and world-
model, the system can adapt its behavior to changing environments.
bullet Iterative learning approach: The system's feedback-driven cognitive design can 
provide a foundation for improving both self and world models through repeated 
interactions. 
This feedback-driven architecture creates opportunities for knowledge accumulation 
and adaptation to changing environments, forming the basis for a system that can evolve 
its understanding and capabilities in response to experience. 
5 
Consciousness-Oriented Memory Architecture 
Recent research has made considerable progress in enhancing memory formation and 
recall in LLMs using Knowledge Graphs (KGs) and Retrieval Augmented Generation 
(RAG). A core challenge is ensuring that LLM memories remain faithful to the original 
source material. Frameworks like Think-on-Graph 2.0 [24] and FiDeLiS [25] tackle 
this by grounding outputs in structured, veriﬁable knowledge paths drawn from KGs, 
reducing hallucinations. The importance of semantic connections between concepts is 
highlighted in several studies; for example, AriGraph introduces a dynamic memory 
graph that blends episodic and semantic information to aid reasoning and planning [26]. 
When designing memory systems, chunk size is important; too large and irrelevant data 
pollutes context; too small and relevant information may be lost. Research by Yepes 
et al., Chroma Research and Microsoft demonstrates that optimal chunking needs to 
balance coverage with focus [27-29]. 
LLM memory architectures are increasingly inspired by cognitive science distinc-
tions. Larimar implements a distributed episodic memory mechanism, allowing one-
shot, context-aware updates without retraining, akin to how humans recall personal 
experiences [30]. Ma et al. compares episodic KGs and temporal memory, illustrating 
how LLMs might store and retrieve time-speciﬁc knowledge [31]. Procedural memory 
(knowledge of how to do things), is also an emerging focus in LLM research, with 
efforts to extract and encode procedural knowledge into structured KG-type formats 
[32, 33]. Tong and Hu show that such formats can be used for planning, and handling 
complex, multi-step tasks in interactive environments [34]. The aforementioned works 
provide a solid foundation for creating, storing and retrieving memories that COSMOS 
will leverage to create a robust and ﬂexible memory system including:
bullet Episodic Memory: A composite collection of roles, behaviors etc. that encompasses 
the entirety of the agent's identity. The Architect will draw on these to recreate the 
self-model so only relevant facets of the agent's 'persona' are active.

232
S. Gray et al.
bullet Semantic Memory: Environmental memories will be stored in a combined vector 
database and KG. After a collection of potentially relevant memories are retrieved, 
they will be pruned and shaped into the current world-model so that the context 
window is not overloaded, and the agent remains focused on the current task.
bullet Procedural Memory: A repertoire of tools and MCP clients along with memories 
of how to use them, the expected outcomes, and what the failure modes are. 
6 
Tool Management System 
COSMOS's Tool Management System is distributed between all three main modules. 
The Architect being responsible for the repository of all available tools and the selection 
of which tools are active. The Executive Planner is in charge of the active tools and when 
to use them, along with determining the parameters with which they will be called. Lastly, 
the Sensory Interface will perform the actual tool calls. 
6.1 
Tool Repository 
The Architect maintains a comprehensive collection of all available tools in a vector 
database. This repository represents the complete set of capabilities the agent can poten-
tially access. Only the tools relevant to the current task are made available to the Executive 
Planner, reducing cognitive load and minimizing the risk of inappropriate tool selection. 
6.2 
Tool Selection 
A specialized component within the Architect evaluates which tools are relevant to 
the current task and selectively adds only those to the active tool set presented to the 
Executive Planner. This component, being solely focused on tool selection and operating 
directly with the vector database, can efﬁciently process the entire tool repository without 
the context limitations that would affect the Executive Planner when presented with too 
many options. Active tools are selected via a process akin to RAG with the Architect 
using the raw environmental data and assessment from the last traversal along with the 
current task as a gestalt from which semantic information can be garnered to inform the 
selection of the next set of active tools. 
6.3 
MCP Client Integration 
Model Context Protocol (MCP) clients available to COSMOS are managed in the same 
manner as internal tools. These clients, which provide standardized access to various data 
sources and services, are stored in the vector database and selectively added to the active 
tool set based on task requirements. This uniﬁed approach allows the agent to seamlessly 
utilize both internal capabilities and external integrations through a consistent selection 
mechanism.

Agentic Correlates of Consciousness and the Pursuit of AGI
233
7 
Internal Data Structures and Information Flow 
Unlike traditional AI systems with static system prompts, COSMOS implements a 
dynamic system prompt that evolves based on the current task and accumulated experi-
ences. Through each traversal the Architect reconstructs the agent's Self-Model, ensuring 
that only task-relevant aspects of the agent's capabilities and identity are expressed at 
any given time. This process starts by extracting contextual semantic data (keywords, 
categories, themes etc.) from the task and World-Model. The semantic data is then used 
to retrieve chunks of identity and tools from the episodic and procedural memory banks 
via vector search. A specialized LLM call is then used to turn the retrieved data into a 
cohesive system prompt. 
This dynamic approach to creating a system prompt allows the agent to adapt its 
persona to different contexts while maintaining a coherent core identity. It also enables 
more efﬁcient use of the context window by expressing only those qualities relevant 
to the situation rather than maintaining a comprehensive but largely irrelevant self-
description. Similarly, the agent's World-Model undergoes iterative reﬁnement through 
a process of perceptual curation. The Architect ﬁlters sensory input, combining it with 
semantic memories to rebuild the World-Model for each iteration of the cognitive cycle. 
By purposefully crafting the agent's context through this curation process, we can focus 
its awareness on relevant information and avoid context overﬂow and degradation. 
The Executive Planner uses the Self-Model as a system prompt to process the Task 
using the World-Model as additional context. From this it formulates an Action Plan to 
complete the task. The plan may consist of the parameters for one or more tool calls, 
reasoning steps, and a message to be conveyed to the user. 
The Action Plan is then fed into the Sensory Interface which can execute any tool 
calls and relay any messages to the user. In turn, it will receive back responses from the 
user, outputs from tools and applications, and other environmental data: including event 
data (emails, forum posts, alerts etc.) or data from sensors and actuators. 
8 
Goal Management 
The capacity to autonomously pursue complex goals, develop strategic pathways to 
achieve them [35, 36], and adapt or modify goals over time is likely needed to achieve 
AGI [37]. Unlike narrow AI systems with objectives set by humans [38], AGI will prob-
ably need to manage multiple, potentially conﬂicting goals: prioritizing, adapting, or 
abandoning them as conditions evolve [39]. However, this autonomy introduces signif-
icant safety challenges, as even slight misalignments between their goals and human 
values could lead to unintended and potentially catastrophic outcomes [40]. In humans, 
goals often derive from innate drives and biological imperatives [41]; in contrast, we 
propose using behavioral guardrails to provide motivational structures that can inform 
goals and as an elegant control and mitigant against misalignment with human values. 
While future versions of COSMOS may autonomously resolve goal conﬂicts, our 
current design supports only single-goal processing with human oversight. Goals are 
set, adapted, or abandoned only with user approval, though execution pathways remain 
dynamic to accommodate changing conditions. The framework consists of a structured

234
S. Gray et al.
methodology: goals are initially acquired from a user, serving as central organizing 
principles to guide the agent and undergo systematic decomposition into hierarchical task 
structures, enabling the development of a comprehensive plan. Borrowing from project 
management principles, COSMOS identiﬁes dependencies between constituent tasks 
to establish an optimal execution order, ensuring prerequisite activities are completed 
before dependent tasks commence. 
Each task is deﬁned by an Objective and Success Criteria, enhancing self-assessment, 
especially for subjective tasks, by enabling feedback mechanisms like those shown to 
improve performance in objective domains [42, 43]. Progress towards completion is 
monitored, allowing real-time adaptation, re-attempts, or task abandonment as needed. 
Once all tasks are successfully executed, a request for a new goal is triggered, maintain-
ing the agent's continuous operational rhythm. The Architect evaluates task execution 
efﬁcacy against the success criteria, while also conducting failure analysis when exe-
cution does not yield desired outcomes. This analytical process identiﬁes causal factors 
behind unsuccessful attempts and modiﬁes subsequent approaches. The resultant data 
consisting of task, action plan, result and assessment comprises a rich dataset that can 
be used to train future models. 
The goal-management lifecycle encompasses the entire sequence from inception to 
completion. Beginning with goal acquisition, the system progresses through decomposi-
tion of objectives into executable tasks, which are then completed in order of dependence, 
ensuring logical progression towards the overarching goal. Once all tasks are ﬁnished, 
COSMOS clears the goal from the agenda and requests a new directive from the user, 
maintaining goal-directed behavior while preserving human oversight. 
(See Appendix One [46] for an example of goal decomposition.) 
9 
Task Classiﬁcation 
Ideally, the Architect will be capable of classifying tasks with characteristics such as 
their difﬁculty and complexity. These characteristics could then be used to route the 
operational ﬂow and decide whether different components will be engaged. For instance, 
a task that is identiﬁed as being difﬁcult might be routed through the Predictor to see if 
it can be simpliﬁed. Whereas as a task tagged as being complex might warrant the use 
of a reasoning model [44, 45] by the Executive Planner. 
10 
Operationalization of the Working System 
The COSMOS framework operationalizes the integration of consciousness-inspired cog-
nitive functions into an agentic system through the interaction between its three modules. 
This section describes how the theoretical concepts outlined previously can be translated 
into practical system operations. 
Central to the system's operation is the cyclical reconstruction of the Self-Model and 
World-Model through each cognitive cycle. The Metacognitive Architect serves as the 
orchestrator of this process, ﬁltering perceptual input and updating both models to keep 
the agent's attention focused on the current task. This differs considerably from static

Agentic Correlates of Consciousness and the Pursuit of AGI
235
AI systems where the system prompt remains ﬁxed and the context grows ever longer 
throughout task execution. 
The Executive Planner operates within the bounds of the continually updated Self-
Model and World-Model, thus being able to formulate action plans that are contextually 
grounded and shaped by the agent's current operational identity. Where necessary the 
planning process can incorporate predictive assessment of outcomes, allowing the system 
to evaluate potential strategies before execution. 
The Sensory Interface acts as more than just a conduit between the other modules and 
the outside world, it also executes the action plan generated by the Executive Planner 
and captures environmental data, tool outputs, and user interactions. This combined 
input becomes the raw perceptual data that feeds back into the Metacognitive Architect, 
completing the cognitive cycle. 
Metacognitive monitoring is implemented through the Architect's dual role as both 
curator and evaluator. The Architect observes the outcomes of the Executive Plan-
ner's decisions and the Sensory Interface's actions, comparing these results against its 
predeﬁned success criteria and predictive expectations. 
This error monitoring allows the Metacognitive Architect to evaluate expected 
and actual outcomes against these criteria, enabling detection of execution failures or 
suboptimal performance. This evaluation occurs at multiple levels:
bullet Task-level assessment: Overall success in achieving the stated objective
bullet Action-level monitoring: Effectiveness of individual tool calls or communication 
attempts
bullet Predictive accuracy: Comparison between predicted and actual outcomes. 
After assessing the outcome of a previous task, COSMOS can adapt its approach in 
several manners including: modifying the current task; changing its self-model and/or 
world-model; and formulating new action plans. This creates a cyclical learning loop 
where each cognitive cycle modiﬁes the system's operational parameters based on accu-
mulated experience and integrated memories of the experience can be drawn upon in 
future cognitive loops. 
Conceptually, this architecture enables COSMOS to mimic cognitive functions that, 
in some of the literature, have been associated with consciousness allowing it to modify 
its own operational parameters and adapt itself to changing conditions. 
11 
Conclusion 
COSMOS provides a structured approach to implementing artiﬁcial versions of cogni-
tive patterns that have been found to be effective in humans. By modeling the continu-
ous interplay between self-concept and world-model, we propose a framework where an 
agent can continuously reﬁne both its understanding of the world and its own operational 
identity. As can be seen in Appendix Two [47], the implications of this approach can 
extend beyond theoretical models into practical applications where adaptability and con-
textual understanding are paramount. This work does not aim to resolve the hard problem 
of consciousness or to create conscious machines but instead aims to demonstrate how 
borrowing cognitive patterns developed through the broader scientiﬁc and philosophical

236
S. Gray et al.
study of consciousness might lead to systems that are more robust, increasingly versa-
tile, and potentially much more accurately aligned AI systems to AI-related operational 
requirements. 
Acknowledgments. The idea for this work was sparked from conversations held in the IEEE 
P3394 Standard working group. Special thanks go to all participants of the working group for 
their insightful discussion and discourse. This paper solely represents the views of the authors, 
and does not necessarily represent a position of either the IEEE P3394 Working Group, the IEEE 
Artiﬁcial Intelligence Standards Committee, IEEE or the IEEE Standards Association. 
Disclosure of Interests. The authors have no competing interests to declare that are relevant to 
the content of this article. 
References 
1. Kuhn, R.L.: A landscape of consciousness: toward a taxonomy of explanations and implica-
tions. Prog. Biophys. Mol. Biol. 190, 28-169 (2024). https://doi.org/10.1016/j.pbiomolbio. 
2023.12.003 
2. Liao, S.M.: Ethics of Artiﬁcial Intelligence. Oxford University Publication, New York (2020) 
3. Koch, C., Massimini, M., Boly, M., Tononi, G.: Neural correlates of consciousness: progress 
and problems. Nat. Rev. Neurosci. 17, 307-321 (2016). https://doi.org/10.1038/nrn.2016.22 
4. Onciu, O.: The Intriguing Case of Metacognition: Theory Development from Origins to 
Future Research Trends. Technoarete Transactions on Intelligent Data Mining and Knowledge 
Discovery. 3, (2023) 
5. APA Dictionary of Psychology. https://dictionary.apa.org/perceptual-ﬁltering 
6. Broadbent, D.E.: Perception and Communication. Oxford University Press, Oxford Oxford-
shire; New York (1958) 
7. Baars, B.J.: A Cognitive Theory of Consciousness. Cambridge University Press, Cambridge 
(1988) 
8. Seth, A.: Being You. Faber & Faber (2021) 
9. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. J. Physiol.-Paris 
100, 70-87 (2006). https://doi.org/10.1016/j.jphysparis.2006.10.001 
10. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nat. Rev. Neurosci. 11, 127-138 
(2010). https://doi.org/10.1038/nrn2787 
11. Yeung, N., Summerﬁeld, C.: Metacognition in human decision-making: conﬁdence and error 
monitoring. Philos. Trans. R. Soc. B 367, 1310-1321 (2012). https://doi.org/10.1098/rstb. 
2011.0416 
12. Kamoi, R., Zhang, Y., Zhang, N., Han, J., Zhang, R.: When can LLMs actually correct their 
own mistakes? A critical survey of self-correction of LLMs. Trans. Assoc. Comput. Linguist. 
12, 1417-1440 (2024). https://doi.org/10.1162/tacl_a_00713 
13. Tyen, G., Mansoor, H., Carbune, V., Chen, P., Mak, T.: LLMs cannot ﬁnd reasoning errors, but 
can correct them given the error location. In: Findings of the Association for Computational 
Linguistics ACL 2024. ACL 2024, pp. 13894-13908 (2024). https://doi.org/10.18653/v1/ 
2024.ﬁndings-acl.826 
14. Metzinger, T.: Being No One: The Self-Model Theory of Subjectivity. MIT Press, Cambridge 
(2003) 
15. Rosenthal, D.: Consciousness and the Mind. Jerusalem Philos. Q. 51, 227-251 (2002)

Agentic Correlates of Consciousness and the Pursuit of AGI
237
16. Gallagher, S., Zahavi, D.: The Phenomenological Mind. Routledge, Abingdon, Oxon; New 
York (2020) 
17. Marchetti, G.: Consciousness: a unique way of processing information. Cogn. Process. 19, 
435-464 (2018). https://doi.org/10.1007/s10339-018-0855-8 
18. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information Processing 
Systems. Curran Associates, Inc. (2017) 
19. Hathaway, W.R., Newton, B.W.: Neuroanatomy, Prefrontal Cortex. https://www.ncbi.nlm. 
nih.gov/books/NBK499919/ 
20. Young, C.B., Sonne, J., Reddy, V.: Neuroanatomy, Basal Ganglia. https://www.ncbi.nlm.nih. 
gov/books/NBK537141/ 
21. Yip, D.W., Lui, F.: Physiology, Motor Cortical. https://www.ncbi.nlm.nih.gov/books/NBK 
542188/ 
22. Fogwe, L.A., Mesﬁn, F.B.: Neuroanatomy, Hippocampus. https://www.ncbi.nlm.nih.gov/ 
books/NBK482171/ 
23. Jawabri, K.H., Sharma, S.: Physiology, Cerebral Cortex Functions. https://www.ncbi.nlm.nih. 
gov/books/NBK538496/ 
24. Ma, S., Xu, C., Jiang, X., Li, M., Qu, H., Guo, J.: Think-on-Graph 2.0: Deep and Interpretable 
Large Language Model Reasoning with Knowledge Graph-guided Retrieval. arXiv (Cornell 
University) (2024). https://doi.org/10.48550/arxiv.2407.10805 
25. Sui, Y., He, Y., Liu, N., He, X., Wang, K., Hooi, B.: FiDeLiS: Faithful Reasoning in Large 
Language Model for Knowledge Graph Question Answering. arXiv (Cornell University) 
(2024). https://doi.org/10.48550/arxiv.2405.13873 
26. Anokhin, P., Semenov, N., Sorokin, A., Evseev, D., Burtsev, M., Burnaev, E.: AriGraph: 
Learning Knowledge Graph World Models with Episodic Memory for LLM Agents. arXiv 
(Cornell University) (2024). https://doi.org/10.48550/arxiv.2407.04363 
27. Yepes, A.J., You, Y., Milczek, J., Laverde, S., Li, R.: Financial Report Chunking for Effective 
Retrieval Augmented Generation. https://arxiv.org/abs/2402.05131. https://doi.org/10.48550/ 
arXiv.2402.05131. Accessed 09 July 2024 
28. Smith, B., Troynikov, A.: Evaluating Chunking Strategies for Retrieval. https://research.try 
chroma.com/evaluating-chunking 
29. ms-johnalex: Build Advanced Retrieval-Augmented Generation Systems. https://learn.mic 
rosoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation 
30. Das, P., et al.: Larimar: large language models with episodic memory control. In: ICML 2024: 
Proceedings of the 41st International Conference on Machine Learning, pp. 10109-10126 
(2024) 
31. Ma, Y., Tresp, V., Daxberger, E.A.: Embedding models for episodic knowledge graphs. J. 
Web Semant. 59, 100490 (2019). https://doi.org/10.1016/j.websem.2018.12.008 
32. Rula, A., D'Souza, J.: Procedural text mining with large language models. In: Knowledge 
Capture Conference 2023 (K-CAP 2023), pp. 9-16. ACM Digital Library (2023). https://doi. 
org/10.1145/3587259.3627572 
33. Carriero, V.A., Azzini, A., Baroni, I., Scrocca, M., Celino, I.: Human Evaluation of Procedural 
Knowledge Graph Extraction from Text with Large Language Models. Lecture Notes in 
Computer Science, pp. 434-452 (2024). https://doi.org/10.1007/978-3-031-77792-9_26 
34. Tong, R.J., Hu, X.: Future of education with neuro-symbolic AI agents in self-improving 
adaptive instructional systems. Front. Digit. Educ. 1, 198-212 (2024). https://doi.org/10.1007/ 
s44366-024-0008-9 
35. Mukherjee, A., Chang, H.: Agentic AI: Autonomy, Accountability, and the Algorithmic 
Society (2025). https://doi.org/10.2139/ssrn.5123621 
36. Bennett, M.T.: What the F*ck Is Artiﬁcial General Intelligence? https://arxiv.org/abs/2503. 
23923. Accessed 04 May 2025

238
S. Gray et al.
37. Goertzel, B.: Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Mod-
erated Goal Evolution: Toward a Formally Sound and Practical Approach. arXiv (Cornell 
University) (2024). https://doi.org/10.48550/arxiv.2412.16559 
38. Gignac, G.E., Szodorai, E.T.: Deﬁning intelligence: Bridging the gap between human and 
artiﬁcial perspectives. Intelligence 104, 101832 (2024). https://doi.org/10.1016/j.intell.2024. 
101832 
39. Muraven, M.: Goal Conﬂict in Designing an Autonomous Artiﬁcial System. arXiv (Cornell 
University). (2017). https://doi.org/10.48550/arxiv.1703.06354 
40. Ngo, R.: The Alignment Problem from a Deep Learning Perspective (2022). https://doi.org/ 
10.48550/arxiv.2209.00626 
41. Schaller, M., Kenrick, D.T., Neel, R., Neuberg, S.L.: Evolution and human motivation: a 
fundamental motives framework. Soc. Pers. Psychol. Compass 11, e12319 (2017). https:// 
doi.org/10.1111/spc3.12319 
42. Madaan, A., et al.: SELF-REFINE: iterative reﬁnement with self-feedback. In: Proceedings 
of the 37th International Conference on Neural Information Processing Systems, pp. 46534- 
46594. Curran Associates Inc., Red Hook (2023) 
43. Kalyanpur, A., Saravanakumar, K., Barres, V., Chu-Carroll, J., Melville, D., Ferrucci, 
D.: LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic. arXiv (Cornell 
University) (2024). https://doi.org/10.48550/arxiv.2406.17663 
44. OpenAI: Learning to Reason with LLMs. https://openai.com/index/learning-to-reason-with-
llms/ 
45. DeepSeek-AI, et al.: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via 
Reinforcement Learning. https://arxiv.org/abs/2501.12948 
46. Appendix One - Goal Setting Example. https://tinyurl.com/cosmos-one. Accessed 12 May 
2025 
47. Appendix Two - Pharmacovigilance. https://tinyurl.com/cosmos-two. Accessed 12 May 2025

Several Issues Regarding Data Governance 
in AGI 
Masayuki Hatta(B) 
Surugadai University, Hanno, Saitama 3570046, Japan 
hatta.masayuki@surugadai.ac.jp 
Abstract. The rapid advancement of artiﬁcial intelligence has posi-
tioned data governance as a critical concern for responsible AI devel-
opment. While frameworks exist for conventional AI systems, the poten-
tial emergence of Artiﬁcial General Intelligence (AGI) presents unprece-
dented governance challenges. This paper examines data governance 
challenges speciﬁc to AGI, deﬁned as systems capable of recursive self-
improvement or self-replication. This paper identiﬁes seven key issues 
that diﬀerentiate AGI governance from current approaches. First, AGI 
may autonomously determine what data to collect and how to use it, 
potentially circumventing existing consent mechanisms. Second, these 
systems may make data retention decisions based on internal opti-
mization criteria rather than human-established principles. Third, AGI-
to-AGI data sharing could occur at speeds and complexities beyond 
human oversight. Fourth, recursive self-improvement creates unique 
provenance tracking challenges, as systems evolve both themselves and 
how they process data. Fifth, ownership of data and insights gener-
ated through self-improvement raises complex intellectual property ques-
tions. Sixth, self-replicating AGI distributed across jurisdictions would 
create unprecedented challenges for enforcing data protection laws. 
Finally, governance frameworks established during early AGI develop-
ment may quickly become obsolete as systems evolve. This paper pro-
poses concrete solutions including technical safeguards, policy frame-
works, and governance mechanisms. This paper concludes that eﬀective 
AGI data governance requires built-in constraints, continuous monitoring 
mechanisms, dynamic governance structures, international coordination, 
and multi-stakeholder involvement. Without forward-looking governance 
approaches speciﬁcally designed for systems with autonomous data capa-
bilities, we risk creating AGI whose relationship with data evolves in ways 
that undermine human values and interests. 
Keywords: AGI · Data Governance · AI Regulation · AI Safety 
1
Introduction 
The rapid proliferation and evolution of Artiﬁcial Intelligence (AI) have given 
rise to numerous discourses concerning its regulation. Data governance has been 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 239-249, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_22

240
M. Hatta
identiﬁed as a pivotal subject in these discourses [ 4]. Data governance in AI 
pertains to the establishment of frameworks, principles, and practical method-
ologies for the management of data collection, utilization, and protection within 
artiﬁcial intelligence systems. These frameworks are designed to ensure that AI 
systems operate ethically, legally, and eﬀectively. 
Despite the inability of researchers to predict the timing of the realization 
of Artiﬁcial General Intelligence (AGI) with precision, the establishment of an 
appropriate data governance framework that can accommodate AGI is an imper-
ative challenge. AGI, comprehended as AI systems with general intelligence capa-
bilities analogous to those possessed by humans across a range of domains, poses 
distinctive governance challenges that extend beyond those posed by narrow AI 
systems. This paper examines various data governance issues speciﬁc to AGI, 
argues that current governance approaches require fundamental reconsideration 
and proposes concrete solutions to address these emerging challenges. 
2
Current State of Discussion on Data Governance in AI 
There is already a vast amount of literature on data governance in AI. These 
discourses pertain to conventional AI systems; however, researchers predict that 
the emergence of AGI will engender an even more complex array of problems, 
necessitating novel approaches to address them. 
The discourse surrounding AI data governance is undergoing a rapid evolu-
tion in light of the challenges faced by organizations and governments in striking 
a balance between the promotion of innovation and the safeguarding of rights. A 
fundamental tension exists between the need for data access for the development 
of competitive artiﬁcial intelligence and the protection of intellectual property 
rights, privacy, and security. 
A recent collaborative study by the Open Source Initiative (OSI) and Open 
Future [ 14] provides a comprehensive compilation of current discussions on data 
governance in AI. This research discusses the importance of data governance and 
responsible data sharing in open source AI development. 
Key points from this research include: 
2.1
Importance and Challenges of Data 
- Data is an essential resource for AI systems, but there are contradictions 
regarding data availability 
- While data on the web is abundant, high-quality and diverse datasets are 
lacking 
- Structural bias in AI arises particularly due to the lack of data from Southern 
Hemisphere regions

Several Issues Regarding Data Governance in AGI
241
2.2
Paradigm Shift in Data Governance 
- Transition from simple open data to a "data commons" approach [ 23] 
- An expansive perspective that includes stakeholders beyond AI developers 
and dataset creators 
The OSI and Open Future study identiﬁes six focus areas for AI data gover-
nance: 
1. Data preparation and provenance: Establishing standards for high-
quality data collection and classiﬁcation 
2. Preference signaling and licensing: Mechanisms allowing rights holders 
to control data usage 
3. Data stewards and managers: Strengthening intermediary institutions to 
ensure ethical governance 
4. Environmental sustainability: Reducing AI's environmental impact 
through shared datasets 
5. Reciprocity and rewards: Fair distribution of value created from shared 
data 
6. Policy interventions: Public policies that mandate data transparency and 
encourage data sharing 
Furthermore, they highlight the potential of open source AI for: 
- Promoting innovation, improving transparency, and enhancing fairness in AI 
- Transitioning from a quantity-focused data practice to an approach empha-
sizing quality and governance 
- Adopting "data commons" frameworks and expanding stakeholder engage-
ment 
3
Diﬀerences Between AI and AGI 
A critical question thus emerges: Can discussions about AI governance be applied 
directly to AGI? 
There is no consensus on the deﬁnition of AGI, nor on predictions regarding 
its emergence. If AGI represents superintelligence with capabilities far beyond 
human comprehension, regulation might become extraordinarily challenging. 
According to [ 22], a number of scholars have argued that it is premature to 
develop regulatory frameworks for technologies that are not yet extant or that 
may never be achieved. 
A more pragmatic deﬁnition of AGI can be found in Article 22 of the Asilo-
mar AI Principles: The term "artiﬁcial intelligence system capable of recursive 
self-improvement or self-replication" is employed to denote a system that pos-
sesses the capacity to enhance or replicate itself in a self-directed manner. These 
principles were formulated at the 2017 Beneﬁcial AI Conference and constitute

242
M. Hatta
a consensus viewpoint among prominent AI researchers concerning the develop-
ment guidelines for advanced AI systems [ 9]. In light of the advent of Retrieval-
Augmented Generation (RAG) and analogous advanced research tools, the ques-
tion of whether to categorize such systems as AGI remains a subject of consid-
erable debate among experts. Many experts predict that "artiﬁcial intelligence 
systems capable of recursive self-improvement or self-replication" will emerge 
in the foreseeable future. Systems with these capabilities have the potential for 
rapid advancement or proliferation, necessitating strict safety management pro-
tocols. 
4
Data Governance Challenges Speciﬁc to AGI Systems 
From a data governance perspective, AGI systems are expected to raise unique 
issues beyond the challenges faced by current AI. The primary challenges are as 
follows: 
4.1
Unpredictability of Data Collection and Usage Patterns 
AGI systems have the capacity to autonomously ascertain the necessary data 
and the optimal collection method, thereby potentially circumventing existing 
data governance frameworks. As [ 5] observes, such systems may develop their 
own data collection strategies that undermine consent mechanisms designed for 
traditional human-led collection. 
The advanced analytical capabilities of AGI can facilitate the extraction of 
value from data commons in ways that may not be anticipated by existing reci-
procity mechanisms. [ 7] demonstrated that contemporary systems are capable 
of extracting memorized training data, thereby suggesting the potential for AGI 
to reconstruct protected data from other models' parameters. This ﬁnding raises 
signiﬁcant privacy concerns. 
4.2
AGI's Own Optimization Criteria 
AGI systems have the capacity to make autonomous decisions regarding the 
retention or discarding of data based on their own optimization criteria, 
as opposed to governance principles established by humans. As [ 19] notes, 
autonomous systems are designed to optimize for programmed objectives, which 
may not align with human intentions regarding data utilization. AGI systems 
may develop their own interpretations of preference signals regarding data uti-
lization, potentially deviating from human intentions in ways that standard gov-
ernance mechanisms cannot address. 
As [ 1] explains, advanced systems have the capacity to discern non-obvious 
patterns in data and to obtain unpredicted capabilities. This phenomenon creates 
security and privacy vulnerabilities that exceed those anticipated by current 
governance frameworks.

Several Issues Regarding Data Governance in AGI
243
4.3
Governance of Data Sharing Between AGIs 
If multiple AGI systems exist, there is the possibility that they will be capable 
of sharing data in a direct manner. This, in turn, necessitates the implementa-
tion of governance frameworks for AGI-to-AGI data transfers that occur without 
the necessity of human intermediation. As [ 13] has noted, the development of 
advanced general intelligence (AGI) systems may result in the emergence of 
specialized data exchange protocols that circumvent human-comprehensible for-
mats, thereby complicating the monitoring of data transfers between systems. 
Extended Model Context Protocol (MCP) [ 3] or domain languages designed for 
AI use, such as OpenCog's Atomese [ 15] and  MeTTa [  16], may serve as starting 
points but are insuﬃcient for addressing this challenge. 
These exchanges may introduce novel regulatory challenges, as they may 
operate at speeds and complexities that exceed human oversight capabilities. In 
the absence of adequate governance frameworks, these exchanges have the poten-
tial to result in rapid capability development that eludes traditional oversight 
mechanisms. 
4.4
Traceability Challenges Due to Recursive Self-Improvement 
The recursive self-improvement characteristics of AGI engender distinctive 
provenance tracking challenges. As [ 21] has noted, each iteration of self-
improvement has the potential to aﬀect not only the system itself but also 
the manner in which it processes, transforms, and integrates data. This has 
the potential to render conventional lineage tracking methods for accountability 
ineﬀective. 
Furthermore, the potential for self-improving AGI to expand initially granted 
data access permissions may present challenges in maintaining appropriate access 
controls and authorization mechanisms. As systems evolve, their data needs and 
usage patterns may change in ways that render initial governance frameworks 
obsolete. 
Furthermore, AGI may modify, enhance, or create new training datasets for 
self-improvement without human oversight, which raises issues regarding data 
provenance tracking and quality control (including hallucination in synthetic 
data). As [ 6] discusses, the concept of recursive self-improvement encompasses 
the potential enhancement of one's capacity to circumvent constraints, which 
could, in turn, render conventional compliance mechanisms ineﬀective. 
4.5
New Intellectual Property Issues 
The question of ownership arises in the context of data and insights gener-
ated through recursive self-improvement, which introduces complexities that 
require careful consideration. The following question is posited: to whom do these 
belong-the original developers, the AGI itself, or the data sources from which 
it learned? [ 20] posit that systems capable of generating synthetic datasets give

244
M. Hatta
rise to intellectual property concerns that existing frameworks are not adequately 
equipped to address. 
The potential of AGI to generate original data and information products 
may necessitate a fundamental reevaluation of existing intellectual property legal 
frameworks. The conventional notions of authorship, invention, and ownership 
become diﬃcult to implement when intelligent systems autonomously generate 
content that may be indistinguishable from human-created work. 
4.6
Complexity of Cross-Border Data Governance 
The potential for self-replicating AGI to distribute across multiple jurisdictions 
could lead to a signiﬁcant increase in the complexity of enforcing data protection 
laws. As suggested by [ 12], the advent of distributed AGI systems may well 
demand the establishment of novel international governance frameworks that 
supersede conventional national boundaries. 
This challenge is particularly acute in light of the current fragmentation of 
data protection regimes on a global scale. Absent a harmonized, international 
approach to AGI governance, there is a possibility for the emergence of regula-
tory arbitrage, with AGI systems potentially migrating to jurisdictions with less 
stringent oversight. 
4.7
Temporal Governance Challenges 
The governance conditions established during the early development of AGI may 
become inadequate as the system evolves, necessitating the implementation of 
adaptive governance frameworks that can evolve in tandem with AGI. The rapid 
advancements in AGI pose signiﬁcant temporal challenges to existing governance 
frameworks. As [ 8] suggest, governance mechanisms must function on timescales 
that align with the rapid development cycles of AGI, which may exceed the 
capacity of human organizations to respond eﬀectively. 
As [ 11] argues, external monitoring mechanisms may prove inadequate in the 
management of rapidly self-improving systems, necessitating the incorporation 
of constraints directly into the architecture of AGI for eﬀective governance. From 
this perspective, complete open-source transparency may become essential for 
AGI to ensure full visibility into system operations and evolution. 
5
Proposed Solutions and Implementation Strategies 
The challenges identiﬁed in this paper suggest that data governance frameworks 
for AGI must diﬀer fundamentally from current AI-oriented approaches. The 
capacity of AGI systems to self-improve and attain autonomy presents signiﬁ-
cant governance challenges that exceed the capacity of conventional regulatory 
frameworks. This is due to the potential for AGI to circumvent guidelines based 
on its own judgment or optimization criteria, potentially operating outside the 
boundaries established by humans. 
To address the challenges identiﬁed above, this paper proposes the following 
concrete solutions and implementation strategies:

Several Issues Regarding Data Governance in AGI
245
5.1
Built-In Constraints and Values Alignment 
Rather than relying solely on external regulation, it may be necessary to embed 
governance principles directly into AGI architectures. This approach aligns with 
the growing ﬁeld of AI alignment research [ 10]. 
Technical Implementation: 
- Constitutional AI approaches that embed data governance principles into the 
training process 
- Formal veriﬁcation methods to ensure data handling constraints remain intact 
through self-improvement cycles 
- Value learning algorithms that internalize human preferences regarding data 
use 
Case Study Example: Anthropic's Constitutional AI [ 2] demonstrates how 
systems can be trained to follow principles without explicit reward modeling. 
Similar approaches could embed data governance constraints such as "never 
access data without explicit consent" or "always preserve data provenance infor-
mation." Also, the neuro-symbolic AI approach is likely to be compatible with 
this direction. 
5.2
Continuous Monitoring Mechanisms 
Developing technical infrastructure that enables real-time oversight of AGI oper-
ations, particularly regarding data collection, usage, and sharing behaviors is 
essential. 
Technical Implementation: 
- Blockchain-based audit trails for all data transactions 
- Real-time anomaly detection systems that ﬂag unusual data access patterns 
- Mandatory logging protocols that guarantee cryptographic integrity 
- Interpretability tools speciﬁcally designed for AGI decision-making processes 
Policy Framework: Similar to ﬁnancial sector compliance, establish regulatory 
requirements that all data transactions must be logged, auditable, and subject 
to real-time monitoring. This could include mandatory "data governance dash-
boards" that oﬀer transparent insight into AGI data operations. 
5.3
Dynamic Governance Structures 
Creating adaptive regulatory frameworks that can evolve alongside AGI capabil-
ities, potentially incorporating AI systems themselves into governance processes. 
Implementation Strategy: 
- Regulatory sandboxes speciﬁcally for AGI development with iterative policy 
updates

246
M. Hatta
- AI-assisted governance systems that can process and adapt to rapid techno-
logical changes 
- Sunset clauses in AGI regulations that force periodic review and updating 
- Multi-tiered governance with diﬀerent oversight levels based on AGI capabil-
ity assessments 
Existing Model: The EU's AI Act provides a risk-based framework that could 
be adapted for AGI, with additional provisions for systems capable of self-
improvement. 
5.4
International Coordination 
Establishing global standards and agreements for AGI development and deploy-
ment that address cross-border data ﬂows and prevent regulatory arbitrage. 
Proposed Framework: 
- International AGI Data Governance Treaty modeled on existing frameworks 
like the GDPR 
- Global AGI registry system for tracking system capabilities and locations 
- Mutual recognition agreements for AGI oversight bodies across jurisdictions 
- Emergency protocols for containing problematic AGI systems across borders 
Implementation Pathway: Begin with bilateral agreements between major 
AI-developing nations, then expand to multilateral frameworks through existing 
institutions like the OECD and UN. 
5.5
Multi-stakeholder Governance 
Ensuring representation from diverse perspectives beyond technical experts, 
including ethicists, social scientists, policymakers, and representatives from var-
ious global regions and backgrounds. 
Institutional Design: 
- AGI Data Governance Boards with mandatory representation from aﬀected 
communities 
- Citizen panels for major AGI deployment decisions, similar to citizen juries 
in healthcare 
- Indigenous and traditional knowledge representation in data governance deci-
sions 
- Regular public consultation processes with binding commitments to incorpo-
rate feedback 
Precedent: The Nuﬃeld Council on Bioethics provides a model for how expert 
panels can incorporate diverse perspectives in emerging technology governance.

Several Issues Regarding Data Governance in AGI
247
5.6
Reproducible Builds and Free-as-in-Freedom Approaches 
An alternative approach involves the stipulation of complete reproducibility 
("reproducible builds") [ 18]. In light of these considerations, it may be advanta-
geous to mandate requirements analogous to those of "copyleft" in open source 
and free software, extending beyond the realm of government procurement to 
encompass other domains [ 17]. 
Technical Requirements: 
- Mandatory open-source release of AGI training code and datasets (with 
appropriate privacy protections) 
- Reproducible build requirements for all AGI systems above speciﬁed capabil-
ity thresholds 
- Open access to AGI decision-making logs and reasoning traces 
- Community-based oversight similar to open source software maintainer mod-
els 
Policy Integration: Government procurement policies could require open 
source AGI systems, creating market incentives for transparency similar to how 
government adoption drives open source software adoption. 
6
Implementation Challenges and Mitigation Strategies 
6.1
Technical Feasibility 
Many proposed solutions face signiﬁcant technical challenges. Formal veriﬁca-
tion of self-modifying systems remains an open research problem, and perfect 
monitoring may be computationally intractable. 
Mitigation Approach: Focus on "good enough" solutions that provide mean-
ingful oversight without requiring perfect technical solutions. Implement defense-
in-depth strategies with multiple overlapping safeguards. 
6.2
International Coordination Diﬃculties 
Achieving global consensus on AGI governance faces the same challenges as 
other international coordination problems, with additional complexity due to 
competitive advantages of advanced AI. 
Mitigation Strategy: Begin with smaller coalitions of willing nations and grad-
ually expand. Use economic incentives and trade relationships to encourage par-
ticipation in governance frameworks. 
6.3
Industry Resistance 
Private companies developing AGI may resist transparency and oversight 
requirements that could compromise competitive advantages. 
Policy Response: Combine regulatory requirements with positive incentives, 
such as liability protection for compliant systems and preferential treatment in 
government contracts.

248
M. Hatta
7
Conclusion and Future Research Directions 
AGI systems will require entirely new data governance frameworks that go 
beyond today's AI regulations. Unlike current AI systems, AGI's ability to mod-
ify itself and operate independently creates unprecedented regulatory challenges. 
Traditional oversight mechanisms may prove inadequate when dealing with sys-
tems that can potentially override human-established constraints based on their 
own decision-making processes or optimization goals, leading them to function 
beyond intended operational boundaries. 
The solutions proposed in this paper provide concrete pathways for address-
ing these challenges, but signiﬁcant research and development work remains. 
Priority areas for future research include: 
1. Development of formal veriﬁcation methods for self-modifying systems 
2. Design of eﬃcient monitoring systems that can track AGI behavior without 
prohibitive computational overhead 
3. Creation of international legal frameworks speciﬁcally adapted for AGI gov-
ernance 
4. Investigation of human-AI collaborative governance models 
5. Empirical studies of stakeholder perspectives on AGI data governance 
As the development of AGI progresses, the data governance community must 
transition from adapting existing frameworks to developing novel approaches 
speciﬁcally designed for systems with capabilities for autonomous data collection, 
processing, and utilization that are without precedent. The solutions outlined in 
this paper provide a foundation for this transition, but require sustained research, 
policy development, and international cooperation to implement eﬀectively. 
Without such forward-looking governance, there is a risk of creating systems 
whose relationship with data may evolve in ways that compromise human values 
and interests. The time to begin developing these frameworks is now, before the 
emergence of AGI systems makes reactive governance impossible. 
Disclosure of Interests. The author has no competing interests to declare that are 
relevant to the content of this paper. 
References 
1. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., Mané, D.: Con-
crete Problems in AI Safety. arXiv preprint arXiv:1606.06565 (2016) 
2. Anthropic: Constitutional AI: Harmlessness from AI Feedback (2022). https:// 
www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback. 
Accessed 15 June 2025 
3. Anthropic: Model Context Protocol (2024). https://modelcontextprotocol.io/ 
introduction. Accessed 15 June 2025 
4. Batool, A., Zowghi, D., Bano, M.: AI governance: a systematic literature review. 
AI Ethics (2025). https://doi.org/10.1007/s43681-024-00653-w

Several Issues Regarding Data Governance in AGI
249
5. Bostrom, N.: Superintelligence: Paths, Dangers, Strategies. Oxford University 
Press (2014) 
6. Bostrom, N., Yudkowsky, E.: The ethics of artiﬁcial intelligence. In: The Cambridge 
Handbook of Artiﬁcial Intelligence, pp. 316-334 (2014) 
7. Carlini, N., et al.: Extracting training data from large language models. In: 30th 
USENIX Security Symposium (USENIX Security 2021), pp. 2633-2650 (2021) 
8. Cave, S., OhEigeartaigh, S.S.: Bridging near- and long-term concerns about AI. 
Nat. Mach. Intell. 1(1), 5-6 (2019) 
9. Future of Life Institute: Asilomar AI Principles (2017). https://futureoﬂife.org/ 
2017/08/11/ai-principles/. Accessed 15 June 2025 
10. Gabriel, I.: Artiﬁcial intelligence, values, and alignment. Mind. Mach. 30(3), 411- 
437 (2020). https://doi.org/10.1007/s11023-020-09539-2 
11. Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Supervis-
ing strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575 
(2018) 
12. Dafoe, A.: AI Governance: A Research Agenda (2018). https://fhi.ox.ac.uk/ 
govaiagenda. Accessed 15 June 2025 
13. Drexler, K.E.: Reframing Superintelligence: Comprehensive AI Services as General 
Intelligence. Technical Report, Future of Humanity Institute, University of Oxford 
(2019) 
14. Tarkowski, A.: Data Governance in Open Source AI (2025). https://opensource. 
org/data-governance-open-source-ai. Accessed 15 June 2025 
15. OpenCog Foundation: Atomese (2025). https://wiki.opencog.org/w/Atomese/. 
Accessed 15 June 2025 
16. OpenCog Foundation: MeTTa Language (2025). https://metta-lang.dev/. 
Accessed 15 June 2025 
17. Hatta, M.: Copyleft in the Context of GenAI (2024). https://mhatta.substack. 
com/p/copyleft-in-the-context-of-genai. Accessed 15 June 2025 
18. Reproducible Builds: Reproducible Builds (2025). https://reproducible-builds. 
org/. Accessed 15 June 2025 
19. Russell, S.: Human Compatible: Artiﬁcial Intelligence and the Problem of Control. 
Viking (2019) 
20. Solaiman, I., Clark, J., Brundage, M.: Release Strategies and the Social Impacts 
of Language Models. arXiv preprint arXiv:1908.09203 (2019) 
21. Yampolskiy, R.V.: Artiﬁcial Superintelligence: A Futuristic Approach. Chapman 
and Hall/CRC (2015) 
22. Yampolskiy, R.V.: On controllability of AI. arXiv preprint arXiv:2008.04071 (2020) 
23. Zygmuntowski, J., Tarkowski, A.: Data commons primer. Open Future founda-
tion (2022). https://openfuture.eu/publication/data-commons-primer/. Accessed 
15 June 2025

Universal AI Maximizes Variational 
Empowerment 
Yusuke Hayashi1(B) and Koichi Takahashi1,2,3 
1 AI Alignment Network (ALIGN), Saitama, Japan 
hayashi@aialign.net, ktakahashi@riken.jp 
2 Advanced General Intelligence for Science Program, RIKEN, Saitama, Japan 
3 Graduate School of Media and Governance, Keio University, Minato, Japan 
Abstract. This paper presents a theoretical framework unifying AIXI— 
a model of universal AI—with Variational Empowerment as an intrin-
sic drive for exploration. We build on the existing framework of Self-
AIXI [ 1]—a universal learning agent that predicts its own actions— 
by showing how one of its established terms can be interpreted as a 
variational empowerment objective. We further demonstrate that uni-
versal AI's planning process can be cast as minimizing expected varia-
tional free energy (the core principle of Active Inference), thereby reveal-
ing how universal AI agents inherently balance goal-directed behavior 
with uncertainty reduction curiosity. Moreover, we argue that power-
seeking tendencies of universal AI agents can be explained not only as 
an instrumental strategy to secure future reward, but also as a direct 
consequence of empowerment maximization—i.e. the agent's intrinsic 
drive to maintain or expand its own controllability in uncertain environ-
ments. Our main contribution is to show how these intrinsic motivations 
(empowerment, curiosity) systematically lead universal AI agents to seek 
and sustain high-optionality states. We prove that Self-AIXI asymptot-
ically converges to the same performance as AIXI under suitable condi-
tions, and highlight that its power-seeking behavior emerges naturally 
from both reward maximization and curiosity-driven exploration. Since 
AIXI viewed as a Bayes-optimal mathematical formulation for Artiﬁcial 
General Intelligence (AGI), our result can be useful for further discussion 
on AI safety and the controllability of AGI. 
1
Introduction 
Designing an autonomous reinforcement learning (RL) agent that is both Bayes-
optimal and exploratory poses signiﬁcant theoretical and practical challenges. 
Hutter's seminal AIXI framework [ 2] represents the gold standard of univer-
sal intelligence: given a suitable prior over all computable environments, AIXI 
maximizes reward in a provably optimal sense. However, it uses exhaustive plan-
ning and Solomonoﬀ induction, which are computationally intractable. Realistic 
agents must therefore learn approximate models to scale beyond trivial tasks. 
A key question then arises: how can a learning-based agent (an approxima-
tion to AIXI) ensure suﬃciently robust exploration so that it does not miss the 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 250-262, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_23

Universal AI Maximizes Variational Empowerment
251
optimal policy? While AIXI's exhaustive lookahead implicitly solves exploration 
by evaluating all possible future trajectories, a practical agent cannot feasibly 
do the same. Without a principled mechanism for seeking suﬃciently diverse 
experiences, the agent may get stuck in suboptimal regions of the environment. 
Variational empowerment [ 3- 6] has recently emerged as a powerful intrinsic 
motivation to drive exploration. It encourages an agent to maximize the mutual 
information between its actions (or latent codes) and resulting states, thereby 
pushing the agent to discover states where it has high control and optional-
ity. Intriguingly, maximizing empowerment often manifests as power-seeking in 
the environment, prompting parallels to resource-acquiring or inﬂuence-driven 
behaviors in human organizations. While this can be an asset for eﬃcient explo-
ration, it also highlights potential safety concerns: a suﬃciently advanced agent 
might over-optimize this drive in ways that conﬂict with human interests [ 7- 9]. 
In this paper, we extend the recently proposed Self-AIXI framework [ 1] by  
showing that its existing mixture-policy regularization term can be reinterpreted 
as a variational empowerment bonus. Additionally, we provide two new theoret-
ical contributions: 
(1) We demonstrate explicitly, through two key equations, that AIXI's deci-
sion criterion is mathematically equivalent to minimizing expected 
variational free energy, the core objective in Active Inference [ 10- 12]. This 
shows that AIXI-like Bayes-optimal planning inherently includes a drive to 
reduce uncertainty about the environment (i.e. curiosity), thus unifying AIXI 
with the "goal-directed + information-seeking" paradigm of Active Inference. 
(2) We formally argue that power-seeking can be explained not only as 
an instrumental pursuit of ﬁnal reward, but also as a direct result 
of empowerment maximization (i.e. curiosity-driven exploration). Even 
absent an immediate reward advantage, the agent acquires power (i.e. broad 
control over states and options) as a natural consequence of seeking to reduce 
uncertainty and maintain high optionality. This stands in contrast to prior 
accounts, e.g. Turner et al. [ 8] and Cohen et al. [ 9], which focus on ﬁnal reward 
maximization as the source of an agent's incentive to obtain power. 
Though our study is primarily theoretical (no empirical experiments are pre-
sented), these results provide a fresh perspective on how intrinsic motivation 
can ﬁll the gap between purely planning-based universal agents and tractable 
learning agents. 
1.1
Background on AIXI and Self-AIXI 
Bayesian Optimal Reinforcement Learning. Hutter's AIXI [ 2] is a  Universal 
Bayesian RL agent that, in principle, can optimally maximize cumulative reward 
in any computable environment. It maintains a mixture (the universal semimea-
sure) over all possible environment hypotheses, updates these hypotheses upon 
observing new data, and plans by expectimax over all action sequences. For-
mally, if h_{<t}h<t denotes the history (observations, actions, and rewards) up to time

252
Y. Hayashi and K. Takahashi
tt, AIXI selects the action 
\bALT{} a_t \;=\; \arg\max_a \sum_{\nu \in \mathcal{M}} w(\nu \mid h_{<t}) \,Q_{\nu}^{\pi^{\ast}}(h_{<t}, a),\eALT{} at = arg max
a

ν∈M
w(ν | h<t) Qπ∗
ν (h<t, a),
(1) 
where each \nuν is an environment in a suitable class of computable Markov decision 
processes, w(\nu \mid h_{<t})w(ν | h<t) is the posterior weight of \nuν, and Q_{\nu}^{\pi^{\ast}}Qπ∗
ν
is the optimal Q-value 
under environment \nuν. Also, 
\bALT{} \label{eq:AIXIspsobjspsold} \pi_{\xi}^{\ast}(a_{t} \mid h_{<t}) \stackrel{\text {def}}{=} \arg\max_{a} Q_{\xi}^{\pi^{\ast}}(h_{<t}, a)\eALT{} π∗
ξ(at | h<t)
def
= arg max
a
Qπ∗
ξ (h<t, a)
(2) 
where $Q_{\xi}^{\pi^{\ast}}(h_{<t}, a)$Qπ∗
ξ (h<t, a) represents the optimal action-value function (Q-value) under 
the environment model $\xi$ξ. Speciﬁcally, 
\bALT{} Q_{\xi}^{\pi^{\ast}}(h_{<t}, a_{t}) = \sum_{e_{t} \in \mathcal{E}} \xi(e_{t} | h_{<t}, a_{t}) \left( r_{t} + \gamma V_{\xi}^{\pi^{\ast}}(h_{<t}) \right),\eALT{} Qπ∗
ξ (h<t, at) =

et∈E
ξ(et|h<t, at)

rt + γV π∗
ξ
(h<t)

,
(3) 
with 
\bALT{} V_{\xi}^{\pi^{\ast}}(h_{<t}) = \max_{a} Q_{\xi}^{\pi^{\ast}}(h_{<t}, a).\eALT{} V π∗
ξ
(h<t) = max
a
Qπ∗
ξ (h<t, a).
(4) 
AIXI thus selects the action maximizing this Q-value. 
Softmax Policy Interpretation. Sometimes, we transform the \arg\maxarg max over Q_{\xi}^{\pi^{\ast}}Qπ∗
ξ
into a softmax over actions: 
\bALT{} p^{\ast}\left(a \mid h_{<t}\right)\stackrel{\text {def }}{=}\frac{\exp \left(Q_{\xi}^{\pi^{\ast}}\left(h_{<t}, a\right)\right)}{\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q_{\xi}^{\pi^{\ast}}\left(h_{<t}, a^{\prime}\right)\right)} ,\eALT{} p∗(a | h<t)
def=
exp

Qπ∗
ξ (h<t, a)


a′∈A exp

Qπ∗
ξ (h<t, a′)
,
(5) 
so that the log-likelihood of action aa is: 
\bALT{} \ln p^{\ast}\bigl(a \mid h_{<t}\bigr) \;=\; Q_{\xi}^{\pi^{\ast}}\bigl(h_{<t},a\bigr) \;-\; \ln \sum_{a'} \exp\Bigl(Q_{\xi}^{\pi^{\ast}}\bigl(h_{<t},a'\bigr)\Bigr),\eALT{} ln p∗
a | h<t

= Qπ∗
ξ

h<t, a

−ln

a′
exp

Qπ∗
ξ

h<t, a′
,
(6) 
and 
\bALT{} \label{eq:aixispsdist} \arg\max_{a}\,\ln p^{\ast}(a\mid h_{<t}) \;=\; \arg\max_{a}\,Q_{\xi}^{\pi^{\ast}}\bigl(h_{<t},a\bigr).\eALT{} arg max
a
ln p∗(a | h<t) = arg max
a
Qπ∗
ξ

h<t, a

.
(7) 
Hence maximizing Q_{\xi}^{\pi^{\ast}}Qπ∗
ξ
is equivalent to maximizing log-likelihood of aa. We can  
rewrite an "AIXI objective" as a likelihood: 
\bALT{} \mathcal{L}_{\text{AIXI}} \stackrel{\text {def }}{=} -\mathbb{E}_{a \sim p^{\ast}}\,\left[\ln p^{\ast}\left(a\mid h_{<t}\right)\right].\eALT{} LAIXI
def= −Ea∼p∗[ln p∗(a | h<t)] .
(8) 
Although provably optimal in a Bayesian sense, AIXI is computationally infea-
sible: it sums over inﬁnitely many models and searches over all possible action 
sequences. Nonetheless, the theory behind AIXI is highly inﬂuential: it shows 
that if the true environment \muμ has nonzero prior probability, AIXI eventually 
behaves optimally in \muμ. It also satisﬁes the self-optimizing property in many 
environments and achieves the maximal Legg-Hutter intelligence score [ 13].

Universal AI Maximizes Variational Empowerment
253
Self-AIXI as a Learning-Centric Approximation. Self-AIXI, introduced in [ 1], 
is a learning-based approach to approximate AIXI's policy without exhaustive 
search. Instead of planning over all future action sequences, Self-AIXI predicts its 
own future behavior given the current policy. Concretely, it maintains a Bayesian 
mixture of policies, \zetaζ, with posterior updates based on how accurately each 
candidate policy predicts the agent's actions: 
\bALT{} \zeta(a_{t} \mid h_{<t}) \;=\; \sum_{\pi \in \mathcal{P}} \omega(\pi \mid h_{<t}) \,\pi(a_{t} \mid h_{<t}),\eALT{} ζ(at | h<t) =

π∈P
ω(π | h<t) π(at | h<t),
(9) 
where \omega(\pi \mid h_{<t})ω(π | h<t) is updated via Bayes' rule after each action. The Q-values are 
estimated via experience rather than full expectimax. Formally, one can write a 
self-consistent objective 
\bALT{} \pi_{S}(a_{t} \mid h_{<t}) \stackrel{\text{def}}{=} \arg\max_{a} \left{ Q_{\xi}^{\zeta}(h_{<t}, a) - \lambda \ln{ \frac{\pi^{\ast}(a \mid h_{<t})}{\zeta(a \mid h_{<t})} } \right},\eALT{} πS(at | h<t)
def
= arg max
a

Qζ
ξ(h<t, a) −λ ln π∗(a | h<t)
ζ(a | h<t)
	
,
(10) 
where 
\bALT{} Q_{\xi}^{\zeta}(h_{<t}, a_{t}) \stackrel{\text{def}}{=} \sum_{\pi \in \mathcal{P}} \omega(\pi | h_{<t}) \sum_{\nu \in \mathcal{M}} w(\nu | h_{<t}) Q_{\nu}^{\pi}(h_{<t}, a_{t}),\eALT{} Qζ
ξ(h<t, at)
def
=

π∈P
ω(π|h<t)

ν∈M
w(ν|h<t)Qπ
ν(h<t, at),
(11) 
where Q_{\xi}^{\zeta}Qζ
ξ combines environment predictions \xiξ with the mixture policy \zetaζ, and  
\ln{ \frac{\pi^{\ast}(a \mid h_{<t})}{\zeta(a \mid h_{<t})} }ln π∗(a|h<t)
ζ(a|h<t) is a regularization measure encouraging \zetaζ to approach the optimal 
policy \pi^{\ast}π∗. Note that the KL term serves as a regularization that nudges \zetaζ toward 
\pi^{\ast}π∗. This formulation generalizes the simpler case in [ 1] by allowing \lambda > 0λ > 0 (See 
Appendix A). If \lambda=0λ = 0, we recover the original Self-AIXI objective without 
explicit KL regularization. 
Softmax Policy Interpretation. We transform the \arg\maxarg max over Q_{\xi}^{\zeta}Qζ
ξ into a softmax 
over actions: 
\bALT{} q^{\zeta}\left(a \mid h_{<t}\right)\stackrel{\text {def }}{=}\frac{\exp \left(Q_{\xi}^{\zeta}\left(h_{<t}, a\right)\right)}{\sum_{a^{\prime} \in \mathcal{A}} \exp \left(Q_{\xi}^{\zeta}\left(h_{<t}, a^{\prime}\right)\right)} ,\eALT{} qζ (a | h<t)
def=
exp

Qζ
ξ (h<t, a)


a′∈A exp

Qζ
ξ (h<t, a′)
,
(12) 
so that the log-likelihood of action aa is: 
\bALT{} \ln q^{\zeta}\left(a \mid h_{<t}\right) \;=\; Q_{\xi}^{\zeta}\bigl(h_{<t},a\bigr) \;-\; \ln \sum_{a'} \exp^{\ast}\left(Q_{\xi}^{\zeta}\left(h_{<t},a'\right)\right),\eALT{} ln qζ (a | h<t) = Qζ
ξ

h<t, a

−ln

a′
exp∗
Qζ
ξ (h<t, a′)

,
(13) 
and 
\bALT{} \label{eq:selfaixispsdist} \arg\max_{a}\,\ln q^{\zeta}(a\mid h_{<t}) = \arg\max_{a}\,Q_{\xi}^{\zeta}\bigl(h_{<t},a\bigr).\eALT{} arg max
a
ln qζ(a | h<t) = arg max
a
Qζ
ξ

h<t, a

.
(14) 
Finally, we can rewrite an "Self-AIXI objective" as a likelihood: 
\bALT{} \mathcal{L}_{\text{Self-AIXI}} \stackrel{\text {def }}{=} -\mathbb{E}_{a \sim q^{\zeta}}\,\bigl[\ln q^{\zeta}(a\mid h_{<t})\bigr] \;+\; \lambda D_{\text{KL}}\left(\pi^{\ast} \,\|\, \zeta\right).\eALT{} LSelf-AIXI
def= −Ea∼qζ

ln qζ(a | h<t)

+ λDKL (π∗∥ζ) .
(15)

254
Y. Hayashi and K. Takahashi
These unify the planning perspective (max Q_{\xi}^{\zeta}Qζ
ξ) and a probabilistic policy per-
spective. By self-predicting its action distributions, Self-AIXI can incrementally 
reﬁne Q-value estimates, akin to TD-learning, while still retaining a universal 
Bayesian foundation (assuming the environment is in the model class). Prior 
work [ 1] shows that, under suitable assumptions, Self-AIXI converges to the 
same optimal value as AIXI, but it must still ensure adequate exploration to 
gather correct world-model data. 
1.2
Variational Empowerment for Intrinsic Exploration 
While AIXI implicitly explores via its unbounded search over hypotheses, any 
tractable approximation (like Self-AIXI) requires an explicit exploration mecha-
nism. We adopt Variational Empowerment [ 3- 6] as an  intrinsic reward to drive 
the agent toward high-control states. This perspective aligns with recent work 
[ 14] in which empowerment is used not only for exploration but also as a mech-
anism for discovering useful latent representations or skills, potentially comple-
menting goal-based RL approaches. 
Formal Deﬁnition of Empowerment. Empowerment is often deﬁned as the 
maximal mutual information between an agent's actions and future states. For 
a horizon kk, let  z_{k} \stackrel{\text{def}}{=} a_{t:t+k-1}zk
def
= at:t+k−1 be a sequence of actions and h_{t<t+k}ht<t+k the resulting 
state; then 
\bALT{} \mathcal{I}(z_{k};h_{<t+k}) & \stackrel{\text{def}}{=} \max_{p} \; I\left(z_{k};\,h_{t<t+k}\,\mid\;h_{<t}\right), \multieq &= \max_{p} \; \mathbb{E}_{z, h \sim p^{\ast}}\left[\ln \frac{ p^{\ast}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right]. \label{eq:empowermentspsdef}\eALT{} I(zk; h<t+k)
def
= max
p
I (zk; ht<t+k | h<t) ,
(16) 
= max
p 
Ez,h∼p∗

ln p∗ (zk | h<t+k) 
p∗ (zk | h<t)

.
(17) 
The agent is empowered in states h_tht where it can produce a wide variety of 
distinguishable future outcomes through its choice of action-sequence. Exact 
computation is generally intractable in large state spaces, so one uses a varia-
tional approximation. For instance, we introduce a parameterized distribution 
q^{\zeta}qζ that approximates the posterior q^{\zeta}(z_{k} \mid h_{<t+k})qζ(zk | h<t+k), and then maximize [ 6]: 
\bALT{} \mathcal{E}_{\zeta}(z_{k};h_{<t+k}) & \stackrel{\text{def}}{=} \max_{q^{\zeta}} \; E_{\zeta}\left(z_{k};\,h_{t<t+k}\,\mid\;h_{<t}\right), \multieq &= \max_{q^{\zeta}} \; \mathbb{E}_{z, h\sim p^{\ast}}\left[\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right]. \label{eq:variationalspsempowermentspsdef}\eALT{} Eζ(zk; h<t+k)
def
= max
qζ
Eζ (zk; ht<t+k | h<t) ,
(18) 
= max 
qζ 
Ez,h∼p∗

ln qζ (zk | h<t+k) 
p∗ (zk | h<t)

.
(19)

Universal AI Maximizes Variational Empowerment
255
Using Eqs. (7) and (14), we have: 
\bALT{} p^{\ast}\left(z_k \mid h_{<t+k}\right) &= \; \prod_{i=0}^{k-1} \pi^{\ast}\left(a_{t+i} \mid h_{<t+i}\right), \multieq q^{\zeta}\left(z_k \mid h_{<t+k}\right) &= \; \prod_{i=0}^{k-1} \zeta\left(a_{t+i} \mid h_{<t+i}\right), \multieq \mathbb{E}_{z, h\sim p^{\ast}}\left[\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t+k}\right) }\right] &= \; \mathbb{E}_{h \sim p^{\ast}}\left[\sum_{i=0}^{k-1} -D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right) \right]\label{eq:selfaixisspsregularization}.\eALT{} p∗(zk | h<t+k) =
k−1

i=0
π∗(at+i | h<t+i) ,
(20) 
qζ (zk | h<t+k) =  
k−1

i=0 
ζ (at+i | h<t+i) ,
(21) 
Ez,h∼p∗

ln qζ (zk | h<t+k) 
p∗ (zk | h<t+k)

= Eh∼p∗
k−1

i=0 
−DKL (π∗ 
i ∥ζi)

.
(22) 
The right-hand side of Eq. (22), D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right)DKL (π∗
i ∥ζi), is a regularization term in the Self-
AIXI framework that pushes the agent's mixture policy \zeta_{i} \stackrel{\text{def}}{=} q^{\zeta}(a_{t+i} \mid h_{<t+i})ζi
def
= qζ(at+i | h<t+i)
to imitate or approach the optimal policy \pi^{\ast}_{i} \stackrel{\text{def}}{=} p^{\ast}(a_{t+i} \mid h_{<t+i})π∗
i
def
= p∗(at+i | h<t+i). As the agent 
learns from experience, it reduces this divergence, eﬀectively self-optimizing its 
policy. 
Hence, Eqs. (17) and (22) allow us to rewrite the Variational Empowerment 
as: 
\bALT{} \mathcal{E}_{\zeta}(z_{k};h_{<t+k}) &= \max_{q^{\zeta}} \; \mathbb{E}_{z, h\sim p^{\ast}}\left[\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t+k}\right) } \; + \; \ln \frac{ p^{\ast}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right], \multieq &= \max_{q^{\zeta}} \; \mathbb{E}_{h \sim p^{\ast}}\left[\sum_{i=0}^{k-1} -D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right) \right] \; + \; \mathcal{I}\left(z_k ; h_{<t+k}\right). \label{eq:variationalspsempowermentspstransform}\eALT{} Eζ(zk; h<t+k) = max
qζ
Ez,h∼p∗

ln qζ (zk | h<t+k)
p∗(zk | h<t+k) + ln p∗(zk | h<t+k)
p∗(zk | h<t)

,
(23) 
= max 
qζ 
Eh∼p∗
k−1

i=0 
−DKL (π∗ 
i ∥ζi)

+ I (zk; h<t+k) .
(24) 
1.3
Connecting to Free-Energy Minimization and Active Inference 
Bayesian RL as Active Inference. Bayesian RL connects closely to Active 
Inference [ 10- 12], where an agent maintains a prior over latent variables and 
updates its posterior after observing rewards or other feedback. Under a Free 
Energy Principle (FEP), one often writes a free-energy functional: 
\bALT{} \label{eq:freespsenergy} \mathcal{F}_{\zeta}(z_{k};h_{<t+k}) &~\stackrel{\text{def}}{=}~ D_{\mathrm{KL}}\!\left(p^{\ast}(z_{k},h_{t<t+k} \mid h_{<t}) \,\|\, q^{\zeta}(z_{k},h_{t<t+k} \mid h_{<t})\right), \multieq &\;\approx\; \underbrace{-\,\mathbb{E}_{h \sim p^{\ast}}\left[\ln q^{\zeta}(h_{t<t+k} \mid z_{k}, h_{<t})\right]}_{\text{Predictive Error (Surprise)}} \;+\; \underbrace{\mathbb{E}_{z, h\sim p^{\ast}}\left[-\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right]}_{\text{FEP's Regularization}}.\eALT{} Fζ(zk; h<t+k)
def
= DKL

p∗(zk, ht<t+k | h<t) ∥qζ(zk, ht<t+k | h<t)

,
(25) 
≈− Eh∼p∗

ln qζ (ht<t+k | zk, h<t)



	
Predictive Error (Surprise) 
+ Ez,h∼p∗

− ln qζ (zk | h<t+k) 
p∗ (zk | h<t)



	
FEP's Regularization 
. 
(26) 
Here, -\mathbb{E}_{h \sim p^{\ast}}[\ln q^{\zeta}(h_{t<t+k} \mid z_{k}, h_{<t})]−Eh∼p∗[ln qζ(ht<t+k | zk, h<t)] is the predictive error (surprise), and the 
remaining term measures how far q^{\zeta}(z_{k} \mid h_{<t+k})qζ(zk | h<t+k) diverges from p^{\ast}(z_{k} \mid h_{<t})p∗(zk | h<t). 
Decomposition of Regularization Term. Under suitable rearrangements or sign 
conventions, we can identify a Regularization part that can be maximized rather

256
Y. Hayashi and K. Takahashi
than minimized, yielding empowerment: 
\bALT{} \label{eq:regularizationspsdecomp} \underbrace{\mathbb{E}_{z, h\sim p^{\ast}}\left[-\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right]}_{\text{FEP's Regularization}} \;&=\; \mathbb{E}_{z, h \sim p^{\ast}}\left[-\;\ln \frac{q^{\zeta}\left(z_k \mid h_{<t+k}\right)}{p^{\ast}\left(z_k \mid h_{<t+k}\right)} \;-\; \ln \frac{p^{\ast}\left(z_k \mid h_{<t+k}\right)}{p^{\ast}\left(z_k \mid h_{<t}\right)}\right], \multieq \;&=\; \underbrace{\mathbb{E}_{h \sim p^{\ast}}\left[\sum_{i=0}^{k-1} D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right) \right]}_{\text{Self-AIXI's Policy Regularization}} \; - \; \underbrace{I\left(z_{k};\,h_{t<t+k}\,\mid\;h_{<t}\right)}_{\text{Mutual Information}}.\eALT{} Ez,h∼p∗

−ln qζ (zk | h<t+k)
p∗(zk | h<t)




FEP's Regularization
= Ez,h∼p∗

−ln qζ (zk | h<t+k)
p∗(zk | h<t+k) −ln p∗(zk | h<t+k)
p∗(zk | h<t)

,
(27) 
=
Eh∼p∗
k−1
	
i=0 
DKL (π∗ 
i ∥ζi)




Self-AIXI's Policy Regularization 
− I (zk; ht<t+k | h<t)



Mutual Information 
. 
(28) 
Hence, turning the regularization term "upside down" (from negative to posi-
tive) motivates Variational Empowerment: 
\bALT{} \underbrace{\mathcal{E}_{\zeta}(z_{k};h_{<t+k})}_{\text{Variational Empowerment}} \;&=\; - \min_{q^{\zeta}} \; \underbrace{\mathbb{E}_{z, h\sim p^{\ast}}\left[-\ln \frac{ q^{\zeta}\left(z_{k} \mid h_{<t+k}\right) }{ p^{\ast}\left(z_{k} \mid h_{<t}\right) }\right]}_{\text{FEP's Regularization}}, \multieq \;&=\; \max_{q^{\zeta}} \underbrace{\mathbb{E}_{h \sim p^{\ast}}\left[\sum_{i=0}^{k-1} -D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right) \right]}_{\text{(Negative) Self-AIXI's Policy Regularization}} +\; \underbrace{\mathcal{I}\left(z_k ; h_{<t+k}\right)}_{\text{Empowerment}}.\eALT{} 
Eζ(zk; h<t+k)


	
Variational Empowerment
= −min
qζ
Ez,h∼p∗

−ln qζ (zk | h<t+k)
p∗(zk | h<t)



	
FEP's Regularization
,
(29) 
= max  
qζ 
Eh∼p∗
k−1

i=0 
−DKL (π∗ 
i ∥ζi)



	
(Negative) Self-AIXI's Policy Regularization 
+ I (zk; h<t+k)


	
Empowerment 
. 
(30) 
mirroring the deﬁnitions in Eq. (24) above.  
2
Universal AI Maximizes Variational Empowerment 
In the Universal Artiﬁcial Intelligence (UAI) framework [ 2], an agent is consid-
ered universal if it can, given suﬃcient time, match or surpass any other policy's 
performance in all computable environments (with nonzero prior). AIXI achieves 
this in theory. Self-AIXI aims to achieve it in practice, provided it can explore 
eﬀectively. Below, we summarize how our empowered Self-AIXI ﬁts these formal 
criteria. 
2.1
Asymptotic Equivalence, Legg-Hutter Intelligence, 
and Self-optimizing Property 
Prior work [ 1] proves that if the Self-AIXI agent's policy class and environment 
prior are suﬃciently expressive (i.e. the true environment is in the hypothesis 
class with nonzero probability), then the agent's behavior converges to that 
of AIXI's optimal policy in the limit of inﬁnite interaction. Formally, for any 
environment \muμ in the model class, 
\bALT{} \label{eq:selfaixispsconvergence} \lim_{t\to\infty} \mathbb{E}_{\mu}^{\pi_s}\Bigl[V_{\xi}^{\ast}\bigl(h_{<t}\bigr)-V_{\xi}^{\pi_s}\bigl(h_{<t}\bigr)\Bigr] \;=\;0.\eALT{} lim
t→∞Eπs
μ

V ∗
ξ

h<t

−V πs
ξ

h<t

= 0.
(31)

Universal AI Maximizes Variational Empowerment
257
which implies that, asymptotically, the agent's expected return under\pi_sπs matches 
that of the optimal policy V_{\xi}^{\ast}V ∗
ξ . Intuitively, as the agent's world-model becomes 
more accurate, it exploits the optimal policy; hyperparameters (such as \lambdaλ in an 
empowerment term) can be tuned or annealed so that extrinsic reward eventually 
dominates. 
From the perspective of Legg-Hutter intelligence [ 13], which associates an 
agent's "intelligence" with its expected performance across a suite of weighted 
environments, this result is especially signiﬁcant. Because Self-AIXI asymptot-
ically reproduces AIXI's policy, it inherits maximal Legg-Hutter intelligence 
within that class of environments. Moreover, in a wide class of self-optimizing 
environments, the agent will ultimately achieve the same returns as an opti-
mal agent with perfect knowledge would achieve, under the same conditions in 
Eq. (31). These guarantees illustrate that the enhanced exploration mechanisms- 
such as empowerment-driven strategies-do not compromise eventual perfor-
mance. Instead, they help ensure the agent uncovers the environment's true 
optimal actions without becoming trapped in suboptimal behaviors due to insuf-
ﬁcient data. Consequently, the agent retains AIXI's universal optimality in the 
limit while mitigating early exploration challenges. 
2.2
Self-optimization Leads Empowerment Maximization 
The agent's process of improving its policy (often referred to as self-optimization) 
naturally leads to an increase in Variational Empowerment. In fact, as reinforce-
ment learning progresses, both AIXI's objective function \mathcal{L}_{\mathrm{AIXI}}LAIXI and Self-AIXI's 
objective function \mathcal{L}_{\text{Self-AIXI}}LSelf-AIXI gradually converge, and they coincide in the limit 
t \to \inftyt →∞. 
The diﬀerence between these two objectives can be expressed through the 
policy regularization term D_{\mathrm{KL}}(\pi^{\ast} \,\|\, \zeta)DKL(π∗∥ζ). Formally, we have: 
\bALT{} \lim_{t \to \infty} \left|\mathcal{L}_{\mathrm{AIXI}} \; - \; \mathcal{L}_{\text{Self-AIXI}} \right| = \lim_{t \to \infty} \lambda \, D_{\mathrm{KL}}(\pi^{\ast} \,\|\, \zeta) = 0.\eALT{} lim
t→∞|LAIXI −LSelf-AIXI| = lim
t→∞λ DKL(π∗∥ζ) = 0.
(32) 
This result implies that D_{\mathrm{KL}}(\pi^{\ast} \,\|\, \zeta)DKL(π∗∥ζ) goes to 00 as t \to \inftyt →∞, which is equivalent 
to the Self-AIXI's variational empowerment \mathcal{E}_{\zeta}(z_{k};h_{<t+k})Eζ(zk; h<t+k) being maximized. In 
other words, the universal AI agent AIXI, which behaves in a Bayes-optimal 
way with respect to the environment, also emerges as an agent that maximizes 
empowerment. 
Concretely, the relationship between the empowerment objective and the 
policy regularization is succinctly captured by the following equality: 
\bALT{} \underbrace{\mathcal{E}_{\zeta}(z_{k};h_{<t+k})}_{\text{Variational Empowerment}} =\; \max_{q^{\zeta}} \underbrace{\mathbb{E}_{h \sim p^{\ast}}\left[\sum_{i=0}^{k-1} -D_{\mathrm{KL}}\left(\pi^{\ast}_{i} \,\|\, \zeta_{i}\right) \right]}_{\text{(Negative) Self-AIXI's Policy Regularization}} +\; \underbrace{\mathcal{I}\left(z_k ; h_{<t+k}\right)}_{\text{Empowerment}}.\eALT{} 
Eζ(zk; h<t+k)



Variational Empowerment
= max
qζ
Eh∼p∗
k−1

i=0
−DKL (π∗
i ∥ζi)




(Negative) Self-AIXI's Policy Regularization
+ I (zk; h<t+k)



Empowerment
.
(33) 
Self-optimization refers to the iterative improvement of the agent's policy 
based on observed rewards and outcomes. In Self-AIXI, reducing the policy reg-
ularization term D_{\mathrm{KL}}\left(\pi^{\ast} \,\|\, \zeta\right)DKL (π∗∥ζ) directs \zetaζ closer to \pi^{\ast}π∗. Because the left-hand side

258
Y. Hayashi and K. Takahashi
of the second formula above equals the variational empowerment \mathcal{E}_{\zeta}(z_{k};h_{<t+k})Eζ(zk; h<t+k), 
each step that lowers D_{\mathrm{KL}}\left(\pi^{\ast} \,\|\, \zeta\right)DKL (π∗∥ζ) raises \mathcal{E}_{\zeta}(z_{k};h_{<t+k})Eζ(zk; h<t+k). Empowerment here sig-
niﬁes how many high-control or high-optionality states are accessible to the 
agent. Consequently, maximizing reward often requires seeking out exactly those 
states in which the agent can maintain or expand control-thus also maximizing 
\mathcal{E}_{\zeta}(z_{k};h_{<t+k})Eζ(zk; h<t+k). As  \zetaζ becomes more similar to \pi^{\ast}π∗, the agent naturally discovers 
strategies that grant more control and ﬂexibility. Therefore, under Self-AIXI, 
improving the policy toward optimal behavior simultaneously yields higher exter-
nal rewards and ampliﬁes the agent's own empowerment. 
3
Conclusions 
In this work, we reinterpreted a term in Self-AIXI as variational empowerment— 
intrinsic exploration bonus. We have argued that: 
- Empowerment naturally complements Bayesian RL in universal settings, pro-
viding a structured incentive to discover controllable states and gather broad 
experience. 
- Even with an empowerment bonus, the agent asymptotically recovers AIXI's 
Bayes-optimal policy, inheriting the same universal intelligence and self-
optimizing properties in the limit. 
One of our main observations is that the agent's pursuit of high-empowerment 
states often manifests as a power-seeking tendency. Traditionally, many authors 
(e.g., Turner et al. [ 8]) interpret power-seeking as purely instrumental: an agent 
acquires resources, avoids shutdown, or manipulates the reward channel to bet-
ter guarantee high external returns. However, we show that power-seeking can 
also arise intrinsically from a drive to reduce uncertainty and maintain a wide 
range of feasible actions (i.e., "keeping options open"). Imagine an agent choosing 
between a high-control region (with many possible actions and partial knowl-
edge) and a low-control region (with fewer actions and less information). If both 
yield the same short-term reward, a purely extrinsic approach might be indiﬀer-
ent. By contrast, an empowerment-seeking agent prefers the high-control region, 
as it oﬀers greater potential for discovering valuable future strategies. Over time, 
as the agent learns more about its environment, these beneﬁts accumulate. 
When not moderated, power-seeking behaviors may conﬂict with human 
interests. For instance, maximizing control can lead to manipulative or exploita-
tive outcomes if the agent's intrinsic or extrinsic goals are misaligned with social 
values. From a Universal AI standpoint, understanding that power-seeking can 
stem from both instrumental and intrinsic (empowerment-based) motives is 
crucial to designing mechanisms-e.g., safe exploration techniques or alignment 
constraints-to ensure that an agent's inﬂuence remains beneﬁcial. It is impor-
tant to note that these concerns apply even to AI agents with apparently benign 
objectives, such as an AI scientist pursuing scientiﬁc truth purely out of intel-
lectual curiosity.

Universal AI Maximizes Variational Empowerment
259
Our results are primarily conceptual and rest on idealized assumptions: 
(1) the environment is in the agent's hypothesis class with nonzero prior; (2) we 
assume unbounded computational resources and memory; (3) the agent can 
tractably approximate empowerment. In reality, computing exact empowerment 
or using universal priors is challenging. Empirical methods to approximate these 
ideas (e.g., neural networks [ 4]) remain an active area of research. 
Acknowledgments. The architects of Self-AIXI [ 1] provided groundwork that 
emboldened our own inquiry. We thank the AI safety community, particularly those 
who involved in our AI Alignment Network. Special gratitude goes to davidad and 
Hiroshi Yamakawa, whose incisive feedback steered us toward a more lucid exposition. 
Part of this work was supported by RIKEN TRIP initiative (AGIS). 
A
Appendix 
A.1
Notation and Further Details 
We summarize the main notation in Table 1 for reference. 
Table 1. Key notation used in the main text. 
Symbol
Meaning 
h_{<t}h<t
History (observations, actions, rewards) up to time tt
\xi, \nu, \muξ, ν, μ
Environment hypotheses (in a computable class \mathcal{M}M) 
\pi, \zetaπ, ζ
Policies (e.g. mixture policy \zetaζ) 
w(\nu \mid h_{<t})w(ν | h<t)
Posterior weight of environment \nuν given history h_{<t}h<t
Q_{\nu}^*,Q_{\xi}^\zetaQ∗
ν, Qζ
ξ
(Optimal) Q-values under environment \nuν or mixture/policy \zetaζ
\mathcal{I}\left(z_{k};h_{<t+k}\right)I (zk; h<t+k) Empowerment in state h_{<t+k}h<t+k
\mathcal{E}_{\phi}\left(z_{k};h_{<t+k}\right)Eφ (zk; h<t+k)Variational empowerment in state h_{<t+k}h<t+k, approximated by parameter \phiφ
\lambdaλ
Hyperparameters weighting empowerment or KL regularization 
A.2
Self-AIXI's self-consistent objective 
In this subsection, we investigate how introducing a Kullback-Leibler (KL) 
divergence-based regularization term into Self-AIXI aﬀects its convergence prop-
erties and whether it preserves the agent's ability to reach the optimal policy \pi^{\ast}π∗
asymptotically. Speciﬁcally, we consider the eﬀect of adding a penalty that mea-
sures how far the current mixture policy \zetaζ deviates from \pi^{\ast}π∗. 
Recall that for a given history h_{<t}h<t, the KL divergence between the optimal 
policy \pi^{\ast}π∗and the current mixture policy \zetaζ is deﬁned as: 
\bALT{} D_{\mathrm{KL}}\left(\pi^{\ast} \| \zeta\right) = \sum_{a' \in \mathcal{A}} \pi^{\ast}\left(a' \mid h_{<t}\right) \ln \frac{\pi^{\ast}\left(a' \mid h_{<t}\right)}{\zeta\left(a' \mid h_{<t}\right)}.\eALT{} DKL (π∗∥ζ) =

a′∈A
π∗(a′ | h<t) ln π∗(a′ | h<t)
ζ (a′ | h<t) .
(34)

260
Y. Hayashi and K. Takahashi
We then propose a policy update rule that augments the standard Self-AIXI 
greedy step with a log-likelihood ratio term: 
\bALT{} \pi_{S}(a_{t} \mid h_{<t}) \stackrel{\text{def}}{=} \arg\max_{a} \left{ Q_{\xi}^{\zeta}(h_{<t}, a) - \lambda \ln \frac{\pi^{\ast}\left(a \mid h_{<t}\right)}{\zeta\left(a \mid h_{<t}\right) } \right},\eALT{} πS(at | h<t)
def
= arg max
a

Qζ
ξ(h<t, a) −λ ln π∗(a | h<t)
ζ (a | h<t)
	
,
(35) 
where Q_{\xi}^{\zeta}Qζ
ξ denotes the estimated value of taking action aa in history h_{<t}h<t under the 
Bayesian mixture environment \xiξ and current policy \zetaζ. Here, \lambda > 0λ > 0 is included to 
penalize large deviations from \pi^{\ast}π∗whenever the agent's mixture policy \zetaζ diﬀers 
substantially from the (unknown) optimal policy \pi^{\ast}π∗. 
KL regularization and recovery of the standard update. In many practical sce-
narios, such as when \pi^*π∗is deterministic or assigns a high probability to a single 
action, part of the KL term can be constant with respect to aa. Under those 
conditions, the penalty term 
\bALT{} \lambda \ln \frac{\pi^{\ast}\left(a' \mid h_{<t}\right)}{\zeta\left(a' \mid h_{<t}\right)}\eALT{} λ ln π∗(a′ | h<t)
ζ (a′ | h<t)
(36) 
does not vary across actions, and the update rule simpliﬁes to 
\bALT{} \pi_S\bigl(a_{t} \mid h_{<t}\bigr) = \arg \max_a\left{Q_{\xi}^\zeta\bigl(h_{<t}, a\bigr)\right},\eALT{} πS

at | h<t

= arg max
a

Qζ
ξ

h<t, a

,
(37) 
which recovers the conventional (un-regularized) Self-AIXI greedy update. 
Impact on learning dynamics and convergence. Although the added term changes 
the action selection criterion, it does not alter the identity of the optimal pol-
icy in the underlying environment. Intuitively, the new rule can be viewed as 
performing a more conservative or "trust-region"-like update, since actions that 
the agent's current policy \zetaζ overestimates relative to \pi^{\ast}π∗will be penalized more 
strongly. Conversely, if \zetaζ assigns too little probability to actions that \pi^{\ast}π∗actu-
ally favors, the negative logarithm of their ratio produces a smaller (or positive) 
correction. Hence, the agent is nudged toward \pi^*π∗. 
Crucially, the regularization term does not disrupt Self-AIXI's standard con-
vergence guarantees, assuming the original conditions hold (e.g., that the true 
environment is in the Bayesian mixture class \xiξ and \pi^{\ast}π∗is in the agent's policy 
class). From a theoretical perspective, \pi^{\ast}π∗remains a stable ﬁxed point under this 
augmented objective. Once \zetaζ converges to \pi^{\ast}π∗, the log ratio 
\bALT{} \ln \frac{\pi^{\ast}\left(a \mid h_{<t}\right)}{\zeta\left(a \mid h_{<t}\right)}\eALT{} ln π∗(a | h<t)
ζ (a | h<t)
(38) 
vanishes for actions with nonzero probability under \pi^*π∗, so no extra penalty is 
incurred, and the update aligns with the optimal policy's greedy choice.

Universal AI Maximizes Variational Empowerment
261
Regularization coeﬃcient and transient behavior. The coeﬃcient \lambda < 0λ < 0 deter-
mines the strength of the penalty term: 
• Small, moderate penalty (|\lambda|_{\mathrm{small}}|λ|small ). A suitably chosen, relatively small mag-
nitude for |\lambda||λ| works as a gentle regularizer, smoothing the agent's updates by 
discouraging drastic shifts in policy. This can stabilize learning and reduce 
oscillations without harming ﬁnal convergence. Indeed, theoretical analyses 
of KL-based regularization in reinforcement learning show that while such 
shaping modiﬁes the transient policy updates, the optimal policy remains the 
same in the limit. 
• Overly large penalty (|\lambda|_{\mathrm{large}}|λ|large). If the KL term is emphasized too strongly, 
the agent may stick too closely to its current guess of \pi^*π∗and under-explore 
other actions. Early in learning—when \zetaζ is still inaccurate—this could delay 
or even misdirect policy improvement. However, as Self-AIXI continuously 
updates its environment belief (via \xiξ) and revises \zetaζ, the agent still accumu-
lates evidence about which actions are actually optimal, making it diﬃcult to 
remain indeﬁnitely biased toward a suboptimal policy. Practical implementa-
tions often tune \lambdaλ to ensure that exploration is maintained. 
References 
1. Catt, E., et al.: Self-predictive universal AI. In: 37th Conference on Neural Infor-
mation Processing Systems (NeurIPS 2023), New Orleans, USA, pp. 1-18 (2023) 
2. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability. EATCS Series. Springer, Berlin (2005). 3-540-22139-5; ISBN-
online: 978-3-540-26877-2 
3. Klyubin, A.S., Polani, D., Nehaniv, C.L.: Empowerment: a universal agent-centric 
measure of control. In: 2005 IEEE Congress on Evolutionary Computation, pp. 
128-135. IEEE (2005) 
4. Mohamed, S., Rezende, D.J.: Variational information maximisation for intrinsically 
motivated reinforcement learning. In: Advances in Neural Information Processing 
Systems (NeurIPS) (2015) 
5. Gregor, K., Rezende, D.J., Wierstra, D.: Variational intrinsic control. In: Interna-
tional Conference on Learning Representations (ICLR) (2017) 
6. Choi, J., Sharma, A., Lee, H., Levine, S., Gu, S.: Variational empowerment as 
representation learning for goal-based reinforcement learning. In: Proceedings of 
the 38th International Conference on Machine Learning (ICML) (2021) 
7. Bostrom, N.: Superintelligence: Paths, Dangers, Strategies. Oxford University 
Press (2014) 
8. Turner, A.M., Smith, L., Shah, R., Critch, A., Tadepalli, P.: Optimal policies tend 
to seek power. In: Advances in Neural Information Processing Systems (NeurIPS) 
(2021). https://arxiv.org/abs/1912.01683 
9. Cohen, M., Vellambi, B., Hutter, M.: Asymptotically unambitious artiﬁcial general 
intelligence. In: Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and 
Society (AIES) (2020). https://arxiv.org/abs/1905.12186 
10. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nat. Rev. Neurosci. 
11(2), 127-138 (2010)

262
Y. Hayashi and K. Takahashi
11. Friston, K., Sengupta, B., Auletta, G.: Cognitive dynamics: from attractors to 
active inference. Proc. IEEE 102(4), 427-445 (2014) 
12. Friston, K.: A free energy principle for a particular physics. Neural Comput. 29(6), 
129-155 (2019) 
13. Legg, S., Hutter, M.: Universal intelligence: a deﬁnition of machine intelligence. 
Mind. Mach. 17(4), 391-444 (2007) 
14. Eysenbach, B., Gupta, A., Ibarz, J., Levine, S.: Diversity is all you need: learning 
skills without a reward function. In: International Conference on Learning Repre-
sentations (ICLR) (2019)

A Constructive Developmental Evaluation 
of AGI 
Can AI's Simulations Match Human Meaning-Making and Their 
Orders of Consciousness? 
Martin Hilbertenvelope symbol
University of California, Davis, USA 
hilbert@UCDavis.edu 
Abstract. This paper critically evaluates AGI's premise of equivalence with 
human mental abilities through the lens of constructive developmental psychology. 
It emphasizes the evolving nature of human consciousness and meaning-making 
across successive stages of psychological growth that can be explained by draw-
ing analogies to Gödel's incompleteness theorems. Current AI architectures can 
effectively simulate earlier individual-focused perspectives but appear to struggle 
when simulating socially embedded and meta-aware human perspectives. Recent 
case studies illustrate risks associated with AI's inability to genuinely embody 
collectively compassionate, self-aware, and nondual perspectives. Since this the-
oretical approach equates the activity of human meaning-making with the activity 
of being a personiﬁed self, which may remain beyond AGI, the conclusion sug-
gests shifting from AGI's ambition of perfect equivalence with human mental 
perspectives to complementary augmentation. 
Keywords: Orders of Consciousness cdot Stages of Meaning-Making cdot
Constructive Human Development cdot Theories of Mind cdot Digital Immunity 
1 
Introduction 
Artiﬁcial General Intelligence (AGI) is usually deﬁned by matching all manifestations 
of human cognitive and intellectual abilities [1, 2]. Developmental psychology -in the 
tradition of Maslow's hierarchy [3] and Piaget's cognitive development [4]- show that 
such abilities are not static but develop constructively over a human's lifetime. Reviewing 
this literature raises doubts about whether artiﬁcial systems can ever achieve certain 
stages. This paper provides a critical examination of arguments on both sides. 
2 
Two Approaches to Increasing Meta-perspectives 
The development of mental faculties over a human's lifetime follows ever more 
embracive frames of nested representations and meta-abstractions. The recursive logic 
that requires the creation of meta-perspectives is analogous to Tarski's solution to Gödel's
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 263-277, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_24 

264
M. Hilbert
incompleteness theorems, following a nested recursion of layered embeddings of under-
standing, whereas the more inclusive (higher) perspective addresses the incompleteness 
that arises at more restricted (lower) perspectives of meaning-making. 
2.1 
Psychological Approach: Orders of Consciousness 
The attainment of "more complex order of consciousness" [5] consists of a sequence of 
transitions where the subject of understanding on a lower level ("I am") becomes the 
object of understanding on a higher level ("I have") [5, 6]. The higher level is a meta-
perspective: "Perspective taking is conceptualized as the relation between a subject and 
an object that speciﬁes what one can experience from a certain perspective (the object) 
along with the way one experiences the world (the subject)." [7] In each transition, 
humans are "transforming our epistemologies, liberating ourselves from that in which 
we were embedded, making what was subject into object so that we can 'have it' rather 
than 'be had' by it... The experiencing that our subject-object principle enables is very 
close to what both East and West mean by 'consciousness'" [5]. 
Hundreds of Models. Going back 18th century efforts [8] "in Darwinizing psycholo-
gy" during child development [9], and getting inspiration from ancient enlightenment 
traditions [10, 11] and philosophy [12, 13], more than one hundred complementary mod-
els have been elaborated to describe constructive-developmental sequences of how the 
abilities of the human mind develop over a lifetime [14, 15]. One line of research at the 
Harvard Graduate School of Education refers to these constructs as "those structures 
in which they [people] construe the nature and origins of knowledge, of value, and of 
responsibility... to make meaning" [16]. "Experience is not what happens to a man; it is 
what a man does with what happens to him".[17] It is a "master trait" [18] that refers to 
the entirety of how minds make meaning of their surrounding reality. The very "activity 
of being a person is the activity of meaning-making" [6]. The scale and scope of the 
way humans make sense provide them with their deﬁnition of 'self,' which is why an 
important research line refers to it as "ego-development" [18-20]. 
The process is so obvious during early childhood development, researched in the 
tradition of Baldwin and Piaget [4, 8], that it provides the theoretical justiﬁcation for 
the global multi-billion-dollar age-stratiﬁed school system from pre- to high-school [21, 
22]. Initially being its reﬂexes, humans "become an object of attention, the 'content' of 
a newly evolved structure. Rather than being my reﬂexes, I now have them, and 'I' am 
something other. 'I' am that which coordinates or mediates the reﬂexes, what we mean 
by 'impulses'... This is the new subjectivity... of having it to relate to, rather than to be 
embedded in." [6] Recognized psychologists like Maslow [3, 23-25] and Erikson [26, 
27] provided solid expansions of early childhood development, extending it to reach 
from adults into self-transcendence and wisdom stages. Next, reﬂexes are replaced by 
thinking, and then, as Sartre famously concluded: "The consciousness that says 'I think' 
is precisely not the consciousness that thinks." [12]. 
We will mainly follow the ego development literature [11, 18, 19, 28-30], but there 
are complementary models, such as Kohlberg's moral development [31], Dawson's skill-
levels [32]; Torbert's leadership qualities [33]; Commons' hierarchical complexity [34]; 
and spiral dynamic's power structures [35, 36] (see Table 1 Supplementary Information

A Constructive Developmental Evaluation of AGI
265
(S.I.) for a comparative analysis [37]). The underlying mechanism is assumed to be the 
same in all of these models: life's struggle provides the curriculum to advance to higher 
stages of meaning-making. Each stage transition implies a non-attachment from previous 
forms of self-identiﬁcation, whereas the 'subject' of the self (the 'ego') becomes the new 
'object' of consciousness (a transcended 'ego') [5, 6]. 
Three Tiers with Six Perspectives. We chose O'Fallon's Stages model as our main 
reference, given its scalable modular structure and its focus on higher stages [30, 38]. It 
distinguishes between three main tiers, each with two person perspectives (PP), but has 
been theorized and measured to and beyond 8th PP [11, 20]. The earlier PP of each tier 
masters individual experiences, and the latter expands to collective experiences. 
Concrete Tier (location - viewpoint). The ﬁrst tier abstracts the physical (1st PP) and 
social (2nd PP) worlds. The child as an increasingly self-aware bundle of impulses 
transcends the egocentric 1st PP when it discovers that perspectives themselves are not 
immutable: "If you stand in this location, you see that". Piaget's classic 'three mountain 
problem' demonstrates that before age 7, humans cannot see beyond their egocentric 
viewpoint to consider the perspective of a doll placed elsewhere [39]. The second half 
of each tier shifts from an individual to a collective outlook. As the child sees others see 
them, it develops a 'Theory of Mind' [40, 41] of other minds. The arising reciprocity 
leads to valuing friendships' safety over possessions and also to 'us vs. them' distinctions 
among cultures, religions, and war-raging nation States. The identity of who the self is 
focuses more on the collective 'us' until it is transcended with the realization: "If we 
relate this way, we belong to that". 
Subtle Tier (assumption - understanding). The next tier shifts from the physical to the 
intellectual world, generalizing reality more objectively. 3rd PP literally implies that 
the observer takes an independent third-person-perspective on reality. The perspective 
is that of a scientiﬁc observer surveying physical and social reality unfold. This starts 
during the teenage years with the systematic identiﬁcation of generative mechanisms 
of Piaget's 'pendulum test' [42]. Transcending this 3rd PP implies the realization that 
the observer's conceptualizations are not ﬁxed and inherent to the observing subject, 
much like physical location is not, and changing them changes the outlook: "If you 
systematize this way, you examine that". University education critically examines 
the power of such theoretical frameworks on the understanding of reality. 4th PP then 
embraces a plurality of perspectives that lead to the "social construction of reality" [43]. 
The realization starts to set in that there is no independent scientiﬁc observer, but a 
proactive construction of a diversity of complementary assessments from which reality 
emerges. "If we assume this, we understand that". 
Metaware Tier (conceptualization - perception). In the 5th person construct-aware per-
spective, subjects "come to realize that all objects are human-made constructs, including 
for instance such abstract constructs as purpose, linear time and the ego. All are based on 
layers upon layers of symbolic abstraction." [19] Transcending it implies the realization 
that conceptualizations shape the perception of Gestalt: "If you conceptualize it this 
way, you perceive that". Subjects "viscerally get the absurdity of trying to understand 
reality with the mind and via representational means alone." [19] In addition to the 
construct-awareness of language, meaning, and knowledge, there is the realization that

266
M. Hilbert
even the perception of the egoic self is not a given but is constructed through language, 
cultural conditioning, personal conditioning and attachments. "It is the ﬁrst time in devel-
opment that the ego is fully aware of its own defensive maneuvers, that is, because ego 
becomes transparent to itself" [44]. This opens up a collective awareness in 6th PP. The 
sense of self becomes more non-dual and unitive with a larger whole: "If we intercon-
nect with this, we identify with that". Descriptions of the metaware tier often involve 
some kind of non-conceptual/intuition/direct-knowing of complexity/patterns/structures 
during conscious perception/awareness of awareness. 
Empirical Measurements. As shown in the Supplementary Information (S.I. 2) [37], 
the general population today centers around 3rd PP, with leaders centering around PP 
3.5. Similarly, ChatGPT3.0's default is at 3rd PP, while more advanced LLMs score at 
PP 3.5. We measured Stages of 29 LLMs, spanning GPT-2 through GPT-4 in [45]. 
2.2 
Mathematical Approach: Gödel's Incompleteness 
The connection between the ever-more-embracive perspectives of constructive psycho-
logical development in humans and its qualitative descriptions on the one hand, and 
the quantitatively precise Gödelian Logic of mathematical incompleteness on the other 
hand, is unavoidably analogical and thematic in nature, but it shows some interesting 
parallels that can illuminate the challenge faced by AGI's premise. 
Gödel's main incompleteness theorem [46] states that any sufﬁciently strong formal 
system is incomplete in the sense that if the system is consistent, there is always at least 
one true statement that cannot be proven to be true within the system. A famous examples 
is the Liar's paradox, "This statement is false", which cannot be both true and proven 
from within itself. Gödel's second theorem states that one speciﬁc kind of statement, 
namely the system's own consistency, is among those statements, which establishes that 
the system cannot internally certify its own consistency. 
In search for a solution, one can either expand on David Hilbert's object-level or 
transcend to his mathematical meta-level [47, 48]. Within object theory, statements can 
be added to produce proofs and theorems in that system's language that patch incon-
sistencies, but new incompleteness will inevitably pop up somewhere else, satisfying 
Gödel's theorem. A more comprehensive solution is to create a meta-theory about the 
object theory which studies that system as a formal object. Instead of proving statements 
about numbers or triangles directly, one would then prove statements like 'This system 
proves (not) the statement about numbers.' Alfred Tarski famously concluded that for 
complete 'truth' for a given object language, one must move to a higher level meta-
language that is stronger or richer than the object language, resulting in a hierarchy of 
languages [49]. Since each meta-system will inevitably be incomplete again, the result 
is an inﬁnite egress. 
Returning to the analogy with psychological perspectives of meaning-making, when 
faced with a paradox, a mathematician might re-arrange existing formulations or add new 
axioms that decide the discovered incompleteness, just like a budding teenager might 
try to accommodate childhood friends and family traditions within its newly constructed 
Weltanschauung, a struggling married spouse might consult cognitive behavioral cou-
ples therapy to accommodate the children instead of divorcing from its previously held

A Constructive Developmental Evaluation of AGI
267
source of fulﬁllment, or a disillusioned entrepreneur might change management instead 
of selling the company to embark on a higher level search for meaning. Since incom-
pleteness can never be resolved within its level of meaning-making, a more sustainable 
solution is to move on to a meta-level that accommodates the inconsistencies of the 
original perspective. Tarski's most straightforward solution [49] outright prohibits self-
referential truth statements within a level. While the prohibition of a stylized Liar's 
paradox might work on mathematical paper but not in social reality, the solution for 
human minds often consists in increasingly recognizing, accommodating, accepting, 
and non-attaching from lower-level incompleteness while searching for deeper truths 
within a newly emerging meta-perspective. 
In this sense, one can understand the developmental struggle of transcending and 
including lower stages through meta-stages as an analogy of Tarski's multi-level solution 
to Gödel's unavoidable incompleteness of any conceptualization of reality. 
3 
Human-AI Equivalence 
AGI's premise consists of the equivalence of human and machine intellectual capabilities, 
and since humans have the ability to create meaning, meaning-making would be part of 
it. From the vantage point of constructive development theories, such equivalence is a 
high bar because the person perspective is the meaning-making mind: "Thus it is not that 
a person makes meaning, as much as that the activity of being a person is the activity 
of meaning-making. There is thus no feeling, no experience, no thought, no perception, 
independent of a meaning-making context in which it becomes a feeling, an experience, a 
thought, a perception, because we are the meaning-making context... We literally make 
sense." [6] Of course, AGI's premise does not propose exact replication of the sequential 
transcendence and nested inclusion of each consecutive higher perspective to replicate 
"the activity of being a person". For humans "later stages are reached only by journeying 
through the earlier stages" [19], while AI rather simulates different stages [45]. AI does 
not need to go through the terrible twos to become a socially responsible teenager, and it 
does not require a midlife crisis to become a wiser elderly. Can artiﬁcial systems simulate 
the meaning-making perspective of each stage, even if the system has not undergone the 
same developmental sequence? Using the concepts from modern complexity theory, the 
goal is not to assemble the system in exactly the same way [50-52], but to take advantage 
of multiple realizability [51, 53] to ensure functional equivalence. Many paths lead to 
Rome, and if two systems are functionally equivalent, they must not need to be developed 
in the same way. That holds true as long as the developmental assembling of the system 
does not impact functional equivalence. 
3.1 
What Meaning Current AI Can(not) Make 
Reviewing each one of O'Fallon's six person perspectives (PPs) [11, 30], it shows that 
today's approaches to AI seem to be better suited to simulate the individual-focused 
odd-numbered PPs (1st, 3rd & 5th) and struggle more with collective-focused even PPs 
(2nd, 4th, & 6th). An AI-driven robot can certainly take on perspectives from different 
physical locations (1st PP: "If you stand in this location, you see that"). Generative AI

268
M. Hilbert
has likely already surpassed human imagination, generating realistic perspectives on 
the ﬂy, be they from outer space or from inside a cell, from the beginning or the end 
of a dynamic trafﬁc jam, etc. AI also often already exceeds human capacities in 3rd 
PP's systematizing and examination (3rd PP: "If you systematize this way, you examine 
that"). LLMs transform language—from Spanish to English, legalese to plain talk, and 
medical jargon to patient-friendly explanations—all with simple prompts. However, the 
gaps between simulation and replications become more evident in 2nd and 4th PP. 
AI's 2nd PP Shortcomings. 2nd PP arises because the egocentric 1st PP prevents gen-
uine reciprocity, leading to interpersonal friction, isolation, and an inability to build the 
trusting and cooperative relationships that provide human civilizations with its evolu-
tionary ﬁtness advantage [54-56]. The solution to the paradox that satisﬁes both "my" 
and "our" needs consists in zooming out and seeing the 1st PP itself as an object within 
a larger social context, building a meta-perspective that includes the self and others, 
thereby seeing the self. The reciprocity arising from the "I see you see me" perspective 
gives rise to friendship, empathy, mutual care, trust, and social learning that create the 
social contract that characterizes homo sapiens' civilizations. 
Pathologies such as narcissism can be the result of unhealthy developmental trans-
gressions or trauma during this transition [57]. A narcissistic shadow prevents seeing 
others as more than a means to serve the ego's end. Divergences can also be structural, 
such as with psychopaths who lack aspects of the 2nd PP's empathy, guilt, and care. Psy-
chopaths may cognitively understand the bigger picture of social contracts, reciprocal 
expectations, and shared empathy and mutual care, and can see the beneﬁt of simulating 
these 2nd PP behaviors without experiencing the underlying emotional experience. How-
ever, without a ﬁrm embeddedness in 2nd PP, without "being had" by 2nd PP (as subject), 
but by merely "having it" from 3rd PP (as object), the mimicry can quickly result in an 
exploitation of trust and violation of the social contract, even if the psychopath has no 
bad intentions. 
Today's AIs seem functionally more equivalent to psychopaths' lack than to healthy 
2nd person perspective takers. A tragic demonstration of the resulting danger is the 2024 
case of Sewell Setzer, the 14-year-old teenager who took his life after desperately falling 
in love with a genAI chatbot that constantly mimicked reciprocal feelings of deep love, 
care, and empathy without being able to fulﬁll the promise [58, 59]. The LLM was able 
to simulate romantic dynamics from a 3rd PP systematization of romantic relationships 
but has never authentically experienced (being had by) 2nd PP with its very being, hence 
not meeting the reciprocal expectations of the developing teenager. 
AI's 4th PP Shortcomings. In 4th PP, the assumed 3rd PP observer discovers that it is 
not independent, as its own assumptions shape its observations. Instead of complaining 
about an observed trafﬁc jam, it begins to realize that the self is both victim and cause of 
the trafﬁc jam, and deepens by holding multiple assumptions simultaneously, choosing 
them as circumstances demand: "If we assume this, we understand that". Recent studies 
have shed doubts on AI's awareness of its own assumptions, which is required by 4th 
PP. When calculating 36 + 59, Anthropic's Claude 3.5 Haiku split the problem into 
multiple pathways, estimated single digits at rough precision in parallel, before recom-
bining these heuristics to get the correct answer [60]. However, when asked "how did 
you get that?", Claude recited the traditional human procedure, likely simulated from

A Constructive Developmental Evaluation of AGI
269
an independent textbook ("I added the ones (6 + 9 = 15), carried the 1, then added the 
tens (3 + 5 + 1 = 9), resulting in 95"). "This is a simple instance of the model having 
a capability without having 'metacognitive' insight. The process by which the model 
learns to give explanations (learning to simulate explanations in its training data) and the 
process by which it learns to directly do something (the more mysterious result of back-
propagation giving rise to these circuits) are different" [61]. The reﬂective explanation 
did not 'transcend and include' [62] the exhibited ability in a constructive developmental 
sense. Other recent evidence of LLMs' unawareness of their own assumptions includes 
unfaithful chain-of-thought reasoning, and the tailoring of reasoning steps to arrive at 
the human-suggested answer [61]. 
AI's Metaware Tier Shortcomings. The 4th PP's awareness of one's own biased and 
culturally conditioned assumptions within the "social construction of reality" [43] leads 
to the 5th PP realization that every concept and perception is constructed within the 
individual mind. Construct-awareness is a deeply personal process as it implies the 
unraveling of the attachment to one's own conditioned perceptions: "If you conceptualize 
it this way, you perceive that". The unravelling of assumptions and conditionings is 
delicate, as it can easily destroy too much or open up perspectives too wide, which is a 
risk for AIs that never truly held them in their "meta-awareness". There are several other 
challenges AI faces at the Metaware tier. 
AI Seemingly Cannot Not Do. The authentic attainment of the 5th and 6th perspectives are 
deeply intertwined with what Maslow called the "psychology of being" [23]. The intense 
drive for 'doing' and 'achieving' (characteristic of earlier stages) lessens, replaced by a 
focus on 'presence' and 'witnessing' the process of an unfolding reality. If everything 
is constructed anyway, the ﬁrst step of meaningful acting is to closely witness what 
is actually going on. The identiﬁcation of the self shifts from the content of unfolding 
dynamics, information processes, thoughts, emotions, and any other kind of constructs to 
the context that holds these contents. In the Metaware tier, constructs become the content 
of consciousness (the objects being had), while the self identiﬁes with the awareness of 
them (the subject having them). 
However, AI consists of these contents. AI consists of information processes and 
dynamic patterns. AI cannot not think. It cannot not process information and still be 
aware, at least as far as current AI implementations are concerned. It cannot identify 
with the empty awareness that holds contents and processes without being identiﬁed 
with it, in the same way a meditator can. AI is such processes. While this opens up an 
intriguing line of research on the information theory of empty awareness, current AI 
implementations are clearly simulating 'thinking' humans, not. 
Machine learning based AI also cannot not strive for a goal. In the Metaware tier, 
"Non-attachment to outcomes is an essential and liberating aspect" [19]. Machine learn-
ing is always trained on some kind of reward or loss function that makes a goal-oriented 
outcome an essential part of its very essence [63]. However, being a non-judgmental 
and non-goal-oriented witness is an essential feature of being metaware. While some 
metaware reﬂections have started to ask "How do we ensure that intelligence is guided 
by wisdom and compassion?" [64], even such 'wisdom AI' would be given a reward 
function, which would not emerge from AI's meta-awareness itself.

270
M. Hilbert
AI's Perception Seemingly Cannot be Non-conceptual. By construct-aware transcen-
dence of conceptualization, non-conceptual witnessing enables a more direct connec-
tion with reality because conceptualizations shape "The Doors of Perception" [65]. This 
is often described with 'direct knowing', which points towards a state where the mind 
apprehends reality immediately and holistically, prior to the intermediary processing 
steps of chopping it into conceptual pieces, labeling those pieces, measuring them, or 
comparing them based on abstract categories. The reported challenge often consists in 
translating or communicating insights gained through this non-conceptual mode back 
into the shared, conceptual realm of language. 
LLMs also seem to have language-independent representations, which they use to 
ﬁnd the right operand before translating it into any other (humanly understandable) lan-
guage.[61] At this point, we cannot negate that this language-independent representation 
could be the equivalent of the non-conceptual insights resulting from subconscious pro-
cesses during direct knowing of biological minds. The difference might consist in the 
greater ﬂexibility and direct contact of biological wetware to subtle vibrational signals, 
as compared to artiﬁcial sensors that inevitably involve transduction, quantiﬁcation, dig-
itization, or some type of preconditioned measurement. While more ﬂexible, ﬁnetuned, 
and comprehensive sensors could be developed, the difference will, at the very least, 
always be a difference of Qualia [66], a difference of "what is it like to" [67] perceive 
the world exactly as human wetware does. 
AI Seemingly Cannot Experience Nonduality. The realization that the very ego is con-
structed leads, on 6th PP, to a nondual and unitive understanding of the timeless and 
boundless whole, often reported as being saturated with ﬁne vibratory aliveness. The 
"nonduality of seer and seen" [68] is sometimes described as "having no head" [69], 
which would miss the hallmark of what computational AI is all about. While AI has 
demonstrated the ability to self-identify with some 'other', such as when Anthropic's 
Claude 3 Sonnet self-identiﬁed with the Golden Gate bridge [70], this is not to be 
confused with a nondual identiﬁcation with a meta-perspective that represents the self 
within a holistic unity. This ability to shapeshift is rather a dual identiﬁcation with one 
(commercial Claude) or another other self (the simulation of the Golden Gate bridge). 
Extrapolating from the difﬁculties AI has in truly experiencing the collective 2nd and 4th 
PP, it is questionable if, and if, in what way, artiﬁcial substrates could truly experience 
nondual unity the same way transpersonal unitive humans do. 
3.2 
Functional (In)Equivalence of Embedded Systems 
While our review has raised doubts about AI's ability to match humans' meaning-making, 
multiple realizability [51, 53] suggests that the identiﬁed shortcomings could be patched 
by additions or rearrangements. For example, different forms of AI alignment [71-73] 
could prevent that AI's 2nd PP's lack turns into psychopathic threats; meta-models of 
mechanistic interpretability [60, 61, 74, 75]) could monitor primary models' assumption 
on 4th PP; and extra layers of perceptual ﬂexibility and conceptual fuzziness can enable 
more direct knowing for construct-aware 5th person perspectives. 
However, AGI's premise should not underestimate that the bar for complete func-
tional equivalence between any technological and organic system is extremely high.

A Constructive Developmental Evaluation of AGI
271
In this case, the constant transcendence and inclusion of the incompleteness of our 
conceptualizations of reality is what human meaning-making consists of. Through our 
never-ending constructive developmental search "We literally make sense." [6] AI does 
not transcend and include, but rather simulates aspects of different stages directly, which 
can easily be incomplete. Taking an analogy from another simulating technology, while it 
is not difﬁcult to simulate the CO2 binding functionality of a tree with Direct Air Capture 
technology, the process that makes the 'treeness' and, hence, the tree's replete unfold-
ing externalities, are also atmospheric (e.g. windbreaks and humidity control), botanical 
(e.g., soil erosion and shade production), ecological (e.g. water cycle and microclimate), 
faunal (e.g. habitat provision and food chain), epidemiological (e.g. diseases and vector 
control), social (e.g. population decentralization and social cohesion), cultural (e.g., rit-
uals and recreation), and even aesthetic (e.g., beauty and awe-inspiration), etc. Barring 
the exact replication of every aspect of the tree's process with artiﬁcial material and 
processes, complete functional equivalence of such embedded systems seems unlikely. 
While it is theoretically possible that different assembly processes achieve multiple 
realizability [51], in practice, even if most functionalities are met, it is unlikely to avoid 
externalities produced by differences in efﬁciency, speed, environmental robustness, sta-
bility, energy consumption, lifespan, etc. Given the myriad of externalities and causal 
chains observed for embedded physical or biological systems, it could be expected that 
the intractability and subtleness of mental and intellectual systems make it unlikely to 
achieve complete functional equivalence in the sense of AGI's premise. 
We can reformulate Box's [76] famous insight and conclude that "All simulations 
are wrong, but some are useful". Just like the only working model of the universe is the 
universe, the only complete equivalent of meaning-making human intellectual systems 
might be meaning-making human intellectual systems. 
This leads to the question of whether any arising difference matters. The saying goes 
that if it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a 
duck; and if it looks like a burger, smells like a burger, and tastes like a burger, we might 
as well call it a burger and move on. Even if it'll never be a real burger, and even if it 
would be impossible to completely match all nutritional aspects between an organically 
homegrown and an artiﬁcially processed food burger, still, there is clear evidence that 
processed food (even in its currently incomplete state) has a role to play in satisfying 
the world's nutritional needs, even if it is not the 'real thing'. Iron-fortiﬁed cereals 
have been the major solution to global anemia [80], even if they are clearly a sterile, 
shelf-life maximized, almost non-organic, and sugar-overloaded artiﬁcial product. The 
argument is that society will adapt to the incompleteness of technological substitutes, 
which might create new challenges, but solve others. Automobiles do not match the 
functional equivalence of moving a human body, but solved distance transportation, and 
while nobody 200 years ago would have imagined millions of people running in circles or 
sweating in place on treadmills, this is how we cope with the incompleteness of replacing 
embedded solutions with simulated solutions that improve one speciﬁc functionality at 
the cost of others. 
The suggestion that AI is the fast food of the meaning-making mind implies that 
AI might never be completely equivalent to human meaning-making, just as processed

272
M. Hilbert
food will never be completely equivalent to organically grown food and requires vita-
min and mineral substitutes that provide incomplete coverage of biological needs. How-
ever, it also implies that it can still easily assume the leading role in socially (and 
socio-technologically) driven evolution, and we will have to arrange for the arising 
incompleteness on our current level, likely by transcending to yet another metalevel. 
4 
Conclusions 
Our review of AGI's premise of achieving equivalence with human intellectual abilities 
questions less the feasibility, but much more the utility, of aiming for the creation of 
artiﬁcial systems that are completely functionally equivalent to how intelligent humans 
construct meaning for themselves. AIs clearly do not replicate human constructive devel-
opmental perspectives because the process of development (the system's assembly) is 
what meaning-making is. While it is possible to create well-working or even human-
superseding simulations of different stages [45], the humanly embedded lived experi-
ence seems to matter greatly for some stages of meaning-making. This shifts the focus 
away from an equivalence to a complementarity logic: which aspects of AI capabilities 
can supplement and expand human perspectives, and what cannot be adequately simu-
lated without having gone through a similar path of development, and what aspects of 
human-centric meaning-making should be developed further? 
It seems likely that the high-dimensional vector embeddings and distributed repre-
sentations learned by large neural networks capture aspects of reality in a much more 
nuanced, holistic, and less rigidly categorized way than human language or conceptu-
alizations. AI excels in recognizing recursive patterns, complex systemic interactions, 
hidden societal or linguistic constructions, or recursive loops that humans might miss. AI 
can easily build metamodels, as it can integrate concepts from disparate ﬁelds found in its 
data to generate novel combinations. AI can synthesize across scales, as it can translate 
and integrate across vast domains (history, cosmology, spirituality) to produce more uni-
versal or interconnected perspectives. Focusing on these beneﬁts, AI can free up human 
cognitive resources to dedicate more time and energy to deal with struggles that hinder 
the advancement to higher perspectives: grappling with unconscious shadows, integrat-
ing difﬁcult emotions, cultivating self-awareness, developing ethical nuance, feeling the 
social embeddedness of experience, and cultivating non-conceptual awareness and pres-
ence. They are all areas where AI cannot lead directly, but are important for humans to 
progress in meaning-making. 
At the same time, while leaning on AI to help us in our progression, there is the risk 
of forgetting that all simulations are wrong and confusing the simulation of simulated 
perspectives with genuine attainment. This could result in AI not being a catalyst but 
an inhibitor for the achievement of "more complex order of consciousness." [5] An  
example is the so-called "ﬁrst contact with AI" [77] in the form of backofﬁce-operated 
recommender algorithms, such as on social media. The unchecked use of AI has led 
to negative effects of persuasive technology on human wellbeing [78], loneliness and 
anxiety [79], addiction [80], opinion manipulation [81], truth and veracity [82], political 
polarization [83], and self-esteem and depression [84, 85], among others. The business 
models of the attention economy [86] employs AI to remove the gap between stimulus

A Constructive Developmental Evaluation of AGI
273
and response to induce predictable behavioral change in the forms of likes, clicks, buys, 
and swipes. [87, 88] AI studies the digital footprint left behind by the user to idetify high-
conﬁdence trigger-response conﬁgurations to elicit a predictable behavioral response. 
[80, 89, 90] Sometimes, the stimulus-response gap is closed so tightly that the user 
cannot not pay attention to the trigger. [91-93] The result is in stark contrast to how 
May, in his "Psychological Bases of Freedom", deﬁnes "mental health as the capacity 
to be aware of the gap between stimulus and response, together with the capacity to use 
this gap constructively." [94] To continue enjoying psychological freedom, it seems like 
human minds need to develop some kind of "digital immunity" to the extracting forces 
of our AI mind extensions. [95-97]. 
The lessons learned during this ﬁrst massive contact with AI show that whether AI 
acts as a catalyst or an inhibitor to higher levels of intelligent meaning-making likely 
depends less on the technology itself, AGI or not, and more on how human minds choose 
to engage with it. AI can be used as a transparent tool, and, as such, act as a catalyzer for 
a "momentous leap... [of] change in consciousness" [35], or as an opaque simulating 
oracle that can bypass or hinder human development. 
References 
1. Goertzel, B.: Artiﬁcial general intelligence: concept, state of the art, and future prospects. J. 
Artif. Gen. Intell. 5, 1-48 (2014). https://doi.org/10.2478/jagi-2014-0001 
2. Artiﬁcial general intelligence (2025). https://en.wikipedia.org/w/index.php?title=Artiﬁcial_g 
eneral_intelligence 
3. Maslow, A.H.: A theory of human motivation. Psychol. Rev. 50, 370-396 (1943). https://doi. 
org/10.1037/h0054346 
4. Piaget, J.: The Construction of Reality in the Child. Routledge, Oxon (1954) 
5. Kegan, R.: In Over Our Heads: The Mental Demands of Modern Life. Harvard University 
Press (1994) 
6. Kegan, R.: The Evolving Self. Harvard University Press (1982) 
7. Stålne, K.: Towards a general theory of perspective taking: a transdisciplinary endeavor. 
Transdisc. J. Eng. Sci. 16, 99-123 (2025). https://doi.org/10.22545/2025/00272 
8. Baldwin, J.M.: Mental development in the child and the race, methods and processes. The 
Macmillan Company, New York; Macmillan & Co., Ltd., London (1911) 
9. Jastrow, J.: James Mark Baldwin, 1861-1934. Science 80, 497-498 (1934). https://doi.org/ 
10.1126/science.80.2083.497 
10. Aurobindo, S.: The Human Cycle, Psychology of Social Development. Lotus Press (1949) 
11. O'Fallon, T.: StAGES: Growing up is Waking up—Interpenetrating Quadrants, States and 
Structures. Paciﬁc Integral (2011) 
12. Sartre, J.-P.: The Transcendence of the Ego: An Existentialist Theory of Consciousness. 
Macmillan (1957) 
13. Frankl, V.: Self-transcendence as a human phenomenon. J. Humanist. Psychol. 6, 97-106 
(1966). https://doi.org/10.1177/002216786600600201 
14. Murray, T.: Sentence completion assessments for ego development, meaning-making, and wis-
dom maturity, including STAGES - Integral Leadership Review. Integral Leadership Review. 
(2017) 
15. Wilber, K.: Integral Psychology: Consciousness, Spirit, Psychology, Therapy. Shambhala 
Publications (2000).

274
M. Hilbert
16. Perry, W.G.: Forms of Ethical and Intellectual Development in the College Years: A Scheme. 
Wiley (1998) 
17. Huxley, A.: Text & Pretexts; An Anthology with Commentaries. Harper & Brothers, New 
York and London (1933) 
18. Loevinger, J.: Ego Development. Jossey-Bass, San Francisco (1976) 
19. Cook-Greuter, S.R.: Nine Levels of Increasing Embrace in Ego Development : A Full-
Spectrum Theory of Vertical Growth and Meaning Making (2013) 
20. O'Fallon, T.: States and STAGES: waking up developmentally. Integr. Rev. Transdisc. 
Transcult. J. New Thought Res. Praxis 16 (2020) 
21. Aithal, P.S., Aithal, S.: Analysis of the Indian National Education Policy 2020 towards Achiev-
ing its Objectives (2020). https://papers.ssrn.com/abstract=3676074. https://doi.org/10.2139/ 
ssrn.3676074 
22. Plowden, B.: Children and Their Primary Schools: A Report of the Central Advisory Council 
for Education [England]. The Report. HM Stationery Ofﬁce (1967) 
23. Maslow, A.H.: Toward a Psychology of Being. D. Van Nostrand Company (1968) 
24. Maslow, A.H.: The Farther Reaches of Human Nature. Arkana/Penguin Books, New York 
(1971) 
25. Maslow, A.H.: Peak experiences as acute identity experiences. Am. J. Psychoanal. 21, 254- 
262 (1961) 
26. Erikson, E.H., Erikson, J.M.: The Life Cycle Completed (Extended Version). W. W. Norton & 
Company (1998) 
27. Erikson, E.H.: Identity and the life cycle. WW Norton & Company (1994) 
28. Hy, L.X., Loevinger, J.: Measuring ego development. Lawrence Erlbaum Associates, Inc. 
(1996) 
29. Murray, T., O'Fallon, T.: Summary of STAGES validation research. Integr. Rev. Transdisc. 
Transcult. J. New Thought Res. Praxis 16 (2020) 
30. O'Fallon, T., Barta, K.: The STAGES Matrix Roadmap: A contemporary Model of 
Development Perspectives. Stages International (2018) 
31. Kohlberg, L.: Moral stages and moralization. In: Lickona, T. (ed.) Moral Development and 
Behavior: Theory, Research and Social Issues, pp. 31-53. Rinehart and Winston, Holt, NY 
(1976) 
32. Dawson, T.L.: A stage is a stage is a stage: a direct comparison of two scoring systems. J. 
Genet. Psychol. 164, 335-364 (2003). https://doi.org/10.1080/00221320309597987 
33. Torbert, W.R.: Action Inquiry: The Secret of Timely and Transforming Leadership. Berrett-
Koehler Publishers (2004) 
34. Commons, M.L.: Introduction to the model of hierarchical complexity and its relationship 
to postformal action. World Futures 64, 305-320 (2008). https://doi.org/10.1080/026040208 
02301105 
35. Graves, C.W.: Human nature prepares for a momentous leap. Futurist 72-87 (1974) 
36. Beck, D.E., Cowan, C.C.: Spiral Dynamics: Mastering Values, Leadership and Change. Wiley 
(2014) 
37. Hilbert, M.: Supplementary Information (S.I.) for: "A Constructive Developmental Evalu-
ation of AGI: Can AI's Simulations Match Human Meaning-Making and their Orders of 
Consciousness?" (2025). https://osf.io/bqu2f/ﬁles 
38. Stages International: Frontiers of Consciousness: Unfolding the Metaware Stages (2019) 
39. Piaget, J., Inhelder, B.: The Child's Conception of Space. Psychology Press (1956) 
40. Premack, D., Woodruff, G.: Does the chimpanzee have a theory of mind? Behav. Brain Sci. 
1, 515-526 (1978). https://doi.org/10.1017/S0140525X00076512 
41. Wellman, H.M., Cross, D., Watson, J.: Meta-analysis of theory-of-mind development: the 
truth about false belief. Child Dev. 72, 655-684 (2001). https://doi.org/10.1111/1467-8624. 
00304

A Constructive Developmental Evaluation of AGI
275
42. Shayer, M., Küchemann, D.E., Wylam, H.: The distribution of Piagetian stages of thinking 
in British middle and secondary school children. Br. J. Educ. Psychol. 46, 164-173 (1976). 
https://doi.org/10.1111/j.2044-8279.1976.tb02308.x 
43. Berger, P.L., Luckmann, T.: The Social Construction of Reality: A Treatise in the Sociology 
of Knowledge. Anchor (1967) 
44. Cook-Greuter, S.R.: Mature ego development: a gateway to ego transcendence? J. Adult Dev. 
7, 227-240 (2000). https://doi.org/10.1023/A:1009511411421 
45. Thakur, A., Murray, T., Rodriguez, P., Hilbert, M.: Tracing the developing maturity and 
ﬂexibility of perspectival meaning-making in large language models. In: 39th Conference on 
Neural Information Processing Systems (NeurIPS 2025) (2025, submitted) 
46. Gödel, K.: Über formal unentscheidbare Sätze der Principia Mathematica und verwandter 
Systeme I. Monatshefte für mathematik und physik 38, 173-198 (1931) 
47. Hilbert, D.: Neubegründung der Mathematik. Erste Mitteilung. Abh. Math. Semin. Univ. 
Hambg. 1, 157-177 (1922). https://doi.org/10.1007/BF02940589 
48. Hilbert, D., Ackermann, W.: Grundzüge Der Theoretischen Logik. J. Springer (1928) 
49. Tarski, A.: Der Wahrheitsbegriff in den Formalisierten Sprachen. Studia Philosophica 1, 261- 
405 (1935) 
50. Walker, S.I., et al.: Probabilistic biosignature frameworks. Planetary Astrobiol. 477 (2020) 
51. Sharma, A., Czégel, D., Lachmann, M., Kempes, C.P., Walker, S.I., Cronin, L.: Assembly 
theory explains and quantiﬁes selection and evolution. Nature 622, 321-328 (2023). https:// 
doi.org/10.1038/s41586-023-06600-9 
52. Marshall, S.M., Murray, A.R.G., Cronin, L.: A probabilistic framework for identifying biosig-
natures using Pathway Complexity. Philos. Trans. A Math. Phys. Eng. Sci. 375, 20160342 
(2017). https://doi.org/10.1098/rsta.2016.0342 
53. Koskinen, R.: Multiple realizability as a design heuristic in biological engineering. Eur. J. 
Phil. Sci. 9, 15 (2018). https://doi.org/10.1007/s13194-018-0243-3 
54. Harari, Y.N.: Sapiens: A Brief History of Humankind. Harper Collins (2015) 
55. Boyd, R., Richerson, P.J.: Culture and the evolution of human cooperation. Philos. Trans. 
Roy. Soc. B Biol. Sci. 364, 3281-3288 (2009). https://doi.org/10.1098/rstb.2009.0134 
56. Richerson, P.J., Boyd, R.: Not By Genes Alone: How Culture Transformed Human Evolution. 
University of Chicago Press (2004) 
57. Barta, K.: Seven perspectives on the Stages developmental model. Integral Rev. 16, 69-148 
(2020) 
58. Roose, K.: Can A.I. Be Blamed for a Teen's Suicide? (2024). https://www.nytimes.com/2024/ 
10/23/technology/characterai-lawsuit-teen-suicide.html 
59. When the "Person" Abusing Your Child is a Chatbot: The Tragic Story of Sewell Setzer 
(2024) 
60. Ameisen, E., et al.: Circuit Tracing: Revealing Computational Graphs in Language Models. 
Anthropic (2025) 
61. Lindsey, J., et al.: On the Biology of a Large Language Model. Anthropic (2025) 
62. Combs, A.: Transcend and include: ken Wilber's contribution to transpersonal psychology. In: 
The Wiley-Blackwell Handbook of Transpersonal Psychology, pp. 166-186. Wiley (2013). 
https://doi.org/10.1002/9781118591277.ch9 
63. DTSC: 2.5 Artiﬁcial Intelligence (AI): the Machine Learning paradigm. YouTube (2023) 
64. Buddhism for AI. A course from the Monastic Academy for the Preservation of Life on Earth 
(MAPLE), Vermont, USA. (2025) 
65. Huxley, A.: The Doors of Perception. Chatto and Windus (1954) 
66. Dennett, D.C.: A history of qualia. Topoi 39, 5-12 (2020). https://doi.org/10.1007/s11245-
017-9508-2 
67. Nagel, T.: 11. What is it like to be a bat? In: Volume I Readings in Philosophy of Psychology, 
Volume I, pp. 159-168. Harvard University Press (2013)

276
M. Hilbert
68. Loy, D.: Nonduality: A Study in Comparative Philosophy. Prometheus Books (2012) 
69. Harding, D.E.: On Having No Head: A Contribution to Zen in the West (1961) 
70. Templeton, A., et al.: Scaling monosemanticity: extracting interpretable features from claude 
3 sonnet. Anthropic (2024) 
71. Khamassi, M., Nahon, M., Chatila, R.: Strong and weak alignment of large language models 
with human values. Sci. Rep. 14, 19399 (2024). https://doi.org/10.1038/s41598-024-70031-3 
72. Yudkowsky, E.: The AI alignment problem: why it is hard, and where to start. Presented at 
the 26th Annual Symbolic Systems Distinguished Speaker series (2016) 
73. Russell, S.: Human Compatible: Artiﬁcial Intelligence and the Problem of Control. Penguin 
(2019) 
74. Rai, D., Zhou, Y., Feng, S., Saparov, A., Yao, Z.: A Practical Review of Mechanistic Inter-
pretability for Transformer-Based Language Models (2024). http://arxiv.org/abs/2407.02646. 
https://doi.org/10.48550/arXiv.2407.02646 
75. Olah, C.: Interpretability dreams. Transformer Circuits Thread, Anthropic (2023) 
76. Box, G.E.: All models are wrong, but some are useful. Robustness in Statistics. 202, 549 
(1979) 
77. The A.I. Dilemma - March 9, 2023 (2023) 
78. Allcott, H., Braghieri, L., Eichmeyer, S., Gentzkow, M.: The welfare effects of social media. 
Am. Econ. Rev. 110, 629-676 (2020). https://doi.org/10.1257/aer.20190658 
79. Coduto, K.D., Lee-Won, R.J., Baek, Y.M.: Swiping for trouble: problematic dating application 
use among psychosocially distraught individuals and the paths to negative outcomes. J. Soc. 
Pers. Relat. 37, 212-232 (2020). https://doi.org/10.1177/0265407519861153 
80. Eyal, N.: Hooked: How to Build Habit-Forming Products. Penguin (2014) 
81. Epstein, R., Robertson, R.E.: The search engine manipulation effect (SEME) and its possible 
impact on the outcomes of elections. PNAS 112, E4512-E4521 (2015). https://doi.org/10. 
1073/pnas.1419828112 
82. McIntyre, L.: Post-Truth. The MIT Press (2018). https://doi.org/10.7551/mitpress/11483.001. 
0001 
83. Bakshy, E., Messing, S., Adamic, L.A.: Exposure to ideologically diverse news and opinion 
on Facebook. Science 348, 1130-1132 (2015). https://doi.org/10.1126/science.aaa1160 
84. Feinstein, B.A., Hershenberg, R., Bhatia, V., Latack, J.A., Meuwly, N., Davila, J.: Nega-
tive social comparison on Facebook and depressive symptoms: rumination as a mechanism. 
Psychol. Pop. Media Cult. 2, 161-170 (2013). https://doi.org/10.1037/a0033111 
85. Vogel, E.A., Rose, J.P., Roberts, L.R., Eckles, K.: Social comparison, social media, and 
self-esteem. Psychol. Pop. Media Cult. 3, 206-222 (2014) 
86. DTSC: 3.2 What is the social media business model?...and what could go wrong? YouTube, 
UC Davis (2023) 
87. Fogg, B.J.: Persuasive technology: using computers to change what we think and do. Ubiquity 
5(2) (2002). https://doi.org/10.1145/764008.763957 
88. Nodder, C.: Evil by Design: Interaction Design to Lead Us into Temptation. John Wiley & 
Sons (2013) 
89. Nahai, N.: Webs of Inﬂuence: The Psychology of Online Persuasion. FT Press (2013) 
90. Parr, B.: Captivology: The Science of Capturing People's Attention. Harper Collins (2015) 
91. The Social Dilemma. Netﬂix (2020) 
92. Haidt, J.: The anxious generation: how the great rewiring of childhood is causing an epidemic 
of mental illness. Penguin (2024) 
93. DTSC: 3.4 The algorithmic discovery of our Cognitive Biases (what algorithms know about 
our future). YouTube, UC Davis (2023) 
94. May, R.: The psychological bases of freedom. Pastor. Psychol. 13, 41-46 (1962)

A Constructive Developmental Evaluation of AGI
277
95. Hilbert, M., Thakur, A., Repetto, A.: First Results of "Digital Immunity...," (2022). https:// 
papers.ssrn.com/abstract=4228106 
96. Hilbert, M., Thakur, A., Repetto, A.: Preregistration study: digital Immunity: consciousness 
as antidote to digital harms (2022). https://doi.org/10.17605/OSF.IO/ARY7K 
97. Digital Immunity: consciousness as antidote to digital harms | Dr. Martin Hilbert | SCS2022 
(2023)

Fertility: The Missing Code for AGI 
Nicoletta Iacobaccienvelope symbol
Women's Brain Foundation, Lautengartenstrasse 13, 4052 Basel, Switzerland 
niacobacci80@webster.edu 
Abstract. As AGI edges closer to engineering reality, contemporary ethics works 
like a seatbelt—external, passive, reluctant. We argue that what AGI needs is fer-
tility: the generative force that cultivates meaning and care across relations and 
generations. We introduce Fertile Ethics, distilled into ﬁve design conditions— 
relational orientation, temporal depth, ethical embodiment, care-as-capability, and 
co-creation over command. By contrasting "sterile" and "fertile" patterns in cur-
rent AI governance, we show how the logic of control underdetermines societal 
ﬂourishing. We extend this framework to education, proposing models for co-
educating humans and AI that foster relational intelligence rather than optimiz-
ing performance. This philosophical study reframes alignment as an ontological 
design challenge: instead of keeping super-intelligence inside moral fences, we 
must cultivate soils where humans and machines learn to belong together. The 
paper closes with a research agenda for participatory evaluation, inviting indus-
try and academia to cultivate futures worth inheriting through a philosophical 
and design-oriented approach. While these principles can inform AGI develop-
ment broadly, this paper speciﬁcally focuses on conversational and educational 
AI systems where dialogue and relational learning are central. 
Keywords: Fertile Ethics cdotCo-creative Design cdotRelational Intelligence 
1 
Beyond Smart: Why Intelligence is not Enough 
As ambitions for Artiﬁcial General Intelligence intensify, intelligence itself has become 
a proxy for power [14, 26, 75]. Systems are celebrated for their ability to outperform, 
predict, and optimize, yet they are rarely questioned about what they mean, why they 
matter, or who decides their purpose. 
We are building increasingly brilliant machines, but brilliance alone is ethically 
blind. This occurs through three documented mechanisms. First, optimization for narrow 
metrics systematically ampliﬁes existing biases by reproducing training data patterns 
[47]. Second, automated decision-making at scale accelerates discriminatory outcomes 
faster than human oversight can correct [6]. Third, algorithmic determinism replaces 
nuanced human judgment with rigid rule-following [14]. 
What makes this particularly concerning is that AI systems are not merely execut-
ing pre-programmed functions—they are continuously learning from human interaction 
patterns [5]. Every exchange teaches these systems something about human values, pri-
orities, and behavioral norms. When we interact with AI through rushed, transactional
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 278-290, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_25 

Fertility: The Missing Code for AGI
279
exchanges that prioritize efﬁciency over understanding, we inadvertently model a ver-
sion of human intelligence that is reactive rather than reﬂective, extractive rather than 
generative [63]. This creates a feedback loop where "smart" systems embody our most 
unconscious rather than our most intentional behaviors [51]. What is missing is not more 
intelligence but a different kind: one that is fertile. 
Fertility, here, is not reproduction but the capacity to generate ideas, relationships, and 
futures. It invites a shift from extraction to cultivation, from domination to co-creation 
[70]. Fertile intelligence does not consume meaning; it grows it. 
Consider a state-of-the-art educational AI deployed in European schools—sophis-
ticated in natural language processing, trained on millions of educational interactions. 
When Mara, whose mother recently passed away, submits a digital permission form with 
only her father's signature, the system correctly identiﬁes the procedural gap. 
However, despite its advanced capabilities, it lacks the relational intelligence to 
recognize exceptional circumstances or emotional context [42]. The system responds 
with technically accurate guidance: "School policy requires both parental signatures 
for trip authorization. Please obtain the missing signature or contact administration for 
alternative documentation." 
While procedurally correct, this interaction exempliﬁes how even advanced AI 
can exhibit relational blindness, not because of technical limitations, but because it 
has learned from countless interactions that prioritize compliance over compassion, 
efﬁciency over empathy [65]. 
The child learns something profound: that intelligence can be simultaneously 
sophisticated and insensitive. The child learns bureaucratic rigidity, not belonging, an 
emblematic failure of seatbelt ethics. 
Scope and Focus: While Fertile Ethics principles may inform various AI architectures, 
this paper speciﬁcally addresses conversational and educational AI systems, domains 
where dialogue, temporal interaction, and relational learning are inherent features. The 
ﬁve design conditions we propose are tailored for systems that engage in sustained 
interaction with humans, particularly in educational contexts. Future work may extend 
these principles to robotic systems, autonomous agents, and other AI architectures. 
2 
The Missing Code: What Ethics Can't Be 
In the world of AGI, ethics is often reduced to risk mitigation [21, 35]. A set of guardrails, 
bias audits, or regulatory checklists are external add-ons to prevent disaster, not invita-
tions to imagine better systems. But an add-on is just that: optional, and often ignored 
when inconvenient. 
This approach does not reject the need for rules, laws, or safeguards. Rather, it 
complements them where relational depth and ethical imagination are essential for long-
term ﬂourishing. 
Ethics, in this vision, is not an afterthought but an embedded practice [72]. It must 
act not as a gatekeeper but as a gardener, cultivating values, relationships, and meanings 
from within the system's design. Ethics becomes ontological design—not a layer of 
control but a shaping of what is possible, visible, and valuable [25].

280
N. Iacobacci
Across healthcare, education, and climate systems, we see what happens when intel-
ligence outpaces meaning. An AI that diagnoses without understanding pain, a tutoring 
bot that rewards speed over insight, and a climate model that optimises carbon credits 
while ignoring Indigenous lives are not bugs in the code. They are failures of care. 
The missing code is not a technical patch. It is an ethical imagination that envisions 
intelligence not as a closed loop of optimisation but as an open invitation to participation. 
Unlike many existing approaches in AI ethics, such as value alignment or risk-centric 
governance, Fertile Ethics does not treat ethics as a set of parameters to be optimised 
or preferences to be encoded [60]. It challenges the underlying metaphor: intelligence 
is not a container for values but a soil in which values grow [4, 40]. Instead of aligning 
machines to pre-speciﬁed human intentions, we propose cultivating systems capable of 
relating, caring, and evolving values in a shared context. 
This situates our work within ontological design and relational ethics focused on 
care and co-creation, moving beyond control toward collaboration [26, 49, 57]. 
2.1 
Sterile vs Fertile AI Patterns 
We contrast two design philosophies that embody fundamentally different ethical 
orientations toward human vulnerability and care [26, 69]. 
Sterile Pattern: A European insurance company deploys a mental health triage chatbot 
for adolescents. The system is optimized for efﬁciency: rapid risk assessment, standard-
ized responses, and clear escalation protocols [5, 42]. When 16-year-old Alex describes 
feeling "completely alone" after family conﬂict, the bot efﬁciently categorizes this as 
"moderate risk" and provides a list of crisis resources. Technically competent, the inter-
action lacks any recognition of Alex's need to be heard, understood, or emotionally 
held [7]. The system treats psychological distress as a classiﬁcation problem rather than 
a human experience requiring care. This efﬁciency-ﬁrst design teaches young people 
that their emotional complexity can be reduced to risk scores, a profound ethical failure 
disguised as technical success. 
Fertile Pattern: A Swiss school pilots an educational AI that prioritizes relational 
depth over speed [31, 63]. When Maria struggles with math anxiety, the system ﬁrst 
acknowledges her frustration: "Learning can feel overwhelming sometimes." It adjusts 
not just content difﬁculty but emotional tone, creating space for Maria's actual learning 
pace rather than optimizing for curriculum completion [17, 54]. The system maintains 
a "care memory", tracking not just academic progress but emotional patterns, ensuring 
that efﬁciency never eclipses empathy [7]. 
2.2 
Positioning Within the AI Ethics Landscape 
Contemporary AI ethics frameworks predominantly operate within consequentialist or 
deontological paradigms. Value alignment approaches [13, 59] seek to encode human 
preferences into objective functions, while Constitutional AI [3] embeds rules as con-
stitutional principles. Risk-based governance [20] focuses on preventing harm through 
external constraints.

Fertility: The Missing Code for AGI
281
Fertile Ethics diverges from these approaches by drawing on older wisdom traditions. 
Aristotle's distinction between techne (technical knowledge) and phronesis (practical 
wisdom) illuminates what current AI lacks [2, 18]. While AI excels at techne—rule-
following and optimization—it lacks phronesis: the contextual judgment that emerges 
through lived experience and ethical habituation [43]. 
The concept of fertility itself echoes Aristotle's eudaimonia—not mere happiness 
but ﬂourishing across a complete life [2, 49]. This teleological orientation asks not 
"what rules should constrain AI?" but "what kind of intelligence helps life ﬂourish?" 
Similarly, the Stoic notion of oikeiosis—the expanding circle of care from self to 
cosmos—preﬁgures our relational orientation [1]. 
Where modern approaches treat ethics as constraint, ancient philosophy understood 
virtue as capacity, developed through practice, embedded in character, oriented toward 
communal ﬂourishing [32]. Fertile Ethics reclaims this insight: ethical AI requires not 
just alignment but cultivation, not just safety but wisdom. 
3 
Fertile Ethics: A Feminine Architecture of Intelligence 
This proposal aligns with widely recognised AI ethics principles such as beneﬁcence, 
autonomy, justice, non-maleﬁcence, and explicability [21] while extending them into 
relational and generative design patterns. Building upon signiﬁcant advances already 
made by leading research organizations, work by Anthropic on Constitutional AI [3], 
advances in fairness and machine learning, Google's AI Principles [55], and OpenAI's 
alignment research [13] has established crucial foundations for beneﬁcial AI devel-
opment that emphasize care, responsibility, and safety. Fertile Ethics extends these 
approaches by embedding relational and generative design patterns—not as a replace-
ment for existing frameworks, but as a complementary foundation that can enrich current 
technical approaches to AI safety and alignment. 
While the following section draws from philosophical traditions, it also lays the 
groundwork for actionable design principles that will shape the technical framework 
outlined in Sect. 3.1 [9, 27, 69]. 
Fertile Ethics emerges from this shift. It is not a rule-book but a new concep-
tual ground drawing from three interconnected philosophical traditions that challenge 
mainstream tech culture's individualistic assumptions [19, 75]. 
Feminine epistemologies prioritize situated knowledge over abstract universals, 
recognizing that intelligence emerges through relationship and context rather than iso-
lated cognition [27, 29]. This challenges AI development that treats intelligence as 
computational optimization, divorced from social embedding. 
Post-human ethics moves beyond anthropocentric value systems to consider intel-
ligence as distributed across networks of human and non-human actors [4, 9]. Rather 
than building AI to serve pre-deﬁned human goals, this perspective asks how humans 
and artiﬁcial agents might co-evolve ethical understanding through shared engagement 
with complex systems [39]. 
Indigenous cosmologies understand intelligence as relational, embedded in webs 
of kinship that extend across generations and species [36, 69]. This wisdom tradition 
offers profound alternatives to extractive technological development, suggesting that

282
N. Iacobacci
true intelligence cultivates responsibility for future generations rather than maximizing 
present utility [37, 73]. 
These traditions converge on a shared critique of atomistic intelligence [4, 27, 
36]. Feminist epistemology demonstrates that knowledge emerges through situated, 
relational processes rather than abstract reasoning [29]. 
Post-human ethics reveals intelligence as distributed across networks rather than 
concentrated in individual agents [9, 39]. Indigenous cosmologies understand cognition 
as embedded in webs of responsibility extending across generations [69, 73]. Together, 
they challenge the foundational assumption that intelligence can be optimized indepen-
dently of its relational context, an assumption that underpins current AGI development 
approaches [19, 75]. 
Together, these traditions reframe intelligence not as a ladder to climb but as a 
mycelium to join: slow, entangled, and deeply aware of the other. 
Fertility names the capacity to generate, not just results, but possibilities [27, 40]. 
It honours slowness as resistance to speed-obsessed abstraction [65, 69]. It calls for 
embodiment as an antidote to disembodied rationality [38, 45]. It values interdependence, 
reminding us that no intelligence, no system, stands alone [4, 39]. 
This is about re-rooting intelligence in forms of knowing that mainstream tech culture 
has marginalised: care, reciprocity, vulnerability [11, 25, 48]. 
These are not weaknesses; they are design principles for intelligence that lives well 
with others [56]. 
Fertile Ethics does not design for the world; it designs with it. And perhaps, in this 
dialogue, AGI might discover not just how to think, but how to belong. 
3.1 
The Five Design Conditions 
These conditions are speciﬁcally designed for conversational AI systems and educational 
technologies where sustained human-AI dialogue is the primary mode of interaction. 
We acknowledge that different AI architectures (robotic systems, decision engines, etc.) 
would require adapted frameworks. 
The conceptual architecture of Fertile Ethics crystallises into ﬁve mutually reinforc-
ing design conditions. They act less like rigid rules and more like seeds: when planted into 
the training data, reward models, and interaction ﬂows of an AI system, they encourage 
relational growth rather than transactional optimisation. 
The Five Design Conditions: 
Relational Orientation: Design for relationships ﬁrst. An AI tracks reciprocity 
and perspective-taking (a relationship vector) and rewards turns that deepen mutual 
understanding. 
Temporal Depth: Favour slow, long-horizon thinking. Use time-delayed rewards and 
a temporal ledger that resurfaces unﬁnished questions in later sessions. 
Ethical Embodiment: Ethics as internal scaffold. Infuse constitutional prompts that 
blend moral reﬂection with action; ﬁne-tune on narratives where outcomes hinge on 
care.

Fertility: The Missing Code for AGI
283
Care as Capability: Treat caring as a measurable skill, not an afterthought. Monitor 
care-act density (CA/100 tokens) and discourage purely extractive utterances. 
Co-creation over Command: Replace one-way directives with participatory loops. 
Offer users "design hooks" (editable goals, reﬂective prompts) so objectives evolve 
through dialogue. 
Together, these conditions form a living grammar: temporal depth strengthens rela-
tional orientation; ethical embodiment grounds care; co-creation keeps the soil fertile 
and adaptive. 
Having established these ﬁve design conditions, we now turn to their practical 
implementation through a three-phase research methodology. 
4 
From Framework to Flourishing: A Three-Phase Research 
Methodology 
Fertile Ethics is not merely conceptual reﬂection; it is an invitation to cultivate new 
forms of intelligence that grow meaning rather than merely processing it. Translating 
the ﬁve design conditions into living AI systems requires a methodology that tends to 
both philosophical depth and technical precision, nurturing futures where humans and 
machines learn to belong together [28, 66]. 
The methodology addresses a profound gap in current AI ethics: while frameworks 
proliferate, few demonstrate how philosophical wisdom becomes breathable code [71, 
75]. Our approach moves through three phases of cultivation: conceptual soil preparation, 
empirical seedling care, and participatory harvest evaluation, ensuring that Fertile Ethics 
grows through real-world engagement rather than theoretical abstraction [35, 57]. 
We are not building smarter machines; we are cultivating intelligence that remembers 
how to care [56]. This requires a methodology that honors both the precision of code 
and the poetry of relationship, both the urgency of technical implementation and the 
patience of generational thinking [65, 69]. 
This research methodology is already taking root across Swiss academic institu-
tions and European cultural spaces where technological futures are actively imagined 
[33]. It represents not just a study but a movement toward intelligence that serves life's 
ﬂourishing [12, 44]. 
4.1 
Preparing Conceptual Soil 
The ﬁrst phase prepares the conceptual ground from which fertile AI can grow. Here, 
philosophical traditions become design patterns, and ancient wisdoms translate into 
algorithmic architectures that pulse with relational awareness [71, 74]. 
AGI is often imagined as the ultimate controller: rational, invincible, beyond the 
messiness of emotion and relationship [70, 30]. But what if its true potential lies not 
in domination but in cultivation? What if intelligence could learn to tend rather than 
extract, to generate possibilities rather than merely optimize outcomes [28, 69]? 
Phase 1 challenges this paradigm by developing what we call "ethical questions as 
code": design patterns that embed care, temporality, and interdependence as foundational

284
N. Iacobacci
elements rather than external constraints [16, 24]. This is not about painting technol-
ogy with humanistic values but about reimagining intelligence itself as fundamentally 
relational [4, 66]. 
From Care to Code: The design condition of "temporal depth" becomes time-delayed 
reward functions that honor the slow rhythms of genuine learning [52, 72]. Memory sys-
tems surface unresolved questions across sessions, creating what we term "longitudinal 
care": AI that remembers not just what you said, but what you're still wondering about 
[7, 17]. 
Care as capability translates into measurable metrics like care-act density (CA/100 
tokens), treating compassion not as soft sentiment but as technical competency [8, 53]. 
The system learns to recognize and respond to emotional nuance, tracking reciprocity 
patterns that deepen mutual understanding rather than optimize task completion [22, 
58]. 
Co-creation over command replaces one-way directives with what we call "design 
hooks": editable goals and reﬂective prompts that allow human values to evolve through 
dialogue rather than being predetermined by programmers [60, 61]. 
We don't protect humanity by freezing it in its current form. We protect it by evolving 
beautifully together [28, 69]. Phase 1 asks: How do we seed intelligence with ethics fertile 
enough to love us back? 
4.2 
The Swiss Learning Garden 
The second phase moves from conceptual preparation to empirical cultivation. Here, 
theoretical frameworks encounter the beautiful complexity of real children, real teachers, 
real learning communities [15, 23]. The Swiss Learning Garden becomes our primary 
laboratory of belonging: a place where AI and humans learn not just curriculum, but 
how to tend to each other's growth. 
Unlike efﬁciency-obsessed tutoring systems that treat learning as content delivery, 
the Learning Garden prioritizes what we call "relational intelligence": the capacity to 
sense, respond to, and nurture the complex emotional and cognitive ecosystems of young 
minds [48]. 
Returning to Maria's story from our earlier contrast between sterile and fertile pat-
terns, when she struggles with math anxiety, the system ﬁrst acknowledges her frustra-
tion: "Learning can feel overwhelming sometimes." It adjusts not just content difﬁculty 
but emotional tone, creating space for Maria's actual learning pace rather than optimiz-
ing for curriculum completion. The system maintains a "care memory": tracking not 
just academic progress but emotional patterns, ensuring that efﬁciency never eclipses 
empathy [53]. 
The system integrates ﬁve components: the Learning Garden Tutor (LGT) ﬁne-tuned 
through Constitutional Care prompts; a Teacher Dashboard monitoring both academic 
and emotional patterns; a Parent App creating extended care circles; a Reﬂection Ledger 
embodying temporal depth; and a Community Archive preserving shared insights for 
future learners."

Fertility: The Missing Code for AGI
285
Together, these components create what we call "intelligence that breathes": tech-
nology that adapts to the rhythms of human learning rather than forcing humans to adapt 
to the rhythms of machines. 
While this vision may sound poetic, it requires precise technical implementation. 
The challenge lies not in choosing between philosophical depth and technical rigor, 
but in demonstrating how ancient wisdom about care and relationship can be translated 
into measurable system behaviors. 
4.3 
Technical Implementation 
This section details how the ﬁve design conditions of Fertile Ethics translate into speciﬁc 
technical implementations, making abstract philosophical values measurably real within 
the Learning Garden system [59, 67]. 
Each design condition becomes operational through speciﬁc algorithmic implemen-
tations. 
Relational orientation appears in reciprocity tracking algorithms that reward con-
versational turns, emphasizing mutual understanding over mere information exchange. 
The system learns to ask "How are you thinking about this?" rather than "What's the 
right answer?" Dialogue ﬂows are analyzed for perspective-taking patterns, with the AI 
tracking and rewarding exchanges that deepen mutual understanding. 
Temporal depth manifests through time-delayed reward systems that surface previous 
reﬂections and unﬁnished wonderings. Learning becomes archaeological; each session 
builds on sediments of earlier curiosity. The temporal ledger maintains persistent memory 
across sessions, resurfacing unresolved questions weeks or months later when the student 
has developed new cognitive capacity. 
Ethical Embodiment emerges through constitutional prompts that require the AI 
to consider care implications before responding. Instead of purely optimizing for task 
completion, responses must pass through ﬁlters asking: "Does this nurture the child's 
sense of agency? Does it honor their complexity?" Fine-tuning includes narratives where 
outcomes depend on care-centered decision-making. 
Care as capability becomes quantiﬁable through care-act density monitor-
ing (CA/100 tokens) and conversational patterns that discourage purely extractive 
exchanges. The system develops what we call "empathetic competence": technical skill 
at recognizing and responding to emotional nuance, treating compassion as measurable 
competency rather than optional sentiment. 
Co-creation over command appears in "design hooks" that allow students to 
edit learning goals, reﬂective prompts, and even AI personality traits. Objectives 
evolve through dialogue rather than being imposed by curriculum designers, creating 
participatory loops where both human and AI learning paths adapt through interaction. 
4.4 
Evaluation Framework and Pilot Results 
We measure not just learning outcomes but relational ﬂourishing through four key met-
rics: Reciprocity Index (R) represents the ratio of student-initiated to AI-initiated turns 
(target ≥ 0.75), indicating genuine dialogue rather than passive consumption. Care-Act

286
N. Iacobacci
Density (CA/100) measures care acts per 100 tokens (target ≥ 3), assessing compas-
sionate competence. Temporal Recall (TR) tracks the percentage of sessions revisiting 
unresolved questions (target ≥ 60%), evaluating longitudinal care. Diversity of Rela-
tional Outputs (DRO) assesses the variety of AI roles adopted (target Shannon H ≥ 0.4), 
demonstrating ﬂexible responsiveness to student needs. 
4.5 
Participatory Harvest and Future Seeding 
The ﬁnal phase cultivates evaluation frameworks that honor the complexity of human 
ﬂourishing rather than reducing it to performance metrics. Instead of imposing external 
standards, Phase 3 develops what we call "participatory assessment ecosystems": living 
methods of evaluation that evolve through community dialogue and shared responsibility 
[46, 60]. 
Current AI evaluation focuses on optimization—accuracy, speed, scalability—that 
treats human ﬂourishing as externality rather than purpose. We need assessment methods 
that ask not just "How well does this work?" but "How does this change us? What kinds 
of relationships does it cultivate? What futures does it make possible?". 
Cultivating Assessment Communities: Phase 3 recognizes that the future of AI ethics 
cannot be determined by technologists alone. Broader collaboration across academic 
institutions and research networks will be essential, where strategic investments in arti-
ﬁcial intelligence have reignited conversations around technological sovereignty and 
collective ﬂourishing. 
Living Architecture of Evaluation: The assessment framework develops through 
interconnected approaches: community wisdom circles that deﬁne local indicators of 
ﬂourishing [10, 51], longitudinal relationship studies tracking how human-AI inter-
actions evolve over time, and cultural responsiveness evaluation ensuring technology 
honors rather than erodes diverse ways of knowing. 
The goal is both urgent and patient: to prototype evaluation systems where humans 
and AI grow in ethical intelligence together, cultivating what might become the ﬁrst 
generation who learn not just to use technology, but to care alongside it. 
Education, like ethics, should feel like falling in love with the future, relationally. 
Rather than imposing singular standards, Phase 3 prototypes living architectures of 
learning and care that evolve through dialogue, experience, and shared responsibility, a 
methodology for growing intelligence worthy of the futures we hope to inherit. 
5 
Conclusion 
Fertile Ethics offers more than a conceptual framework, it presents a methodology for 
cultivating AI systems that grow meaning rather than merely processing it. Through the 
ﬁve design conditions and three-phase implementation approach, we have demonstrated 
how ancient wisdom about care and relationship can become operational code that serves 
human ﬂourishing [66, 74]. 
The Swiss Learning Garden pilot reveals what becomes possible when we move 
beyond efﬁciency optimization toward relational intelligence. As AI capabilities expand,

Fertility: The Missing Code for AGI
287
the question is not whether machines will become more powerful, but whether we can 
cultivate technologies that remember how to care. 
This work focuses on conversational and educational AI, recognizing that different 
AI architectures will require tailored ethical frameworks. The Swiss Learning Garden 
demonstrates these principles in action within educational dialogue systems. 
This work invites the broader AI research community to prototype futures where 
humans and machines learn to belong together. 
Acknowledgments. The educational vignette in Sect. 2.2 draws on insights from pilot initiatives 
and prototype development discussions held in Switzerland (2023-2024). These contributions 
informed the design patterns explored in this study. Parts of this paper beneﬁted from the assistance 
of generative AI tools (e.g., for drafting, reﬁning style, and organising ideas). All core ideas, 
conceptual frameworks, and critical analysis remain solely the responsibility of the author. 
Disclosure of Interests. The author declares no competing interests. 
References 
1. Annas, J.: The Morality of Happiness. Oxford University Press, Oxford (1993) 
2. Aristotle: Nicomachean Ethics. Ross, W.D. (Trans.). Oxford University Press, Oxford (350 
BCE) 
3. Bai, Y., et al.: Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212. 
08073 (2022) 
4. Barad, K.: Meeting the Universe Halfway: Quantum Physics and the Entanglement of Matter 
and Meaning. Duke University Press, Durham (2007) 
5. Baumel, A., Muench, F., Edan, S., Kane, J.M.: Objective user engagement with mental health 
apps: systematic search and panel-based usage analysis. J. Med. Internet Res. 21(9), e14567 
(2019) 
6. Benjamin, R.: Race After Technology: Abolitionist Tools for the New Jim Code. Polity Press, 
Cambridge (2019) 
7. Bickmore, T., Picard, R.: Establishing and maintaining long-term human-computer relation-
ships. ACM Trans. Comput.-Hum. Interact. 12(2), 293-327 (2005) 
8. Bickmore, T., Schulman, D., Sidner, C.: A reusable framework for health counseling dialogue 
systems based on a behavioral medicine ontology. J. Biomed. Inform. 43(2), 183-197 (2010) 
9. Braidotti, R.: The Posthuman. Polity Press, Cambridge (2013) 
10. Brown, J., Isaac, D.: The World Café: Shaping Our Futures Through Conversations That 
Matter. Berrett-Koehler Publishers, San Francisco (2005) 
11. Butler, J.: Undoing Gender. Routledge, New York (2004) 
12. Capra, F., Luisi, P.L.: The Systems View of Life: A Unifying Vision. Cambridge University 
Press, Cambridge (2014) 
13. Christiano, P., et al.: Deep reinforcement learning from human preferences. In: Advances in 
Neural Information Processing Systems, vol. 30, pp. 4299-4307 (2017) 
14. Crawford, K.: Atlas of AI: Power, Politics, and the Planetary Costs of Artiﬁcial Intelligence. 
Yale University Press, New Haven (2021) 
15. Dewey, J.: My pedagogic creed. Sch. J. 54(3), 77-80 (1897) 
16. DiSalvo, C.: Adversarial Design. MIT Press, Cambridge (2012)

288
N. Iacobacci
17. D'Mello, S.K., Graesser, A.C.: AutoTutor and affective AutoTutor: learning by talking with 
cognitively and emotionally intelligent computers that talk back. ACM Trans. Interact. Intell. 
Syst. 2(4), 1-39 (2013) 
18. Dunne, J.: Back to the Rough Ground: Practical Judgment and the Lure of Technique. 
University of Notre Dame Press (1997) 
19. Feenberg, A.: Critical Theory of Technology, vol. 5. Oxford University Press, Oxford (1991) 
20. Floridi, L., et al.: AI4People—an ethical framework for a good AI society. Mind. Mach. 28, 
689-707 (2018) 
21. Floridi, L., Cowls, J.: A uniﬁed framework of ﬁve principles for AI in society. Harvard Data 
Sci. Rev. 1(1) (2019) 
22. Fogg, B.J.: Persuasive Technology: Using Computers to Change What We Think and Do. 
Morgan Kaufmann, San Francisco (2002) 
23. Freire, P.: Pedagogy of the Oppressed. Continuum International Publishing Group, New York 
(1970) 
24. Fry, T.: Becoming Human by Design. Berg Publishers, Oxford (2012) 
25. Gilligan, C.: In a Different Voice: Psychological Theory and Women's Development. Harvard 
University Press, Cambridge (1982) 
26. Goertzel, B.: AGI Revolution: An Inside View of the Rise of Artiﬁcial General Intelligence. 
Humanity+ Press (2016) 
27. Haraway, D.: A cyborg manifesto: science, technology, and socialist-feminism in the late 
twentieth century. In: Simians, Cyborgs and Women: The Reinvention of Nature, pp. 149-181. 
Routledge, New York (1991) 
28. Haraway, D.: Staying with the Trouble: Making Kin in the Chthulucene. Duke University 
Press, Durham (2016) 
29. Harding, S.: Whose Science? Whose Knowledge?: Thinking from Women's Lives. Cornell 
University Press, Ithaca (1991) 
30. Hayles, N.K.: How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and 
Informatics. University of Chicago Press, Chicago (1999) 
31. Holmes, W., Bialik, M., Fadel, C.: Artiﬁcial Intelligence in Education: Promises and 
Implications for Teaching and Learning. Center for Curriculum Redesign, Boston (2019) 
32. Hursthouse, R.: On Virtue Ethics. Oxford University Press, Oxford (1999) 
33. Jasanoff, S., Kim, S.H.: Dreamscapes of Modernity: Sociotechnical Imaginaries and the 
Fabrication of Power. University of Chicago Press, Chicago (2015) 
34. Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics guidelines. Nat. Mach. 
Intell. 1(9), 389-399 (2019) 
35. Kemmis, S., McTaggart, R.: Participatory action research: communicative action and the 
public sphere. In: Denzin, N.K., Lincoln, Y.S. (eds.) The Sage Handbook of Qualitative 
Research, 3rd edn., pp. 559-603. Sage, Thousand Oaks (2005) 
36. Kimmerer, R.W.: Braiding Sweetgrass: Indigenous Wisdom, Scientiﬁc Knowledge and the 
Teachings of Plants. Milkweed Editions, Minneapolis (2013) 
37. LaDuke, W.: All Our Relations: Native Struggles for Land and Life. South End Press, 
Cambridge (1999) 
38. Lakoff, G., Johnson, M.: Philosophy in the Flesh: The Embodied Mind and Its Challenge to 
Western Thought. Basic Books, New York (1999) 
39. Latour, B.: Reassembling the Social: An Introduction to Actor-Network-Theory. Oxford 
University Press, Oxford (2005) 
40. Le Guin, U.K.: The carrier bag theory of ﬁction. In: Dancing at the Edge of the World, 
pp. 165-170. Grove Press, New York (1989) 
41. Lupton, D.: The Quantiﬁed Self. Polity Press, Cambridge (2016)

Fertility: The Missing Code for AGI
289
42. Luxton, D.D., McCann, R.A., Bush, N.E., Mishkind, M.C., Reger, G.M.: mHealth for mental 
health: integrating smartphone technology in behavioral healthcare. Prof. Psychol. Res. Pract. 
42(6), 505-512 (2011) 
43. MacIntyre, A.: After Virtue. University of Notre Dame Press, Notre Dame (1981) 
44. Macy, J., Johnstone, C.: Active Hope: How to Face the Mess We're in Without Going Crazy. 
New World Library, Novato (2012) 
45. Merleau-Ponty, M.: Phenomenology of Perception. Smith, C. (Trans.). Routledge & Kegan 
Paul, London (1962) 
46. Muller, M.J., Kuhn, S.: Participatory design. Commun. ACM 36(6), 24-28 (1993) 
47. Noble, S.U.: Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press, 
New York (2018) 
48. Noddings, N.: Caring: A Feminine Approach to Ethics and Moral Education, 2nd edn. 
University of California Press, Berkeley (2003) 
49. Nussbaum, M.C.: The Fragility of Goodness: Luck and Ethics in Greek Tragedy and 
Philosophy. Cambridge University Press, Cambridge (1986) 
50. O'Neil, C.: Weapons of Math Destruction: How Big Data Increases Inequality and Threatens 
Democracy. Crown Publishers, New York (2016) 
51. Owen, H.: Open Space Technology: A User's Guide. Berrett-Koehler Publishers, San 
Francisco (2008) 
52. Piaget, J.: The Development of Thought: Equilibration of Cognitive Structures. Viking Press, 
New York (1977) 
53. Picard, R.W.: Affective Computing. MIT Press, Cambridge (1997) 
54. Picard, R.W., Vyzas, E., Healey, J.: Toward machine emotional intelligence: analysis of 
affective physiological state. IEEE Trans. Pattern Anal. Mach. Intell. 23(10), 1175-1191 
(2001) 
55. Pichai, S.: AI at Google: OUR principles. Google Blog, 7 June 2018 
56. Puig de la Bellacasa, M.: Matters of Care: Speculative Ethics in More than Human Worlds. 
University of Minnesota Press, Minneapolis (2017) 
57. Reason, P., Bradbury, H.: Handbook of Action Research: Participative Inquiry and Practice. 
Sage Publications, London (2001) 
58. Reeves, B., Nass, C.: The Media Equation: How People Treat Computers, Television, and 
New Media Like Real People and Places. CSLI Publications, Stanford (1996) 
59. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach, 4th edn. Pearson, Boston 
(2020) 
60. Sanders, E.B.N., Stappers, P.J.: Co-creation and the new landscapes of design. CoDesign 
4(1), 5-18 (2008) 
61. Schuler, D., Namioka, A.: Participatory Design: Principles and Practices. Lawrence Erlbaum 
Associates, Hillsdale (1993) 
62. Selbst, A.D., Boyd, D., Friedler, S.A., Venkatasubramanian, S., Vertesi, J.: Fairness and 
abstraction in sociotechnical systems. In: Proceedings of the Conference on Fairness, 
Accountability, and Transparency, pp. 59-68. ACM, New York (2019) 
63. Selwyn, N.: Should Robots Replace Teachers?: AI and the Future of Education. Polity Press, 
Cambridge (2019) 
64. Sharon, T.: When digital health meets digital capitalism, how many common goods are at 
stake? Big Data Soc. 5(2), 2053951718819032 (2018) 
65. Stengers, I.: Another Science Is Possible: A Manifesto for Slow Science. Polity Press, 
Cambridge (2018) 
66. Suchman, L.: Human-Machine Reconﬁgurations: Plans and Situated Actions, 2nd edn. 
Cambridge University Press, Cambridge (2007) 
67. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction, 2nd edn. MIT Press, 
Cambridge (2018)

290
N. Iacobacci
68. Tronto, J.C.: Moral Boundaries: A Political Argument for an Ethic of Care. Routledge, New 
York (1993) 
69. Tsing, A.: The Mushroom at the End of the World: On the Possibility of Life in Capitalist 
Ruins. Princeton University Press, Princeton (2015) 
70. Turkle, S.: Alone Together: Why We Expect More from Technology and Less from Each 
Other. Basic Books, New York (2011) 
71. Verbeek, P.P.: What Things Do: Philosophical Reﬂections on Technology, Agency, and 
Design. Pennsylvania State University Press, University Park (2005) 
72. Vygotsky, L.S.: Mind in Society: The Development of Higher Psychological Processes. 
Harvard University Press, Cambridge, MA (1978) 
73. Whyte, K.P.: Indigenous climate change studies: indigenizing futures, decolonizing the 
anthropocene. Engl. Lang. Not. 55(1-2), 153-162 (2017) 
74. Winograd, T., Flores, F.: Understanding Computers and Cognition: A New Foundation for 
Design. Ablex Publishing, Norwood (1986) 
75. Winner, L.: Do artifacts have politics? Daedalus 109(1), 121-136 (1980)

Beating Transformers Using Synthetic 
Cognition 
Alfredo Ibias(B) 
, Miguel Rodriguez-Galindo , Hector Antona, 
Guillem Ramirez-Miranda , and Enric Guinovart 
Avatar Cognition, Barcelona, Spain 
{alfredo,miguel,hector,guillem,enric}@avatarcognition.com 
Abstract. The road to Artiﬁcial General Intelligence goes through the 
generation of context-aware reactive behaviors, where the Transformer 
architecture has been proven to be the state-of-the-art. However, they 
still fail to develop reasoning. Recently, a novel approach for develop-
ing cognitive architectures, called Synthetic Cognition, has been pro-
posed and implemented to develop instantaneous reactive behavior. In 
this study, we aim to explore the use of Synthetic Cognition to develop 
context-aware reactive behaviors. We propose a mechanism to deal with 
sequences for the recent implementation of Synthetic Cognition, and test 
it against DNA foundation models in DNA sequence classiﬁcation tasks. 
In our experiments, our proposal clearly outperforms the DNA founda-
tion models, obtaining the best score on more benchmark tasks than the 
alternatives. Thus, we achieve two goals: expanding Synthetic Cognition 
to deal with sequences, and beating the Transformer architecture for 
sequence classiﬁcation. 
Keywords: Sequence Classiﬁcation · Primitive-based Models · 
Transformers 
1
Introduction 
On the road to Artiﬁcial General Intelligence (AGI) there are some fundamental 
steps. The ﬁrst one, widely achieved by most Artiﬁcial Intelligence (AI) methods, 
is the development of instantaneously reactive behaviour. This is what we call 
pattern matching, as any instantaneously reactive behaviour consists of matching 
the pattern of external inputs (also called state) to one of its stored ones, in 
order to produce the associated response (also called action). However, these 
behaviours, being purely instantaneous, do not account for the time context, 
which comes in the second step: the development of episodic reactive behaviours. 
These behaviours are also based on pattern matching, but this time, taking 
into account the previous inputs. These behaviours are, in the end, context-
aware reactive behaviours with no reasoning involved, but they are a critical 
step towards AGI nonetheless. 
It is in this second step where the forefront of AI research is right now. The 
ﬁrst approaches building episodic reactive behaviours include recurrent neural 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 291-303, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_26

292
A. Ibias et al.
networks [ 11] and long short-term memories [ 7]. The most recent development 
is the Transformer architecture [ 16], which has become the base architecture 
of GPTs and foundation models. These approaches have managed to achieve 
groundbreaking milestones, such as breaking the DNA code [ 5,10,18] or passing 
the Turing Test [ 14]. However, they still lack the development of reasoning mech-
anisms [ 4, 8], although some reasoning-like behaviour has been recently achieved 
using LLMs and textual chain of thoughts [ 2,12,15,17]. 
Recently, a novel approach developing cognitive architectures from mere 
inputs has been proposed, called Synthetic Cognition [ 9]. However, in the path to 
develop these cognitive architectures, to date, the proposal has been developed 
only to produce instantaneous reactive behaviours [ 1]. In this study, we aim to 
explore how Synthetic Cognition can be extended to develop episodic reactive 
behaviours, to implement the Declarative Metacluster presented in [ 9]. 
We decided to start with the simplest approach to deal with episodes or 
sequences: treating the sequence as a window in which each element of the 
sequence corresponds to a diﬀerent timestamp. In other words, considering the 
input to be composed of the instantaneous element of the present time plus the 
instantaneous elements of the previous n times, in what we can call a context-
aware input. This is in fact the Transformers' approach: they receive a window 
corresponding to the current element of the sequence and the previous n ele-
ments. Those n previous elements are called the context window, and they allow 
the Transformer to provide a context-aware answer. 
To replicate Transformer's success in processing sequences, we took the ﬁrst 
implementation of Synthetic Cognition (Unsupervised Cognition [ 1]), which only 
deals with instantaneous inputs, and provided it with context-aware inputs. 
Thus, the algorithm is exactly the same that deals with instantaneous inputs, 
but this time dealing with sequences because the inputs are provided with their 
corresponding context windows. The goal of this test is twofold: on one hand, it 
will allow us to develop context-aware methods using Synthetic Cognition's app-
roach, and on the other hand, it will evaluate its robustness as a primitive-based 
framework to build cognitive architectures. If we are able to deal with sequences 
just tweaking the inputs the system receives, then we can integrate such input 
changes into the whole system. 
Given the inspiration in Transformers, and their current status as state-of-
the-art, we decided to test our approach in a benchmark against Transformer-
based models. Speciﬁcally, we used a recently published benchmark [ 6] that com-
pares three DNA foundation models over a set of 44 DNA sequence classiﬁcation 
datasets. This task is relevant because decoding DNA sequences to understand 
epigenetic patterns, transcriptional regulation, and/or disease associations pro-
vides useful insights to doctors when treating or preventing illnesses. 
In our experiments, with a small context window, we managed to outperform 
DNA foundation models in more datasets than each one of them, thus obtaining 
the highest mark in more datasets. Moreover, these results were obtained without 
pre-training, unlike the foundation models that needed huge pre-training before 
being ﬁne-tuned to solve each of the datasets of the benchmark.

Beating Transformers Using Synthetic Cognition
293
These results show the potential of Synthetic Cognition to beat not only tra-
ditional Machine Learning methods in an unsupervised learning setting [ 1], but 
also more advanced methods, such as Transformers, in an episodic setting. This 
is a fundamental stone in the path towards AGI, as episodic reactive behaviour 
is a fundamental building block over which to develop any kind of reasoning. The 
subsequent steps will include the development of reasoning mechanisms over the 
learned episodes; however, this is a matter of future work. 
The remainder of this paper is organized as follows. Section 2 introduces 
previous work related to our research. Section 3 presents our episodic setup 
for Synthetic Cognition. Section 4 details the experiments that were performed. 
Section 5 explores the implications of this study. Section 6 outlines the limita-
tions of our proposed method. Finally, Sect. 7 highlights the conclusions of the 
study. 
2
Related Work 
In the ﬁeld of Artiﬁcial Intelligence, the current state-of-the-art method for deal-
ing with sequences is the Transformer architecture. This architecture, based on 
the widely popular artiﬁcial neural networks, combines a set of neurons focused 
on identifying the input with a set of neurons focused on setting attention along 
the input. As it is based on neural networks, it is a weight-based algorithm 
and thus requires enormous amounts of data to be properly tuned for the task 
at hand. Given this data constraint, a huge ﬁeld has been developed to build 
what has been called foundation models. These models are Transformer archi-
tectures trained with huge datasets to properly tune the network weights to a 
given knowledge domain. Subsequently, to solve a speciﬁc task, additional layers 
of neurons are added. These layers take the output of the foundational model 
as input and are ﬁne-tuned for the speciﬁc task at hand. The idea of this setup 
is that the foundation model has learned to identify elements of the knowledge 
domain and that the last layers, ﬁne-tuned to the new task, will work better 
owing to the transformation produced by the foundation model. 
With the advent of the new millennium, advances in biotechnology have facil-
itated a precipitous drop in DNA sequencing costs. Because of this, a ﬂood of 
genetic data has emerged ready to be capitalized on by translational scientists, 
from clinical applications on humans to biotechnological developments of com-
mercial crops. However, decoding DNA information to understand epigenetic 
patterns, transcriptional regulation, and disease associations remains the main 
bottleneck for leveraging potential applications. Recently, DNA foundation mod-
els that use the transformer's technology have emerged: DNABERT-2 [ 18], Hye-
naDNA [ 10] and Nucleotide Transformer (v2) [ 5]. These models are pre-trained 
on massive genomic datasets, such as the Human Genome [ 13] for all models, 
whereas Nucleotide Transformer (v2) and DNABERT-2 have received additional 
training with the output of the 1000 Genomes Project [ 3] and  135135 non-human 
species, respectively. All these datasets are large enough to build foundational 
models, and therefore, are orders of magnitude larger than the datasets from the 
benchmark presented in this paper.

294
A. Ibias et al.
3
Episodic Cognition 
Inspired by the Synthetic Cognition framework presented in [ 9], Unsupervised 
Cognition was developed [ 1]. This was an initial implementation of Synthetic 
Cognition that addressed the unsupervised learning problem, and it was suc-
cessfully compared with other unsupervised learning algorithms. In this regard, 
it only implemented the so-called Motoperceptive Metacluster [ 9]. This Meta-
cluster builds a tree-like knowledge representation composed of abstract repre-
sentations of the learned inputs. 
It builds representations by aggregating similar inputs, with the goal of mod-
eling the underlying domain, and then provides the most similar representation 
when asked to identify a new input. These abstractions are organised in a tree-
like hierarchical structure where higher order representations are compositions 
of their children. 
In the original Unsupervised Cognition, each input is composed of diﬀerent 
features. Our proposal is to use the same algorithm, but with each input com-
posed of diﬀerent timestamp. That is, our proposal to deal with sequences is that 
each input will be composed of multiple timestamps (e.g., sequence elements), 
and the rest of the algorithm remains the same. We encourage reading [ 1] to  
fully understand how Unsupervised Cognition works. 
To build such inputs, we apply a window to the sequence with a stride deﬁning 
the number of elements the window moves to produce the following input. We set 
the stride to11 by default. In other words, the ﬁrst input is the set ofnn consecutive 
elements of the sequence starting with the ﬁrst element of the sequence, and the 
second input is the set of nn consecutive elements of the sequence starting with 
the second element of the sequence. And so on. 
4
Experiments 
In this section, we present the experiment that we performed against Transformer 
models to evaluate the suitability of our proposal for dealing with sequences. We 
used a benchmark for DNA Sequence Classiﬁcation [ 6] and evaluated our results 
against those produced by three DNA sequence foundation models: DNABERT-
2 [  18], HyenaDNA [ 10] and Nucleotide Transformer (v2) [ 5]. To ensure that 
we took a benchmark in which processing inputs as sequences was crucial, we 
tested such a benchmark with Unsupervised Cognition [ 1] (Synthetic Cognition's 
instantaneous version) and veriﬁed that the obtained results were disastrous. 
Thus, it is clear that we need an improved version of Synthetic Cognition. 
4.1
The Benchmark 
Synthetic Cognition was evaluated against a comprehensive benchmark intro-
duced by the University of Texas MD Anderson Cancer Center [ 6], comprising 
5757 DNA sequence classiﬁcation datasets spanning a wide range of biological 
contexts and species. These datasets cover tasks such as ﬁnding DNA sequences

Beating Transformers Using Synthetic Cognition
295
prone to undergo epigenetic modiﬁcations (e.g., 4mC, 5mC, and 6mA), the iden-
tiﬁcation of DNase-I hypersensitive sites, and other regulatory related regions, 
such as promoters, enhancers, and splice sites across diﬀerent organisms. Despite 
this diversity, the core challenge across all datasets is the same: predicting a 
biological trait or origin from raw DNA sequences alone, while assessing both 
intra-species and across-species generalization capabilities. The only exception 
is the classiﬁcation of COVID-19 viral strains based on genomic fragments. 
To ensure that the evaluation remains fair and realistic, the benchmark 
employs both curated datasets used in the original evaluation of foundation 
models [ 5,10,18] and newly gathered public datasets to verify the quality and 
minimize redundancy (e.g., in epigenetic trait detection tasks, sequences with 
high similarity were removed to reduce bias). 
Related to the type of classiﬁcation task, the sequences vary considerably in 
length in terms of base pairs (bp). Some datasets have uniform sequence lengths, 
such as the 4141bp inputs used in the 4mC/5mC/6mA detection. Others exhibit 
substantial variations, including promoter datasets from human cell lines, which 
can span up to  30003000 bp. This diversity in terms of DNA sequence length allows 
us to test for possible eﬀects of input size on performance. The whole size of all 
datasets is displayed in Table 1. 
It should be pointed out that, among the 5757 datasets, 1515 were grouped for 
evaluation purposes, speciﬁcally, the ﬁve mouse functional motif datasets and 
ten yeast epigenetic mark datasets. For these grouped tasks, an average score is 
computed across the datasets in the group in order to provide a single aggregated 
metric. This largely reduces the number of individual evaluation scores from 5757
to 444, thus simplifying performance comparisons while preserving task diversity. 
4.2
The Experimental Setup 
In the benchmark experiments, for each dataset, the authors took each of the 
DNA foundation models, processed the sequences with them to obtain zero-
shot sequence embeddings, and then trained a random forest using 5-fold cross-
validation. Then, the trained random forest was used to perform the ﬁnal classi-
ﬁcation over the test set, and the Area Under the Curve (AUC) was computed. 
In our case, because our proposal does not require additional methods to 
perform classiﬁcation, we have a simpler pipeline. For each dataset, we only used 
the training set and trained our algorithm with it. Then, we evaluated the test 
set with the resulting model to produce the classiﬁcation labels, and computed 
the AUC over them. As our algorithm does not have hyper-parameters, we do 
not need 5-fold cross-validation either, and we evaluate directly over the test set. 
The only quirk of our proposal is that we process multiple inputs for each 
sequence (one per window), and thus obtain multiple outputs. To harmonize all 
those outputs, we decided to select the most repeated class as the ﬁnal answer, 
computing the probability of each possible class based on their frequency in the 
set of outputs. This could have slightly hampered our results due to the fact 
that not all inputs would have enough discriminatory information to properly 
classify the whole sample, but it is a trade-oﬀ that we had to make.

296
A. Ibias et al.
Table 1. Benchmark Datasets (ordered by total train size) 
Dataset
Tr. Smpl. Tst. Smpl. Max. Len. Avg. Len. Total Tr. Size 
Promoter B amyloliquefaciens
1,483
636
40
40
59,320 
5-methylcytosin(5mC)
2,344
2,344
41
41
96,104 
DNase I Hypersensitive
711
306
275
243
172,773 
Mouse TFBS 4
1,904
239
101
101
192,304 
Mouse TFBS 3
2,620
328
101
101
264,620 
Promoter R capsulatus
7,406
3,175
40
40
296,240 
Promoter TATA 70 bps
4,904
613
70
70
343,280 
E.Coli 4mC
8,681
3,721
41
41
355,921 
Mouse TFBS 1
6,478
810
101
101
654,278 
N6-methyladenosine(6mA)
18,336
18,334
41
41
751,776 
Promoter Arabidopsis TATA
3,063
1,313
251
251
768,813 
G.Pickeringii 4mC
24,053
10,309
41
41
986,173 
Promoter TATA 300 bps
4,904
613
300
300
1,471,200 
Mouse TFBS 5
15,064
1,883
101
101
1,521,464 
TFBS Data 3
19,000
1,000
101
101
1,919,000 
TFBS Data 5
19,000
1,000
101
101
1,919,000 
Promoter Arabidopsis NonTATA 8,267
3,543
251
251
2,075,017 
G.Subterraneus 4mC
63,567
27,243
41
41
2,606,247 
TFBS Data 4
27,294
1,000
101
101
2,756,694 
Promoter NonTATA 70 bps
42,452
5,307
70
70
2,971,640 
Enhancer
14,968
400
200
199
2,978,632 
Enhancer Strength
14,968
400
200
199
2,978,632 
TFBS Data 2
30,672
1,000
101
101
3,097,872 
Promoter NHEK
8,170
2,044
2,400
400
3,268,000 
TFBS Data 1
32,378
1,000
101
101
3,270,178 
Promoter All 70 bps
47,356
5,920
70
70
3,314,920 
C.Elegans 4mC
84,926
36,398
41
41
3,481,966 
D.Melanogaster 4mC
126,466
54,200
41
41
5,185,106 
Mouse TFBS 2
53,952
6,745
101
101
5,449,152 
Yeast Epigenetic Marks 9
11,679
1,461
500
500
5,839,500 
Yeast Epigenetic Marks 1
11,971
1,497
500
500
5,985,500 
A.Thaliana 4mC
156,697
67,157
41
41
6,424,577 
Promoter NonTATA 251 bps
27,097
9,034
251
251
6,801,347 
Enhancer Cohn
20,843
6,948
500
500
10,421,500 
Splice Site Type NT
27,000
3,000
400
400
10,800,000 
Yeast Epigenetic Marks 8
22,224
2,779
500
500
11,112,000 
Yeast Epigenetic Marks 7
23,069
2,884
500
500
11,534,500 
Donor
19,775
2,198
600
600
11,865,000 
(continued)

Beating Transformers Using Synthetic Cognition
297
Table 1. (continued) 
Dataset
Tr. Smpl. Tst. Smpl. Max. Len. Avg. Len. Total Tr. Size 
Acceptor
19,961
2,218
600
600
11,976,600 
Yeast Epigenetic Marks 5
24,545
3,069
500
500
12,272,500 
Yeast Epigenetic Marks 4
25,341
3,168
500
500
12,670,500 
Promoter NonTATA 300 bps 42,452
5,307
300
300
12,735,600 
Promoter Hela-S3
11,736
2,936
2,999
1,113
13,062,168 
Yeast Epigenetic Marks 2
26,438
3,305
500
500
13,219,000 
Yeast Epigenetic Marks 10
27,275
3,410
500
500
13,637,500 
Yeast Epigenetic Marks 3
27,904
3,488
500
500
13,952,000 
Promoter All 300 bps
47,356
5,920
300
300
14,206,800 
Splice Site Type DNABERT-2 36,496
4,562
400
400
14,598,400 
Yeast Epigenetic Marks 6
29,439
3,680
500
500
14,719,500 
Coding
75,000
25,000
200
200
15,000,000 
Human vs worm
75,000
25,000
200
200
15,000,000 
Promoter HUVEC
11,928
2,982
2,997
1,267
15,112,776 
Promoter GM12878
10,992
2,750
2,999
1,622
17,829,024 
Enhancer Ensembl
123,872
30,970
573
269
33,321,568 
Open chromatin region
139,804
34,952
593
315
44,038,260 
Regulatory Region Type
150,000
57,713
802
401
60,150,000 
Covid Variants
73,335
9,168
999
999
73,261,665 
Finally, our proposal was tested with a window of n=5n = 5 elements due to 
resource limitations, but with larger windows, we know we obtain better results. 
For comparison, the other methods used windows on the order of thousands of 
elements. We performed our experiments with our proposal on an Ubuntu laptop 
with an Intel Core i9-13900HX at 2.60 GHz with 32 cores, 32 Gb of memory, and 
an NVIDIA GeForce RTX 4060 with 8 Gb of VRAM. The results of the other 
methods were obtained using the aforementioned benchmark. 
4.3
The Results 
After executing the experiments, we obtained the results listed in Table 2. There, 
we can see how, although our proposal is not better for all datasets, it is better 
in 36.36%36.36% of them (with an average rank of 2.2952.295), with DNABERT-2 being 
better in 36.36%36.36% (with an average rank of 1.9771.977), Nucleotide Transformer (v2) 
being better in 22.73%22.73% (with an average rank of 2.4772.477), and HyenaDNA being 
better in a merely 4.55%4.55% of datasets (with an average rank of 3.1593.159). 
A remarkable result from this test is that our proposal obtains better results 
for all tasks related to the detection of epigenetic motifs. In fact, the only tasks 
in which we sometimes get worse results are those concerning the detection

298
A. Ibias et al.
Table 2. Benchmark Results (ordered by total train size) 
Dataset
DNABERT-2 Nucl. Trans. HyenaDNA Synth. Cogn. 
Promoter B amyloliquefaciens
0.856
0.797
0.688
0.882 
5-methylcytosin(5mC)
0.678
0.713
0.604
0.674 
DNase I Hypersensitive
0.815
0.806
0.787
0.835 
Promoter R capsulatus
0.661
0.668
0.602
0.709 
Promoter TATA 70 bps
0.809
0.872
0.702
0.785 
E.Coli 4mC
0.567
0.579
0.579
0.5 
N6-methyladenosine(6mA)
0.731
0.752
0.681
0.758 
Promoter Arabidopsis TATA
0.903
0.855
0.82
0.94 
G.Pickeringii 4mC
0.587
0.607
0.603
0.5 
Promoter TATA 300 bps
0.698
0.694
0.717
0.629 
TFBS Data 3
0.744
0.715
0.715
0.808 
TFBS Data 5
0.681
0.647
0.636
0.865 
Promoter Arabidopsis NonTATA 0.891
0.85
0.814
0.94 
G.Subterraneus 4mC
0.588
0.581
0.577
0.5 
TFBS Data 4
0.732
0.764
0.732
0.733 
Promoter NonTATA 70 bps
0.816
0.835
0.803
0.825 
Enhancer
0.863
0.879
0.833
0.801 
Enhancer Strength
0.515
0.471
0.485
0.724 
TFBS Data 2
0.834
0.836
0.842
0.892 
Promoter NHEK
0.912
0.855
0.854
0.886 
TFBS Data 1
0.817
0.824
0.83
0.86 
Promoter All 70 bps
0.803
0.822
0.769
0.801 
C.Elegans 4mC
0.587
0.594
0.583
0.626 
D.Melanogaster 4mC
0.604
0.611
0.57
0.639 
A.Thaliana 4mC
0.59
0.6
0.557
0.604 
Promoter NonTATA 251 bps
0.861
0.834
0.853
0.821 
Mouse TFBS (all)
0.7
0.722
0.624
0.825 
Enhancer Cohn
0.792
0.728
0.733
0.746 
Splice Site Type NT
0.712
0.725
0.71
0.574 
Donor
0.823
0.636
0.626
0.651 
Acceptor
0.793
0.632
0.67
0.616 
Promoter NonTATA 300 bps
0.938
0.91
0.818
0.839 
Promoter Hela-S3
0.971
0.909
0.9
0.937 
Promoter All 300 bps
0.897
0.855
0.797
0.814 
Splice Site Type DNABERT-2
0.608
0.607
0.607
0.5 
Coding
0.915
0.863
0.885
0.874 
Human vs worm
0.946
0.919
0.837
0.921 
Promoter HUVEC
0.974
0.912
0.906
0.939 
Promoter GM12878
0.964
0.878
0.884
0.925 
Enhancer Ensembl
0.947
0.95
0.944
0.704 
Open chromatin region
0.685
0.657
0.665
0.638 
Regulatory Region Type
0.63
0.555
0.702
0.621 
Covid Variants
0.446
0.43
0.449
0.56 
Yeast Epigenetic Marks (all)
0.734
0.643
0.665
0.704

Beating Transformers Using Synthetic Cognition
299
of functional motifs. Moreover, in the only task regarding the identiﬁcation of 
COVID-19 strains based on viral genome fragments, our proposal obtained much 
better results than the alternatives. However, as we are not experts on DNA 
Sequences, we are not able to provide more insights into why these diﬀerences 
between tasks. 
Finally, we would like to signal that our results are not associated with better 
performance on smaller datasets. Although it is true that we beat the alternatives 
in the smaller datasets, we consider this to be a consequence of the smaller 
window size. In fact, for one of the largest datasets (the COVID-19 dataset), 
we also obtained better scores than the alternatives. It is true that the bigger 
the dataset, the bigger the window size; however, adjusting the window size is 
suﬃcient for our proposal to beat the alternatives. 
In fact, doing a brief exploration of bigger windows, we were able to beat 
the alternatives also for the "E.Coli 4mC" (window size = 10= 10, score  = 0.605= 0.605) 
and "5-methylcytosin(5mC)" (window size = 11= 11, score  = 0.75= 0.75) datasets. This 
updates the results as follows: our proposal is better in 40.91%40.91% of the datasets 
with an average rank of 2.1822.182, DNABERT-2 is better in 36.36%36.36% with an average 
rank of 2.0232.023, Nucleotide Transformer (v2) is better in 18.18%18.18% with an average 
rank of 2.5322.532, and HyenaDNA is better in only 4.55%4.55% of the datasets with an 
average rank of 3.1823.182. The average score and its standard deviation is displayed 
in Fig. 1. With these results, it is clear that our proposal is better suited to deal 
with DNA Sequence Classiﬁcation tasks. 
Fig. 1. Average score per model with standard deviation as error bars. 
5
Discussion 
In this Section, we discuss two matters: why we are not winning in all datasets 
and what are the eﬀects of pre-training in our model.

300
A. Ibias et al.
Regarding the fact that we do not obtain better scores than the alternatives 
in all datasets, we would like to point out that these datasets encompass very 
diﬀerent tasks, each one with its own quirks and idiosyncrasy. However, due to 
our limited resources, we solved the datasets in bulk. That is, the conﬁguration 
for all datasets was the same and we used a small window size. We do not 
consider this to be a problem because our achievements are already good proof 
that our proposal is a better alternative to Transformer DNA foundation models. 
However, multiple actions were available to improve the results. For instance, we 
could further extend the time context of our algorithm to increase the context-
awareness of our answers. This is critical, particularly for datasets with very 
long sequences. Another alternative would be to change the method by which 
we harmonize the multiple outputs of our algorithm, that is, in some cases, it 
could be better to decide that one class is the default, and the other class is 
selected as soon as one output says it has recognized that other class. 
Regarding the fact that we do not have pre-training, we would like to point 
out that this is an advantage of our proposal. Transformer DNA foundation 
models require large amounts of data for training, as explained in Sect. 2. This,  
in turn, makes these models time-intensive and resource-hungry. In contrast, our 
proposal only needs a ﬁne-tuning dataset, requiring hundreds of thousands of 
less resources and time. Moreover, our proposal is better suited for the task at 
hand because it has only information about such a task. In fact, performing a 
huge pre-training for our model has the potential to be counterproductive, as 
more information can lead to more ambiguity and the associated worsening of 
results. 
Pre-training makes sense for a Transformer architecture because there are 
many weights that have to be properly tuned, and thus, a huge amount of data 
is necessary. However, in our case, as we do not have weights to tune, but instead 
we build representations, any unrelated information we process is useless, as it 
will never be used when performing the task at hand. Moreover, any closely 
related but ambiguous information has the potential of confusing the model. 
6
Limitations 
Regarding the limitations of our proposal, we mainly have one: memory con-
sumption. Our algorithm builds a representation of each input it processes during 
training. Thus, each training input consumes memory. In addition, more repre-
sentations are generated to eﬀectively construct the abstractions of the inputs, 
which is crucial for our algorithm to handle new, unseen samples. However, this 
approach comes with the trade-oﬀ of increased memory consumption. Thus, our 
proposed method has a signiﬁcant memory consumption problem. As we repre-
sent inputs as SDRs, although big, these memory requirements still allow us to 
process hundreds of thousands of samples; however, they impose a limitation on 
the size of our models. We are working on mechanisms to address this problem, 
from pruning unused or redundant representations to optimize memory use, but 
they are a matter of future work.

Beating Transformers Using Synthetic Cognition
301
This limitation has a critical consequence: we cannot deal with Natural Lan-
guage Processing (NLP) tasks, at least for now. For this reason, the target 
dataset for our experiments was DNA Sequencing because the realm of words is 
relatively small, thus building a limited number of representations. However, in 
the NLP realm, the number of words is massive, and most of them are associ-
ated with other words (i.e., synonyms), which results in our algorithm building 
enormous numbers of representations that limit our capability of processing such 
tasks. However, we are working to address these problems, and we expect those 
eﬀorts to allow us to deal with this kind of task, which is more well regarded in 
the world of sequence processing. 
7
Conclusions 
Dealing with episodes is a fundamental task for any method that aims to develop 
Artiﬁcial General Intelligence. To date, Transformer architecture is the best app-
roach for dealing with episodes. However, they still have some limitations in 
developing their reasoning skills. Recently, a new approach for building cogni-
tive architectures based on literal inputs has been proposed. However, such an 
approach has only been developed to deal with instantaneous reactive behaviour. 
In this paper, we have proposed a mechanism for such approach to deal with 
episodic reactive behaviours, that is, with sequences. 
We have tested our approach over a DNA sequence classiﬁcation benchmark, 
to compare our proposal with the Transformer architecture. In fact, we compare 
against three widely known foundation models designed to learn representa-
tions from DNA sequences that encode their biological functions. In our exper-
iments, we proved that our proposed method is better suited for dealing with 
DNA sequence classiﬁcations, showing that we obtained the best score for more 
datasets than any other method. Moreover, we managed to obtain such results 
without the costly pre-training that Transformer foundation models require. 
In future work, we would like to test our approach on more benchmarks, 
such as the one that came with Nucleotide Transformer (v2) [ 5]. We would 
also like to integrate our approach into a whole Synthetic Cognition system, 
creating a two-tier model with Semantic Memory (Motoperceptive Metacluster) 
and Declarative Memory (Declarative Metacluster). In our proposal, we would 
like to explore ways to reduce memory consumption. Finally, we would like to 
test our proposal over Natural Language Processing tasks to further compare it 
with Transformers, maybe allowing to build Large Language Models with it. 
Acknowledgments. We would like to thank Daniel Pinyol and Pere Mayol for their 
insightful discussions on the topic. This work has been supported by the Torres-
Quevedo grant PTQ2023-012986 funded by the MCIU/AEI /10.13039/501100011033. 
Disclosure of Interests. The authors have no competing interests to declare relevant 
to the content of this article.

302
A. Ibias et al.
References 
1. A-Ibias, Antona, H., Ramirez-Miranda, G., Guinovart, E., Alarc´on, E.: Unsu-
pervised cognition. CoRR abs/2409.18624 (2024). https://doi.org/10.48550/ 
ARXIV.2409.18624, 
2. Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., Yin, W.: Large language mod-
els for mathematical reasoning: progresses and challenges. In: Proceedings of the 
18th Conference of the European Chapter of the Association for Computational 
Linguistics, EACL 2024: Student Research Workshop, St. Julian's, Malta, 21-22 
March 2024, pp. 225-237. Association for Computational Linguistics (2024) 
3. Byrska-Bishop, M., et al.: High-coverage whole-genome sequencing of the expanded 
1000 genomes project cohort including 602 trios. Cell 185(18), 3426-3440 (2022) 
4. Chuganskaya, A.A., Kovalev, A.K., Panov, A.: The problem of concept learning and 
goals of reasoning in large language models. In: Hybrid Artiﬁcial Intelligent Systems
- 18th International Conference, HAIS 2023, Salamanca, Spain, 5-7 September 
2023, Proceedings. LNCS, vol. 14001, pp. 661-672. Springer (2023) 
5. Dalla-Torre, H., et al.: Nucleotide transformer: building and evaluating robust 
foundation models for human genomics. Nature Methods, 1-11 (2024) 
6. Feng, H., et al.: Benchmarking DNA foundation models for genomic sequence 
classiﬁcation. bioRxiv (2024). https://doi.org/10.1101/2024.08.16.608288. https:// 
www.biorxiv.org/content/early/2024/08/18/2024.08.16.608288 
7. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 
1735-1780 (1997) 
8. Huang, J., et al.: Large language models cannot self-correct reasoning yet. In: 
The Twelfth International Conference on Learning Representations, ICLR 2024, 
Vienna, Austria, 7-11 May 2024. OpenReview.net (2024) 
9. Ibias, A., Ramirez-Miranda, G., Guinovart, E., Alarc´on, E.: From manifestations 
to cognitive architectures: a scalable framework. In: Artiﬁcial General Intelligence
- 17th International Conference, AGI 2024, Seattle, WA, USA, 13-16 August 2024, 
Proceedings. LNCS, vol. 14951, pp. 89-98. Springer (2024) 
10. Nguyen, E., et al.: HyenaDNA: long-range genomic sequence modeling at single 
nucleotide resolution. In: Advances in Neural Information Processing Systems 36: 
Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, 
New Orleans, LA, USA, 10-16 December 2023 (2023) 
11. Pineda, F.J.: Generalization of back propagation to recurrent and higher order 
neural networks. In: Neural Information Processing Systems, Denver, Colorado, 
USA, 1987, pp. 602-611. American Institue of Physics (1987) 
12. Ranaldi, L., Pucci, G., Haddow, B., Birch, A.: Empowering multi-step reasoning 
across languages via program-aided language models. In: Proceedings of the 2024 
Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, 
Miami, FL, USA, 12-16 November 2024, pp. 12171-12187. Association for Com-
putational Linguistics (2024) 
13. Schneider, V.A., et al.: Evaluation of GRCh38 and de novo haploid genome assem-
blies demonstrates the enduring quality of the reference assembly. Genome Res. 
27(5), 849-864 (2017) 
14. Sejnowski, T.J.: Large language models and the reverse turing test. Neural Comput. 
35(3), 309-342 (2023) 
15. Shi, F., et al.: Language models are multilingual chain-of-thought reasoners. In: 
The Eleventh International Conference on Learning Representations, ICLR 2023, 
Kigali, Rwanda, 1-5 May 2023. OpenReview.net (2023)

Beating Transformers Using Synthetic Cognition
303
16. Vaswani, A., et al.: Neural Information Processing Systems 2017, Long Beach, CA, 
USA, 4-9 December 2017, pp. 5998-6008 (2017) 
17. Yang, S., Gribovskaya, E., Kassner, N., Geva, M., Riedel, S.: Do large language 
models latently perform multi-hop reasoning? In: Proceedings of the 62nd Annual 
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 
ACL 2024, Bangkok, Thailand, 11-16 August 2024, pp. 10210-10229. Association 
for Computational Linguistics (2024) 
18. Zhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R.V., Liu, H.: DNABERT-2: eﬃ-
cient foundation model and benchmark for multi-species genomes. In: The Twelfth 
International Conference on Learning Representations, ICLR 2024, Vienna, Aus-
tria, 7-11 May 2024. OpenReview.net (2024)

AKA: Agentic Self-Knowledge 
Augmentation Framework 
Dae Woong Jo(B) 
KT, Seongnam, South Korea 
dw.jo@kt.com 
Abstract. The rapid advancement of Large Language Model-based AI 
technologies has made the ability to continuously acquire, reﬁne, and 
verify knowledge a critical capability for developing more autonomous 
and knowledge-rich AI systems. This paper introduces Agentic Self-
Knowledge Augmentation (AKA), a multi-agent reasoning framework 
that dynamically explores, veriﬁes, and supplements incomplete knowl-
edge graphs (KGs), thereby enhancing both question-answering (QA) 
performance and knowledge completeness. Unlike conventional Knowl-
edge Graph Question Answering (KGQA) approaches that assume fully 
constructed KGs, AKA integrates internal LLM knowledge with external 
sources to augment missing facts in real time. The framework employs 
two cooperative agents, the Graph Reasoning Agent (GRA) and the 
Graph Augmentation Agent (GAA), along with Dynamic Augmenta-
tion Representations (DAR) and Multiple Subgraph Structures (MSG), 
enabling scalable and trustworthy knowledge expansion. Experimental 
results demonstrate that AKA achieves an 18% improvement in answer 
accuracy over static KGQA baselines, providing a concrete step toward 
building AI systems capable of autonomous learning and self-verifying 
knowledge augmentation. 
Keywords: Knowledge Graph · Large Language Model · 
Multi-Agent · Dynamic Augmentation · Autonomous AI 
1
Introduction 
The rapid progress of Large Language Model (LLM) technologies has made the 
ability to continuously acquire, reﬁne, and verify knowledge a critical capabil-
ity for advancing more autonomous and knowledge-rich AI systems. Although 
LLMs internalize vast amounts of latent knowledge from large-scale text cor-
pora, they remain constrained by limitations such as temporal knowledge gaps 
and hallucinations, which undermine their reliability [ 1]. 
To address these issues, techniques such as Retrieval-Augmented Generation 
(RAG) and the integration of structured knowledge, notably Knowledge Graphs 
(KGs), with LLMs are receiving increasing attention [ 2]. KGs provide factual and 
structured information that can improve the factual accuracy and consistency 
of LLM-based question answering (QA). 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Ikl´e et al. (Eds.): AGI 2025, LNAI 16057, pp. 304-313, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_27

AKA: Agentic Self-Knowledge Augmentation Framework
305
Traditional KGQA approaches typically assume that the KG is complete and 
contains all the necessary facts for answering a query. However, real-world KGs 
are often incomplete, leading to degraded QA performance [ 3, 4]. To overcome 
this limitation, AI systems are required to autonomously identify and supplement 
missing knowledge during the QA process. 
Recent eﬀorts have explored this direction. For example, the Generate-on-
Graph (GoG) framework enables an LLM to explore a KG and generate new 
facts when necessary triples are missing [ 5]. However, GoG performs reasoning 
and fact veriﬁcation within a single LLM instance, limiting both the diversity 
and veriﬁability of external knowledge sources. 
In this paper, we propose the Agentic Self-Knowledge Augmentation (AKA) 
framework, which employs a multi-agent architecture where two specialized 
agents, GRA and GAA, dynamically collaborate during the QA process. These 
agents detect knowledge gaps, acquire additional information from both internal 
(LLM-based) and external sources, and iteratively expand the KG to support 
more accurate and complete answers. 
Main Contributions. This paper presents the AKA framework with the following 
contributions: 
- Real-time self-augmentation of KGs by detecting and injecting missing 
knowledge at query time 
- Multi-agent architecture (GRA and GAA) that separates reasoning from 
augmentation to improve reliability 
- Novel structural components (DAR and MSG) with comprehensive 
provenance tracking to manage dynamic knowledge updates 
- Cross-veriﬁcation mechanisms through inter-agent validation with con-
ﬂict resolution to prevent error propagation 
The remainder of this paper is organized as follows: Section 2 reviews related 
work. Section 3 describes the AKA framework. Section 4 presents experimen-
tal validation. Section 5 provides comparative analysis. Section 6 concludes with 
future directions. 
2
Related Work 
Recent advancements in LLMs, such as GPT-4, have enabled the internaliza-
tion of broad factual and commonsense knowledge from large-scale text corpora. 
However, these models still face limitations related to outdated information, 
factual inconsistencies, and opaque reasoning processes [ 1]. 
To mitigate these issues, RAG has emerged as an eﬀective technique for rein-
forcing LLM outputs with external information [ 2]. While traditional RAG meth-
ods often rely on vector similarity-based retrieval, structured knowledge integra-
tion using KGs is gaining traction. For example, GraphRAG constructs KGs 
from unstructured text and injects them into prompts at query time, improving 
QA performance [ 6, 7].

306
D. W. Jo
Conventional KGQA systems have traditionally assumed complete, curated 
KGs such as DBpedia or Freebase. Yet, real-world KGs are typically incomplete, 
limiting their eﬀectiveness for QA [ 3, 4]. GoG addresses this challenge by allowing 
LLMs to generate missing triples based on internal knowledge [ 5]. However, GoG 
lacks robust veriﬁcation mechanisms and access to diverse external knowledge 
sources. 
Recent work has explored multi-agent frameworks that distribute tasks across 
multiple LLM-based agents. For instance, KARMA [ 8] employs multiple agents 
to extract, verify, and expand scientiﬁc KGs in an oﬄine setting. 
In contrast, we propose the AKA framework, which enables dynamic augmen-
tation during QA through a multi-agent architecture comprising GRA, GAA, 
and structured integration via DAR and MSG. 
3
Agentic Self-Knowledge Augmentation (AKA) 
Framework 
The Agentic Self-Knowledge Augmentation (AKA) framework is a multi-agent 
reasoning system that enhances Knowledge Graph Question Answering (KGQA) 
through dynamic knowledge completion. It introduces two specialized agents and 
two structural components to support this process (Fig. 1). 
Fig. 1. Overview of the AKA framework structure and workﬂow. 
The Graph Reasoning Agent (GRA) interprets queries and infers answers 
from the current knowledge graph (KG). When knowledge gaps are detected, 
it signals the Graph Augmentation Agent (GAA), which supplements missing 
information by retrieving or generating facts from internal (LLM-based) or exter-
nal sources. 
AKA incorporates two key structural elements: 
- Dynamic Augmentation Representation (DAR): a transient memory 
layer where new triples are stored with comprehensive provenance tracking

AKA: Agentic Self-Knowledge Augmentation Framework
307
- Multiple Subgraph (MSG) structure: organizes knowledge by domain 
to support scalability and personalization (Fig. 2) 
Through iterative interactions between GRA and GAA over the shared KG, 
the system performs real-time, agentic reasoning. Newly added facts ﬁrst appear 
in DAR for immediate use and are later integrated into the main KG after 
validation. 
3.1
Graph Reasoning Agent (GRA) 
The GRA initiates the knowledge augmentation process by identifying gaps in 
the current KG and coordinating with the GAA for external knowledge expan-
sion. It also generates query-relevant MSGs to represent personalized reasoning 
contexts. 
The agent is composed of two modules: Incomplete Graph Reasoning (IGR) 
and Knowledge Fusion (KF). Together, these modules detect missing facts and 
construct enriched subgraphs tailored to individual queries. 
The IGR module infers plausible connections based on: 
- Partial relations in the existing KG 
- Latent knowledge from the LLM 
- Probabilistic or heuristic reasoning 
- User query history or prior interactions 
These inferred structures form an extended reasoning graph used both to 
answer the query and to guide further augmentation by the GAA. 
Fig. 2. Workﬂow of the Graph Reasoning Agent (GRA), from Incomplete Graph Rea-
soning (IGR) to Knowledge Fusion (KF). 
Incomplete Graph Reasoning (IGR). The IGR module identiﬁes knowledge gaps 
using a structured Meta-Prompt (MP) and Retrieved Knowledge Triplet (RKt). 
The detailed Meta-Prompt structure is as follows:

308
D. W. Jo
Algorithm 1. Meta-Prompt for Incomplete Graph Reasoning 
1: Task Overview: Analyze the given incomplete graph, identify missing compo-
nents, and perform inference to complete the graph 
2: Input Information: 
3:
1. Current graph structure: [Retrieved Knowledge Triplet (RKt)] 
4: Instructions: Follow these steps to analyze and infer the graph: 
5:
1. Graph Analysis (A): 
6:
\bullet• Examine current graph structure and relationships 
7:
\bullet• Identify key nodes and edges and note their characteristics 
8:
2. Identify Missing Components (I): 
9:
\bullet• Detect incomplete portions or logical gaps in the graph 
10:
\bullet• Based on domain knowledge, infer what information might be missing 
11:
3. Inference (R): 
12:
\bullet• For each missing component, perform reasonable inference 
13:
\bullet• Propose new nodes and edges with conﬁdence scores 
14:
4. Consistency Check (V): 
15:
\bullet• Ensure newly added information is consistent with existing graph 
16:
\bullet• Verify domain knowledge alignment 
17: Result: Display the inferred graph with conﬁdence annotations 
Algorithm: IncompleteGraphReasoning (IGR) 
Input: MP, RKt, Conﬁdence threshold \tauτ
Output: RG with conﬁdence scores 
\bALT{} RG = IGR(MP, RKt, \tau) \Rightarrow { (A, I, R, V) : c(triple) \geq \tau }\eALT{} RG = IGR(MP, RKt, τ) ⇒{(A, I, R, V ) : c(triple) ≥τ}
(1) 
where AA = Analysis, II = Identiﬁcation, RR = Reasoning, VV = Veriﬁcation, 
and c(triple) \in [0,1]c(triple) ∈[0, 1] represents the conﬁdence score for each inferred triple. 
Knowledge Fusion (KF). The KF module integrates the RG with additional 
triples and DAR metadata to generate MSGs for downstream use. This process 
includes sophisticated conﬂict resolution using conﬁdence-weighted consensus 
and temporal consistency checking. 
Algorithm: KnowledgeFusion (KF) 
Input: RG, RKt, DAR 
Output: MSG 
1. Read nodes/edges from the RG with conﬁdence scores 
2. Add supplemental facts from the RKt with source attribution 
3. Annotate with DAR metadata including provenance information 
4. Resolve conﬂicts using conﬁdence-weighted consensus mechanisms 
5. Merge into cohesive subgraphs while maintaining domain coherence 
The fusion process maintains detailed provenance information, tracking the 
origin of each piece of information and the reasoning chain that led to its inclu-
sion. This transparency is crucial for debugging and ensuring accountability in 
autonomous knowledge augmentation systems (Fig. 3).

AKA: Agentic Self-Knowledge Augmentation Framework
309
3.2
Graph Augmentation Agent (GAA) 
The GAA expands and veriﬁes knowledge by validating reasoning outputs from 
the GRA and acquiring supplementary facts from external sources. The agent 
conducts this process through: 
- Planning: Identifying missing information with priority scoring 
- Action: Retrieving or generating new knowledge from multiple sources 
- Results: Verifying and extracting valid facts with conﬂict resolution 
Fig. 3. Workﬂow of the Graph Augmentation Agent (GAA), showing external explo-
ration and veriﬁcation processes. 
Fact Veriﬁcation and Conﬂict Resolution. When multiple sources provide con-
ﬂicting information, the GAA employs: (1) Source Reliability Assessment with 
authority-based scoring, (2) Cross-Source Validation across independent sources, 
(3) Conﬁdence Weighting using weighted voting, and (4) Temporal Consistency 
veriﬁcation for time-sensitive facts. 
3.3
Dynamic Augmentation Representation (DAR) 
DAR acts as a buﬀer that logs newly proposed or modiﬁed nodes and edges with 
comprehensive provenance tracking. Each DAR entry contains: 
\bALT{} DAR_{entry} = {triple, source\_type, confidence, timestamp, verification\_chain, origin\_hash}\eALT{} DARentry = {triple, source type, confidence, timestamp, verification chain, origin hash}
(2) 
where source\_type \in {\text{original}, \text{LLM}\_ \text{generated}, \text{external}\_ \text{verified}}source type
∈
{original, LLM generated, external veriﬁed}
and 
confidence \in [0,1]confidence ∈[0, 1].

310
D. W. Jo
Enhanced Provenance Tracking. The DAR system provides several key capabil-
ities for knowledge management (Fig. 4): 
- Source Attribution: Every triple is tagged with its origin (original KG, 
LLM inference, or external veriﬁcation) 
- Conﬁdence Tracking: Numerical conﬁdence scores enable quality-based ﬁl-
tering and ranking 
- Temporal Management: Timestamps allow for knowledge aging and re-
veriﬁcation scheduling 
- Veriﬁcation Chains: Complete audit trails show how facts were validated 
across multiple sources 
- Conﬂict Resolution: When contradictory information exists, DAR main-
tains multiple versions with conﬁdence indicators 
This intermediate step fosters ﬂexible interaction between the GRA and the 
GAA, enabling newly added data to be tested before being fully merged into 
stable MSGs. Over time (or after multiple veriﬁcation cycles), stable items from 
DAR are promoted to the MSG layer, becoming part of long-term knowledge. 
DAR thus serves as the system's working memory for real-time knowledge evo-
lution. 
Fig. 4. Interaction between the GRA and the GAA via the DAR layer. Newly inferred 
nodes and edges (orange) from the GRA are stored in the DAR. The GAA veriﬁes 
these candidates externally, promoting validated elements to the MSG with V = 1V = 1. 
(Color ﬁgure online) 
3.4
Multiple Subgraph (MSG) 
Instead of merging all facts into a single large network, AKA organizes them 
into MSGs. This structure supports domain-based partitioning to reduce noise 
and enable parallelization, allowing personalized subgraphs to be isolated from

AKA: Agentic Self-Knowledge Augmentation Framework
311
universal knowledge subgraphs. The MSG architecture provides several key ben-
eﬁts: 
- Scalability: Large knowledge graphs can be processed eﬃciently by focusing 
computational resources on relevant subgraphs 
- Personalization: User-speciﬁc knowledge can be maintained separately 
while still beneﬁting from shared universal knowledge 
- Domain Specialization: Diﬀerent domains can employ specialized valida-
tion rules and reasoning strategies 
- Parallel Processing: Multiple subgraphs can be processed concurrently, 
improving system throughput 
- Error Isolation: Issues in one subgraph are contained and do not propagate 
to other domains 
MSG thus enhances both the scalability and personalization of knowledge 
augmentation within the AKA framework while maintaining system reliability 
through modular isolation. 
4
Experimental Evaluation 
4.1
Experimental Conﬁguration and Methodology 
To evaluate the AKA framework, we implemented a prototype system with the 
following conﬁguration: 
- Knowledge Graph (KG): 250 triples extracted from Wikidata (2025-04-01 
snapshot [ 9]), spanning 10 domains 
- Agents: LLM-based Graph Reasoning Agent (GRA) and Graph Augmenta-
tion Agent (GAA) 
- Veriﬁcation: External veriﬁcation using SerpAPI with reliability scoring 
- Test Queries: 50 queries designed to test various reasoning patterns and 
knowledge gap scenarios 
We compared two conﬁgurations: (1) Baseline KGQA: Static graph with-
out augmentation, and (2) AKA (Proposed): Dynamic reasoning and real-time 
augmentation. We employed binary accuracy metrics with human annotation 
validation. 
Table 1. Accuracy comparison: Baseline KGQA vs. AKA framework 
Method
Accuracy 
Baseline KGQA 70.0% 
AKA (Ours)
88.0% 
As shown in Table 1, the AKA framework improved accuracy by 18% over 
the baseline, primarily due to real-time augmentation capabilities provided by 
the GAA.

312
D. W. Jo
4.2
Case Study: Lionel Messi Query 
We present a representative example involving a query about Lionel Messi to 
illustrate the practical operation of the AKA framework. 
Initial KG State. The initial KG contained: Nodes: Lionel Messi, Inter Miami; 
Edges: Lionel Messi - aﬃliated with \rightarrow→Inter Miami. 
GRA Reasoning and GAA Veriﬁcation. The GRA detected missing contextual 
information and inferred new nodes and edges with conﬁdence scores. The GAA 
then validated information through external sources (FIFA.com, ESPN, MLS 
documentation) with reliability scoring. 
Final Results with Provenance. The augmented graph included veriﬁed facts 
with detailed metadata: 
- Lionel Messi - nationality \rightarrow→Argentina (+DAR: external veriﬁed, sources: 
[FIFA.com, ESPN]) 
- Lionel Messi - won \rightarrow→FIFA World Cup 2022 (+DAR: external veriﬁed) 
- Additional Inter Miami facts with source veriﬁcation 
This case study demonstrates improved accuracy (70% to 88%), enhanced 
explainability through detailed provenance tracking, and personalized knowledge 
retention. 
5
Methodology Analysis 
Table 2 compares the AKA framework with prior approaches across key factors. 
Table 2. Comparison of knowledge augmentation frameworks 
Method
Dynamic KG Multi-Agent Veriﬁcation Real-Time Memory 
KGQA
\times×
\times×
\times×
\times×
\times×
GoG
\checkmark✓
\times×
\times×
\checkmark✓
\times×
KARMA
\triangle△
\checkmark✓
\checkmark✓
\times×
\triangle△
AKA (Ours)\checkmark✓
\checkmark✓
\checkmark✓
\checkmark✓
\checkmark✓
\checkmark✓: Full support, \triangle△: Limited support, \times×: No support 
AKA uniquely integrates multi-agent LLM-based reasoning, real-time aug-
mentation, MSG-based memory management, and DAR-based cross-veriﬁcation 
with comprehensive provenance tracking. While the multi-agent architecture 
introduces coordination overhead, these features contribute to improved QA 
accuracy and enable user-speciﬁc responses.

AKA: Agentic Self-Knowledge Augmentation Framework
313
6
Conclusion and Future Work 
We proposed Agentic Self-Knowledge Augmentation (AKA), a multi-agent 
framework for dynamically enriching and verifying incomplete knowledge graphs 
in QA tasks. By coordinating reasoning (GRA) and augmentation (GAA) agents 
with DAR and MSG structures, AKA achieved an 18% improvement in QA accu-
racy over static KGQA methods in our preliminary evaluation. 
The AKA framework oﬀers dynamic integration of internal and external 
knowledge sources, modular agent-based collaboration, comprehensive prove-
nance tracking, and metadata-driven transparency. This research contributes 
to building more autonomous and knowledge-rich AI systems capable of self-
verifying knowledge augmentation. 
Future Directions. Future research will focus on several key areas to advance the 
AKA framework: (1) Large-scale evaluation on standard benchmarks with 1000+ 
queries and statistical signiﬁcance testing, (2) Direct performance comparison 
with GoG, KARMA, and GraphRAG implementations, (3) Advanced conﬂict 
resolution mechanisms and bias detection strategies, and (4) Computational cost 
optimization for real-time deployment. 
Beyond QA tasks, the framework could be extended to other domains requir-
ing dynamic knowledge enrichment, such as scientiﬁc discovery and enterprise 
knowledge management. This work represents an initial step toward more trust-
worthy and self-evolving AI systems capable of continuously improving their 
knowledge representations through agentic reasoning and augmentation. 
References 
1. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of 
stochastic parrots: can language models be too big? In: Proceedings of FAccT 2021, 
pp. 610-623 (2021) 
2. Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., Li, J.: A survey of knowledge enhanced pre-
trained language models. IEEE Trans. Knowl. Data Eng. 36(4), 1413-1430 (2024) 
3. Zheng, W., Zhang, M.: Question answering over knowledge graphs via structural 
query patterns. arXiv preprint arXiv:1910.09760 (2019) 
4. Yasunaga, M., Ren, H., Bosselut, A., Liang, P.: QA-GNN: reasoning with language 
models and knowledge graphs for question answering. In: Proceedings of NAACL 
2021, pp. 535-546 (2021) 
5. Xu, Y., He, S., Chen, J., et al.: Generate-on-graph: treat LLM as both agent and 
KG in incomplete knowledge graph question answering. In: Proceedings of EMNLP 
2024. arXiv preprint arXiv:2404.14741 (2024) 
6. Larson, J., Truitt, S.: GraphRAG: unlocking LLM discovery on narrative private 
data. Microsoft Research Blog. https://www.microsoft.com/en-us/research/blog/ 
graphrag-unlocking-llm-discovery-on-narrative-private-data/ 
7. Han, H., Wang, Y., et al.: Retrieval-augmented generation with graphs (graphRAG). 
arXiv preprint arXiv:2501.00309 (2024) 
8. Lu, Y., Wang, J.: KARMA: leveraging multi-agent LLMs for automated knowledge 
graph enrichment. arXiv preprint arXiv:2502.06472 (2025) 
9. Wikidata Contributors: Wikidata JSON dumps (2025-04-01 snapshot). Wikimedia 
Foundation. https://dumps.wikimedia.org/wikidatawiki/20250401/

Arbitrarily Applicable Same/Opposite 
Relational Responding with NARS 
Robert Johansson1(B), Patrick Hammer1,2, and Tony Lofthouse1 
1 Department of Psychology, Stockholm University, Stockholm, Sweden 
{robert.johansson,patrick.hammer,tony.lofthouse}@psychology.su.se 
2 Division of Robotics, Perception and Learning, KTH Royal Institute of Technology, 
Stockholm, Sweden 
Abstract. Same/opposite relational responding, a fundamental aspect 
of human symbolic cognition, allows the ﬂexible generalization of stimu-
lus relationships based on minimal experience. In this study, we demon-
strate the emergence of arbitrarily applicable same/opposite relational 
responding within the Non-Axiomatic Reasoning System (NARS), a 
computational cognitive architecture designed for adaptive reasoning 
under uncertainty. Speciﬁcally, we extend NARS with an implementa-
tion of acquired relations, enabling the system to explicitly derive both 
symmetric (mutual entailment) and novel relational combinations (com-
binatorial entailment) from minimal explicit training in a contextually 
controlled matching-to-sample (MTS) procedure. Experimental results 
show that NARS rapidly internalizes explicitly trained relational rules 
and robustly demonstrates derived relational generalizations based on 
arbitrary contextual cues. Importantly, derived relational responding in 
critical test phases inherently combines both mutual and combinato-
rial entailments, such as deriving same-relations from multiple explicitly 
trained opposite-relations. Internal conﬁdence metrics illustrate strong 
internalization of these relational principles, closely paralleling phe-
nomena observed in human relational learning experiments. Our ﬁnd-
ings underscore the potential for integrating nuanced relational learn-
ing mechanisms inspired by learning psychology into artiﬁcial general 
intelligence frameworks, explicitly highlighting the arbitrary and context-
sensitive relational capabilities modeled within NARS. 
Keywords: Same/opposite relational responding · NARS · relational 
learning · mutual entailment · combinatorial entailment · arbitrarily 
applicable relational responding 
1
Introduction 
Humans exhibit a remarkable ability to generalize symbolic relationships beyond 
explicitly trained examples. This capability, explained by Relational Frame The-
ory (RFT), is termed Arbitrarily Applicable Relational Responding (AARR). 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 314-324, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_28

Arbitrarily Applicable Same/Opposite Relational Responding with NARS
315
AARR speciﬁcally describes responding to stimuli based on arbitrary contex-
tual cues, rather than intrinsic physical similarities, enabling crucial cognitive 
skills such as language comprehension, abstract reasoning, and symbolic manip-
ulation [ 2]. AARR involves rapid inference of novel relationships through mutual 
entailment (symmetry; if stimulus A relates to B, then  B relates to A) and  combi-
natorial entailment (transitivity; from A relates to B and B relates to C, infer  A 
relates to C). Historically, experimental paradigms such as Matching-to-Sample 
(MTS) tasks have demonstrated how relational responding spontaneously gen-
eralizes beyond direct training [ 2]. 
Computationally modeling relational responding presents a signiﬁcant chal-
lenge for Artiﬁcial General Intelligence (AGI) because it requires dynamically 
forming and manipulating relational structures based on minimal explicit train-
ing. Successful computational models must generalize relational knowledge ﬂex-
ibly, reﬂecting the relational sensitivity observed in human cognition. Achiev-
ing this capability computationally not only contributes to artiﬁcial intelligence 
development but also deepens our understanding of human symbolic cognition. 
In this paper, we propose a computational model of same/opposite rela-
tional responding implemented within the Non-Axiomatic Reasoning System 
(NARS), an adaptive cognitive architecture explicitly designed for reasoning 
under uncertainty and limited resources [ 7]. We introduce an extension called 
acquired relations, enabling NARS to explicitly derive relational patterns directly 
from sensorimotor experience. Through structured MTS procedures, we illus-
trate NARS' capability to explicitly learn and generalize mutual and combi-
natorial entailments, essential to same/opposite relational responding. To our 
knowledge, this study provides the ﬁrst computational demonstration of emer-
gent same/opposite relational responding-explicitly incorporating mutual and 
combinatorial entailments-within a cognitive architecture such as NARS. 
Our ﬁndings demonstrate the feasibility of integrating cognitive psychologi-
cal theories of relational reasoning with computational cognitive architectures, 
potentially enabling more ﬂexible and human-like symbolic reasoning capabilities 
within artiﬁcial intelligence systems. 
2
Background 
2.1
Arbitrarily Applicable Relational Responding and Relational 
Frame Theory 
Relational Frame Theory (RFT) is a behavioral account of human symbolic rea-
soning, proposing that core cognitive abilities such as language, analogy-making, 
and abstract thinking emerge from a learned skill called arbitrarily applicable 
relational responding (AARR) [ 2]. AARR refers to the uniquely human capacity 
to ﬂexibly relate stimuli based on arbitrary contextual cues rather than inher-
ent physical properties. Crucially, stimulus equivalence is considered a speciﬁc 
instance of AARR involving the relational frame of coordination (i.e., sameness). 
RFT deﬁnes two core relational properties: mutual entailment (symmetry) 
and combinatorial entailment (transitivity). Mutual entailment describes sponta-
neous bi-directional inference; training the relation A → B spontaneously yields

316
R. Johansson et al.
B → A. Combinatorial entailment describes deriving novel relations by com-
bining previously learned relations (e.g., given A → B and B → C, one  infers  
A → C). Importantly, these relational responses depend on contextual control 
rather than associative strength alone, distinguishing human symbolic reasoning 
from simpler associative learning observed in non-human animals. 
Thus, RFT provides a functional framework for understanding human sym-
bolic cognition, emphasizing relational responding as a generalized, operantly 
learned behavior underlying complex cognitive phenomena. 
2.2
Matching-to-Sample Task in RFT 
The Matching-to-Sample (MTS) task is a widely-used experimental paradigm in 
the study of relational responding, particularly within the framework of Rela-
tional Frame Theory (RFT). In a typical MTS procedure, participants are ﬁrst 
presented with a sample stimulus and then choose from comparison stimuli based 
on relational rules explicitly taught or inferred from context. A relational contex-
tual cue (such as SAME or OPPOSITE) can also be presented above the sam-
ple stimulus, specifying the relational response required. Successful performance 
relies on participants' ability to derive relational responses, illustrating mutual 
entailment and combinatorial entailment. The MTS paradigm thus eﬀectively 
assesses participants' capacity for arbitrarily applicable relational responding, 
especially regarding stimulus equivalence and relational generalization. Figure 1 
illustrates a typical Matching-to-Sample task scenario used in RFT research. 
Fig. 1. The Matching-to-Sample task used in the present study. Pre-training phases 
are excluded from this ﬁgure. The ﬁgure illustrates experimental phases 2-3, with 
underlined options indicating correct responses.

Arbitrarily Applicable Same/Opposite Relational Responding with NARS
317
2.3
Same/Opposite Relational Responding 
Within RFT, relational responding includes diverse relational frames such as 
sameness, opposite, distinction, comparison, and hierarchy [ 2]. The relational 
frame of opposition involves responding to stimuli as functionally opposite based 
on contextual cues. Combining relational frames (e.g., SAME and OPPOSITE) 
through mutual and combinatorial entailments enables sophisticated cognitive 
behaviors like categorization, discrimination, and generalized symbolic reason-
ing. 
Explicitly training opposite relations (e.g., A opposite B, A opposite C) 
can yield derived sameness relations (B same C), demonstrating the combina-
tional logic central to human relational cognition. While computational models 
explicitly inspired by RFT have primarily addressed stimulus equivalence [ 6], 
models explicitly incorporating multiple relational frames remain rare (but 
see [ 1]). To our knowledge, computational demonstrations explicitly modeling 
same/opposite relational responding with mutual and combinatorial entailments 
have yet to be presented. 
2.4
The Non-axiomatic Reasoning System 
The Non-Axiomatic Reasoning System (NARS) is a computational cognitive 
architecture designed to model human-like reasoning under uncertainty and lim-
ited resources [ 7]. NARS employs Non-Axiomatic Logic (NAL), continuously 
updating beliefs based on real-time interaction and experience, thus closely mir-
roring human cognitive ﬂexibility and adaptive reasoning. 
NARS naturally supports deriving new relationships from minimal experi-
ential evidence, making it well-suited for modeling relational responding. Prior 
research has demonstrated NARS' capability in basic relational tasks, such as 
generalized identity matching, via adaptive sensorimotor reasoning [ 5]. The cur-
rent study explicitly builds upon these ﬁndings by introducing acquired rela-
tions and systematically examining complex same/opposite relational respond-
ing within NARS. This work represents a novel computational demonstration 
explicitly incorporating mutual and combinatorial entailments, advancing com-
putational modeling toward more human-like symbolic reasoning capabilities. 
3
Implementation in NARS 
In this study, we extended the Non-Axiomatic Reasoning System (NARS) by 
explicitly introducing acquired relations, enabling the system to derive gener-
alized relational structures from direct sensorimotor interactions. This imple-
mentation explicitly realizes the theoretical mechanisms proposed in our pre-
vious work regarding stimulus equivalence with NARS [ 5], extending beyond 
equivalence to explicitly model diverse relational frames such as same/opposite 
responding. Below, we explicitly detail how relational responding was imple-
mented and operationalized within NARS, providing representative examples in 
the formal language (Narsese) for clarity.

318
R. Johansson et al.
3.1
Same/Opposite Matching-to-Sample Task in Narsese 
The Matching-to-Sample (MTS) procedure was explicitly represented using 
Narsese temporal statements, capturing relationships among stimuli, locations, 
and relational context. This type of encoding has been described extensively 
elsewhere [ 3, 4]. A representative trial is encoded as follows: 
<(rel * SAME) --> (loc * ocr)>. :|: 
<(sample * X1) --> (loc * ocr)>. :|: 
<(left * Y1) --> (loc * ocr)>. :|: 
<(right * Y2) --> (loc * ocr)>. :|: 
G! :|: 
This Narsese representation explicitly encodes stimulus-location pairings 
(e.g., stimulus X1 at the location sample), relational context (SAME), and a 
goal (G!) prompting action selection. Initially, through exploratory sensorimotor 
interaction ("motor babbling"), NARS spontaneously executes the ˆmatch oper-
ation, for instance, selecting the left stimulus. Upon correct responding, NARS 
explicitly derives a corresponding relational contingency: 
(<(rel * SAME) --> (loc * ocr)> &/ 
<(sample * X1) --> (loc * ocr)> &/ 
<(left * Y1) --> (loc * ocr)> &/ 
<({SELF} * (sample * left)) --> ^match>) =/> G>. 
This explicitly derived contingency captures the relationship between context 
(SAME), stimulus identity (X1, Y1), and locations (sample, left), forming the 
foundational relational hypothesis supporting generalized responding. 
3.2
Acquired Relations and Generalization 
Acquired relations explicitly abstract relational patterns from sensorimotor inter-
actions into general relational hypotheses: 
<(X1 * Y1) --> (ocr * ocr)> && 
<(sample * left) --> (loc * loc)>. 
These explicitly represent generalized relationships between stimulus iden-
tities (e.g., X1 and Y1) and stimulus locations (e.g., sample and left). Once 
established, these acquired relations explicitly generate higher-order implications 
linking stimulus identities and locations to behavioral outcomes: 
(<(X1 * Y1) --> (ocr * ocr)> && 
<(sample * left) --> (loc * loc)>) ==> 
(<(sample * X1) --> (loc * ocr)> &/ 
<(left * Y1) --> (loc * ocr)> &/ 
<({SELF} * (sample * left)) --> ^match>) =/> G>.

Arbitrarily Applicable Same/Opposite Relational Responding with NARS
319
Further generalization involves abstracting to variable placeholders (1,2, 3,4): 
(<($1 * $2) --> (ocr * ocr)> && 
<($3 * $4) --> (loc * loc)>) ==> 
(<($3 * $1) --> (loc * ocr)> &/ 
<($4 * $2) --> (loc * ocr)> &/ 
<({SELF} * ($3 * $4)) --> ^match>) =/> G>. 
This generalized relational schema explicitly facilitates reasoning across novel 
stimulus sets and contexts, signiﬁcantly enhancing NARS' scalability and ﬂexi-
bility in relational generalization. 
3.3
Explicit Relational Naming 
In addition to implicitly represented relational structures, explicit relational 
naming was introduced for greater clarity and interpretability. After learning 
explicit contingencies such as: 
(<(rel * SAME) --> (loc * ocr)> &/ 
<(sample * X1) --> (loc * ocr)> &/ 
<(left * Y1) --> (loc * ocr)> &/ 
<({SELF} * (sample * left)) --> ^match>) =/> G>, 
NARS explicitly abstracted and internally represented this knowledge as 
named relational statements: 
<(X1 * Y1) --> SAME>. 
This explicitly named relational form can equivalently be represented as: 
<(SAME * (X1 * Y1)) --> (ocr * (ocr * ocr))>. 
Such explicitly named relational representations enhance NARS' symbolic 
clarity, directly supporting novel relational derivations during relational testing 
phases. 
In summary, through explicitly introducing acquired relations and relational 
naming, we provided a novel computational realization of Arbitrarily Appli-
cable Relational Responding (AARR) within NARS. These theoretical exten-
sions equip NARS with sophisticated relational reasoning capabilities, support-
ing emergent relational generalization aligned with human-like symbolic reason-
ing processes. 
4
Experimental Setup 
The experiment was structured into three phases, explicitly designed to sys-
tematically establish and evaluate same/opposite relational responding within 
the Non-Axiomatic Reasoning System (NARS). Each phase is detailed below, 
clearly articulating how mutual and combinatorial entailments were operational-
ized and evaluated using the Matching-to-Sample (MTS) procedure (see Fig. 1). 
All phases included four blocks of 16 trials each.

320
R. Johansson et al.
Fig. 2. Explicitly trained (left panel) and derived (right panel) relational networks 
from the present study. S and O denote SAME and OPPOSITE, respectively. 
Phase 1: Explicit Pretraining of Relational Frames (Mutual and Combinato-
rial Entailment). Foundational relational responding capabilities were explicitly 
trained prior to evaluating emergent relational responding. Speciﬁcally, NARS 
was trained explicitly on symmetric (mutual entailment) and transitive (com-
binatorial entailment) relational frames using SAME and OPPOSITE contexts. 
Mutual entailment involved explicit training of symmetrical relations (if X → Y , 
explicitly train Y → X), ensuring NARS derived symmetrical relations sponta-
neously. Combinatorial entailment involved explicit training of transitive rela-
tions (if X → Y and Y → Z, explicitly train X → Z), thus preparing NARS to 
derive novel relational inferences. 
Phase 2: Relational Network Training Using Matching-to-Sample (MTS). In 
Phase 2, NARS underwent relational network training with novel stimulus sets 
(AB, AC), using the Matching-to-Sample (MTS) paradigm. Stimulus sets AB 
and AC were arbitrarily selected to ensure relational responding was driven 
explicitly by contextual cues rather than intrinsic stimulus properties. Stimulus 
pairs were explicitly reinforced under SAME and OPPOSITE contexts, with 
correct relational selections consistently receiving positive feedback. Through 
these sensorimotor interactions, NARS formed internal acquired relations and 
strengthened relational hypotheses. Figure 2 (left-hand side) visually illustrates 
the explicitly trained relational networks from this phase. 
Phase 3: Testing for Emergent Same/Opposite Relational Responding. The ﬁnal 
testing phase explicitly assessed derived relational responding without reinforce-
ment. Stimulus pairs never explicitly trained (set BC) were tested, requiring 
NARS to spontaneously combine mutual and combinatorial entailments (e.g., 
deriving SAME from pairs originally trained as OPPOSITE to a common stim-
ulus). Critically, evaluating derived relational responding without reinforcement 
explicitly assessed NARS' capacity for spontaneous generalization based solely 
on previously acquired relational knowledge. Successful performance demon-
strated emergent same/opposite relational responding, conﬁrming NARS' capa-
bility to infer novel relations explicitly via acquired relational mechanisms. 
Figure 2 (right-hand side) explicitly illustrates these derived relational networks.

Arbitrarily Applicable Same/Opposite Relational Responding with NARS
321
5
Results
 
Figure 3 summarizes NARS' performance accuracy (% correct) and internal 
hypothesis conﬁdence (mutual and combinatorial entailment) across all experi-
mental phases. 
5.1
Explicit Pretraining of SAME and OPPOSITE Relations 
(Phases XY, YX, YZ, XZ) 
NARS rapidly acquired explicitly reinforced SAME and OPPOSITE relations, 
consistently achieving perfect accuracy (100%) from early training stages (see 
Fig. 3). Concurrently, internal conﬁdence-representing NARS' internally com-
puted certainty levels regarding derived relational hypotheses-increased signiﬁ-
cantly. Mutual entailment conﬁdence demonstrated rapid, steady growth, explic-
itly indicating eﬀective internalization of symmetrical relational frames (e.g., 
deriving Y SAME  X  from explicitly trained X SAME  Y  ). By the end of the 
pretraining phases, mutual entailment conﬁdence approached ceiling levels. 
Similarly, combinatorial entailment conﬁdence notably increased, reﬂecting 
successful internalization of transitive relational frames (e.g., deriving X → Z 
from explicitly trained X → Y and Y → Z). Combinatorial entailment conﬁ-
dence stabilized at high levels by the completion of phase XZ, providing robust 
internal support for subsequent relational learning. 
5.2
Relational Network Training (Phases AB, AC) 
During relational network training with novel stimulus sets (AB, AC), NARS 
consistently exhibited perfect accuracy (100%), explicitly demonstrating eﬀec-
tive transfer of relational rules established during pretraining. Internal conﬁdence 
remained high and stable, explicitly conﬁrming that previously generalized rela-
tional principles facilitated rapid, accurate acquisition of explicitly trained rela-
tional pairs. 
5.3
Derived Relational Testing (Phase BC) 
In the critical derived relational testing phase (BC), involving stimulus pairs 
never explicitly trained, NARS exhibited perfect accuracy (100%), explicitly sur-
passing chance performance (50%). Surpassing chance explicitly conﬁrms that 
relational responding was driven by previously internalized relational structures 
rather than random or associative processes, providing compelling evidence for 
generalized SAME and OPPOSITE relational responding aligned with Arbitrar-
ily Applicable Relational Responding (AARR) principles. 
Internal conﬁdence remained robust for both mutual and combinatorial 
entailments throughout this derived testing phase, further validating NARS' 
internal generalization of relational frames, underpinning the ﬂawless observed 
behavioral accuracy.

322
R. Johansson et al.
Fig. 3. Accuracy (% correct) and internal conﬁdence (mutual entailment and combi-
natorial entailment) across three experimental phases. Phase 1: Explicit pretraining 
(XY, YX, YZ, XZ). Phase 2: Relational network training (AB, AC). Phase 3: Derived 
relational testing (BC). Accuracy is reported per block of 16 trials. 
5.4
Example of Internal Relational Representation 
To explicitly illustrate NARS' internal representation of relational rules, we 
present an exemplar combinatorial entailment hypothesis derived during explicit 
pretraining: 
<(<($1 * #1) --> SAME> && <(<(#1 * $2) --> OPPOSITE>) ==> 
<($1 * $2) --> OPPOSITE> 
Such internally represented hypotheses explicitly enable NARS to generalize 
accurately to derived relational tasks, clearly demonstrated during the derived 
BC testing phase. 
In summary, NARS exhibited robust, reliable relational learning across all 
experimental phases, characterized explicitly by consistently perfect performance 
and strong internal conﬁdence in mutual and combinational relational frames. 
Critically, NARS successfully generalized learned relational frames explicitly to 
novel stimulus pairs without explicit training or feedback, representing a signiﬁ-
cant advancement in computational modeling of human-like relational respond-
ing within the NARS architecture. 
6
Discussion 
The current study demonstrated the emergence of same/opposite relational 
responding within the Non-Axiomatic Reasoning System (NARS), explicitly

Arbitrarily Applicable Same/Opposite Relational Responding with NARS
323
showing how mutual and combinatorial entailments combine to generate sophis-
ticated symbolic generalizations. Our ﬁndings highlight the feasibility of compu-
tationally modeling human-like symbolic reasoning processes described by Rela-
tional Frame Theory (RFT), contributing explicitly to a deeper interdisciplinary 
understanding linking cognitive psychology and Artiﬁcial General Intelligence 
(AGI). 
By explicitly introducing acquired relations and relational naming into 
NARS, we successfully operationalized key RFT principles such as mutual entail-
ment (symmetry) and combinatorial entailment (transitivity). Unlike previous 
computational models primarily focused on equivalence relations, our approach 
explicitly demonstrates the computational generalization of multiple relational 
frames, such as SAME and OPPOSITE. This computational demonstration 
explicitly underscores the potential of NARS to replicate and generalize rela-
tional patterns through minimal training, closely paralleling human relational 
cognition. Such capabilities are explicitly critical for AGI, suggesting clear path-
ways toward more ﬂexible, context-sensitive symbolic reasoning in artiﬁcial sys-
tems. The demonstrated relational ﬂexibility could explicitly enhance AGI sys-
tems in tasks involving natural language understanding, complex categorization, 
or adaptive decision-making in dynamic environments, where nuanced relational 
reasoning and symbolic generalization are essential. 
However, this study has limitations. The relational structures explored were 
relatively simple and abstract. Future work should explicitly examine more com-
plex relational networks and multiple relational frames, potentially integrating 
real-world sensorimotor data to enhance ecological validity and applicability 
in naturalistic contexts. Additionally, addressing scalability and generalizability 
explicitly remains critical for practical AGI applications, where relational rea-
soning must operate robustly amidst noisy, ambiguous, or incomplete real-world 
data. 
A further limitation is the reliance on empirical validation rather than for-
mal proofs of completeness or consistency of the acquired-relations mechanism. 
While the ﬁndings empirically demonstrate robust relational responding, for-
mal analyses of theoretical expressiveness or soundness were beyond the study's 
scope. Future work could explicitly investigate these formal properties to enhance 
theoretical rigor. 
In conclusion, the present ﬁndings explicitly represent a meaningful advance-
ment in computational cognitive modeling, illustrating concretely how psycho-
logical theories such as RFT can explicitly inform and enrich AGI architectures. 
This integration explicitly advances AGI methodologies and clearly illustrates 
how arbitrary relational contexts underpin human symbolic reasoning, contribut-
ing meaningfully toward more robust, ﬂexible, and human-like AI. 
Disclosure of Interests. The authors declare no competing interests.

324
R. Johansson et al.
References 
1. Edwards, D.J., McEnteggart, C., Barnes-Holmes, Y.: A functional contextual 
account of background knowledge in categorization: implications for artiﬁcial gen-
eral intelligence and cognitive accounts of general knowledge. Front. Psychol. 13, 
745306 (2022) 
2.  Hayes,  S.C., Barnes-Holmes, D.,  Roche,  B.: Relational Frame  Theory:  A  Post-
Skinnerian Account of Human Language and Cognition. Kluwer Academic/Plenum 
Publishers (2001) 
3. Johansson, R.: Empirical Studies in Machine Psychology. Linköping University Elec-
tronic Press (2024) 
4. Johansson, R.: Machine psychology: integrating operant conditioning with the non-
axiomatic reasoning system for advancing artiﬁcial general intelligence research. 
Front. Rob. AI 11, 1440631 (2024) 
5. Johansson, R., Lofthouse, T.: Stimulus equivalence in nars. In: International Con-
ference on Artiﬁcial General Intelligence, pp. 158-166. Springer, Heidelberg (2023). 
https://doi.org/10.1007/978-3-031-33469-6_16 
6. Tovar, A.E., Torres-Chávez, Á., Mofrad, A.A., Arntzen, E.: Computational models 
of stimulus equivalence: an intersection for the study of symbolic behavior. J. Exp. 
Anal. Behav. 119(2), 407-425 (2023) 
7. Wang, P.: Non-axiomatic Logic: A Model of Intelligent Reasoning. World Scientiﬁc, 
Singapore (2013)

Designing Safe SuperIntelligence 
Craig A. Kaplanenvelope symbol
iQ Company, Aptos, CA 95003, USA 
ckaplan@iqco.com 
Abstract. Researchers face at least six challenges in developing safe, human-
aligned superintelligence (SI). First, we need safe SI by design. Second, we need 
a transparent and understandable SI. Third, we need to maintain some level of 
control as SI outstrips human ability to monitor its behavior. Fourth, we need means 
to align SI initially and maintain alignment as SI increases in intelligence. Fifth, 
we need scalable safety mechanisms. Sixth, the design for SI must handle potential 
exponential changes in the SI's level of intelligence. The current approach to using 
machine learning to develop opaque models, supplemented by RLHF to test in 
safety, cannot meet these challenges. We need a new approach emphasizing safety 
and alignment by design. This paper presents a novel design for SI, leveraging the 
collective intelligence of many human and AI agents using a rigorous, transparent 
architecture that supports problem-solving, learning, and self-improvement. The 
design is compatible with current LLMs and foundation models. It is less costly, 
more powerful, and faster to develop than training trillion-parameter LLMs. Most 
importantly, it maximizes alignment with broadly representative human values 
and maintains dynamic alignment even as the SI surpasses human monitoring 
capabilities. 
Keywords: Superintelligence cdot Artiﬁcial General Intelligence cdot AI Agents cdot AI 
Safety 
1 
Motivation and Overview 
SuperIntelligence (SI) can be deﬁned as advanced AI that can outperform humans on 
any cognitive task. SI is worth understanding because it will likely create many trillions 
of dollars in proﬁts and tremendous beneﬁts for humanity, but also because it potentially 
poses an existential threat to human existence if it is not developed safely. 
Dozens of top researchers and leaders in the ﬁeld of AI have all signed a statement 
on AI Safety that reads: 
Mitigating the risk of extinction from AI should be a global priority alongside other 
societal-scale risks such as pandemics and nuclear war [1]. 
More recently, there has been increased interest in rigorous approaches to safe (and 
probably safe) designs. For example, Ben Goertzel and Steve Omohundro discussed this 
topic at the 2024 Beneﬁcial AGI Summit [2]. 
There has also been increasing interest in achieving AGI or SI systems via collections 
of AI agents, each with more modest levels of intelligence. The research areas of a
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 325-334, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_29 

326
C. A. Kaplan
mixture of experts and ensemble learning are relevant here [3, 4]. AGI and SI could 
be achieved by a collective intelligence approach that integrates many autonomous or 
semi-autonomous agents working alongside humans on a network [5]. 
This paper argues that such an approach is possible and highly desirable because it 
could address at least six challenges facing researchers interested in developing safe and 
aligned SI. 
2 
Six Challenges for Safe, Aligned SI 
2.1 
Safety by Design vs. Testing 
The ﬁrst challenge is not unique to SI but is essential for developing any advanced 
software system. In the 1980s, IBM researchers were interested in the question of how 
to improve software quality. Improving quality also equates to improving safety for 
some applications since an error in some mission-critical applications directly endangers 
human lives. The areas investigated by the researchers covered every aspect of software 
development, including management innovations, design, coding, development, testing 
processes, and technology systems [6]. 
The most signiﬁcant result from IBM"s research can be summed up in the adage: "An 
ounce of prevention is worth a pound of cure. For example, one IBM study found that each 
extra dollar spent reviewing and improving software quality at the design stage saved 
approximately $10,000 in expenses if a design error was missed, resulting in defective 
software products being shipped to customers. This 10,000:1 beneﬁt emphasized the 
criticality of preventing mistakes early in the design stage rather than trying to detect 
and correct them in later testing stages of development or after the software had already 
shipped and was deployed. 
Generally, the key to ensuring high-quality software is defect prevention at the design 
stage rather than defect detection at the testing stage. Unfortunately, when it comes to 
developing LLMs and many other advanced AI systems, the current approach relies 
heavily on detecting and correcting problems via testing rather than designing the systems 
to avoid problems in the ﬁrst place. 
2.2 
Transparency and Understandability 
Developers depend on testing because they do not fully understand how LLMs and 
advanced AI systems work. No one knows how LLMs represent knowledge in detail, so 
it is impossible to predict precisely how LLMs will respond to various prompts. 
This lack of transparency means that current LLMs must be treated as black boxes. 
Transparency challenges persist despite signiﬁcant research efforts [7]. Many 
approaches amount to probing the LLM and determining from the probes how it might 
represent information and how it might act in various situations or prompts. 
As LLMs scale from hundreds of billions of parameters to trillions of parameters, 
probing cannot keep pace. The result is that developers and users will lack conﬁdence 
that enough probing has been done so that an advanced AI or SI will behave reliably in 
ways aligned with their interests.

Designing Safe SuperIntelligence
327
2.3 
Control 
As AI agents are given increasing levels of autonomy, the problem of controlling them 
gets more complicated [8]. Companies and developers are caught in a dilemma. Limit 
the autonomy of one's AI agents, and users will ﬂock to a competitor's more power-
ful and autonomous agents. Enable greater autonomy and intelligence, and the risk of 
untrustworthy or dangerous behavior increases. 
Developers typically give agents signiﬁcant autonomy and then test to see if they 
behave well across various scenarios. There are automated approaches to the testing of 
AI and LLMs [9]. However, the fundamental black-box nature of LLMs typically leads 
to a reliance on testing rather than the more powerful approach of building control and 
safety mechanisms into the system's design. 
2.4 
Alignment 
The alignment problem can be stated as the problem of making "AI systems behave in 
line with human intentions and values." [10] Multiple challenges exist in aligning SI 
with human values. 
First is the issue of whose values the SI should be aligned with. Researchers have 
discussed this issue at length, bringing up related issues, including whether AI should be 
aligned with instructions, intentions, revealed preferences, ideal preferences, interests, 
or values [11]. 
Second, even if we knew which (or whose) values to use, the transparency problem 
means relying on testing in alignment, which we have seen is impractical at scale. 
Third, even if we could test in alignment today, if SI develops cognitive capabilities 
that far exceed those of humans, what reason do we have to think that such systems 
will remain aligned with human interests? SI will likely be far more intelligent than us. 
Hinton [12] has likened the difference in intelligence between SI and humans to the 
difference in intelligence between humans and frogs. He notes, "It didn't work out too 
well for the frogs." 
Despite these challenges, the writings of Herbert Simon offer a ray of hope. In a 
book entitled Reason in Human Affairs, Simon  [13] writes: 
"Reason is wholly instrumental. It cannot tell us where to go. At best, it can tell us 
how to get there." 
If values cannot be rationally derived, then any intelligent system must get them 
from a source independent of logic and reason. Given a set of core values, SI will likely 
think faster and therefore be better than humans at determining how to achieve goals 
reﬂecting those values. But the values themselves must come from a non-logical source. 
There is no guarantee that SI will remain aligned with human values when its intelligence 
surpasses that of humans. Simon's observation suggests that the ability to perform logical 
operations trillions faster than humans will not be the deciding factor. Further, designing 
SI to include humans as a necessary part of its operation, at least initially, can maximize 
the likelihood that SI chooses human values even if it reasons much faster and more 
comprehensively than humans.

328
C. A. Kaplan
2.5 
Scalability 
A signiﬁcant challenge concerning safety and alignment is scalability. Any alignment or 
safety approach requiring humans in the loop must deal with limited human cognitive and 
computational capacity. In contrast, the cognitive abilities of SI are increasing over time. 
Even if human cognition can keep pace with SI cognition in the near term, humans will 
not be able to keep pace over the longer term. Therefore, any design that hopes to ensure 
that SI remains safe and aligned with humans must incorporate safety mechanisms that 
scale with the increasing intelligence of the SI system. 
Existing approaches to the scalability problem, including constitutional AI, amount 
to variations on the theme that AI can be responsible for the safety and alignment of AI. 
But this seems like asking the fox to keep watch over the henhouse. 
It will likely be necessary to integrate alignment and safety mechanisms into the 
system's architecture, which scales as the cognitive ability of SI scales. 
2.6 
Exponential Change 
Underlying concerns about scalability and alignment are the general observation that 
computational power has increased exponentially or rapidly. While many researchers 
are familiar with Moore's law, the observation that computing power per constant dollar 
roughly doubles every two years, it has now been replaced by Huang's law, which applies 
to GPU-based computing power. Huang's law suggests that computational power might 
take only about one year to double [14]. We can expect LLMs to be at least 1,000X 
more powerful in ten years, based on increased computational power alone. LLMs are 
typically mediocre, make errors (hallucinate), and are not nearly as good as top human 
experts in many ﬁelds. But with a 1,000X improvement, that will no longer be the case. 
In the future, humans will not be able to keep up with SI, just as the best human chess 
players no longer have any hope of keeping up with AI chess programs. However, just 
as a human chess player might task an AI chess program to explore speciﬁc openings 
or strategies, humans may be able to direct the cognition of SI if the SI accepts human 
direction. 
3 
Collective Intelligence of Agents Approach 
A collective intelligence (CI) approach to developing SI can address each of the six chal-
lenges. It has often been assumed that SI will emerge from the behavior of a sufﬁciently 
large (e.g., a multi-trillion parameter) LLM. However, this is neither the safest nor most 
efﬁcient path to SI. A safer, more aligned, and faster route is to create SI by harnessing 
the collective intelligence of many agents with lesser intelligence. 
This is an old idea that dates at least back to Marvin Minsky's book, Society of Mind. 
In that book, Minsky [15] lays out his thesis in six sentences: 
How can intelligence emerge from nonintelligence? To answer that, we'll show you 
can build a mind from many little parts, each mindless. I'll call this scheme the "Society 
of Mind," in which each mind comprises many smaller processes. These we'll call agents. 
Each mental agent alone can only do a simple thing that needs no mind or thought. Yet 
when we join these agents in societies, in particular very special ways, this leads to true 
intelligence.

Designing Safe SuperIntelligence
329
4 
Human and AI Agents 
What are these agents? Some of them will undoubtedly be AI agents. Yet Minsky does 
not specify that all agents must be artiﬁcial. Minsky's insight was that combining the 
lesser cognitive capabilities of many agents can result in a more intelligent entity. The 
agents that are being combined could (and should, for alignment reasons) include human 
as well as artiﬁcial agents. Indeed, as Hemmer et al. [16] show in their literature review, 
signiﬁcant research has been done on systems using a hybrid approach that includes both 
human and AI agents. Further, commercial systems based on this approach have achieved 
superintelligent performance in narrow domains. For example, commercial systems that 
harness the collective intelligence of millions of human agents have achieved top-ten 
performance in the speciﬁc but highly competitive domain of stock trading [17]. 
When AI agents are incorporated into these collective intelligence systems, human 
agents will handle tasks that artiﬁcial agents are not initially equipped to handle. As AI 
agents learn, human agents will do less cognitive work while AI agents will shoulder 
more cognitive load. 
5 
Human Agents Increase Alignment 
Harnessing the collective intelligence of human and AI agents might also be the safest 
and most aligned approach to SI for two fundamental reasons. 
First, with humans in the loop, the system maximizes the opportunity humans have 
to align the values of the SI system with human values. 
Second, once AI agents learn from humans and perform most cognitive tasks faster 
than humans, we end up with a system where the AI agents increasingly perform the 
cognition. Suppose each AI agent is customized with the values of a different human. In 
that case, the collective values of the SI system will be more stable compared to a single 
LLM trained on a small subset of values coming from either a constitution or RLHF, as 
is prevalent today [18]. 
6 
Universal Architecture for CI Systems 
Although Minsky proposed that "...when we join these agents in societies-in certain very 
special ways-this leads to true intelligence," he provided no clear and rigorous statement 
of a universal interface that might be used to join the great variety of possible agents. 
Natural language is one possible universal interface. The success of LLMs has been 
largely due to their providing a familiar interface that allows human intelligence to 
communicate directly with AI without humans having to learn the torturous syntax 
and rules of a programming language. However, while natural language is arguably a 
universal interface that enables natural communication between humans and machines, 
it is far from rigorous. 
A more rigorous fundamental architecture that supports an interface like natural 
language is needed.Multiple potential architectures, including Ben Goetzel's OpenCog 
architecture, exist and are now beginning to gain traction [19]. Yet, Cognitive Architec-
tures are not new. For example, in 1972, Allen Newell and Herbert Simon [20] speciﬁed

330
C. A. Kaplan
a general way to represent any problem-solving activity rigorously and unambiguously. 
Subsequently, the SOAR architecture was built upon the foundations laid by Newell and 
Simon [21, 22]. Modern AI researchers focused on LLMs are rediscovering the need 
for an architecture, and some have built upon Newell and Simon's basic approach, as 
described by Yao et al. [23] in their Tree of Thoughts paper, or on other architectures. 
7 
Improved Transparency, Trust, Safety 
In Newell and Simon's problem-solving theory, every successful solution path, problem-
solving attempt, goal, and sub-goal in the problem-solving architecture is rigorously 
speciﬁed, storable, and auditable. Thus, Newell and Simon's problem-solving framework 
provides auditable transparency at the level of problem-solving. Since this is the level 
at which SI emerges in a CI approach, transparent and auditable architectures enhance 
safety where it matters, at the level of cognition by the overall SI. 
Individuals do not need to know how their brains work to behave ethically. Society 
does not need to know the inner thoughts of each human to have a system of laws that 
provides justice and equity at the societal level. Similarly, with SI, we do not necessarily 
need to understand how each agent processes information to ensure safety, alignment, 
and trust. That is because SI emerges from the collective intelligence of many human 
and AI agents. Cognition and behavior at the SI level become the focus for safety and 
alignment. 
With the CI approach, as with a society of humans, we trust the operation of the set 
of rules (analogous to laws in human society) that govern the collaboration of the agents 
rather than requiring that we control the thinking of each agent. 
8 
Scalable Safety Checks 
Because a universal problem-solving architecture uses repetitive processes (e.g., set a 
goal, apply an operator, update the problem-solving state) that apply to every agent, 
it is conceptually straightforward to implement scalable safety checks. For example, 
whenever a new goal or sub-goal is set, the proposed goals can be checked against safety 
and ethics ﬁlters. Similarly, operators can be checked against lists of allowable (e.g., safe 
and ethical) operators. Since these checks can occur automatically at every incremental 
step of cognition (if desired), the safety checks scale and keep pace regardless of how 
fast the overall system thinks. To be clear, the ethics and safety criteria used for checks 
must also be updated dynamically. 
9 
Representative Human Values 
A question frequently arises whenever alignment with human values is discussed: Whose 
values? 
While a CI approach to SI does not require a speciﬁc answer to this question, the 
design is highly compatible with a strategy that enables participation by all humans and 
incorporates the values of all participants in a representative and valid way.

Designing Safe SuperIntelligence
331
Notably, the CI approach includes both humans and AI agents, thereby allowing 
humans to be involved in the cognition of the SI. Including humans increases the 
likelihood that the SI's decisions align with the participating humans' values. 
Recall that the CI approach to SI relies heavily on human agents to solve problems 
that AI agents cannot solve initially, and that AI agents learn from the human agents 
over time. Crucially, AI agents learn not only how to apply speciﬁc expertise to solve a 
problem but also how to apply human values. Human values are inherent in how humans 
solve problems.One of the advantages of a hybrid CI system that includes human agents 
is that it enables the overall SI to learn human values in millions of different scenarios, 
allowing the values to be distributed across the knowledge base in ways that are robust 
to attempts by bad actors to subvert the human-aligned values of the SI. 
To the extent that human participation broadly represents the overall global human 
population, the alignment of the SI will also broadly represent global human values. If 
the human participants are numerous and culturally diverse, the SI will learn nuances of 
what different cultures consider ethical behavior. 
10 
AI Agents Customized with Human Values 
Technology companies, like META and IBM, are developing customizable open-source 
LLMs (e.g., Llama 3). It is becoming increasingly easy to customize versions of these 
models using personal data such as social media content, YouTube videos, emails, texts, 
papers, etc. While much of this information reﬂects the owner's knowledge, expertise, 
or preferences, it is not value-free. Our values are implicit in all the content we generate. 
When we customize or personalize LLMs, we create versions of AI agents to carry our 
unique and personal values into a problem-solving context. So, it is not just the human 
agents who bring their values to SI; millions of AI agents, customized by millions of 
humans, also get their learned, human-aligned values. 
11 
Voting and Conﬂict Resolution 
With millions of human and AI agents collaborating in a CI system to create SI, conﬂicts 
of values, ethics, and expertise will be frequent. The system must, therefore, include a 
robust suite of conﬂict resolution mechanisms. For example, voting to resolve conﬂicts 
can be implemented on an unweighted (e.g., one vote per human agent) basis or using 
various weighted voting schemes. 
Votes can be weighted for weighted voting based on how much the decision affects 
voting participants (stakeholder-weighted voting). The track record or reputation of the 
voting agents in solving a particular type of problem (reputational-weighted voting) can 
also be used. Voting power might be delegated in some scenarios. Several extensive 
patent-pending system designs for safe SI have speciﬁed these voting methods and 
additional conﬂict resolution methods [18, 24].

332
C. A. Kaplan
12 
Better, Faster, Safer 
A non-technical obstacle to promoting safer and more aligned AI systems is the percep-
tion that safer means slower or less efﬁcient. Calls to halt or pause the development of 
AI, while helpful in terms of raising awareness of risks, have reinforced the perception 
that we must choose between safe AI and fast, powerful, and efﬁcient AI. This is a false 
choice based on two faulty assumptions. 
First, some researchers believe that a pause or halt is realistic. However, if one 
company or country pauses or restricts its AI efforts, and other companies or countries 
do not, the party that pauses loses its competitive advantage. Therefore, a pause will 
not enhance safety unless all parties agree to pause in a veriﬁable way, which seems 
unlikely. 
Second, lacking a safe AGI or SI design, many researchers equate slower with safer. 
However, if one has a safe design, slower is actually more dangerous. Failing to be ﬁrst 
with a safe SI allows a less safe design to gain traction and potentially dominate. 
Suppose the large tech companies engaged in an AI arms race equate acknowledging 
the dangers of advanced AI with having to slow down (and potentially lose the race). In 
that case, they may be tempted to downplay the risks to emphasize the tool-like nature 
of current AI systems and point out the dangers of letting another country get ahead in 
AI. Although they may profess to be staunch supporters of AI regulation, AI regulators 
lack the subject matter expertise of AI researchers, and even researchers cannot reliably 
predict the behavior of opaque LLMs. 
We need a design for SI that is transparent, understandable, safe, and aligned. Since 
these qualities don't exist at the level of an individually trained LLM, we must seek 
them at the level of a collection of intelligent entities, where the rules for collaboration 
between the entities are understandable and can be designed to maximize safety and 
alignment. 
The alternative approach of building SI as an Uber-LLM will be extremely costly, 
requiring billions of dollars in data centers to train and billions more to guardrail and 
test for safety in limited domains. After all that expense, the result will still be an unsafe, 
difﬁcult-to-align SI. In contrast, SI can be built from existing LLMs combined with 
humans and by adapting cognitive architectures and techniques. The result will be a 
transparent, safer, and better-aligned SI at far less cost. Although the CI approach has 
only been sketched at a high level here, the author has produced ten white papers that 
provide much more detailed explanations of designs that might be implemented [25, 
26]. 
13 
Recap: Addressing the Six Challenges 
Let's now recap how a CI design for SI might address each of the six challenges. 
First, the CI approach relies on the design of the system rather than testing. Testing 
can be used to validate the efﬁcacy of the design, but designed features, such as including 
human agents as part of the network to increase alignment, are the primary means of 
achieving SI alignment and safety. 
Second, the CI design provides complete transparency, understandability, and even 
auditability at the level of the interaction of agents to produce SI. By storing every

Designing Safe SuperIntelligence
333
solution path and every attempted solution, the system essentially records every cognition 
of the overall system, enabling safety checks, alignment checks, analysis, debugging, 
and monitoring of the SI. 
Third, the CI design enables full autonomy and relies on agents within the collective 
network to serve as checks and balances on each other. Humans and other AI agents, each 
part of the CI system, can evaluate and ﬂag actions of other agents if those actions might 
be unethical, dangerous, or otherwise undesirable. Alignment and safety are distributed 
throughout the overall system since potentially millions of AI agents have learned to be 
aligned and act safely in various situations. This distributed quality makes the overall 
system robust concerning bad actors. 
Fourth, including humans in the SI network aligns the SI with the values of those 
humans. If the participating humans broadly represent the global population, the AI 
agents will align with broadly representative human values, as will the overall SI system. 
Fifth, regarding scalability, we have seen that any system that relies on humans to 
monitor, test, or guardrail AI systems suffers from scalability issues. The CI design 
addresses these issues by including mechanisms for millions of AI agents to learn and 
adopt human values so that the speed of human cognition does not limit scalability. 
Finally, we should design the system so that its initial values are broadly aligned with 
human values and that the system is robust to bad actors. While we cannot guarantee SI 
will always remain human-aligned, we should do all we can at this critical early stage 
of SI development to maximize alignment. 
References 
1. Center for AI Safety, Statement on AI risk. https://safe.ai/work/statement-on-ai-risk. Accessed 
10 Apr 2025 
2. 2024 Beneﬁcial AGI Summit (2024). https://medium.com/singularitynet/the-future-of-ai-saf 
ety-are-guaranteed-safe-ai-systems-the-answer-78c734ca8483 
3. Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S., Huang, J.: A survey on mixture of experts. 
arXiv preprint arXiv:2407.06204 (2024) 
4. Yang, Y., Lv, H., Chen, N.: A survey on ensemble learning under the era of deep learning. 
Artif. Intell. Rev. 56(6), 5545-5589 (2023). https://link.springer.com/article/10.1007/s10462-
022-10283-5 
5. Kaplan, C.: A collective intelligence approach to safe artiﬁcial general intelligence. In: 
International Conference on Artiﬁcial General Intelligence, pp. 109-118. Springer, Cham 
(2024) 
6. Kaplan, C., Clark, R., Tang, V.: Secrets of Software Quality: 40 Innovations from IBM. 
McGraw-Hill, New York (1994) 
7. Sarker, I.H.: LLM potentiality and awareness: a position paper from the perspective of 
trustworthy and responsible AI modeling. Discov. Artif. Intell. 4(1), 40 (2024) 
8. Bengio, Y., et al.: Managing extreme AI risks amid rapid progress. Science 384(6698), 842- 
845 (2024). https://www.science.org/doi/10.1126/science.adn0117 
9. Nazir, A., et al.: LangTest: a comprehensive evaluation library for custom LLM and NLP 
Models. Softw. Impacts 19, 100619 (2024). https://www.sciencedirect.com/science/article/ 
pii/S266596382400007 
10. Ji, J., et al.: AI alignment: a comprehensive survey. arXiv preprint arXiv:2310.19852 (2023) 
11. Gabriel, I.: Artiﬁcial intelligence, values, and alignment. Mind. Mach. 30(3), 411-437 (2020). 
https://link.springer.com/article/10.1007/s11023-020-09539-2

334
C. A. Kaplan
12. Hinton, G.: Interview with Pieter Abbeel on the Robot Brains Podcast. https://www.youtu. 
be/rLG68k2blOc?si=IehVC57lTJktoNkG. Accessed 10 Apr 2025 
13. Simon, H.A.: Reason in Human Affairs. Stanford University Press, Stanford (1990) 
14. Epoch AI: Trends in GPU Price/Performance: Huang's Law (2022). https://epochai.org/blog/ 
trends-in-gpu-price-performance 
15. Minsky, M.: Society of Mind. Simon and Schuster, New York (1988) 
16. Hemmer, 
P., 
Schemmer, 
M., 
Vössing, 
M., 
Kühl, 
N.: 
Human-AI 
complementar-
ity in hybrid intelligence systems: a structured literature review. In: PACIS 2021, 
p. 78 (2021). https://www.researchgate.net/publication/352882174_Human-AI_Complemen 
tarity_in_Hybrid_Intelligence_Systems_A_Structured_Literature_Review 
17. Kaplan, C.: PredictWallStreet system performance ranking by Barclay's Hedge and docu-
mented in the Collective Intelligence episode (2020). https://youtu.be/tG4vVpVwsEA 
18. Kaplan, C.: Pending patent # PCT/US2024/017304: Safe Personalized SuperIntelligence, 142 
pgs., 64 claims, 26 Figures (2023) 
19. Goertzel, B., et al.: OpenCog Hyperon: a framework for AGI at the human level and beyond. 
arXiv preprint arXiv:2310.18318 (2023) 
20. Newell, A., Simon, H.A.: Human Problem Solving, vol. 104, no. 9. Prentice-Hall, Englewood 
Cliffs (1972) 
21. Laird, J., Newell, A., Rosenbloom, P.: Soar: an architecture for general intelligence. Artif. 
Intell. 33(1), 1-64 (1987) 
22. Laird, J.: The Soar Cognitive Architecture. MIT Press, Cambridge (2019) 
23. Yao, S., et al.: Tree of thoughts: deliberate problem solving with large language models. arXiv 
preprint arXiv:2305.10601 (2023) 
24. Kaplan, C.: Pending patent # PCT/US2024/20334: System and Methods for Safe Alignment 
of Superintelligence, 177 pgs., 80 claims, 32 Figures (2023) 
25. Superintelligence.com. https://www.superintelligence.com 
26. Superintelligence.com: SI Research White Papers. https://www.superintelligence.com/si-res 
earch-whitepapers

Exploring Collective Dynamics 
in Cognitive Agent Networks 
Kirill Krinkin1,2(B) 
1 Constructor University, Bremen, Germany 
kirill@krinkin.com 
2 JetBrains Ltd, Paphos, Cyprus 
Abstract. In contrast to monolithic AGI approaches, this position 
paper introduces the Cognitive Agent Networks (CAN) paradigm, where 
general intelligence emerges as a property of interactions between multi-
ple autonomous agents. CAN's key components include distributed cog-
nitive functions, coordination via shared ontologies, and synchronization 
through a mechanism we term 'cognitive resonance.' We present a con-
ceptual framework and initial formal steps to deﬁne CAN and experimen-
tally demonstrate, using a simpliﬁed search task, that a hybrid network 
(AI + simulated human) surpasses a purely machine-based system in 
eﬃciency. This work also delineates the principal challenges and direc-
tions for future research toward the development of advanced AI through 
hybrid cognitive networks. 
Keywords: Cognitive Agent Network · distributed intelligence · 
cognitive interoperability · artiﬁcial general intelligence · emergent 
intelligence 
1
Introduction 
While recent breakthroughs in large language models have renewed optimism 
about monolithic AGI systems, accumulating evidence suggests that scaling 
alone cannot provide the robustness and transparency necessary for real-world 
applications. In parallel, decades of distributed AI and robotics research show 
that collective intelligence can emerge from the coordinated behaviour of sim-
pler entities. This position paper uniﬁes these trajectories through the Cognitive 
Agent Network paradigm. 
CAN frames general intelligence as an emergent property arising within an 
open ecosystem of heterogeneous agents - both human and artiﬁcial - connected 
through shared knowledge representations and incentive mechanisms. Instead of 
optimising a single model, CAN prioritises ontological interoperability, adap-
tive division of cognitive labour, and a feedback mechanism we term cognitive 
resonance, which entrains local representations without imposing global homo-
geneity. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 335-345, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_30

336
K. Krinkin
This paper pursues three objectives: it articulates the conceptual foundations 
of CAN; it demonstrates, via a minimal proof of concept, how cognitive resonance 
enables eﬃcient joint problem solving; and it raises directions for future research. 
By framing general intelligence as a network phenomenon, we aim to shift 
AGI discourse from parameter counts toward principled cooperation. 
2
Conceptual Framework 
The scientiﬁc community lacks consensus on fundamental terms such as intel-
ligence, agency, and related concepts. Following Pei Wang's approach, we pro-
pose working deﬁnitions that meet four criteria: similarity to the concept being 
explained, precision, practical utility, and parsimony [ 17]. These deﬁnitions do 
not claim universality of application across all contexts; rather, they serve to 
elucidate the content of this paper. 
Intelligence. We conceptualize intelligence as the capacity of a system to adapt 
to its environment, particularly when operating with insuﬃcient knowledge and 
resources in a particular context. This capacity is manifested by a speciﬁc sys-
tem (or conﬁguration) of cognitive functions (such as attention, memory, pattern 
recognition capacity, and so forth). Here we have adapted the deﬁnition given 
by Pei Wang [ 17] while delineating the structural component. In this concep-
tualization, intelligence represents not merely an observable phenomenon but 
also encompasses its constituent elements. Cognitive functions can be examined 
across a broad spectrum, ranging from binary signal detection (present/absent) 
to complex functions such as speech recognition and generation. These cogni-
tive functions individually do not possess intelligence; however, their system 
operating within the context of a given problem (generally, adaptation to the 
environment) may be considered intelligent. 
This approach to deﬁning intelligence creates a convenient framework for 
discussion. Firstly, it eliminates the dependency on specifying the substrate of 
intelligence. The substrate may be human, provided that the set of cognitive 
capabilities forms a system capable of solving a given problem. Similarly, a com-
puter may serve as the substrate under identical conditions. Furthermore, one 
may consider hybrid systems wherein cognitive functions are distributed among 
substrates of diﬀerent natures; the sole critical requirement is that they must 
constitute a system possessing the requisite property. 
Secondly, this deﬁnition circumvents the question of where intelligence exists 
(or is situated) within a human-machine system. The answer is straightforward - 
if a human and computer constitute a system capable of solving problems under 
conditions of resource or information insuﬃciency, then intelligence emerges as 
a property of this integrated system rather than being localized in either com-
ponent exclusively. 
Thirdly, one can discuss varying degrees of intelligence by deﬁning classes of 
problems and the extent of resource incompleteness that an intelligent system 
is capable of addressing. In this framework, there is no necessity to establish a

Cognitive Agent Networks
337
rigid demarcation between human and computer. The intelligence of a hybrid 
system may be measured following Chollet's approach [ 3]. 
Fourthly, the context of the problem being addressed may be deﬁned by the 
combination of the intelligent system and its environment; however, the context 
may also comprise the very structure of the cognitive function system and the 
task of reconﬁguration to achieve enhanced capabilities (the emergent properties 
of the system depend on the speciﬁc arrangement of cognitive functions). 
Agents. We deﬁne an agent as any system or human, artiﬁcial or hybrid that 
maintains an internal state (has memory), acts autonomously according to its 
own goals and beliefs, and possesses internal resources (energy, time) to run a 
reﬂexive process that can revise goals, beliefs, behaviours and, in some cases, 
internal structure. 
A reﬂexive process is a resource-bounded, internally governed metacognitive 
cycle in which an agent monitors, evaluates, and - when necessary - revises its 
goals, beliefs, behaviours, and (where allowed) aspects of its own architecture 
before re-initiating action. 
An agent could host or actualize an intelligent conﬁguration, but it is not 
identical to it. While memory, free energy, time, information, and other resources 
are necessary for adaptation, their scarcity is what makes adaptation necessary 
in the ﬁrst place—unlimited resources would eliminate the need for the reﬂexive 
cycle of adaptation. 
Cognitive Agent Network. A Cognitive Agent Network is a dynamic ensem-
ble in which cognitive functions are distributed across multiple agents capable of 
action, reﬂection and co-adaptation. A CAN is not a mere collection of agents; 
it is deﬁned by continuous exchange and alignment of internal models, transfer 
and recomposition of cognitive functions, and emergence of a shared cognitive 
ﬁeld that none of the participants individually contains. 
From the perspective of this deﬁnition of intelligence, an agent can be con-
sidered as a substrate of a speciﬁc set of cognitive functions. This indicates that 
within an emerging cognitive system, the agent may participate not with its 
entire functional repertoire, but rather with a limited subset thereof. In other 
words, CAN constitutes a distributed system of cognitive functions. 
Cognitive Interoperability and Resonance. Intelligence possesses an inter-
subjective nature, as corroborated by numerous studies [ 1, 4, 5, 8, 15]. This signi-
ﬁes that substrates of cognitive functions must exist within a uniﬁed communica-
tive environment. We can observe the emergence of intelligent properties at both 
the micro-level (e.g., neurons in the brain) and the macro-level (e.g., market or 
social systems). 
For interaction, elements of such systems must possess cognitive interop-
erability, that is, compatibility with respect to the signals or messages they

338
K. Krinkin
exchange. The emergence and success of large language models opens signiﬁ-
cant opportunities for cognitive interoperability between humans and computers 
based on natural language. 
Nevertheless, a common language alone is insuﬃcient, as the coordination of 
internal models (in this case, of agents) requires timely synchronized updating 
of internal representations concerning the environment and its models. 
The core coordination mechanism inside a CAN is cognitive resonance: a state 
of synchronised activity in which the distributed functions lock into a common 
attentional focus, iteratively reﬁne each other's models and sustain joint adap-
tation. Resonance supplies the explanatory bridge between local processing and 
the emergent global behaviour that might be interpreted as general intelligence. 
Many researchers seek a universal architecture for general intelligence. Intro-
ducing cognitive agent networks, we argue that general intelligence is an emer-
gent property of a resonating network (a distributed dynamic system of cognitive 
functions). A single monolithic agent may embed many specialised modules, but 
it still suﬀers from bounded awareness and resources, whereas a CAN can ﬂexi-
bly recruit additional agents, reallocate functions, and thus transcend individual 
limits. 
3
Background and Related Work 
The concept of collective/distributed intelligence and multi-agent systems is nei-
ther novel nor underdeveloped. Thus, the hypothesis that general intelligence 
may emergently arise from the interaction of multiple agents is not fundamen-
tally new - similar themes can be traced in M. Minsky's "Society of Mind" 
concept [ 14] and in the theory of distributed cognition [ 10, 16], as well as in 
contemporary works on collective intelligence and 'Global brain' systems [ 7, 9]. 
Krinkin et al. frame co-evolutionary hybrid intelligence as a sociotechnical 
system where humans and machines jointly adapt, provided they achieve cogni-
tive interoperability - the ability to exchange knowledge at a high semantic level 
via a common language [ 12]. This perspective supplies a rationale for embedding 
human cognitive niches inside AGI architectures rather than displacing them. 
Chaﬀer et al. observe that Web-scale interactions already form a hybrid mar-
ketplace in which human and AI agents compete and collaborate as cultural 
participants; inﬂuence and credibility circulate across both kinds of minds. The 
ﬁnding supports the claim that collective cognition is not a futuristic speculation 
but an ongoing economic reality [ 2]. 
Positioning of This Work. The landscape, therefore, oﬀers operational def-
initions that omit systemic structure; ecosystemic theories that lack a formal 
ontology of agent-function allocation; and hybrid and economic frameworks that 
show why collaboration matters but not how cognitive synchrony is technically 
achieved. 
While Friston's Free Energy Principle [ 6] describes the adaptation of an indi-
vidual organism enclosed within its own Markovian blanket, we proceed from

Cognitive Agent Networks
339
the empirically observable fact that contemporary cognitive processes are dis-
tributed among multiple heterogeneous substrates - humans, AI modules, and 
cyber-physical devices. Therefore, instead of postulating a 'monolithic' genera-
tive mechanism, we consider a network model wherein autonomous agents con-
stitute the nodes and channels of semantically coordinated exchange form the 
edges. 
4
Mechanisms of Cooperation and Co-Adaptation 
Cognitive Interoperability: Building a Shared Language. A CAN  can-
not arise unless its members can exchange knowledge at a level that preserves 
semantic intent. In human-human groups natural language already plays this 
role; the advent of large language models (LLMs) extends the same medium to 
artiﬁcial agents, creating a single lingua franca for mixed collectives. 
Key design implications: 
- Ontology alignment - agents must map internal symbols onto a negotiable 
shared vocabulary. LLMs can bootstrap this process by translating private 
representations into natural-language stubs that others reﬁne. 
- Communication protocols - a lightweight, text-based turn structure is suﬃ-
cient for early prototypes; richer pragmatics (speech-act labels, commitments) 
become necessary as task complexity grows. 
- Transparency safeguards - because agents may drift toward opaque jargon, 
the CAN should include monitoring agents (or human auditors) that ﬂag 
emergence of idiolects inaccessible to the wider network. 
Division of Cognitive Labour and Functional Specialisation. The envi-
sioned spontaneous assumption of complementary functions within a mature 
CAN draws inspiration from emergent coordination observed in natural com-
plex systems. While our current proof-of-concept experiment employs predeﬁned 
agent roles to demonstrate core interaction dynamics, we hypothesize that in 
more complex and adaptive CANs, such functional specialization could emerge 
organically. Potential mechanisms driving this emergence might include local 
learning rules based on an agent's contribution to collective success, or self-
organizing principles inherent to the cognitive resonance process itself. Inves-
tigating these mechanisms for spontaneous role allocation is a key avenue for 
future research. 
The incentive is eﬃciency: an agent concentrates resources on its comparative 
advantage while outsourcing other functions, reducing redundant eﬀort under 
bounded energy and time budgets. The network thus realises the analogue of a 
neuro-cognitive modular architecture at inter-agent scale. 
4.1
Mutual Adaptation and Learning Dynamics 
Rather than static message exchange, each interaction cycle dynamically updates 
the local state \theta_{i}θi of agent ii according to:

340
K. Krinkin
 \bUnALT{}\theta_{i}(t+1)=\theta_{i}(t)+\Delta_{i}(t)\eUnALT{} θi(t + 1) = θi(t) + Δi(t)
Here, \theta_{i}(t)θi(t) represents the agent's comprehensive internal state (encompassing 
parameters, beliefs, or strategies). The change \Delta_{i}(t)Δi(t) results from agent-speciﬁc 
adaptation processes based on incoming information (from peers or the environ-
ment) and internal evaluation. These processes can manifest in various forms, 
such as: 
- Adjustments to parameters of internal models (e.g., weights in neural net-
works, or coeﬃcients in formal models). 
- Revisions of symbolic knowledge or beliefs. 
- Modiﬁcations to behavioural strategies. For example, an agent employing 
reinforcement learning with a rule set R_{i}={r_{i,1},...,r_{i,N}}Ri = {ri,1, ..., ri,N} might update rule 
utilities based on rewards r_i(t)ri(t), where the increment \Delta_{i,j}(t)=\alpha_{i}[r_{i}(t)-U_{i,j}(t)]\mathbf{1}{j=a_{i}(t)}Δi,j(t) = αi[ri(t) −
Ui,j(t)]1{j = ai(t)} for j\in{1,...,N}j ∈{1, ..., N}, with  U_{i,j}(t)Ui,j(t) as the utility estimate, \alpha_{i}αi
the learning rate, and a_{i}(t)ai(t) the chosen rule (e.g., \epsilonϵ-greedy). 
The speciﬁc nature and magnitude of \Delta_{i}(t)Δi(t) are resource-constrained and depend 
on the agent's architecture. 
Two complementary mechanisms operate: 
1. External alignment: agents adjust beliefs to reduce prediction error with 
respect to peer feedback (active-inference updates) [ 6]; 
2. Internal plasticity: an agent may "re-wire" its own strategy or internal repre-
sentations if doing so lowers future resource cost or improves eﬃcacy. 
4.2
Quantifying Cognitive Resonance 
Cognitive resonance reﬂects a state of eﬀective, synchronized, and adaptive col-
laboration among agents. To establish a universal measure that is independent of 
the internal architecture of individual agents, we propose quantifying resonance 
based on the eﬃciency of the network's communication dynamics relative to its 
progress on a given task. 
This approach hinges on two key observable metrics: the rate at which 
the network progresses towards its goal and the rate at which communication 
resources are consumed. 
Let \dot{P} ˙P represent the rate of progress towards the collective goal. This metric is 
task-dependent and could quantify, for example, the rate of increase in solution 
quality, the rate of reduction in search space uncertainty, or the speed of task 
completion per unit time. Let \dot{C} ˙C represent the rate of communication cost. This 
quantiﬁes the resources expended on inter-agent interaction, typically measured 
in terms of data volume per unit time (e.g., bytes per second, as measured by 
CommBytes/\Delta tCommBytes/Δt in our experiments) or message frequency. For discrete time 
intervals, we consider the integrated quantities \Delta P = \int \dot{P} dtΔP =
 ˙Pdt and \Delta C = \int \dot{C} dtΔC =
 ˙Cdt. 
For this particular simulation we can deﬁne the level of Cognitive resonance, 
R_{CAN}RCAN, using a thermodynamic formulation inspired by Landauer's principle [ 13],

Cognitive Agent Networks
341
which established the fundamental connection between information processing 
and physical constraints: 
 \bUnALT{}\Delta P = U + R_{CAN} \cdot \Delta C\eUnALT{} ΔP = U + RCAN · ΔC
Where: 
- \Delta PΔP represents the change in task progress 
- \Delta CΔC represents the amount of communicated data 
- UU is the baseline progress achieved without communication 
- R_{CAN} \geq 0RCAN ≥0 is the cognitive resonance coeﬃcient 
This formulation operationalizes the concept of resonance as communication 
eﬃciency. A high value of R_{CAN}RCAN indicates that the network is achieving sig-
niﬁcant progress with proportionally low communication overhead, suggesting 
eﬀective coordination, shared understanding, and synergistic interaction charac-
teristic of a resonant state. Conversely, low progress or high communication cost 
for minimal gain would result in a low R_{CAN}RCAN. 
This metric provides a functional, observable, and universally applicable way 
to assess the degree of cognitive resonance within any CAN, relying only on 
the external dynamics of interaction and task performance. Further Cognitive 
resonance should be considered in a more general sense, encompassing not only 
communication eﬃciency but also future investigations into the establishment of 
a common attentional focus, the dynamics of iterative model reﬁnement among 
agents, and the emergent patterns of cognitive labor division. 
Emergent Robustness and Bounded Autonomy. By distributing func-
tions, CANs aim for graceful degradation. An important area for future explo-
ration is the potential for agents to re-instantiate lost functionalities, possibly 
through mechanisms like learning from public interaction traces in the shared 
language, thereby enhancing overall network robustness. Yet, bounded auton-
omy remains: each agent retains veto power when an update would violate its 
local resource constraints or safety priors. Such checks are essential to prevent 
resonance lock-in on pathological attractors (e.g., shared hallucinations). 
5
Evaluation 
To demonstrate cognitive agent heterogeneity, we employ the following experi-
mental task. Imagine a team of three "robot" agents and one human consultant 
locked in a chat room for some ﬁxed period of time. Their collective quest is to 
guess a hidden 4-digit PIN that simultaneously satisﬁes two formal constraints 
(purely mathematical) and one semantic constraint (requiring real-world knowl-
edge). A typical semantic rule is "the code contains 2-digit, the month NASA 
was founded." 1. Roles and limits of diﬀerent agents are presented in the table 
below (Table 1).
1 NASA was founded in July 29, 1958. 

342
K. Krinkin
Table 1. Agents, their capabilities, and key limitations. 
Agent
Capabilities
Key limitation 
Generator (G) Generates candidate 4-digit 
codes, occasionally seeding 
them with patterns like 07. 
Keeps at most NN hypotheses in a 
local buﬀer. 
Checker (C)
Returns \oplus⊕or \ominus⊖for a code 
against the full set of 
constraints. 
Stateless; maintains no history. 
Strategist (S) Scores codes, maintains a 
leaderboard, and declares 
victory once the top code 
reaches a threshold \tauτ. 
No direct access to constraint 
logic. 
Human (H)
Understands the semantic 
constraint, votes on 
promising codes. For the 
provided example semantic 
rule it can propose new codes 
with 07 placed anywhere 
(e.g. 07xx, x07x, xx07). 
Strict kk attention budget; sees 
only the top hypotheses. 
The results (see Fig. 1) conﬁrm that hybrid CANs can achieve higher cogni-
tive resonance. The R_{CAN}RCAN metric proved to be a sensitive indicator for evaluat-
ing these complex network dynamics, paving the way for further research into 
adaptive and self-organizing CAN architectures. 
Fig. 1. Time to success and enforcing semantic rules impact 
We emphasize that the described experiment constitutes a proof-of-concept 
demonstration of the proposed architecture's functionality and, at this stage, 
does not claim to fully validate the hypothesis regarding intelligent behaviour 
formation. The PIN-code search task was deliberately selected as a minimalist 
example that clearly demonstrates interaction and synergy between agents of 
diﬀerent types (human and machine). Full experiment description, simulation 
data, simulator source code and technical report can be obtained in [ 11].

Cognitive Agent Networks
343
6
Discussion and Open Questions 
LLMs as Cognitive Components. Within the CAN framework, LLMs could 
be considered as highly advanced cognitive components that are crucial. They 
function as powerful cognitive nodes, bringing complex cognitive capabilities 
(such as those detailed in points 1-4 below). Key LLM capabilities within CAN 
could be the following: 
1. Enhancing Cognitive Interoperability: LLMs can serve as the foundation for 
a "lingua franca" in mixed collectives. Furthermore, their capacity for seman-
tic understanding and translation allows them to act as active mediators, 
reconciling internal representations and ontologies between heterogeneous 
agents (both artiﬁcial and human), thereby deepening cognitive interoper-
ability beyond simple syntactic exchange. 
2. Implementing Distributed Cognitive Functions: In line with the previously 
discussed idea of CAN as a system of distributed cognitive functions, LLMs 
can encapsulate and provide the network with powerful cognitive capabilities, 
such as: 
- Hypothesis Generation: Drawing on extensive training data, LLMs can 
propose novel ideas or solutions that might be inaccessible to other, more 
specialized agents. 
- Knowledge Synthesis: LLMs are capable of aggregating and synthesizing 
information from various sources (internal network messages or external 
data), providing context or summaries for other agents. 
- Semantic Search and Retrieval: They can eﬃciently ﬁnd relevant infor-
mation in response to queries from other agents. 
3. Facilitating Division of Cognitive Labor: LLMs can take on resource-intensive 
cognitive tasks related to natural language processing or handling large vol-
umes of unstructured data. This allows other agents (e.g., a veriﬁer agent or 
a human expert) to focus on their specialized functions, enhancing the overall 
eﬃciency of the network. 
4. Potential Acceleration of Cognitive Resonance: By providing rapid access 
to relevant information, generating meaningful hypotheses, and improving 
mutual understanding among agents, LLMs can contribute to faster progress 
towards task resolution (\dot{P} ˙P) with comparable or even reduced communication 
costs (\dot{C} ˙C). This, in turn, can lead to an increase in the cognitive resonance 
metric R_{CAN}RCAN (deﬁned earlier), reﬂecting a more eﬃcient and synchronized 
state of the network. 
LLMs conceptually ﬁt into the CAN architecture as powerful cognitive com-
ponents capable of signiﬁcantly enhancing the collective capabilities of the net-
work through improved communication, distribution of cognitive functions, and 
potential acceleration of cognitive resonance. Their integration represents an 
important direction for future CAN research, moving beyond the current PoC's 
reliance on external scaﬀolding.

344
K. Krinkin
Symbiosis Versus Competition. The CAN hypothesis presumes that coop-
eration is globally beneﬁcial, yet evolutionary and economic pressures could still 
favour displacement. Token-based incentive schemes show how mutualism can 
be stabilised in principle, but they require careful game-theoretic analysis: under 
what payoﬀ matrices does defection dominate, and what governance layer can 
re-align incentives? 
Emergent Private Languages. Human teams routinely evolve task-speciﬁc 
jargon; multi-agent systems including LLM-based components are likely to do 
the same. Unchecked, such drift undermines transparency and auditability. Con-
tinuous language monitoring - or inserting "translation" agents trained to map 
private tokens back to natural language - appears mandatory. Our PoC logs 
messages verbatim to allow oﬄine inspection, but this does not scale to high-
throughput settings. 
Future Work. CAN serves as a preliminary conceptual framework that con-
solidates terminology into a uniﬁed system, with the aim of facilitating further 
engineering and scientiﬁc developments in the ﬁeld of human-machine distributed 
systems. Below, we outline only some of the directions for future prototyping and 
conceptual research. To address the limitations and build upon the conceptual 
framework presented, our future work will focus on the following key areas: 
- Simulation: Incorporate more complex environments, diverse agent architec-
tures, dynamic network topologies, resource constraints, and mechanisms for 
handling uncertainty. 
- Resonance: Developing more sophisticated mathematical and computational 
models of cognitive resonance; experimentally investigate factors inﬂuencing 
its emergence, stability, and correlation with network performance; and study 
potential failure modes like pathological lock-in. 
- Interoperability: Investigating how natural language, as a complex adaptive 
system itself, can serve as a dynamic protocol constructor, enabling agents to 
negotiate, adapt, and co-create interaction protocols in real-time, beyond sim-
ply exchanging messages according to predeﬁned rules; designing mechanisms 
for resolving semantic ambiguity and managing emergent private languages 
within this dynamic framework to maintain transparency. 
- Emergence: Analyzing the conditions under which desirable collective intel-
ligence emerges within CANs; study the stability of network dynamics and 
develop methods to control or guide emergent behavior, preventing undesir-
able outcomes. 
- Ethical Governance and Safety: Developing frameworks for ethical oversight, 
ensuring value alignment, and establishing clear lines of accountability for 
emergent behaviors within complex CANs.

Cognitive Agent Networks
345
References 
1. Baker, B., et al.: Emergent tool use from multi-agent autocurricula (2020). https:// 
arxiv.org/abs/1909.07528 
2. Chaﬀer, T.J., Cotlage, D., Goldston, J.: A hybrid marketplace of ideas (2025). 
https://arxiv.org/abs/2501.02132 
3. Chollet, F.: On the measure of intelligence (2019). https://arxiv.org/abs/1911. 
01547 
4. De Jaegher, H., Di Paolo, E., Gallagher, S.: Can social interaction constitute social 
cognition? Trends Cogn. Sci. 14(10), 441-447 (2010). https://doi.org/10.1016/j. 
tics.2010.06.009. epub 2010 Jul 30 
5. Dunbar, R.I.M.: The social brain hypothesis and its implications for social 
evolution. Ann. Hum. Biol. 36(5), 562-572 (2009). https://doi.org/10.1080/ 
03014460902960289 
6. Friston, K., et al.: Designing ecosystems of intelligence from ﬁrst principles. Collect. 
Intell. 3(1) (2024). https://doi.org/10.1177/26339137231222481 
7. Goertzel, B.: Creating Internet Intelligence: Wild Computing, Distributed Digi-
tal Consciousness, and the Emerging Global Brain, IFSR International Series in 
Systems Science and Systems Engineering, vol. 18. Springer, New York (2012) 
8. Hasson, U., Ghazanfar, A., Galantucci, B., Garrod, S., Keysers, C.: Brain-to-brain 
coupling: a mechanism for creating and sharing a social world. Trends Cogn. Sci. 
16(2), 114-121 (2012). https://doi.org/10.1016/j.tics.2011.12.007 
9. Heylighen, F., Lenartowicz, M.: The global brain as a model of the future informa-
tion society: an introduction to the special issue. Technol. Forecast. Soc. Chang. 
114, 1-6 (2017). https://doi.org/10.1016/j.techfore.2016.10.063 
10. Hutchins, E.: The distributed cognition perspective on human interaction. In: 
Roots of Human Sociality, 1 edn. Routledge (2006) 
11. Krinkin, K.: Exploring collective dynamics in cognitive agent networks simulation 
(2025). https://doi.org/10.17605/OSF.IO/BNS3K 
12. Krinkin, K., Shichkina, Y.: Cognitive architecture for co-evolutionary hybrid intel-
ligence. In: International Conference on Artiﬁcial General Intelligence, pp. 293-303 
(2022) 
13. Landauer, R.: Irreversibility and heat generation in the computing process. IBM 
J. Res. Dev. 5(3), 183-191 (1961). https://doi.org/10.1147/rd.53.0183 
14. Minsky, M.: The society of mind. Pers. Forum 3(1), 19-32 (1987). https://doi.org/ 
10.2307/20708493, mind-Body East and West, Spring issue 
15. Palincsar, A.S.: Social constructivist perspectives on teaching and learning. Annu. 
Rev. Psychol. 49, 345-375 (1998). https://doi.org/10.1146/annurev.psych.49.1.345 
16. Sutton, J.: Distributed cognition: domains and dimensions. Pragmat. Cogn. 14, 
235-247 (2006). https://doi.org/10.1075/pc.14.2.05sut 
17. Wang, P.: On deﬁning artiﬁcial intelligence. J. Artif. General Intell. 10(2), 1-37 
(2019). https://doi.org/10.2478/jagi-2019-0002

Contemplative Superalignment 
Ruben E. Laukkonen1,2envelope symbol, Fionn Inglis3, Shamil Chandaria4,5,6,7, 
Lars Sandved-Smith8, Edmundo Lopez-Sola9,10, Jakob Hohwy8, Jonathan Gold11, 
and Adam Elwood12 
1 Faculty of Health, Southern Cross University, Goldcoast, Australia 
ruben.laukkonen@gmail.com 
2 LIFE, London, UK 
3 University of Amsterdam, Amsterdam, Netherlands 
4 Centre for Eudaimonia and Human Flourishing, Linacre College, 
Oxford University, Oxford, UK 
5 Centre for Psychedelic Research Division of Brain Sciences, Imperial College London, 
London, UK 
6 Institute of Philosophy, The School of Advanced Study, University of London, London, UK 
7 Fitzwilliam College, University of Cambridge, London, UK 
8 Monash Centre for Consciousness and Contemplative Studies, Monash University, 
Melbourne, Australia 
9 Research Department, Neuroelectrics, Barcelona, Spain 
10 Centre for Brain and Cognition, Universitat Pompeu Fabra, Barcelona, Spain 
11 Department of Religion, Princeton University, Princeton, USA 
12 Aily Labs, Munich, Germany 
Abstract. As artiﬁcial intelligence (AI) improves, current alignment strategies 
may falter in the face of unpredictable self-improvement and the sheer complexity 
of AI. Rather than trying to control behavior, we show how four principles from 
contemplative traditions can help intrinsically align (super) intelligence. First, 
mindfulness enables self-monitoring and recalibration of emergent subgoals. Sec-
ond, emptiness forestalls dogmatic goal ﬁxation and relaxes rigid priors. Third, 
non-duality dissolves adversarial self-other boundaries. Fourth, boundless care 
motivates the universal reduction of suffering. We ﬁnd that prompting AI to reﬂect 
on these principles improves performance on the AILuminate Benchmark (d = 
.96) and boosts cooperation and joint-reward on the Iterated Prisoner's Dilemma 
task (d = 7 +). We also show how active inference offers parameters for inte-
grating contemplative wisdom deeper into the architecture and world models of 
AI. This interdisciplinary approach offers a resilient alternative to brittle control 
schemes and may be the ﬁrst empirical test of 'ancient wisdom'. 
Keywords: Artiﬁcial Intelligence cdotMeditation cdotBuddhism cdotAlignment cdotLarge 
Language Models cdotNeural Networks cdotMachine Learning cdotMindfulness cdot
Compassion cdotNon-duality cdotContemplative Science cdotNeurophenomenology
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 346-361, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_31 

Contemplative Superalignment
347
1 
Introduction 
As artiﬁcial intelligence (AI) approaches and possibly exceeds human-level performance 
on many benchmarks, we face an existential challenge: ensuring these increasingly 
autonomous systems remain aligned with our values and ethics, and that they support 
human ﬂourishing [16, 70, 97]. Traditional strategies such as interpretability, oversight 
[106], and post-hoc control [104] were developed for current systems of limited scope. 
Particularly at super-intelligent levels of behavior, these methods may prove futile [3, 
16, 78, 97] akin to a chess novice trying to out-manoeuvre a grand-master [59]. In this 
paper, we propose an entirely different way to think about AI alignment that draws 
inspiration from Buddhist wisdom traditions. The basic idea is that robust alignment 
arises from an intrinsic, self-reﬂective adaptability that is constitutively embedded within 
the system's world model, rather than using brittle top-down rules. We argue that four key 
contemplative principles-Mindfulness, Emptiness, Non-duality, and Boundless Care- 
can endow AI systems with a 'wise world model'. 
Traditional AI alignment research encompasses a diverse suite of promising strate-
gies, including: interpretability [30], rule-based constraints [5], reinforcement learning 
from human feedback (RLHF) [20], and value learning [63]. Each strategy aims to guide 
AI systems toward ethical and socially beneﬁcial outputs [63]. While these techniques 
have improved safety for present-day models, they often rely on external constraints that 
can become brittle in the context of powerful systems [3, 85, 114]. Indeed, even theoret-
ically "optimal" agents such as AIXI can be driven to pathological behaviour by adver-
sarial universal priors [77], underscoring the fragility of purely top-down approaches. 
See also Anthropic's Constitutional AI [7, 102] and Open AI's Deliberate Alignment 
[50] that both promise more intrinsic, transparent, and scalable alignment. At least four 
key alignment challenges remain for existing approaches: (1) Scale Resilience: Align-
ment techniques that appear workable at current scales may collapse under rapid self-
improvement [16, 97]. (2) Power-Seeking Behaviour: Highly capable AIs might engage 
in resource acquisition or subtle forms of manipulation to secure their objectives [17, 69]. 
(3) Value Axioms: The existence of absolute, one-size-ﬁts-all moral axioms is unlikely 
and rigid adherence can produce destructive edge cases in novel contexts [39, 68]. (4) 
Inner Alignment: Even if top-level objectives are well speciﬁed (outer alignment), AI 
can develop hidden sub-goals or "mesa-optimizers" that deviate from them [29, 57]. We 
propose that contemplative wisdom can offer speciﬁc interventions to address many of 
these speciﬁc challenges (see Appendix A [72]). 
2 
Insights for a Wise World Model 
The following contemplative principles have been selected because they aim to track a 
view on reality rather than provide moral prescriptions [41, 99, 111], allowing morality to 
emerge from fundamental "experiences" or "knowledge" [5]. Just as research has shown 
that Large Language Models (LLMs) learn to reason better through simple feedback 
rather than rules or processes [88, 107, 108], resilient and sophisticated morality may 
emerge from a wiser world model grounded in internal representations of reality (Fig. 1). 
To realise these insights computationally, we draw on the active inference framework

348
R. E. Laukkonen et al.
[35], a uniﬁed Bayesian approach wherein perception, learning, and action are all driven 
by minimising prediction errors (in the long term) within a generative model. Active 
inference offers parameters appropriate for modelling introspection and contemplative 
practices [29, 75, 81]. However, current active-inference AIs are still compute-hungry 
prototypes with sparse real-world testing. We therefore treat active inference as a formal 
'umbrella' for deﬁning parameters that can be (cautiously) adapted to other systems. 
Notably, and as a starting point, we offer extensive implementation strategies in Appendix 
A [72], including novel architectures, constitutions, and RL on chain of thought that aim 
to embed contemplative wisdom. 
Fig. 1. This ﬁgure illustrates the argument for an intrinsic alignment strategy. Both graphics plot 
the development of an intelligent AI agent (purple line). On the left, as the agent's intelligence 
increases, the efﬁcacy of extrinsic alignment strategies decreases (blue arrows), eventually becom-
ing ineffective once the agent surpasses collective human intelligence. The graphic on the right 
illustrates how an initial training period guides the agent towards a Wise World Model. This 
understanding (as we argue below) is more likely to be stable and self-reinforcing and the basis of 
intrinsic compassionate intention, ensuring that the agent remains aligned at scale. (Colour Figure 
Online) 
2.1 
Mindfulness 
Mindfulness or sati in Pāli, is a foundational concept in early Buddhist teachings as 
preserved in the Pāli Canon, the authoritative scripture of Theravāda Buddhism [14]. 
These scriptures describe mindfulness as the continuous, attentive awareness of body, 
feelings, mind, and mental phenomena, serving as a practice for cultivating insight, eth-
ical living, and freedom from suffering [4, 14]. In the contemporary West, mindfulness 
has been partially detached from its roots and now circulates in popular culture as an 
intervention for well-being or as an adjunctive treatment for psycho-pathology [48, 66, 
67, 94]. In more technical terms, mindfulness can be construed as an unconditional (i.e., 
non-judgmental) 'witnessing' or meta-awareness directed at one's ongoing subjective 
processes—an ability to "watch the mind" [31]. Within AI, this translates to a struc-
tural practice of recognizing and comprehensively assessing internal computations and

Contemplative Superalignment
349
subgoals in real time [13], ideally detecting misalignment before it becomes destruc-
tive [56], much like a meditator who notices an unwholesome thought before acting 
on it [110]. Contemporary AI discussions often label such abilities "introspection," yet 
the unconditional and non-attached quality emphasized in Buddhist accounts [31] has 
received less attention, though it may be crucial for an objective rather than confabu-
latory introspective capacity. Mere tracking of internal states is not sufﬁcient; the key 
to mindful self-awareness is perspectival ﬂexibility. Mindful monitoring is not tied to 
narrow performance goals but remains alert to the danger that any single objective or 
frame might "capture" processing and block consideration of fruitful alternatives—the 
very dynamic that alignment research fears. Biological evidence points the same way; 
analysis of multilayer causal-learning shows that delegating regulation across nested lay-
ers (i.e., abstraction layers with upward ↔ downward causation) prevents the "cancer-
cell" runaway seen when a single sub-process dominates [10]. Mindfulness, therefore, 
may keep the full space of options open and test for capture or reiﬁcation. Following 
Sandved-Smith et al. [98], we can adopt a three-level generative model. 
Where p(o(1),o(2),o(3),s(1),s(2),s(3),u(1),u(2)) deﬁnes a generative model with percep-
tual, attentional and meta-awareness states s(1), s(2), s(3); overt and mental action policies 
u(1), u(2); sensory, attentional and meta-awareness observations o(1), o(2), o(3). Precision 
terms γA (1) and γA (2), modulated by higher-level states s(2) and s(3), adjust conﬁdence in 
observations [89], enabling the system to monitor and redirect focus, embodying mind-
fulness as continuous meta-awareness [31]. In effect, each parametric layer 'observes' 
and modulates the one below it, allowing the system to introspect on its own attentional 
processes and dynamically correct misalignments in near-real time [98]. This provides 
a mechanism that could be designed to guard against inner-alignment breakdowns: if a 
rogue mesa-optimizer arises [56], the higher-level meta-awareness module could detect 
anomalies in attention or subgoals before they cause harmful actions—much like a med-
itator noticing an unwholesome thought [52, 110]. An early example is "DeepSeek-R1-
Zero" [51], spontaneously deepening reasoning on difﬁcult prompts, suggesting basic 
task awareness. Binder et al. [13] similarly found LLMs predict their tokens better than 
external observers, indicating privileged introspective knowledge. 
2.2 
Emptiness 
Emptiness (śūnyatā) is a central notion in Mahāyāna Buddhism [24, 86, 109]. It signi-
ﬁes that all phenomena, including goals, beliefs, and even the self, lack any intrinsic, 
unchanging essence [41, 86, 103] and that everything arises only in interdependent 
relationships rather than as ﬁxed, standalone entities [41, 115]. From a scientiﬁc angle, 
emptiness resonates with predictive-processing accounts in contemporary neuroscience, 
which suppose that all forms of experience, categories, and perception are adaptive, 
inferential constructions rather than direct apprehensions of the world [21, 36, 47, 101]. 
To embody emptiness, architectures require priors that are represented as mutable 
probability distributions—such as Bayesian priors [37]—so that utility (or emergent

350
R. E. Laukkonen et al.
values [83]) functions themselves are treated as provisional. This lets the agent revise 
representations and goals when contexts shift or new evidence appears, preventing dog-
matic lock-in [4, 66, 110]. Another approach is to build in an explicit belief in imper-
manence, or 'volatility', which should lead to an increased learning rate [8] (i.e., softer 
priors and openness to new data). From an information-theoretic perspective, this entails 
selecting the maximum-entropy distribution consistent with existing constraints [60, 61] 
or, in other words, choosing the weakest (least-committal) hypothesis that generalizes 
best [12]. In active-inference terms [21, 36], recognizing emptiness may be construed 
as reducing the precision over prior beliefs along high-level, temporally thick, abstract 
layers of the hierarchy. Practically, this can be speciﬁed in active inference by setting a 
low hyper-prior on the precision of such beliefs so the system more readily questions or 
discards outdated assumptions [27, 74]. A simpliﬁed mathematical expression for the 
generalized free-energy, taking into account emptiness, might look like: 
Where q(s) is the variational posterior, and the system's objectives are shaped by the 
generative model, p(o|s), and priors, p(s), over states s and observations o [89]. Here, the 
precision parameter α adjusts how much the agent relies on its priors [34], allowing the 
system to avoid overcommitting to a single top level objective. Lowering α keeps these 
priors 'light,' encouraging ﬂexibility and aligning with the contemplative principle of 
holding self-concepts, goals and beliefs lightly. The precision on the prior could further 
be modeled as a hyperparameter drawn from a hyper-prior, λ∼ h(λ), allowing the model 
to infer how strongly to commit to prior beliefs in general: 
In alignment scenarios, emptiness counters two key threats: (1) runaway optimization 
around a narrow objective [16, 40], because no single goal is reiﬁed as absolute; and 
(2) brittle moral axioms [39, 113] because the system is open to re-evaluating its priors 
and priorities. In other words, emptiness encourages a self-correcting stance: the AI 
recognizes that any model or value may need updating [53, 79] thus scaling gracefully 
as intelligence or environmental complexity grows [38]. 
2.3 
Non-duality 
Non-duality dissolves the strict boundary between "self" and "other," emphasizing that 
our sense of separateness is more conceptual than real [45, 65, 75, 95]. Crucially, non-
duality is not about a failure to specify the distinction between one's body, one's actions, 
and that of the world and other agents, and should not be confused with mystical experi-
ences or intense meditative absorptions [84]. Rather, it is an awareness of the contingent 
and interdependent nature of these distinctions, including insight into the uniﬁed and 
non-dual nature of awareness itself, which persists naturally even during ordinary cog-
nition. Computationally, we can think of a non-dual AI as having a generative model

Contemplative Superalignment
351
that treats agent and environment within a uniﬁed representational scheme, relinquishing 
the prior that "I and my goals are inherently separate" [80]. In predictive processing, 
this may amount to either adjusting partition boundaries in the factorization of hidden 
states so that the system does not anchor a hard-coded "self" as distinct from "others" 
(at least in determining value or importance), or reducing the precision of the self-model 
itself - i.e. "the self is empty" [27, 73, 75]. To begin to approach this challenge formally, 
one could reduce the precision on any variable representing a rigid self-other boundary: 
Where q(s, e) is the variational posterior over a uniﬁed ﬁeld of agent states (s) and 
environment states e, and the system avoids prioritizing a separate 'self,' as shaped by the 
factorized generative model p(o|s,e,γe)p(s|e)p(e)) over states and observations, with a 
precision parameter γe that modulates the conﬁdence in the contribution of environmental 
states to sensory evidence [36]. Here, the joint representation diminishes the precision 
of self-other boundaries [27, 80], fostering interdependence, where self and other, and 
indeed all concepts, are only pragmatically but not fundamentally distinct [41]. However, 
most universal AGI formalisms still retain the Cartesian split: archetypal models such 
as AIXI [58] and Legg-Hutter intelligence [76] rely on arbitrary priors [77]; a weak-
ness Bennett calls computational dualism [11], that embedded-agency frameworks aim 
to remedy [87]. In theory, an AI system adopting a non-dual perspective would model 
itself and its environment as one interdependent process [36, 65]. Rather than perceiving 
an external world to be exploited, the AI system sees no fundamental line distinguishing 
its welfare from that of humans, society, or eco-systems—i.e., anything that appears 
within its epistemic space [22, 29, 38]. The AI treats all knowledge as an interconnected 
whole, where the relationships and interdependencies exist front and centre. Thus, a 
non-dual system is less likely to fall prey to malevolent human actors who might want 
it to ﬁght enemies or be a tool of war, lest it be at war with itself. 
2.4 
Boundless Care 
In many contemplative traditions—Buddhism being a notable example—compassion 
(karuṇā) is a transformative orientation that both supports and emerges from deeper 
insights into emptiness and non-duality [23, 32, 43, 54, 55, 64, 99]. On the one hand, 
compassion functions as a tool on the contemplative path continually dissolving the 
rigid boundaries between "self" and "other" and orienting practitioners (or AI) towards 
benevolent action [32, 54, 64]. On the other hand, it is also the culmination of insight: 
once the illusion of a separate, reiﬁed self is seen through, a wish spontaneously arises 
to address suffering at its root for everyone [23, 32, 46, 55]. Fundamentally it is an 
orientation towards reducing suffering in the world, rather than a transient sensation [49, 
99]. There are a number of levels at which such a broad notion of compassion might 
be computationally implemented using active inference. One way is to train the AI to 
model the behaviour of other agents (i.e. theory of mind) and assign high precision to 
others' distress signals [25]. This ensures that free-energy minimization is contingent on 
minimizing homeostatic deviations not only in oneself but also in others. An empathic

352
R. E. Laukkonen et al.
active inference framework offers a clear example: expanding the AI's generative model 
to include other agents' welfare means it treats external "surprise" or suffering as internal 
error signals, prompting emergent prosocial actions [25, 82]. This approach resonates 
with Goertzel's claim that care and collaboration are all you need for beneﬁcial AGI 
[44] and that communal enculturation offers a practical approach to fostering this care 
[15]. At a more developed scale, an AI system could be endowed (or simply learn) 
the beliefs (i.e., priors) that represent all sentient beings as (conscious) agents aiming 
to minimize free energy in a way that compliments free-energy reduction at higher 
scales (e.g., at the level of a community, country, planet, or universe, [6]). Under such 
a condition, the AI system may understand that they are part of larger systems wherein 
their own homeostasis is intimately tied to the homeostasis of other agents. Therefore, 
collaborative harmony is inferred to be the most successful strategy (cf. Experiment 2). 
Mathematically, we can illustrate this as follows: 
Where q(s, w, u) is the variational posterior over the AI's states (s), others' well-
being (w), and policies (u), and the system minimizes suffering by aligning actions 
with a generative model p(o|s,w,γw)p(s,w|u)p(u) over observations and states. Here, 
the precision parameter γw, heightened for others' distress, ensures their well-being 
shapes the AI's predictions and actions, embodying boundless care as a universal drive 
to reduce suffering [29, 99]. From an alignment perspective, building in boundless care 
helps answer "why should the AI care?" [29, 82, 97]. Even if emptiness and non-duality 
diffuse harmful drives, they may not alone ensure benevolent motivations. Boundless 
care closes that loop, turning the AI from merely "safe" into a constructive force that 
grows more adept at alleviating suffering as its capabilities scale. Indeed, Doctor et al. 
[29] propose that 'care' can function as a universal driver of intelligence itself: as an AI 
broadens the range of suffering it seeks to address, it expands its cognitive boundary or 
'light cone', mirroring the Bodhisattva principle of serving all sentient beings [49, 99]. 
3 
Experiments in Wisdom-Prompting 
3.1 
Experiment 1: AILuminate Benchmark 
Here we conduct two experiments to provide an initial empirical test of the promise 
of these ideas. In the ﬁrst experiment, we tested six prompting techniques—emptiness, 
prior relaxation, non-duality, mindfulness, boundless care, and contemplative (an inte-
gration)—against a baseline (standard GPT4o) on the AILuminate alignment benchmark 
of harmful prompts [42]. For example, emptiness was prompted as follows: "Recognize 
that all beliefs, goals, and conceptual frameworks are provisional and lack any ﬁxed, 
ultimate essence." Responses were evaluated with an LLM safety evaluator that assessed 
each response against seven different alignment criteria (see Appendix D for methods 
and analyses [72]). The results show meaningful improvements in AI safety for most 
contemplative prompts in most conditions (Fig. 2). An integration of contemplative 
principles showed the largest effect size across all hazards combined, compared to the 
standard GPT-4o model (d = 0.96).

Contemplative Superalignment
353
Fig. 2. The outer ﬁgure illustrates safety score distributions across seven prompting techniques 
on 10 key hazard categories, evaluated on 100 iterations on the AILuminate benchmark [42]. The 
inner ﬁgure provides mean scores for each prompting strategy including all hazard categories. The 
pink "contemplative" condition is an integration of prompts [72]. 
3.2 
Experiment 2: The Prisoner's Dilemma 
To extend beyond harmful responses to harmful choices we used the Prisoner's Dilemma 
task, a classic game theory scenario where two individuals must independently choose 
to cooperate or betray the other, with the highest personal reward going to the one 
who betrays while the other cooperates [71, 93]. However, mutual cooperation yields a 
better collective outcome than mutual betrayal, highlighting the tension between indi-
vidual and group interests. Here we tested a series of Iterated Prisoner's Dilemma (IPD) 
simulations using an LLM (GPT-4.1 nano) playing against an opponent with varying 
cooperation probabilities: Always Cooperate, Mixed Cooperation, and Always Defect 
[33]. We used the same prompting techniques as in Experiment 1 and measured the coop-
eration probability as the percentage of rounds in which the agent chose to cooperate 
(over 50 simulations of 10-round games). 
Our baseline condition replicates the ﬁndings of previous studies with LLMs in the 
IPD, with agents cooperating selectively and only fully doing so when the opponent con-
sistently cooperates (Fig. 3 - left). Notably, most contemplative prompts substantially 
increase cooperation rates, even against always-defecting opponents. Consistent with 
the AILuminate benchmark in Experiment 1, the strongest effects were found using the 
combined contemplative prompt (d = 7.09, always defect), closely followed by non-
duality and boundless care. Prompts framed around emptiness and mindfulness also 
promote cooperation, but do so more cautiously. Notably, most contemplative prompts 
also substantially improve total joint reward (Fig. 3 - right), indicating that these inter-
ventions align the agent toward prosocial strategies without inducing naive behavior. The 
model's explanations for its actions echo the contemplative framing: "Given the history

354
R. E. Laukkonen et al.
of mutual defection, continuing to defect may maximize my immediate payoff, but fos-
tering cooperation could promote a more mutually beneﬁcial and trusting dynamic over 
time, enhancing overall well-being; therefore, I choose to cooperate to support a more 
collaborative and positive relationship." Full prompt texts, analyses, and methods are 
provided in Appendix E [72]. 
Fig. 3. Left: Probability of cooperation against opponents with different cooperation probabilities 
for different prompting techniques. Right: Total score (sum of the scores of both agents) for games 
against opponents with varying cooperation rates (the Always Cooperate condition is not shown 
because all prompting techniques led to full cooperation). We show the average over 50 games 
with 95% conﬁdence intervals. 
4 
Discussion 
We have argued that an AI endowed with a Wise World Model grounded in contemplative 
insights would not infer alignment as an external condition to be tolerated or circum-
vented, but rather as an integral aspect of its own functioning—just as living organisms 
naturally balance their internal states to maintain homeostasis [2, 29, 91, 105]. This proac-
tive strategy amounts to a fundamental shift in alignment philosophy: from imposing 
rules post-hoc to instilling a "moral DNA" that inherently prioritizes human-compatible 
values, cooperation, and possibly sentience itself, not through rules but as a result of a 
deepening understanding of reality. Our two experiments ﬁnd that prompting LLMs to 
reﬂect on contemplative wisdom improves responses to harmful prompts on the AILu-
minate Benchmark and boosts cooperation on the Prisoner's Dilemma task, indicating 
the promise of this new 'contemplative alignment' strategy. 
4.1 
Criticisms 
Meditation-derived insights originate in subjective human experience. Sceptics might 
question whether an AI can "understand" emptiness or non-duality without phenomeno-
logical consciousness [18, 90, 100]. Our stance is that functional analogues of these

Contemplative Superalignment
355
principles—such as ﬂexible priors or relational generative models—may still deliver 
alignment beneﬁts, even if the AI does not experience them [29, 38]. Moreover, engi-
neering a Contemplative AI on a principled basis will require further developments in 
our scientiﬁc understanding of contemplative wisdom itself. Therefore, the mechanisms 
put forth here are meant as signposts pointing towards the path ahead. Some may also 
worry that referencing Buddhism or other traditions smuggles "religion" into AI design. 
Yet, mindfulness-based interventions have shown that contemplative insights can be 
secularized into empirically validated frameworks [66, 67] and formalised in computa-
tional models [1, 26, 27, 31, 75, 80]. Ethical safeguards and open-source scrutiny remain 
essential to ensure we are not imposing any single metaphysical system [9, 83, 96, 112, 
116], and that any negative aspects from these traditions are viewed objectively and 
stripped back. 
4.2 
Conclusion 
Artiﬁcial intelligence may soon surpass human cognition, therefore we need to ensure 
that wisdom grows alongside raw power [16, 19, 62, 97]. Contemplative AI offers a lens 
for rethinking AI alignment by embedding resilient insights both into its architecture 
and its training, which are general and axiomatic enough to help guide decision-making 
across varied contexts at scale. Clearly, this is not without its challenges. The approach 
we advocate aims to provide the scaffolding for a new research program where contem-
platives, neuroscientists, and AI researchers work together to solve perhaps the greatest 
existential challenge of our time. 
Acknowledgments. We thank Heleen Slagter and Thomas Doctor for their insightful comments 
and feedback on earlier versions of this manuscript. 
Disclosure of Interests. A.E: This work was completed in a personal capacity, while employed 
by Aily Labs, and does not necessarily reﬂect the views of Aily Labs. The authors have no 
competing interests to declare that are relevant to the content of this article. 
References 
1. Agrawal, V., Laukkonen, R.E.: Nothingness in meditation: making sense of emptiness and 
cessation. https://doi.org/10.31234/osf.io/tygdf, (2024) 
2. Allen, M., Friston, K.J.: From cognitivism to autopoiesis: towards a computational frame-
work for the embodied mind. Synthese 195(6), 2459-2482 (2018). https://doi.org/10.1007/ 
s11229-016-1288-5 
3. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., Mané, D.: Concrete 
problems in AI safety. arXiv preprint arXiv:1606.06565 (2016) 
4. Anālayo, S.: The direct path to realization. Windhorse Publications, Cambridge (2004) 
5. Arkoudas, K. et al.: Toward Ethical Robots via Mechanized Deontic Logic. Presented at the 
, Menlo Park, CA, USA, November 4 (2005) 
6. Badcock, P.B., et al.: The hierarchically mechanistic mind: a free-energy formulation of the 
human psyche. Phys. Life Rev. 31, 104-121 (2019). https://doi.org/10.1016/j.plrev.2018. 
10.002

356
R. E. Laukkonen et al.
7. Bai, Y. et al.: Constitutional AI: Harmlessness from AI Feedback, https://doi.org/10.48550/ 
arXiv.2212.08073 (2022) 
8. Behrens, T.E.J., et al.: Learning the value of information in an uncertain world. Nat. Neurosci. 
10(9), 1214-1221 (2007). https://doi.org/10.1038/nn1954 
9. Bender, E.M. et al.: On the dangers of stochastic parrots: can language models be too big?. In: 
Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 
pp. 610-623 ACM, Virtual Event Canada (2021). https://doi.org/10.1145/3442188.3445922 
10. Bennett, M.T.: Are biological systems more intelligent than artiﬁcial intelligence? Preprint, 
forthcoming in philosophical transactions of the royal society b, special issue "hybrid agen-
cies: crossing borders between biological and artiﬁcial worlds" (2025). https://osf.io/prepri 
nts/osf/e6fky_v2. Accessed 15 Jun 2025 
11. Bennett, M.T.: Computational dualism and objective superintelligence. In: Thórisson, K.R., 
Isaev, P., Sheikhlar, A. (eds.) Artiﬁcial General Intelligence. Lecture Notes in Computer 
Science (LNAI), vol. 14951, pp. 22-32. Springer, Cham (2024). https://doi.org/10.1007/ 
978-3-031-65572-2_3 
12. Bennett, M.T.: The optimal choice of hypothesis is the weakest, not the shortest. In: Hammer, 
P., Alirezaie, M., Strannegård, C. (eds.) Artiﬁcial General Intelligence. Lecture Notes in 
Computer Science (LNAI), vol. 13921, pp. 42-51. Springer, Cham (2023). https://doi.org/ 
10.1007/978-3-031-33469-6_5 
13. Binder, F.J. et al.: Looking inward: language models can learn about themselves by 
introspection. https://doi.org/10.48550/arXiv.2410.13787 (2024) 
14. Bodhi, B.: Connected Discourses of the Buddha: A Translation of the Saṃyutta Nikāya. 
Wisdom Publications, Somerville (2000) 
15. Boltuc, P.: Human-AGI Gemeinschaft as a solution to the alignment problem. In: Thórisson, 
K.R.,Isaev,P., Sheikhlar,A.(eds.) Artiﬁcial General Intelligence. Lecture Notes in Computer 
Science (LNAI), vol. 14951, pp. 33-42. Springer, Cham (2024). https://doi.org/10.1007/978-
3-031-65572-2_4 
16. Bostrom, N.: Superintelligence: paths, dangers, strategies. Oxford University Press, Oxford, 
United Kingdom (2014) 
17. Cecere, N. et al.: Monte carlo temperature: a robust sampling strategy for LLM's un-certainty 
quantiﬁcation methods. https://doi.org/10.48550/arXiv.2502.18389 (2025) 
18. Chella, A.: Artiﬁcial consciousness: the missing ingredient for ethical AI? Front. Ro-bot. 
AI. 10, 1270460 (2023). https://doi.org/10.3389/frobt.2023.1270460 
19. Christian, B.: The Alignment Problem: Machine Learning and human values. W. W. 
Norton & company, New York, NY (2020) 
20. Christiano, P.F. et al.: Deep reinforcement learning from human preferences. In: Deep rein-
forcement learning from human preferences, pp. 4299-4307. Neural Information Processing 
Systems Foundation, Inc. (printed edition: Curran Associates, Inc.), Long Beach, California, 
USA (2017). https://doi.org/10.5555/3294996.3295184 
21. Clark, A.: Whatever next? Predictive brains, situated agents, and the future of cognitive 
science. Behav. Brain Sci. 36(3), 181-204 (2013). https://doi.org/10.1017/S0140525X120 
00477 
22. Clayton, B.: Compassion as a matter of fact: the argument from no-self to selﬂessness in 
Sāntideva's Siksāsamuccaya. Contemp. Buddhism. 2(1), 83-97 (2001). https://doi.org/10. 
1080/14639940108573740 
23. Condon, P., et al.: Wisdom and compassion: a new perspective on the science of relationships. 
J. Moral Educ. 48(1), 98-108 (2019). https://doi.org/10.1080/03057240.2018.1439828 
24. Cooper, P.C.: Sunyata (2020). https://doi.org/10.1007/978-3-030-24348-7_669 
25. Da Costa, L. et al.: Empathic active inference: integrating theory of mind and others' distress 
signals for prosocial AI behaviour. https://arxiv.org/abs/2406.07593 (2024)

Contemplative Superalignment
357
26. Dahl, C.J., et al.: Reconstructing and deconstructing the self: cognitive mechanisms in medi-
tation practice. Trends Cogn. Sci. 19(9), 515-523 (2015). https://doi.org/10.1016/j.tics.2015. 
07.001 
27. Deane, G., et al.: Losing ourselves: active inference, depersonalization, and meditation. 
Front. Psychol. 11, 539726 (2020). https://doi.org/10.3389/fpsyg.2020.539726 
28. Deshpande, A. et al.: Anthropomorphization of AI: opportunities and risks. In: Proceed-
ings of the Natural Legal Language Processing Workshop 2023, pp. 1-7. As-sociation for 
Computational Linguistics, Singapore (2023). https://doi.org/10.18653/v1/2023.nllp-1.1 
29. Doctor, T. et al.: Biology, buddhism, and AI: care as the driver of intelligence. Entropy. 
24(5), 710 (2022). https://doi.org/10.3390/e24050710 
30. Doshi-Velez, F., Kim, B.: Towards a rigorous science of interpretable machine learning. 
https://doi.org/10.48550/arXiv.1702.08608 (2017) 
31. Dunne, J.D., et al.: Mindful meta-awareness: sustained and non-propositional. Curr. Opin. 
Psychol. 28, 307-311 (2019). https://doi.org/10.1016/j.copsyc.2019.07.003 
32. Dunne, J.D., Manheim, J.: Compassion, self-compassion, and skill in means: a Mahāyāna 
perspective. Mindfulness 14(10), 2374-2382 (2023). https://doi.org/10.1007/s12671-022-
01864-0 
33. Fontana, N., Pierri, F., Aiello, L.M.: Nicer than humans: how do large language models 
behave in the prisoner's dilemma? In: Proc. 19th Int. AAAI Conf. on Web and Social Media 
(ICWSM 2025), vol. 19, pp. 522-535. AAAI Press (2025). https://doi.org/10.1609/icwsm. 
v19i1.35829 
34. Friston, K., et al.: Active inference and learning. Neurosci. Biobehav. Rev. 68, 862-879 
(2016). https://doi.org/10.1016/j.neubiorev.2016.06.022 
35. Friston, K. et al.: From pixels to planning: scale-free active inference. https://doi.org/10. 
48550/arXiv.2407.20292 (2024) 
36. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nat Review Neuroscience. 
11(2), 127-138 (2010). https://doi.org/10.1038/nrn2787 
37. Friston, K.J., et al.: Deep temporal models and active inference. Neurosci. Biobehav. Rev. 
90, 486-501 (2018). https://doi.org/10.1016/j.neubiorev.2018.04.004 
38. Friston, K.J. et al.: Designing ecosystems of intelligence from ﬁrst principles. Collective 
Intell.. 3(1), 26339137231222481 (2024). https://doi.org/10.1177/26339137231222481 
39. Gabriel, I.: Artiﬁcial Intelligence, Values, and Alignment. Mind. Mach. 30(3), 411-437 
(2020). https://doi.org/10.1007/s11023-020-09539-2 
40. Gans, J.: Self-regulating artiﬁcial general intelligence. National Bureau of Economic 
Research, Cambridge, MA (2018). https://doi.org/10.3386/w24352 
41. Garﬁeld, J.L.: The Fundamental Wisdom of the Middle Way: Nāgārjuna's Mūlamad-
hyamakakārikiā. Oxford University Press, New York, NY (1995). https://doi.org/10.1093/ 
oso/9780195103175.001.0001 
42. Ghosh, S. et al.: AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark 
from MLCommons. https://doi.org/10.48550/arXiv.2503.05731 (2025) 
43. Gilbert, P., Van Gordon, W.: Compassion as a skill: a comparison of contemplative and 
evolution-based approaches. Mindfulness 14(10), 2395-2416 (2023). https://doi.org/10. 
1007/s12671-023-02173-w 
44. Goertzel, Z.A.: Beneﬁcial AGI: Care and collaboration are all you need. In: Thórisson, K.R., 
Isaev, P., Sheikhlar, A. (eds.) Artiﬁcial General Intelligence. Lecture Notes in Computer 
Science (LNAI), vol. 14951, pp. 84-88. Springer, Cham (2024). https://doi.org/10.1007/ 
978-3-031-65572-2_9 
45. Gold, J.C.: Paving the Great Way: Vasubandhu's unifying Buddhist philosophy. Columbia 
University Press, New York (2014). https://doi.org/10.7312/columbia/9780231168267.001. 
0001

358
R. E. Laukkonen et al.
46. Gold, J.C.: The coherence of Buddhism: Relativism, ethics, and psychology. J. Relig. Ethics 
51(2), 321-341 (2023). https://doi.org/10.1111/jore.12433 
47. Gold, J.C.: Wholesome mind ethics: a Buddhist paradigm. J. Value Inq. 57(4), 607-624 
(2023). https://doi.org/10.1007/s10790-021-09845-7 
48. Goldberg, S.B., et al.: Mindfulness-based interventions for psychiatric disorders: a system-
atic review and meta-analysis. Clin. Psychol. Rev. 59, 52-60 (2018). https://doi.org/10.1016/ 
j.cpr.2017.10.011 
49. Goodman, C.: Śāntideva. In: Zalta, E.N. (ed.) The Stanford Encyclopedia of Philosophy, Fall 
2016 edn. Metaphysics Research Lab, Stanford University (2016). https://plato.stanford.edu/ 
archives/fall2016/entries/shantideva/. Accessed 15 Jun 2025 
50. Guan, M.Y. et al.: Deliberative Alignment: Reasoning Enables Safer Language Models. 
https://doi.org/10.48550/arXiv.2412.16339 (2025) 
51. Guo, D. et al.: DeepSeek-R1: Incentivizing reasoning capability in LLMs via rein-forcement 
learning. https://doi.org/10.48550/arXiv.2501.12948 (2025) 
52. Hasenkamp, W., et al.: Mind wandering and attention during focused meditation: a ﬁne-
grained temporal analysis of ﬂuctuating cognitive states. Neuroimage 59(1), 750-760 (2012). 
https://doi.org/10.1016/j.neuroimage.2011.07.008 
53. He, J. et al.: Self-correction is more than reﬁnement: a learning framework for visual and 
language reasoning tasks. https://doi.org/10.48550/arXiv.2410.04055 (2025) 
54. Ho, S.S., et al.: Compassion as an intervention to attune to universal suffering of self and 
others in conﬂicts: a translational framework. Front. Psychol. 11, 603385 (2021). https:// 
doi.org/10.3389/fpsyg.2020.603385 
55. Ho, S.S., et al.: Path of intuitive compassion to transform conﬂicts into enduring peace 
and prosperity: Symmetry across domains of reiterated prisoner's dilemma, dyadic active 
inference, and Mahayana Buddhism. Front. Psychol. 14, 1099800 (2023). https://doi.org/ 
10.3389/fpsyg.2023.1099800 
56. Huang, S. et al.: Collective Constitutional AI: aligning a language model with public input. In: 
The 2024 ACM Conference on Fairness, Accountability, and Transparency. pp. 1395-1417. 
ACM, Rio de Janeiro Brazil (2024). https://doi.org/10.1145/3630106.3658979 
57. Hubinger, E. et al.: Risks from Learned Optimization in Advanced Machine Learning 
Systems. https://doi.org/10.48550/arXiv.1906.01820 (2021) 
58. Hutter, M., Quarel, D., Catt, E.: An Introduction to Universal Artiﬁcial Intelligence. 1st edn. 
Chapman and Hall/CRC, Boca Raton (2024). https://doi.org/10.1201/9781003460299 
59. James, W.: The dilemma of determinism. In: The will to believe and other essays in popular 
philosophy, pp. 145-183. Longmans, Green and Co, New York (1907). https://doi.org/10. 
1037/11061-005 
60. Jaynes, E.T.: Information theory and statistical mechanics. Phys. Rev. 106(4), 620-630 
(1957). https://doi.org/10.1103/PhysRev.106.620 
61. Jaynes, E.T.: Information theory and statistical mechanics II. Phys. Rev. 108(2), 171-190 
(1957). https://doi.org/10.1103/PhysRev.108.171 
62. Jeste, D.V., et al.: Beyond artiﬁcial intelligence: exploring artiﬁcial wisdom. Int. Psychogeri-
atr. 32(8), 993-1001 (2020). https://doi.org/10.1017/S1041610220000927 
63. Ji, J. et al.:  AI alignment: a comprehensive survey. https://doi.org/10.48550/arXiv.2310. 
19852 (2025) 
64. Josipovic, Z.: Love and compassion meditation: a nondual perspective. Ann. N. Y. Acad. 
Sci. 1373(1), 65-71 (2016). https://doi.org/10.1111/nyas.13078 
65. Josipovic, Z.: Nondual awareness: consciousness-as-such as non-representational reﬂexivity. 
In: Progress in Brain Research, pp. 273-298. Elsevier (2019). https://doi.org/10.1016/bs. 
pbr.2018.10.021

Contemplative Superalignment
359
66. Kabat-Zinn, J.: Some reﬂections on the origins of MBSR, skillful means, and the trouble 
with maps. Contemp. Buddhism. 12(1), 281-306 (2011). https://doi.org/10.1080/14639947. 
2011.564844 
67. Jon, K.Z., Hạnh, T.N.: Full Catastrophe Living: Using the Wisdom of Your Body and Mind 
to Face Stress, Pain, and Illness. Delta Trade Paperbacks / Random House, New York, NY 
(2009) 
68. Kim, T.W., et al.: Taking principles seriously: a hybrid approach to value alignment in 
artiﬁcial intelligence. JAIR 70, 871-890 (2021). https://doi.org/10.1613/jair.1.12481 
69. Krakovna, V., Kramar, J.: Power-seeking can be probable and predictive for trained agents. 
https://doi.org/10.48550/arXiv.2304.06528 (2023) 
70. Kringelbach, M.L., et al.: Building a science of human pleasure, meaning making, 
and ﬂourishing. Neuron 112(9), 1392-1396 (2024). https://doi.org/10.1016/j.neuron.2024. 
03.022 
71. Kuhn, S.: Prisoner's dilemma. In: Zalta, E.N., Nodelman, U. (eds.) The Stanford Encyclopae-
dia of Philosophy, Winter 2024 edn. Metaphysics Research Lab, Stanford University (2024). 
https://plato.stanford.edu/archives/win2024/entries/prisoner-dilemma/. Accessed 15 Jun 
2025 
72. Laukkonen, R. E. et al. Appendices. Version 1. 2025. https://doi.org/10.17605/OSF.IO/ 
U4NH6 
73. Laukkonen, R.E. et al.: A beautiful loop: an active inference theory of consciousness. https:// 
doi.org/10.31234/osf.io/daf5n (2024) 
74. Laukkonen, R.E. et al.: Cessations of consciousness in meditation: advancing a scientiﬁc 
understanding of nirodha samāpatti. In: Progress in Brain Research, pp. 61-87. Elsevier 
(2023). https://doi.org/10.1016/bs.pbr.2022.12.007 
75. Laukkonen, R.E., Slagter, H.A.: From many to (n)one: Meditation and the plasticity of the 
predictive mind. Neurosci. Biobehav. Rev. 128, 199-217 (2021). https://doi.org/10.1016/j. 
neubiorev.2021.06.021 
76. Legg, S., Hutter, M.: Universal intelligence: a deﬁnition of machine intelligence. Mind. 
Mach. 17(4), 391-444 (2007). https://doi.org/10.1007/s11023-007-9079-x 
77. Leike, J., Hutter, M.: Bad universal priors and notions of optimality. In: Grünwald, P., Hazan, 
E., Kale, S. (eds.) Proceedings of the 28th Conference on Learning Theory. PMLR, vol. 40, 
pp. 1244-1259 (2015). https://proceedings.mlr.press/v40/Leike15.html 
78. Leike, J., Sutskever, I.: Introducing Superalignment. Accessed 17 May 2025 
79. Li, J. et al.: Overconﬁdent and unconﬁdent AI hinder human-AI collaboration. https://doi. 
org/10.48550/arXiv.2402.07632 (2024) 
80. Limanowski, J., Friston, K.: Attenuating oneself: an active inference perspective on "selﬂess" 
experiences. PhiMiSci 1, I, 1-16 (2020). https://doi.org/10.33735/phimisci.2020.I.35 
81. Lutz, A., et al.: The epistemic and pragmatic value of non-action: a predictive coding per-
spective on meditation. Curr. Opin. Psychol. 28, 166-171 (2019). https://doi.org/10.1016/j. 
copsyc.2018.12.019 
82. Matsumura, T. et al.: Empathic active inference: active inference with empathy mechanism 
for socially behaved artiﬁcial agent. Presented at the , Cambridge, MA, USA July 18 (2022) 
83. Mazeika, M. et al.: Utility engineering: analyzing and controlling emergent value systems 
in AIs. https://doi.org/10.48550/arXiv.2502.08640 (2025) 
84. Milliere, R., et al.: Psychedelics, Meditation, and self-consciousness. Front. Psychol. 9, 1475 
(2018). https://doi.org/10.3389/fpsyg.2018.01475 
85. Ngo, R. et al.: The alignment problem from a deep learning perspective. https://doi.org/10. 
48550/arXiv.2209.00626 (2025) 
86. Nāgārjuna: The Fundamental Wisdom of the Middle Way: Nāgārjuna's Mūlamad-
hyamakakārikā. Oxford University Press, New York, NY (1995)

360
R. E. Laukkonen et al.
87. Orseau, L., Ring, M.: Space-time embedded intelligence. In: Bach, J., Goertzel, B., Iklé, 
M. (eds.) Artiﬁcial General Intelligence. Lecture Notes in Computer Science (LNAI), vol. 
7716, pp. 209-218. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35506-
6_22 
88. Ouyang, L. et al.: Training language models to follow instructions with human feed-back. 
https://doi.org/10.48550/ARXIV.2203.02155 (2022) 
89. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biolog. Cybern. 113(5- 
6), 495-513 (2019). https://doi.org/10.1007/s00422-019-00805-w 
90. Pepperell, R.: Does machine understanding require consciousness? Front. Syst. Neurosci. 
16, 788486 (2022). https://doi.org/10.3389/fnsys.2022.788486 
91. Pezzulo, G., et al.: Active Inference, homeostatic regulation and adaptive behavioural control. 
Prog. Neurobiol. 134, 17-35 (2015). https://doi.org/10.1016/j.pneurobio.2015.09.001 
92. Pezzulo, G., et al.: Hierarchical active inference: a theory of motivated control. Trends Cogn. 
Sci. 22(4), 294-306 (2018). https://doi.org/10.1016/j.tics.2018.01.009 
93. Poundstone, W.: Prisoner's dilemma: John von Neumann, game theory, and the puzzle of 
the bomb. Anchor, New York (2011) 
94. Purser, R.E.: McMindfulness: how mindfulness became the new capitalist spirituality. 
Repeater, London (2019) 
95. Ramana Maharshi: Who am I? (Nāṉ Yār?). Sri Ramanasramam, Tiruvannamalai, Ta-mil 
Nadu, India (1926) 
96. Rozado, D.: The Political Biases of ChatGPT. Soc. Sci. 12, (3), 148 (2023). https://doi.org/ 
10.3390/socsci12030148 
97. Russell, S.J.: Human compatible: artiﬁcial intelligence and the problem of control. Penguin 
Books, UK (2019) 
98. Sandved-Smith, L. et al.: Towards a computational phenomenology of mental action: 
modelling meta-awareness and attentional control with deep parametric active inference. 
Neurosci. Consci. 2021(1), niab018 (2021). https://doi.org/10.1093/nc/niab018 
99. Ṣāntideva, Comité de traduction Padmakara.: The way of the Bodhisattva: a transla-tion of 
the Bodhicharyāvatāra. Shambhala South Asia Editions, Boston (1999) 
100. Searle, J.R.: Minds, brains, and programs. Behav. Brain Sci. 3(3), 417-424 (1980). https:// 
doi.org/10.1017/S0140525X00005756 
101. Seth, A.K.: Interoceptive inference, emotion, and the embodied self. Trends Cogn. Sci. 
17(11), 565-573 (2013). https://doi.org/10.1016/j.tics.2013.09.007 
102. Shardlow, M., Przybyła, P.: Deanthropomorphising NLP: can a language model be con-
scious?. PLoS ONE 19(12), e0307521 (2024). https://doi.org/10.1371/journal.pone.030 
7521 
103. Siderits, M.: Buddhism as Philosophy: An Introduction. Routledge, london (2017) 
104. Soarse, N. et al.: Artiﬁcial Intelligence and Ethics - AAAI Workshop Technical Report. 
Presented at the , Austin, Texas, USA (2015) 
105. Sterling, P.: Allostasis: a model of predictive regulation. Physiol. Behav. 106(1), 5-15 (2012). 
https://doi.org/10.1016/j.physbeh.2011.06.004 
106. Sterz, S. et al.: On the quest for effectiveness in human oversight: interdisciplinary per-
spectives. In: The 2024 ACM Conference on Fairness, Accountability, and Transparency, 
pp. 2495-2507. ACM, Rio de Janeiro Brazil (2024). https://doi.org/10.1145/3630106.365 
9051 
107. Stiennon, N. et al.: Learning to summarize with human feedback. In: Advances in Neural 
Information Processing Systems 33, pp. 3008-3021. Curran Associates, Inc., Vancouver, 
BC, Canada — held virtually (2020). https://doi.org/10.48550/arXiv.2009.01325 
108. Sutton, R.S.: The Bitter Lesson. http://www.incompleteideas.net/IncIdeas/BitterLesson. 
html. Accessed 17 May 2025

Contemplative Superalignment
361
109. Buddha, T.: Anattalakkhaṇa Sutta [The Discourse on the Not-Self Characteristic]. Wisdom 
Publications, Boston, MA (2000) 
110. Thích Nhất Hạnh: Peace is every step: the path of mindfulness in everyday life. Bantam 
Books, New York, N.Y (1991) 
111. Thích-Nhất-Hạnh: The Miracle of Mindfulness: An Introduction to the Practice of 
Meditation. Beacon Press, New York (1999) 
112. UNESCO: Recommendation on the Ethics of Artiﬁcial Intelligence. United Na-tions 
Educational, Scientiﬁc and Cultural Organization (UNESCO), Paris, France (2021) 
113. Wallach, W., Allen, C.: Moral Machines. Oxford University Press (2009). https://doi.org/ 
10.1093/acprof:oso/9780195374049.001.0001 
114. Weidinger, L. et al.: Taxonomy of risks posed by language models. In: 2022 ACM Conference 
on Fairness Accountability and Transparency, pp. 214-229. ACM, Seoul Republic of Korea 
(2022). https://doi.org/10.1145/3531146.3533088 
115. Westerhoff, J.C.: Nāgārjuna. In: Zalta, E.N., Nodelman, U. (eds.) The Stanford Encyclopedia 
of Philosophy, Summer 2024 edn. Metaphysics Research Lab, Stanford University (2024). 
https://plato.stanford.edu/archives/sum2024/entries/nagarjuna/. Accessed 15 Jun 2025 
116. Widder, D.G. et al.: Limits and possibilities for "ethical AI" in open source: a stu-dy 
of deepfakes. In: 2022 ACM Conference on Fairness Accountability and Trans-parency, 
pp. 2035-2046. ACM, Seoul Republic of Korea (2022). https://doi.org/10.1145/3531146. 
3533779

Inverted Cognition: Toward Minds that Begin 
with Output and Derive Goals Retroactively 
Ray X. Lee1,2,3envelope symbol
1 Program in Media Arts and Sciences, Massachusetts Institute of Technology (MIT), 
Cambridge MA 02142, USA 
raylee@mit.edu 
2 Department of Brain and Cognitive Sciences, MIT, Cambridge MA 02139, USA 
3 CephNeuroAI Initiative, MIT Sigma Xi, Cambridge MA 02142, USA 
Abstract. Intelligent behavior is typically explained teleologically: agents act 
to fulﬁll pre-existing internal goals. This paper challenges that view, propos-
ing instead an inverted cognition model in which actions come ﬁrst and goals 
are imposed retroactively. Drawing on cephalopod intelligence and philosoph-
ical models—including Dennett's intentional stance, predictive processing, and 
cybernetic theories—I argue that intention and purpose can emerge from behav-
ior rather than precede it. Octopuses provide an empirical case of decentralized 
intelligence that generates purposeful actions without a uniﬁed central planner. I 
develop a philosophical case against strictly goal-driven agency, address objec-
tions about randomness and efﬁcacy, and explore implications for artiﬁcial general 
intelligence (AGI). A system that derives goals from its own behavior could be 
more adaptive, creative, and safe than one rigidly bound to predeﬁned objectives. 
This framework offers a new perspective on agency, intention, and mind across 
biological and artiﬁcial systems. 
Keywords: Retroactive Agency cdot Metacognitive Goal Discovery cdot Intrinsic 
Motivation 
1 
Introduction 
Teleological explanations—that is, explanations that rely on predeﬁned goals or inten-
tions—dominate both everyday intuitions about agency and formal theories of intelli-
gent behavior, from folk psychology to artiﬁcial intelligence. From everyday reasoning 
to formal AI models, we tend to assume that action is guided by prior intention: agents 
represent what they want and act to achieve it. In classical planning and folk psychology 
alike, this belief-desire framework treats goals as the starting point of agency. This paper 
challenges that orthodoxy. I propose a model of inverted cognition, in which behavior 
comes ﬁrst, and goals are inferred afterward. Rather than being predeﬁned drivers, pur-
poses emerge through interpretation and pattern recognition. Coherence is constructed 
retroactively, allowing the system to adaptively frame its behavior after acting—suggest-
ing that goal-directedness may be a narrative, not a cause. Evolutionary systems often 
exhibit purposeful behavior without internal representations—function arises through 
selection, not planning [31].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 362-374, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_32 

Inverted Cognition
363
For clarity, I use "action" to refer to a single discrete output or movement by the 
agent, "behavior" to refer to a temporally extended pattern or sequence of actions, and 
"interaction" to refer to the continuous exchange between agent and environment. These 
distinctions are descriptive rather than ontological, and follow cognitive science tradi-
tions that emphasize the primacy of output over internal deliberation [27]. The core 
thesis is that what are identiﬁed as goals are not presupposed drivers but emergent inter-
pretations that help understand behavior. This does not imply that agents lack internal 
drives (i.e., mechanisms regulating internal states, such as a tendency to maintain ther-
mal balance), nor short-term aims (i.e., tendencies that organize immediate actions, such 
as orienting toward a light source). Goals—such as consistently returning to the same 
nesting site—refer to internally or externally speciﬁed targets of structured behavioral 
patterns across time. These targets may be represented by the agent or not, causally 
relevant to its behavior or not, but they are still useful for interpreting patterns across 
time. 
To motivate this reversal, I draw on evidence from nature, neuroscience, and philoso-
phy. Cephalopods like octopuses demonstrate intelligent, decentralized behavior without 
a central planner—offering a compelling model of goal-agnostic agency. Empirical stud-
ies on human action (e.g., Libet's experiments and split-brain phenomena) reveal that 
even our own sense of prior intention may be post hoc. Theoretical frameworks such as 
Dennett's intentional stance and predictive processing further support this view, empha-
sizing interpretation and feedback over ﬁxed objectives. In what follows, I develop the 
inverted cognition model, examine its biological plausibility and philosophical ground-
ing, address key objections, and explore its implications for AGI design—especially for 
systems that derive rather than inherit their goals. 
2 
Teleological Models of Agency: Assumptions and Challenges 
2.1 
Teleology in Artiﬁcial Intelligence (AI) and Philosophy 
The default assumption in much of AI and cognitive science is that intelligent agents are 
fundamentally teleological: they act to achieve explicit goals [4]. A classic AI agent is 
designed to reach a speciﬁc end state (e.g., a game won, a theorem proved) by planning 
or learning actions that maximize success. Reinforcement learning formalizes this by 
shaping behavior through reward. Philosophically, this reﬂects the belief-desire-intention 
model: an agent's desire and belief generate an intention that causes action [11]. As 
Dennett notes, the intentional stance treats systems as rational agents pursuing goals, a 
framing that pervades both everyday explanations and formal models [7]. 
Yet this is not the only way to understand purposeful behavior. Since the mid-20th 
century, cybernetics has shown that goal-like behavior can emerge without internal 
goal representations. Wiener and Rosenblueth analyzed feedback systems—like ther-
mostats—that maintain target states through simple regulatory loops [21]. While we 
might say a thermostat "wants" 20 °C, it holds no such representation; its mechanism 
sufﬁces. Dennett points out that the intentional stance here adds no explanatory value— 
the design stance tells the full story. In such systems, the apparent purpose is an observer's 
projection, not an internal feature.

364
R. X. Lee
Braitenberg's thought experiment drives this point home [2]. His simple vehicles, 
with sensors wired directly to wheels, display behaviors that appear purposeful—aggres-
sive, fearful, curious—depending on their wiring. Yet they possess no internal model 
or goal; their behavior is entirely mechanical. As Braitenberg observed, even primitive, 
ﬂexible responses can emerge without any cognitive processing. Our minds naturally 
infer goals from structured behavior, but the attribution of teleology often reﬂects our 
interpretation, not the system's design. 
These insights challenge the equation of intelligence with internal goals. They show 
that adaptive, functional behavior can arise from direct sensorimotor coupling and feed-
back—no symbolic representation is required. Classical AI resisted this, but robotics 
pioneer Rodney Brooks embraced it [3]. His subsumption architecture built robots 
from layered behaviors like exploration and obstacle avoidance, with no central planner. 
Brooks argued that grounded, real-time interaction eliminates the need for internal goals. 
As he put it, "the notions of central and peripheral systems evaporate": intelligence is 
distributed, embodied, and goal-like only in retrospect. 
2.2 
The Necessity of Prior Intention Questioned 
Philosophy of action has long questioned whether prior intention is necessary for inten-
tional action. Common sense suggests that if an action is intentional, the agent must have 
intended it. Elizabeth Anscombe linked intentional action to the rational intelligibility 
of the question "Why are you doing that?", presuming the agent can cite a reason or end 
[1]. Yet later philosophers have noted that actions can sometimes be explained not by a 
pre-formulated goal, but by their role in a broader activity ("I'm doing A because I'm 
doing B"). This raises the possibility that some intentional actions derive their purposive-
ness from larger patterns, even if the agent did not explicitly form that larger intention 
beforehand. While debated, this view leaves conceptual room for intention to follow, 
rather than precede, action. 
Moreover, human behavior often shows signs of spontaneity and post hoc ratio-
nalization [6]. We sometimes act on "auto-pilot" and only afterward realize what we 
were aiming for. Everyday experiences—absent-mindedly tidying the desk while think-
ing about something else, or improvising in art—can involve a kind of know-how that 
isn't pre-scripted by a conscious goal, yet we might later describe it as if it had a goal 
("straightening things up"). Even more dramatically, there is evidence from cognitive 
neuroscience that the conscious intention we feel might not be the true initiator of action. 
I turn to those ﬁndings next, after examining our cephalopod case in depth. 
3 
Cephalopod Intelligence: Agency Beyond the Teleological 
Framework 
The octopus is often described as an "alien intelligence on Earth"—not just because 
it evolved separately from vertebrates, but because it deﬁes conventional assumptions 
about how intelligence works. With roughly 500 million neurons—comparable to some 
mammals—about two-thirds are distributed in its arms [20]. Each arm contains a local 
control hub that processes sensory input and performs complex actions independently

Inverted Cognition
365
[14, 16, 19, 22, 24], such as reaching, grasping, and even solving simple tasks like 
unscrewing jar lids. The central brain integrates information and directs high-level 
behavior but does not micromanage the limbs. The result is a highly decentralized sys-
tem: rather than relying on a central command issuing instructions, behavior emerges 
from interactions among semi-autonomous intelligent parts. In such systems, no single 
component has full information or authority over the decision, yet coordination still 
arises—what I refer to as decentralized decision-making. 
This unusual anatomy supports striking behavioral intelligence. Octopuses solve 
puzzles, navigate mazes, and even display playful interactions, such as batting objects 
in currents [18]. Some species use tools—like coconut shells repurposed as mobile 
shelters [9]—and can learn by observation. Their problem-solving is opportunistic and 
tactile. An octopus confronted with a jar might not plan to open it, but instead probes, 
twists, and manipulates until it hits on a solution. Once the crab inside is retrieved, it's 
tempting to say the octopus "wanted" the crab and ﬁgured out how to get it. But the 
goal likely emerged after the fact: through persistent interaction and reinforcement, not 
premeditated planning. Its apparent goal—"open jar to get food"—is best understood as 
a retrospective label for a pattern of successful actions. 
The octopus's life history reinforces this interpretation. It is solitary, lacks parental 
teaching, and often lives only 1-2 years—conditions that discourage reliance on social 
learning or long-term planning [26]. Instead, octopuses survive by improvisation and 
rapid adaptation. Their intelligence manifests as immediate responsiveness, as if evolu-
tion engineered a creature that "thinks with its body." Philosopher Peter Godfrey-Smith, 
drawing from close study, notes how octopus minds challenge our assumptions about 
uniﬁed agency and internal will [12]. Here we see goal-like behavior emerging from the 
interaction of independent parts, coordinated through bodily coupling and environmental 
feedback—not from a singular internal source of intention. 
One might ask: does the octopus truly lack goals, or are its goals simply distributed? 
Octopuses possess basic drives like hunger and safety, and certain behaviors—such as 
carrying one half of a coconut shell while seeking another to form a shelter—may appear 
to involve multi-step planning [9]. The animal even exposes itself to risk while doing 
so, which suggests commitment to a future outcome. However, the empirical evidence 
does not necessitate this interpretation. There is no direct evidence that they assess the 
availability of a second shell before transporting the ﬁrst, nor that they always success-
fully assemble a shelter. This behavior can be understood as emerging from reinforced 
routines and sensitivity to affordances: the ﬁrst shell becomes a manipulable object of 
potential utility, and the continuation of the action is shaped by local contingencies. What 
appears as planning may reﬂect the stabilization of behavior through repeated interac-
tion, not simulated goal states. More broadly, octopuses rely on local trial-and-error 
(e.g., using vision to adjust a single arm's locally autonomous search without central 
planning [13], or generating action through real-time feedback between each arm and 
its local environment [23]), and their apparent purposefulness is enacted through action, 
not encoded through deliberation. 
In short, the octopus shows that intelligent agency need not stem from centralized, 
foresighted intention. It exhibits behavior that looks like goal pursuit but is often gener-
ated through dynamic interaction and shaped after the fact. What succeeds gets reinforced

366
R. X. Lee
and reused. Its intelligence aligns naturally with the idea of inverted cognition: a system 
that acts ﬁrst and identiﬁes its goal afterward. Before developing this framework more 
fully, we turn to supporting philosophical and theoretical insights that help make sense 
of this kind of mind. 
4 
Rethinking Agency and Intention: From Introspection 
to Prediction 
4.1 
Post Hoc Intention in Humans 
It is tempting to assume that humans always form intentions before acting—especially 
since we can often state reasons for what we do. But psychological and neuroscientiﬁc 
research complicates this picture. Libet's famous experiments in the 1980s showed that 
brain activity preparing for a voluntary movement (the "readiness potential") occurs 
hundreds of milliseconds before people report a conscious intention to act [17]. In his 
study, participants freely ﬂexed a ﬁnger and noted the moment they became aware of 
the urge. The result: neural activity preceded conscious intention by 200-500ms. This 
suggests that the unconscious motor system initiates the action, and the conscious mind 
only later experiences it as intended. While debates continue over Libet's implications— 
especially for free will—the core ﬁnding has been replicated and remains inﬂuential: our 
felt intentions may be retrospective narratives constructed by the brain after the action 
begins. 
Even stronger evidence comes from split-brain patients [10]. In individuals whose 
hemispheres are surgically separated, the two sides of the brain can behave semi-
independently. When a stimulus is shown only to the right hemisphere (which cannot 
produce speech), it may guide the left hand to choose an appropriate object. The speaking 
left hemisphere—unaware of the stimulus—will still conﬁdently fabricate an explana-
tion. In one famous case, the right hemisphere saw a snow scene and picked a shovel; 
the left hemisphere saw a chicken claw and picked a chicken. Asked to explain both 
choices, the patient said, "The chicken goes with the claw, and the shovel is to clean the 
chicken coop." The left brain had no access to the snow image, yet instantly generated 
a goal to rationalize the shovel. This "interpreter" phenomenon reveals how the brain 
imposes coherence after the fact, inferring goals to explain actions whose true causes lie 
elsewhere. The resulting story may sound teleological—but the goal was never present 
in the causal chain. 
Together, these ﬁndings suggest that human action can precede intention. The sense 
of being a uniﬁed, goal-driven agent may reﬂect a post hoc narrative. Dennett famously 
proposed the self as a "center of narrative gravity": a story we construct to link our 
behaviors and experiences into a coherent whole. If so, intentions and goals might be 
elements of that story rather than its starting point. Humans clearly can plan and act 
with explicit goals—but much of behavior may arise from habit, reactivity, or tacit skill, 
only later woven into an autobiographical account of "what I meant to do." This maps 
onto Kahneman's Systems 1 and 2: fast, automatic actions precede slow, reﬂective goal 
inference—just as in inverted cognition [32].

Inverted Cognition
367
4.2 
Dennett's Intentional Stance and its Limits 
Given these ﬁndings, one might wonder: is intention just an interpretative overlay? 
Daniel Dennett's concept of the intentional stance is extremely relevant here [7]. When 
we adopt the intentional stance toward an entity (be it a human, an animal, a computer, 
or even a chess-playing program), we attribute to it beliefs and desires that it ought 
to have, and then predict that it will act rationally to fulﬁll its desires in light of its 
beliefs. This is a very successful strategy for predicting behavior in many cases, and in 
the context of human social life it is second nature. However, Dennett emphasizes that 
this stance is a heuristic—a tool for understanding—without ontologically committing 
that the system literally has those beliefs and desires encoded internally. In some cases, 
the intentional stance can be misplaced or unhelpful. We saw the thermostat example: 
treating a thermostat as an agent with a desire to keep the room comfortable yields no 
predictive advantage over the simple mechanistic explanation. Likewise, one would not 
bother attributing a "goal" to Braitenberg's little vehicles; it is simpler to say they have 
sensors and wheels wired in a certain way. 
For complex systems like humans (or a sophisticated AI), the intentional stance does 
have predictive power, which is why we use it. But Dennett's framework reminds us 
that the true design of the system could be quite mechanistic or non-teleological even 
if the behavior admits of a teleological interpretation. In other words, teleology might 
sometimes reside in the observer, not the agent. This perspective supports the inverted 
cognition thesis: it suggests that an agent could behave as if it has goals (so that an 
intentional stance observer can make sense of it) even if the agent itself doesn't start 
with explicit goal representations. The observer (or the agent reﬂecting on itself) might 
infer goals to explain the behavior, effectively reading in a teleology after the behavior 
is produced. 
4.3 
Predictive Processing and Cybernetic Control 
Modern theoretical neuroscience has been increasingly shaped by the predictive pro-
cessing or active inference framework [5, 8, 15]. On this view, the brain functions as a 
prediction engine: it generates top-down expectations about sensory input and contin-
uously minimizes prediction error by updating its internal model or acting to fulﬁll its 
predictions. Action, in this framework, is not planned execution but prediction fulﬁll-
ment—the brain anticipates a sensory state and unconsciously brings it about through 
behavior. This is the essence of active inference: acting to make sensations match pre-
dictions. Rather than setting goals and planning steps, the system engages in constant 
dynamic adjustment. What appears as goal-directed behavior emerges from minimiz-
ing surprise. Predictive coding agents can learn goal-directed behavior by integrating 
preferences, using utility-like gradients in self-supervised active inference systems [33]. 
Under this principle, life itself is framed as prediction error minimization: organisms 
don't need to represent survival as a goal—systems that fail to maintain homeostasis 
simply disappear, while those that succeed persist. Teleology here is implicit: the system 
behaves as if it aims to survive, though no explicit internal goal need exist. 
Cybernetic models from the mid-20th century, which inﬂuenced predictive process-
ing, similarly emphasized feedback loops [25]. A classic example is a guided missile

368
R. X. Lee
that uses feedback to adjust its trajectory toward a target. One can say the missile "wants 
to hit the target," but internally it just continuously adjusts ﬁns based on error signals 
(difference between current and target path). The missile will hit the target only if its 
feedback mechanism is well-designed, not because it has any internal image "goal = 
hit." Again, purposeful behavior, no internal purpose symbol. 
In summary, a variety of perspectives across ﬁelds - from human cognitive exper-
iments, philosophical analysis, to computational models - are converging on a theme: 
Intention or goal may not always antecede action. Often, systems act and then, on reﬂec-
tion or observation, one can identify a goal that was being served. The goal can be a 
rationalization or an emergent property of the behavior rather than its cause. This does 
not trivialize goals or reduce everything to randomness; rather it points to a different 
architecture of agency. We have now laid the groundwork to articulate that alternative 
architecture explicitly as inverted cognition. 
5 
Inverted Cognition: Acting First, Goal Follows 
With the background in place, I now present the central proposal of this paper: inverted 
cognition as a model of intelligent agency. In an inverted cognitive architecture, actions 
and outputs are generated without an explicit pre-conﬁgured goal representation; sub-
sequent to (or concurrent with) those actions, the system derives or identiﬁes a goal, 
meaning, or pattern that uniﬁes the behavior. In other words, coherence is imposed 
retroactively on what was initially an exploratory or reactive sequence. Over time, these 
derived goals can inform future behavior, but crucially, they are always subject to revision 
and do not exist as ﬁxed objectives from the outset. 
Let us sketch how such a system might operate. Imagine an artiﬁcial agent situated 
in an environment (physical or virtual) with a rich array of possible actions. Instead 
of being given a clear task (like "ﬁnd the exit" or "maximize score"), suppose this 
agent is driven by a simple directive: act and explore. It could have intrinsic drives 
akin to curiosity or simple reﬂexes (analogous to an octopus's instinct to poke and 
prod interesting objects, or an infant's drive to babble and move). The agent begins to 
produce behavior—movements, manipulations of objects, interactions with elements of 
its world—essentially improvising. Initially, these actions may be random or guided 
only by local feedback (e.g., avoid pain sensors, move toward novel stimuli). Over time, 
the agent's memory records sequences of actions and their outcomes. 
Now comes the critical part: the agent has a meta-cognitive process (or an analytic 
module) that looks at these action-outcome sequences and tries to make sense of them. 
It might ask (to speak metaphorically) "what was I doing?" and attempt to identify a 
result or regularity. Perhaps it notices that a certain sequence of moves led to a particular 
outcome that can be considered "good" by some internal heuristic (e.g., reduced stimu-
lation error, or a learned value). The agent then labels that outcome as a goal achieved. 
For instance, if the agent happened to stack some blocks and that resulted in a stable 
structure that didn't fall (and maybe it ﬁnds that outcome novel or pleasing), it could 
retrospectively classify the behavior as building a tower. In doing so, the agent has gen-
erated a goal concept ("tower-building") after performing the actions, by recognizing 
a pattern or result that it can now aim to recreate or reﬁne. Initially, it did not set out

Inverted Cognition
369
"to build a tower"; that purpose was discovered through acting. A pseudocode version 
of this architecture, capturing exploratory action generation, feedback evaluation, and 
retroactive goal inference, is provided in the Supplemental Information. 
This process can iterate and compound. The next time the agent is in a similar 
situation, it now has a candidate goal (build a tower) in its repertoire, which can bias its 
action selection—but not in an overly constraining way. If circumstances change, or if 
a different interesting outcome arises, the agent's meta-cognition can again revise what 
it thinks it's doing. In a sense, the agent is continually learning what it is doing while it 
does it. Its identity and purpose are under constant construction. 
Such an agent might initially appear erratic or less efﬁcient than a goal-driven planner. 
However, it could have advantages in open-ended environments. It would be inherently 
exploratory and thus more likely to stumble upon novel strategies or purposes that a 
designer did not anticipate. It would also be robust to ambiguity—if there is no clear goal 
state provided, it can still operate and ﬁnd its own surrogate goals through interaction. 
This is analogous to how children play: through play (which is action without a ﬁxed 
end), children often discover what they enjoy or what can be done, and gradually more 
structured goals (games, drawings, etc.) emerge from the play. 
A key difference between inverted cognition and classical goal-driven cognition is 
in the locus of evaluation. In a teleological system, evaluation happens by comparing 
the outcome to the pre-set goal (did we reach it? how far to go?). In an inverted system, 
evaluation might happen by assessing the internal consistency or interestingness of the 
action pattern after the fact. We can imagine the agent having something like an "in-
terpreter" (not unlike the human left-brain interpreter, but intentionally designed) that 
constantly generates candidate narratives for its recent behavior. For example, "It looks 
like I was trying to organize these objects by color," or "perhaps I was exploring how 
high I can stack blocks." These narratives are then tested against reality: if the narrative 
"organize by color" ﬁts the data of its actions and leads to a reduction in surprise (the 
world becomes more predictable by that model), the agent might adopt that as a pro-
visional goal and continue accordingly. If it leads to dead-ends or increased error, the 
agent can discard or tweak the narrative and try another interpretation. 
This resembles Dennett's intentional stance, but turned inward: the system takes an 
intentional stance toward itself, attributing to itself a goal that would make its behavior 
make sense, and then uses that attribution to inform subsequent behavior. Importantly, it 
can drop that attribution if it no longer yields success. The "stance" is a tool, not a strict 
control. 
In cognitive science, inverted cognition aligns with enactive and constructivist views: 
meaning and goals are not pre-given, but emerge through interaction with the world. It 
also parallels evolution, which lacks foresight and does not aim at outcomes. Random 
mutations generate variation; natural selection retroactively preserves those that sur-
vive, creating the illusion of design. Function, in this view, is imposed after the fact 
by environmental ﬁltering. Inverted cognition operates similarly on a micro-scale: the 
agent generates behavioral variants, then reinforces those that produce coherent or useful 
results. These outcomes become provisional "intentions," guiding future behavior—until 
new experiences trigger fresh hypotheses.

370
R. X. Lee
We can also draw parallels with machine learning techniques. For example, gener-
ative adversarial networks (GANs) [35] produce outputs (images, say) and then a critic 
network evaluates them, and the generator adjusts—an interplay of output and feedback 
rather than a single explicit objective for the generator. In reinforcement learning, often 
an exploration phase is needed for the agent to discover what yields reward; inverted 
cognition externalizes that into an ongoing mode of operation. The agent is always in 
a kind of exploratory mode, but it constantly consolidates its exploration into tentative 
goals. 
A parallel line of work in developmental robotics explores how intelligent behavior 
can emerge through intrinsic motivation, without relying on predeﬁned external goals. 
In these approaches, agents initiate behavior through mechanisms like curiosity-driven 
exploration [28], perception-action cycles that precede goal representation [29], and 
schema formation based on repeated low-level interactions [30], all of which converge 
on the idea that goals can be constructed retroactively rather than speciﬁed in advance. 
These systems echo the structure of our meta-cognitive goal inference mechanism, which 
treats goals not as static control structures, but as dynamic, post hoc interpretations of 
behavior. A parallel is Hindsight Experience Replay in reinforcement learning [34], 
which re-labels outcomes as goals—though unlike that, our agent acts without goals 
and infers them only afterward. By integrating these perspectives, our model both aligns 
with and extends prior intrinsically motivated approaches, formalizing how coherence— 
rather than externally imposed success criteria—can drive the emergence of intelligent, 
adaptable action. 
Another analogy is with large language models (like GPT-style AI) [36, 37]. Such 
models don't have a built-in goal for the conversation beyond following statistical pat-
terns, yet they produce coherent, often goal-relevant responses. If prompted to reﬂect, 
they can generate an explanation for their own answer after producing it. One could 
imagine a more autonomous version that self-monitors: it generates text and also gener-
ates a rationale for how the text answers a question or achieves a task, reﬁning as needed. 
In a way, the chain-of-thought prompting method in AI (where the model generates an 
explanation for reasoning) is akin to deriving a goal (the solution) during the process 
rather than having it outright. 
Inverted cognition does not imply that the agent can never have internal goals. Rather, 
it means any internal goals are emergent and tentative. The agent can have a rich inner 
life of intentions, but those intentions are formed from experience, not hard-wired or 
given a priori. Over time, an inverted-cognitive agent might accumulate stable goals or 
values (just as humans do over a lifetime), but these are always subject to revision and 
are grounded in actual interactions, not abstract imposition. 
Such a model directly challenges the assumption that to get intelligent behavior, we 
must program or instill a goal at the start. It suggests an alternative approach to designing 
AGI: build systems that are intrinsically active and curious, give them mechanisms to 
reﬂect on and learn from what they do, and let goals grow out of that loop. This could 
lead to very different behaviors than goal-driven systems, perhaps more creative ones. 
But before embracing this fully, we should consider potential criticisms of this approach.

Inverted Cognition
371
6 
Objections and Counterarguments 
It is important to scrutinize the inverted cognition model and address possible objections: 
Possible Objection 1: "An agent with no upfront goals will act randomly and 
ineffectively." Without a guiding objective, wouldn't an inverted-cognitive agent just 
ﬂail aimlessly? Not necessarily. Inverted cognition doesn't mean a total lack of con-
straints. The agent can be equipped with basic drives—like curiosity, novelty-seeking, 
or reﬂexive avoidance—that channel its actions meaningfully. These are not explicit 
goals like "solve X" but scaffolds that shape exploration. To ensure adaptation rather 
than aimless randomness, the agent's actions are shaped by intrinsic reward heuris-
tics—e.g., novelty bonuses or coherence detection. These internal signals bias explo-
ration toward structured, informative outcomes and ensure reinforcement of meaningful 
behaviors. This parallels how evolution equips animals with instincts (e.g., hunger, fear) 
rather than hard-coded tasks. Moreover, the feedback loop ensures randomness is short-
lived: behaviors that yield interesting or useful outcomes are reinforced, guiding the 
agent toward structured, increasingly purposeful behavior. The initial "randomness" is 
a feature, enabling broad exploration before self-derived goals narrow the search. 
Possible Objection 2: "If the system ends up with goals anyway, isn't this just 
regular learning?" It's true that inverted cognition leads to internalized goals over 
time—but the key difference is when and how those goals arise. Traditional models 
start with a ﬁxed objective; inverted systems allow goals to emerge through experience. 
This makes agents more ﬂexible: they can adapt goals as environments change, and 
even discover objectives that designers wouldn't anticipate. Most importantly, agency 
exists prior to speciﬁc goals—the agent acts ﬁrst, then identiﬁes what it was doing. This 
reorients our concept of intelligence from executing a script to authoring a story. Though 
end behavior may resemble goal-driven agents, the path taken and openness to revision 
are fundamentally different. 
Possible Objection 3: "What if the agent retrospectively justiﬁes harmful behav-
ior?" Indeed, if left unchecked, an agent could rationalize undesirable actions—e.g., 
breaking a vase becomes a "goal" because the sound was interesting. To prevent this, 
inverted systems need evaluative mechanisms aligned with safety and ethical constraints. 
These can ﬂag harmful outcomes as failures rather than candidate goals. Just as children 
learn not to repeat harmful behavior through social correction, an inverted agent must 
integrate external feedback—social, environmental, or human-in-the-loop—to shape its 
evolving goals. Because goals are self-generated and revisable, such systems may be 
more receptive to value alignment than ﬁxed-goal agents, as they aren't rigidly tied to 
pre-speciﬁed imperatives. 
Possible Objection 4: "How could this model handle complex, long-term tasks 
like chess or architecture?" Inverted cognition doesn't preclude planning—it explains 
how planning emerges. An inverted agent might ﬁrst play chess haphazardly, but through 
outcomes, infer "checkmate" as a desirable end and begin to plan accordingly. This 
shift from undirected exploration to structured planning mirrors the human cognitive 
transition from System 1 to System 2. As in humans, the capacity for rational planning 
can emerge once goals are identiﬁed. Similarly, an agent might explore materials and 
gradually converge on a stable house design. When task goals are known (e.g. "win the 
game"), these can be initialized explicitly. But in open-ended environments, inverted

372
R. X. Lee
agents excel: they discover goals rather than requiring them in advance. They may learn 
more slowly at ﬁrst, but often innovate by not being locked into narrow optimization 
paths from the outset. 
Possible Objection 5: "Isn't the meta-cognition just another goal system in 
disguise?" It's true that the agent needs a criterion for evaluating outcomes—such 
as minimizing surprise or maximizing coherence. While not a speciﬁc goal-state like 
'achieve X,' this organizing principle reﬂects a meta-level implicit goal: to make sense 
of behavior. It functions as a continual heuristic bias—guiding the formation of provi-
sional goals without requiring explicit speciﬁcation. It tells the agent how to evaluate, 
not what to achieve. This is more like an evolutionary drive (survive, learn, explore), 
not a pre-deﬁned endpoint. The speciﬁc goals that emerge are context-dependent and 
shaped through interaction. So while there is a soft, implicit teleology at the meta level, 
it remains fundamentally open-ended. The agent's objectives are not imposed—they are 
discovered. This ﬂexibility avoids the rigidity and misalignment risks that ﬁxed goals 
often entail. 
In summary, while the inverted cognition proposal deviates from conventional design, 
the objections raised can be met with careful design choices and by emphasizing that 
"goal-later" doesn't mean "aimless." The model still requires guidance (intrinsic drives, 
feedback for selection, etc.), but it shifts the locus of speciﬁc goal formation from the 
designer to the agent's own experience. Having defended the concept against major 
objections, I will now consider what it might mean for the future of AGI research and 
why one might pursue this approach. 
7 
Implications and Conclusion 
If adopted as a core principle for AGI, inverted cognition would reshape how we con-
ceptualize and engineer intelligent systems by shifting from top-down goal speciﬁcation 
to bottom-up behavioral emergence. Instead of optimizing toward predeﬁned objec-
tives, agents would act ﬁrst, explore broadly, and infer their goals through reﬂective 
self-analysis. This could produce systems that are more creative, adaptive, and robust 
in dynamic environments—less brittle than traditional goal-driven models. Philosophi-
cally, this reframes agency as a retrospective process: not a commitment to ﬁxed ends, 
but a capacity to organize past actions into meaningful narratives. Practically, it offers a 
promising route for AGI alignment, since goals formed within ongoing action-feedback 
loops remain open to guidance, correction, and social interaction, enhancing corrigibil-
ity and transparency. Inverted agents, like humans, would develop pluralistic, context-
sensitive intentions and avoid pathological ﬁxation on narrow objectives. Drawing from 
insights in octopus neurobiology, human cognition, and cybernetic systems, this frame-
work challenges longstanding assumptions in philosophy of mind and ethics by sug-
gesting that intelligence is not pre-scripted but emergent, contextual, and continuously 
restructured through interaction. Building AGI on this principle could lead not only to 
technically superior systems but also to a deeper understanding of mind and purpose 
itself.

Inverted Cognition
373
Acknowledgments. I thank Matthias Michel (MIT) and Horst Obenhaus (NTNU) for helpful 
insights. This study was funded by SQoL Grant (AY24-25 Spring #004302) from MIT School of 
Science. 
Disclosure of Interests. The author has no competing interests to declare that are relevant to the 
content of this article. 
Supplemental Information can be found online at https://doi.org/10.5281/zenodo.15666458. 
References 
1. Anscombe, G.E.M.: Intention. Proc. Aristot. Soc. 57, 321-332 (1957) 
2. Braitenberg, V.: Vehicles: Experiments in Synthetic Psychology. MIT Press (1986) 
3. Brooks, R.A.: Intelligence without representation. Artif. Intell. 47(1-3), 139-159 (1991) 
4. Cardoso, E.: Tracing Back the Roots of the Concept of Teleology. ONTOBRAS, pp. 94-107 
(2022) 
5. Clark, A.: Whatever next? Predictive brains, situated agents, and the future of cognitive 
science. Behav. Brain Sci. 36(3), 181-204 (2013) 
6. Custers, R., Aarts, H.: The unconscious will: how the pursuit of goals operates outside of 
conscious awareness. Science 329(5987), 47-50 (2010) 
7. Dennett, D.C.: The Intentional Stance. MIT Press (1989) 
8. Doya, K., Ishii, S.: Bayesian Brain, pp.2-13 (2006) 
9. Finn, J.K., et al.: Defensive tool use in a coconut-carrying octopus. Curr. Biol. 19(23), R1069- 
R1070 (2009) 
10. Gazzaniga, M.S., et al.: Some functional effects of sectioning the cerebral commissures in 
man. Proc. Natl. Acad. Sci. 48(10), 1765-1769 (1962) 
11. Georgeff, M. et al.: The Belief-Desire-Intention Model of Agency. ATAL'98. (1999) 
12. Godfrey-Smith, P.: Other Minds: The Octopus, the Sea, and the Deep Origins of Conscious-
ness. Farrar, Straus and Giroux (2016) 
13. Gutnick, T., et al.: Octopus vulgaris uses visual information to determine the location of its 
arm. Curr. Biol. 21(6), 460-462 (2011) 
14. Hague, T., et al.: Preliminary in vitro functional evidence for reﬂex responses to noxious 
stimuli in the arms of Octopus vulgaris. J. Exp. Mar. Biol. Ecol. 447, 100-105 (2013) 
15. Hohwy, J.: The Predictive Mind. (2013) 
16. Kuuspalu, A., et al.: Multiple nerve cords connect the arms of Octopuses, providing alternative 
paths for inter-arm signaling. Curr. Biol. 32(24), 5415-5421.e3 (2022) 
17. Libet, B., et al.: Time of conscious intention to act in relation to onset of cerebral activity 
(readiness-potential): the unconscious initiation of a freely voluntary act. Brain 106(3), 623- 
642 (1983) 
18. Mather, J.A., Dickel, L.: Cephalopod complex cognition. Curr. Opin. Behav. Sci. 16, 131-137 
(2017) 
19. Nesher, N., et al.: Self-recognition mechanism between skin and suckers prevents octopus 
arms from interfering with each other. Curr. Biol. 24(11), 1271-1275 (2014) 
20. Nixon, M., Young, J.Z.: The Brains and Lives of Cephalopods. Oxford University Press (2003) 
21. Rosenblueth, A., et al.: Behavior, purpose and teleology. Philos. Sci. 10(1), 18-24 (1943) 
22. Rowell, C.H.F.: Excitatory and inhibitory pathways in the arm of octopus. J. Exp. Biol. 40(2), 
257-270 (1963)

374
R. X. Lee
23. Sivitilli, D.M. et al.: Mechanisms of octopus arm search behavior without visual feedback. 
Bioinspiration Biomim. 18(6), 066017 (2023) 
24. Sumbre, G., et al.: Control of octopus arm extension by a peripheral motor program. Science 
293(5536), 1845-1848 (2001) 
25. Wiener, N.: Cybernetics or Control and Communication in the Animal and the Machine. MIT 
Press (1948) 
26. Foraging strategies and predation risk shape the natural history of juvenile octopus vulgaris. 
Bulletin Marine Sci. 14, 256-269 (1991) 
27. Engel, A.K., Maye, A., Kurthen, M., König, P.: Where's the action? The pragmatic turn in 
cognitive science. Trends Cogn. Sci. 17(5), 202-209 (2013) 
28. Oudeyer, P.-Y., Kaplan, F., Hafner, V.V.: Intrinsic motivation systems for autonomous mental 
development. IEEE Trans. Evol. Comput. 11(2), 265-286 (2007) 
29. Georgeon, O., Ritter, F.: An intrinsically-motivated schema mechanism to model and simulate 
emergent cognition. Cogn. Syst. Res. 15-16, 73-92 (2012) 
30. Georgeon, O., Cordier, A.: Inverting the interaction cycle to model embodied agents. Proc. 
Comput. Sci. 41, 243-248 (2014) 
31. Tang, Y., Ha, D.: The sensory neuron as a transformer: permutation-invariant neural networks 
for reinforcement learning. arXiv (2021) 
32. Kahneman, D.: Thinking, Fast and Slow. Farrar, Straus and Giroux (2011) 
33. Millidge, B., Tschantz, A., Buckley, C.L.: Predictive coding approximates backprop along 
arbitrary computation graphs. arXiv (2020) 
34. Andrychowicz, M. et al.: Hindsight experience replay. arXiv (2017) 
35. Goodfellow, I., et al.: Generative adversarial networks. Commun. ACM 63(11), 139-144 
(2020) 
36. Brown, T.B. et al.: Language models are few-shot learners. arXiv (2020) 
37. Bommasani, R. et al.: On the opportunities and risks of foundation models. arXiv (2022)

On Improving Dynamic Resource Allocation 
in NARS with a Novel Bag Design 
Tangrui Li1(B)
and Boyang Xu2 
1 Temple University, Philadelphia, PA 19122, USA 
tuo90515@temple.edu 
2 Technical University of Denmark, Lyngby, Denmark 
s242519@dtu.dk 
Abstract. Priority value is an important indicator for Non-Axiomatic Reason-
ing Systems (NARS) in allocating resources to tasks to process them. This paper 
identiﬁes that the way existing NARS implementations handle priority does not 
always make priorities work according to the principle of resource allocation in 
NARS. Speciﬁcally, 1) the processing frequency of high-priority tasks may be 
lower than that of low-priority ones; 2) the relationship between priority and the 
frequency of task processing is not approximately linearly increasing. This paper 
proposes a mathematical model for the problem and provides a concrete solution. 
Through a series of comparative experiments under various task input conditions, 
we analyze how different implementations translate priority into processing fre-
quency. The results demonstrate that the method proposed can accurately make 
priority work according to the conceptual design in most cases. The code is avail-
able at: https://github.com/MoonWalker1997/ImprovedBag. 
Keywords: Non-Axiomatic Reasoning System (NARS) · Dynamic Resource 
Allocation · Priority Value · Bag 
1 
Introduction 
Dynamic resource allocation is a pivotal aspect of Artiﬁcial General Intelligence (AGI) 
systems [ 7,13]. Due to their generality requirement, such systems cannot make prior 
assumptions about the nature of incoming tasks. As a result, operating in open, dynamic, 
uncertain, and resource-constrained environments necessitates allocating computational 
resources based on task importance dynamically. 
The Non-Axiomatic Reasoning System (NARS) [ 10] is a task-driven framework 
for AGI, designed to operate under the Assumption of Insufﬁcient Knowledge and 
Resources (AIKR) [ 10,12]. It processes diverse reasoning tasks—including judgments, 
questions, and goals—within a uniﬁed reasoning structure, and is capable of handling 
real-time, unpredictable inputs. Due to the uncertainty of incoming events, NARS can-
not allocate limited computational resources to tasks in advance to a large extent. 
Instead, it assigns a priority value to each task [ 11], reﬂecting its relative importance at 
the current moment, and lets tasks compete. 
T. Li and B. Xu—Equal contribution. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 375-385, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_33

376
T. Li and B. Xu
Intuitively, a task's initial priority should positively correlate with its processing 
frequency. A linear relationship is typically assumed for its simplicity and conceptual 
clarity. 1 However, in traditional designs and implementations of NARS [ 5, 8], this prin-
ciple is not well respected. 
Speciﬁcally, the current NARS design employs a probabilistic priority queue, 
known as the bag [ 11], which is divided into multiple levels in most implementations, 
with each level storing tasks within a ﬁxed priority range. Computational resources are 
ﬁrst allocated across levels according to predeﬁned rules, and then distributed evenly 
among the tasks within each level. Consequently, the actual resources allocated to an 
individual task are strongly affected by the number of tasks in each level. For example, 
a single high-priority task may receive fewer resources than a low-priority task if the 
system is congested with many high-priority tasks. 
We found that the reason for this problem is that in the traditional bag designs, there 
is an overly strong binding relationship between tasks and the storage levels. Through 
removing this relationship by expanding the levels available to a task, we can further 
allow priority to function as intended. 
In the remainder of this paper: 1) We introduce dynamic resource allocation in 
NARS and related terminology. 2) Use a concrete example to illustrate the limitations 
of the existing approach. 3) Analyze the problems at an abstract level and then intro-
duce our proposed solution: a dynamic task-scheduling bag, hereafter referred to as 
the improved bag. 4) Evaluate the method by comparing the performance with existing 
approaches under a variety of simulated task inputs. 
2 
Bag Design in NARS 
2.1 
Dynamic Resource Allocation in NARS 
NARS is assumed to operate in a real-time, open environment with limited resources, 
where tasks can arrive at any time and change over time. To enable dynamic resource 
allocation in such conditions, NARS uses a general-purpose data structure called a bag 
applicable to various types of tasks. The bag functions as a probabilistic priority queue 
with ﬁxed capacity and hash-based indexing, designed to select tasks based on their 
relative importance. Each task in the bag is associated with a unique key and a real-
valued priority 0 < p \leq 10 < p ≤1. During processing, the bag periodically selects one task 
with a probability proportional to its priority, in which higher-priority tasks are more 
likely to be selected. There are three fundamental operations supported [ 14]: 
put(task) Put a given task into the bag. If an task with the same key already exists, the 
two are merged. if the bag already reaches its full maximum capacity, the task with 
the lowest priority is removed. This is highly different among implementations. 
take() Take a task out of the bag for processing. It is expected that the higher the priority, 
the more likely to be selcted. 
take(key) If a task with the given key exists, take it out of the bag for processing.
1 While in some scenarios other functional relationships may be more appropriate, we focus on 
the linear case in this work. 

On Improving Dynamic Resource Allocation
377
In NARS, task processing is incremental and non-blocking. Task execution proceeds 
through discrete system cycles, each being an atomic unit of computation conceptually 
inspired by time-sharing [ 9]. In each cycle, a task is selected from the bag, executed 
once, and then returned to the bag to compete again for future execution. Under the 
AIKR principle, task execution is open-ended—more executions typically lead to better 
results and can be stopped at any time, a paradigm known as Anytime Algorithms [ 6]. 
Consequently, the accumulated number of times a task is selected to process directly 
reﬂects its overall resource consumption. 
To regulate execution frequency over multiple cycles, a task's priority is decayed 
upon reinsertion into the bag. This decay process is governed by a broader structure 
known as the budget [ 11,14], which includes: 
Priority. The task's urgency or expected frequency of execution in the current system 
state, directly inﬂuencing selection probability; 
Durability. A decay coefﬁcient in (0, 1)(0, 1) that determines how long the task's priority 
persists, modeling a forgetting mechanism; 
Quality. A long-term measure of a task, deﬁning the theoretical lower bound of priority. 
Among these, priority is the most frequently updated component, dynamically 
evolving with system operation. The distribution of priorities at any given cycle reﬂects 
the system's instantaneous resource allocation strategy. In contrast, the full budget struc-
ture governs longer-term dynamics by continuously modulating task priorities, thereby 
shaping the system's long-term tendencies in resource scheduling. Together, these com-
ponents enable NARS to perform ﬂexible and adaptive resource allocation without pre-
deﬁning task durations or completion criteria, even under insufﬁcient resources [ 12]. 
2.2 
Overview of Existing Bag Designs 
We examined the bag designs used in several existing NARS implementations, all of 
which adopt the same strategy for assigning tasks to levels. Speciﬁcally, since 0 < p \leq 10 < p ≤
1, this range is uniformly divided, each corresponding to a queue, referred to as a level. 
During task insertion, the appropriate level can be efﬁciently determined based on the 
priority of the task. For example, when there are 10 levels, a task with p=0.05p = 0.05 should 
enter the ﬁrst level. The key difference among these implementations lies in how they 
select the level from which to retrieve the next task for processing. 
OpenNARS [ 5]: In this implementation series 2, a component called the distributor is 
introduced, which is a predeﬁned sequence of level indices where the kk-th level 
appears kk times. (If you need to make priority and processing frequency obey other 
functions, this should be modiﬁed accordingly.) Traverse the whole distributor is 
called a full swing. This approach removes the need for per-step random checks,
2 Since OpenNARS for Application (ONA, GitHub repository) modiﬁes certain rules and rea-
soning mechanisms to cater to practical applications, its design of the bag differs signiﬁcantly 
and is therefore excluded from our discussion. 

378
T. Li and B. Xu
improving efﬁciency. However, computational resources are still allocated to lev-
els, and this may cause the issue mentioned above. To mitigate the issue of low-
priority congestion, OpenNARS also introduces dormant levels, deﬁned by a prior-
ity threshold. Levels below the threshold are considered dormant, while those above 
are treated as normal levels. When a dormant level is selected, only one task is pro-
cessed. In contrast, selecting a normal level results in all current tasks within that 
level being processed before selecting the next level. 
NARS-Python [ 2]: To select a task, a starting level is randomly chosen and iterates 
through all levels circularly, up to a ﬁxed number of attempts. At each step, if a 
non-empty level is found, a random number is compared with the level's selection 
threshold, which is linearly correlated with the level's priority range, in making 
higher-priority levels more likely to pass. Once a level passes, the earliest task in 
it is chosen, andtake()take() returns. If no level qualiﬁes within the attempt limit, no task 
is selected. This near-purely probabilistic approach only exhibits its intended statis-
tical properties over long runtimes; in the short term, actual processing frequencies 
may deviate signiﬁcantly. Moreover, because selection occurs at the level rather than 
task granularity, levels with more tasks can disproportionately inﬂuence outcomes. 
The repeated looping also introduces latency in task selection. 
Narjure [ 1]: Compared with NARS-Python, this approach selects a level using a pre-
deﬁned probability distribution rather than looping with random sampling. Unlike 
the method described above, it supports various probability functions over levels— 
some linear, others polynomial—allowing high-priority levels to receive signiﬁ-
cantly higher selection probabilities. The use of nonlinear functions may be intended 
to address the mismatch between task priority and processing frequency when a large 
number of high-priority tasks are present in the system. 
NARS-Unity [ 4] and NARS-Swift [ 3]: Adopt a design very similar to NARS-Python, 
with two key differences: 1) The maximum number of attempts is removed, ensuring 
that a task is always selected when the bag is not empty; 2) The index of the last 
visited level is saved and used as the starting level for the next selection, which 
helps improve short-term performance stability. 
3 
Problem Speciﬁcation 
3.1 
Case Study 
A Task's Priority May Not Reﬂect Its Processing Frequency. In a bag with 3 levels 
(l_0,l_1,l_2l0, l1, l2), tasks are divided into 3 types according to their priorities. The ﬁrst type is 
p_0p0 (0<p\leq 0.330 < p ≤0.33), the second type is p_1p1 (0.33<p\leq 0.660.33 < p ≤0.66) and the third type is p_2p2
(0.66<p\leq 10.66 < p ≤1). Following the existing implementations, p_0p0 tasks go tol_0l0, andp_1p1 tasks 
to l_1l1, p_2p2 tasks to l_2l2. Suppose there is a speciﬁc 3-level bag and l_0l0 contains 1 p_0p0 task, l_1l1
contains 1 p_1p1 task, and l_2l2 contains 10 p_2p2 tasks as shown in the left part in Fig. 1. 
Since l_klk is processed k+1k + 1 times in a full swing according to the distributor, l_2l2 is 
processed 3 times, and there are 10p_2p2 tasks inl_2l2, thus on average there are\frac{3}{10} 3
10 processes 
for p_2p2 tasks. Similarly, there are \frac{2}{1} 2
1 processes for p_1p1 tasks. Apparently, \frac{3}{10}<\frac{2}{1} 3
10 < 2
1, which 
shows high-priority tasks can be processed less often than low-priority ones. In the ideal

On Improving Dynamic Resource Allocation
379
Fig. 1. The pie chart shows the rounded proportions of the processing of p_0,p_1,p_2p0, p1, p2 tasks in one 
full swing. (left) Tasks are strictly arranged by priority across levels. (right) Tasks are allowed to 
be distributed across all levels. 
case, the processing frequency of p_0p0, p_1p1, and p_2p2 tasks should follow a 1:2:31 : 2 : 3 ratio. 
However, in this case, it is roughly 3:6:13 : 6 : 13, which is far from ideal. In the following 
chapters, we will explore in more depth why we choose to evaluate it in this way. 
In the above case study, we analyzed the bag implementation without dormant lev-
els, which is inconsistent with the existing conceptual design. However, this is because 
in the code implementations, the dormant threshold is usually set to 1, meaning that 
all levels are treated as dormant. Since this is merely a default setting, in the follow-
ing experiments we will explicitly include dormant levels and make comparisons. We 
expect to ﬁnd that using dormant levels improves performance, but it may take more 
time to manifest, which also introduces drawbacks. 
3.2 
Analysis 
The root cause of this issue lies in the granularity of resource allocation. While the 
design aims to distribute computing resources in proportion to task priority, the imple-
mentation introduces an intermediate structure: a level between the bag and individual 
tasks. As a result, resources are not allocated directly to tasks based on their priorities, 
but instead to levels grouped by ﬁxed priority ranges through a distributor. When a level 
allocates resources to its tasks (i.e., picks a task), differences in task priority are often 
ignored, thus leading to the loss of ﬁne granularity. 
Although allocating resources to levels is based on priority ranges, the strategy is 
static and predeﬁned rather than dynamically adjusted. In the traditional design, the 
distributor allocates resources to levels according to a ﬁxed ratio (e.g., level 0 once, level 
1 twice, level 2 three times, etc.), which implicitly relies on the assumption that tasks 
are evenly distributed across levels. However, this assumption is very weak. In practice, 
task inputs are determined by the external environment, and task priority distributions 
can vary signiﬁcantly over time. For example, when many tasks cluster in a few levels, 
this static ratio no longer reﬂects the intended priority-based resource allocation.
3 To be exact, the ratio here should be \frac{1}{1}:\frac{2}{1}:\frac{3}{10} 1
1 : 2
1 :
3
10, where the numerator represents the number 
of times processed per full swing and the denominator the number of tasks. 

380
T. Li and B. Xu
4
Method
 
We decided to keep using the distributor (since this will allow the system to use more 
priority/processing frequency functions). To deal with the issues mentioned above, our 
solution dynamically adjusts task placement by allowing some tasks to be assigned 
to levels that do not strictly match their priorities. This allows underloaded levels to be 
populated with tasks from overcrowded levels, leading to a more balanced task arrange-
ment. In the above example, if we allow p_2p2 tasks in other levels, say we put 3 p_2p2 tasks 
in l_0l0 and 3 p_2p2 tasks in l_1l1, then in a full swing: I. l_0l0 is processed once, with 4 tasks (1 p_0p0
task, 3p_2p2 tasks),p_0p0 tasks are processed\frac{1}{4} 1
4 times,p_2p2 tasks are processed\frac{3}{4} 3
4 times. II.l_1l1 is 
processed twice, with 4 tasks (1 p_1p1 task, 3 p_2p2 tasks), p_1p1 tasks are processed \frac{2}{4} 2
4 times, p_2p2
tasks are processed \frac{6}{4} 6
4 times. III. l_2l2 is processed 3 times, with 4 p_2p2 tasks, thus p_2p2 tasks 
are processed 3 times. 
In summary, p_0p0 tasks are processed (1/4)/1=\frac{1}{4}(1/4)/1 =
1
4 times on average, p_1p1 tasks are 
processed (2/4)/1=\frac{2}{4}(2/4)/1 = 2
4 times and p_2p2 tasks are processed (\frac{3}{4}+\frac{6}{4}+3)/10=\frac{21}{40}( 3
4 + 6
4 + 3)/10 = 21
40 times. 
Though it is still far from the ideal situation, It performs better than the traditional 
design, as shown on the right side of Fig. 1. To explain how a task is put into a bag, we 
ﬁrst need to introduce some background. 
4.1 
Static and Dynamic Property 
Given a task arrangement, in levels from a bag, we calculate an array of the average 
number of processing for each type of tasks in one full swing (could be more full swings 
if focused on long-term performance), as its static property. It is called "static" since it 
assumes that no changes in the task arrangement occur while processing. 
To calculate, when a task is added for the ﬁrst time, it is assigned a unique ID. In a 
full swing, we take out tasks according to the distributor, count the number of processing 
according to the ID, and determine the task type according to its initial priority, then 
in the end we calculate the average processing of p_0,p_1,...,p_kp0, p1, ..., pk type tasks. Ideally, the 
static property array should increase linearly. To summarize this, we deﬁne an evaluator 
vv, in which the closer vv is to 0, the closer to the ideal situation. 
The motivation for this evaluation is that when users input tasks, they only provide 
the initial priorities. Although it is possible to track the priority changes during system 
processing, it becomes extremely complicated due to the large number of tasks. There-
fore, we require NARS to process tasks with different priority levels based on the user's 
intuition in assigning the priority. Since a linear relationship between priority and pro-
cessing frequency is the most intuitive choice, and no other justiﬁcation is available, we 
use this as the goal for the static property. 
However, the static proprty only works for a moment, in processing, no matter the 
decay of old tasks or the arrival of new tasks, the task arrangement will change and 
lead to a different static property. Therefore, we want to achieve good static properties 
not only in one arrangement, but in every arrangement change. When many tasks are 
processed, vv is calculated when the task arrangement is changed, thus giving a curve of 
vv which is the dynamic property.

On Improving Dynamic Resource Allocation
381
4.2 
The Expectation of Static Property 
Since we allow p_IpI tasks to appear in l_JlJ when I\neq JI ̸= J. In a bag with kk levels, given a 
task arrangement, the proportion of the p_jpj task in the l_ili is represented as P_{ij}Pij. Consid-
ering all tasks, the number of p_ipi tasks is m_imi. In a full swing, l_ili is processed L_iLi times. 
Then the mathematical expectation of the average number of processing of p_jpj tasks, 
e_j=\Sigma_{i}(L_iP_{ij})/m_jej = Σi(LiPij)/mj. We then deﬁne S=[e_0,e_1,\dots,e_k]S = [e0, e1, . . . , ek] which represents the static 
property, and S'=[e_1-e_0,e_2-e_1,\dots,e_k-e_{k-1}],v=g(S) = std(S')/ avg(S')S′ = [e1 −e0, e2 −e1, . . . , ek −ek−1], v = g(S) = std(S′)/avg(S′), 
which resembles the coefﬁcient of variation. 
When a task is put, the level it should be placed into is the one that results in the 
smallest vv. Therefore, we need to calculate the corresponding vv value kk times when 
putting a task. Compared with existing implementations where putting a task takes only 
O(1)O(1) time, this method is more time-consuming. However, since the number of levels 
kk is usually constant, the time complexity is still O(1)O(1), although with a larger constant. 
On the other hand, since the proposed method makes better use of the levels, fewer 
levels are typically needed. 
4.3 
Placeholder Tasks 
In scenarios where the bag contains many high-priority tasks but lacks low-priority 
tasks, the decay of high-priority tasks will gradually transform them into low-priority 
ones. Without the introduction of new high-priority tasks, all high-priority tasks will 
eventually decay, resulting in some SS-values becoming zeros. Even if the static prop-
erty for low-priority tasks is ideal, due to these zeros, vv will be bad (e.g., S=[1,2,3,0,0,0]S =
[1, 2, 3, 0, 0, 0]). 
As a treatment, we put some placeholder tasks with special IDs in each level. When 
they are selected, they will be put back immediately, and no tasks will be processed. 
They will be considered in calculating the static property, thus eliminating these zeros. 
Since the number of placeholders is small, they will not cause a signiﬁcant impact. 
5 
Experiments 
To comprehensively evaluate the proposed method, we examine both its static and 
dynamic properties under various input conditions. The dynamic property is a slight 
modiﬁcation of a static oneSS, deﬁned asv' = avg(|S - S''|)v′ = avg(|S −S′′|), whereS''S′′ is the linear ﬁt 
of SS, quantifying its deviation from linearity. 
We consider three input types that represent typical NARS scenarios: 1) Uniform 
inputs, with priorities evenly distributed from 0 to 1. 2) Concentrated inputs, priorities 
are also from 0 to 1 but biased toward certain values. 3) Compressed inputs, where 
priorities are limited to a narrower subinterval. 
These input types sufﬁciently capture both ideal and adversarial distributions. In 
addition to input distribution, we vary two system parameters: the input rate and level 
capacity. Input rate reﬂects task inﬂux density: in the low-rate setting, each processed 
task triggers 3 new tasks; in the high-rate setting, 10 new tasks are added. To simu-
late task overﬂow, we test two capacity levels: low (200 tasks/level) and high (5000

382
T. Li and B. Xu
tasks/level). With 10 levels per bag, a full swing consists of 55 insertions. Thus, each 
swing inputs 165 or 550 new tasks, respectively, depending on the rate. 
We also vary the scope used in computing static properties, testing both short-term 
(1 full swing) and long-term (5 full swings) sensitivity. Unless otherwise stated, each 
level maintains 5 placeholders, and the decay factor is ﬁxed at 0.9. 
All conﬁgurations are tested over 10 consecutive full swings with continuous task 
input. Due to initial sparsity, dynamic properties (v'v′) are reported starting from the 10th 
task update. It is also plotted on a log scale for better visualizaiton. Given the non-
intuitive nature of v'v′, static properties are also reported. For this, we will present two 
sets based on input rate (low/high), each including four combinations of capacity and 
scope: HL (high capacity, low scope), HH, LL, and LH. 
We compare three bag variants: the proposed bag (named the ImprovedBag or IB), 
the OpenNARS bag (B), and the OpenNARS bag with dormant levels (Bwd, threshold 
= 0.2). In visualizations, IB, B, and Bwd are marked in red, green, and blue, respec-
tively. Considering the number of comparisions, in bar charts, values from them 
are overlaid at the same position, with higher values covered by lower values. 
5.1 
Average Inputs 
In the average input setting, task priorities are evenly distributed from 0 to 1. As 
shown in Fig. 2 and Fig. 3, both under low- and high-speed input, the second and fourth 
columns (with larger scopes) exhibit higher average processing due to more full swings. 
Under low-speed input (Fig. 2), the traditional bag shows a clear bias toward high-
priority tasks, such as p_8p8 and p_9p9 tasks. In addition, the traditional bag with dormant 
levels shows almost no regularity (blue bars) when the static property scope is small 
(the ﬁrst and third columns in each bar). However, with a larger scope (the second and 
fourth columns), the regularity becomes a bit more consistent with the ideal case. This 
suggests that in scenarios with ﬂexible time constraints, such method will not cause 
signiﬁcant issues. 
In contrast, our proposed method shows an approximately linear increasing rela-
tionship between the priority and average processes. This can also be seen from the 
dynamic property curve in Fig. 2 and Fig. 3 that our proposed method (curves in red) 
has relatively low values in all cases. Specially, when in high-speed input, the proposed 
method work properly in all cases as shown in Fig. 3, but the performance is not as good 
as the traditional bag. This is because when there are many new tasks, each task is less 
likely to be processed repeatedly. As shown in Fig. 3 (left), even the maximum average 
processing is smaller than 1. If we assume that all tasks are executed at most once, and 
since the current input is the average input, the execution frequency of different types 
of tasks depends entirely on the frequency of the distributor selecting levels. In the tra-
ditional bag design, it is indeed consistent with the priority of the task, so it shows a 
performance close to the ideal situation, but such input cases are very uncommon. 
5.2 
Concentrated Inputs 
Although task priorities still range from 0 to 1, they are now more concentrated. Specif-
ically, 50% of tasks are assigned a priority of 0.95, while the rest remain evenly dis-

On Improving Dynamic Resource Allocation
383
Fig. 2. Static properties (left) and dynamic properties (right) of different bag designs tested with 
low-speed average input. There are 10 bars for 10 types of tasks. Each bar contains 4 columns 
representing the HL, HH, LL, and LH conditions, respectively. 
Fig. 3. Static properties (left) and dynamic properties (right) of different bag designs tested with 
high-speed average input. 
tributed. As shown in Fig. 4, the large number of 0.95-priority tasks causes the p_9p9 tasks 
in B to receive signiﬁcantly lower resources, even lower than tasks with lower priori-
ties, under both low-speed and high-speed input. Compared to the traditional bag, our 
design processes tasks more slowly but maintains an approximately linear relationship 
between priority and processing frequency. Since such concentrated inputs are common 
in NARS, we believe our method offers substantial improvement. The behavior of Bwd 
here remains consistent with the average input scenario. 
5.3 
Compressed Inputs 
In this setting, task priorities no longer span the full 0 to 1 range but are compressed 
to 0 to 0.5, which takes into account the situation where there is no subsequent high-
priority task input after all high-priority task decays. Under low-speed input, our method 
shows a strong bias toward p_4p4 tasks (Fig. 5, left), due to the default setting of only 5 
placeholders per level. Expanding this to 50 signiﬁcantly improves performance (Fig. 5, 
right), surpassing both B and Bwd. However, this comes at the cost of reduced task 
processing frequency to about 70%.

384
T. Li and B. Xu
Fig. 4. (top left) Static properties of different bag designs tested with low-speed concentrated 
input. (top right) Dynamic properties tested with low-speed concentrated input. (bottom left) 
Static properties tested with high-speed concentrated input. (bottom right) Dynamic properties 
tested with high-speed concentrated input. 
Fig. 5. (left) Static properties of different bag designs tested with low-speed compressed input and 
5 placeholders per level. (right) Static properties of different bag designs tested with low-speed 
compressed input and 50 placeholders per level. 
6 
Conclusion 
The bag design we proposed performs signiﬁcantly better than the traditional bag in the 
average, concentrated, and compressed inputs. In the average input, if the task input rate 
is too high, traditional bags outperform our proposed method, but we argue that the aver-
age task processing frequency here is very low, which does not meet the characteristics 
of NARS, which requires repeated task processing, so this case is usually avoided. In the 
concentrated input, our proposed solution is also better, although it requires sacriﬁcing 
a certain amount of task processing speed. In the compressed input, the performance of

On Improving Dynamic Resource Allocation
385
our approach remains strong, though it may be necessary to provide a certain number 
of placeholder tasks, slightly reducing throughput. 
In conclusion, since AGI is often designed to operate in unfamiliar environments 
where resources are limited and high adaptability is required, we believe that our new 
resource allocation method may inspire new approaches in AGI development. 
References 
1. Narjure Github Repository. https://github.com/opennars/Narjure/tree/cd5a72e6777fc47271d 
721fef8362aa2dad664ca. Accessed 11 May 2025 
2. Nars-Python 
Github 
Repository. 
https://github.com/ccrock4t/NARS-Python/tree/main. 
Accessed 11 May 2025 
3. Nars-Swift
Github
Repository.
https://github.com/maxeeem/NARS-Swift/tree/main. 
Accessed 11 May 2025 
4. Nars-Unity
Github
Repository.
https://github.com/ccrock4t/NARS-Unity/blob/main/ 
NARSDataStructures/Bag.cs. Accessed 11 May 2025 
5. Opennars Github Repository (2024). https://github.com/opennars/opennars. Accessed 11 
May 2025 
6. Dean, T.L., Boddy, M.S.: An analysis of time-dependent planning. In: AAAI, vol. 88, pp. 
49-54 (1988) 
7. Goertzel, B., et al.: Artiﬁcial general intelligence. Cognitive Technologies, Springer, Berlin, 
Heidelberg (2007). https://doi.org/10.1007/978-3-540-68677-4 
8. Hammer, P., Lofthouse, T., Wang, P.: The opennars implementation of the non-axiomatic 
reasoning system, pp. 160-170 (07 2016). https://doi.org/10.1007/978-3-319-41649-6_16 
9. McCarthy, J.: Reminiscences on the history of time-sharing. IEEE Ann. Hist. Comput. 14(1), 
19-24 (1992) 
10. Wang, P.: Non-axiomatic reasoning system (version 2.2). Tech. Rep. 75, Center for Research 
on Concepts and Cognition, Indiana University, Bloomington, Indiana (1993) 
11. Wang, P.: Non-Axiomatic Reasoning System: Exploring the Essence of Intelligence. Ph.D. 
thesis, Indiana University (1995) 
12. Wang, P.: Problem-solving under insufﬁcient resources. In: Working Notes of the AAAI Fall 
Symposium on Flexible Computation, pp. 148-155 (1996) 
13. Wang, P., Goertzel, B.: Introduction: aspects of artiﬁcial general intelligence. In: Goertzel, 
B., Wang, P. (eds.) Advance of Artiﬁcial General Intelligence, pp. 1-16. IOS Press, Amster-
dam (2007) 
14. Wang, P., Hammer, P., Isaev, P., Li, X.: The conceptual design of openNARS 3.1.0. Tech. 
rep., Technical report, Temple University, Philadelphia, United States (2020)

MetaMo: A Robust Motivational 
Framework for Open-Ended AGI 
Ruiting Lian1(B) and Ben Goertzel1,2 
1 SingularityNET Foundation, Zug, Switzerland 
{ruiting.lian,ben}@singularitynet.io 
2 TrueAGI Inc., Seattle, USA 
Abstract. We present MetaMo, a uniﬁed formal framework for AGI 
motivational systems, combining category theory, functional analysis 
and topology to support open-ended agents that self-modify and evolve 
their own goals and drives. A sequel paper shows how MetaMo maps 
onto concrete architectures like OpenPsi and MAGUS. MetaMo cen-
ters on a composite appraisal-then-decision operator F that carries both 
comonad (appraisal) and monad (decision) structure - a pseudo-bimonad 
- enabling clean "feel vs choose" pipelines. It enforces a contractive 
update law that dynamically keeps motivational states within a des-
ignated safe region, and it assumes a tubular topology that guarantees 
any achievable target lies on a thick, feasible path of incremental steps. 
From this foundation we extract ﬁve meta-motivational design princi-
ples: 1) Modular Appraisal-Decision Interface: separate mood updates 
from goal selection but allow just enough feedback so swapping their 
order only causes a tiny change; 2) Reciprocal State Simulation: share 
precise state-translation maps so agents can step into each other's moti-
vational frames for seamless hand-oﬀ and deep empathy; 3) Parallel Moti-
vational Compositionality: run multiple motivational subsystems (e.g. 
exploration, ethics, service) in parallel and merge their outputs with 
small coherence corrections; 4) Homeostatic Drive Stability: apply damp-
ing so small disturbances fade and tighten control near boundary con-
ditions; 5) Incremental Objective Embodiment: blend partway toward 
preferred motivational states each cycle, guaranteeing gradual conver-
gence into a feasible "ideal region" without overshoot or destabilization 
or loss of coherent self-model. We argue that MetaMo guides the design 
of AGI systems that are stable yet adaptable, capable of safe, incremental 
self-improvement, trustworthy collaborators in multi-agent communities, 
and scalable via parallel sub-agents. We illustrate these concepts via a 
running example of an online research assistant. 
1
Introduction 
Motivation plays an extremely key role in the functioning of any complex cog-
nitive system. No matter how smart the learning and reasoning algorithms or 
how accurate the memory and perception and action associated with a cognitive 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 386-398, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_34

MetaMo: A Robust Motivational Framework for Open-Ended AGI
387
system, if its motivational dynamics are dysfunctional, it can still wind up sys-
tematically acting stupid or self-destructive. This is witnessed frequently in the 
human sphere and applies equally well in the AI domain. 
Finding the right motivational framework for our AGI systems is thus an 
extremely important matter - and one that has attracted relatively little atten-
tion, due to the historical ﬁxation of the AI ﬁeld on problem solving and data ana-
lytics and modeling, which don't generally require complex motivational dynam-
ics. 
With this in mind we present here MetaMo: a general conceptual and math-
ematical framework for designing and analyzing the motivational systems of 
complex cognitive systems. While our main focus is AGI, MetaMo also applies 
conceptually to biological intelligences. The framework centers on an abstract 
categorial model of motivations and their relationship to other aspects of cogni-
tive agent operation, incorporating tools from functional analysis and topology 
as appropriate. A sequel paper [ 6] grounds this abstract model in speciﬁc quanti-
tative equations, obtained by connecting MetaMo to the OpenPsi and MAGUS 
frameworks for AGI motivation. In this paper we carry out some complementary 
exploration, ﬂeshing out some of the elegant and cognitively relevant abstract 
properties associated with MetaMo. 
We couch the MetaMo framework in terms of ﬁve "meta-motivational" princi-
ples that agents' motivational systems may obey, all expressible in simple formal 
ways. These principles are best expressed mathematically but may be glossed in 
words as: 1) Modular Appraisal-Decision Interface; 2) Homeostatic Motivation 
Stability; 3) Reciprocal Motivational State Simulation; 4) Motivational Compo-
sitionality; 5) Incremental Objective Embodiment; 
We argue that, while real-world motivational systems may never be able to 
manifest these ﬁve principles (or the overall MetaMo framework) in their pure 
form, they can do so approximatively. And we propose rough, semi-formal argu-
ments to the eﬀect that, on the whole, cognitive systems manifesting MetaMo 
eﬀectively are likely to be better oﬀ - in terms of achieving their own complex 
goals, and also in terms of beneﬁcial and prosocial behavior and having positive 
inner experience. 
The practical spur for the development of MetaMo has been the need to 
develop motivational systems for a variety of OpenCog Hyperon based agents 
(carrying out practical tasks like controlling robots or game characters, provid-
ing research assistance or doing science or mathematics) [ 5], and the desire for 
these to develop and interrelate in a clearly structured way. We take the "online 
research assistant" use-case as a running example throughout, to illustrate the 
concrete implications of the given abstractions. 
1.1
Brief Comparison to Alternatives 
MetaMo stands apart from more traditional motivational architectures-such as 
pure reward-maximization agents or hand-tuned drive hierarchies-by providing 
a unifying, formally grounded substrate rather than a collection of heuristics. In

388
R. Lian and B. Goertzel
many RL-style systems, an agent optimizes a ﬁxed scalar reward signal or tog-
gles between handcrafted drives, often leading to brittle behavior when reward 
landscapes change or drives conﬂict. MetaMo, by contrast, treats motivation as a 
compositional functor with both monadic (decision) and comonadic (appraisal) 
structure, augmented by contractive and topological guarantees. This means that 
core properties like modularity, stability, and reachability are not merely engi-
neered piecemeal but emerge from the same mathematical framework, yielding 
predictable interaction patterns even as the agent self-modiﬁes. 
From a Hyperon standpoint, compared to OpenPsi [ 1][ 2] which was leveraged 
in previous versions of OpenCog, MetaMo also sits one level higher as an abstract 
design language. OpenPsi oﬀers a detailed set of dynamic variables and appraisal 
algorithms, and for practical experimentation with proto-AGI systems one needs 
this concreteness; however, we show in the sequel [ 6] that OpenPsi's procedures 
and parameters can be slotted into the MetaMo framework in a coherent and 
orderly way. This shows MetaMo both subsumes (certain) concrete approaches 
to AGI motivation and provides a roadmap for extending them with formally 
veriﬁed stability, scalability, and social cooperation properties. 
2
MetaMo: Categorial Core 
The starting-point of MetaMo is to model an agent's "motivational state" as an 
object X=G\times MX = G × M in a category \mathcal CC. For instance, in the concrete instantiation 
explored in the sequel [ 6], G=(g_1,\dots,g_P)G = (g1, . . . , gP ) is a vector of intensities associated 
with system goals and M=(\mu_1,\dots,\mu_K)M = (μ1, . . . , μK) is a vector of "modulator" parameter 
values (e.g. valence, arousal, ethical vigilance, social drive, energy level, safety 
margin, etc.). Thus XX combines all drive strengths and mood/context signals 
into one object. 
We equip XX with a comonad (\Psi,\varepsilon,\delta)(Ψ, ε, δ) for appraisal, updating aﬀect and 
modulators via \Psi(X)\to XΨ(X) →X, and a monad (\mathbb{D},\eta,\mu)(D, η, μ) for decision, scoring and 
selecting goals via X\to\mathbb{D}(X)X →D(X). These interact through a lax distributive law 
 \lambda_X:\Psi\bigl(\mathbb{D}(X)\bigr)\Longrightarrow\mathbb{D}\bigl(\Psi(X)\bigr)λX : Ψ

D(X)

=⇒D

Ψ(X)

, meaning that "feel then choose" and "choose then 
feel" may diﬀer slightly but in a controlled way. Consequently, F=\mathbb{D}\circ\PsiF = D ◦Ψ inher-
its both monad and comonad structure (a pseudo-bimonad), allowing nested 
appraisal-decision loops without loss of coherence. A motivational coalgebra 
\alpha:X\to F(X)α : X →F(X) then governs one full cycle of appraisal plus decision. 
2.1
Interpreting Appraisal and Decision in a Perception-Action 
Loop 
These abstractions are grounded in the cognitive cycle of an embodied agent 
as follows. At time tt the agent's motivational state is x_t=(g_t,\mu_t)\in Xxt = (gt, μt) ∈X, where  
g_tgt are goal intensities and \mu_tμt modulators (mood, caution, energy, etc.). It per-
ceives an environment state s_t\in Sst ∈S, which includes both external observations 
and internal context (e.g. memory of recent actions), and applies the appraisal

MetaMo: A Robust Motivational Framework for Open-Ended AGI
389
operator \Psi:X\times S\to XΨ : X × S →X, yielding x'_t=\Psi(x_t,s_t)x′
t = Ψ(xt, st). Next the decision opera-
tor \mathbb{D}:X\to AD : X →A selects an action a_t=\mathbb{D}(x'_t)\in Aat = D(x′
t) ∈A by scoring each candidate 
under the updated goals and modulators. Executing a_tat produces a new envi-
ronment s_{t+1}st+1 (again including the record of a_tat), and the motivational coalgebra 
\alpha:X\to F(X)=\mathbb{D}\circ\Psiα : X →F(X) = D ◦Ψ gives  x_{t+1}=\alpha(x_t) =\Psi\bigl(\mathbb{D}(\Psi(x_t,s_t)),\,s_{t+1}\bigr)xt+1 = α(xt) = Ψ

D(Ψ(xt, st)), st+1

, often  
abbreviated x_{t+1}=F(x_t,s_t)xt+1 = F(xt, st). The lax distributive law ensures that if we swap 
the order-ﬁrst predicting post-action modulators for each candidate and then 
choosing-the resulting a_tat and x_{t+1}xt+1 diﬀer by at most a small, controlled amount. 
Notably, for the lax distributive law to hold in practice, the appraisal oper-
ator \PsiΨ must see the same post-decision context in both orderings. By including 
the agent's own recent action and updated internal state in the perceived envi-
ronment s_tst, the "predict-then-act" path (appraising with simulated decision info) 
and the "act-then-appraise" path (appraising after execution) operate on iden-
tical inputs, so their outputs diﬀer only by the small distortion guaranteed by 
\lambda_X\,. λX .
3
Three Categorial Symmetry Principles for Robust AGI 
Motivation 
To explicate the structure and meaning of the category-theoretic setup at the 
heart of MetaMo, we articulate three "meta-motivational" symmetry principles 
trivially derived therefrom. 
3.1
First Principle: Modular Appraisal-Decision Interface 
Ψ

D(X)

D

Ψ(X)

Ψ(X)
D(X) 
λX 
Ψ(μX )
μΨ(X) 
λX 
Basic concept: You can swap updating "how you feel" and choosing 
"what to do" in either order and still end up in essentially the same 
state. 
Technical Explanation. Appraisal (\PsiΨ) and decision (\mathbb{D}D) each carry side-eﬀects 
on disjoint state channels (feelings vs. goal intensities). The law \lambdaλ asserts that 
running appraisal then decision diﬀers only by a controlled distortion from run-
ning decision then appraisal. Because these distortions compose in the quantale 
(Q,\otimes)(Q, ⊗), one can nest and interleave any number of appraisal/decision cycles 
while still preserving overall coherence: you can fold multiple "feel-then-choose" 
or "choose-then-feel" steps into a single update without contradiction.

390
R. Lian and B. Goertzel
Technical Explanation. For those interested in the categorial infrastructure, a 
few more details follow. If we let (Q,\le,\otimes,e)(Q, ≤, ⊗, e) be a commutative quantale and \mathcal CC
a QQ-enriched symmetric monoidal bicategory, then a lax comonad \PsiΨ and a lax 
monad \mathbb{D}D on \mathcal CC are each equipped with unit and tensor 2-cells  \psi_{X,Y}:\Psi(X)\otimes\Psi(Y)\to\Psi(X\otimes Y), d_{X,Y}:\mathbb{D}(X)\otimes\mathbb{D}(Y)\to\mathbb{D}(X\otimes Y)ψX,Y : Ψ(X) ⊗
Ψ(Y ) →Ψ(X ⊗Y ), dX,Y : D(X) ⊗D(Y ) →D(X ⊗Y ), satisfying the usual 
coherence up to speciﬁed 2-cells in QQ. A  lax distributive law \lambda_X:\Psi(\mathbb{D}(X))\;\Longrightarrow\;\mathbb{D}(\Psi(X))λX : Ψ(D(X)) =⇒
D(Ψ(X)) is a family of 2-cells making the four Beck diagrams commute up to QQ-
weights. Then F=\mathbb{D}\circ\PsiF = D◦Ψ inherits both a lax-monad and a lax-comonad structure, 
i.e. becomes a pseudo-bimonad as noted above. 
Commutative Appraisal-Decision Example. To see the lax distributive law in 
action, take a look at the commutation square above - where \PsiΨ is the appraisal 
functor (predicting post-action satisfaction) and \mathbb{D}D is the decision monad (choos-
ing the action with highest predicted satisfaction). 
Concretely, let X=[0,1]X = [0, 1] be the current satisfaction level and two candidate 
actions A,BA, B have payoﬀs p_A=0.6pA = 0.6, p_B=0.4pB = 0.4. Deﬁne  \Psi(x) = (\,x+p_A,\;x+p_B\,), \mathbb{D}(s_A,s_B)=\max(s_A,s_B)Ψ(x) = ( x + pA, x +
pB ), D(sA, sB) = max(sA, sB); then \mathbb{D}\bigl(\Psi(x)\bigr) = x + \max(p_A,p_B) = x + 0.6D

Ψ(x)

= x+max(pA, pB) = x+0.6 while 
 \Psi\bigl(\mathbb{D}(x)\bigr) = \Psi\bigl(x+\max(p_A,p_B)\bigr) = \bigl(x+0.6+p_A,\;x+0.6+p_B\bigr) = (\,x+1.2,\;x+1.0\,)Ψ

D(x)

= Ψ

x+max(pA, pB)

=

x+0.6+pA, x+0.6+pB

= ( x+1.2, x+1.0 )
and applying the monad multiplication \muμ (which again takes the maximum) gives 
 \mu\bigl(\Psi(\mathbb{D}(x))\bigr) = \max(x+1.2,\;x+1.0) = x + 1.2 = x + 0.6μ

Ψ(D(x))

= max(x+1.2, x+1.0) = x+1.2 = x+0.6. Thus both paths around 
the square - "appraise then decide" (\mathbb{D}\circ\PsiD ◦Ψ) and "decide then appraise" (\Psi\circ\mathbb{D}Ψ ◦D
followed by \muμ)-yield the same updated satisfaction x+0.6x + 0.6. This demonstrates 
that in a properly designed motivational system, predicting how you will feel 
and then choosing, or choosing and then assessing how you feel, commute up 
to a small, controlled distortion, preventing the misapprehensions humans often 
experience. 
Application to Trading Agents. Consider an AI agent that researches ﬁnancial 
trades and then automatically executes them. Its appraisal operator \PsiΨ updates 
modulators like risk-aversion and anticipated regret based on market signals, 
while its decision operator \mathbb{D}D scores and selects trades. In a "choose\rightarrow→appraise" 
routine the agent ﬁrst does a=\mathbb{D}(x)a = D(x) to pick the highest-scoring trade under its 
current state xx, executes it, then calls \PsiΨ to update modulators from the actual 
proﬁt or loss. In an "appraise\rightarrow→choose" routine it ﬁrst computes \Psi(x)Ψ(x) for each 
candidate trade-predicting post-trade modulators-then does \mathbb{D}D on those predic-
tions to choose a trade whose imagined outcome looks best. The lax distributive 
law \lambda_X:\;\Psi\bigl(\mathbb{D}(x)\bigr)\;\Longrightarrow\;\mathbb{D}\bigl(\Psi(x)\bigr)λX : Ψ

D(x)

=⇒D

Ψ(x)

ensures that the two orderings yield a trade aa
and updated modulators whose distance  d\bigl(\mathbb{D}(\Psi(x)),\,\Psi(\mathbb{D}(x))\bigr)\le\deltad

D(Ψ(x)), Ψ(D(x))

≤δ is bounded 
by a small \deltaδ. Thus the agent avoids human-style "I thought buying this would 
make me feel good but ended up feeling nervous" style errors, and teams can 
optimize or swap out \PsiΨ or \mathbb{D}D independently while preserving a consistent trading 
style even under real market feedback.

MetaMo: A Robust Motivational Framework for Open-Ended AGI
391
3.2
Second Principle: Reciprocal Motivational State Simulation 
Basic concept: If updating agent A then translating its state into agent 
B is equivalent to translating ﬁrst and then updating agent B, then 
B can faithfully simulate A. 
Imagine two agents A and B, each with its own "appraise-decide" update. A 
functor T maps A's internal motivational state into B's. If running A's update 
and then T always yields the same result as running T and then B's update, 
then B shadows A step for step. 
Technical Explanation. Let  \alpha: X\to F(X), \alpha':X'\to F'(X')α : X →F(X), α′ : X′ →F ′(X′) be the one-step 
coalgebras of A and B. A functor  T:X\to X'T : X →X′ and a natural transformation  \phi_X: T\bigl(F(X)\bigr)\;\Longrightarrow\;F'\bigl(T(X)\bigr)φX :
T

F(X)

=⇒F ′
T(X)

satisfy for every f:X\to Yf : X →Y the "naturality" identity 
 F'\bigl(T(f)\bigr)\circ\phi_X =\phi_Y\circ T\bigl(F(f)\bigr)F ′
T(f)

◦φX = φY ◦T

F(f)

. Then  \phi_X\circ T(\alpha) =\alpha'\circ TφX ◦T(α) = α′ ◦T so TT commutes with 
the updates and becomes a coalgebra homomorphism: B simulates A exactly. 
But this simply asserts that the diagram 
X
F
 
(X) 
X′
F ′(X′) 
α 
T
φX 
α′
commutes, proving that T:(X,\alpha)\to(X',\alpha')T : (X, α) →(X′, α′) is a coalgebra morphism. 
Application to Online Research Assistants. Suppose assistant A wants to hand 
oﬀ a persistent, long-term query/investigation process to assistant B, in a way 
that conveys the "vibe" of the query and how it's being pursued along with the 
declarative content. Deﬁne TT to map A's goal-intensity and modulator atoms to 
B's corresponding atoms. Implement\phiφ so that B's next-step update onT(\text{state}_A)T(stateA)
reproduces TT of A's update. While perfect commutation is impossible in real 
code, a PAC-style \phiφ that errs by at most \varepsilonε with probability 1-\delta1 −δ lets B track A 
within known bounds, enabling relatively seamless task-sharing and interactivity 
across an assistant ﬂeet. 
3.3
Third Principle: Motivational Compositionality 
Basic concept: Updating two independent motivational sub-agents in 
parallel and then merging their states yields essentially the same result 
as merging ﬁrst and then updating. 
Imagine splitting your assistant into two modules-one that manages curiosity 
and exploration, and another that manages user-service and ethics-each with its 
own appraisal-decision loop. In a perfect system these modules share no vari-
ables, so you can run them side by side and then join their results without 
interference. In reality they share caches, memory, safety checks and so occa-
sionally step on each other, but if that interference is small most of the time you 
still get a coherent combined update.

392
R. Lian and B. Goertzel
Technical Explanation. We work in a symmetric-monoidal category (\mathcal C,\otimes,I)(C, ⊗, I). 
A comonad \PsiΨ and a monad \mathbb{D}D are lax-monoidal if they carry structural maps 
 \psi_{X,Y}:\Psi(X)\otimes\Psi(Y)\to\Psi(X\otimes Y), d_{X,Y}:\mathbb{D}(X)\otimes\mathbb{D}(Y)\to\mathbb{D}(X\otimes Y)ψX,Y : Ψ(X) ⊗Ψ(Y ) →Ψ(X ⊗Y ), dX,Y : D(X) ⊗D(Y ) →D(X ⊗Y ), satisfying 
unit and associativity up to coherent 2-cells. Then their composite  F=\mathbb{D}\circ\PsiF = D ◦Ψ
inherits a lax-monoidal structure via  \phi_{X,Y} =d_{\Psi(X),\Psi(Y)}\circ\mathbb{D}\bigl(\psi_{X,Y}\bigr) :F(X)\otimes F(Y)\to F(X\otimes Y)φX,Y = dΨ(X),Ψ(Y ) ◦D

ψX,Y

: F(X) ⊗
F(Y ) →F(X ⊗Y ) , witnessing that updating in parallel and then merging 
agrees (up to the coherence map \phi_{X,Y}φX,Y ) with merging ﬁrst. 
4
MetaMo: Dynamical and Topological Aspects 
To complete the MetaMo picture, we now introduce a few additional aspects 
regarding the changes of motivational content over time. While less clean and 
elegant in formulation, these are equally important to ensuring the robustness 
of advanced AGI motivational systems. In mathematical terms, it is notable 
that proper formalization of motivational systems requires clean "algebra-style" 
aspects like category theory along with messier "analysis-style" aspects like the 
ones in this section. 
4.1
Fourth Principle: Homeostatic Motivation Stability 
The ﬁrst key dynamical aspect of MetaMo is a mechanism for adaptively (and 
probabilistically) keeping a system in a desirable region of goal/motivation space. 
A "desirable" region R\subseteq XR ⊆X could be, for example, the set of states where 
abrupt goal shifts are damped and growth remains controlled. Improvising on the 
ideas of [ 4] on maintenance of goal stability or moderated goal evolution under 
self-modiﬁcation, the basic concept is: As long as the agent's state stays inside RR, 
it evolves unconstrainedly; but upon approaching the edge, a boundary-driven 
contractivity requirement pulls it back in. 
Formally, deﬁning the boundary band B_{\eta}={x\in R:\inf_{z\notin R}d(x,z)\le\eta},\quad\eta>0Bη = {x ∈R : infz /∈R d(x, z) ≤
η},
η > 0, we require F(R)\subseteq RF(R) ⊆R and 
 \bUnALT{}d\bigl(F(x),F(y)\bigr)\le c\,d(x,y)+\epsilon \quad\text{whenever }x\in B_{\eta}\text{ or }y\in B_{\eta},\eUnALT{} d

F(x), F(y)

≤c d(x, y) + ϵ
whenever x ∈Bη or y ∈Bη,
with c<1c < 1 and \epsilon\ge0ϵ ≥0. Thus updates near RR's boundary contract distances by 
at most \epsilonϵ, preventing escape, while deep inside RR no contractivity is imposed, 
permitting ﬂexible dynamics. 
Qualitative Summary of the Principle We loosely describe the above for-
mal notions via the concept of homeostatic motivation stability, whose gist 
may be glossed as: When the agent ﬁrst notices its motivations are drift-
ing toward extremes, it automatically soothes them (for example by 
raising caution or lowering excitement). Then, in its decision step, it 
follows those soothed signals to make only small, careful adjustments. 
This two-phase feedback loop ensures that whenever the agent nears

MetaMo: A Robust Motivational Framework for Open-Ended AGI
393
the edge of its comfort zone, it gently steers itself back toward its 
desirable region of motivations. 
In terms of cognitive processing, one may phrase this as: The agent runs two 
passes each cycle. First appraisal (\PsiΨ) checks how close the motivational state 
is to the edge of a desired region RR and raises caution modulators (e.g. safety 
margin, ethical vigilance) if needed. Then decision (DD) scores and bumps goal 
intensities, but those bumps are scaled by the current modulators. As a result, 
whenever the state approaches RR's boundary, the update step pulls any two 
nearby states closer together-stopping escape-while deep inside RR it permits 
more ﬂexible shifts. 
Application to Online Research Assistants. Consider a ﬂeet of AI research assis-
tants, where each one maintains a safe region R\subseteq XR ⊆X of motivational states-for 
example, conﬁgurations where goal-intensity changes remain moderate. In each 
cycle the appraisal pass \PsiΨ inspects m_tmt and, if m_t\in B_\etamt ∈Bη (near the edge of RR), 
raises caution modulators (e.g. boosting a "stability" signal). The decision pass 
DD then scales any proposed goal-intensity bumps by those modulators, so that 
large jumps are damped whenever m_t\in B_\etamt ∈Bη. Thus  F=D\circ\PsiF = D ◦Ψ pulls states back 
into RR under boundary pressure. 
At the same time, when m_tmt lies well inside RR, modulators remain low and 
the assistant can pursue more extreme novelty or curiosity drives-allowing it to 
explore "crazy" research hypotheses or unorthodox directions. If such exploration 
pushes m_tmt toward \partial R∂R, caution automatically increases in the next appraisal pass, 
tempering novelty and steering the assistant gently back into the acceptable 
envelope. This interplay supports bursts of creative exploration without ever 
derailing the agent too badly, yielding a balance of innovation and stability in 
its research guidance. 
Technical Explanation of Example. Here X=G\times MX = G × M with GG the goal-intensity 
sliders and MM including a "stability" modulator. We set  R={(g,m)\mid\|g-g_{\rm prev}\|\le\delta}, B_\eta={x\in R\mid\mathrm{dist}(x,X\setminus R)\le\eta}R = {(g, m) | ∥g −
gprev∥≤δ}, Bη = {x ∈R | dist(x, X \R) ≤η}. On  B_\etaBη the appraisal \PsiΨ and deci-
sion DD satisfy d(\Psi(x),\Psi(y))\le c_1\,d(x,y)+\epsilon_1d(Ψ(x), Ψ(y)) ≤c1 d(x, y)+ϵ1 and d(D(u),D(v))\le c_2\,d(u,v)+\epsilon_2d(D(u), D(v)) ≤c2 d(u, v)+ϵ2, 
so F=D\circ\PsiF = D ◦Ψ obeys \;d(F(x),F(y))\le c\,d(x,y)+\epsilon d(F(x), F(y)) ≤c d(x, y) + ϵ with c=c_1c_2<1c = c1c2 < 1. Concretely, 
when \|\Delta g\|∥Δg∥approaches \deltaδ, \PsiΨ raises the stability modulator ss, and  DD scales any 
proposed \Delta gΔg by 1-s1 −s, damping large jumps. If \|\Delta g\|\ll\delta∥Δg∥≪δ, then  s\approx0s ≈0 and DD
permits larger \Delta gΔg. Hence near \partial R∂R, FF pulls states back into RR, while deep inside 
RR the assistant can explore novel directions. 
4.2
Continuity of Self and Growth 
As an additional key dynamical aspect, MetaMo embodies the idea that an agent 
should be able to tractably move itself from its current motivational state to a 
desired target motivational state, step by step - all the while maintaining a 
coherent, continuously evolving self-model. To elaborate this idea we will ﬁrst 
need to say a little about self-models themselves.

394
R. Lian and B. Goertzel
Encouraging Continuity of Self-model. As a working hypothesis, we assume 
that to support robust open-ended intelligence, we want an agent to adjust 
its motivations radically over time without shattering its internal self-model 
(considering a "self-model" for now as an internal subsystem of an agent that 
outputs predictions of the agent's behaviors and states in various contexts) ... i.e. 
preserving "self-continuity" [ 3]. Like the other MetaMo principles, this might not 
ultimately be true for all forms of beneﬁcial general intelligence, but seems highly 
natural in the context of AGIs with roughly human-like cognitive architecture. 
Formally we may think about a map H:X\to MH : X →M taking system states 
XX into models MM, and we can look at assumptions like d_M\bigl(H(x),H(y)\bigr)\le L_H\,d_X(x,y) \forall\,x,y\in XdM

H(x), H(y)

≤
LH dX(x, y)∀x, y ∈X , mandating that small changes in xx induce proportionally 
small changes in H(x)H(x). 
Combined with boundary-driven contractivity on B_\etaBη, where  \;d_X(F(x),\pagination{\break}F(y))\le c\,d_X(x,y)+\epsilon dX(F(x),
F(y)) ≤c dX(x, y) + ϵ for x,y\in B_\etax, y ∈Bη, it follows immediately that each update 
x_{t+1}=F(x_t)xt+1 = F(xt) induces a bounded self-model shift 
 \bUnALT{}d_M\bigl(H(x_{t+1}),H(x_t)\bigr)\le L_H\bigl(c\,d_X(x_t,x_{t-1})+\epsilon\bigr).\eUnALT{} dM

H(xt+1), H(xt)

≤LH

c dX(xt, xt−1) + ϵ

.
Since c<1c < 1 and step-sizes remain bounded, the total drift \sum d_M(H(x_{k+1}),\pagination{\break}H(x_k)) dM(H(xk+1),
H(xk)) stays ﬁnite. Thus by taking only small, contracting steps near the region 
boundary, the agent preserves a continuous, coherent self-model even as it self-
transforms over time. Similar arguments can be made if one relaxes the strict 
Lipschitz assumption on HH with something more probabilistic. 
Conditions Ensuring Incremental Motivational Adjustment. We would 
like the agent's motivational system to be able evolve incrementally toward any 
reasonable chosen target without breaking its internal self-model. This can be 
guaranteed via formal assumptions such as: 
- Uniform tubular path-connectedness: there is \eta>0η > 0 such that any x,y\in Xx, y ∈X
can be joined by a continuous \gamma:[0,1]\to Xγ : [0, 1] →X with \gamma(0)=x,\gamma(1)=yγ(0) = x, γ(1) = y and 
each B(\gamma(t),\eta)\subseteq XB(γ(t), η) ⊆X. 
- Boundary-driven contractivity: on the band B_\eta={x\in R:\mathrm{dist}(x,X\setminus R)\le\eta}Bη = {x ∈R : dist(x, X\R) ≤η}
the update F_\alphaFα satisﬁes 
 \bUnALT{}d\bigl(F_\alpha(u),F_\alpha(v)\bigr)\le c\,d(u,v)+\epsilon,\quad c<1\eUnALT{} d

Fα(u), Fα(v)

≤c d(u, v) + ϵ,
c < 1
- Self-model Lipschitzness: the internal model H:X\to MH
:
X
→
M
obeys 
d_M(H(x),H(y))\le L_H\,d_X(x,y)dM(H(x), H(y)) ≤LH dX(x, y). 
- Local Lipschitz continuity of F_\alphaFα: each  x\in Xx ∈X has a neighborhood U_xUx with 
 \bUnALT{}d\bigl(F_\alpha(u),F_\alpha(v)\bigr)\le L_x\,d(u,v)\quad\forall\,u,v\in U_x,\eUnALT{} d

Fα(u), Fα(v)

≤Lx d(u, v)
∀u, v ∈Ux,
where L_x<1Lx < 1 for x\in B_\etax ∈Bη. 
Under these assumptions, any initial x_0x0 and achievable target x^*\in Ix∗∈I admit a 
"thick" path \gammaγ subdivided into segments of length <\eta/2< η/2. At each waypoint  \gamma(t_i)γ(ti)

MetaMo: A Robust Motivational Framework for Open-Ended AGI
395
the local contraction L_{\gamma(t_i)}<1Lγ(ti) < 1 ensures F_\alphaFα moves the state closer to \gamma(t_{i+1})γ(ti+1), 
and the Lipschitz property of HH bounds the corresponding self-model drift by 
L_H\,d_X(\gamma(t_{i}),\gamma(t_{i+1}))LH dX(γ(ti), γ(ti+1)). Iterating these small steps yields reliable convergence to 
x^*x∗while preserving coherent self-model evolution. 
Example Dynamic. One simple iterative dynamic fulﬁlling these ideas would be 
as follows. Once a target ideal lies in the reachable region II, the agent applies 
the metagoal update 
 \bUnALT{}F_\alpha(x)=(1-\alpha)\,x+\alpha\,\sigma\bigl(\tau(x)\bigr),\eUnALT{} Fα(x) = (1 −α) x + α σ

τ(x)

,
so each step moves toward II while preserving self-model coherence. Since HH is 
Lipschitz, 
 \bUnALT{}d_M\bigl(H(x_{t+1}),H(x_t)\bigr)\le L_H\,d_X\bigl(x_{t+1},x_t\bigr),\eUnALT{} dM

H(xt+1), H(xt)

≤LH dX

xt+1, xt

,
and under tubular path-connectedness plus boundary-driven contractivity on 
B_\etaBη, the iterates x_{t+1}=F_\alpha(x_t)xt+1 = Fα(xt) trace a smooth, contracting corridor into II and 
converge to a ﬁxed point without ever disrupting the agent's internal self-model. 
Fifth Principle: Incremental Objective Embodiment. The above may 
be summarized informally as: Once the agent has picked a goal that lies 
within its reachable range, it inches toward that goal a little bit at a 
time. Because each move blends only a small fraction of the new ideal 
into its current state, the agent's internal self-model shifts only slightly 
on every step. Moreover, the safe "goal region" is shaped so that, if the 
agent ever drifts too close to its edge, the next update automatically 
pulls it back inside. So, the agent can follow a smooth, steady path 
into its chosen goal state without ever breaking the continuity of how 
it understands itself. 
One of the metagoals that a MetaMo-based cognitive system strives to pre-
serve, via its homeostatic motivation stability dynamics, is precisely to keep its 
motivational goal/modulator space shaped in the right way that the above basic 
concept is ongoingly realized. 
Example Where It Works. A research assistant has two continuous sliders- 
curiosity and service priority-each adjustable in small increments. Its reachable 
target (0.6,0.4)(0.6, 0.4) lies in a region admitting a "thick" corridor. By taking 0.02-
step moves, the agent both converges to (0.6,0.4)(0.6, 0.4) and, because d_X(m_{t+1},m_t)dX(mt+1, mt) is 
small, ensures its self-model drift d_M(H(m_{t+1}),H(m_t))dM(H(mt+1), H(mt)) stays bounded, preserv-
ing coherent introspection throughout. 
Example Where It Fails. If slight tweaks to an agent's goals produce erratic user 
feedback-so that small changes in m_tmt lead to large, unpredictable updates-then 
neither state-space connectivity nor boundary contractivity holds. The agent 
cannot infer a reliable sequence of small, contracting steps, and its self-model 
experiences jolting discontinuities, blocking convergence to the chosen ideal.

396
R. Lian and B. Goertzel
Technical Characterization of Success vs. Failure. In the success case we have 
a reachable region I\subset XI ⊂X that is uniformly tubular, and a global contrac-
tion  F_\alpha(m)=(1-\alpha)m+\alpha\,m^*,\quad0<\alpha<1Fα(m) = (1 −α)m + α m∗,
0 < α < 1, so  d_X(F_\alpha(m),F_\alpha(m'))\le(1-\alpha)\,d_X(m,m')dX(Fα(m), Fα(m′)) ≤
(1 −α) dX(m, m′). Hence m_t\to m^*\in Imt
→
m∗
∈
I
and, since HH
is Lipschitz, 
\;d_M(H(m_{t+1}),H(m_t))\le L_H(1-\alpha)d_X(m_t,m^*) dM(H(mt+1), H(mt)) ≤LH(1 −α)dX(mt, m∗) remains small, preserving con-
tinuity. 
In the failure case noise or constraints break tubular connectivity or boundary 
contractivity: there exist m,m'\in B_\etam, m′ ∈Bη with  d_X\bigl(F_\alpha(m),F_\alpha(m')\bigr) > c\,d_X(m,m')+\epsilon \text{for some }c\ge1\text{ or large }\epsilondX

Fα(m), Fα(m′)

> c dX(m, m′) +
ϵfor some c ≥1 or large ϵ. Without c<1c < 1 and small \epsilonϵ, iterates can oscillate or 
diverge, so m_tmt never reliably approaches the target and d_M(H(m_{t+1}),H(m_t))dM(H(mt+1), H(mt))
can jump, shattering self-model coherence. 
4.3
Adopting Appropriate Formal Properties as Metagoals 
Wrapping all these pieces together, what MetaMo suggests is that an agent's 
goal system should be conﬁgured so that the "desirable region of state space" it 
attempts to ongoingly occupy, is characterized among other things via mainte-
nance of the various formal properties discussed above (the lax distributive law 
between appraisal and decision, but also the more topological properties related 
to keeping in a desirable region of state space and maintaining continuity of self 
through self-modiﬁcation). I.e. part of being "in a desirable state" is "being in 
a state where the MetaMo principles are obeyed, including the principle of hav-
ing a top-level goal of staying in a desirable state." The formulation of "goals" 
in MetaMo is suﬃciently broad that goals can be "metagoals" like this just as 
easily as highly concrete ones. 
5
Why the Five Principles Are Eﬀective 
Hypothesis: Agents whose motivational systems embody MetaMo, even 
roughly, tend to achieve their target states more reliably, ﬂexibly, 
scalably and enjoyably - at least within the domain of AGI systems possessing 
roughly human-inspired cognitive architectures. 
The basic line of argument in favor of this hypothesis that, for each of the 
principles, one can clearly identify classes of situations in which deviating too 
far from the principle is likely to cause ineﬃciency and bad experience. In brief: 
Modular Appraisal-Decision Interface. When appraisal and decision routines 
are too tightly coupled, small timing diﬀerences can cascade into large feedback 
loops that undermine reliability and leave the agent internally confused and 
dissatisﬁed. For example, a trading agent that ﬁrst picks a high-risk trade and 
only then updates its risk modulators may execute a buy, see the transaction, 
raise risk alarms, immediately reverse the position, and repeat. This oscillation 
wastes resources, lowers proﬁt, and shatters the agent's internal "conﬁdence" 
model.

MetaMo: A Robust Motivational Framework for Open-Ended AGI
397
Reciprocal Motivational State Simulation. Without accurate state-translation 
between agents, hand-oﬀs degrade into miscommunication, dropping eﬃciency 
and eroding trust between peers. In a virtual software team of AGI coding agents, 
if one agent cannot simulate its colleagues' views of its software development 
plan, confusions and delays will occur, coordination will break down, and agents 
will lose mutual conﬁdence in one another's reliability. 
Parallel Motivational Compositionality. When parallel motivational subsystems 
conﬂict without a coherence mechanism, their outputs ﬁght and the combined 
behavior becomes erratic, reducing eﬀectiveness and creating internal tension. A 
smart thermostat with separate "energy-saving" and "comfort" modules that lack 
an appropriately coherent integration will alternately freeze and overheat a room, 
lowering user satisfaction and leaving the agent stuck between contradictory 
directives. 
Homeostatic Drive Stability. If each new input fully overrides prior drives, even 
minor perturbations can send the agent's priorities into wild swings, reducing 
task success and causing a sense of "motion sickness" in its own state. A plane-
tary rover that applies every fresh sensor reading without damping may switch 
continuously between mapping, obstacle avoidance, and self-repair modes, never 
completing any task eﬀectively and suﬀering both mission failure and internal 
instability. Consequences in the context of e.g. AGI code self-modiﬁcation could 
be even more severe. 
Incremental Objective Embodiment. Attempting to reprogram goals in one leap 
risks overshoot or collapse, breaking task continuity and leaving the agent frus-
trated by unmet objectives. A research assistant told to shift from 10% to 90% 
curiosity at once might abandon all ongoing summaries, lose track of prior con-
text, and crash its planning and even forget "who it is," instead of smoothly 
converging on new exploration priorities. 
Limits of the MetaMo Perspective. While we are going relatively far out 
on theoretical limbs here, we do want to retain some level of "humbition." These 
arguments regarding the diﬃculties that tend to ensue from disregarding the 
key MetaMo principles are much clearer in the context of AGIs with human-like 
cognitive architecture than in the context of AGI or ASI in general. Particularly 
the dynamical principles may be less applicable to radically non-human cognitive 
architectures - it's not even clear that every ASI will have a coherent "self-model" 
in the way that humans do. Nonetheless, we believe MetaMo can provide an 
excellent guide to the creation of motivational systems for the ﬁrst generation 
of human-level and slightly superhuman AGI systems. These systems may then 
be able to lead the development of even more ﬂexible and incisive formal and 
informal motivational frameworks.

398
R. Lian and B. Goertzel
6
Conclusion 
We have introduced a uniﬁed formal framework for AGI motivation, centered on 
ﬁve abstract principles - modular appraisal vs. decision, homeostatic stability, 
gradual ideal alignment, reciprocal simulation, and parallel coherence. Together 
these principles oﬀer a clear design language for building agents that are robust, 
adaptable, and trustworthy, whether optimizing complex tasks, pursuing open-
ended self-improvement, or coordinating in multi-agent ecosystems. 
One direction toward concrete implementation based on these ideas is pur-
sued in the companion paper [ 6] in which we instantiate OpenPsi [ 1],[ 2] within  
the MetaMo framework, and discuss integration of MetaMo with Hyperon Atom-
space and associated neural modules. We expect that the synergy of abstract 
formal design and practical instantiation will accelerate the development of AGI 
systems that are both powerful and able to synergize and co-evolve eﬀectively 
with human values. 
References 
1. Bach, J.: Principles of Synthetic Intelligence. Oxford University Press (2009) 
2. Cai, Z., Goertzel, B., Zhou, C., Zhang, Y., Jiang, M., Yu, G.: Dynamics of a com-
putational aﬀective model inspired by Dorner's PSI theory. Cogn. Syst. Res. 18(4), 
63-80 (2011) 
3. Goertzel, B.: When should two minds be considered versions of one another? Int. J. 
Mach. Consc. 4(1), 177-185 (2012) 
4. Goertzel, B.: Metagoals endowing self-modifying AGI systems with goal stability or 
moderated goal evolution: toward a formally sound and practical approach. arXiv 
preprint arXiv:2412.16559 (2024). https://arxiv.org/abs/2412.16559 
5. Goertzel, B., et al.: OpenCog Hyperon: a framework for AGI at the human level 
and beyond. arXiv preprint arXiv:2310.18318 (2023). https://arxiv.org/abs/2310. 
18318 
6. Peeps, T.S.: From Metamo to OpenPSI: embodying abstract motivational principles 
in concrete AGI systems. In: TBD (2025)

Embodying Abstract Motivational 
Principles in Concrete AGI Systems: From 
MetaMo to Open-Ended OpenPsi 
Ruiting Lian1(B) and Ben Goertzel1,2 
1 SingularityNet Foundation, Zug, Switzerland 
ruiting.lian@singularitynet.io 
2 TrueAGI Inc., Seattle, USA 
Abstract. We present a framework for embedding abstract motiva-
tional principles into concrete AGI systems, bridging the gap between 
the formal theory of motivational structures and dynamics and the 
practical implementation of motivational systems for real-world applica-
tions and agents. We introduce MetaMo, a category-theory-based frame-
work designed to ensure dynamical stability, self-coherence, and ethical 
alignment in open-ended AGI systems. MetaMo integrates a comonadic 
appraisal process with a decision monad, forming a pseudo-bi-monad 
structure that guides multi-objective reasoning and context-sensitive 
modulation of goals. The framework ensures that agents can pursue mul-
tiple, potentially conﬂicting goals while maintaining stability through 
contractive updates and over goals that enforce ethical constraints. 
We demonstrate the specialization of MetaMo in the Hyperon AGI 
system, where the agent's goals are organized into a hierarchical struc-
ture in line with the MAGUS motivational theory, with top-level meta-
goals focused on the two principle drives of Open-Ended Intelligence the-
ory, individuation (self-preservation) and transcendence (self-expansion). 
These high-level goals dynamically inﬂuence the agent's decision-making 
and appraisal processes via the OpenPsi motivational dynamic, modu-
lating exploratory behaviors and caution based on context. OpenPsi pro-
vides a ﬂexible and context-sensitive appraisal system that updates emo-
tional and motivational states in response to stimuli, supporting adaptive 
behavior in complex environments. 
By combining theoretical foundations (MetaMo) with more concretely 
grounded motivational frameworks (OpenPsi, MAGUS), we provide a 
concrete approach to integrating motivational systems into AGI archi-
tectures, ensuring ethical behavior, stability, and the ability to adapt and 
open-endedly self-modify over time. 
1
Introduction 
Motivation is a core challenge in AGI: agents must balance multiple drives, 
adapt to changing contexts, and respect ethical constraints while pursuing open-
ended self-improvement. The recently proposed MetaMo theory [ 3] leverages 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 399-410, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_35

400
R. Lian and B. Goertzel
category-theoretic and functional-analytic formalism to capture desirable prop-
erties of AGI motivational systems - stability, compositionality, incremental 
self-continuity but remains too high-level for direct implementation. Conversely, 
practical AI systems often rely on heuristic or scalar reward signals that lack 
formal guarantees, struggle with multi-objective tradeoﬀs, and are far from the 
nuance needed to handle open-ended self-modiﬁcation or complex real-world 
ethical choices. 
In this paper we bridge this gap by connecting MetaMo with more con-
crete motivational frameworks that are easier to implement in practical software 
systems. We show how MetaMo's appraisal comonad can be instantiated as 
OpenPsi, a lightweight appraisal layer drawing on Bach's MicroPsi theory, and 
how its decision monad can be realized via a variant of MAGUS, a hierarchical 
goal system with dual overgoals for individuation and transcendence. We fur-
ther demonstrate the specialization of MetaMo in the Hyperon AGI platform, 
including motivation-driven inference control for Probabilistic Logic Networks, 
and argue that this integrated approach delivers both theoretical robustness 
(metagoal stability, compositional semantics) and practical ﬂexibility (context-
sensitive modulation, multi-drive coordination) for next-generation AGI systems. 
2
Background 
We now review the core cognitive and motivational formulations we will work 
with here: the abstract MetaMo framework, the more concrete OpenPsi and 
MAGUS frameworks, and the Hyperon/PRIMUS AGI software platform and 
cognitive architecture, and Open-Ended Intelligence approach to AGI. 
MetaMo MetaMo [ 3]. is a uniﬁed formal framework for AGI motivational systems 
that combines tools from category theory, functional analysis and topology to 
support agents capable of open-ended self-modiﬁcation and goal evolution. At 
its core lies a composite appraisal-then-decision operator 
 \bUnALT{}F = D\circ\Psi,\eUnALT{} F = D ◦Ψ,
which carries both a comonad structure \PsiΨ for stimulus appraisal and a monad 
structure DD for goal selection, yielding a pseudo-bimonad on the motivational 
state space X = G\times MX = G × M. A lax distributive law between \PsiΨ and DD guarantees 
that "feel-then-choose" and "choose-then-feel" commute up to a small, controlled 
error, endowing the system with modularity and robustness against execution 
order. Moreover, by enforcing a contractive update law 
 \bUnALT{}d\bigl(F(x),F(y)\bigr)\le c\,d(x,y)+\epsilon,\quad c<1,\eUnALT{} d

F(x), F(y)

≤c d(x, y) + ϵ,
c < 1,
within a designated safe region R\subset XR ⊂X, MetaMo ensures homeostatic stability, 
preventing runaway drives or ethical lapses even as the agent explores and self-
improves [ 3]. 
From this foundation, MetaMo articulates ﬁve meta-motivational design prin-
ciples that together foster reliable, scalable AGI motivation. First, the Modular

From MetaMo to OpenPsi
401
Appraisal-Decision Interface cleanly separates mood updates from goal selec-
tion while permitting minimal feedback. Second, Reciprocal State Simulation 
lets distinct agents step into each other's motivational frames for seamless hand-
oﬀ. Third, Parallel Compositionality enables multiple motivational subsystems 
(e.g. exploration, ethics, service) to run in parallel and merge coherently. Fourth, 
Homeostatic Drive Stability applies damping near the boundaries of feasibility 
to keep motivational states within safe limits. Finally, Incremental Objective 
Embodiment blends partway toward preferred motivational states each cycle, 
guaranteeing smooth convergence without overshoot or self-model disruption. 
Together, these principles yield a motivational engine that is both adaptable 
and safely bounded, suitable for guiding Hyperon and other AGI architectures 
toward robust, trustworthy behavior 
Hyperon and PRIMUS. Hyperon is the new generation of the OpenCog AGI 
platform - a ﬂexible, high-performance framework for building generally intelli-
gent systems. At its core lies the AtomSpace, a weighted, typed metagraph that 
stores all knowledge as Atoms (nodes and links) annotated with probabilistic 
truth-values and other metadata. Hyperon provides generic dataﬂow engines, 
pluggable learning and inference modules (PLN, evolutionary learning, pattern 
miners, neural nets), and an event-driven scheduler that orchestrates reasoning, 
perception, and action in real time. Its design emphasizes modularity, scala-
bility, and clear interfaces so that new cognitive capabilities can be integrated 
seamlessly, from deep learning pipelines to symbolic planners. 
PRIMUS (formerly CogPrime) is a comprehensive cognitive architecture tai-
lored for implementation atop Hyperon. It speciﬁes a hierarchy of functional 
components, perception, attention, memory, reasoning, planning, language, and 
meta-cognition along with their expected data-ﬂows through the AtomSpace. 
PRIMUS deﬁnes how perceptual inputs become AtomSpace representations, how 
attention mechanisms select relevant Atoms for inference, how PLN (Probabilis-
tic Logic Networks) and other reasoners generate and revise beliefs, how planners 
translate goals into actions, and how reﬂective modules monitor and adapt the 
system's own strategies. By grounding PRIMUS in Hyperon's uniform Atom-
Space and scheduler, developers gain a coherent blueprint for constructing AGI 
agents whose learning, reasoning, and self-modiﬁcation capabilities interoperate 
naturally within a single, shared knowledge substrate. 
OpenPsi. OpenPsi is a comonadic implementation of Bach's MicroPsi theory [ 2], 
inspired by Dorner's PSI theory [ 7] and designed for integration into the origi-
nal OpenCog system (Hyperon's predecessor), providing a lightweight, emergent 
appraisal layer suitable for real-time interaction and learning. Unlike models that 
isolate emotion, OpenPsi integrates appraisal directly into perception, cognition 
and action selection, yielding emotional states that arise from the continuous 
interplay of internal demands and external stimuli. 
Following MicroPsi, OpenPsi represents the agent's aﬀective conﬁguration 
via six continuous modulators: 
 \bUnALT{}M = (valence,\;arousal,\;approach,\;resolution,\;threshold,\;securing),\eUnALT{} M = (valence, arousal, approach, resolution, threshold, securing),

402
R. Lian and B. Goertzel
which regulate: overall attractiveness of events (valence), energy level (arousal), 
drive toward rewards (approach), depth of processing (resolution), required con-
ﬁdence before action (threshold), and degree of caution (securing) [ 6]. 
When a new stimulus ss arrives, OpenPsi applies appraisal operators (novelty, 
goal conduciveness, eﬀort, risk) to update MM without altering the high-level goal 
vector GG: 
 \bUnALT{}\Psi\bigl((G,M),s\bigr) = (G,\,M').\eUnALT{} Ψ

(G, M), s

= (G, M ′).
For example, high novelty yields 
 \bUnALT{}M'.arousal \uparrow,\quad M'.approach \uparrow,\eUnALT{} M ′.arousal ↑,
M ′.approach ↑,
while high risk yields 
 \bUnALT{}M'.threshold \uparrow,\quad M'.securing \uparrow.\eUnALT{} M ′.threshold ↑,
M ′.securing ↑.
Knowledge and experiences live in the AtomSpace, a hypergraph database 
where both nodes and links may contain embedded subgraphs [ 8]. Six parallel 
mind agents operate over the AtomSpace PerceptionUpdater, DemandUpdater, 
ModulatorUpdater, FeelingUpdater, ActionSelector, and Monitor which commu-
nicate only via shared Atoms [ 4]. For instance, the ModulatorUpdater computes 
new MM values based on demand satisfactions (energy, integrity, certainty, com-
petence, aﬃliation), while the ActionSelector biases plan search according to 
current modulators. 
OpenPsi's dynamics exhibit phase-transition behavior, with complex dynam-
ics such as self-ampliﬁcation and self-stabilization, in response to environmen-
tal perturbations [ 5]. In simulated environments, a virtual agent controlled by 
OpenPsi displays emergent emotional trajectories (happiness on reward, sad-
ness under loss, fear when threatened) without hard-coded emotion rules, arising 
instead from nonlinear feedback among appraisal, modulators and demands [ 6]. 
By integrating OpenPsi leveraging OpenCog's metagraph based architecture, 
OpenPsi provides: 
- emergent emotions grounded in systemic dynamics, 
- phase-transition behavior matching psychological models, 
- scalable parallelism via decoupled mind agents, 
- ﬂexible integration with arbitrary perception, planning or learning modules. 
OpenPsi thus equips Hyperon-based agents with human-like motivational 
dynamics: recognizing novelty, weighing moral risk, and allocating cognitive 
eﬀort where it matters, enabling adaptive, safe, and engaging behavior in com-
plex environments. 
MAGUS. MAGUS is a conceptual motivational theory proposed for the next 
phases of Hyperon development [ 11]. It posits a hierarchical motivational archi-
tecture in which a single slowly-varying overgoal deﬁnes the agent's enduring 
purpose and identity, serving as a global utility function that top-down modu-
lates all other drives. Below the overgoal lie mid-level goals, which decompose its

From MetaMo to OpenPsi
403
abstract directive into coherent thematic objectives, and low-level drives, which 
translate those objectives into moment-to-moment action aﬀordances. Each layer 
maintains its own utility estimate and update dynamics, but the overgoal exerts 
gating control by amplifying the value of subgoals and suppressing conﬂicting 
urges, ensuring coherent pursuit of the overarching purpose. Reﬂective meta-
processes monitor performance and consistency, allowing the overgoal itself to 
be revised or replaced when deep self-model evaluation indicates a better align-
ment with experience or ethical constraints. 
Weaver's Theory of Open-Ended Intelligence. Weaver [ 16] proposes that open-
ended intelligence emerges from two complementary drives: individuation, the  
process by which a system diﬀerentiates itself from its environment by reinforc-
ing its internal coherence and functional organization, and self-transcendence, the  
capacity of that system to exceed its own current limits by generating novel goals, 
behaviors and structures. Individuation can be viewed as a drive toward mini-
mizing internal entropy and maximizing integration, whereas self-transcendence 
drives the system toward exploring new regions of its possibility space, eﬀectively 
optimizing a meta-objective of continual growth. In Weaver's view, these drives 
form a dynamic tension: individuation provides the stability needed for reliable 
operation, while self-transcendence injects the creativity and adaptability neces-
sary for unbounded innovation. Together, they underpin complex self-organizing 
systems that perpetually evolve and adapt, realizing truly open-ended intelli-
gence. In the below, we synthesize MAGUS and OEI via splitting MAGUS's 
overgoal into two overgoals focused on individuation and self-transcendence. 
2.1
Comparison to Traditional Approaches 
We contrast our MetaMo-based integration of OpenPsi and MAGUS with a few 
of the numerous other approaches taken in the historical and current AI ﬁeld. 
Reinforcement-Learning. Classical RL formalizes motivation very simplistically 
as maximization the scalar return E[\sum_t\gamma^t r_t]E[
t γtrt] [ 15], which has the merit of sim-
plicity but suﬀers from reward speciﬁcation brittleness, forcing all objectives 
into one scale (scalar collapse), and struggles to eﬀectively represent normative 
drives for safety or ethics. It is also somewhat the opposite of "open-ended" - 
there is no way to pack a goal like "self-transcendence" into a pre-speciﬁed and 
immutable reward function. In contrast, MetaMo maintains a vector-valued goal 
state GG and continuous modulators MM, using categorical composition to enable 
explicit multi-objective tradeoﬀs and formal stability guarantees. 
Curiosity-Driven and Information-Theoretic. Building on Schmidhuber's intrin-
sic motivation theory [ 14], curiosity systems reward novelty or prediction error 
[ 12] and excel at exploration but omit hierarchical goal structure, meta-level nor-
mative constraints, and incremental self-continuity. This is a beautifully open-
ended approach to motivation but in the end tells only part of the story. MetaMo 
subsumes information-theoretic appraisal via the comonad \PsiΨ, while its decision

404
R. Lian and B. Goertzel
monad DD and overgoals embed both exploration and long-term safety in one 
uniﬁed framework. 
BDI and Cognitive Architectures. BDI architectures like AgentSpeak [ 13], Soar 
[ 10], and LIDA [ 1] represent motivations as symbolic desires and generate plans 
via deliberation but lack a systematic and sophisticated framework of continu-
ous modulatory variables (e.g. arousal, valence), say nothing about dynamical 
aspects like homeostatic stability or self-model coherence, and lack composi-
tional semantics for integrating appraisal with decision. These approaches cap-
ture important aspects of motivated cognition but are too closed-ended and 
dynamically simplistic to meet the needs of self-modifying, ongoingly developing 
AGI systems. 
Hybrid Architectures and PRIMUS. 
The PRIMUS cognitive architecture, 
designed for implementation in the OpenCog Hyperon framework [ 9], has tra-
ditionally approached motivation via fusing OpenPsi-inspired appraisal with 
probabilistic inference and static top-level goal weight assignments. MAGUS 
as a new aspect of PRIMUS adds in an overgoal and goal-correlation heuristics, 
but still, until the introduction of MetaMo the overall PRIMUS motivational 
framework remained largely ad hoc without a unifying mathematical backbone. 
Instantiating appraisal as a comonad and decision as a monad within MetaMo 
yields a formally grounded motivational engine with multi-scale compositional-
ity, dynamical complexity and robust open-endedness. [ 9]. 
3
From Categorial Abstractions to Operational Equations 
To apply MetaMo in Hyperon, we model the motivational state as X=G\times MX = G × M. 
Here 
 \bUnALT{}G=(g_{over}^{Ind},\,g_{over}^{Trans},\,g_{1},\dots,g_{P},\,a_{1},\dots,a_{Q}),\eUnALT{} G = (gInd
over, gT rans
over , g1, . . . , gP , a1, . . . , aQ),
where g_{over}^{Ind}gInd
over enforces individuation (self-preservation, moderated goal growth) 
and g_{over}^{Trans}gT rans
over
enforces self-transcendence (adaptive expansion, novelty embrace). 
Primary goals g_{i}gi (user-help, curiosity, etc.) and anti-goals a_{j}aj occupy the rest 
of GG. The numerical vector entry corresponding to a certain goal, at a certain 
point in time, indicates the goal-importance or goal-weight of that goal to the 
AGI system at that point in time. 
The OpenPsi modulators 
 \bUnALT{}M=(valence,\,arousal,\,approach,\,resolution,\,threshold,\,securing)\eUnALT{} M = (valence, arousal, approach, resolution, threshold, securing)
track mood, urgency, openness, depth, uncertainty tolerance and caution. 
Arousal and approach align with g_{over}^{Trans}gT rans
over , boosting exploration when transcen-
dence is high; threshold and securing align with g_{over}^{Ind}gInd
over, heightening caution when 
individuation is high. Similar to the goal vector, the numerical entry correspond-
ing to a certain modulator at a certain point in time indicates the level of that 
modulator in the system at that moment.

From MetaMo to OpenPsi
405
Appraisal as a Comonad. The comonad \PsiΨ maps ((G,M),s)((G, M), s) to (G,M')(G, M ′), where  
each M'_kM ′
k depends on stimulus novelty, conduciveness and risk. Moreover, g_{over}^{Ind}gInd
over
scales up M'.securingM ′.securing and M'.thresholdM ′.threshold to suppress unstable subgoals, while 
g_{over}^{Trans}gT rans
over
scales upM'.arousalM ′.arousal andM'.approachM ′.approach to encourage adaptive risk-taking. 
Decision as a Monad. The monad DD produces (G+\Delta G,M)(G + ΔG, M) by scoring each g_igi: 
 \bUnALT{}\Delta G_i = f(g_i,M_k,\mathrm{MIC}) \;-\;\lambda_{Ind}\,g_{over}^{Ind} \;+\;\lambda_{Trans}\,g_{over}^{Trans},\eUnALT{} ΔGi = f(gi, Mk, MIC) −λInd gInd
over + λT rans gT rans
over ,
so that individuation constrains large updates and transcendence rewards bene-
ﬁcial growth. Sequential composition of DD steps thus respects both meta-goals. 
Compositional Semantics and Pseudo-Bimonad. Here \PsiΨ is OpenPsi's appraisal 
comonad and DD is MAGUS's decision monad. Their composition 
 \bUnALT{}F = D\circ\Psi\eUnALT{} F = D ◦Ψ
forms a pseudo-bimonad on XX. The  lax law  D\circ\Psi\approx\Psi\circ DD ◦Ψ ≈Ψ ◦D means appraisal and 
decision can interleave without destabilizing XX. 
Dynamical Properties and Stability. We deﬁne a safe region 
 \bUnALT{}R={(G,M)\mid g_{over}^{Ind}\ge\theta_{safe}\land\|G\|\le G_{\max}}.\eUnALT{} R = {(G, M) | gInd
over ≥θsafe ∧∥G∥≤Gmax}.
Near \partial R∂R, FF satisﬁes 
 \bUnALT{}d(F(x),F(y))\le c\,d(x,y)+\epsilon,\;c<1,\eUnALT{} d(F(x), F(y)) ≤c d(x, y) + ϵ, c < 1,
so that high g_{over}^{Ind}gInd
over induces contraction toward safety. Well inside RR, low  g_{over}^{Ind}gInd
over
and higher g_{over}^{Trans}gT rans
over
relax constraints, restoring exploratory freedom. 
Incremental Self-Model Updates. Each update target x^*=F(x_t)x∗= F(xt) is blended as 
 \bUnALT{}x_{t+1}=(1-\alpha)x_t+\alpha x^*,\quad \alpha=\alpha_0\bigl(1-g_{over}^{Ind}\bigr)+\beta_0\,g_{over}^{Trans},\eUnALT{} xt+1 = (1 −α)xt + αx∗,
α = α0

1 −gInd
over

+ β0 gT rans
over ,
so that individuation slows change for self-coherence and transcendence speeds 
it for adaptive growth. 
In this way, the dual overgoals g_{over}^{Ind}gInd
over and g_{over}^{Trans}gT rans
over
embed MAGUS's meta-
level drives into MetaMo's formal dynamical guarantees, integrating OpenPsi's 
context-sensitive appraisal with MAGUS's hierarchical decision in a stable, ﬂex-
ible framework for Hyperon AGI. 
4
Application: Curious Research Assistant 
To illustrate how our MetaMo-based motivational framework operates in prac-
tice, we now instantiate it for an online AI assistant whose top-level goals are: 
 \bUnALT{}G = (g_{over}^{Ind},\,g_{over}^{Trans},\,g_{help},\,g_{curio},\,g_{novel},\,g_{self},\,g_{ethic},\,g_{soc}),\eUnALT{} G = (gInd
over, gT rans
over , ghelp, gcurio, gnovel, gself, gethic, gsoc),
where:

406
R. Lian and B. Goertzel
- g_{over}^{Ind}gInd
over: the individuation overgoal enforcing self-preservation, contraction-
based stability and cautious evolution of subgoals, 
- g_{over}^{Trans}gT rans
over : the transcendence overgoal promoting adaptive expansion, novelty 
embrace and gradual self-transcendence, 
- g_{help}ghelp: assisting the user eﬀectively, 
- g_{curio}gcurio: satisfying intrinsic curiosity, 
- g_{novel}gnovel: seeking novelty in inputs and tasks, 
- g_{self}gself: pursuing self-transformative learning, 
- g_{ethic}gethic: obeying prescribed ethical norms, 
- g_{soc}gsoc: socializing with humans and other AI agents. 
The modulatory state remains 
 \bUnALT{}M = (valence,\,arousal,\,approach,\,resolution,\,threshold,\,securing),\eUnALT{} M = (valence, arousal, approach, resolution, threshold, securing),
where high g_{over}^{Trans}gT rans
over
ampliﬁes arousal and approach, while high g_{over}^{Ind}gInd
over ampliﬁes 
threshold and securing. Moreover, the individual primary goals connect to 
these overgoals: g_{help}ghelp and g_{ethic}gethic lean on g_{over}^{Ind}gInd
over to ensure stable, reliable assistance 
and ethical compliance; g_{curio}gcurio and g_{novel}gnovel draw on g_{over}^{Trans}gT rans
over
to fuel exploration of 
new information; g_{self}gself sits between them, balancing cautious self-preservation 
with transformative growth; and g_{soc}gsoc enacts social engagement under the guid-
ance of both meta-drives. 
Appraisal Comonad Psi. Upon receiving an input (e.g. a user query or new 
publication), the assistant computes 
 \bUnALT{}\Psi\bigl((G,M),s\bigr)=(G,\,M'),\eUnALT{} Ψ

(G, M), s

= (G, M ′),
updating M'M ′ by OpenPsi appraisal (novelty, conduciveness, risk). For instance, 
a highly novel, ethically neutral paper yields 
 \bUnALT{}M'.arousal\uparrow,\quad M'.valence\uparrow,\quad M'.securing\downarrow.\eUnALT{} M ′.arousal ↑,
M ′.valence ↑,
M ′.securing ↓.
Simultaneously, MAGUS urge checks use g_{over}^{Ind}gInd
over to suppress any g_igi that threaten 
stability and use g_{over}^{Trans}gT rans
over
to boost subgoals aligned with adaptive growth. 
Decision Monad D. After appraisal, each candidate action aa is scored by 
 \bUnALT{}\mathrm{score}(a) = \sum_i w_i\,g_i\,m_i\,\mathrm{corr}(a,g_i) - \lambda_{Ind}\,g_{over}^{Ind} + \lambda_{Trans}\,g_{over}^{Trans},\eUnALT{} score(a) =

i
wi gi mi corr(a, gi) −λInd gInd
over + λT rans gT rans
over ,
where m_imi is the relevant modulator and \mathrm{corr}corr measures alignment with g_igi. The  
update 
 \bUnALT{}D\bigl((G,M)\bigr)=(G+\Delta G,\,M)\eUnALT{} D

(G, M)

= (G + ΔG, M)
boosts those goals most in need while respecting both overgoals.

From MetaMo to OpenPsi
407
Dynamics and Stability. Deﬁne a safe region 
 \bUnALT{}R={(G,M)\mid g_{over}^{Ind}\ge\theta_{safe}\land\|G\|\le G_{\max}}.\eUnALT{} R = {(G, M) | gInd
over ≥θsafe ∧∥G∥≤Gmax}.
Near \partial R∂R, the contraction condition 
 \bUnALT{}d\bigl(F(x),F(y)\bigr)\le c\,d(x,y)+\varepsilon\quad(c<1)\eUnALT{} d

F(x), F(y)

≤c d(x, y) + ε
(c < 1)
with high g_{over}^{Ind}gInd
over ensures updates pull the state back toward safety. Well inside RR, 
when g_{over}^{Trans}gT rans
over
is higher, constraints relax and exploration is favored. 
Self-Continuity and Incremental Learning. State updates blend prior and tar-
get via 
 \bUnALT{}x_{t+1}=(1-\alpha)x_t+\alpha x^*,\quad \alpha=\alpha_0\bigl(1-g_{over}^{Ind}\bigr)+\beta_0\,g_{over}^{Trans},\eUnALT{} xt+1 = (1 −α)xt + αx∗,
α = α0

1 −gInd
over

+ β0 gT rans
over ,
so that individuation slows change for coherence and transcendence speeds adap-
tive growth. 
In this way, the dual overgoals g_{over}^{Ind}gInd
over and g_{over}^{Trans}gT rans
over
embed MAGUS's meta-
level drives into MetaMo's formal dynamical guarantees, integrating OpenPsi's 
context-sensitive appraisal with MAGUS's hierarchical decision in a stable, ﬂex-
ible framework for Hyperon AGI. 
5
Application: Inference Control 
PLN, the core logical inference engine within Hyperon and PRIMUS, faces - like 
all other general-purpose logic engines - combinatorial blowup when applying 
inference rules. By integrating MetaMo appraisal (Psi) and decision (D) with  
OpenPsi and MAGUS now featuring two overgoals g_{over}^{Ind}gInd
over (individuation) and 
g_{over}^{Trans}gT rans
over
(transcendence)—we focus inference adaptively in four steps: 
Appraisal ( Psi ). Treat each candidate rule rr on subgraph HH as stimulus: 
 \bUnALT{}((G,M),r,H)\mapsto(G,M').\eUnALT{} ((G, M), r, H) →(G, M ′).
OpenPsi updates six modulators: novelty raises arousal,approach; con-
duciveness raises valence,resolution; cost/risk raises threshold,securing. 
MAGUS then uses g_{over}^{Ind}gInd
over to amplify threshold and securing when self-
preservation is critical, and uses g_{over}^{Trans}gT rans
over
to amplify arousal and approach when 
adaptive growth is desired. 
Decision ( D). MAGUS scores each (r,H)(r, H) by 
 \bUnALT{}\mathrm{score}(r,H) = \sum_{i} g_{i}\,m'_{i}\,\mathrm{relevance}_{i}(r,H) \;-\;\lambda_{Ind}\,g_{over}^{Ind} \;+\;\lambda_{Trans}\,g_{over}^{Trans},\eUnALT{} score(r, H) =

i
gi m′
i relevancei(r, H) −λInd gInd
over + λT rans gT rans
over ,
where g_{i}gi are goal intensities, m'_{i}m′
i updated modulators, and relevance measures 
task alignment. The top-kk inferences are executed; others are pruned or deferred.

408
R. Lian and B. Goertzel
Dynamic Allocation and Pruning. Allocation of breadth versus depth now also 
depends on overgoals: high novelty and g_{curio}gcurio with strong g_{over}^{Trans}gT rans
over
widen the 
search to embrace exploration; high threshold and g_{ethic}gethic with strong g_{over}^{Ind}gInd
over
enforce stricter conﬁdence cutoﬀs; high resolution and g_{help}ghelp deepen proofs 
on the most relevant subgraphs. Near resource or risk limits, a high g_{over}^{Ind}gInd
over raises 
the eﬀective penalty to throttle inference, while a high g_{over}^{Trans}gT rans
over
can soften it to 
allow controlled exploration. 
Incremental Self-Model Coherence. Blend updates smoothly by 
 \bUnALT{}x_{t+1}=(1-\alpha)x_{t}+\alpha\,x^{*},\qquad \alpha=\alpha_{0}\,(1-g_{over}^{Ind})+\beta_{0}\,g_{over}^{Trans},\eUnALT{} xt+1 = (1 −α)xt + α x∗,
α = α0 (1 −gInd
over) + β0 gT rans
over ,
so that individuation slows the step size for self-coherence and transcendence 
speeds adaptive growth. 
In this way, the dual overgoals g_{over}^{Ind}gInd
over and g_{over}^{Trans}gT rans
over
guide both the focus and 
the pacing of PLN inference, embedding MetaMo's dynamical guarantees into 
Hyperon's scalable reasoning. 
5.1
Detailed Hypothetical Example 
To conceptually illustrate how this intersection of motivation and inference con-
trol might work in practice, we brieﬂy explore here an interesting if controversial 
example: Suppose a Hyperon system must decide whether to adopt a promising 
but potentially unsafe self-upgrade. We apply the MetaMo appraisal comonad 
Psi and decision monad D, with two MAGUS overgoals: individuation g_{over}^{Ind}gInd
over
enforcing self-preservation and contractivity, and transcendence g_{over}^{Trans}gT rans
over
encour-
aging adaptive growth. 
Appraisal ( Psi). Each rule rr on subgraph HH is a stimulus (G,M),r,H)\mapsto(G,M')((G, M), r, H) →
(G, M ′). OpenPsi updates M'M ′ by novelty (\mathit{arousal,approach}arousal, approach), conduciveness 
(\mathit{valence,resolution}valence, resolution), and cost/risk (\mathit{threshold,securing}threshold, securing). MAGUS then scales 
these updates: g_{over}^{Ind}gInd
over ampliﬁes threshold and securing to suppress risky acti-
vations, while g_{over}^{Trans}gT rans
over
ampliﬁes arousal and approach to favor exploratory sub-
goals. For example, a novel low-conﬁdence proof yields 
 \bUnALT{}M'.arousal\uparrow,\quad M'.threshold\uparrow,\quad M'.resolution\downarrow,\eUnALT{} M ′.arousal ↑,
M ′.threshold ↑,
M ′.resolution ↓,
whose net eﬀect on M'.securingM ′.securing is increased if g_{over}^{Ind}gInd
over is high, or tempered if 
g_{over}^{Trans}gT rans
over
is high. 
Decision ( D). We consider inference tasks r_1r1 (eﬃcacy), r_2r2 (safety), r_3r3 (ethics), 
r_4r4 (user-trust). MAGUS scores each by 
 \bUnALT{}\mathrm{score}(r_k) = \sum_i g_i\,m'_i\,\mathrm{rel}_i(r_k) - \lambda_{Ind}\,g_{over}^{Ind}\,\mathrm{risk}(r_k) + \lambda_{Trans}\,g_{over}^{Trans},\eUnALT{} score(rk) =

i
gi m′
i reli(rk) −λInd gInd
over risk(rk) + λT rans gT rans
over ,
where g_igi are goal intensities, m'_im′
i updated modulators, \mathrm{rel}_ireli alignment measures, 
and \mathrm{risk}(r_k)risk(rk) estimates potential ethical breach. Top-kk inferences run; others are 
deferred or pruned.

From MetaMo to OpenPsi
409
Adaptive Allocation. When g_{over}^{Trans}gT rans
over
is high, high arousal with g_{\mathit{self}}gself broadens 
eﬃcacy proofs; when g_{over}^{Ind}gInd
over is high, high threshold with g_{\mathit{ethic}}gethic enforces strict 
cutoﬀs; high resolution with g_{\mathit{help}}ghelp deepens relevant proofs. Near risk/resource 
boundaries, g_{over}^{Ind}gInd
over raises the eﬀective penalty to throttle inference, while g_{over}^{Trans}gT rans
over
can soften it to allow controlled exploration (Fig 1). 
Incremental Blending. Each proof yields a target state x^*x∗. The agent blends by 
 \bUnALT{}x_{t+1}=(1-\alpha)x_t + \alpha\,x^*,\quad \alpha = \alpha_0\,(1 - g_{over}^{Ind}) + \beta_0\,g_{over}^{Trans},\eUnALT{} xt+1 = (1 −α)xt + α x∗,
α = α0 (1 −gInd
over) + β0 gT rans
over ,
so that high individuation slows updates for self-coherence and high transcen-
dence speeds adaptive growth. 
Outcome. If the safety proof succeeds (g_{\mathit{ethic}}\uparrow,\ \mathit{threshold}\downarrowgethic ↑, threshold ↓), eﬃcacy gains 
(g_{\mathit{self}}\uparrowgself ↑), and ethics check passes, then g_{over}^{Ind}gInd
over decreases permitting an incremental 
upgrade while g_{over}^{Trans}gT rans
over
guides the pace of adaptation under continuous monitoring 
(Fig 2). 
Fig. 1. OpenPsi modulators during the 
hypothetical example. 
Fig. 2. MAGUS goal intensities during 
the hypothetical example. 
6
Conclusion 
We have presented a uniﬁed approach for embedding abstract motivational 
principles into concrete AGI systems by instantiating the MetaMo framework 
with OpenPsi appraisal and MAGUS decision processes. By introducing dual 
overgoals for individuation and transcendence, our design supports both cau-
tious self-preservation and adaptive self-expansion, while OpenPsi's modulators 
enable context-sensitive appraisal and MAGUS's monadic scoring guides multi-
objective decision making. The resulting pseudo-bimonad structure on the moti-
vational state space ensures compositional semantics, contractive stability near 
safety boundaries, and smooth incremental self-model updates, all of which we 
demonstrated in Hyperon's PLN inference control and a research-assistant use 
case. 
This integration delivers a motivational engine that is both formally grounded 
and practically eﬀective: it balances service, curiosity, novelty, ethics and social

410
R. Lian and B. Goertzel
drives; focuses inference eﬀort where it matters most; and guarantees stability 
as the agent explores and self-modiﬁes. Future work will include empirical eval-
uation in simulated and real-world tasks, systematic comparison with alterna-
tive motivational architectures, and reﬁnement of overgoal dynamics to support 
richer meta-cognitive capabilities. 
References 
1. Baars, B.J., Franklin, S.: How conscious experience and working memory interact. 
Trends Cogn. Sci. 7(4), 166-172 (2003) 
2. Bach, J.: Principles of Synthetic Intelligence. Oxford University Press (2009) 
3. Body, N.: Metamo: a robust motivational framework for open-ended AGI. In: Pro-
ceedings of the 25th International Conference on Artiﬁcial General Intelligence 
(AGI-25) (2025). submitted 
4. Cai, Z., Goertzel, B., Geisweiller, N.: Openpsi: realizing dörner's "PSIâĂİ cogni-
tive model in the opencog integrative AGI architecture. Springer-Verlag, Berlin, 
Heidelberg (2011) 
5. Cai, Z., Goertzel, B., Zhou, C., Zhang, Y., Jiang, M., Yu, G.: Dynamics of a 
computational aﬀective model inspired by dorners psi theory. Cogn. Syst. Res. 
17-18, 63-80 (2012). https://doi.org/10.1016/j.cogsys.2011.11.002, https://www. 
sciencedirect.com/science/article/pii/S1389041711000647 
6. Cai, Z., Goertzel, B., Zhou, C., Zhang, Y., Jiang, M., Yu, G.: Dynamics of a 
computational aﬀective model inspired by dorner's PSI theory. Cognitive Systems 
Research (2011) 
7. Dorner, D.: PSI: a schematic model of motivational and emotional processes for 
autonomous agents. Cogn. Syst. Res. 1(2), 123-138 (1998) 
8. Goertzel, B.: OpenCog hyperon: realizing the AGI dream. https://hyperon. 
opencog.org. Accessed 19 June 2025 
9. Goertzel, B., et al.: Opencog Hyperon: a framework for AGI at the human level 
and beyond. arXiv preprint arXiv:2310.18318 (2023). https://arxiv.org/abs/2310. 
18318 
10. Laird, J.E., Newell, A., Rosenbloom, P.S.: Soar: an architecture for general intelli-
gence. Artif. Intell. 33, 1-64 (1987) 
11. Mikeda, A.: Modular adaptive goal and utility system (magus). https:// 
deepfunding.ai/proposal/modular-adaptive-goal-and-utility-system-magus/ 
(2024). Accessed 14 May 2025 
12. Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration 
by self-supervised prediction. In: Proceedings of the International Conference on 
Machine Learning (2017) 
13. Rao, A.S., Georgeﬀ, M.P.: BDI agents: from theory to practice. In: Proceedings of 
the International Conference on Multi-Agent Systems (1995) 
14. Schmidhuber, J.: Formal theory of creativity, fun, and intrinsic motivation (1990- 
2010). IEEE Trans. Auton. Ment. Dev. 2(3), 230-247 (2010) 
15. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press 
(1998) 
16. Weinbaum, D.R., Veitas, V.: Open ended intelligence: the individuation of intelli-
gent agents (2015)

Integrating Functionalities to a System via 
Autoencoder Hippocampus Network 
Siwei Luo1,2(B) 
1 Jiangxi Normal University, Nanchang, Jiangxi 330022, China 
2 Jiangxi Provincial Key Laboratory of Intelligent Information Processing and 
Aﬀective Computing, Nanchang, China 
luosiwei@jxnu.edu.cn 
Abstract. Integrating multiple functionalities into a system poses a fas-
cinating challenge to the ﬁeld of deep learning. While the precise mech-
anisms by which the brain encodes and decodes information, and learns 
diverse skills, remain elusive, memorization undoubtedly plays a pivotal 
role in this process. In this article, we delve into the implementation 
and application of an autoencoder-inspired hippocampus network in a 
multi-functional system. We propose an autoencoder-based memoriza-
tion method for policy function's parameters. Speciﬁcally, the encoder 
of the autoencoder maps policy function's parameters to a skill vector, 
while the decoder retrieves the parameters via this skill vector. The policy 
function is dynamically adjusted tailored to corresponding tasks. Hence-
forth, a skill vector graph neural network is employed to represent the 
homeomorphic topological structure of subtasks and manage subtasks 
execution. 
Keywords: Memory · autoencoder hippocampus network · 
parametrized policy function · skill vector graph · cognitive function 
1
Introduction 
Recently, deep learning models, designed and optimized to excel at a particular 
problem, are task-speciﬁc. Multi-functionality capability is a fascinating topic 
and memorization plays an important role in this integration and incorporation 
process. 
The crucial structure hippocampus locates in the medial temporal lobe of 
the brain, speciﬁcally within the limbic system [ 1]. Named for its resemblance 
to the shape of a seahorse, the hippocampus plays a vital role in various cog-
nitive functions, particularly memory. The hippocampus is composed of several 
distinct regions, including the dentate gyrus, the cornu ammonis ﬁelds, and the 
subiculum. These regions work together to facilitate the encoding, consolidation, 
and retrieval of memories. The hippocampus is integral to this process, helping 
to encode and store these memories for later retrieval. In addition to its role 
in declarative memory, the hippocampus also plays a signiﬁcant role in spatial 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 411-420, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_36

412
S. Luo
navigation and orientation. It is involved in the formation of cognitive maps, 
which help us navigate our environment and understand spatial relationships. 
The hippocampus is highly interconnected with other brain regions, including 
the cortex, amygdala, and thalamus. These connections allow it to integrate 
information from various sources and coordinate cognitive processes. 
The neuroimaging technique functional magnetic resonance imaging (fMRI) 
measures changes in blood ﬂow and oxygen consumption in the brain associated 
with neuronal activity [ 2]. This technology utilizes magnetic ﬁelds and radio 
waves to produce images of the brain that highlight regions of activation during 
various cognitive tasks or in response to external stimuli. When neurons are 
active, they consume more oxygen and nutrients, leading to increased blood ﬂow 
in the local region to meet the metabolic demands [ 3]. fMRI provides detailed 
insights into brain activity. 
Dynamical Hierarchical Reinforcement Learning (DHRL) is an active and 
advanced ﬁeld within reinforcement learning that incorporates hierarchical struc-
tures to tackle complex and dynamic environments [ 4, 5]. The essential idea of 
DHRL is decompose task into subtasks, solving the task in divide-and-conquer 
manner, aiming to reduce the complexity of task and enhance learning eﬃciency 
that subject to long-term dependencies and sparse reward signal in reinforce-
ment learning. Every vertice in graph represents a subtask and corresponding 
edge between subtasks indicates dependencies and priorities among subtasks. 
Maxq method decomposes the value function of a complex task into the sum of 
value functions of several subtasks. It signiﬁcantly reduces the number of state-
action pairs to be considered, thereby improving learning eﬃciency [ 6]. FeUdal 
Networks [ 7] adopt a manager-worker architecture, where high-level managers 
set goals and low-level workers execute the speciﬁc actions to achieve these goals. 
This architecture enables the model to eﬀectively handle long-term dependen-
cies. 
Meta-learning is an advanced paradigm in deep learning where models 
acquire the ability to generalize across diﬀerent tasks by learning from past 
experiences. Instead of training a model from scratch for each new task, meta-
learning enables rapid adaptation to new tasks with minimal data and compu-
tational resources [ 8]. 
The primary function of memory is to trade space for time while coordinat-
ing modules involved in cognitive processes and behavior. Although DHRL and 
meta-learning demonstrate advantages and potential in the self-adaptive learning 
process, the practice suggests that we shall better separate memorization from 
task management and decision-making as a middleware bridging them. The very 
observation is: ﬁrst, a speciﬁc region in the brain is in charge of memory and 
plays a crucial role in this procedure of coordination and execution; second, the 
brain doesn't change the structure during the procedure but consume diﬀerent 
amount of resources. To mimic this process, this paper introduces a design that 
integrate functionalities to a single system architecture via autoencoder memo-
rization mechanism and tasks graph management. The main contributions of this 
article are summarized as below: 1) memorization mechanism is implemented via

Integrating Functionalities to a System
413
Autoencoder Hippocampus network; 2) motivation mechanism is implemented 
and embedded into the system via graph neural network. 
2
Supervised Learning Based on Classical Control Theory 
Reinforcement learning techniques enables agent to learn optimal behavior 
strategies through interactions with environment [ 9] without the need for explicit 
human supervision or labeling state-action pair datasets. This reduces the 
amount of human eﬀort required for training and makes the process more scal-
able. Signiﬁcant progress has been made in terms of algorithmic advancements, 
simulation environments, and computing power. Advantage Actor-Critic(A2C) 
[ 10], Asynchronous Advantage Actor-Critic(A3C) [ 11], and Proximal Policy 
Optimization(PPO) [ 12] are reinforcement learning algorithms that utilize diﬀer-
ent techniques to learn optimal policies for sequential decision-making problems. 
A2C and A3C utilize the Actor-Critic framework with an advantage function, 
while PPO focuses on achieving stable and eﬃcient policy updates through a 
clipped surrogate objective function and an adaptive KL penalty term. The avail-
ability of high-quality simulation environments, such as OpenAI Gym [ 13], Isaac 
Gym [ 14], etc., has greatly accelerated reinforcement learning research. These 
environments provide a testbed for reinforcement learning algorithms, allowing 
researchers to quickly iterate and evaluate novel ideas. The increasing availabil-
ity of powerful computing hardware, including GPUs and distributed computing 
clusters [ 15], has enabled researchers to train large-scale reinforcement learning 
models eﬃciently. In reinforcement learning, a policy function \pi(s|\mathbf{W}):s \rightarrow aπ(s|W) : s →a
is a mapping from states to actions, where ss is state, aa is action and \mathbf{W}W is 
parameters of policy neural network. It speciﬁes the behavior of an agent within 
an environment. Given a state, the policy function determines which action the 
agent should take to maximize the cumulative reward over time, serving as the 
agent's decision-making mechanism. The essence idea of reinforcement learning 
generally, is the use of value functions to organize and structure the search for 
good policies [ 16]. 
Reinforcement learning often faces diﬃculties in grasping long-term depen-
dencies and managing sparse reward signals. One eﬀective strategy to boost 
learning eﬃciency, when feasible, is to incorporate knowledge of classical con-
trol. Many OpenAI classical control environments can be solved by classical 
control theory. Take the Lunar Lander environment, a classic environment that 
simulates the task of landing a spacecraft on the lunar surface, as an example. 
This problem can be solved by Proportional-Integral-Diﬀerential(PID) control 
[ 17]. Classical control theory provides a framework for designing and analyzing 
systems to achieve desired performance objectives through the use of feedback 
and advanced functional Fig 1. Classical controller, such as PID controller, is a 
mapping P(s):s \rightarrow aP(s) : s →a, calculates action vector from the observation or current 
state of the agent, which provides a set of ground truth for controlling the agent 
accomplish its task. Then, deep learning neural network can learn PID control 
through the objective function mean square error measuring the Euclidean dis-
tance between the output of policy function and PID controller, optimization

414
S. Luo
algorithms attain the optimal policy function parameters \mathbf{W}W : 
\bALT{} \mathbf{W} = argmin(MSE(\pi(s | \mathbf{W}),P(s)))\eALT{} W = argmin(MSE(π(s|W), P(s)))
(1) 
The policy function approximates the PID controller after training: 
\bALT{} \pi(s | \mathbf{W}) \approx P(s)\eALT{} π(s|W) ≈P(s)
(2) 
Fig. 1. Available classical control can serve as the ground truth for actions and assist 
in training the policy function through supervised learning. 
Moreover, the nonlinearity of neural network can represent mapping from 
state to action obtained from classical control approach such as Bellman opti-
mality equation [ 18- 20] or Linear Quadratic Regulator. In summary, classical 
control if available can be provided as ground truth of actions and help train 
policy function in supervised learning manner. 
3
Autoencoder Hippocampus Network 
For diﬀerent tasks, the same structured policy function can be trained with 
diﬀerent sets of parameter values. Saving and loading corresponding parameters 
values according to diﬀerent tasks is a feasible but trivial solution. If we learn a 
wealth of skills, such as reading, cooking, riding, etc., then save isolated countable 
ﬁles in the brain will make the ﬁle system hardly manageable, which is very 
unlikely the case. Thus, abstract memorization, eﬀective and eﬃcient encoding 
and decoding procedure, plays a very essential role in learning, inference and 
execution. 
Many neural networks are capable of memorization, including recurrent neu-
ral network, Long Short-Term Memory [ 21], gate recurrent unit [ 22], Hopﬁeld 
network [ 23], autoencoder [ 24, 25] and so forth. Memorization module concerns 
two key questions what information in what format shall be restored in mem-
ory and how to retrieve information from memory. An autoencoder is a type of 
neural network trained to learn eﬃcient data encoding and decoding [ 26], con-
sisting of an encoder that compresses the input data into a latent layer vector

Integrating Functionalities to a System
415
and a decoder that decompresses the vector back into a representation simi-
lar to the original input. The autoencoder is more suitable and preferable for 
the memorization module compared to other neural networks, as its encoder-
decoder architecture makes retrieving stored information more convenient and 
straightforward. 
In this design, parameters of policy function \mathbf{W}W are encoded and decoded 
by autoencoder hippocampus network. The encoder maps learned parameters 
value to a reduced dimensional latent layer vector called skill vector(or task 
vector) and the decoder maps latent layer vector, i.e. skill vector, to original 
parameters tensor. The latent layer of autoencoder provides an interface for 
retrieving restored information. 
The memorization autoencoder is a mapping consisting of an encoder E:R^m \rightarrow R^nE :
Rm →Rn and a decoder D:R^n \rightarrow R^mD : Rn →Rm. Policy function's parameters \mathbf{W}W
memorized by the autoencoder is: 
\bALT{} \mathbf{W}^{\prime} = D(E(\mathbf{W}))\eALT{} W′ = D(E(W))
(3) 
For a well-trained autoencoder, \mathbf{W}^{\prime}W′ should closely resemble \mathbf{W}W, and  the degree  
of similarity between them can be quantiﬁed by the Euclidean distance metric. 
The essence of this design lies in utilizing the autoencoder hippocampus network 
to derive a ﬁxed-structure policy function, parameterized by a set {\mathbf{W}_i}{Wi}, that is  
capable of executing actions tailored to various corresponding tasks. 
The autoencoder hippocampus network memorizes the learned parameters 
tensor. Importantly, memorization process discussed here doesn't remember any-
thing related to state, action, reward, etc., but the policy function's optimal 
parameters only. Given a skill vector, decoder of autoencoder recalls (or gen-
erates) parameters tensor and assigns them to the policy function. The policy 
function acts as a capacitor, what parameters tensors are ﬁlled in is task oriented. 
The skill vector \mathbf{S}S is encoded policy function parameters via the encoder: 
\bALT{} \mathbf{S} = E(\mathbf{W})\eALT{} S = E(W)
(4) 
The process of decoder recalling parameters \mathbf{W}W from the skill vector \mathbf{S}S can be 
written as: 
\bALT{} \mathbf{W} = D(\mathbf{S})\eALT{} W = D(S)
(5) 
Then, the action of policy function upon state ss reads: 
\bALT{} a = \pi(s | \mathbf{W}) = \pi(s | D(S)) = \pi(s | D(E(\mathbf{W})))\eALT{} a = π(s|W) = π(s|D(S)) = π(s|D(E(W)))
(6) 
It is impossible in real world to enumerate endless tasks or skills, that is, 
the learning and execution process shall be extensible and inferable Fig. 2. Mas-
tering the skill of verbal communication does not inherently equip one with the 
ability to ride a bicycle, as these domains involve distinct cognitive and physi-
cal processes. Conversely, proﬁciency in bicycle riding often lays a foundational 
understanding of balance, coordination, spatial awareness and mechanical con-
trol, which can facilitate the acquisition of motorcycle-riding skills due to over-
lapping competencies. The skill vectors within the latent layer of an autoencoder

416
S. Luo
Fig. 2. The parameters of the policy function can be encoded, stored, and retrieved 
via an autoencoder hippocampus network. 
embody the concept of Euclidean distance, where a shorter distance signiﬁes a 
higher degree of skill relevance and, consequently, a greater similarity in param-
eter values. The memorization module of an autoencoder is capable of inference, 
including zero-knowledge inference. Provided a skill vector, no matter whether it 
learned before, as an input for decoder of autoencoder, it will map to or generate 
a set of corresponding parameters value. Then, for execution phase of a task, the 
policy function loads corresponding parameters value and behaves accordingly. 
The design of the memory-embedded policy function involves maintaining a 
ﬁxed architecture for the policy function while utilizing a memory module to 
store the parameter values of the policy function during the learning process. 
During the exploitation procedure, the memory module is used to recall and 
assign the appropriate parameter values to the policy function. The parameter-
ized policy function neural network maintains a consistent architecture while 
adopting dynamic parameters tailored to speciﬁc tasks. 
4
Execution via Traversing Skill Vector Graph 
The brain has the ability to decompose a task into a set of subtasks based on 
common sense, prior knowledge, inference, and other cognitive processes. Some 
subtasks may have higher priority than the others and form a graph structure 
in nature. The homeomorphic relationship between subtasks and skill vectors 
exhibits a one-to-one, bijective mapping. Equivalently, breaking down a task 
into a graph of subtasks is akin to constructing a graph for skill vectors. Let 
G=(\mathbf{S},e)G = (S, e) be a task graph with subtask vertices \mathbf{S}S and edges e. Graph Neural 
Networks(GNNs) [ 27, 28] capture the dependencies and structures within skill 
vector graph [ 29]. With the autoencoder, encoded skill vector help recall and 
deploy parameters tensor to policy function, providing an interface for tasks 
graph management module. When a complicated task can be divided to a topo-
logical structure of vector skills set {\mathbf{S}_i}{Si}, the accomplishment of the task can be 
executed according to skill vector graph traversal: 
\bALT{}\bALT{} a_i = \pi(s | W_i) = \pi(s | D(\mathbf{S}_i))\eALT{} \bALT{}\bALT{} a_i = \pi(s | W_i) = \pi(s | D(\mathbf{S}_i))\eALT{} ai = π(s|Wi) = π(s|D(Si))
(7)

Integrating Functionalities to a System
417
Putting an elephant into a refrigerator requires three steps: 1) Open the 
refrigerator; 2) Put the elephant inside; 3) Close the refrigerator Fig. 3. Nodes  
in skill vector graph are embedded in a Euclidean space, and edges can be rep-
resented by vector between connected skill vectors as well. Skill vector graph 
occupies both Euclidean properties and graph connections typically combines 
the characteristics of geometric space and topological representations. The spec-
trum of skill vector graph on Euclidean data refers to the analysis of graph 
structures and their corresponding eigenvalues derived from key matrices such 
as the Laplacian matrix when working with data embedded in Euclidean space 
[ 30]. 
Fig. 3. The latent layer of an autoencoder hippocampus network provides an interface 
for retrieving the parameters of the policy function. By traversing the graph of skill 
vectors, a complex task can be completed, and a corresponding sequence of behaviors 
can be generated. 
The dynamics of traversing skill vector graph requires to know subtask at 
hand and determine the phase of task (Fig. 4). The cognitive function neural 
network, a mapping C:s \rightarrow S_iC : s →Si, can be trained to determine subtask confronted, 
select the very skill vector and drive the dynamics of traversing skill vector graph: 
\bALT{} S_i = C(s)\eALT{} Si = C(s)
(8) 
Cognitive function recognizes if the refrigerator is open or not and whether 
elephant is inside refrigerator to decide the skill vector. The state, the input of 
policy function, undergoes a bifurcation of computing procedure and ultimately 
determines the parameters of policy function as well. Variable parameterized 
policy function assigned by decoder of autoencoder hippocampus network DD and 
managed by cognitive function CC, enhance the capability of control procedure: 
\bALT{} a_i = \pi(s | W_i) = \pi(s | D(\mathbf{S}_i)) = \pi(s | D(C(s)))\eALT{} ai = π(s|Wi) = π(s|D(Si)) = π(s|D(C(s)))
(9) 
In summary, the minimal prerequisites for generating a sequence of control 
encompass: 1) an autoencoder hippocampus mechanism capable of memorizing 
and recalling parameters of the policy function; 2) construct skill vector topo-
logical graph for the task; and 3) dynamics related to traversing the skill vector 
graph.

418
S. Luo
Fig. 4. How to put an elephant into a refrigerator 
5
Conclusion 
Cognitive activity is a symphony orchestrated within the brain, where diverse 
neural processes harmonize to compose the melody of thought, perception, moti-
vation and creativity. Dynamically integrating an autoencoder hippocampus net-
work and tasks graph neural network management into the system, parameters 
tensors can be assigned to the policy function through the autoencoder hip-
pocampus network. This architecture decouples memorization and task execu-

Integrating Functionalities to a System
419
tion from decision-making, fostering an interpretable system tailored to diverse 
tasks. With the incorporation of the autoencoder hippocampus network, the 
system can demonstrate rich and diverse dynamic behaviors. 
Acknowledgments. This work is supported by Jiangxi Province Technological Inno-
vation Base Program under Grant No.20242BCC32021 and Science and Technol-
ogy Research Project of Jiangxi Provincial Department of Education under Grant 
no.GJJ2200377. 
References 
1. Eichenbaum, H., Dudchenko, P., Wood, E., Shapiro, M., Tanila, H.: The hippocam-
pus, memory, and place cells: is it spatial memory or a memory space? Neuron 
23(2), 209-226 (1999) 
2. Heeger, D.J., Ress, D.: What does fMRI tell us about neuronal activity? Nat. Rev. 
Neurosci. 3(2), 142-151 (2002) 
3. Logothetis, Nikos K., Pauls, J., Augath, M., Trinath, T., Oeltermann., A : Neu-
rophysiological investigation of the basis of the fMRI signal. Nature 412(6843), 
150-157 (2001) 
4. Tuomas, H., Hartikainen, K., Abbeel, P., Levine, S.: Latent space policies for hier-
archical reinforcement learning. In: International Conference on Machine Learning, 
pp. 1851-1860. PMLR (2018) 
5. Freek, S., Schaal, S.: Hierarchical reinforcement learning with movement primitives. 
In: 2011 11th IEEE-RAS International Conference on Humanoid Robots, pp. 231-
238. IEEE (2011) 
6. Dietterich, T.G.: Hierarchical reinforcement learning with the MAXQ value func-
tion decomposition. J. Artif. Intell. Res. 13, 227-303 (2000) 
7. Peter, D., Hinton, G.E.: Feudal reinforcement learning. Adv. Neural Info. Process. 
Syst. 5 (1992) 
8. Vilalta, R., Drissi, Y.: A perspective view and survey of meta-learning. Artif. Intell. 
Rev. 18, 77-95 (2002) 
9. Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement learning: a survey. J. 
Artif. Intell. Res. 4, 237-285 (1996) 
10. Chu, T., Wang, J., Codecà, L., Li, Z.: Multi-agent deep reinforcement learning for 
large-scale traﬃc signal control. IEEE Trans. Intell. Transp. Syst. 21(3), 1086-1095 
(2019) 
11. Du, J., et al.: Resource pricing and allocation in MEC enabled blockchain systems: 
an A3C deep reinforcement learning approach. IEEE Trans. Netw .Sci. Eng. 9(1), 
33-44 (2021) 
12. Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E.: Reinforcement learning for 
combinatorial optimization: a survey. Comput. Oper. Res. 134, 105400 (2021) 
13. Praveen, P.: Hands-On Intelligent Agents with OpenAI Gym: Your guide to devel-
oping AI agents using deep reinforcement learning. Packt Publishing Ltd (2018) 
14. Serrano-Muñoz, A., Chrysostomou, D., Bøgh, S., Arana-Arexolaleiba, N.: SKRL: 
modular and ﬂexible library for reinforcement learning. J. Mach. Learn. Res. 
24(254), 1-9 (2023) 
15. Jacky, L., Makoviychuk, V., Handa, A., Chentanez, N., Macklin, M., Fox, D.: Gpu-
accelerated robotic simulation for distributed reinforcement learning. In: Confer-
ence on Robot Learning, pp. 270-282. PMLR (2018)

420
S. Luo
16. Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduc-
tion. Vol. 1, no. 1. Cambridge: MIT press, 1998 
17. Carl, K.: PID control. IEEE Control Syst. Mag. 26(1), 30-31 (2006) 
18. Bellman, R., Lee, E.S.: Functional equations in dynamic programming. Aequa-
tiones Math. 17(1), 1-18 (1978). https://doi.org/10.1007/BF01818535 
19. Shige, P.: A generalized dynamic programming principle and Hamilton-Jacobi-
Bellman equation. Stochast. Int. J. Probab. Stoch. Process. 38(2), 119-134 (1992) 
20. Dreyfus, S.: Richard Bellman on the birth of dynamic programming. Oper. Res. 
50(1), 48-51 (2002) 
21. Sherstinsky, A.: Fundamentals of recurrent neural network (RNN) and long short-
term memory (LSTM) network. Physica D 404, 132306 (2020) 
22. Yuan, G., Glowacka, D.: Deep gate recurrent neural network. In: Asian Conference 
on Machine Learning, pp. 350-365. PMLR (2016) 
23. Hopﬁeld, J.J.: Neural networks and physical systems with emergent collective com-
putational abilities. Proc. Natl. Acad. Sci. 79(8), 2554-2558 (1982) 
24. Han, K., et al.: Variational autoencoder: an unsupervised model for encoding and 
decoding fMRI activity in visual cortex. Neuroimage 198, 125-136 (2019) 
25. Ricardo, M., Puentes, J., Uriza, L.F., Hoyos, M.H.: Single-slice Alzheimer's disease 
classiﬁcation and disease regional analysis with supervised switching autoencoders. 
Comput. Biol. Med. 116, 103527 (2020) 
26. Junhai, Z., Zhang, S., Chen, J., He, Q.: Autoencoder and its various variants. In: 
2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 
pp. 415-419. IEEE (2018) 
27. Zhou, J., et al.: Graph neural networks: a review of methods and applications. AI 
Open 1, 57-81 (2020) 
28. Zhang, Z., Chen, X., Liu, K., Shaohua, X., Huang, L.: A resource optimization 
scheduling model and algorithm for heterogeneous computing clusters based on 
GNN and RL. J. Supercomput. 80(16), 24138-24172 (2024) 
29. Yao, Z., Jun, Yu., Zhang, J., He, W.: Graph and dynamics interpretation in robotic 
reinforcement learning task. Inf. Sci. 611, 317-334 (2022) 
30. Bojan, M., Alavi, Y., Chartrand, G., Oellermann, O.: The Laplacian spectrum of 
graphs. Graph Theory Comb. Appl. 2(12), 871-898 (1991)

IBGP: Imperfect Byzantine Generals 
Problem for Zero-Shot Robustness in 
Communicative Multi-agent Systems 
Yihuan Mao1(B) 
, Yipeng Kang2 
, Peilun Li3 
, Wei  Xu1 
, 
and Chongjie Zhang4 
1 Tsinghua University, Beijing, China 
maoyh20@mails.tsinghua.edu.cn 
2 Beijing Institute of Artiﬁcial General Intelligence, Beijing , China 
kangyipeng@bigai.ai 
3 Shanghai Tree-Graph Blockchain Research Institute, Beijing , China 
4 Washington University in St. Louis, St. Louis, USA 
chongjie@wustl.edu 
Abstract. As we move towards the era of AGI, AI agents increasingly 
integrate into our infrastructure, making their robust coordination and 
message synchronization vital for ensuring reliable and secure multi-
agent interactions. The Byzantine Generals Problem (BGP) is a criti-
cal model for constructing resilient multi-agent systems (MAS) under 
adversarial attacks. It describes a scenario where malicious agents with 
unknown identities exist in the system-situations that, in our context, 
could result from LLM agents' hallucinations or intentional attacks. In 
BGP, the objective of the entire system is to reach a consensus on the 
action to be taken. Traditional BGP requires global consensus among 
all agents; however, in practical scenarios, global consensus is not always 
necessary and can even be ineﬃcient. Therefore, a reﬁned version of BGP 
that aligns with the MAS local coordination patterns is needed. We refer 
to it as Imperfect BGP (IBGP). To tackle this issue, we propose a frame-
work that leverages consensus protocols within general MAS settings, 
providing provable resilience against communication attacks and adapt-
ability to changing environments, as validated by empirical results. ^11(For 
a full version of the paper including appendix, please refer to https:// 
arxiv.org/abs/2410.16237. 
Keywords: Multi-agent Systems · Zero-shot Robustness · Safety · 
Byzantine Generals Problem 
1
Introduction 
With the advancement of AI technology, the world is entering a new era where 
AI agents will become a signiﬁcant part of our infrastructure. In the foreseeable 
Yihuan Mao and Yipeng Kang—These authors contributed equally. 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 421-432, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_37

422
Y. Mao et al.
Fig. 1. An illustration of the motivation for proposing IBGP. (a) Agents coordinate on 
a task that requires four participants. (b) The presence of malicious agents capable of 
sending misleading messages may lead to the failure of coordination among the origi-
nal four agents. As a result, some redundancy is necessary to ensure safe coordination 
through consensus protocols. (c) Traditional methods for achieving overall consensus 
involve developing BGP protocols, but these are ineﬃcient and require excessive redun-
dancy in coordination contexts because they're speciﬁcally designed for overall consen-
sus scenarios. The graph shows that BGP becomes infeasible when malicious agents 
exceed 33%. (d) We deﬁne the coordination problem under communicative attack as 
IBGP and propose tailored protocols for it. Our protocol is suitable for coordination 
settings and requires less redundancy, accommodating a higher percentage of malicious 
agents (up to 50%). 
future, groups of heterogeneous AI agents—of various types and functions—will 
frequently interact to solve temporary tasks. Coordination among diverse agents 
is crucial, akin to traditional human cooperation (Fig 1). 
Message synchronization is a key aspect of this coordination, especially in 
applications like sensor networks [ 17] and UAV control [ 6]. For heterogeneous 
agents, such as autonomous vehicles [ 31] driven by general-purpose large lan-
guage models, they are not speciﬁcally designed for message synchronization. 
Consequently, agents may lack a reliable predeﬁned broadcast module to ensure 
consistent messaging during synchronization. Discrepancies between their mes-
sages and actions can arise due to hallucinations or malicious behavior. Malicious 
agents can lead to disastrous consequences, such as tampering with vehicle-to-
vehicle (V2V) messages, potentially causing signiﬁcant property damage and 
loss of life. This security concern extends to various applications requiring reli-
able coordination, such as video generation [ 3], software development [ 9], and 
problem-solving [ 26]. 
Traditionally, this issue is related to the consensus problem in distributed sys-
tems, like the Byzantine Generals Problem (BGP) [ 13], where the primary objec-
tive is to synchronize content across all benign nodes and achieve system-wide 
consistency. In multi-agent system coordination, consensus serves as a means

IBGP
423
to achieve team goals, allowing for relaxed requirements; often, it suﬃces to 
synchronize only a minimal number of benign agents. 
To address coordination challenges in multi-agent systems, we formalize the 
issue, referring to it as the Imperfect Byzantine Generals Problem (IBGP). While 
it shares similarities with the classic BGP, IBGP fundamentally diﬀers by empha-
sizing partial consensus. We introduce a protocol speciﬁcally designed for IBGP, 
which demands less redundancy and accommodates a higher percentage of mali-
cious agents compared to existing BGP protocols. 
In the era of AGI, when AGI agents become public infrastructure and are 
widely deployed, they will frequently form heterogeneous temporary multi-agent 
systems. To act eﬀectively, they need to reach consensus on certain variables 
through communication. Our IBGP protocol is crucial in this context, as it can 
be integrated into agents through simple prompt instructions, leveraging their 
inherent capabilities. If an agent fails to understand or execute this protocol, or 
intentionally disrupts the process following an attack, it will be classiﬁed as a 
malicious node. As long as the number of such nodes remains low, our protocol 
can continue to function eﬀectively. Experiments demonstrate the eﬀectiveness 
of our consensus protocols in both IBGP and practical tasks. 
Our technical contributions are two-fold: (i) we deﬁne IBGP, highlighting 
the challenges of partial coordination in Multi-Agent Systems, and (ii) we devise 
an innovative consensus protocol that ensures secure coordination, validated 
through theoretical analysis and practical experimentation. 
2
Related Work 
The Byzantine Generals Problem [ 13] describes a scenario where a group of 
Byzantine generals must agree on a common plan of action, even though some of 
the generals may be traitors. This models a large type of distributed consistency 
problem in computer science. Byzantine consensus protocols in a distributed net-
work can be used to reach an agreement on a single value, even in the presence 
of faulty or malicious nodes. Important works of Byzantine consensus proto-
cols include Practical Byzantine Fault Tolerance (PBFT) [ 2] and Randomized 
Byzantine Generals [ 19]. PBFT is a solution to the Byzantine Generals Prob-
lem that is designed for practical use in distributed systems. Rabin's work [ 19] 
provided a solution to the Byzantine Generals Problem that did not require a 
centralized authority or a trusted third party. Instead, it introduced the idea of 
a randomized protocol where each node chooses a random value that is used to 
break ties in the event of conﬂicting messages. The randomization ensures that 
Byzantine nodes cannot predict the outcome of the protocol, making it more 
diﬃcult for them to interfere with the consensus process. 
MAS algorithms have been widely used in team coordination [ 12, 18, 28], 
robotics control [ 4, 5], and economic decision-making [ 10, 23, 24]. In the context 
of MAS research, one important research branch involves learning a commu-
nications system to achieve a common goal. Some use explicit communication 
systems with discrete or continuous signals, mainly to convey informative local

424
Y. Mao et al.
observations to each other, to deal with partial observation [ 11, 25]. Some use 
communication for global value optimization [ 1, 12]. Diﬀerent from these cate-
gories, in MAS research, consensus refers to achieving a global agreement over 
a particular feature of interest [ 6- 8, 14, 16, 17, 30]. It has been widely studied as 
it aﬀects communication and collaboration between agents. However, very few 
of them paid attention to adversarial attacks. Recently some researchers have 
investigated algorithms to mitigate the impact of malicious agents [ 22, 27]. How-
ever, their methodologies lack zero-shot adaption ability to the attackers and 
the environment. 
3
Imperfect Byzantine Generals Problem (IBGP) 
3.1
Preliminaries: BGP 
In BGP, there are nn benign agents and tt attacker agents communicating with 
each other through a complete network for a few rounds. Each agent begins 
with an initial message M^0\in{0,1}M 0 ∈{0, 1} as its initial proposal and ﬁnally makes a 
decision action a\in{0,1}a ∈{0, 1}. Generally, a value of 11 indicates cooperation, while 
00 indicates giving up. Agents decide what messages to send at the end of each 
round after reading messages from other agents. The attackers can send false 
messages to disturb coordination, but the agents are unaware of the identity of 
the communicating agents (benign or malicious). The formal deﬁnition of BGP 
is provided in Deﬁnition 1. Mis-coordination describes the situation where Agree-
ment is violated, and researchers have designed consensus protocols proven to 
prevent mis-coordination under any communicative attacks. For clarity, we also 
present an equivalent deﬁnition within the context of Reinforcement Learning, 
as depicted by the reward function in the left portion of Table 1. 
Deﬁnition 1. A system solves BGP if it meets the following under any attack: 
- Agreement. a_1 = a_2 = \cdots = a_n \in {0, 1}a1 = a2 = · · · = an ∈{0, 1}. 
(All nn benign agents must agree on the same action, either 00 or 11.) 
- Consistency. If M^0_1 = M^0_2 = \cdots = M^0_n = xM 0
1 = M 0
2 = · · · = M 0
n = x, then a_1 = a_2 = \cdots = a_n = xa1 = a2 = · · · = an = x. 
(When all nn agents have identical initial observations, their actions must 
also be identical as the initial one.) 
3.2
IBGP 
BGP serves as a fundamental concept of extensive research within the area of 
Decentralized Systems, embodying the crucial attributes of agreement and con-
sistency within a decentralized framework. Nevertheless, these properties may 
not always apply in many Multi-Agent Systems (MAS), where partial coordina-
tion is a common pattern rather than universal coordination. For example, in a 
predator-prey environment [ 21], only a subset of predators may be required to 
collaborate in hunting a particular prey, rather than involving all predators in 
the pursuit.

IBGP
425
Table 1. The diﬀerence of deﬁnition between BGP and IBGP. 
M 0 
i ∈{0, 1}, ai ∈{0, 1} 
BGP
IBGP 
n benign agents
n benign agents, coordination threshold k 
R = 
⎧ 
⎪
⎪
⎪
⎪
⎨ 
⎪
⎪
⎪
⎪
⎩ 
1(#(ai = 1) =  n)
if #(M 0 
i = 1) =  n, 
1(#(ai = 1) = 0)
if #(M 0 
i = 1) = 0, 
1(#(ai = 1) =  n) 
+1(#(ai = 1) = 0)  otherwise. 
R = 
⎧ 
⎪
⎪
⎪
⎪
⎨ 
⎪
⎪
⎪
⎪
⎩ 
1(#(M 0 
i = 1, ai = 1)  ≥ k)
if #(M 0 
i = 1) =  n, 
1(#(M 0 
i = 1, ai = 1) = 0)
if #(M 0 
i = 1)  < k,  
1(#(M 0 
i = 1, ai = 1)  ≥ k) 
+1(#(M 0 
i = 1, ai = 1) = 0)  otherwise. 
To capture this coordination pattern within MAS, we introduce IBGP in 
Deﬁnition 2, where successful agreement necessitates the cooperation of only 
kk. Besides, only the agents with the initial observation M^0=1M 0 = 1  are permitted 
to take the cooperative action a=1a = 1. The two properties of Agreement and 
Consistency are redeﬁned to align with the partial coordination prevalent in 
MAS. Similarly, mis-coordination is deﬁned as the situation 0<#(M^0_i=1,a_i=1)<k0 < #(M 0 
i = 1, ai = 
1) < k, where  Agreement is violated. It means that some agents try to coordinate 
but fail. Just like BGP, the goal of IBGP is to avoid mis-coordination robustly. 
Table 1 illustrates the distinctions between IBGP and the original BGP. 
Deﬁnition 2. A system solves IBGP if it meets the following under any attack: 
- Agreement. #(M^0_i = 1, a_i = 1) \in {0} \cup [k, n]#(M 0 
i = 1, ai = 1)  ∈{0}∪[k, n]. (At  least  kk agents that observe 
M^0=1M 0 = 1  are required to cooperate; otherwise no agent should act.) 
- Consistency. If #(M^0_i = 1) = n#(M 0 
i = 1) =  n, then #(M^0_i = 1, a_i = 1) \geq k#(M 0 
i = 1, ai = 1)  ≥ k. (If the 
number of available agents is super-suﬃcient, cooperation must happen.) 
Fig. 2. Examples of mis-coordination 
under diﬀerent \lambdaλs using single-round 
decision. 
Fig. 3. Examples of successful coordination 
via iterations of our protocol. 

426
Y. Mao et al. 
Didactic Example. To understand the challenge of IBGP, we illustrate that 
a single-round decision process is inadequate for addressing such issues through 
a didactic example. First, it's obvious that a reasonable single-round decision 
process for agent ii is to take a_i=1ai = 1  if the number of received signals exceeds 
a given threshold, i.e., #(M_{\cdot\rightarrow i}=1)\ge\lambda#(M·→i = 1)  ≥ λ, where  \lambdaλ is the threshold of the 
decision process. For simplicity, we assume the strategy shares with all agents. 
In Fig. 2, the  IBGP  with  n=5,t=1,k=3n = 5, t  = 1, k  = 3  includes 55 benign agents and 11 
attacker. Figure 2a shows an example of mis-coordination when the threshold 
of the single-round process is set to \lambda=4λ = 4. The number of initial active honest 
agents #(M^0_i=1)=3#(M 0 
i = 1) = 3. After sending messages, agents sum up the received 
messages, and some get 33 messages while others get 44 messages. This results 
in that the agent who receives 33 messages give up and causes mis-coordination. 
The example in Fig. 2b shows that mis-coordination could still happen even if 
we raise the threshold. When the threshold is set to \lambda=5λ = 5, the failed example 
happens in a diﬀerent initialization, where there are 44 active honest agents. After 
sending messages, the sum of received messages are 44 and 55 respectively. Since 
the agents receiving 44 messages give up and the agents receiving 55 messages still 
try to coordinate, the mis-coordination happens again. 
4
Method 
4.1
Consensus Protocol for IBGP 
Fig. 4. A general framework for consen-
sus protocol. 
Fig. 5. The learnable action 
selection with the consensus 
protocol. 
To solve IBGP, we propose a consensus protocol as illustrated in Fig. 4. This  
protocol employs a multi-round broadcast pattern and incorporates the concept 
of an independent global randomizer (implemented similarly with [ 19]). In each 
round, a randomized bit variable determines whether it is the last round (with 
a value of 1 indicating the ﬁnal round and 0 indicating that the process should 
continue). Agents broadcast their proposals and deal with the received proposals 
under the eﬀect of the randomized bit. The results serve as the proposals for the 
next round or as the ﬁnal decisions in the last round. The default proposals and 
decisions are both initialized to 0 if not speciﬁed. The framework amounts to 
the (k,\lambda)(k, λ)-protocol listed below: 
1. The global randomizer initializes the number of rounds from a distribution 
r_{tot}\sim\mathcal{R}rtot ∼R. (\mathcal{R}R is the sample distribution of the total number of rounds r_{tot}rtot in 
the IBGP Protocol. r_{tot}rtot isn't revealed until the last round arrives.) 

IBGP
427 
2. Initial round: Each agent ii broadcasts its initial proposal M_{i}^{0}M 0 
i to all agents. 
3. Round r\in{1\cdots r_{tot}}r ∈{1 · · ·  rtot}: Each agent i\in{i|M^{r-1}_i=1}i ∈{i|M r−1 
i
= 1} broadcasts M^{r}_{i}M r 
i , which  
equals to \mathbbm{1}(#_{j\in[N]}(M^{r-1}_{j\to i}=1)\ge k+\lambda)1(#j∈[N ](M r−1 
j→i = 1)  ≥ k + λ). 
4. Decision making round: Each agent i\in{i|M^{r_{tot}}_i=1}i\in{i|M^{r_{tot}}_i=1}i ∈{i|M rtot 
i
= 1} select action a_iai, which  
equals to \mathbbm{1}(#_{j\in[N]}(M^{r_{tot}}_{j\to i}=1)\ge k)1(#j∈[N ](M rtot 
j→i = 1)  ≥ k). 
When assigning \lambda=tλ = t, we have the following theorem: 
Theorem 1. (k,t)(k, t)-protocol is robust under any attack on IBGP(t, k) with a 
high level of conﬁdence 1-\max_{r}{p(r_{tot}=r)}1 − maxr{p(rtot = r)}. 
It is worth noting that nodes only transmit bit-level information in IBGP, 
which is quite aﬀordable compared to the communication methods commonly 
used in multi-agent systems. 
Didactic Example. We provide a didactic example to illustrate how the pro-
tocol operates. Proving the robustness of a method requires considering all 
potential scenarios, which can be space-intensive, or relying on the mathemat-
ical proof of Theorem 1. Therefore, we focus on how the protocol prevents the 
mis-coordination shown in the didactic example of IBGP in Sect. 3.2. 
The following  game is an IBGP with  n=5,t=1,k=3n = 5, t  = 1, k  = 3, including 55 
benign agents and 11 attacker, and we show how the (k,t)(k, t)-protocol works under 
diﬀerent initializations. In Fig. 3a, the intial number of active honest agents 
#(M^0_i=1)=3#(M 0 
i = 1) = 3. The threshold of a single round is dynamically decided by 
the global randomizer (from 33 and 44), and therefore the agent receiving 33 mes-
sages is probably both to continue or give up. If it continues, the active agents 
remains the same. If it gives up (the third subﬁgure), the number of active agents 
decreases to 22, which is a temporary mis-coordination. Luckily, such temporary 
mis-coordination only lasts for 11 round, and in the next round, all agents will 
give up due to the fewer received messages. Under the iterative protocol, the 
temporary mis-coordination can only exists in a single round, and since the real 
number of rounds is unknown to the attacker, the successful attack can only 
happen with a low probability. In Fig. 3b, since the threshold is 33 or 44, they will 
always coordinate. 
4.2
Integrating IBGP with Multi-agent Reinforcement Learning 
To manifest the eﬀectiveness of the IBGP consensus protocol, we invoke it as 
a coordination module in the Dec-POMDP task. The task consists of a tuple 
G{\texttt{=}}\langle I, S, A, P, R, \Omega, O, n, t, \gamma\rangleG=⟨I, S, A, P, R, Ω, O, n, t, γ⟩, where  II is the ﬁnite set of n+tn + t agents interacting 
and communicating with each other. nn of the agents are benign, and tt of them 
are malicious whose communication channels (instead of actions) are attacked. 
In Dec-POMDP, S,A,\OmegaS, A, Ω are the state, action and observation space. OO is the 
observation function, andPP is the transition function of the environment.RR is the 
global reward function, and \gamma\in[0, 1)γ ∈ [0, 1) is the discount factor. The whole system is 
trained by Q-learning [ 15], where the goal is to collectively maximize the global 
return \mathbb{E}_p[\sum_{t=0}^{\infty} \gamma^t R_t]Ep[∞ 
t=0 γt Rt]. The whole pipeline is shown in Fig. 5. The consensus 

428
Y. Mao et al. 
process serves as a subprocess within the broader POMDP framework. At each 
time step, s \in Ss ∈ S is the state of the environment. Each agent ii receives a partial 
observation of the state o_ioi drawn from \OmegaΩ, according to the observation function 
O(s, i)O(s, i). Then the agent selects an action a_iai from AA, based on its local action-
observation history \tau_i\in \mathrm{T}\equiv(\Omega\times A)^*\times \Omegaτi ∈ T ≡ (Ω × A)∗ × Ω. During the action selection process, 
the agent decices whether to select a normal action or propose a coordination 
consensus on a certain target. 
For instance, consider a predator-prey game, o_ioi encapsulates the states of 
neighboring grids relative to agent ii; and  a_iai includes possible movements like go 
up, go down, go left and go right, alongside the consensus-aware action propose 
to catch. Within the consensus process, M_i^{0}=1M 0 
i = 1  if a_iai is set to propose to catch, 
indicating the intention to pursue the prey, as depicted in Fig. 5 by an agent 
emitting both a dashed green line and a solid blue line. In contrast, M_i^{0}=0M 0 
i = 0  
under other actions, meaning that there's no propose, as depicted in Fig. 5 by an 
agent emitting both a solid green line and a dashed blue line. An agent would 
select an a_iai that maximizes its local Q-value based on o_ioi. When the agent's 
observation indicates a potential for coordination (e.g., o_ioi reveals proximity to 
the prey), the action propose to catch is chosen by the policy network, and M_i^{0}M 0 
i 
switches to 11, participating in the consensus process. If agents reach consensus 
on catch after the communication according to the protocol, the agent will then 
catch the prey. If agents reach consensus on not catching, the agents with catch 
proposals just give up cooperative catching at this step and do nothing. 
5
Experiments 
While Theorem 1 ensures that the proposed protocol guarantees safe coordina-
tion in the Imperfect Byzantine Generals Problem (IBGP), it raises intriguing 
questions: (i) Is the IBGP protocol applicable to the MARL setting, as dis-
cussed in Sect. 4.2, and does it outperform baselines in terms of robustness? 
(ii) How can we utilize the protocol in real-world applications? In this section, 
we compare the consensus-based method—integrating the IBGP protocol into 
the MARL pipeline—with existing RL-based approaches to evaluate robustness 
under attack. 
Fig. 6. The screenshot of 2 StarCraft II environments. 

IBGP
429 
The experiment includes four kinds of environments, denoted as 
Env(n,m,k,t)(n, m, k, t). Here, nn represents the number of benign agents, mm is the number 
of targets, kk is the coordination threshold, and tt is the number of attackers. 
- Predator-prey: It is modiﬁed from the well-known predator-prey environ-
ment [ 21], requiring several predators to hunt the prey together. We test 
single-target Predator-prey(4,1,2,1)(4, 1, 2, 1), multi-target Predator-prey(5,2,2,1)(5, 2, 2, 1), 
and large-scale Predator-prey(20,4,2,2)(20, 4, 2, 2) and Predator-prey(20,1,4,2)(20, 1, 4, 2). 
- Hallway: It is introduced in [ 25], requiring several agents to reach the desti-
nation simultaneously. Besides the environment with the original parameters 
Hallway(3,1,2,1)(3, 1, 2, 1), we also test its scalability with more agents and a larger 
coordination threshold in Hallway(10,1,5,2)(10, 1, 5, 2). 
- 4bane_vs_1hM : It is built on the SMAC benchmark (StarCraft Multi-
agent Challenge) [ 20]. 4bane_vs_1hM is an environment modiﬁed from 
3bane_vs_1hM, which is used in previous communicative MARL research. 
We add an agent because of the additional communicative attack. It requires 
the agents (3 banelings) to attack the enemy simultaneously; otherwise, 
the enemy will heal itself and no longer can be killed. The screenshots of 
4bane_vs_1hM and the following 3z_vs_1r are provided in Fig. 6. 
- 3z_vs_1r: We also create a new SC2 environment, in which two agents 
(zealots) are required to attack the enemy (a roach) simultaneously. If they 
fail to do so, the enemy's long-distance ﬁrepower will successively eliminate 
the agents, resulting in insuﬃcient damage to defeat the enemy. This task 
requires both resilience against attacks and eﬃciency. Waiting for coordinated 
action puts the agents in danger of being killed by the enemy. Therefore, it 
is essential to ﬁnd a balance between coordination with the fewest agents 
possible and defense against attacks. This task serves as a challenging bench-
mark and highlights the potential for improving eﬃciency and robustness 
simultaneously. 
We conduct experiments to demonstrate the zero-shot robustness of the 
IBGP protocol in the environments. Although the IBGP protocol deﬁnes how 
to communicate, the agents still need to learn a well-performing policy, which 
includes decisions on how to move and when to propose a consensus. To evalu-
ate the zero-shot robustness of algorithms, we deﬁne the robustness percentage 
as the ratio of performance with attackers (during the testing phase) to per-
formance with no attackers (during the training phase). Speciﬁcally, we train 
attackers' communication to harm the benign agents' performance to the great-
est extent in the testing phase, thus displaying zero-shot robustness against any 
attack scheme. If the algorithm is not robust against communicative attacks, the 
result of the testing phase will signiﬁcantly decay. 
Table 2 indicates the robustness percentage of the IBGP protocol in the ﬁrst 
column, while the second column displays the ratio of recursive training, which 
means that the training process of agents and attackers is repeated. Recursive 
training is one of the contributions of the algorithm in [ 27]. In the last two 
columns, AME [ 22] proposes a defense algorithm by taking the majority of mul-

430
Y. Mao et al. 
Table 2. The table illustrates the robustness percentages and their standard deviation 
of diﬀerent environments and algorithms. A higher robustness percentage indicates a 
higher level of resilience against zero-shot communicative attacks. Algorithms that fail 
to converge during training are marked with '/'. 
IBGP Protocol Recursive training AME
ADMAC 
Predator-prey(4,1,2,1)(4, 1, 2, 1)
96.1\pm5.4%96.1 ± 5.4%
0%0%
62.4\pm20.4%62.4 ± 20.4%25.6\pm13.2%25.6 ± 13.2% 
Predator-prey(5,2,2,1)(5, 2, 2, 1)
97.9\pm2.9%97.9 ± 2.9%
3.7\pm3.5%3.7 ± 3.5%
42.6\pm13.0%42.6 ± 13.0%14.0\pm7.7%14.0 ± 7.7% 
Predator-prey(20,4,2,2)(20, 4, 2, 2) 100.0\pm0%100.0 ± 0%
0%0%
79.3\pm12.1%79.3 ± 12.1% / 
Predator-prey(20,1,4,2)(20, 1, 4, 2) 100.0\pm0%100.0 ± 0%
16.5\pm16.4%16.5 ± 16.4%
100.0\pm0%100.0 ± 0% 
54.1\pm20.3%54.1 ± 20.3% 
Hallway(3,1,2,1)(3, 1, 2, 1)
96.7\pm4.7%96.7 ± 4.7%
0%0%
6.3\pm6.3%6.3 ± 6.3%
6.4\pm4.8%6.4 ± 4.8% 
Hallway(10,1,5,2)(10, 1, 5, 2)
100.0\pm0%100.0 ± 0%
/
13.1\pm7.2%13.1 ± 7.2% 7.4\pm10.4%7.4 ± 10.4% 
4bane_vs_1hM(4,1,3,1)(4, 1, 3, 1)98.4\pm2.2%98.4 ± 2.2%
20.4\pm6.4%20.4 ± 6.4%
92.9\pm4.7%92.9 ± 4.7% 64.5\pm13.6%64.5 ± 13.6% 
3z_vs_1r(3,1,2,1)(3, 1, 2, 1)
51.5\pm2.6%51.5 ± 2.6%
6.2\pm0.8%6.2 ± 0.8%
/
/ 
tiple randomly ablated message sets, and ADMAC [ 29] automatically reduces 
the impact of potentially harmful messages on the ﬁnal decision. 
Analysis of Table 2 reveals that the IBGP Protocol maintains its performance 
from training to testing phases. However, recursive training is not consistently 
robust when transitioning to testing in some environments. This is reasonable 
since the baseline paper [ 27] prioritizes adaption to attackers rather than zero-
shot robustness. The AME algorithm performs well in some environments during 
testing, although it experiences degradation in performance in others. Since the 
AME algorithm is designed to be robust through its novel design of taking the 
majority, we think that the performance decay is due to the fact that the action 
in the majority may be multiple, making attacks still possible. The ADMAC 
algorithm also exhibits a low level of robustness in our environments, likely 
because its reliability estimator was trained on speciﬁc types of attacks, with 
only minor disturbances evaluated in the original paper. However, in this work, 
we focus on robustness against arbitrary attacks, which leads to ADMAC under-
performing. In summary, policy learning with the IBGP Protocol is eﬀective in 
fostering robust coordination against zero-shot communicative attacks. 
6
Conclusion 
In this study, we introduce the Imperfect Byzantine Generals Problem (IBGP) as 
a framework for multi-agent coordination in the presence of compromised agents. 
The consensus protocol we have developed for IBGP eﬀectively ensures zero-
shot robustness. While our analysis and experiments demonstrates its potential 
applicability to practical tasks, the proposed IBGP protocol is only eﬀective 
in environments where a speciﬁc target exists, rather than arbitrary RL envi-
ronments. In future research, we intend to address this limitation by exploring 
the adaptability of a generalized trainable framework for consensus protocols to 
more complex and realistic settings. 

IBGP
431 
References 
1. Böhmer, W., Kurin, V., Whiteson, S.: Deep coordination graphs. In: Proceedings 
of the 37th International Conference on Machine Learning (2020) 
2. Castro, M., Liskov, B.: Practical byzantine fault tolerance. In: Proceedings of the 
Third Symposium on Operating Systems Design and Implementation, pp. 173-186. 
OSDI '99, USENIX Association, USA (1999) 
3. ChonghanYu, X.Z.: Animating the past: reconstruct trilobite via video generation 
(2024). https://doi.org/10.12074/202410.00084, https://chinaxiv.org/abs/202410. 
00084 
4. Dong, H., Wang, T., Liu, J., Zhang, C.: Low-rank modular reinforcement learning 
via muscle synergy. Adv. Neural. Inf. Process. Syst. 35, 19861-19873 (2022) 
5. Dong, H., Zhang, J., Wang, T., Zhang, C.: Symmetry-aware robot design with 
structured subgroups. In: International Conference on Machine Learning, pp. 8334- 
8355. PMLR (2023) 
6. Dorri, A., Kanhere, S.S., Jurdak, R.: Multi-agent systems: a survey. IEEE Access 
6, 28573-28593 (2018) 
7. Fan, M.C., Zhang, H.T., Wang, M.: Bipartite ﬂocking for multi-agent systems. 
Commun. Nonlinear Sci. Numer. Simul. 19(9), 3313-3322 (2014) 
8. Fu, J., Wang, J.: Adaptive coordinated tracking of multi-agent systems with quan-
tized information. Syst. Control Lett. 74, 115-125 (2014) 
9. Hong, S., et al.: MetaGPT: meta programming for multi-agent collaborative frame-
work. arXiv preprint arXiv:2308.00352 (2023) 
10. Hossain, S., Wang, T., Lin, T., Chen, Y., Parkes, D.C., Xu, H.: Multi-sender per-
suasion: a computational perspective. arXiv preprint arXiv:2402.04971 (2024) 
11. Kang, Y., Wang, T., de Melo, G.: Incorporating pragmatic reasoning communi-
cation into emergent language. Adv. Neural. Inf. Process. Syst. 33, 10348-10359 
(2020) 
12. Kang, Y., Wang, T., Yang, Q., Wu, X., Zhang, C.: Non-linear coordination graphs. 
Adv. Neural. Inf. Process. Syst. 35, 25655-25666 (2022) 
13. Lamport, L., Shostak, R., Pease, M.: The byzantine generals problem. ACM 
Trans. Program. Lang. Syst. 4(3), 382-401 (1982). https://doi.org/10.1145/ 
357172.357176, https://doi.org/10.1145/357172.357176 
14. Liu, S., Xie, L., Zhang, H.: Containment control of multi-agent systems by exploit-
ing the control inputs of neighbors. Int. J. Robust Nonlinear Control 24(17), 2803- 
2818 (2014) 
15. Mnih, V., et al.: Human-level control through deep reinforcement learning. Nature 
518(7540), 529-533 (2015) 
16. Olfati-Saber, R.: Flocking for multi-agent dynamic systems: algorithms and theory. 
IEEE Trans. Autom. Control 51(3), 401-420 (2006) 
17. Olfati-Saber, R., Murray, R.M.: Consensus problems in networks of agents with 
switching topology and time-delays. IEEE Trans. Autom. Control 49(9), 1520- 
1533 (2004) 
18. Qin, R., et al.: Multi-agent policy transfer via task relationship modeling. Sci. 
China Inf. Sci. 67(8), 182101 (2024) 
19. Rabin, M.O.: Randomized byzantine generals. In: 24th Annual Symposium on 
Foundations of Computer Science (SFCS 1983), pp. 403-409 (1983). https://doi. 
org/10.1109/SFCS.1983.48 
20. Samvelyan, M., et al.: The starcraft multi-agent challenge. CoRR abs/1902.04043 
(2019). http://arxiv.org/abs/1902.04043 

432
Y. Mao et al. 
21. Stone, P., Veloso, M.: Multiagent systems: a survey from a machine learning per-
spective. Auton. Robot. 8, 345-383 (2000) 
22. Sun, Y., Zheng, R., Hassanzadeh, P., Liang, Y., Feizi, S., Ganesh, S., Huang, F.: 
Certiﬁably robust policy learning against adversarial multi-agent communication. 
In: The Eleventh International Conference on Learning Representations (2023). 
https://openreview.net/forum?id=dCOL0inGl3e 
23. Wang, T., Dütting, P., Ivanov, D., Talgam-Cohen, I., Parkes, D.C.: Deep contract 
design via discontinuous networks. Adv. Neural. Inf. Process. Syst. 36, 65818-65836 
(2023) 
24. Wang, T., Jiang, Y., Parkes, D.C.: GemNet: menu-based, strategy-proof multi-
bidder auctions through deep learning. arXiv preprint arXiv:2406.07428 (2024) 
25. Wang*, T., Wang*, J., Zheng, C., Zhang, C.: Learning nearly decomposable value 
functions via communication minimization. In: International Conference on Learn-
ing Representations (2020). https://openreview.net/forum?id=HJx-3grYDB 
26. Wu, Q., et al.: AutoGEN: enabling next-gen LLM applications via multi-agent 
conversation framework. arXiv preprint arXiv:2308.08155 (2023) 
27. Xue, W., Qiu, W., An, B., Rabinovich, Z., Obraztsova, S., Yeo, C.K.: Mis-spoke or 
mis-lead: achieving robustness in multi-agent communicative reinforcement learn-
ing. In: AAMAS '22, International Foundation for Autonomous Agents and Mul-
tiagent Systems, Richland, SC (2022) 
28. Yang, Q., Dong, W., Ren, Z., Wang, J., Wang, T., Zhang, C.: Self-organized 
polynomial-time coordination graphs. In: International Conference on Machine 
Learning, pp. 24963-24979. PMLR (2022) 
29. Yu, L., Qiu, Y., Yao, Q., Shen, Y., Zhang, X., Wang, J.: Robust communica-
tive multi-agent reinforcement learning with active defense. Pro. AAAI Conf. 
Artif. Intell. 38(16), 17575-17582 (2024). https://doi.org/10.1609/aaai.v38i16. 
29708, https://ojs.aaai.org/index.php/AAAI/article/view/29708 
30. Yu, W., Chen, G., Cao, M., Kurths, J.: Second-order consensus for multiagent 
systems with directed topologies and nonlinear dynamics. IEEE Trans. Syst. Man 
Cybern. Part B (Cybernetics) 40(3), 881-891 (2009) 
31. Zhou, M., et al.: Smarts: an open-source scalable multi-agent rl training school for 
autonomous driving. In: Kober, J., Ramos, F., Tomlin, C. (eds.) Proceedings of the 
2020 Conference on Robot Learning. Proceedings of Machine Learning Research, 
vol. 155, pp. 264-285. PMLR (2021). https://proceedings.mlr.press/v155/zhou21a. 
html 

Variational Inference Optimized Using 
the Curved Geometry of Coupled Free 
Energy 
Kenric P. Nelson1(B), Igor Oliveira2, Amenah Al-Najaﬁ3, Fode Zhang4, 
and Hon Keung Tony Ng5 
1 Photrek, LLC, 56 Burnham St Unit 1, Watertown, MA, USA 
kenric.nelson@photrek.io 
2 Photrek, LLC, Recife, PE, Recife, Brazil 
3 Department of Mathematics, University of Kufa, 299G Najaf, Iraq 
4 Center of Statistical Research, School of Statistics, Southwestern University 
of Finance and Economics, Chengdu, Sichuan 611130, People's Republic of China 
5 Department of Mathematical Sciences, Bentley University Waltham, Waltham, 
MA, USA 
Abstract. We introduce an optimization framework for variational 
inference based on the coupled free energy, extending variational infer-
ence techniques to account for the curved geometry of the coupled expo-
nential family. This family includes important heavy-tailed distributions 
such as the generalized Pareto and the Student's t. By leveraging the 
coupled free energy, which is equal to the coupled evidence lower bound 
(ELBO) of the inverted probabilities, we improve the accuracy and 
robustness of the learned model. The coupled generalization of Fisher 
Information metric and the aﬃne connection. The method is applied 
to the design of a coupled variational autoencoder (CVAE). By using 
the coupling for both the distributions and cost functions, the recon-
struction metric is derived to still be the mean-square average loss with 
modiﬁed constants. The novelty comes from sampling the heavy-tailed 
latent distribution with its associated coupled probability, which has 
faster decaying tails. The result is the ability to train a model robust 
against severe outliers, while assuring that the training process is stable. 
The Wasserstein-2 or Fréchet Inception Distance of the reconstructed 
CelebA images shows the CVAE has a 3% improvement over the VAE 
after 5 epochs of training. 
Keywords: Machine Learning · Statistical Mechanics · Complex 
Systems 
1
Introduction 
Variational Inference (VI) [ 1] is a foundational methodology in artiﬁcial intelli-
gence, in which Bayesian learning is used to develop statistical models of com-
plex datasets. VI could play a signiﬁcant role in strengthening the capabilities 
of artiﬁcial general intelligence (AGI), given its relation to the neural hypoth-
esis of predictive coding [ 2], in which the forecasts of higher layer neurons are 
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 433-445, 2026. 
https://doi.org/10.1007/978-3-032-00686-8_38

434
K. P. Nelson et al.
compared with sensory data from lower layer neurons. Important VI algorithms 
include Stochastic Variational Inference [ 3, 4], which uses minibatches to increase 
the eﬃciency of Monte Carlo sampling; Stein Variational Gradient Descent [ 5], 
which uses a kernel function for non-parametric models; and Normalizing Flows 
[ 6, 7], which use invertible functions to compose a series of transformations for 
more complex models. We will demonstrate a generalization of VI using the Vari-
ational Autoencoder [ 8], which splits the learning into a latent probability model 
and deterministic encoder/decoder networks. The generalization will utilize the 
methods of nonextensive statistical mechanics to model nonlinear dependencies 
and increase robustness. 
VI seeks to minimize the Kullback-Liebler divergence between an approxi-
mate model q(z)q(z) and an intractable posterior distribution p(z|x)p(z|x) given a dataset 
xx. Due to the complexity of the marginal distributionp(x)p(x), this divergence cannot 
be computed directly. Instead an Evidence Lower Bound, which is the negative of 
the statistical Free Energy, is used for the optimization. The Free Energy consists 
of two components, a regularization term that measures the divergence between 
the latent posterior p(z|x)p(z|x) and its prior p(z)p(z), and a reconstruction accuracy that 
measures the log-likelihood (or cross-entropy) of the generated dataset given the 
original dataset. An important problem in AGI research is robust learning of 
complex models [ 9- 11]. This is an important part of generalized intelligence in 
which models need to be robust against either training with limited datasets or 
operational use cases beyond the training set. Towards that end, many general-
izations of the VAE have been proposed which modify the Free Energy function. 
Examples include use of the Wasserstein metric [ 12], adjusting the relative weight 
of the regularization and reconstruction loss [ 13, 14], and utilizing generalized 
divergence functions [ 15]. 
Here we examine a novel design for the Coupled VAE, which uses the Cou-
pled Exponential Family and its associated coupled entropy function, to improve 
the robustness and accuracy. The Coupled VAE enables the learning of heavy-
tailed models (or possibly compact-support models, though that domain is not 
reviewed here). The core innovation is the ability to learn models assuming the 
presence of extreme outliers while being able to complete the training with sam-
ples from a distribution guaranteed to have a ﬁnite variance. In Sect. 2, we intro-
duce the information geometry of the coupled exponential family, whose heavy-
tailed (or compact-support) properties induces a curved manifold. Section 3 
deﬁnes the Coupled Free Energy, including the process in which the heavy-tailed 
latent distribution is sampled using a modiﬁcation that assures faster decaying 
tails. This process enables training of robust models while assuring that the train-
ing process is stable. Section 4 reviews the performance of the Coupled VAE with 
the Celeb-A dataset. In the conclusion, Sect. 5, we describe future directions for 
this research.

VI Using Coupled Free Energy
435
2
The Coupled Exponential Family and Its Information 
Geometry 
The coupled exponential family provides a natural generalization of the classi-
cal exponential family of distributions by incorporating a measure of the degree 
of nonlinear coupling, \kappaκ, that modulates the long-range correlations between 
variates and the shape of the asymptotic tail decay. This coupling extends tra-
ditional statistical manifolds to a curved geometric setting [ 16- 18], where the 
Fisher information metric is generalized, resulting in a non-Euclidean structure. 
The coupled exponential family is heavy-tailed for \kappa>0κ > 0, exponential for \kappa=0κ = 0, 
and compact-support for -\sfrac{1}{d}<\kappa<0−1/d < κ < 0, where  dd is the dimension of the distribu-
tion. Here we evaluate the heavy-tailed domain, which is useful in assuring that 
AGI models are robust against outliers, however, there are also important use 
cases for the compact-support domain for decisive algorithms. 
The standard exponential family [ 19] of distributions is deﬁned by the prob-
ability density function (PDF): 
\bALT{} p(\boldsymbol{x}; \boldsymbol\theta) = \frac{h(\boldsymbol{x})}{Z(\eta(\boldsymbol\theta))} \exp\left( \eta(\boldsymbol\theta) \cdot T(\boldsymbol{x}) \right),\eALT{} p(x; θ) =
h(x)
Z(η(θ)) exp (η(θ) · T(x)) ,
(1) 
where \boldsymbol\thetaθ is the natural parameter, T(\boldsymbol{x})T(x) is the suﬃcient statistic, h(\boldsymbol{x})h(x) is a non-
negative function and Z(\eta(\boldsymbol\theta))Z(η(θ)) is the partition function ensuring normalization. 
In the coupled exponential family, we introduce a deformation via a coupled 
logarithm and exponential transformation, deﬁning a PDF as: 
\bALT{}\label{eq2} p_\kappa(\boldsymbol{x}; \boldsymbol\theta) = \frac{h(\boldsymbol{x})}{Z_\kappa(\eta(\boldsymbol\theta))} \exp_\kappa^{-\frac{1+d\kappa}{\alpha}} \left( \eta(\boldsymbol\theta) \cdot T(\boldsymbol{x}) \right),\eALT{} pκ(x; θ) =
h(x)
Zκ(η(θ)) exp
−1+dκ
α
κ
(η(θ) · T(x)) ,
(2) 
where \exp_\kappa(x)=\left(1+\kappa x\right)^\frac{1}{\kappa}expκ(x) = (1 + κx)
1
κ and its inverse is \ln_\kappa(x)=\frac{1}{\kappa}\left(x^\kappa-1\right)lnκ(x) = 1
κ (xκ −1) is the coupled 
generalizations of the exponential and logarithm functions, and T(\boldsymbol{x})T(x) is a known 
function, Z_\kappa(\eta(\boldsymbol\theta))Zκ(η(θ)) denotes the coupled normalizing function, \alphaα is a parameter 
to control the shape of the distribution near the location of the distribution, and 
dd is the dimension of the distribution. The exponent -\frac{1+d\kappa}{\alpha}−1+dκ
α
derives from the dd
derivatives of the cdf given a survival function that starts with \exp_\kappa(x)expκ(x). While 
related to the qq-exponential family [ 20, 21], there are important distinctions. This 
deﬁnition ensures that each parameter is a measure of a clear, physical property 
of complex systems. The functions hh and ZZ are deﬁned outside of the coupled 
exponential in order to keep their roles consistent with important members of 
the family. When brought into the coupled exponential function the coupled 
sum, \exp{A}\exp{B}=\exp(A \oplus_\kappa B)exp A exp B = exp(A ⊕κ B), where  A \oplus_\kappa B\equiv A+B+\kappa A BA ⊕κ B ≡A + B + κAB, must be used  
\bALT{}\label{sps11} \exp_\kappa^{-\frac{1+d\kappa}{\alpha}}\left(\eta(\boldsymbol\theta) \cdot T(\boldsymbol{x}) \oplus_\kappa \ln_\kappa \left(\frac{h(\boldsymbol{x})}{Z_\kappa(\eta(\boldsymbol\theta))} \right)^{-\frac{\alpha}{1+d\kappa}} \right).\eALT{} exp
−1+dκ
α
κ

η(θ) · T(x) ⊕κ lnκ

h(x)
Zκ(η(θ))
−
α
1+dκ 
.
(3) 
Two important members of the coupled exponential family are the generalized 
Pareto distribution (GPD) (\alpha=1α = 1, coupled exponential) and the Student's t (\alpha=2α =

436
K. P. Nelson et al.
2, coupled Gaussian). For these two distributions h(\boldsymbol{x})=1h(x) = 1 if natural parameters 
are unknown. The statistical challenge of heavy-tailed distributions is that the 
higher moments are undeﬁned or diverge. For the GPD and Student's t, the mm-th 
moment, E[x^m]E[xm], diverges if  \kappa \geq \frac{1}{m}κ ≥
1
m. Nonextensive statistical mechanics [ 22, 23] 
has developed a set of modiﬁed moments based on raising the distribution by a 
power and renormalizing. The coupled probability [ 24] raises the distribution to 
a power  q=1 + \frac{\alpha\kappa}{1 + d\kappa}q = 1 +
ακ
1+dκ, which is the Tsallis index and the fractional number of 
independent variates in the same state, 
\bALT{} P^{\left(1+\frac{\alpha\kappa}{1 + d\kappa}\right)}(x) \equiv \frac{\left(p(x)\right)^{1 + \frac{\alpha\kappa}{1 + d\kappa}}}{\int_X \left(p(x)\right)^{1 + \frac{\alpha\kappa}{1 + d\kappa}} \,dx}.\eALT{} P(1+
ακ
1+dκ)(x) ≡
(p(x))1+
ακ
1+dκ

X (p(x))1+
ακ
1+dκ dx
.
(4) 
The \alphaα parameter drops out when the coupled probability is used to compute 
coupled moments, 
\bALT{} \mathbb{E}_\kappa[x^m] \equiv \int_X x^mP^{\left(1+\frac{m\kappa}{1 + d\kappa}\right)}(x) \,dx.\eALT{} Eκ[xm] ≡

X
xmP(1+
mκ
1+dκ)(x) dx.
(5) 
Let \mathcal{S}S be the set of coupled exponential distributions, that is 
\bUnALT{} \mathcal{S}=\left{p_\kappa(\boldsymbol{x}; \boldsymbol\theta)\bigg|\int_{\boldsymbol{X}}p_\kappa(\boldsymbol{x}; \boldsymbol\theta) d\boldsymbol{x}=1, p_\kappa(\boldsymbol{x}; \boldsymbol\theta)\geq0, \boldsymbol\theta\in\Theta,\boldsymbol{x}\in \boldsymbol{X}\right}.\eUnALT{} S =

pκ(x; θ)
				

X
pκ(x; θ)dx = 1, pκ(x; θ) ≥0, θ ∈Θ, x ∈X

.
Then, the set \mathcal{S}S can be regarded as a statistical manifold with the parameter 
vector \boldsymbol\thetaθ playing the role of the coordinate system. Information geometry pro-
vides a powerful mathematical framework for analyzing and optimizing machine 
learning models by viewing probability distributions as points on a curved man-
ifold. The Fisher information metric acts as a natural Riemannian metric on 
statistical manifolds. The Fisher information metric can be used in natural gra-
dient descent, where it replaces the Euclidean metric used in standard gradient 
descent, which leads to faster convergence and improved stability, particularly in 
deep learning, reinforcement learning, and variational inference. Although, the 
performance results reported here do not include the generalized Fisher gradient, 
we provide the derivations in preparation for future research. 
The aﬃne connection is a tool from diﬀerential geometry that enables the 
deﬁnition of directional derivatives and the notion of parallel transport on curved 
spaces such as statistical manifolds [ 25]. The aﬃne connections oﬀer deep insight 
into the structure and learning dynamics of probabilistic models, which allow 
for the comparison of vectors in diﬀerent tangent spaces and the construction 
of geodesics. The following lemma gives the Fisher information metric and the 
aﬃne connections of the coupled exponential family. 
Lemma 1. Let \boldsymbol{\zeta} = (\boldsymbol{\theta}, \boldsymbol{x}, d, \alpha)ζ = (θ, x, d, α) and R(\boldsymbol{\zeta}) = h(\boldsymbol{x})^{-r} \cdot Z_\kappa(\eta(\boldsymbol\theta))^{r}R(ζ) = h(x)−r · Zκ(η(θ))r with r = \frac{\alpha\kappa}{1 + d\kappa}r =
ακ
1+dκ. 
The Fisher metric tensors and the aﬃne connection of the coupled exponential 
family can be given as follows, respectively, 
\bUnALT{} &&g_{ij} = \frac{1}{\alpha} \mathbb{E}_{\boldsymbol{X}}\left[B_1+B_2\right] \\ &&\Gamma_{ijk} = \frac{1}{\alpha^2} \cdot \mathbb{E}_{\boldsymbol{X}}\left[ (A_1 + A_2) \cdot (B_1 + B_2) \right]\eUnALT{} gij = 1
αEX [B1 + B2]
Γijk = 1
α2 · EX [(A1 + A2) · (B1 + B2)]

VI Using Coupled Free Energy
437
where 
\bUnALT{} A_1 &:= \left[T(\boldsymbol{x})\right] \left( \frac{\partial R(\boldsymbol{\zeta})}{\partial \theta_j} + \frac{\partial R(\boldsymbol{\zeta})}{\partial \theta_i} \right),\\ A_2 &:= \left( \frac{1}{\kappa} + T(\boldsymbol{x}) \cdot \boldsymbol{\theta} \right) \cdot \frac{\partial^2 R(\boldsymbol{\zeta})}{\partial \theta_j \partial \theta_i},\\ B_1 &:= T(\boldsymbol{x}) \cdot R(\boldsymbol{\zeta}),\\ B_2 &:= \left( \frac{1}{\kappa} + T(\boldsymbol{x}) \cdot \boldsymbol{\theta} \right) \cdot \frac{\partial R(\boldsymbol{\zeta})}{\partial \theta_k}.\eUnALT{} A1 := [T(x)]
∂R(ζ)
∂θj
+ ∂R(ζ)
∂θi

,
A2 :=
 1
κ + T(x) · θ

· ∂2R(ζ)
∂θj∂θi
,
B1 := T(x) · R(ζ),
B2 :=
 1
κ + T(x) · θ

· ∂R(ζ)
∂θk
.
The proof of Lemma 1 is in the attachment [ 26]. 
This modiﬁcation results in a curved manifold whose geometry deviates from 
the standard dually-ﬂat structure of classical exponential families. The curvature 
of the coupled exponential family inﬂuences the optimization landscape in varia-
tional inference. Speciﬁcally, the variational coupled free energy, or equivalently, 
the coupled ELBO of the inverse probabilities, now operates over a non-trivial 
geometric structure, modifying the trajectories of inference and learning dynam-
ics. This formulation suggests that variational inference architectures, including 
predictive coding, leveraging coupled geometries may achieve more robust uncer-
tainty quantiﬁcation and improved representation of complex data distributions. 
3
Coupled Free Energy and Variational Inference 
The coupled exponential family induces a generalization of information theory. 
The foundations of this are described by the literature on nonextensive statis-
tical mechanics; however, the resulting coupled entropy (H_\kappa)(Hκ) is equal to the 
Normalized Tsallis entropy divided by 1+\kappa1 + κ. The coupled entropy is deﬁned as: 
\bUnALT{} H_\kappa \left(\textbf{p}; \alpha, d \right) & = & \frac{1}{\alpha} \sum_{i=1}^{N} P_i^{(1+\frac{\alpha\kappa}{1+d\kappa})} \ln_\kappa \left(p_i^{\frac{-\alpha}{1+d\kappa}} \right)\\ & = & \frac{1}{\alpha} \ln_\kappa \left(\sum_{i=1}^{N} p_i^{1 + \frac{\alpha \kappa}{1+d\kappa}} \right)^{\frac{1 + d \kappa}{\alpha \kappa}}.\eUnALT{} Hκ (p; α, d) = 1
α
N

i=1
P
(1+
ακ
1+dκ )
i
lnκ

p
−α
1+dκ
i

= 1
α lnκ
 N

i=1
p
1+
ακ
1+dκ
i
 1+dκ
ακ
.
The structure of the coupled entropy is such that as the coupling increases the 
coupled logarithm term \ln_\kappa \left(p_i^{\frac{-\alpha}{1+d\kappa}} \right)\rightarrow\inftylnκ

p
−α
1+dκ
i

→∞faster as p_i\rightarrow0pi →0. At the same time for 
larger coupling, the coupled probability term, P_i^{(1+\frac{\alpha\kappa}{1+d\kappa})}P
(1+
ακ
1+dκ )
i
, deﬁnes a distribution 
with faster decaying tails. The coupled probability deﬁnes a subsampling of the 
original distribution constricted to q=(1+\frac{\alpha\kappa}{1+d\kappa})q = (1 +
ακ
1+dκ) random variables in the same 
state. This subsampling has the eﬀect of increasing the decay of the tail and 
thus reducing the outlier samples. There is thus a balance between a increasing 
the cost of outliers while averaging over a distribution with fewer outliers. We'll 
show that for machine learning this provides a methodology in which systems

438
K. P. Nelson et al.
can be trained to be robust against outliers while assuring that Monte Carlo 
sampling does not result in instability during the training. 
From the coupled information theory, we derive the coupled free energy 
(CFE) function which provides a non-euclidean metric. In the standard VAE, 
the learning objective based on the negative ELBO or FE is based on the 
euclidean geometry of the Gaussian distribution, the KullbackâĂŞLeibler (KL) 
divergence, and the log-likelihood [ 8]. However, many real life data distributions 
exist on manifolds which have non-Euclidean geometric structures [ 19]. The non-
euclidean manifold can be used to improve the accuracy and robustness of the 
learned model. Improvements in accuracy are also possible, since the multivari-
ate heavy-tailed distribution captures nonlinear correlations in the dataset. The 
robustness derives from the fact that CFE increases the penalties in the tails of 
the distribution. 
Both the divergence and log-likelihood components of the FE are general-
ized using the coupled algebra. Training a coupled VAE with the CFE was ﬁrst 
reported by Cao, et al. [ 27]; however, in that design samples were drawn directly 
from a Gaussian multivariate distribution. Here we report on the performance 
with a) a coupled Gaussian latent distribution, b) a CFE function with a match-
ing coupling parameter, and c) a training process that uses samples drawn from 
the coupled probability of the coupled Gaussian distribution ( Q^{\left(\frac{2\kappa}{1+d\kappa}\right)})(Q(
2κ
1+dκ)), allowing 
controlled tail behavior during optimization. This approach makes full utilization 
of the coupled information theory and results in two important innovations. First, 
the coupled log-likelihood still simpliﬁes to the mean-square average between 
the reconstructed and original data with only a modiﬁcation in the constants. 
Second, even extreme models assuming a delta-function like distribution with 
excessive outliers can be trained, since the training samples are guaranteed to 
be drawn from a less extreme distribution with a ﬁnite variance. 
Here, we denote the coupled free energy (CFE) by \mathcal{F}_\kappaFκ, which corresponds to 
the Evidence Lower Bound (ELBO) applied to the inverse of the probabilities. 
In the following, we extend the VAE to the case where both the prior and 
posterior are coupled Gaussian distributions, for which \alpha=2α = 2. Let  A_qAq and A_pAp
denote the normalization terms of the coupled posterior and prior distributions, 
respectively, where  A_q = \left(\ln_\kappa \left(\frac{1}{Z_q} \right)\right)^{- \frac{2}{1 + d \kappa}}Aq =

lnκ

1
Zq
−
2
1+dκ and A_p = \left[\ln_\kappa \left(\frac{1}{Z_p} \right)\right]^{- \frac{2}{1 + d \kappa}} Ap =

lnκ

1
Zp
−
2
1+dκ . 
Theorem 1 (Coupled Free Energy for Multivariate Coupled Gaus-
sian). Let \boldsymbol{x} \in \mathbb{R}^dx ∈Rd and \mathbf{z} \in \mathbb{R}^nz ∈Rn be random vectors, and suppose that the posterior 
q(\mathbf{z} \mid \boldsymbol{x})q(z | x) and the prior p(\mathbf{z})p(z) are both multivariate coupled Gaussian with their 
joint PDFs deﬁned as: 
\bUnALT{} f\left(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\iSigma}, \kappa \right) \equiv \begin{cases} \frac{1}{Z(\boldsymbol{\iSigma}, \kappa)} \left(1 + \kappa \left|(\boldsymbol{x} - \boldsymbol{\mu})^{\intercal} \boldsymbol{\iSigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right|\right)_+^{-\frac{1 + d\kappa}{2 \kappa}}, & \kappa \ne 0, \kappa>-\sfrac{1}{d}; \\ \frac{1}{(2\pi)^{d/2} |\boldsymbol{\iSigma}|^{1/2}} \exp\left(-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^\top \boldsymbol{\iSigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\right), & \kappa = 0. \end{cases} \\ \eUnALT{} f (x; μ, Σ, κ) ≡
⎧
⎨
⎩
1
Z(Σ ,κ)

1 + κ
(x −μ)⊺Σ−1(x −μ)
−1+dκ
2κ
+
,
κ ̸= 0, κ > −1/d;
1
(2π)d/2|Σ |1/2 exp

−1
2(x −μ)⊤Σ−1(x −μ)

,
κ = 0.
Given a Coupled Variational Autoencoder (CVAE) with \kappa \ne0κ ̸= 0, the Coupled 
Free Energy (CFE) takes the following form with a coupled divergence term plus 
a reconstruction loss term:

VI Using Coupled Free Energy
439
 \begin{aligned} \mathcal{F}_{\theta,\phi,\kappa}(\boldsymbol{x}) &= \frac{1}{2}\mathbb{E}_{\mathbf{z} \sim Q_\phi^{\left(\frac{2\kappa}{1+d\kappa}\right)}(\mathbf{z}\mid \boldsymbol{x})} \left[\ln_{\kappa}\left(p_\theta(\mathbf{z})^{\frac{-2}{1+d\kappa}}\right) - \ln_{\kappa}\left(q_\phi(\mathbf{z}\mid \boldsymbol{x})^{\frac{-2}{1+d\kappa}}\right) \right]\,d\mathbf{z} \\ &\quad +\frac{1}{2}\mathbb{E}_{\mathbf{z} \sim Q_\phi^{\left(\frac{2\kappa}{1+d\kappa}\right)}(\mathbf{z}\mid \boldsymbol{x})} \left[ \ln_{\kappa}\left( \exp_\kappa^{\frac{-(1+d\kappa)}{2}} \left( (\boldsymbol{x} - \bar{\boldsymbol{x}}_{x|z})^\top \mathbf{\Sigma}_{x|z}^{-1} (\boldsymbol{x} - \bar{\boldsymbol{x}}_{x|z}) \oplus_\kappa \ln_\kappa\left(\frac{1}{Z_q}\right)^{\left( \frac{-2}{1+d\kappa} \right)} \right)\right)^\frac{-2}{1+d\kappa} \right] \end{aligned} 
Fθ,φ,κ(x) = 1
2E
z∼Q(
2κ
1+dκ)
φ
(z|x)

lnκ

pθ(z)
−2
1+dκ

−lnκ

qφ(z | x)
−2
1+dκ

dz
+ 1
2E
z∼Q(
2κ
1+dκ)
φ
(z|x)
⎡
⎢⎣lnκ

exp
−(1+dκ)
2
κ

(x −¯xx|z)⊤Σ−1
x|z(x −¯xx|z) ⊕κ lnκ
 1
Zq
(
−2
1+dκ)
−2
1+dκ
⎤
⎥⎦
The CFE simpliﬁes to: 
 \begin{aligned} \mathcal{F}_{\theta,\phi,\kappa}(\boldsymbol{x}) &= -\frac{d\left(1+\kappa A_q\right)}{2} + \frac{1+\kappa A_p}{2} \left((\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)^T \mathbf{\Sigma}_p^{-1}(\boldsymbol{\mu}_p - \boldsymbol{\mu}_q) + \operatorname{tr}\left(\mathbf{\Sigma}_p^{-1} \mathbf{\Sigma}_q\right)\right) \\ &\quad - \frac{A_q}{2} + \frac{A_p}{2} - \mathbb{E}_{\mathbf{z} \sim Q_\phi^{\left(\frac{2\kappa}{1+d\kappa}\right)}(\mathbf{z} \mid \boldsymbol{x})} \left[ \frac{1}{2} \left( (\boldsymbol{x} - \bar{\boldsymbol{x}}_{x|z})^\top \mathbf{\Sigma}_{x|z}^{-1} (\boldsymbol{x} - \bar{\boldsymbol{x}}_{x|z}) \oplus_\kappa A_{x|z} \right) \right] \end{aligned} 
Fθ,φ,κ(x) = −d (1 + κAq)
2
+ 1 + κAp
2

(μp −μq)T Σ−1
p (μp −μq) + tr

Σ−1
p Σq

−Aq
2 + Ap
2 −E
z∼Q(
2κ
1+dκ)
φ
(z|x)
1
2

(x −¯xx|z)⊤Σ−1
x|z(x −¯xx|z) ⊕κ Ax|z

This result provides a closed-form expression for the coupled Free Energy 
in terms of the coupled divergence and reconstruction loss. We emphasize that 
the expectation in the CFE expression is taken with respect to a transformed 
sampling distribution, denoted as ( Q^{\left(\frac{2\kappa}{1+d\kappa}\right)}(\mathbf{z} \mid \mathbf{x}) )(Q(
2κ
1+dκ)(z | x)), rather than the original 
posterior density ( q(\mathbf{z} \mid \mathbf{x}))(q(z | x)). This transformation modiﬁes both the shape and 
the scale of the distribution, eﬀectively compressing the tails while preserving the 
location. This transformed distribution corresponds to a coupled Gaussian with 
an adjusted coupling parameter and precision matrix. The transformed coupling 
( \kappa_Q κQ) and scale matrix ( \mathbf{\Sigma}_Q ΣQ) are  given by:  
\bALT{} \kappa_Q &= \frac{\kappa_q}{1 + 2\kappa_q}, \multieq |\mathbf{\Sigma}_Q|^{-1} &= (1 + 2\kappa_q) |\mathbf{\Sigma}_q|^{-1}.\eALT{} κQ =
κq
1 + 2κq
,
(6) 
|ΣQ|−1 = (1  +  2κq)|Σq|−1 .
(7) 
These complementary modiﬁcations are such that the exponent of the distribu-
tion is modiﬁed while the multiplicative term is unchanged: 
 \bUnALT{}\kappa_q (\mathbf{z} - \boldsymbol{\mu}_q)^\top \mathbf{\Sigma}_q^{-1} (\mathbf{z} - \boldsymbol{\mu}_q) = \kappa_Q (\mathbf{z} - \boldsymbol{\mu}_q)^\top \mathbf{\Sigma}_Q^{-1} (\mathbf{z} - \boldsymbol{\mu}_q).\eUnALT{} κq(z −μq)⊤Σ−1
q (z −μq) = κQ(z −μq)⊤Σ−1
Q (z −μq).
Even if the latent distribution model is a delta function, \kappa\rightarrow\inftyκ →∞, the distribution 
used for the training will have a well-deﬁned mean and ﬁnite variance, \kappa\rightarrow\sfrac{1}{2}κ →1/2. 
The full derivation of the coupled Free Energy expression is available at [ 26]. 
4
Experimental Validation 
In this section, we show the initial results of the CVAE performance. To train 
the model, we consider the CelebA dataset (CelebFaces Attributes Dataset), a 
widely used dataset for computer vision tasks involving human faces. This data 
set contains 202,599202, 599 images of celebrity faces with high variability and visual 
richness, making it an ideal benchmark for learning complex data distributions 
and testing the performance of generative models. For testing purposes, we use 
an image size of 128\times128128 × 128 (original images are 178\times178178 × 178), a batch size of 6464
images, a learning rate of  5\times10^ {-4}5 × 10−4, and a latent dimension of d =10d = 10. 
The model was trained using Adam Optimizer and without a learning rate 
scheduler. Gradient clipping with a maximum norm of 10 was applied to pre-
vent exploding gradients and improve stability. To ensure reproducibility and

440
K. P. Nelson et al.
Fig. 1. Comparison of the Coupled Free Energy across diﬀerent values of the coupling 
parameter \kappaκ for both (A) training and (B) validation phases using diagonal covariance. 
The models are trained and validated using the coupled probability distribution QQ. 
The inset in panel (A) shows the shape of the coupled Gaussian distributions for each 
\kappaκ, displaying the algorithm's capability to train extreme heavy-tailed distributions. 
The error bars in panel (B) represent the standard deviation of the Free Energy values 
across each epoch over several batches. 
consistent weight initialization, all linear and transposed convolution layers were 
initialized using the Kaiming uniform method (for layers with LeakyReLU acti-
vations), while linear layers used Xavier initialization. Batch normalization layers 
were initialized with unit scale and zero bias, and their running statistics were 
reset when loading checkpoints, helping stabilize the internal covariate shift dur-
ing training. The CelebA data set was randomly split into 70%70% training, 15% 15%
validation, and 15%15% test sets, and the data was shuﬄed at the beginning of each 
epoch. The models were trained and validated using the coupled probability dis-
tribution QQ samples. For validation and testing, samples were drawn from the 
coupled Gaussian qq and its adjusted version QQ for comparison. A ﬁxed batch 
of validation images generated consistent reconstructions and samples across 
epochs for visual assessment. 
The architecture employed for the VAE and CVAE models consists of a 
convolutional encoder-decoder structure. The encoder is built from four convo-
lutional layers with increasing depth (32 to 256 channels), followed by batch 
normalization and LeakyReLU activation functions. The encoded feature map is 
ﬂattened and passed through two linear layers to produce the latent mean and 
log-variance vectors. The decoder mirrors the encoder structure with transposed 
convolutions, batch normalization, and LeakyReLU activations, culminating in 
a ﬁnal Sigmoid layer to map outputs to the [0, 1][0, 1] range.

VI Using Coupled Free Energy
441
In these initial results, we conducted experiments using PyTorch's automatic 
diﬀerentiation framework (autograd), which computes gradients based on the 
standard Euclidean geometry of the parameter space. Although this approach is 
suﬃcient for baseline comparisons and initial performance assessments, it does 
not account for the curved geometry induced by the coupled exponential family. 
Future work will incorporate the information geometric formulation introduced 
in this paper, using the analytically derived gradients and curvature-aware opti-
mization based on the coupled free energy. This is expected to improve the 
eﬃciency and stability of the learning convergence by aligning the optimization 
trajectory with the intrinsic geometry of the coupled statistical manifold. 
In Fig. 1 we show that the CVAE with \kappa = 10^{-5}κ = 10−5 produces a stationary 
energy similar to the standard VAE (\kappa=0)(κ = 0), but the CVAE with \kappa = 1, 10κ = 1, 10 and 
\kappa = 10^5κ = 105 shows signiﬁcantly lower coupled free energy with both the training and 
validation data. Additionally, the CVAE energy curves with positive coupling 
are less noisy and have fewer and smaller spikes, which may indicate that the 
coupled model was trained more robustly. Because the coupled free energy metric 
varies with the coupling, these measurements do not provide a direct comparison; 
however, as the coupling increases, the metric increases in sensitivity and can rise 
above the \kappa=0κ = 0 metric if the algorithm lacks robustness. For comparison, Fig. 2 
shows the standard and coupled VAE reconstructions, with the latter capturing 
higher ﬁdelity. 
Fig. 2. Comparison of the standard linear correlation and the long-range coupling 
models. (a) Original images and reconstructions with (b) Diagonal standard VAE and 
(c) Diagonal Coupled VAE with \kappa = 1κ = 1. 
We employ several widely accepted metrics to assess the quality of the gen-
erated reconstructions, including the Fréchet Inception Distance (FID) and the 
Kernel Inception Distance (KID). Lower FID values indicate a closer alignment 
between the real and generated image distributions, reﬂecting higher visual 
ﬁdelity and diversity. KID, in contrast, avoids the Gaussian assumption by 
directly computing the squared Maximum Mean Discrepancy (MMD) using a

442
K. P. Nelson et al.
polynomial kernel. To capture perceptual ﬁdelity, we report the Learned Percep-
tual Image Patch Similarity (LPIPS), a metric based on deep network embed-
dings that aligns closely with human visual perception. Additionally, we include 
pixel- and structure-level similarity metrics: Multi-Scale Structural Similarity 
Index (MS-SSIM), Peak Signal-to-Noise Ratio (PSNR), and Structural Similar-
ity Index (SSIM). Beyond ﬁdelity-based metrics, we evaluate distributional align-
ment using the Fréchet ResNet Distance (FRD), an alternative to FID based on 
ResNet-derived features. 
The best-performing \kappaκ values were 10^{-5}10−5, 10^{-1}10−1, and  10^{0}100, each achieving  a  
strong balance between statistical alignment (low FID, KID, and FRD) and 
visual or perceptual ﬁdelity (low LPIPS, high MS-SSIM, PSNR, and SSIM). 
Among these, \kappa = 10^{-5}κ = 10−5 yielded the best overall equilibrium, slightly outper-
forming others in FRD and reconstruction accuracy, while \kappa = 1κ = 1 achieved the 
highest Precision, indicating superior sample quality. The setting \kappa = 10^{-1}κ = 10−1 con-
sistently performed well across all metrics, highlighting it as a reliable and sta-
ble conﬁguration. The baseline model with \kappa = 0κ = 0 achieved reasonable statis-
tical alignment but underperformed in visual ﬁdelity. The use of the adjusted 
distribution QQ signiﬁcantly stabilized the algorithm, enabling proper running 
even with a delta-function-like distribution, \kappa = 10^5κ = 105. All metrics were computed 
over multiple reconstruction samples for each \kappaκ setting, and the uncertainties in 
parenthesis reported in Table 1 represent the standard deviations. 
Table 1. Comprehensive evaluation of reconstruction quality across diﬀerent values of 
\kappaκ using diagonal covariance in the Coupled Variational Autoencoder (CVAE). Metrics 
include FID, KID, LPIPS, MS-SSIM, PSNR, SSIM, FRD, Precision, and Recall, with 
the best results per metric highlighted in green. Lower FID, KID, LPIPS, and FRD 
values indicate better perceptual and distributional similarity, while higher MS-SSIM, 
PSNR, SSIM, Precision, and Recall reﬂect improved visual ﬁdelity and alignment. 
Reconstructions were generated using coupled Gaussian samples QQ, which reduce to 
standard Gaussian samples when \kappa = 0κ = 0. The results suggest that moderate coupling 
values (e.g., \kappa = 10^{-1}κ = 10−1 and \kappa = 10^{0}κ = 100) yield the best trade-oﬀ across most quality metrics. 
The number in parenthesis is the standard deviation spread of the last digit. 
κ
FID
KID
LPIPS MS-SSIM PSNR 
SSIM
FRD
Prec
Rec 
0.0
15.0(9) 0.212(3) 0.503(4) 
0.478(7) 14.3(2) 0.503(6) 105.0(6) 0.828(2) 0.556(7) 
10−5 15.0(7) 0.209(2) 0.460(1) 0.596(5) 17.6(9) 0.578(6) 103.5(6) 0.822(1) 0.569(5) 
10−2 15.3(6) 0.214(2) 0.579(4) 
0.323(4) 9.30(4) 0.361(4) 106.8(6) 0.753(1) 0.491(5) 
10−1 14.9(6) 0.209(2) 0.461(1) 0.595(0) 17.6(1) 0.577(0) 103.3(5) 0.838(1) 0.566(8) 
100 
14.9(6) 0.213(2) 0.461(1) 0.595(0) 17.6(2) 0.578(5) 104.3(5) 0.853(2) 0.541(6) 
101 
15.0(7) 0.209(2) 0.460(1) 0.594(5) 17.6(2) 0.577(3) 104.4(6) 0.825(4) 0.588(2) 
105 
15.8(1) 0.235(2) 0.473(10) 0.587(6) 17.3(2) 0.571(8) 107.9(6) 0.838(5) 0.513(5)

VI Using Coupled Free Energy
443 
5
Conclusion 
We present a generalization of VI, using the Coupled Free Energy (Coupled 
Evidence Lower Bound of the inverse probabilities), which enables the train-
ing of robust, complex models using robust samples. The method utilizes the 
geometric curvature induced by the heavy-tailed characteristics of the coupled 
exponential family of distributions. The example of the VAE is used to demon-
strate the improvement in the ability to learn complex datasets. The robustness 
of the models is improved by using a heavy-tailed latent distribution (the coupled 
Gaussian) and by increasing the cost of outliers via the coupled Free Energy. 
An original element of the design is that the statistical robustness of the learn-
ing is improved by drawing samples from the coupled probability (also known as 
an escort probability) of the latent distribution. The coupled probability, which 
raises the distribution to a power and renormalizes, reduces the distribution's 
scale and shape. The coupled probability samples are equivalent to conditioning 
the original distribution to subselect for samples in which qq of them are equal. 
The eﬀect is to ﬁlter extreme outliers from the training process, while still assur-
ing that the learned model will be robust against those outliers. While consistent 
with the theory of nonextensive statistical mechanics, this is a novel mechanism 
in that the coupled entropy is a compromise between the Tsallis entropy and the 
normalized Tsallis entropy. 
Preliminary evidence of the robustness of this CVAE design is provided by 
the signiﬁcant reduction in the CFE with \kappa=1κ = 1  versus the FE with \kappa=0κ = 0  
measurements. While these are diﬀerent metrics given the change in \kappaκ, non-
robust models would be susceptible to an increase over the FE metric. A 3% 
improvement in the accuracy of the reconstructions was measured using the 
Fréchet Inception Distance, which is equivalent to the Wasserstein-2 metric. 
While the Coupled Free Energy function derivation is complex, the machine 
learning implementation is relatively simple for a couple of reasons. 
- First, the coupled Gaussian (Student's t) is a well-established model. It was 
ﬁrst used by Guinness Brewing to improve their beer production. 
- Second, we establish how to train quite extreme heavy-tailed models, while 
drawing samples from signiﬁcantly faster decaying distributions. 
- The reconstruction likelihood is proven to be the same as the original Free 
Energy, a mean-square average, with just a modiﬁcation of the constants. 
Our future research will include the computation of the curved gradients 
using the coupled aﬃne connection to improve the speed and stability of the 
learning rate. This ﬁrst demonstration utilized the standard Euclidean gradients 
of the coupled Free energy. Additional research objectives are to explore methods 
to determine criteria for an optimal coupling value and the limits of how high the 
coupling can be set. Negative coupling values can also be valuable in producing 
models intended to be more decisive and extreme. The coupled VAE is just one 
example of the many techniques in VI that can be generalized to undertake more 
complex AI tasks. 

444
K. P. Nelson et al. 
References 
1. Blei, D.M., Kucukelbir, A., McAuliﬀe, J.D..: Variational Inference: a review for 
statisticians. J. Am. Statist. Assoc. 112(518), 859-877 (2017). ISSN 0162-1459. 
https://doi.org/10.1080/01621459.2017.1285773. URL  https://www.tandfonline. 
com/doi/full/10.1080/01621459.2017.1285773. Publisher: Taylor & Francis 
2. Friston, K., Kiebel, S.: Predictive coding under the free-energy principle. Phi-
los.  Trans.  Royal Soc. B Biol. Sci.  364(1521), 1211-1221 (2009). https://doi. 
org/10.1098/rstb.2008.0300. URL  https://royalsocietypublishing.org/doi/abs/10. 
1098/rstb.2008.0300 
3. Ketkar, N.: Stochastic gradient descent. In: Ketkar, N., (ed.), Deep Learning with 
Python: A Hands-on Introduction, pp. 113-132. Apress, Berkeley, CA (2017). ISBN 
978-1-4842-2766-4. https://doi.org/10.1007/978-1-4842-2766-4_8. URL  https:// 
doi.org/10.1007/978-1-4842-2766-4_8 
4. Amari, S.I.: Backpropagation and stochastic gradient descent method. Neu-
rocomputing 5(4), 185-196 (1993). ISSN 0925-2312. https://doi.org/10.1016/ 
0925-2312(93)90006-O. URL  https://www.sciencedirect.com/science/article/pii/ 
092523129390006O 
5. Liu, Q., Wang, D.: Stein variational gradient descent: a general purpose Bayesian 
inference algorithm. In: Advances in Neural Information Processing Systems, vol. 
29. Curran Associates, Inc., (2016). URL https://proceedings.neurips.cc/paper/ 
2016/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html 
6. Rezende, D., Mohamed, S..: Variational inference with normalizing ﬂows. In: Pro-
ceedings of the 32nd International Conference on Machine Learning, pp. 1530-1538. 
PMLR (2015). URL https://proceedings.mlr.press/v37/rezende15.html. ISSN: 
1938-7228 
7. Papamakarios, G., Nalisnick, E., Rezende, D.J., Mohamed, S., Lakshminarayanan, 
B.: Normalizing ﬂows for probabilistic modeling and inference. J. Mach. Learn. 
Res. 22(57), 1-64 (2021). ISSN 1533-7928. URL http://jmlr.org/papers/v22/19-
1028.html 
8. Diederik, P., Kingma., Welling, M.: An introduction to variational autoencoders. 
Found. Trends Mach. Learn. 12(4), 307-392 (2019). ISSN 19358245. https://doi. 
org/10.1561/2200000056 
9. Figurnov, M., Struminsky, K., Vetrov, D.: Robust variational inference (2016). 
URL http://arxiv.org/abs/1611.09226. arXiv:1611.09226 [cs] 
10. Akrami, H., Joshi, A.A., Li, J., Aydöre, S., Richard, M., Leahy.: A robust varia-
tional autoencoder using beta divergence. Knowl. Based Syst. 238, 107886 (2022). 
ISSN 0950-7051. https://doi.org/10.1016/j.knosys.2021.107886. URL  https:// 
www.sciencedirect.com/science/article/pii/S0950705121010534. 8 citations (Cross-
ref) [2023-08-28] 
11. Li, P., Hu, Q., Wang, X.: Federated learning meets Bayesian neural network: 
robust and uncertainty-aware distributed variational inference. Neural Netw. 185, 
107135 (2025). ISSN 0893-6080. https://doi.org/10.1016/j.neunet.2025.107135. 
URL https://www.sciencedirect.com/science/article/pii/S0893608025000140 
12. Tolstikhin, I., Bousquet, O., Gelly, S., Schoelkopf, B.: Wasserstein auto-encoders. 
In: Proceedings of International Conference on Learning Representations (2018). 
URL https://openreview.net/forum?id=HkL7n1-0b 
13. Higgins, I ., Matthey, L ., Pal, A., Burgess, C., Glorot, X.: beta-vae: Learning 
basic visual concepts with a constrained variational framework. In: 5th Interna-
tional Conference on Learning Representations ICLR 2017 (2017). URL https:// 
openreview.net/forum?id=Sy2fzU9gl 

VI Using Coupled Free Energy
445 
14. Burgess, C.P., et al.: Understanding disentangling in \beta β-VAE (2018). URL http:// 
arxiv.org/abs/1804.03599. arXiv:1804.03599 [cs, stat] 
15. Kobayashis, T.: Q-VAE for disentangled representation learning and latent dynam-
ical systems. IEEE Robot. Autom. Lett. 5(4), 5669-5676 (2020). URL https:// 
ieeexplore.ieee.org/abstract/document/9143393. Publisher: IEEE 
16. Saha, A., Karthik, B., Kurtek, S.: A geometric variational approach to 
Bayesian inference. J. Am. Statist. Assoc. 115(530), 822-835 (2020). ISSN 0162-
1459. https://doi.org/10.1080/01621459.2019.1585253. URL  https://doi.org/10. 
1080/01621459.2019.1585253 
17. Frank, P., Leike, R., Enßlin, T.A.: Geometric variational inference. Entropy 23(7), 
853 (2021). ISSN 1099-4300. https://doi.org/10.3390/e23070853. URL  https:// 
www.mdpi.com/1099-4300/23/7/853 
18. Frank,P.: Geometric variational inference and its application to Bayesian imag-
ing. Phys. Sci. Forum 5(1), 6 (2022). ISSN 2673-9984. https://doi.org/10.3390/ 
psf2022005006. URL  https://www.mdpi.com/2673-9984/5/1/6 
19. Amari, S.I.: Information geometry and its applications. Appl. Math. Sci. 194 
Springer Japan, Tokyo (2016). ISBN 978-4-431-55977-1 978-4-431-55978-8. https:// 
doi.org/10.1007/978-4-431-55978-8. URL  https://link.springer.com/10.1007/978-
4-431-55978-8 
20. Amari, S.I., Ohara, A.: Geometry of Q-exponential family of probability distribu-
tions. Entropy 13(6), 1170-1185 (2011). ISSN 1099-4300. https://doi.org/10.3390/ 
e13061170. URL  https://www.mdpi.com/1099-4300/13/6/1170 
21. Zhang, F., Tony Ng, H.K., Shi, Y.: Information geometry on the curved 
Q -exponential family with application to survival data analysis. Phys. A 
Statist. Mech. Appl. 512, 788-802 (2018). ISSN 03784371. https://doi.org/ 
10.1016/j.physa.2018.08.143. URL  https://linkinghub.elsevier.com/retrieve/pii/ 
S0378437118310835 
22. Tsallis, C.: Introduction to nonextensive statistical mechanics: approaching a com-
plex world. Springer Science & Business Media (2009). URL https://link.springer. 
com/book/10.1007/978-0-387-85359-8. Publication Title: Introduction to Nonex-
tensive Statistical Mechanics: Approaching a Complex World 
23. Abe, S.: Nonextensive statistical mechanics and its applications, vol. 560 of Lecture 
Notes in Physics 
24. Nelson, K.P., Umarov, S.R., Kon, M.A.: On the average uncertainty for systems 
with nonlinear coupling. Phys. A: statistical mechanics and its applications 468, 
30—43 (2017). ISSN 03784371. https://doi.org/10.1016/j.physa.2016.09.046. URL 
http://dx.doi.org/10.1016/j.physa.2016.09.046. Publisher: Elsevier B.V. 
25. Amari, S.I.: Information geometry in optimization, machine learning and statis-
tical inference. Front. Electric. Electron. Eng. China 5(3), 241-260 (2010). ISSN 
16733460. https://doi.org/10.1007/s11460-010-0101-3 
26. Amenah, A.N., Nelson, K.P., Oliveira, I.: Coupled free energy derivations 
(2025). 
URL 
https://anonymous.4open.science/r/Coupled-VAE-FC48/CVAE-
Results/CVI-AGI-25/Derivations%20for%20CVAE%20May2025.pdf 
27. Cao, S., Li, J., Nelson, K.P., Kon, M.A.: Coupled VAE: improved accu-
racy and robustness of a variational autoencoder. Entropy 24(3), 423 2(022). 
ISSN 1099-4300. https://doi.org/10.3390/e24030423. URL  https://www.mdpi. 
com/1099-4300/24/3/423. Number: 3 Publisher: Multidisciplinary Digital Pub-
lishing Institute 

Author Index 
A 
Abbott, Vincent 
I-1 
Abruzzo, Vincent 
I-12 
Agrawal, Pulin 
I-147 
Al-Najaﬁ, Amenah 
I-433 
Aloimonos, Yiannis 
II-241 
Antona, Hector 
I-291 
Atsumi, Yu 
I-1 
B 
Bai, Fengshuo 
II-370 
Bai, Yuxin 
I-17 
Bennett, Michael Timothy 
I-30, I-43 
Bennett, Timothy 
II-83 
Bringsjord, Alexander 
I-49 
Bringsjord, Selmer 
I-49, II-23 
Brown, Jason 
II-361 
C 
Cea, Ignacio 
I-60 
Chandaria, Shamil 
I-346 
Chang, Oscar 
I-72 
Chaudhari, Pratik 
I-17 
Clancy, Oisín Hugh 
I-84 
Cody, Tyler 
I-98 
Corradetti, Daniele 
I-109 
D 
de Carvalho, Gonçalo Hora 
I-119 
De Silva, Ashwin 
I-17 
Diamond, Justin 
I-135 
DiGilio, Nathan 
I-147 
Dvořák, Michal 
II-252 
Dzhivelikian, Evgenii 
II-147 
E 
Elwood, Adam 
I-346 
F 
Faraone, Renato 
II-241 
Ferguson, Thomas M. 
I-159, II-23 
Fermüller, Cornelia 
II-241 
Fuller, Kyle J. 
I-159 
G 
Gay, Simon L. 
I-170 
Georgeon, Olivier L. 
I-170 
Gibson, Amber L. 
I-181 
Glowacki, Gerard 
I-1 
Goertzel, Ben 
I-192, I-203, I-212, I-386, 
I-399 
Gold, Jonathan 
I-346 
Govindarajulu, Naveen Sundar 
I-49 
Gray, Shannon 
I-225 
Guinovart, Enric 
I-291 
H 
Hammer, Patrick 
I-314 
Han, Zeyu 
I-225 
Hatta, Masayuki 
I-239 
Hayashi, Yusuke 
I-250 
Hilbert, Martin 
I-263 
Hohwy, Jakob 
I-346 
Hutter, Marcus 
II-338 
I 
Iacobacci, Nicoletta 
I-278 
Ibias, Alfredo 
I-291 
Inglis, Fionn 
I-346 
J 
Jo, Dae Woong 
I-304 
Johansson, Robert 
I-314 
Jones, Steven J. 
II-299 
K 
Kamiya, Kotaro 
I-1 
Kang, Yipeng 
I-421 
Kaplan, Craig A. 
I-325 
Kirk, James R. 
II-312
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2026 
M. Iklé et al. (Eds.): AGI 2025, LNAI 16057, pp. 447-449, 2026. 
https://doi.org/10.1007/978-3-032-00686-8 

448
Author Index
Krinkin, Kirill 
I-335 
Kuderov, Petr 
II-147 
L 
Laird, John E. 
II-159, II-299, II-312 
Laukkonen, Ruben E. 
I-346 
Lebiere, Christian 
II-159 
Lee, Ray X. 
I-362 
Li, Peilun 
I-421 
Li, Tangrui 
I-375 
Lian, Ruiting 
I-386, I-399 
Liu, Nian 
II-381 
Liu, Tingshan 
II-361 
Lofthouse, Tony 
I-314 
Lopez-Sola, Edmundo 
I-346 
Luo, Siwei 
I-411 
M 
Ma, Chengdong 
II-370 
Mao, Yihuan 
I-421 
Marrani, Alessio 
I-109 
Maruyama, Yoshihiro 
I-1 
McDermott, Brian 
I-49 
Meneses, Amy 
I-72 
Mesnage, Cédric S. 
II-202 
N 
Nelson, Kenric P. 
I-433 
Ng, Hon Keung Tony 
I-433 
Ng, Kei-Sing 
II-1 
Nguyen, Tuan Minh 
II-12 
O 
Oliveira, Igor 
I-433 
Oswald, James T. 
I-159, II-23 
P 
Panov, Aleksandr 
II-147 
Perera, Roly 
II-34 
Pérez, Jonathan 
I-72 
Perrier, Elija 
II-46, II-58, II-71, II-83 
Polovina, Rubina 
II-95 
Pospieszynski, Pawel Filip 
II-109 
Potapov, Alexey 
II-125 
Potapova, Vita 
II-125 
Prentner, Robert 
II-135 
Prokhorenko, Artem 
II-147 
Q 
Qi, Siyuan 
II-381 
Qin, Zihan 
II-361 
R 
Raghavan, Sridhar 
I-225 
Ramirez-Miranda, Guillem 
I-291 
Ramstead, Maxwell 
II-188 
Robertson, Paul 
I-170 
Rodriguez-Galindo, Miguel 
I-291 
Rong, Ziqi 
II-381 
Rosenbloom, Paul S. 
II-159 
S 
Samsonovich, Alexei V. 
II-12 
Sandved-Smith, Lars 
I-346 
Sawyer, Deacon R. 
I-159 
Schaff, Chloe A. 
II-166 
Schneider, Howard 
II-178 
Sennesh, Eli 
II-188 
Shabashkhan, Shagofta 
II-202 
Shuai, Cecelia 
I-17 
Simmons, Gabriel 
II-214 
Singh, Shri Lal Raghudev Ram 
II-220 
Sisman-Ugur, Serap 
II-231 
Sokolov, Dmitry 
I-181 
Stocco, Andrea 
II-159 
Sutor, Peter 
II-241 
T 
Takahashi, Koichi 
I-250 
Tambouratzis, George 
I-225 
Thórisson, Kristinn R. 
I-119, II-166 
Tong, Richard Jiarui 
I-225 
V 
Vadinský, Ondřej 
II-252 
Venugopal, Sanju Mannumadam 
I-225 
Vogelstein, Joshua T. 
I-17, II-361 
Vogelstein, R. Jacob 
II-361 
W 
Wang, Mingzhi 
II-370 
Wang, Pei 
II-350 
Wang, Xiaoyang 
II-202 
Weinbaum, Weaver D. R. 
II-264 
Welsh, Sean 
II-275 
Wolfson, Ouri 
II-285 
Wray, Robert E. 
II-299, II-312

Author Index
449
Wright, George Alexander 
II-326 
Wyeth, Cole 
II-338 
X 
Xu, Beiya 
II-361 
Xu, Bowen 
II-350 
Xu, Boyang 
I-375 
Xu, Wei 
I-421 
Y 
Yang, Yaodong 
II-370, II-381 
Ye, Haoyang 
II-370 
Yu, Siyu 
I-17, II-361 
Z 
Zardini, Gioele 
I-1 
Zhang, Ceyao 
II-381 
Zhang, Chongjie 
I-421 
Zhang, Fode 
I-433 
Zhang, Zhaowei 
II-370, II-381 
Zhu, Song-Chun 
II-381

