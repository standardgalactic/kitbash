<h3 id="bkos">BKOS</h3>
<p>This section discusses two crucial aspects of developing geometric
algorithms: degeneracies and robustness.</p>
<ol type="1">
<li><p>Degeneracies: These are special cases or situations within a
problem that can complicate algorithm design and implementation. They
often arise due to the limitations of numerical precision, such as
floating-point arithmetic causing rounding errors. In computational
geometry, degenerate cases can lead to issues like points lying exactly
on a line segment, multiple points having the same coordinates, or
near-collinear points.</p>
<p>In the convex hull example provided, we initially ignored
degeneracies by assuming no three points are collinear and no two points
have equal x-coordinates. However, this assumption is not realistic in
practical applications where input data might contain such cases. To
handle these situations, we can integrate special cases into our
algorithm design rather than handling them separately with case
distinctions. For instance, in the convex hull algorithm, using
lexicographical order instead of only x-coordinate order accommodates
equal x-coordinates without significantly increasing
complexity.</p></li>
<li><p>Robustness: This refers to an algorithm’s ability to maintain
correct and consistent behavior even when faced with degenerate cases or
numerical imprecision. A robust geometric algorithm should be able to
handle these issues gracefully, avoiding crashes or producing incorrect
results.</p>
<p>In the convex hull example, we initially assumed exact arithmetic
with real numbers for simplicity. However, in practice, floating-point
computations are used due to efficiency concerns. This introduces
rounding errors that can cause problems when determining whether a point
lies on one side of a line segment. To address this issue, we can use
techniques such as symbolic perturbation schemes or carefully implement
the algorithm to detect and handle inconsistencies. These techniques may
involve using exact arithmetic libraries, adapting the algorithm to deal
with potential errors, or incorporating additional checks to ensure
correctness.</p>
<p>In summary, when designing geometric algorithms, it’s essential to be
aware of degeneracies (special cases) that can complicate the problem
and to develop robust solutions capable of handling these situations
gracefully. This often involves integrating special cases into the
general algorithm design rather than treating them separately, as well
as understanding and addressing numerical precision issues in
implementation.</p></li>
</ol>
<p>The problem addressed in this section is finding all intersections
among a set of line segments (closed) in the plane. A brute-force
algorithm would take O(n^2) time by testing every pair of segments for
intersection, which is not efficient when most segments intersect only a
few others.</p>
<p>To overcome this, the authors propose an output-sensitive algorithm
called FINDINTERSECTIONS based on the plane sweep paradigm. The key idea
is to use a horizontally moving “sweep line” (event point) that
processes the segments one at a time while maintaining an ordered list
of intersecting segments.</p>
<ol type="1">
<li><strong>Event Queue and Status Structure</strong>: The algorithm
uses two data structures:
<ul>
<li>Event queue (Q): A balanced binary search tree storing events sorted
by their y-coordinate, with ties broken by x-coordinate. It also stores
the corresponding segment at each event point.</li>
<li>Status structure (T): Another balanced binary search tree that
maintains an ordered list of segments intersecting the sweep line.</li>
</ul></li>
<li><strong>Algorithm Workflow</strong>:
<ul>
<li>Initialize Q and T as empty data structures, then insert all segment
endpoints into Q with their associated segments.</li>
<li>While Q is not empty:
<ul>
<li>Remove the next event (highest priority) from Q.</li>
<li>Call HANDLEEVENTPOINT to process this event, which may involve
adding/removing segments from T or reporting an intersection.</li>
</ul></li>
</ul></li>
<li><strong>Handling Events</strong>:
<ul>
<li>For endpoint events (upper and lower), handle by inserting/deleting
associated segments in T based on the segment’s direction (left/right)
relative to the sweep line.</li>
<li>For intersection events, find intersecting segments above and below
the sweep line, then report intersections and adjust T accordingly.</li>
</ul></li>
<li><strong>Correctness</strong>:
<ul>
<li>The algorithm correctly identifies all intersection points using
Lemma 2.2, proving that it computes intersection points and associated
segments accurately.</li>
<li>Lemma 2.3 establishes its time complexity as O((n + k)logn), where k
is the number of intersections, which can be further refined to O((n +
I)logn), where I is the total number of intersections.</li>
</ul></li>
<li><strong>Space Complexity</strong>:
<ul>
<li>Taking storage into account, Theorem 2.4 shows that all intersection
points with associated segments can be reported in O(nlogn+I logn) time
and O(n) space by only storing intersection points among currently
adjacent segments on the sweep line.</li>
</ul></li>
</ol>
<p>This plane sweep algorithm efficiently solves the line segment
intersection problem while being output-sensitive, meaning its running
time depends on the number of intersections rather than just the input
size. This approach is valuable in applications like thematic map
overlay in geographic information systems where not all pairs of line
segments intersect.</p>
<p>The Art Gallery Problem is concerned with guarding an art gallery or
polygonal region using the minimum number of cameras or vertices. A
polygon can be triangulated, which means it can be divided into
triangles by drawing non-intersecting diagonals between its vertices.
This triangulation allows for efficient coverage of the polygon with
cameras placed at specific vertices.</p>
<p><strong>Key Concepts:</strong></p>
<ol type="1">
<li><strong>Triangulation</strong>: A division of a simple polygon into
triangles using non-intersecting diagonals.</li>
<li><strong>Y-Monotone Polygon</strong>: A polygon that is monotone with
respect to the y-axis, meaning its intersection with any vertical line
perpendicular to the y-axis results in connected pieces (line segments,
points, or empty).</li>
<li><strong>Turn Vertex</strong>: A vertex in a polygon where the
direction of movement changes from downward to upward or vice versa when
traversing the boundary chain.</li>
<li><strong>Dual Graph</strong>: The graph derived from a triangulation,
with vertices representing triangles and edges connecting two triangles
that share a diagonal.</li>
<li><strong>3-Coloring</strong>: A coloring scheme for the vertices of a
triangulated polygon such that no two adjacent vertices (connected by an
edge or diagonal) have the same color, ensuring every triangle has one
vertex of each color (white, gray, and black).</li>
</ol>
<p><strong>Art Gallery Theorem (Theorem 3.2):</strong> For any simple
polygon with n vertices, ⌊n/3⌋ cameras are both necessary and sufficient
to ensure that every point in the polygon is visible from at least one
camera. This result comes from triangulating the polygon and using a
3-coloring scheme for placing the cameras at selected vertices
(preferably the gray ones).</p>
<p><strong>Partitioning into Monotone Pieces (Section 3.2)</strong>: To
improve upon the quadratic time complexity of naive triangulation
algorithms, we first decompose the polygon P into y-monotone pieces. A
turn vertex in P is a point where the direction of movement changes when
walking along the boundary chain. By adding diagonals at these vertices,
we can eliminate them and create y-monotone segments that are easier to
triangulate.</p>
<p><strong>Triangulation Algorithm (Anticipated Result - Theorem
3.3):</strong> Using the monotone partitioning approach, a triangulation
of a simple polygon P with n vertices can be computed in O(n log n)
time. Once we have this triangulation represented as a doubly-connected
edge list, it takes linear time to find ⌊n/3⌋ suitable camera positions
by performing a depth-first search on the dual graph and selecting the
smallest color class (preferably gray vertices). This results in an
efficient algorithm for guarding polygons with a worst-case time
complexity of O(n log n) for triangulation and O(n) for placing cameras,
leading to an overall time complexity of O(n log n) for solving the Art
Gallery Problem.</p>
<p>In this section, we discuss the problem of finding the intersection
of a set of half-planes, which is a more general version of the casting
problem introduced earlier. The goal is to determine all points (x, y)
that satisfy n linear constraints simultaneously, where each constraint
is of the form ax + by ≤ c.</p>
<p>The intersection of half-planes results in a convex polygonal region
with at most n edges and vertices. Figures 4.2(ii) and (iii) demonstrate
that this region can be unbounded or even degenerate into a line
segment, point, or empty set.</p>
<p>A divide-and-conquer algorithm for computing the intersection of
half-planes is presented:</p>
<ol type="1">
<li>If there’s only one half-plane (n = 1), return that single
half-plane as the result.</li>
<li>Otherwise, split the set H into two smaller sets, H1 and H2, each
containing approximately n/2 constraints.</li>
<li>Recursively compute the intersection of H1 and H2 using
INTERSECTHALFPLANES(H1) and INTERSECTHALFPLANES(H2).</li>
<li>Use a subroutine called INTERSECTCONVEXREGIONS to combine the
results from steps 3a and 3b, yielding the final convex polygonal region
C.</li>
</ol>
<p>The subroutine INTERSECTCONVEXREGIONS computes the intersection of
two convex polygons. In this case, it can be adapted to handle unbounded
or degenerate cases (segments/points) by following a modified approach
from Chapter 2.</p>
<p>To analyze the algorithm’s running time, we use a recurrence
relation: T(n) = O(1) for n = 1 and T(n) = O(n log n) + 2T(n/2) when n
&gt; 1. Solving this recurrence gives T(n) = O(n log² n).</p>
<p>The new, more efficient plane sweep algorithm maintains the left and
right boundary edges of C1 and C2 separately as sorted lists of
half-planes. The pointers left edge C1, right edge C1, left edge C2, and
right edge C2 track which edges intersect the sweep line at any given
moment.</p>
<p>The sweep line’s y-coordinate is initialized to ystart = min(y1, y2),
where y1 and y2 are the y-coordinates of the topmost vertices in C1 and
C2, respectively. The pointers are updated according to which edges
intersect the current y value of the sweep line.</p>
<p>No explicit event queue is required because the next edge can be
determined in constant time using the pointers. When a new edge e
appears on the boundary, its associated procedure is called based on
whether it belongs to C1 or C2 and if it’s on the left or right
boundary. The algorithm focuses on handling edges appearing on the left
boundary of C1 as an example; similar procedures handle other cases.</p>
<p>The text discusses an algorithm for solving 2-dimensional linear
programming problems, which is an extension of the method used to
compute the intersection of convex polygons. This algorithm, called
Randomized Incremental Linear Programming (RANDOMIZEDLP), aims to find a
solution that maximizes a given linear function under a set of linear
constraints in two variables.</p>
<p>The key idea behind RANDOMIZEDLP is incremental: it adds the
half-planes defining the linear constraints one by one and maintains the
optimal solution at each step. The algorithm requires the solution to
each intermediate problem (feasible region) to be well-defined and
unique, which is ensured by adding two artificial bounding constraints
m1 and m2. These constraints ensure that the feasible region remains
bounded, preventing unbounded linear programs.</p>
<p>The core of RANDOMIZEDLP lies in Lemma 4.5, which describes how the
optimal vertex (the point maximizing the linear function within the
feasible region) changes when a new half-plane is added. In simple
terms:</p>
<ol type="1">
<li>If the previous optimal vertex is contained in the new half-plane,
then it remains the same.</li>
<li>If not, then either the current feasible region is empty (infeasible
linear program), or the new optimal vertex lies on the boundary line of
the half-plane being added.</li>
</ol>
<p>To find the new optimal vertex when it doesn’t lie on the previous
one, RANDOMIZEDLP solves a 1-dimensional linear program along the line
that bounds the half-plane (Lemma 4.6). This 1D LP can be solved in
linear time using standard techniques from operations research.</p>
<p>The main advantage of RANDOMIZEDLP is its randomized nature: it
computes a random permutation of the input half-planes and processes
them one by one, leading to an expected running time of O(n) for a
problem with n constraints. The “expected” here refers to the average
case over all possible permutations of the input, which guarantees a
good performance regardless of the specific order of input
half-planes.</p>
<p>It’s worth noting that while this approach is simple and elegant, its
practical efficiency might be limited by the worst-case scenario, where
the chosen random permutation results in a quadratic running time
(O(n^2)). However, extensive testing has shown that in most cases, the
algorithm performs much faster than this upper bound.</p>
<p>The text also briefly touches on extensions of this method to higher
dimensions and other optimization problems like finding the smallest
enclosing disc for a set of points, demonstrating the versatility of
this randomized incremental approach.</p>
<p>The text discusses two data structures for efficiently answering
rectangular range queries on a set of points in a plane, namely kd-trees
and range trees.</p>
<ol type="1">
<li><strong>kd-trees</strong>:
<ul>
<li>A kd-tree is a binary tree where each node stores a splitting line
(either horizontal or vertical) that divides the space into two
subregions. The left child contains points with smaller x-coordinates
(or y-coordinates, depending on the depth), and the right child contains
points with larger x-coordinates (or y-coordinates).</li>
<li>Construction:
<ul>
<li>Points are sorted on both x and y coordinates.</li>
<li>Starting from the root, a point is chosen as the splitting value
(median of the current set), and the points are partitioned accordingly.
This process continues recursively until every node has a single point
or no points at all.</li>
</ul></li>
<li>Query time: O(√n + k), where n is the number of points and k is the
number of reported points. The query algorithm traverses the tree,
visiting only nodes whose regions intersect the query rectangle, and
reports points stored in leaf nodes that lie within the range.</li>
</ul></li>
<li><strong>Range Trees</strong>:
<ul>
<li>Range trees improve upon kd-trees by providing better query times at
the cost of increased storage. They are designed to handle 1D range
queries more efficiently by organizing points along a line (either x or
y) using an auxiliary data structure called a segment tree or a binary
search tree.</li>
<li>Construction:
<ul>
<li>For each dimension, build a balanced binary search tree (BBST)
storing the sorted points. These trees are interconnected to create a
multi-dimensional structure.</li>
<li>The root of the range tree corresponds to the entire space
containing all n points. Internal nodes represent subspaces divided by
splitting lines, and leaf nodes contain individual points.</li>
</ul></li>
<li>Query time: O(log²n + k), where n is the number of points and k is
the number of reported points. Range trees achieve this query time by
traversing multiple BBSTs simultaneously to identify subspaces that
intersect the query rectangle.</li>
</ul></li>
</ol>
<p>In summary, both kd-trees and range trees are effective data
structures for answering rectangular range queries on sets of points in
a plane. Kd-trees offer better query times when dealing with small
numbers of reported points (O(√n + k)), while range trees provide
improved performance for larger values of k at the cost of increased
storage requirements (O(n log n)). The choice between these data
structures depends on the specific application and desired trade-offs
between time complexity and space usage.</p>
<p>The text describes a solution to the planar point location problem
using a data structure called a trapezoidal map (or vertical
decomposition). Here’s a summary of the key points:</p>
<ol type="1">
<li><p><strong>Planar Point Location Problem</strong>: Given a planar
subdivision S with n edges, find the face containing a query point q. If
q lies on an edge or vertex, return this information.</p></li>
<li><p><strong>Simple Data Structure</strong>: A naive solution involves
drawing vertical lines through all vertices of S and storing their
x-coordinates in sorted order. This creates slabs, and within each slab,
edges are ordered from top to bottom. The face containing q can be found
by binary searching the appropriate array for the slab containing q and
checking the label associated with the segment just below q.</p></li>
<li><p><strong>Quadratic Storage</strong>: While this simple structure
has O(log n) query time, its storage requirements are quadratic (O(n^2))
due to storing an array for each slab, resulting from a reﬁnement of S
that increases complexity.</p></li>
<li><p><strong>Trapezoidal Map</strong>: A better reﬁnement is the
trapezoidal map T(S), which is created by extending every endpoint of
segments in S vertically upwards and downwards until they meet another
segment or the boundary of a bounding rectangle R. This results in a
subdivision with trapezoids, triangles, and possibly unbounded
faces.</p></li>
<li><p><strong>Trapezoidal Map Properties</strong>: Each face in T(S)
has one or two vertical sides and exactly two non-vertical sides (Lemma
6.1). Non-vertical sides are contained within segments of S or
horizontal edges of R, denoted as top(∆) and bottom(∆).</p></li>
</ol>
<p>The trapezoidal map offers a more efficient solution to the point
location problem by creating a reﬁnement that maintains manageable
complexity while still allowing for fast queries. The next sections will
delve into the construction of this map and query algorithms using
it.</p>
<p>The text discusses a randomized incremental algorithm for
constructing a trapezoidal map (T(S)) and a corresponding search
structure D, which allows for efficient point location queries within
the trapezoidal map. Here’s a detailed summary and explanation of the
key points:</p>
<ol type="1">
<li><p><strong>Trapezoidal Map</strong>: The trapezoidal map T(S) is
constructed from a set S of n non-crossing line segments in general
position. It consists of trapezoids formed by these segments, with each
trapezoid uniquely defined by its top, bottom, left, and right vertical
edges.</p></li>
<li><p><strong>Search Structure (D)</strong>: This is a directed acyclic
graph with a single root and one leaf for every trapezoid in T(S). Inner
nodes have out-degree 2 and can be either x-nodes (labeled by an
endpoint of a segment in S) or y-nodes (labeled by the segment itself).
The search structure is designed to guide point location queries by
testing whether a query point lies left, right, above, or below certain
geometric objects.</p></li>
<li><p><strong>Algorithm TRAPEZOIDALMAP</strong>: This randomized
incremental algorithm constructs T(S) and D simultaneously as
follows:</p>
<ul>
<li><strong>Initialization</strong>: Determine the bounding box R for S,
initialize T(S), and D accordingly.</li>
<li><strong>Random Permutation</strong>: Generate a random permutation
s1, s2, …, sn of the elements in S.</li>
<li><strong>Iteration</strong>: For each segment si (i = 1 to n):
<ul>
<li>Find all trapezoids ∆0, ∆1, …, ∆k intersected by si using
FOLLOWSEGMENT algorithm.</li>
<li>Remove these trapezoids from T(S) and add new trapezoids resulting
from the insertion of si.</li>
<li>Update D by removing leaves for ∆0, ∆1, …, ∆k and adding new leaves
for the new trapezoids, connecting them with additional inner nodes as
necessary.</li>
</ul></li>
</ul></li>
<li><p><strong>FOLLOWSEGMENT Algorithm</strong>: This auxiliary
algorithm finds the sequence of intersected trapezoids (∆0, ∆1, …, ∆k)
when a segment si is inserted:</p>
<ul>
<li>Initialize left endpoint p and right endpoint q of si.</li>
<li>Perform a query in D starting from p to find ∆0.</li>
<li>While q lies to the right of rightp(∆j):
<ul>
<li>Determine if ∆j+1 is lower or upper right neighbor based on si’s
position relative to ∆j.</li>
</ul></li>
<li>Return the sequence (∆0, ∆1, …, ∆j).</li>
</ul></li>
<li><p><strong>Handling Trapezoid Updates</strong>: When updating T(S)
and D due to inserting si:</p>
<ul>
<li>If si is completely contained within a trapezoid ∆, four new
trapezoids are formed by partitioning ∆ along si’s endpoints.</li>
<li>If si intersects multiple trapezoids, vertical extensions through
si’s endpoints partition the intersected trapezoids into three new ones
each. Shorten these vertical extensions to merge trapezoids along
si.</li>
</ul></li>
<li><p><strong>Query Structure and Time Complexity</strong>: The search
structure D facilitates point location queries within T(S). Its expected
size is O(n), and query time is O(log n) on average, according to
Theorem 6.3. This result holds even when relaxing the assumptions of
general position and avoidance of vertical lines/segment intersections
through symbolic transformations (Theorem 6.5).</p></li>
<li><p><strong>Tail Estimate (Lemma 6.6 &amp; 6.7)</strong>: These
lemmas provide probabilistic bounds on the query path length in D,
indicating that the maximum search path length is O(log n) with high
probability. This allows constructing a data structure with O(n) storage
and worst-case O(log n) query time (Theorem 6.8).</p></li>
</ol>
<p>In summary, this text presents an efficient algorithm for
constructing a trapezoidal map and associated search structure for point
location in planar subdivisions of line segments, even when relaxing
assumptions about general position or avoidance of vertical
lines/segment intersections. The resulting data structure provides good
average-case performance with high probability, enabling optimal
worst-case query times through additional preprocessing steps if
needed.</p>
<p>The text discusses Voronoi diagrams, specifically focusing on their
computation for sets of point sites and line segments. Here’s a detailed
summary:</p>
<ol type="1">
<li><strong>Voronoi Diagrams for Point Sites:</strong>
<ul>
<li>Definition: A Voronoi diagram subdivides the plane into n regions,
each associated with one site from a set P = {p1, p2, …, pn}. For any
point q in region V(pi), dist(q, pi) &lt; dist(q, pj) for all j ≠
i.</li>
<li>Structure: Each Voronoi cell (region) is an open convex polygon
defined by n-1 half-planes h(pi, p j). The entire diagram consists of
edges (line segments and half-lines), forming a connected planar
subdivision with at most 2n-5 vertices and 3n-6 edges.</li>
<li>Computation: A simple method is to compute each cell separately
using the intersection of n-1 half-planes, resulting in an O(n^2 log n)
algorithm. Fortune’s sweep line algorithm provides a more efficient
solution with O(n log n) time complexity and O(n) storage.</li>
</ul></li>
<li><strong>Voronoi Diagrams for Line Segments:</strong>
<ul>
<li>Definition: Similar to point sites, but the distance is measured
from a point to the closest point on a segment. Bisectors can be curves
(parabolic arcs) if the closest points are endpoints or interiors of
segments.</li>
<li>Structure: A subdivision with straight edges and parabolic arcs,
having O(n) vertices, edges, and faces.</li>
<li>Computation: The sweep line algorithm for point sites can be adapted
to handle segments, resulting in an O(n log n) time complexity using
O(n) storage.</li>
</ul></li>
<li><strong>Applications:</strong>
<ul>
<li>Post Office Problem (Social Geography): Voronoi diagrams model
trading areas of facilities like supermarkets or post offices based on
customer proximity and transportation costs.</li>
<li>Motion Planning: Voronoi diagrams for line segments can guide
collision-free paths for robots, with the arcs between segments offering
the most clearance from obstacles.</li>
</ul></li>
<li><strong>Farthest-Point Voronoi Diagrams:</strong>
<ul>
<li>Definition: A variant where each site represents a circle of radius
r. The goal is to find two circles (Couter and Cinner) that form the
smallest annulus enclosing points in P. There are three cases based on
point distribution on Couter and Cinner, with four points total on these
circles.</li>
<li>Application: Used for evaluating roundness of objects by coordinate
measurement machines, determining the smallest-width annulus containing
a set of nearly circular points.</li>
</ul></li>
</ol>
<p>The text concludes with the observation that Voronoi diagrams’
computation can be time-consuming but offers efficient solutions for
various spatial problems in fields like geography and robotics.</p>
<p>The text discusses various concepts related to computational
geometry, focusing on Voronoi diagrams, duality, and arrangements of
lines. Here’s a summary and explanation of these topics:</p>
<ol type="1">
<li><strong>Voronoi Diagrams</strong>:
<ul>
<li>A Voronoi diagram is a partitioning of a plane into regions based on
distance to points in a specific subset of the plane (called sites or
generators). Each region contains all points closer to its corresponding
site than to any other.</li>
<li>Properties and algorithms for Voronoi diagrams are explored,
including the relationship between Voronoi cells and convex hull
vertices.</li>
</ul></li>
<li><strong>Farthest-Point Voronoi Diagram</strong>:
<ul>
<li>This is a variation of the standard Voronoi diagram, where each cell
corresponds to the set of points farthest from a given site (farthest
point).</li>
<li>Properties such as the tree-like structure of cells and connections
to the smallest enclosing annulus are discussed.</li>
</ul></li>
<li><strong>Duality</strong>:
<ul>
<li>Duality is a transformation between geometric objects in a plane,
mapping points to lines and vice versa while preserving certain
properties like incidence and order.</li>
<li>The duality transform is used to simplify problems by changing the
perspective, often making it easier to solve complex issues related to
point sets.</li>
</ul></li>
<li><strong>Arrangements of Lines</strong>:
<ul>
<li>An arrangement of lines is a subdivision of the plane created by
intersecting a set of lines, consisting of vertices, edges, and faces
(some of which may be unbounded).</li>
<li>The complexity of such an arrangement depends on the number and
configuration of the input lines. Simple arrangements (no three lines
meet at a point and no two are parallel) have maximum possible
complexities.</li>
</ul></li>
<li><strong>Constructing Arrangements</strong>:
<ul>
<li>An incremental algorithm for constructing doubly-connected edge
lists representing planar subdivisions, including line arrangements, is
presented. This algorithm has a time complexity of O(n^2).</li>
</ul></li>
<li><strong>Levels and Discrepancy</strong>:
<ul>
<li>In the context of Voronoi diagrams or line arrangements, “levels”
refer to the number of lines lying strictly above a vertex (or point) in
the dual structure.</li>
<li>The discrepancy problem involves computing how many lines pass
through, lie above, or are below vertices in an arrangement, which is
crucial for evaluating sampling techniques like supersampling in ray
tracing to reduce visual artifacts.</li>
</ul></li>
</ol>
<p>These concepts form the basis for understanding and solving problems
related to spatial partitioning, duality transformations, and efficient
geometric data structures, with applications ranging from computer
graphics (ray tracing) to computational geometry algorithms.</p>
<p>The text discusses an algorithm for computing a Delaunay
triangulation (DT) of a given set P of points in the plane, using a
randomized incremental approach. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Algorithm Initialization</strong>: The algorithm starts
by selecting p0 as the lexicographically highest point in P, and
introducing two additional points p−1 and p−2 that are sufficiently far
away from P so as not to affect the Delaunay triangulation of
P.</p></li>
<li><p><strong>Triangulation Construction</strong>: The main algorithm,
DELAUNAYTRIANGULATION(P), initializes a triangulation T consisting of
the triangle p0p−1p−2 and then proceeds iteratively through each point
pr (r from 1 to n):</p>
<ul>
<li>It finds the triangle in T that contains pr.</li>
<li>If pr lies inside this triangle, it splits the triangle into three
triangles by adding edges pr-pi, pr-pj, and pr-pk (and possibly pr-pl if
necessary).</li>
<li>The procedure LEGALIZEEDGE is then called to ensure the
triangulation remains legal (i.e., all edges are non-illegal) after the
insertion of pr.</li>
</ul></li>
<li><p><strong>Legalize Edge Procedure</strong>: The subroutine
LEGALIZEEDGE flips illegal edges until they become legal:</p>
<ul>
<li>It identifies potentially illegal edges by comparing circles formed
by three points in P (including potentially p−1 and p−2).</li>
<li>If an edge is found to be illegal, it is replaced with a new set of
edges that form a legal configuration (typically through an edge flip
operation).</li>
</ul></li>
<li><p><strong>Handling p−1 and p−2</strong>: The algorithm symbolically
treats points p−1 and p−2 as infinitely far away in both the point
location and illegal edge tests, ensuring they don’t influence the
Delaunay triangulation of P. They are connected to all points on the
convex hull of P (right for p−1 and left for p−2).</p></li>
<li><p><strong>Point Location Data Structure</strong>: To efficiently
locate pr in T during its insertion, a point location data structure D
is maintained alongside T:</p>
<ul>
<li>D represents a directed acyclic graph where leaves correspond to
triangles in T.</li>
<li>As triangles are split or merged due to point insertions, changes
are reflected in D without altering the triangulation’s
correctness.</li>
</ul></li>
<li><p><strong>Correctness and Analysis</strong>: The algorithm is
proven correct by showing no illegal edges remain after processing calls
to LEGALIZEEDGE. Its expected running time complexity is O(n log n) when
using a suitable point location data structure like a trapezoidal
decomposition or a quadtree, which allows for efficient triangle search
and edge flips in the triangulation.</p></li>
</ol>
<p>This algorithm offers an effective method for computing Delaunay
triangulations, crucial in various applications such as terrain
modeling, computer graphics, and mesh generation. By employing a
randomized incremental approach combined with legalization techniques,
it efficiently maintains a valid Delaunay configuration while inserting
points one by one.</p>
<p>This text discusses interval trees, a data structure used for
efficiently handling windowing queries on axis-parallel line segments.
Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Problem Context</strong>: Windowing queries involve
determining the set of objects (in this case, line segments) that lie
within a specified rectangular region or “window” in a 2D space. This
problem is relevant for various applications like vehicle navigation
systems, flight simulation, printed circuit board design, and
more.</p></li>
<li><p><strong>Data Structure - Interval Trees</strong>: To address the
windowing query problem efficiently, we use an interval tree data
structure tailored to axis-parallel line segments.</p></li>
<li><p><strong>Segment Intersections with Window</strong>: A segment can
intersect a window in four ways:</p>
<ul>
<li>Completely inside the window</li>
<li>Touching one boundary once</li>
<li>Crossing two boundaries (touching both horizontal and vertical
sides)</li>
<li>Overlapping one or both boundaries</li>
</ul></li>
<li><p><strong>Range Query on Endpoints</strong>: Most segments will
have at least one endpoint within the query window. To identify these
intersecting segments, we can perform a range query on the 2n endpoints
of the n line segments using a range tree (a 2D generalization of binary
search trees). Range trees allow us to find all endpoints falling within
the window in O(log2 n + k) time, where k is the number of reported
points.</p></li>
<li><p><strong>Segment Selection</strong>: After identifying endpoint
pairs corresponding to intersecting segments using a range tree, we
extract and return those line segments. This approach ensures that we
efficiently locate all relevant segments for a given query
window.</p></li>
<li><p><strong>Storage Efficiency</strong>: Interval trees store n
axis-parallel line segments using O(n log n) storage. While the data
structure itself is not explicitly defined in this text snippet, it is
implied to be an adaptation of range trees optimized for handling
axis-parallel segments and windowing queries efficiently.</p></li>
</ol>
<p>In essence, interval trees for windowing queries on axis-parallel
line segments combine the principles of range trees with
segment-specific optimizations to achieve efficient querying within
specified rectangular windows. This data structure plays a crucial role
in applications requiring fast access to objects confined within
particular regions of 2D space.</p>
<p>The provided text discusses several data structures used for
geometric queries, specifically focusing on finding segments
intersecting a query window or point. Here’s a summary of the main
points:</p>
<ol type="1">
<li><p><strong>Interval Trees</strong>: An interval tree is a binary
search tree that stores intervals (or in this case, line segment
endpoints) and allows efficient range reporting queries. It uses O(n)
storage and can perform queries in O(log n + k) time, where k is the
number of reported intervals.</p></li>
<li><p><strong>Priority Search Trees</strong>: A priority search tree is
a binary search tree that stores points based on their x-coordinates
while maintaining a partitioning by y-coordinate. This structure allows
for efficient range reporting queries, particularly when the range is
unbounded in one direction. It uses O(n) storage and can perform queries
in O(log n + k) time.</p></li>
<li><p><strong>Segment Trees</strong>: A segment tree is an extension of
interval trees, designed to handle segments with arbitrary orientations
(not just horizontal). It stores intervals at internal nodes based on
their “span” over elementary intervals defined by the endpoints of input
segments. Segment trees use O(n log n) storage and can perform queries
in O(log n + k) time.</p></li>
<li><p><strong>Application to Windowing Problem</strong>: These
structures can be applied to solve the windowing problem for
axis-parallel line segments, as well as for arbitrary-oriented line
segments by representing them with their bounding boxes. The solution
involves combining a range tree (for endpoints within the window) with
an intersection query on boundary edges and using segment trees or
interval trees with priority search trees for segments crossing the
window’s edge.</p></li>
<li><p><strong>Dynamic Data Structures</strong>: There has been
significant research on making these static structures dynamic, allowing
insertion and deletion of elements while maintaining efficiency.
Decomposable searching problems provide a framework for transforming
static data structures into dynamic ones.</p></li>
<li><p><strong>Higher Dimensions</strong>: These concepts can be
extended to higher dimensions, with multi-level segment trees used for
stabbing queries in d-dimensional space, achieving storage complexity
O(n log^(d-1) n) and query time O(log^d n).</p></li>
<li><p><strong>R-trees</strong>: In geographic information systems,
R-trees are widely used for storing spatial objects (like points, line
segments, polygons) and answering intersection queries. They are
designed for disk storage and, despite worst-case linear query time,
perform well in practice. Variations like the PR-tree can achieve
near-optimal query performance when dealing with axis-parallel
hyperrectangles as both data and query objects.</p></li>
</ol>
<p>The exercises at the end of the section explore alternative
implementations and efficiency analyses for these data structures in
various scenarios.</p>
<p>The Painter’s Algorithm is a technique used in computer graphics for
rendering 3D scenes, particularly in solving the hidden surface removal
problem. The goal of this algorithm is to determine which parts of
objects are visible at each pixel on the screen, effectively managing
occlusions caused by overlapping or intersecting objects.</p>
<p>Here’s a detailed explanation of how the Painter’s Algorithm
works:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Begin with an empty frame buffer
(the final image that will be displayed) and a sorted list of polygons
(3D shapes like triangles). This sorting can be done based on the
polygon’s depth (distance from the viewer), typically using a
front-to-back or back-to-front method.</p></li>
<li><p><strong>Painter’s Loop</strong>: Iterate over each polygon in the
sorted list:</p>
<ul>
<li><p>For each pixel covered by the current polygon, check if it’s
already been painted with a different color. If not, proceed to step 3;
otherwise, skip this polygon as it is occluded by another closer
polygon.</p></li>
<li><p>Paint the pixel with the color of the current polygon. This
involves calculating which pixels fall within the projected silhouette
(2D representation) of the 3D polygon on the screen.</p></li>
</ul></li>
<li><p><strong>Update Frame Buffer</strong>: After painting all visible
pixels for the current polygon, update the frame buffer to reflect these
changes. This might involve blending the new colors with existing ones
using techniques like alpha blending or depth buffering to manage
transparency and occlusions correctly.</p></li>
<li><p><strong>Repeat until Done</strong>: Continue this process for
each polygon in the sorted list. The final image stored in the frame
buffer will be the rendered scene, where visible parts of objects
replace the hidden ones.</p></li>
</ol>
<p>The Painter’s Algorithm is effective because it only considers
polygons that are closer to the viewer (i.e., have smaller depth values)
when rendering. This ensures that closer objects block the view of those
further away, simulating real-world occlusions.</p>
<p>However, this algorithm can be inefficient for complex scenes with
many overlapping polygons since it may need to process each polygon
multiple times if they are not correctly sorted or if there are
transparency effects involved. To address these limitations, more
sophisticated techniques like depth buffering, z-buffering, or scanline
algorithms are often employed. These methods maintain a depth map in
memory and use it to quickly determine which pixels need updating during
the rendering process.</p>
<p>The text discusses Binary Space Partition (BSP) trees, a data
structure used for efficient hidden surface removal in computer
graphics. BSP trees are binary trees that partition space using
hyperplanes, dividing it into regions, with each node representing a
hyperplane and its children representing the two half-spaces created by
the hyperplane. The leaves of the tree store object fragments or entire
objects within their corresponding regions.</p>
<p>Two algorithms for constructing BSP trees are presented:</p>
<ol type="1">
<li><p><strong>2DBSP</strong>: This algorithm constructs a BSP tree for
a set of line segments in the plane using randomized splitting lines. It
generates a binary space partition by recursively dividing the plane
with hyperplanes, always choosing the next line segment’s containing
line as the splitter. Free splits are made when possible (i.e., when a
line can divide multiple segments without further fragmentation). The
algorithm’s expected number of fragments is O(n log n), and it has an
expected construction time of O(n^2 log n).</p></li>
<li><p><strong>3DBSP</strong>: This algorithm extends the 2D version to
triangles in 3D space, also employing randomized splitting planes
(planes containing a triangle) while making free splits where possible.
The analysis shows that the expected number of object fragments
generated is O(n^2), and there exist configurations for which any BSP
tree must have size Ω(n^2).</p></li>
</ol>
<p>The text also introduces the concept of density to classify scenes
based on how close objects are to each other relative to their sizes,
affecting the complexity of constructing a BSP. It then presents an
algorithm called <code>LOWDENSITYBSP2D</code> tailored for low-density
scenes (where objects are relatively well-separated), which leverages
bounding boxes and guards (vertices of these boxes) to create an
efficient BSP tree with size O(n log λ), where λ is the density of the
input set.</p>
<p>The <code>LOWDENSITYBSP2D</code> algorithm works as follows:</p>
<ol type="1">
<li>Compute the multiset G(S) of 4n bounding-box vertices for a given
set S of n objects in the plane.</li>
<li>Initialize k = 1, a flag ‘done’ to false, and an encompassing square
U for S.</li>
<li>In a loop:
<ul>
<li>Double k, run <code>PHASE1</code> with U, G(S), and the new value of
k. If all leaf regions intersect at most 5k objects (checked by
computing the number of object fragments in each leaf region), set
‘done’ to true; otherwise, continue the loop.</li>
</ul></li>
<li>For each leaf µ of the resulting BSP tree T:
<ul>
<li>Compute the set S(µ) of object fragments within the region
corresponding to µ.</li>
<li>If |S(µ)| &gt; 5k, reset ‘done’ to false and continue the loop.</li>
</ul></li>
<li>Replace each leaf µ in T by a BSP tree Tµ computed using
<code>2DRANDOMBSP</code> on S(µ).</li>
<li>Return T.</li>
</ol>
<p>This algorithm ensures that, for sets of disjoint line segments in
the plane, the BSP’s size is O(n log λ), where λ is the density of the
input set. This approach improves upon the worst-case quadratic bound
for constructing BSP trees when dealing with low-density scenes,
providing a more efficient solution tailored to such cases.</p>
<p>The text discusses the application of Minkowski sums in robot motion
planning, specifically for a translating planar robot moving among
non-intersecting polygonal obstacles. The main results are:</p>
<ol type="1">
<li><p>Minkowski Sums Property: The C-obstacle (configuration space
obstacle) of an obstacle P and a robot R is the Minkowski sum of P and
-R(0,0), where -R(0,0) denotes the reflection of R about its reference
point. This is proven using the definition of Minkowski sums.</p></li>
<li><p>Complexity of Minkowski Sums: The complexity of a Minkowski sum
between two convex polygons P and R with n and m vertices respectively
is O(n+m). For non-convex polygons, if one polygon is convex and the
other is not, the complexity is O(nm), while for both being non-convex,
it’s O(n<sup>2m</sup>2). These bounds are tight in the worst
case.</p></li>
<li><p>Free Configuration Space Complexity: For a convex robot R
translating among a set of n polygonal obstacles with total edges n, the
complexity of the free configuration space Cfree(R,S) is O(n). This is
proven by triangulating each obstacle and showing that the C-obstacles
form a set of pseudodiscs due to their convexity, which allows the use
of Theorem 13.9 to conclude linear complexity for their union (forbidden
space).</p></li>
<li><p>Algorithm to Compute Forbidden Space: An algorithm FORBIDDENSPACE
is provided to compute the forbidden space Cforb(R,S) using a
divide-and-conquer approach on Minkowski sums of triangulated obstacles.
The time complexity of this algorithm is O(nlog2 n), derived from the
triangulation step (O(mlogm) for each obstacle with m vertices) and the
union computation (using an overlay algorithm, which can be done in
logarithmic time given doubly-connected edge lists).</p></li>
</ol>
<p>The overall strategy involves reducing the motion planning problem to
computing Minkowski sums of obstacles and the robot, leveraging
properties of these sums to ensure the resulting configuration spaces
are manageable. The triangulation step and recursive union computation
allow for efficient calculation of the forbidden space (and
consequently, the free space), with a time complexity that scales well
with the number of obstacle edges.</p>
<p>The text describes a method for generating non-uniform triangular
meshes for a 2D square domain with disjoint polygonal components, using
quadtrees as a key component. Here’s a detailed summary and explanation
of the approach:</p>
<ol type="1">
<li><p><strong>Quadtree Construction</strong>: The algorithm begins by
constructing a quadtree subdivision within the given square, considering
all input components. It splits squares until they reach unit size or
cease to intersect any component edge. This ensures that the mesh will
be non-uniform, with smaller triangles near component edges and larger
ones further away.</p></li>
<li><p><strong>Balancing Quadtree</strong>: To ensure well-shaped
triangles (specifically 45°-90°-45°) and conforming mesh properties, the
quadtree subdivision is made balanced using Algorithm BALANCEQUADTREE.
This algorithm ensures that any two neighboring squares differ at most
by a factor of 2 in size, which helps avoid issues like many small
triangles surrounded by large ones or irregularly shaped
triangles.</p></li>
<li><p><strong>Mesh Generation</strong>: The subdivision is then
triangulated to produce the final mesh:</p>
<ul>
<li>Squares with no internal vertices (and not already intersected by a
component edge) are divided by adding diagonals, resulting in
well-shaped 45°-90°-45° triangles.</li>
<li>For squares with internal vertices on their sides, Steiner points
are added at the center, connected to all boundary vertices, producing
45°-90°-45° triangles as well.</li>
</ul></li>
<li><p><strong>Properties of the Mesh</strong>: The resulting mesh has
several desirable properties:</p>
<ul>
<li><strong>Non-uniformity</strong>: Smaller triangles near component
edges and larger ones further away, following the non-uniform
requirement.</li>
<li><strong>Conforming</strong>: Triangles respect input components’
boundaries, ensuring no internal vertices from one triangle to another
on shared edges.</li>
<li><strong>Well-shaped</strong>: All triangles are 45°-90°-45°, meeting
the angle criteria.</li>
</ul></li>
<li><p><strong>Complexity and Time</strong>: The number of resulting
mesh triangles is O(p(S)logU), where p(S) is the sum of perimeters of
components in S and U is a scaling factor (2^j). Preprocessing time is
O(p(S)log²U).</p></li>
</ol>
<p>The approach combines quadtrees’ hierarchical subdivision with
balancing techniques to efficiently generate well-shaped, non-uniform
triangular meshes that respect input components. This method adapts and
builds upon earlier work on structured mesh generation, optimizing for
both shape and size requirements in the presence of disjoint polygonal
obstacles within a square domain.</p>
<p>The text discusses the concept of partition trees for solving
2-dimensional range queries, specifically half-plane range counting
problems, which involves determining the number of points in a set lying
within a specified half-plane.</p>
<ol type="1">
<li><p><strong>Partition Trees</strong>: A partition tree is a
hierarchical data structure used to store and efficiently query
information about a set of points in the plane. The tree consists of
nodes, where each node corresponds to a triangle (or segment) that
encloses a subset of the point set.</p></li>
<li><p><strong>Simplicial Partition</strong>: This is a collection of
triangles (and possibly segments), where each triangle contains a subset
of the points and their union covers all points in the set. The crossing
number, which measures how many times a line intersects these triangles,
is crucial for the efficiency of the partition tree.</p></li>
<li><p><strong>Query Algorithm</strong>: To answer a half-plane range
query using a partition tree, we start at the root and traverse down the
tree recursively. We only visit children whose enclosing triangles
intersect the query half-plane. The points in the queried region are
represented as the disjoint union of the canonical subsets (the subset
of points within each triangle) of these selected nodes.</p></li>
<li><p><strong>Storage Analysis</strong>: The partition tree uses O(n)
storage, which is optimal since we need to store information about each
point. Each node stores a triangle and possibly some metadata about its
enclosed subset of points.</p></li>
<li><p><strong>Query Time Analysis</strong>: The query time depends on
the crossing number of the simplicial partition used in the tree. By
choosing a suitable simplicial partition (one with low crossing number),
we can achieve a query time of O(n^(1/2 + ε)), where ε is a small
positive constant. This is suboptimal compared to logarithmic or
polynomial-time solutions for simpler 1D problems, but it’s the best
possible for the more complex 2D half-plane range counting problem using
a partition tree structure.</p></li>
<li><p><strong>Comparison with Other Structures</strong>: The approach
taken here (partition trees) can be compared with other structures like
range trees and segment trees. These structures aim to precompute
information about all possible subsets that could appear in queries,
leading to fast query times. However, the number of such subsets often
makes this approach impractical due to storage limitations. Instead,
partition trees focus on storing information for canonical subsets (a
carefully chosen subset of potential query results) and expressing each
query as a union of these precomputed subsets. This trade-off between
query time and storage is a common theme in geometric data
structures.</p></li>
</ol>
<p>In summary, the text introduces partition trees as an effective
method to handle 2D range queries, particularly half-plane range
counting problems. These trees leverage simplicial partitions to
efficiently organize points, enabling fast querying at the cost of
suboptimal query times but optimal storage usage. The approach
highlights the trade-offs inherent in designing geometric data
structures, where faster queries typically require more storage or other
compromises.</p>
<p>The text discusses data structures for range searching problems in
computational geometry, specifically focusing on triangular (simplex)
range searching in the plane. Two primary approaches are presented:
partition trees and cutting trees.</p>
<ol type="1">
<li>Partition Trees:
<ul>
<li>A partition tree divides the space into a collection of simpler
regions called canonical subsets.</li>
<li>For half-plane range searching, the number of canonical subsets is
O(n), requiring linear storage. However, to achieve logarithmic query
time, one needs approximately quadratic (Ω(n^2)) canonical subsets.</li>
<li>The partition tree stores these canonical subsets and uses them for
efficient querying. The query time depends on the number of triangles in
the partition crossed by the boundary of the query region. For
triangular queries, this crossing number is at most 3c√r, leading to a
query time of O(n^(1/2 + ε)).</li>
<li>Construction time for a partition tree is O(n^(1+ε)), and it can
report points in O(k) additional time, where k is the number of reported
points.</li>
</ul></li>
<li>Cutting Trees:
<ul>
<li>A cutting tree partitions the space using disjoint triangles (called
a (1/r)-cutting), unlike partition trees that allow overlapping
regions.</li>
<li>For any set L of n lines in the plane and parameter r, a
(1/r)-cutting with O(r^2) triangles can be constructed in O(nr) time
using Theorem 16.7.</li>
<li>A cutting tree stores the lower and upper canonical subsets for each
triangle in the partitioning. These subsets are used to efficiently
count or report lines intersected by a query point.</li>
<li>Using a two-level cutting tree, lines below a pair of query points
can be selected in O(log^2 n) time with O(n^(2+ε)) storage using Lemma
16.9.</li>
<li>By combining partition trees and cutting trees, data structures can
be designed that use less storage than cutting trees while achieving
better query times than partition trees.</li>
</ul></li>
</ol>
<p>The text also mentions higher-dimensional simplex range searching
with similar results: - Simplicial partitions exist for R^d such that
the crossing number is O(r^(1 - 1/d)). - Using these simplicial
partitions, a partition tree can be constructed for simplex range
searching in R^d with linear storage and O(n^(1-1/d + ε)) query time.
The query time can be improved to O(n^(1-1/d)(log n)^O(1)).</p>
<p>Finally, the text highlights that while exact range searching has
lower bounds for query times in terms of storage usage, approximate
range searching allows for logarithmic query times and linear storage
with additional factors.</p>
<p>Title: “Convex Hull Computations”</p>
<p>The convex hull of a set of points in the plane or higher dimensions
is the smallest convex polygon (in 2D) or polytope (in higher
dimensions) that contains all the points. Computing the convex hull has
numerous applications, including computer graphics, computational
geometry, and robotics. This article provides an overview of convex hull
computations, focusing on algorithms in two and three dimensions.</p>
<ol type="1">
<li>Graham’s Scan Algorithm: Graham’s scan is a popular algorithm for
computing the convex hull of a set of points in 2D. It operates by
sorting the points based on their polar angles with respect to a
reference point, typically the leftmost or lowest point (the “extreme”
point). The sorted list is then traversed, and any point that forms a
left turn with the current convex hull edge is added to the hull. This
process continues until all points are processed.</li>
</ol>
<p>Pseudocode: 1. Find an extreme point p. 2. Sort the remaining points
q based on their polar angle with respect to p (counterclockwise order).
3. Initialize the hull H with p and the next sorted point. 4. For each
subsequent q in the sorted list, determine if it forms a left turn with
the last two edges of H. If so, add q to H. 5. Return H as the convex
hull.</p>
<p>Time complexity: O(n log n) due to sorting and O(n) for processing
points.</p>
<ol start="2" type="1">
<li>Jarvis’s March Algorithm (Gift Wrapping): Jarvis’s march is another
algorithm for computing the convex hull of a set of points in 2D. It
works by selecting the leftmost point, p0, and iteratively finding the
next point, pi, that forms a counterclockwise turn with the current edge
connecting the last two points (pi-1 and pi-2) on the hull. This process
continues until all points are processed or a duplicate is found.</li>
</ol>
<p>Pseudocode: 1. Find an extreme point p0. 2. Initialize the hull H
with p0. 3. While there are unprocessed points, do the following: a.
Select pi as the next point forming a counterclockwise turn with H’s
last two edges (pi-1 and pi-2). b. Add pi to H. 4. Return H as the
convex hull.</p>
<p>Time complexity: O(nh), where n is the number of points, and h is the
number of vertices on the hull. In practice, h is often proportional to
n, making it O(n^2) in worst-case scenarios but O(n log n) on
average.</p>
<ol start="3" type="1">
<li>QuickHull Algorithm: QuickHull is a divide-and-conquer algorithm for
computing the convex hull of a set of points in 2D or higher dimensions.
It works by selecting an initial extreme point and recursively
partitioning the set into two subsets based on their relationship to a
hyperplane passing through the current extreme points. The process
continues until all points are processed, resulting in a nested sequence
of convex hulls that eventually converges to the final convex hull.</li>
</ol>
<p>Pseudocode: 1. Find an extreme point p0. 2. Initialize the hull H
with p0 and recursively partition the set into two subsets using
hyperplanes passing through current extreme points. 3. For each subset,
repeat steps 2-3 until all points are processed or a duplicate is found.
4. Return H as the convex hull.</p>
<p>Time complexity: O(n log n) in practice, although worst-case time
complexity can be as high as O(n^2).</p>
<ol start="4" type="1">
<li>Gift Wrapping for 3D Convex Hulls: The gift wrapping algorithm
extends to three dimensions by selecting an extreme point p0 and
iteratively finding the next extreme point pi that forms a
counterclockwise turn with the current convex hull edge connecting the
last two points (pi-1 and pi-2). This process continues until all points
are processed or a duplicate is found.</li>
</ol>
<p>Pseudocode: 1. Find an extreme point p0. 2. Initialize the hull H
with p0. 3. While there are unprocessed points, do the following: a.
Select pi as the next point forming a counterclockwise turn with H’s
last two edges (pi-1 and pi-2). b. Add pi to H. 4. Return H as the
convex hull.</p>
<p>Time complexity: O(n^2) in worst</p>
<p>The provided text appears to be an extensive index of terms and
concepts related to computational geometry, computer graphics, and
algorithm analysis. Here’s a detailed summary and explanation of some
key topics:</p>
<ol type="1">
<li><p><strong>Convex Hull</strong>: The set of all points in a space
that form the smallest convex polygon containing a given set of points.
It’s fundamental in computational geometry with applications in various
fields like data compression, pattern recognition, and more.</p>
<ul>
<li>Convex Hull Algorithm (e.g., Graham’s Scan): Efficient algorithms to
compute the convex hull for a set of points in 2D space.</li>
<li>Gift Wrapping Algorithm: Another method for computing the convex
hull, particularly useful when dealing with 3D points or other
objects.</li>
</ul></li>
<li><p><strong>Triangulation</strong>: Dividing a polygon into triangles
without any intersecting edges. This concept is crucial in various
fields like computer graphics and computational geometry.</p>
<ul>
<li>Ear Clipping: A popular method for triangulating simple polygons by
identifying ‘ears’ (triangles formed by three consecutive vertices of
the polygon).</li>
<li>Delaunay Triangulation: Special triangulations where no point is
inside the circumcircle of any triangle, useful in mesh generation and
data analysis.</li>
</ul></li>
<li><p><strong>Voronoi Diagram</strong>: A partitioning of a space into
regions based on distance to points in a specific subset of the space.
Each region contains all points closer to its generating point than to
any other.</p>
<ul>
<li>Voronoi Cells: The regions defined by Voronoi diagrams, each
associated with a unique generating point.</li>
<li>Delaunay Triangulation and Voronoi Diagram Duality: The dual
relationship between Delaunay triangulations and Voronoi diagrams; they
provide alternative ways to represent the same geometric
information.</li>
</ul></li>
<li><p><strong>Line Segment Intersection</strong>: Algorithms for
determining whether two line segments intersect, with applications in
computer graphics (e.g., collision detection) and computational geometry
(e.g., range searching).</p>
<ul>
<li>Bentley-Ottmann Algorithm: An efficient method for solving the line
segment intersection problem, used to build a planar map from a set of
line segments.</li>
</ul></li>
<li><p><strong>Range Searching</strong>: Finding all points in a dataset
that lie within a given query region (e.g., point inside a rectangle).
Efficient data structures like range trees and k-d trees are employed
for this purpose.</p>
<ul>
<li>Range Trees: Balanced binary search trees designed to answer range
queries efficiently.</li>
<li>k-d Trees: Multidimensional binary search trees optimized for
nearest neighbor searches and range queries.</li>
</ul></li>
<li><p><strong>Sweep Algorithms</strong>: A technique for solving
geometric problems by processing points or lines in a particular order,
often along a sweep line moving through the space.</p>
<ul>
<li>Plane Sweep Algorithm: Applied to solve problems involving line
segments (e.g., finding intersections) or polygons (e.g., convex hull
computation).</li>
</ul></li>
<li><p><strong>Robotics and Motion Planning</strong>: The study of
designing, controlling, and optimizing robotic systems’ movements in
their environment.</p>
<ul>
<li>Configuration Space: A mathematical representation of the set of all
possible positions a robot can assume during its motion.</li>
<li>Path Planning: Techniques to find collision-free trajectories for
robots moving in complex environments (e.g., using roadmaps or potential
fields).</li>
</ul></li>
<li><p><strong>Graph Theory and Algorithms</strong>: Fundamental
concepts and methods from graph theory, including trees, shortest paths,
and minimum spanning trees, with applications across computer
science.</p>
<ul>
<li>Dijkstra’s Algorithm: Efficient method for finding the shortest path
between nodes in a graph, assuming non-negative edge weights.</li>
<li>Prim’s/Kruskal’s Algorithm: Algorithms for computing the Minimum
Spanning Tree (MST) of a connected, undirected graph with weighted
edges.</li>
</ul></li>
<li><p><strong>Dynamic Data Structures</strong>: Data structures that
efficiently support insertion and deletion operations while maintaining
certain properties or providing query capabilities.</p>
<ul>
<li>Dynamic Convex Hull: Maintaining the convex hull of a set of points
as they are inserted/deleted dynamically.</li>
<li>Link/Cut Tree: A data structure for efficiently updating and
querying tree structures under edge insertions/deletions.</li>
</ul></li>
</ol>
<p>These topics form the core of computational geometry, providing the
theoretical foundation and algorithms essential for solving various
geometric problems in computer science, robotics, and other fields.</p>
<h3 id="book-online-aug0619">Book-online-Aug0619</h3>
<p>Chapter 15, titled “Communication Complexity: Modeling Information
Bottlenecks,” delves into an information-theoretic model for two-party
communication known as communication complexity. Despite its simplicity,
this model unveils profound depth and breadth, with basic results having
significant implications for understanding a diverse range of
computational models.</p>
<p>The chapter begins by introducing the concept of communication
complexity and its focus on modeling information bottlenecks in
communication scenarios between two parties. This is achieved through
the study of how much information must be exchanged between these
parties to solve a given problem, with the goal of minimizing this
exchange while still achieving the desired result.</p>
<p>The chapter proceeds by presenting several key results and concepts
within communication complexity:</p>
<ol type="1">
<li><strong>Deterministic Communication Complexity</strong>: This
measures the minimum number of bits that two parties need to exchange
deterministically to compute a function on their inputs. Key problems,
such as the equality function and inner product, are examined in this
context.</li>
<li><strong>Randomized Communication Complexity</strong>: Building upon
deterministic communication complexity, this variant allows for
randomness in the communication process. The chapter explores how
randomness can help reduce the number of bits needed for communication,
sometimes dramatically so.</li>
<li><strong>Information Theory Connections</strong>: The discussion
highlights the connections between communication complexity and
information theory, such as the use of entropy to quantify the amount of
uncertainty or surprise in a message. These connections lead to an
understanding of fundamental limits on communication, like the channel
capacity theorem.</li>
<li><strong>Applications and Extensions</strong>: The chapter reviews
various applications of communication complexity in different areas,
including VLSI design (time-area tradeoffs), formula lower bounds, proof
complexity, extension complexity, and pseudo-randomness. It also
discusses how this model suggests extensions to classical problems in
information theory and coding theory.</li>
<li><strong>Interactive Information Theory and Coding Theory</strong>:
This section delves into the relationship between communication
complexity and interactive information theory, which studies problems
where multiple rounds of interaction are allowed. The chapter explores
protocol compression (information complexity) and its connection to
direct sum theorems. Error correction in interactive communication is
also examined.</li>
</ol>
<p>Throughout this chapter, the author emphasizes the surprising depth
and breadth that arise from studying such a simple model of two-party
communication. The results and concepts discussed not only reveal
fundamental limits on information exchange but also provide insights
into various computational models and have practical implications in
fields like VLSI design and coding theory.</p>
<p>The P vs. NP question is a central open problem in computer science,
which asks whether every problem whose solution can be efficiently
verified (i.e., problems in the class NP) can also be solved efficiently
(i.e., are in the class P). This question has significant implications
for mathematics and various fields, as it addresses the possibility of
solving all “interesting” problems that we can recognize and verify
solutions for.</p>
<p>The class P consists of problems that can be solved efficiently by a
deterministic algorithm, while NP includes problems where given a
potential solution (a certificate or witness), one can verify its
correctness quickly. The P vs. NP question asks if these two classes are
equal (P = NP) or not (P ≠ NP).</p>
<p>The importance of the P vs. NP question stems from its philosophical
and practical implications:</p>
<ol type="1">
<li>Philosophical significance: If P = NP, it would mean that all
problems we can recognize and verify solutions for are also solvable
efficiently. This would have profound consequences for our understanding
of computation, problem-solving, and the limits of automation.</li>
<li>Practical implications: Resolving P vs. NP in either direction (P ≠
NP or P = NP) would significantly impact various fields, such as
cryptography, optimization, artificial intelligence, and more. For
instance, if P = NP, many hard problems currently used for secure
communication and data protection would lose their security guarantees,
while new efficient algorithms could revolutionize numerous industries
and scientific disciplines.</li>
</ol>
<p>The P vs. NP question is unique in its broad impact on mathematics
and other fields, as it touches upon the possibility of automating and
efficiently solving a wide range of problems that humans currently
tackle using creativity, intuition, and heuristics. The question’s
resolution would provide insights into the nature of computation,
problem-solving, and the limits of automation, potentially transforming
our understanding of these concepts.</p>
<p>The P vs. NP question is also closely related to other areas in
computer science, such as proof complexity, approximation algorithms,
and the study of hardness and randomness. Despite decades of research,
the question remains unresolved, and its resolution continues to be an
active area of investigation in theoretical computer science.</p>
<p>Boolean circuits are a fundamental model of computation that can be
seen as the hardware counterpart to software algorithms. They compute
functions by applying a sequence of Boolean operations, or gates, to
input bits, producing output bits. The universal set of gates typically
used is {∧ (AND), ∨ (OR), ¬ (NOT)}.</p>
<p>A Boolean circuit consists of input wires, gate wires, and an output
wire. Each gate wire can be connected to the output of one or more
previous gate wires or input wires. The function computed by a circuit
is determined by the specific connections between gates and the order in
which they are applied.</p>
<p>Boolean circuits can be represented graphically, with gates depicted
as nodes and wires connecting them. The fan-in of a gate refers to the
number of input wires it has, while the fan-out is the number of output
wires connected to its result. The depth of a circuit is the length of
the longest path from an input to the output, and its size (or
complexity) is the total number of gates.</p>
<p>Boolean circuits have been extensively studied in the context of
computational complexity theory due to their connection with Turing
machines and their amenability to combinatorial analysis. Research on
lower bounds for Boolean circuits aims to demonstrate that certain
functions require exponentially many gates or have a high depth,
implying that they cannot be computed efficiently by any circuit.</p>
<p>Lower bound techniques for Boolean circuits include:</p>
<ol type="1">
<li><strong>Time-space tradeoffs</strong>: These methods relate the time
complexity of a function with its space (memory) requirements. By
showing that a function requires exponential time to compute even when
given auxiliary space, one can establish a lower bound on the circuit
size.</li>
<li><strong>Gate elimination and separation</strong>: Techniques that
remove or separate gates in a circuit without changing its computed
function can reveal structural properties that lead to lower bounds. For
instance, if a gate’s removal significantly alters the function, it
suggests that the gate is essential for computation, implying a lower
bound on the circuit size.</li>
<li><strong>Circuit lower bounds via communication complexity</strong>:
This approach exploits the connection between circuit complexity and
communication complexity by demonstrating that certain functions require
exponential communication between parties to compute, implying a high
circuit depth or size.</li>
<li><strong>Algebraic methods</strong>: These techniques use properties
of Boolean functions, such as their degree or symmetry, to establish
lower bounds on circuit complexity. For example, the famous De Morgan’s
laws and other identities can be used to relate the complexity of
different circuits computing the same function, leading to lower bound
results.</li>
<li><strong>Natural proofs</strong>: Introduced by Razborov and Rudich,
natural proofs are a framework for constructing lower bounds based on
properties that are “natural” or easy to express algebraically. This
approach aims to avoid explicit circuit constructions while still
demonstrating the existence of hard functions. However, it has
limitations, as shown by the Natural Proofs Barrier, which states that
certain types of natural properties cannot lead to strong circuit lower
bounds unless P ≠ NP is false.</li>
<li><strong>Circuit lower bounds via interactive proofs</strong>: This
method uses interactive proof systems to establish circuit lower bounds.
By demonstrating that a function requires exponential communication or
computational power in an interactive setting, one can infer a high
circuit depth or size.</li>
<li><strong>Lower bounds from hardness amplification</strong>:
Techniques like the PCP theorem and other hardness amplification methods
can be used to construct functions that are hard to compute by circuits
of a certain size, even when given additional resources such as
randomness or auxiliary inputs.</li>
</ol>
<p>Despite significant progress in Boolean circuit lower bounds, proving
strong results separating P from NP remains elusive. Many of the
mentioned techniques have established lower bounds for specific function
families or under restricted assumptions but have not yet led to a
general separation result between P and NP. The challenge lies in
finding a unifying approach that can capture the full power of
non-uniform algorithms, including Boolean circuits, while avoiding the
limitations imposed by relativization and algebrization barriers.</p>
<p>Proof complexity is a subfield of mathematical logic that studies the
length and structure of proofs for propositional tautologies. The main
goal is to classify these tautologies based on the difficulty of their
proofs, similar to how circuit complexity classifies functions according
to computational complexity. Proof systems are polynomial-time
algorithms that decide whether a given statement T has a proof π such
that M(π, T) = 1.</p>
<p>Key features of a good proof system include completeness (every true
statement has a proof), soundness (no false statement has a proof), and
verification efficiency (easily checking the validity of proofs in
polynomial time). An example of a simple truth-table proof system, MTT,
is provided, which accepts a formula T as a theorem if evaluating it on
all possible inputs results in true. However, MTT’s proofs are
exponential in length regarding the number of variables, leading to the
concept of proof length and polynomially bounded proof systems.</p>
<p>Theorem 6.5 (Cook &amp; Reckhow, [CR79]) states that a polynomially
bounded proof system exists if and only if NP = coNP. This theorem
highlights the connection between proof complexity and computational
complexity.</p>
<p>The chapter then explores three concrete proof systems: algebraic
(Nullstellensatz and Polynomial Calculus), geometric (Cutting Planes and
Sum-of-Squares), and logical (Frege and Resolution).</p>
<ol type="1">
<li>Algebraic Proof Systems:
<ul>
<li>Nullstellensatz (NS) proof system: Based on Hilbert’s
Nullstellensatz, which states that if the polynomials f₁, f₂, …, fₘ have
no common root, then the constant function 1 is in the ideal generated
by these polynomials. A natural measure of proof length is the
description length of the polynomials as lists of all their coefficients
(dense representation).</li>
<li>Polynomial Calculus (PC) proof system: Introduced by Clegg, Edmonds,
and Impagliazzo, this system’s lines are polynomials with two deduction
rules capturing the definition of an ideal: addition of two ideal
elements and multiplication of an ideal element by any polynomial. Proof
length is determined by the minimal degree d of any proof, practically
determining the proof length in this representation.</li>
</ul></li>
<li>Geometric Proof Systems:
<ul>
<li>Cutting Planes (CP) proofs: A refutation derives a basic
contradiction from axioms represented as linear inequalities with
integer coefficients. The pigeonhole principle PHPm n has
polynomial-size CP proofs, but exponential lower bounds exist for other
tautologies, such as CLIQUE k n, which encodes many instances of the
pigeonhole principle.</li>
<li>Sum-of-Squares (SOS) proofs: A stronger geometric proof system for
polynomials over the reals, introduced for optimization, machine
learning, and complexity. It utilizes the Positivstellensatz theorem to
prove that a set of real polynomials has no common root by exhibiting
other polynomials satisfying specific conditions. SOS is more powerful
than CP and PC, as it can be exponentially stronger for certain
tautologies while remaining automatizable (polynomial-time proof
finding).</li>
</ul></li>
<li>Logical Proof Systems:
<ul>
<li>Frege proof system: Allows manipulating formulas without
restrictions. The cut rule, Modus Ponens, is the nontrivial derivation
rule. Frege systems can polynomially simulate Polynomial Calculus and
Cutting Planes systems.</li>
<li>Resolution proof system: A subsystem of Frege with interesting
structural limits on formulas. It has a single nontrivial derivation
rule (Modus Ponens) and is the most widely studied system due to its use
in automated theorem provers. The major open problem in proof complexity
is finding any tautology that does not have polynomial-size proofs in
the Frege system (Open Problem 6.10).</li>
</ul></li>
</ol>
<p>The text discusses abstract pseudo-randomness, a concept that extends
computational pseudo-randomness to arbitrary families of observers. It
highlights three examples of pseudo-random properties: Ramsey graphs,
weak tournaments, and good codes.</p>
<ol type="1">
<li><p>Ramsey graphs: A graph is (r log n)-Ramsey if it contains no
clique or independent set of size r. Erdős proved that almost every
graph on n vertices is (3 log n)-Ramsey. The challenge is to find
explicit pseudo-random objects, like a Ramsey graph, with better
parameters.</p></li>
<li><p>Weak tournaments: A tournament is w-weak if it contains no
dominating set of size w. Erdős proved that almost every tournament is
(1/3 log n)-weak. The Paley tournament, suggested by Graham and Spencer,
is an explicit pseudo-random object in this setting, constructed using
the quadratic character function χ over finite fields.</p></li>
<li><p>Good codes: A subspace V of dimension n/10 over F_n^2 is a
distance-d linear code if every two vectors in V differ in at least d
coordinates. Varshamov proved that almost every such subspace is a
distance-n/10 linear code. The challenge is to find explicit, efficient
codes with these properties.</p></li>
</ol>
<p>The text also introduces the concept of pseudo-randomness in general,
where a property S ⊂ U is ϵ-pseudo-random if |S| ≥ (1 - ϵ)|U|. This
notion is used to study various problems in mathematics and computer
science, often referred to as “finding hay in haystacks.” The text
discusses how this framework can be applied to the Riemann Hypothesis
and the P vs. NP question.</p>
<p>For the Riemann Hypothesis, the drunkard’s walk analogy is used to
formulate a pseudo-random property: a sequence z ∈ U_n (the set of all
n-walks) is d-homebound if it ends up within d of the pub (i.e., |∑z_i|
≤ d). The Riemann Hypothesis is equivalent to the statement that the
Möbius sequence µ_n is n^(1/2 + δ)-homebound for every δ &gt; 0.</p>
<p>The P vs. NP question can be framed in terms of pseudo-randomness by
asking whether SAT (the problem of determining whether a boolean formula
is satisfiable) is hard to compute. Almost all functions are hard to
compute, and the challenge is to determine if SAT falls into this
category.</p>
<p>Interactive proof systems (IP) are a type of probabilistic proof
system where both the prover and verifier engage in a series of
interactions to establish the validity of a statement. In IP, the
verifier is allowed to be probabilistic, meaning it can toss coins and
make random decisions during the interaction with the prover. The
prover’s goal is to convince the verifier that a given input belongs to
a target set S, while the verifier aims to ensure correctness with high
probability.</p>
<p>IP consists of two main components: completeness and soundness.
Completeness requires that the prover always convinces the verifier when
the input is indeed in S. Soundness demands that no cheating prover can
convince the verifier that an input not in S belongs to S with a
probability significantly greater than 1/2, regardless of the prover’s
computational power.</p>
<p>The class IP contains all sets S for which there exists a
probabilistic polynomial-time verifier that satisfies these conditions.
Initially, it was believed that IP could only prove statements in NP, as
deterministic verifiers cannot gain additional power from interaction.
However, the introduction of multiple provers in the MIP
(multiple-prover interactive proof) model by Ben-Or et al. [BOGKW89]
revealed its potential to encompass a broader range of problems.</p>
<p>One significant milestone was achieved by Lund, Fortnow, Karloff, and
Nisan [LFKN90], who demonstrated that IP proofs can be given for every
set in coNP. This result implies that tautologies, which are not
expected to have short NP-proofs (as it would lead to NP = coNP), do
have short interactive proofs within the IP class.</p>
<p>Furthermore, Shamir [Sha92] provided a complete characterization of
IP by showing its equivalence to PSPACE, the class of functions
computable with polynomial memory and potentially exponential time. This
equivalence revealed that IP can capture problems considered much harder
than NP and coNP, such as finding optimal strategies for games.</p>
<p>In summary, interactive proof systems (IP) are a powerful tool in
theoretical computer science that allows for the efficient verification
of statements using probabilistic interactions between a prover and
verifier. The class IP encompasses problems in coNP, demonstrating its
ability to prove statements beyond NP’s scope. Additionally, Shamir’s
characterization of IP as equivalent to PSPACE showcases its capacity to
handle complex problems previously thought to be intractable.</p>
<p>The text discusses several topics related to quantum mechanics,
quantum computing, and their interaction with classical complexity
theory. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Quantum Algorithms</strong>: Quantum algorithms leverage
quantum phenomena like superposition and entanglement to potentially
solve certain problems exponentially faster than classical algorithms.
Examples include Shor’s algorithm for factoring large numbers and
Grover’s search algorithm. However, the power of quantum algorithms is
not fully understood, with most experts believing they are stronger than
probabilistic algorithms (BPP) but not capable of solving NP-complete
problems.</p></li>
<li><p><strong>Quantum Computing Challenges</strong>: Building a
practical quantum computer faces several challenges:</p>
<ul>
<li><strong>Entanglement Maintenance</strong>: Quantum states must be
maintained in a complex entangled form, which is difficult due to the
fragility of these states.</li>
<li><strong>Decoherence Noise</strong>: The state of a quantum computer
is affected by its environment, leading to errors and loss of coherence.
Error-correcting codes are used to combat this, but their practical
implementation is still uncertain.</li>
<li><strong>Theoretical Limitations</strong>: Some researchers propose
that quantum mechanics may need revision for very large systems, which
could explain the slow progress in building a large-scale universal
quantum computer.</li>
</ul></li>
<li><p><strong>Quantum Proofs and Hamiltonian Complexity</strong>: The
concept of proof is extended to the quantum setting with QMA (Quantum
Merlin-Arthur), where a BQP machine verifies a short quantum state
witness. Kitaev discovered a QMA-complete problem, the ground state
energy of local Hamiltonians, which has led to a better understanding of
the complexity of finding ground states in various quantum
systems.</p></li>
<li><p><strong>Ground States and Entanglement</strong>: The study of
ground states and their entanglement structure has revealed deep
connections between quantum mechanics and computational complexity. The
area law conjecture suggests that for gapped systems, entanglement is
proportional to the cut size of the interaction graph, with implications
for understanding the computational complexity of finding these
states.</p></li>
<li><p><strong>Quantum Interactive Proofs</strong>: These are proof
systems where a BQP prover interacts with a classical BPP verifier. This
concept is motivated by testing quantum mechanical predictions and could
have broader applications in certifying randomness from quantum devices
under certain assumptions.</p></li>
<li><p><strong>Quantum Randomness Certification</strong>: No-signaling,
a weak physical assumption that allows boxes to produce correlated
outputs without communication, has been used to demonstrate the power of
quantum strategies over classical ones in games like CHSH. This gap
between classical and quantum strategies can be exploited to certify
randomness from no-signaling devices using techniques inspired by
randomness extractors.</p></li>
</ol>
<p>In summary, while significant progress has been made in understanding
quantum mechanics’ computational implications, many open questions
remain. The development of practical quantum computers continues to face
substantial technical hurdles, and the interplay between quantum theory
and classical complexity theory offers rich areas for further
exploration.</p>
<p>The text discusses several interactions between computational
complexity theory and various mathematical fields. Here are the main
points:</p>
<ol type="1">
<li><p>Number Theory: Gauss’ challenge of efficiently testing primality
and factoring integers has led to significant developments in
algorithmic number theory. The AKS04 deterministic algorithm for
primality testing, which uses polynomials, was a breakthrough that
de-randomized a previous probabilistic algorithm by Agrawal, Kayal, and
Saxena (AB03). Factoring integers remains an open problem with
implications in cryptography.</p></li>
<li><p>Combinatorial Geometry: The Kakeya needle problem asks for the
smallest area of a planar region containing a unit-length segment in
every direction. Besicavitch’s solution shows that this area can be
arbitrarily close to zero, while Dvir’s work using the polynomial method
proved the finite field analog conjecture, establishing lower bounds on
the size of Kakeya sets in higher dimensions.</p></li>
<li><p>Operator Theory: The Kadison-Singer problem, originally posed by
Dirac and Kadison-Singer in 1959, concerns the extension of pure states
from diagonal operators to the full algebra of continuous linear
operators on a Hilbert space. Marcus, Spielman, and Srivastava (MSS13b)
recently resolved this problem using techniques from operator theory,
discrepancy theory, Banach space theory, signal processing, and
probability.</p></li>
<li><p>Metric Geometry: The study of distortion in metric spaces has
connections to computational complexity. Linial, London, and Rabinovich
(LLR95) applied geometric ideas to algorithmic problems like sparsest
cut, while Khot and Vishnoi (KV05) used computational assumptions to
prove lower bounds on the distortion of embedding L2^2 into L1.</p></li>
<li><p>Group Theory: Computational complexity theory has influenced
group theory by introducing concepts like generation and random
generation problems. These problems involve determining whether an
element belongs to a subgroup generated by a set of elements, or
generating a random element in the subgroup, without explicitly listing
all subgroup elements. The discrete logarithm problem and integer
factoring problem are examples related to these issues in specific
groups.</p></li>
</ol>
<p>These examples demonstrate how computational complexity theory has
enriched various mathematical fields by providing new perspectives,
techniques, and problems. Conversely, insights from these mathematical
areas have also contributed to the development of computational
complexity theory.</p>
<p>14.3 Finite Automata and Counting:</p>
<p>This section explores the power of finite automata with limited
memory, specifically focusing on counting problems. It demonstrates that
even with a fixed amount of memory, these automata can solve tasks
previously thought impossible due to lower bounds on space
complexity.</p>
<p>14.3.1 Deterministic Finite Automata (DFAs): - DFAs are Turing
machines with constant space, allowing 2-way input access but no writing
capability. - They compute regular languages, which are sets of
sequences with strong periodicity structure. - Majority function, which
determines if a binary sequence has more 1’s than 0’s, cannot be
computed by DFAs.</p>
<p>14.3.2 Nondeterministic Finite Automata (NFAs) and Alternating
Automata: - Rabin and Scott [RS59] proved that deterministic 2-way
finite automata (2DFAs) are equal in power to nondeterministic ones
(2NFAs). - They also showed that alternating 2-way finite automata
(2AFA) compute no more sets than 2DFAs, i.e., only regular
languages.</p>
<p>14.3.3 Probabilistic Finite Automata (PFAs): - Adding randomness to
DFAs creates PFAs, which can only compute regular languages in the 1-way
model (1PFA). - Surprisingly, 2-way probabilistic finite automata
(2PFAs) can count arbitrarily high.</p>
<p>Theorem 14.10 [Fre81]: There is a 10-state 2PFA that computes
Majority with probability ≥2/3 on every input, and for every ϵ &gt; 0,
there is an integer c = c(ϵ) and a c-state 2PFA that computes Majority
with probability ≥1 −ϵ for every input.</p>
<p>14.3.4 Nonuniform Finite Automata: - Allowing advice
(input-independent information) to nonuniform finite automata,
Barrington [Bar86] proved that Majority can be computed with only 3 bits
of memory using a 5-state nonuniform 2DFA. - This result holds for every
function computed by a polynomial-size Boolean formula.</p>
<p>The proof uses a nonsolvable group and reversibility in the
automaton’s construction, demonstrating that short advice can be
powerful even with limited memory.</p>
<p>14.3.5 Implications: - These results show that some lower bounds on
space complexity may be false due to the surprising power of finite
automata with limited resources. - They also highlight the importance of
understanding the relative power of different computational models and
their capabilities.</p>
<p>On-line algorithms are designed to make periodic decisions without
knowing future events. They are used in various real-life scenarios such
as investment, gym memberships, dating, memory management, and taxi
dispatching. The main challenge is to determine the best course of
action for each incoming signal when future signals are unknown.</p>
<p>Competitive analysis, proposed by Sleator and Tarjan [ST85], is a
bold approach to modeling the quality of on-line algorithms. This method
disregards any knowledge about potential future event distributions and
instead compares the performance of the on-line algorithm against the
best possible algorithm with hindsight, known as an “oﬀ-line” or
“clairvoyant” algorithm.</p>
<p>In competitive analysis, the goal is to minimize the competitive
ratio, which is the worst-case ratio of the cost incurred by the on-line
algorithm to that of the optimal off-line algorithm for any input
sequence. A lower competitive ratio indicates better performance of the
on-line algorithm compared to the ideal offline solution.</p>
<p>Some examples of on-line algorithms include:</p>
<ol type="1">
<li>Ski rental problem: Renting skis for a weekend trip, where the
demand (need for skis) is unknown until the weekend arrives. The goal is
to minimize the total cost over multiple trips by deciding whether to
rent or buy skis each time.</li>
<li>Cache-oblivious algorithms: Managing cache memory in computer
systems without knowing the access pattern of future data requests. The
aim is to optimize cache usage and minimize cache misses.</li>
<li>Auctions and combinatorial optimization: Making bids in auctions or
solving combinatorial optimization problems, where the optimal solution
depends on incomplete information about future events or other bidders’
actions.</li>
<li>Machine learning and online prediction: Updating models based on
streaming data without knowing future instances. The objective is to
maintain model accuracy and adapt to changing patterns in the data.</li>
</ol>
<p>On-line algorithms have applications in various fields, including
game theory, convex optimization, learning theory, and inductive
inference. They are essential for making decisions under uncertainty and
can significantly impact efficiency and performance in real-life
scenarios.</p>
<p>The two main approaches to understanding computational learning are
the linguistic/recursion-theoretic approach (Section 17.3) and the
statistical, PAC (Probably Approximately Correct) learning approach
(Section 17.4).</p>
<ol type="1">
<li>Linguistic/Recursion-Theoretic Approach:
<ul>
<li>Assumes that data arrives adversarially with a finite “teaching”
period after which the learner must make no mistakes.</li>
<li>Identification in the limit is a key concept, where an algorithm
makes only a finite number of mistakes and then converges to the correct
answer.</li>
<li>Gold’s enumeration technique (identiﬁcation through enumeration) is
a powerful method for identifying complex function families, provided
they are enumerable and each function can be eﬃciently tested for
consistency with data.</li>
<li>Examples of identifiable classes in the limit include Boolean
functions computable by polynomial-time algorithms (P), rational
polynomials over Q, and hyperplanes with margin µ.</li>
</ul></li>
<li>Probably Approximately Correct (PAC) Learning:
<ul>
<li>Assumes that data is generated randomly, with a focus on
quantitative bounds on sample size, algorithmic eﬃciency, and tolerating
prediction errors with low probability.</li>
<li>A concept class F = {f : X →{0, 1}} is PAC-learnable if there exists
an efficient (polynomial time) algorithm that can learn f ∗∈F within
error ϵ and confidence δ using T samples from an arbitrary distribution
D on X.</li>
<li>The VC dimension (Vapnik-Chervonenkis dimension) of a concept class
F is the key combinatorial parameter determining PAC learnability:
<ul>
<li>If VC dim(F) is finite, then F is PAC-learnable with sample
complexity T ≈ O(1/ϵ (d log 1/ϵ + log 1/δ)), where d = VC dim(F).</li>
<li>Proper PAC learning, where hypotheses are restricted to the target
class F, has similar learnability conditions.</li>
</ul></li>
<li>Eﬃciency and optimization: While VC dimension determines
learnability in principle, eﬃcient algorithms are crucial for practical
implementation. The Vapnik-Chervonenkis theorem provides optimal
learning rates but does not directly offer eﬃcient algorithms.</li>
<li>Agnostic PAC learning relaxes the assumption of knowing the target
class F by focusing on the hypothesis class H and aiming to approximate
the best hypothesis h∗∈H with respect to the data. The same VC
dimension-based learnability conditions apply, showing that the VC
dimension of H determines agnostic learnability as well.</li>
<li>Compression and Occam’s razor: PAC learning provides a framework for
proving Occam’s Razor (prefer simpler explanations) through compression
arguments. A simpler consistent explanation is more likely to be flagged
as incorrect if it is indeed incorrect, leading to learnability.
Conversely, learnability implies compression in the sense that large
labeled samples from F can be compressed to shorter consistent
hypotheses with high probability.</li>
</ul></li>
</ol>
<p>These two approaches offer diﬀerent perspectives on learning, with
the statistical PAC framework dominating practical applications due to
its focus on eﬃciency and robustness in real-world scenarios.</p>
<p>The text discusses recent advances in cryptography, focusing on three
main topics: homomorphic encryption, delegation of computation, and
program obfuscation.</p>
<ol type="1">
<li><p>Homomorphic Encryption (HE): This is a public-key encryption
scheme that enables computations on encrypted data without decrypting it
first. Initially, partial HE schemes were developed for addition or
multiplication operations over specific rings. However, the challenge
was to create a fully homomorphic encryption (FHE) system that supports
both operations simultaneously. After 40 years of research, Gentry’s PhD
thesis presented an FHE scheme in 2009, based on lattice problems. The
construction was complex and costly, but it sparked further work to
simplify and improve its efficiency. Notable advancements include
[BV14], which simplifies Gentry’s initial construction and relies on the
learning with errors assumption.</p></li>
<li><p>Delegation of Computation: This application addresses scenarios
where a weak computational device (Alice) wants to delegate a
computationally heavy task to a stronger machine (Bob). Alice may not
trust Bob due to potential laziness, faults, or malice. The goal is for
Bob to convince Alice that the answer is correct efficiently.
Goldwasser, Kalai, and Rothblum [GKR08] formulated doubly efficient
interactive proof requirements: polynomial-time provers and nearly
linear-time verifiers. The problem was solved by Kalai, Raz, and
Rothblum [KRR14], who demonstrated that assuming FHE, there exists a
1-round doubly efficient interactive argument for any
computation.</p></li>
<li><p>Program Obfuscation: This is a powerful cryptographic primitive
that transforms Boolean circuits into obfuscated versions while
preserving functionality but hiding all other aspects about them. An
ideal obfuscator would map input C to output O(C) such that O(C)
computes the same function as C and reveals no more information than
black-box access. Software companies would benefit from such an
algorithm, which remains an active area of research due to its numerous
potential applications and challenges.</p></li>
</ol>
<p>In summary, these recent advances in cryptography demonstrate
progress in creating sophisticated encryption schemes (homomorphic and
program obfuscation) and efficient delegation methods for
computationally intensive tasks while preserving privacy and security
guarantees.</p>
<p>The Theory of Computation (ToC) is a fundamental and revolutionary
field that has significantly impacted various disciplines beyond
computer science and engineering. This chapter explores ToC’s
interactions with different fields, including its “parents” (computer
science and engineering, mathematics), neighboring disciplines
(optimization, coding and information theory, statistical physics), and
more remote sciences (biology, economics).</p>
<ol type="1">
<li><p>Close collaborations and interactions:</p>
<ul>
<li>Computer Science and Engineering (CS&amp;E): ToC has contributed to
the development of foundational theories underlying computational
systems. These include algorithms, data structures, computational
complexity, databases, system verification, programming languages, and
more. Theoretical ideas often preceded practical applications and
remained useful even after technologies became obsolete.</li>
<li>Mathematics: Early on, ToC used mathematical techniques from
combinatorics. As the field expanded, it required tools from diverse
mathematical areas like topology, geometry, algebra, analysis, number
theory, and algebraic geometry. Collaborations led to new purely
mathematical results in these fields and rethought existing
structures.</li>
</ul></li>
<li><p>Optimization: ToC’s connections with optimization are natural
since both study efficient algorithms. Breakthroughs like the PCP
theorem, which developed from computational complexity considerations,
have enriched the understanding of algorithmic power and limits within
optimization. LP and SDP hierarchies, powerful algorithmic paradigms in
optimization, have become clearer due to works connecting these areas
with ToC.</p></li>
<li><p>Coding and Information Theory: ToC’s relationship with coding and
information theory involves studying efficient encoding and decoding
methods for data transmission. Error-correcting codes, rate-distortion
theory, and source coding are examples of this interaction.</p></li>
<li><p>Statistical Physics: ToC has found applications in statistical
physics through the study of computational complexity in physical
systems. This includes understanding phase transitions, computing
properties of spin systems, and developing algorithms for simulating
quantum systems.</p></li>
<li><p>Biology and Economics: ToC’s impact on biology involves modeling
biological processes using computational methods, such as cellular
automata, evolutionary algorithms, and networks. In economics, ToC has
contributed to algorithmic game theory, mechanism design, and auction
theory.</p></li>
<li><p>Philosophy and Technology: ToC’s theoretical foundations have
influenced philosophical discussions on the nature of computation,
intelligence, and consciousness. It also plays a crucial role in
technological advancements, shaping our understanding of information
processing and communication systems.</p></li>
</ol>
<p>The study of ToC transcends human-made artifacts and explores natural
and artificial processes of all types. Its expanding connections and
interactions with various sciences integrate computational modeling,
algorithms, and complexity into theories of nature and society, driving
a new scientific revolution.</p>
<p>The text discusses various contributions and interactions between
computational theory (CT) and other disciplines, highlighting the impact
of CT on understanding fundamental concepts and creating novel
definitions. Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><strong>Computational Complexity as a Tool for Understanding
Fundamental Concepts</strong>:
<ul>
<li>Computational theory has been used to provide fresh perspectives on
long-standing philosophical, scientific, and societal concepts. For
instance, Alan Turing’s “Computing Machinery and Intelligence” (1950)
offers a new approach to defining intelligence.</li>
<li>Other examples include:
<ul>
<li><strong>Intelligence</strong>: Turing proposed the “Imitation Game”
or “Turing Test” to define artificial intelligence, focusing on a
machine’s ability to mimic human conversation.</li>
<li><strong>Game</strong>: CT has contributed to game theory by
introducing complexity-theoretic concepts like PPAD (Polynomial-time
Approximation of Discrete Problems) class, which helps understand the
computational difficulty of finding equilibria in games.</li>
</ul></li>
</ul></li>
<li><strong>Novel Definitions and Redefinitions</strong>:
<ul>
<li>Computational theory has sometimes redefined or provided alternative
definitions for various notions:
<ul>
<li><strong>Collusion, Coordination, Conflict</strong>: CT has helped
formalize these concepts by considering the computational resources
required to achieve them, leading to new perspectives on their nature
and significance.</li>
<li><strong>Equilibrium</strong>: Complexity theory has contributed to
game theory by introducing computational aspects into equilibrium
concepts, such as proving that computing Nash equilibria is hard for
classes like PPAD.</li>
<li><strong>Evolution</strong>: Les Valiant’s work on evolvability views
evolution as a restricted form of his PAC learning methodology, offering
a computational perspective on biological processes.</li>
</ul></li>
</ul></li>
<li><strong>Interactions with Other Disciplines</strong>:
<ul>
<li>Computational theory has interacted with and influenced various
fields, including:
<ul>
<li><strong>Physics</strong>: In quantum gravity research, CT concepts
like quantum circuit complexity are being used to analyze black hole
radiation and wormhole length problems, offering new ways to understand
fundamental physical phenomena.</li>
<li><strong>Economics</strong>: Complexity theory has enriched game
theory by introducing computational aspects into equilibrium concepts
(e.g., PPAD class for computing equilibria) and by providing hardness
results limiting the power of mechanisms. It has also contributed to
understanding information asymmetry in financial markets, as seen in
[ABBG11].</li>
<li><strong>Social Science</strong>: The advent of the Internet has led
to collaborations between computer and social scientists, enriching
social science by making it more quantitative. Examples include studying
network growth, power structures, and dynamic processes on networks
using CT tools.</li>
</ul></li>
</ul></li>
<li><strong>Philosophical Implications</strong>:
<ul>
<li>The text emphasizes that computational theory’s impact extends
beyond practical applications to philosophical considerations:
<ul>
<li>It has shown that computation can provide fresh insights into
fundamental concepts like intelligence, coordination, and
evolution.</li>
<li>By introducing complexity-theoretic aspects into these notions, CT
has highlighted their inherent computational difficulty or resource
requirements, offering new perspectives on their nature and
significance.</li>
</ul></li>
</ul></li>
</ol>
<p>In summary, computational theory’s impact goes beyond solving
practical problems; it has contributed to redefining and understanding
fundamental concepts across various disciplines by providing a
computational perspective. This interdisciplinary influence highlights
the intellectual depth of computational theory and its potential to
illuminate long-standing philosophical, scientific, and societal
questions.</p>
<p>Summary:</p>
<p>Title: “The Theory of Computation (ToC): Past, Present, and
Future”</p>
<p>This chapter provides an overview of the Theory of Computation (ToC),
its history, contributions, and potential future developments. The
author argues that ToC is a distinct and essential scientific
discipline, with profound implications for mathematics, science,
technology, and society.</p>
<ol type="1">
<li>Historical Background:
<ul>
<li>The formalism for computation was established by Alan Turing’s 1936
paper introducing the Turing machine model, leading to the creation of a
computational theory.</li>
<li>ToC’s focus on understanding computational limits, efficiencies, and
possibilities has uncovered significant insights, making it one of the
greatest scientific achievements in history.</li>
</ul></li>
<li>Contributions to Mathematics:
<ul>
<li>ToC has developed fundamental principles and tools that enable
progress in various mathematical areas, such as number theory, geometry,
and graph theory.</li>
<li>It has contributed to understanding computational hardness,
randomness, and pseudorandomness, with implications for cryptography,
coding theory, and machine learning.</li>
</ul></li>
<li>Impact on Science and Technology:
<ul>
<li>ToC’s concepts and methods are crucial for modeling scientific
phenomena, simulating complex systems, and developing algorithms in
fields like physics, biology, economics, and artificial
intelligence.</li>
<li>Its study of computational complexity has provided insights into the
limits of efficient computation, informing the design of hardware,
software, and computational models.</li>
</ul></li>
<li>Social and Philosophical Implications:
<ul>
<li>ToC explores profound intellectual questions about computation, such
as the nature of intelligence, the possibility of artificial general
intelligence, and the philosophical implications of algorithmic
decision-making.</li>
<li>It contributes to understanding the foundations of mathematics and
logic, with relevance to Gödel’s incompleteness theorems and Hilbert’s
program.</li>
</ul></li>
<li>Challenges and Future Directions:
<ul>
<li>As ToC expands and diversifies, it faces challenges related to
maintaining a central core of fundamental ideas while accommodating new
research areas and applications.</li>
<li>The author suggests that preserving cohesion within the field is
crucial for facilitating the exchange of fundamental insights across
disciplines.</li>
<li>They propose administrative changes in undergraduate education, such
as creating dedicated ToC majors, certificates, and programs to support
this goal.</li>
</ul></li>
<li>Conclusion:
<ul>
<li>The author emphasizes that understanding ToC’s past achievements and
potential is vital for its continued development and integration with
other scientific disciplines.</li>
<li>They advocate for recognizing ToC as an independent academic
discipline, fostering interdisciplinary collaborations, and promoting
its core principles in undergraduate education.</li>
</ul></li>
</ol>
<p>In essence, this chapter highlights the significance of ToC as a
distinct scientific discipline with profound implications across various
domains. It underscores the need for maintaining a central core while
embracing diversity and interdisciplinary collaborations to drive future
advancements in computation theory, mathematics, science, and
technology.</p>
<p>Title: “Graph Theory and Its Applications”</p>
<p>This paper presents an overview of graph theory, focusing on
connections and applications within the field. Graph theory is a
mathematical discipline that studies graphs, which are structures used
to model pairwise relations between objects. The study of graphs has
wide-ranging applications in various domains such as computer science,
physics, biology, economics, and social networks.</p>
<ol type="1">
<li><p>Basics of Graph Theory: A graph G consists of vertices (or nodes)
and edges connecting pairs of these vertices. It can be represented by a
pair (V(G), E(G)), where V(G) is the set of vertices and E(G) is the set
of edges, each being an unordered pair of distinct vertices.</p></li>
<li><p>Graph Types: Different types of graphs include undirected,
directed, weighted, unweighted, cyclic, acyclic (trees), connected,
disconnected, simple (no loops or multiple edges between the same
nodes), and multigraphs (allows loops and multiple edges).</p></li>
<li><p>Graph Properties: Key properties include degree (number of edges
incident to a vertex), connectivity (minimum number of elements removed
that disconnects the graph), planarity (can be drawn in a plane without
edge crossings), chromatic number (smallest number of colors needed for
proper coloring), and clique and independent sets.</p></li>
<li><p>Graph Algorithms: Various algorithms are used to solve problems
related to graphs, such as depth-first search (DFS) and breadth-first
search (BFS) for traversal, Dijkstra’s algorithm for shortest paths in
weighted graphs, Prim’s and Kruskal’s algorithms for minimum spanning
trees, and network flow algorithms like the Ford-Fulkerson
method.</p></li>
<li><p>Applications of Graph Theory:</p>
<ol type="a">
<li><p>Computer Science: Network routing, data structures (adjacency
lists/matrices), and combinatorial optimization problems (traveling
salesman problem, graph coloring).</p></li>
<li><p>Physics: Molecular structure analysis, electrical circuits
modeling, and quantum computing.</p></li>
<li><p>Biology: Protein-protein interaction networks, gene regulatory
networks, and social network analysis in epidemiology.</p></li>
<li><p>Economics: Game theory, market equilibrium models, and
transportation networks.</p></li>
<li><p>Social Networks: Analyzing relationships between individuals or
organizations, identifying communities and influencers, and modeling
information spread.</p></li>
</ol></li>
</ol>
<p>In conclusion, graph theory is a fundamental area of mathematics with
diverse applications across multiple disciplines. Understanding its
concepts and algorithms enables better modeling, analysis, and solution
of complex problems in various fields.</p>
<p>The provided text is a bibliography of references cited in a research
paper titled “Mathematics and Computation” by Avi Wigderson. The paper
itself is not included, but the bibliography provides a list of sources
that were referenced or used as background for the content within the
paper.</p>
<p>Here’s a summary of some key topics, authors, and papers mentioned in
the bibliography:</p>
<ol type="1">
<li><p><strong>Cryptography</strong>: Many references deal with
cryptography-related topics such as public-key cryptosystems (e.g., El
Gamal and Orlitsky [EGO84]), zero-knowledge proofs (e.g., Feynman
[Fey86]), and the role of relativization in complexity theory (Formanek
[For94]).</p></li>
<li><p><strong>Complexity Theory</strong>: There are numerous references
to fundamental concepts in complexity theory, including P vs NP (Fortnow
[For13], Garey and Johnson [GJ79]), hardness results for various
problems (e.g., Impagliazzo et al. [IKW02], Khot [Kho02]), and
average-case complexity (Impagliazzo [Imp95b]).</p></li>
<li><p><strong>Quantum Computing</strong>: Several references discuss
quantum computing, such as Aaronson and Arkhipov’s paper on the power of
quantum supremacy (Aaronson &amp; Arkhipov [AA13]), and various papers
about quantum algorithms for specific problems (e.g., Aharonov et
al. [AHK+19], Kitaev [Kit03]).</p></li>
<li><p><strong>Interactive Data Compression</strong> (El Gamal &amp;
Orlitsky, 1997): This work introduces an interactive data compression
protocol and analyzes its communication complexity.</p></li>
<li><p><strong>Expander Graphs</strong>: There are multiple references
to expander graphs (e.g., Hoory, Linial, Wigderson [HLW06]), which play
a significant role in the theory of computation due to their
applications in derandomization and complexity lower bounds.</p></li>
<li><p><strong>Learning Theory and Statistical Learning</strong>: Papers
like Jerrum et al. (1989) on approximating the permanent and Jerrum,
Sinclair, &amp; Vigoda (2004) on polynomial-time approximation
algorithms for permanents showcase learning theory’s connection to
complexity.</p></li>
<li><p><strong>Computational Geometry</strong>: References like Hass
&amp; Pippenger (1999) discuss computational geometry problems and their
complexity in terms of graph bisection, as well as the Markov Chain
Monte Carlo method by Jerrum (1996).</p></li>
<li><p><strong>Communication Complexity</strong>: The bibliography
includes papers on communication complexity theory, such as Kleinberg
&amp; Papadimitriou (2004) and references to earlier works like Karp
&amp; Lipton [KL82] and Krajíček (1994).</p></li>
<li><p><strong>Proof Complexity</strong>: Works by Jukna (2012),
Krajíček’s “Bounded Arithmetic, Propositional Logic, and Complexity
Theory” (1995), and his later book on proof complexity [Kra19] are
significant in this area.</p></li>
<li><p><strong>Approximation Algorithms</strong>: Papers on
approximation algorithms for problems like MAX CUT by Khot et al. (2007)
demonstrate the ongoing interest in finding efficient solutions to
NP-hard problems.</p></li>
</ol>
<p>The bibliography reflects a broad range of topics within theoretical
computer science, demonstrating the interconnectedness of various
subfields and their common interest in understanding computational
complexity and developing algorithms for solving challenging problems
efficiently.</p>
<p>The text provided is a bibliography listing various research papers
and books related to the field of computer science, mathematics, and
theoretical computer science. Here’s a summary of some key topics and
notable works mentioned:</p>
<ol type="1">
<li>Learning Theory:
<ul>
<li>Kearns &amp; Valiant (1994a): “Cryptographic limitations on learning
Boolean formulae and finite automata” discusses the inherent limitations
of learning algorithms when dealing with cryptographic primitives.</li>
<li>Kearns &amp; Valiant (1994b): “An introduction to computational
learning theory” provides an overview of the field, covering topics like
PAC learning, VC dimension, and Occam’s Razor.</li>
</ul></li>
<li>Complexity Theory:
<ul>
<li>Kalai &amp; Vempala (2003): “Efficient algorithms for universal
portfolios” deals with the design of efficient trading strategies using
computational methods.</li>
<li>Khot, Vishnoi (2005): “The unique games conjecture, integrality gap
for cut problems and embeddability of negative-type metrics into ℓ1”
proposes a conjecture related to the hardness of approximating certain
optimization problems.</li>
</ul></li>
<li>Graph Theory &amp; Combinatorics:
<ul>
<li>Karchmer &amp; Wigderson (1990): “Monotone circuits for connectivity
require super-logarithmic depth” establishes lower bounds on the depth
required by monotone circuits to compute connectivity functions in
graphs.</li>
<li>Lackenby (2015, 2016): “A polynomial upper bound on Reidemeister
moves” and “The efficient certification of knottedness and Thurston
norm” focus on the computational complexity of problems related to
low-dimensional topology.</li>
</ul></li>
<li>Algorithmic Game Theory &amp; Mechanism Design:
<ul>
<li>Nisan, Ron (1996): “Algorithmic mechanism design” explores how
algorithms can be designed to implement desired outcomes in economic
situations.</li>
</ul></li>
<li>Cryptography:
<ul>
<li>Lasserre (2001, 2009): “Global optimization with polynomials and the
problem of moments,” and “Moments, positive polynomials and their
applications” introduce techniques for solving optimization problems
using semidefinite programming, which have applications in
cryptography.</li>
</ul></li>
<li>Quantum Computing:
<ul>
<li>Shor (1994a, 1997): “Algorithms for quantum computation: discrete
logarithms and factoring,” and “Scheme for reducing decoherence in
quantum computer memory” present fundamental algorithms and techniques
in quantum computing.</li>
<li>Simons (2010): “Selected applications of LLL in number theory”
discusses the application of the Lenstra–Lenstra–Lovász (LLL) lattice
reduction algorithm to computational number theory and
cryptography.</li>
</ul></li>
<li>Machine Learning:
<ul>
<li>Minkowski (1910): “Geometrie der Zahlen” provides a foundational
work on geometric algebra, which underlies many machine learning
algorithms.</li>
<li>Landsberg (2012): “Tensors: geometry and applications” explores the
role of tensors in various mathematical and computational contexts,
including machine learning.</li>
</ul></li>
<li>Distributed Computing &amp; Algorithms:
<ul>
<li>Linial (1992): “Locality in distributed graph algorithms”
investigates how local information can be used to develop efficient
distributed graph algorithms.</li>
<li>Lovász (2012): “Large networks and graph limits, volume 60” explores
the asymptotic behavior of large graphs using techniques from measure
theory.</li>
</ul></li>
</ol>
<p>This bibliography represents a small sample of the rich literature in
theoretical computer science, mathematics, and related fields. It
showcases various research areas, methodologies, and applications within
these domains.</p>
<p>Title: “Arithmetic Circuits: A Survey of Recent Results and Open
Questions” by Shpilka and Yehudayoff (2010)</p>
<p>In this survey paper, Aleksandr Shpilka and Alexander Yehudayoﬀ
present a comprehensive overview of recent advancements in the study of
arithmetic circuits, along with highlighting some open problems in the
field. Arithmetic circuits are mathematical models used to represent
polynomial functions, which play a significant role in computational
complexity theory.</p>
<ol type="1">
<li><p><strong>Arithmetic Circuit Model</strong>: An arithmetic circuit
is defined as a directed acyclic graph where each gate computes an
arithmetic operation (+, ×) and the input variables correspond to leaves
of the circuit. The depth of such a circuit refers to the longest path
from an input variable to the output gate.</p></li>
<li><p><strong>Complexity Measures</strong>: The authors discuss several
complexity measures for arithmetic circuits, such as size (number of
gates), depth, width (maximum number of gates on any level), and rank
(dimension of the subspace spanned by the circuit).</p></li>
<li><p><strong>Lower Bounds</strong>: A primary focus of the paper is
lower bounds on the complexity of arithmetic circuits. The authors
detail several notable results, including:</p>
<ul>
<li>Valiant’s formula size lower bound for computing the determinant
[Val79a].</li>
<li>Strassen’s matrix multiplication algorithm and its implications on
depth-3 circuit size [Str73a].</li>
<li>Szegedy’s result showing a superpolynomial separation between
depth-3 and depth-4 circuits for computing the permanent polynomial
[Sze12].</li>
</ul></li>
<li><p><strong>Upper Bounds</strong>: The authors also discuss
techniques for constructing small arithmetic circuits, such as:</p>
<ul>
<li>Fast parallel computation of polynomials using few processors by
Valiant et al. [VSBR83].</li>
<li>Depth-3 arithmetic circuits over fields of characteristic zero by
Shpilka and Wigderson [SW01].</li>
</ul></li>
<li><p><strong>Open Problems</strong>: The paper concludes with a list
of open problems in the field, which include:</p>
<ul>
<li>Determining tight lower bounds for depth-4 circuits computing
permanent.</li>
<li>Closing the gap between lower and upper bounds on small-depth
circuits computing various polynomials (e.g., determinant,
permanent).</li>
</ul></li>
<li><p><strong>Relation to Other Areas</strong>: The authors highlight
connections between arithmetic circuits and other areas of computational
complexity theory, such as communication complexity, circuit complexity,
and proof complexity.</p></li>
</ol>
<p>In summary, this paper by Shpilka and Yehudayoﬀ provides a detailed
overview of the arithmetic circuit model, presenting recent advances in
lower bounds, upper bounds, and open problems. It establishes the
importance of studying arithmetic circuits as a tool for understanding
fundamental questions in computational complexity theory.</p>
<h3 id="theoryofcomputation">TheoryOfComputation</h3>
<p>The text introduces an undergraduate course on the Theory of
Computation, focusing on three main areas: Complexity Theory,
Computability Theory, and Automata Theory. The purpose of this course is
to understand the mathematical properties of computer hardware and
software, define computations and algorithms rigorously, and explore the
limitations of computers.</p>
<ol type="1">
<li><p><strong>Complexity Theory</strong> deals with classifying
problems based on their difficulty or computational hardness. It aims to
provide a rigorous proof that problems perceived as difficult are indeed
hard to solve.</p></li>
<li><p><strong>Computability Theory</strong>, arising from the works of
Gödel, Turing, and Church in the 1930s, investigates which mathematical
problems can be solved by computers and provides formal definitions for
computer, algorithm, and computation. The development of these
theoretical models eventually led to real-world computers.</p></li>
<li><p><strong>Automata Theory</strong> focuses on defining and
analyzing different computational models such as finite automata,
context-free grammars, and Turing machines. It aims to determine whether
these models have equal power or if one model can solve more problems
than another.</p></li>
</ol>
<p>The course covers Automata Theory first, followed by Computability
Theory. Complexity Theory is covered in a separate course (COMP 3804).
This theory is essential for computer science as it deals with the
fundamental capabilities and limitations of computers, influencing
various fields like programming languages, compilers, artificial
intelligence, and computer security.</p>
<p>The mathematical preliminaries required for this course include
understanding sets, integers, rational numbers, real numbers, subsets,
unions, intersections, Cartesian products, complements, binary
relations, functions, equivalence relations, graphs, alphabets, strings,
languages, Boolean operations, and proof techniques.</p>
<p>Proof techniques discussed in the text are: - Direct proofs: These
involve approaching the theorem directly to prove its truth. -
Constructive proofs: They not only show existence but also give a method
of creating the object in question. - Nonconstructive proofs: These
demonstrate the existence of an object without actually constructing it.
- Proofs by contradiction: By assuming the negation of the statement and
deriving a logical contradiction, we can prove that the original
statement must be true. - Pigeonhole principle: If n + 1 or more objects
are placed into n boxes, at least one box will contain two or more
objects. This principle has surprising consequences, such as proving the
existence of subsequences in a sequence of numbers. - Proofs by
induction: A method to prove statements for all positive integers by
first establishing their truth for the base case (usually n = 1) and
then showing that if they hold for an arbitrary integer k ≥ 1, they must
also be true for k + 1.</p>
<p>This chapter discusses finite automata and regular languages,
focusing on deterministic (DFA) and nondeterministic finite automata
(NFA).</p>
<ol type="1">
<li>Deterministic Finite Automata (DFA):
<ul>
<li>A 5-tuple M = (Q, Σ, δ, q0, F), where Q is a finite set of states, Σ
is an alphabet, δ: Q × Σ → Q is the transition function, q0 is the start
state, and F ⊆ Q is the set of accept states.</li>
<li>DFA processes input strings one symbol at a time, moving from state
to state based on the transition function.</li>
<li>Example given: A toll gate controller that accepts sequences of 5,
10, and 25 cent coins until reaching 25 cents or more.</li>
</ul></li>
<li>Nondeterministic Finite Automata (NFA):
<ul>
<li>Similar structure to DFA but allows multiple next states for a given
state-symbol pair (including zero or more states).</li>
<li>Transition function δ: Q × Σϵ → P(Q), where Σϵ = Σ ∪ {ϵ}.</li>
<li>NFA accepts a string if there exists at least one path that
satisfies specific conditions, without hanging before reading the entire
input.</li>
</ul></li>
<li>Regular Operations:
<ul>
<li>Union (A ∪ B): Set of all strings in A or B.</li>
<li>Concatenation (AB): Set of strings formed by concatenating w ∈ A and
w’ ∈ B.</li>
<li>Star (A∗): Set of all finite concatenations of strings from A,
including the empty string ϵ.</li>
</ul></li>
<li>Closure Properties:
<ul>
<li>The set of regular languages is closed under union. Proven using
DFAs to construct a new DFA that recognizes the union language.</li>
<li>Regular languages are also closed under concatenation and star
operations, which will be proven later in the text by introducing more
general finite automata, such as NFAs.</li>
</ul></li>
<li>Equivalence of DFA and NFA:
<ul>
<li>Theorem 2.5.1 demonstrates that every NFA can be converted into an
equivalent DFA (L(M) = L(N)). This implies DFAs and NFAs have the same
computational power.</li>
</ul></li>
</ol>
<p>The text discusses the equivalence between Deterministic Finite
Automata (DFAs) and Nondeterministic Finite Automata (NFAs), as well as
the closure properties of regular languages under various
operations.</p>
<h3 id="equivalence-of-dfas-and-nfas">Equivalence of DFAs and NFAs</h3>
<ol type="1">
<li><strong>Construction from NFA to DFA:</strong>
<ul>
<li>For each state <code>r</code> in the NFA, the ϵ-closure
<code>C_ϵ(r)</code> is defined as the set of all states reachable by
zero or more ε-transitions.</li>
<li>The start state <code>q'</code> of the DFA is defined as the
ϵ-closure of the initial state <code>{q}</code> of the NFA:
<code>q' = C_ϵ({q})</code>.</li>
<li>For any set of NFA states <code>R</code> in the DFA, the transition
function <code>δ'(R, a)</code> is defined as the union of ϵ-closures of
transitions from each state <code>r ∈ R</code>:
<code>δ'(R, a) = [r∈R C_ϵ(δ(r, a))]</code>.</li>
<li>The set of accepting states in the DFA, <code>F'</code>, consists of
all sets <code>R</code> that intersect with the original NFA’s accepting
states <code>F</code>: <code>F' = {R ∈ Q' : R ∩ F ≠ ∅}</code>.</li>
</ul></li>
<li><strong>Theorem 2.5.2:</strong>
<ul>
<li>This theorem states that a language is regular if and only if there
exists an NFA that accepts it, formalizing the construction process from
NFAs to DFAs.</li>
</ul></li>
</ol>
<h3 id="closure-under-regular-operations">Closure Under Regular
Operations</h3>
<ol type="1">
<li><strong>Union (Theorem 2.6.1):</strong>
<ul>
<li>If <code>A1</code> and <code>A2</code> are regular languages over
the same alphabet Σ, then their union <code>A1 ∪ A2</code> is also a
regular language. This is proven by constructing an NFA that simulates
computations of both NFAs.</li>
</ul></li>
<li><strong>Concatenation (Theorem 2.6.2):</strong>
<ul>
<li>If <code>A1</code> and <code>A2</code> are regular languages over
the same alphabet Σ, then their concatenation <code>A1 * A2</code> is
also a regular language. This is shown by constructing an NFA that
simulates reading a string from <code>A1</code>, followed by a string
from <code>A2</code>.</li>
</ul></li>
<li><strong>Star (Theorem 2.6.3):</strong>
<ul>
<li>If <code>A</code> is a regular language, then its Kleene star
<code>A*</code> is also regular. This is demonstrated through an NFA
that can simulate zero or more repetitions of strings in the original
language.</li>
</ul></li>
</ol>
<h3 id="regular-expressions-and-languages">Regular Expressions and
Languages</h3>
<ol type="1">
<li><strong>Definition of Regular Expressions (Deﬁnition
2.7.1):</strong>
<ul>
<li>The rules for constructing regular expressions include: ϵ, ∅, each
symbol <code>a</code> from the alphabet Σ, union (<code>∪</code>),
concatenation (<code>*</code>), and Kleene star (<code>∗</code>).</li>
</ul></li>
<li><strong>Language Description by Regular Expressions (Deﬁnition
2.7.2):</strong>
<ul>
<li>The language described by a regular expression is defined
recursively based on the structure of the expression.</li>
</ul></li>
<li><strong>Algebraic Identities for Regular Expressions (Theorem
2.7.4):</strong>
<ul>
<li>These identities allow manipulation of regular expressions, aiding
in proving equalities between languages.</li>
</ul></li>
<li><strong>Equivalence Theorem (Theorem 2.8.1):</strong>
<ul>
<li>This theorem states that a language is regular if and only if it can
be described by a regular expression. Proven by showing every regular
expression describes a regular language and vice versa through
converting DFAs to expressions.</li>
</ul></li>
<li><strong>Pumping Lemma (Section 2.9):</strong>
<ul>
<li>A tool for proving languages are not regular by demonstrating the
lack of certain structural properties inherent to all regular languages,
such as memory constraints and repetitive structure in long
strings.</li>
</ul></li>
</ol>
<p>The detailed explanations above provide a comprehensive understanding
of how DFAs and NFAs relate, how regular languages behave under various
operations, and how they can be represented using regular expressions,
culminating in the Pumping Lemma for identifying non-regular
languages.</p>
<p>The provided text discusses Context-Free Grammars (CFGs) and their
role in defining context-free languages, which are a class of formal
languages that have recursive structures. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Context-Free Grammars (CFGs):</strong></p>
<ul>
<li>A CFG is a 4-tuple G = (V, Σ, R, S), where:
<ul>
<li>V is a finite set of variables or non-terminal symbols.</li>
<li>Σ is a finite set of terminal symbols.</li>
<li>V ∩Σ = ∅ (no overlap between variables and terminals).</li>
<li>S is the start variable from V.</li>
<li>R is a finite set of rules, each having the form A → w, where A ∈V
and w ∈(V ∪Σ)*.</li>
</ul></li>
</ul></li>
<li><p><strong>Derivation in CFGs:</strong></p>
<ul>
<li>A string uwv can be derived from uAv by applying rule A →w to the
string uAv.</li>
<li>A string v can be derived from u if one of the following holds:
<ol type="1">
<li>u = v, or</li>
<li>there exist an integer k ≥2 and strings u₁, …, uₖ such that u = u₁
⇒u₂ ⇒…⇒uₖ = v.</li>
</ol></li>
</ul></li>
<li><p><strong>Language of a CFG:</strong></p>
<ul>
<li>The language L(G) of a CFG G is the set of all strings in Σ∗that can
be derived from the start variable S (S*⇒w).</li>
<li>A language L is context-free if there exists a CFG G such that L(G)
= L.</li>
</ul></li>
<li><p><strong>Examples:</strong></p>
<ol type="a">
<li><strong>Properly Nested Parentheses:</strong>
<ul>
<li>V = {S}, Σ = {a, b}, R = {S →ϵ, S →aSb, S →SS}</li>
<li>Derives strings like ((())) and derives the language of properly
nested parentheses.</li>
</ul></li>
<li><strong>Nonregular Language 0<sup>n1</sup>n:</strong>
<ul>
<li>G₁ = (V₁, Σ, R₁, S₁), where V₁ = {S₁}, Σ = {0, 1}, R₁ = {S₁ →ϵ, S₁
→0S₁1}</li>
<li>Derives strings like 0<sup>n1</sup>n for n ≥0.</li>
</ul></li>
<li><strong>Complement of Nonregular Language:</strong>
<ul>
<li>L = {0<sup>n1</sup>n : n ≥0}, L’ = L complement.</li>
<li>G = (V, Σ, R, S), where V = {S, S₁, S₂}, Σ = {0, 1}, and R consists
of rules: S → S₁ | S₂ S₁ → ϵ | 0S₁1 S₂ → 0 | 0S₂ | 0S₂1</li>
<li>Derives L’ by handling strings of type 1., 2., and those containing
‘10’ as substrings.</li>
</ul></li>
</ol></li>
</ol>
<p>These examples demonstrate that context-free grammars can generate a
wide range of languages, including some nonregular ones like
0<sup>n1</sup>n and its complement. Understanding CFGs is crucial for
defining programming language syntax and compiling them into executable
code.</p>
<p>The text discusses Context-Free Languages (CFL) and Pushdown Automata
(PDA), focusing on their equivalence. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Context-Free Grammars (CFG)</strong>: A CFG is a set of
production rules that generate all possible strings in a given language.
The grammar G = (V, Σ, R, S) consists of a set of variables V, an
alphabet Σ, a set of rules R, and a start variable S.</p></li>
<li><p><strong>Chomsky Normal Form (CNF)</strong>: A CFG is said to be
in CNF if all its production rules have one of three forms:</p>
<ul>
<li>A → BC (where A, B, C are variables; B ≠ $, C ≠ $)</li>
<li>A → a (where A is a variable and a is a terminal)</li>
<li>$ → ϵ</li>
</ul></li>
<li><p><strong>Pushdown Automata (PDA)</strong>: PDA is a computational
model that accepts languages described by CFGs. It consists of:</p>
<ul>
<li>Tape: A sequence of cells, each containing a symbol from an alphabet
Σ or a blank symbol 2.</li>
<li>Stack: A last-in, first-out structure with symbols from an alphabet
Γ, including a special symbol $.</li>
<li>State Control: A finite set of states Q, one of which is the start
state q.</li>
</ul></li>
<li><p><strong>Deterministic PDA (DPDA)</strong>: In DPDA, for each
configuration (state, tape symbol, stack top), there’s exactly one
computation step possible. The transition function δ : Q × (Σ ∪ {2}) × Γ
→ Q × {N, R} × Γ* determines the next state, direction of tape head
movement, and stack action (pop, push).</p></li>
<li><p><strong>Nondeterministic PDA (NPDA)</strong>: NPDA allows for
multiple possible computation steps from a given configuration. The
transition function δ : Q × (Σ ∪ {2}) × Γ → Pf(Q × {N, R} × Γ*) returns
a set of possible next configurations.</p></li>
<li><p><strong>Equivalence of CFG and NPDA</strong>: The main theorem
states that for any language A over an alphabet Σ, A is context-free
(i.e., generated by some CFG) if and only if there exists an NPDA that
accepts A. This equivalence demonstrates that CFGs and NPDA are equally
powerful in describing formal languages.</p></li>
<li><p><strong>Construction of NPDA from CFG</strong>: To prove the
theorem, one direction involves constructing an NPDA from a given CFG
(in CNF). The construction parallels the CFG’s derivation process:</p>
<ul>
<li>Each variable in the CFG corresponds to a state in the PDA.</li>
<li>The start variable $ corresponds to the start state of the PDA.</li>
<li>For each rule A → BC, add transitions based on reading B and pushing
C onto the stack (and moving the tape head as necessary).</li>
<li>For rules A → a, add transitions that read a terminal symbol and
push the corresponding variable onto the stack.</li>
<li>For the rule $ → ϵ, ensure the PDA accepts when the stack is empty
and the tape head is at the correct position.</li>
</ul></li>
</ol>
<p>This construction shows how CFGs and NPDA can describe the same set
of languages, highlighting their equivalence in computational power.</p>
<p>Title: Summary of Chapter 3 - Context-Free Languages and the Pumping
Lemma</p>
<ol type="1">
<li><p><strong>Context-Free Grammars (CFG):</strong> A CFG is a set of
production rules that generate context-free languages. It consists of a
vocabulary V, an alphabet Σ, a set of production rules R, and a start
symbol S. The power of a CFG is determined by its ability to handle
nested structures, thanks to the Chomsky Normal Form (CNF), which
restricts rules to be either A → BC or A → a.</p></li>
<li><p><strong>Pushdown Automata (PDA):</strong> PDAs are automata that
use an auxiliary storage stack, allowing them to recognize context-free
languages. They have states Q, input alphabet Σ, stack symbols Γ,
transition function δ, start state q0, and accepting states F. The
instructions for constructing a PDA from a CFG are provided in the
chapter.</p></li>
<li><p><strong>Equivalence of PDAs and CFGs:</strong> It’s proven that
every context-free grammar can be converted into an equivalent
nondeterministic pushdown automaton (NPDA), and vice versa, although
only the conversion from CFG to NPDA is given. This establishes the
equivalence between these two models.</p></li>
<li><p><strong>Pumping Lemma for Context-Free Languages:</strong> The
pumping lemma is a crucial tool for proving that certain languages are
not context-free. It states that any sufficiently long string in a
context-free language can be “pumped” (repeated) according to specific
rules, yielding another valid string in the language.</p>
<ul>
<li><p><strong>Theorem 3.7.2:</strong> This claim explains how to
transform a CFG-generated string into an NPDA configuration and vice
versa, establishing the connection between them.</p></li>
<li><p><strong>Proof of Equivalence:</strong> Using Claim 3.7.2, it’s
shown that a string belongs to L(G) (the language generated by G) if and
only if the NPDA accepts this string, proving that CFGs and NPDAs
recognize the same class of languages.</p></li>
</ul></li>
<li><p><strong>Applications of the Pumping Lemma:</strong> The pumping
lemma is used to prove that certain languages are not context-free by
demonstrating contradictions when attempting to pump strings according
to its rules. Three examples are provided:</p>
<ul>
<li>Language A = {anbncn : n ≥0} (not context-free)</li>
<li>Language B = {wwR : w ∈{a, b}∗} (context-free)</li>
<li>Language C = {ambncmn : m ≥0, n ≥0} (not context-free)</li>
</ul></li>
<li><p><strong>Exercises:</strong> The chapter concludes with several
exercises that ask students to apply the concepts learned in
constructing CFGs for various languages, proving properties of languages
using automata and lemmas, and designing PDAs for given languages. These
exercises help reinforce understanding of context-free languages and
their recognition mechanisms.</p></li>
</ol>
<p>Title: Summary and Explanation of Chapter 5: Decidable and
Undecidable Languages</p>
<p>This chapter explores the limitations of Turing machines,
specifically focusing on decidable and undecidable languages. Here’s a
detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Decidability</strong>: A language A is decidable if there
exists a Turing machine M that correctly determines whether any input
string w belongs to A or not. This means that for every string w, the
computation of M on w either terminates in an accept state (if w ∈ A) or
in a reject state (if w ∉ A).</p>
<ul>
<li><p><strong>ADFA</strong>: The language consisting of pairs ⟨M, w⟩
where M is a deterministic finite automaton accepting w. This language
is decidable because we can construct an algorithm that simulates the
computation of M on w and checks if it accepts w.</p></li>
<li><p><strong>ANFA</strong>: Similar to ADFA but for non-deterministic
finite automata. It’s also decidable, achieved by converting the
non-deterministic automaton into a deterministic one using an
algorithmic construction.</p></li>
<li><p><strong>ACFG</strong>: The language consisting of pairs ⟨G, w⟩
where G is a context-free grammar and w ∈ L(G). This language is
decidable as we can convert the CFG to Chomsky normal form and then
check if w can be derived from the start symbol using 2n - 1
steps.</p></li>
</ul></li>
<li><p><strong>Undecidability</strong>: A language that is not decidable
is called undecidable. An example of an undecidable language is ATM,
which consists of pairs ⟨M, w⟩ where M is a Turing machine accepting w.
The proof of its undecidability uses contradiction and involves
constructing another Turing machine D that behaves oppositely to the
assumed decider for ATM.</p></li>
<li><p><strong>Countable Sets</strong>: The concept of countability is
introduced, which states that a set A is countable if it’s finite or
there exists a bijection between A and N (the natural numbers). The set
R of real numbers is shown not to be countable using Cantor’s diagonal
argument.</p></li>
<li><p><strong>The Halting Problem</strong>: The halting problem (Halt)
is proven undecidable by showing that assuming it were decidable leads
to a contradiction, similar to the proof for the uncountability of real
numbers. This is done by constructing a Java program D that performs the
opposite action of an assumed decider H for the halting
problem.</p></li>
<li><p><strong>Rice’s Theorem</strong>: This theorem states that any
non-trivial semantic property (a property depending on the language
accepted) of Turing machines forms an undecidable language. In other
words, if P is a subset of all Turing machines that satisfies certain
conditions, then P is undecidable. Examples include properties like
whether a machine accepts the empty string or has a finite
language.</p></li>
<li><p><strong>Enumerability</strong>: A language A is enumerable if
there exists a Turing machine M such that, for every string w, if w ∈ A,
then M accepts w in a finite number of steps, and if w ∉ A, then M does
not accept w (either rejects or doesn’t terminate). Every decidable
language is also enumerable. The Hilbert’s problem, which asks whether a
polynomial with integer coefficients has an integral root, is shown to
be enumerable but undecidable.</p></li>
</ol>
<p>This chapter concludes that while Turing machines can simulate any
algorithm we intuitively consider computable (Church-Turing thesis),
there are fundamental limitations to what can be computed—most problems
are undecidable or merely enumerable rather than decidable.</p>
<p>The Complexity Class NP (Nondeterministic Polynomial Time) is a set
of decision problems for which a ‘yes’ answer can be verified quickly,
even if finding the answer may take longer. This class is defined using
nondeterministic Turing machines, which are theoretical machines that
can explore multiple computational paths simultaneously.</p>
<ol type="1">
<li><p><strong>Definition</strong>: A language L is in NP if there
exists a polynomial-time verifier V and a polynomial p(n) such that for
every string x:</p>
<ul>
<li>If x ∈ L (i.e., x is a ‘yes’ instance), then there exists a
certificate c (a witness) of length at most p(|x|) such that V(x,c) =
1.</li>
<li>If x ∉ L (i.e., x is a ‘no’ instance), then for every certificate c
of length at most p(|x|), V(x,c) = 0.</li>
</ul></li>
</ol>
<p>In simpler terms, an NP problem has the property that if we’re given
a proposed solution (certificate), we can quickly check whether it’s
correct or not. However, finding such a solution in the first place
might be difficult and could potentially require exponential time.</p>
<ol start="2" type="1">
<li><p><strong>Examples</strong>:</p>
<ul>
<li><p><strong>3-Coloring Problem</strong>: Given a graph G, is there a
valid 3-coloring of its vertices? Here, the certificate c is a color
assignment for each vertex. The verifier V can quickly check if two
adjacent vertices have different colors.</p></li>
<li><p><strong>Subset Sum Problem</strong>: Given a set S of integers
and a target sum B, does S contain a subset whose elements sum up to B?
The certificate c is a subset of S. The verifier V checks the sum of the
elements in c against B.</p></li>
</ul></li>
<li><p><strong>Relationship with P</strong>: It’s conjectured that NP ≠
P, meaning there are problems in NP that cannot be solved in polynomial
time by a deterministic Turing machine (i.e., they’re not in P).
However, proving this separation is one of the biggest open problems in
computer science—the famous P vs. NP problem.</p></li>
<li><p><strong>NP-Completeness</strong>: A problem is NP-complete if
it’s in NP and every problem in NP can be reduced to it in polynomial
time. In other words, an NP-complete problem is at least as hard as the
hardest problems in NP. Examples of NP-complete problems include the
Traveling Salesman Problem, Vertex Cover, and 3SAT
(3-Satisfiability).</p></li>
<li><p><strong>Practical Implications</strong>: Many real-world
optimization and search problems are NP-hard or NP-complete, meaning
they’re difficult to solve exactly but can be approximated or solved in
specific cases using heuristics or specialized algorithms. Understanding
the complexity class NP helps guide our expectations for what’s
computationally feasible and informs the development of efficient
algorithms.</p></li>
</ol>
<p>The text discusses the concept of NP-completeness, which is a class
of problems in computational complexity theory that are considered the
hardest in NP (Non-deterministic Polynomial time). The key idea is that
if a problem in NP can be reduced to an NP-complete problem using
polynomial-time reductions, then solving the NP-complete problem would
imply a solution for all problems in NP.</p>
<p>Here’s a detailed explanation:</p>
<ol type="1">
<li><p><strong>NP (Non-deterministic Polynomial time)</strong>: This is
a class of decision problems where, given a “certificate” or “witness”,
one can verify that the certificate proves the answer to be ‘yes’ in
polynomial time. In other words, there exists a polynomial-time
algorithm for checking the correctness of solutions (certificates), but
finding such a solution might not be efficient.</p></li>
<li><p><strong>NP-completeness</strong>: A problem is NP-complete if it
belongs to NP and is at least as hard as every other problem in NP. This
means that any problem in NP can be reduced to an NP-complete problem
using polynomial-time reductions. If there exists a polynomial-time
algorithm for solving an NP-complete problem, then P = NP (Polynomial
time equals Non-deterministic Polynomial time), which is one of the most
significant open questions in computer science.</p></li>
<li><p><strong>Reductions</strong>: A reduction from problem A to
problem B is a way to transform instances of problem A into instances of
problem B such that the solutions to the transformed problem (B) can be
used to solve the original problem (A). If there exists a
polynomial-time reduction from A to B, and A ∈ NP, then B is also in
NP.</p></li>
<li><p><strong>Cook-Levin Theorem</strong>: This theorem states that the
Boolean Satisfiability Problem (SAT), a well-known NP-complete problem,
can be reduced in polynomial time to any other problem in NP. In other
words, if there exists an efficient algorithm for solving SAT, then all
problems in NP can be solved efficiently.</p></li>
<li><p><strong>Example of an NP-complete problem - Domino
Tiling</strong>: The text provides an example of an NP-complete problem
called “Domino Tiling.” Given a frame with certain constraints and tile
types, the problem asks whether it’s possible to completely fill the
frame using the given tiles without violating the constraints. This
problem is proven to be NP-complete by reducing another known
NP-complete problem (3SAT) to Domino Tiling in polynomial time.</p></li>
<li><p><strong>Implications</strong>: If an efficient algorithm for
solving any NP-complete problem were found, it would imply P = NP, as we
could solve all problems in NP using that algorithm. This would have
profound implications for computer science and cryptography, as many
security protocols rely on the assumption that certain problems are hard
to solve efficiently (i.e., P ≠ NP).</p></li>
</ol>
<p>In summary, understanding NP-completeness is crucial because it helps
classify problems according to their inherent difficulty and provides a
framework for comparing the relative complexity of various computational
tasks. The existence of NP-complete problems also highlights the gap
between what we can verify efficiently (NP) and what we can find
efficiently (P), which remains an open question in computer science.</p>
<p>The text provided discusses several concepts related to computational
complexity theory, specifically focusing on NP-complete problems. Here’s
a detailed explanation:</p>
<ol type="1">
<li><p><strong>Turing Machines and Domino Puzzles</strong>: The text
first explains how Turing machines can be used to model domino puzzles.
Each configuration of the puzzle corresponds to a state of the Turing
machine, and the movement of the tape head mirrors the placement of
dominoes. This establishes an equivalence between solutions to domino
puzzles and computations of certain Turing machines.</p></li>
<li><p><strong>NP-completeness</strong>: NP-complete problems are a
class of computational problems that are both in NP (can be solved in
non-deterministic polynomial time) and can reduce any other NP problem
to them, meaning they are at least as hard as the hardest problems in
NP.</p></li>
<li><p><strong>SAT Problem</strong>: The Satisfiability (SAT) problem is
about determining if a Boolean formula can be satisfied by some
assignment of truth values to its variables. The text shows that SAT is
NP-complete by reducing the Domino problem to SAT. It does this by
encoding each domino configuration as a Boolean formula, where variables
represent tile types at different positions, and clauses enforce the
rules of the puzzle (e.g., no two tiles of the same type can occupy the
same position).</p></li>
<li><p><strong>3-SAT Problem</strong>: 3-SAT is a special case of SAT
where each clause has exactly three literals. The text proves that 3-SAT
is NP-complete by reducing SAT to 3-SAT: if a given Boolean formula in
any form (not necessarily 3-CNF) can be satisfied, it can also be
converted into an equivalent 3-CNF formula that can also be
satisfied.</p></li>
<li><p><strong>Clique Problem</strong>: The Clique problem involves
finding the largest complete subgraph (a subset of vertices where every
two are connected by edges) in a given graph. The text proves that
Clique is NP-complete using reductions from 3-SAT, demonstrating how any
instance of 3-SAT can be translated into an equivalent Clique problem
and vice versa.</p></li>
<li><p><strong>Other NP-Complete Problems</strong>: The text lists
several other problems known to be NP-complete, such as the Traveling
Salesman Problem, Bin Packing, Time Tables, Motion Planning, Map
Labeling, etc. These problems vary widely in their nature (graph theory,
optimization, scheduling), but all share the property that a solution
can be verified quickly (in polynomial time) but finding such a solution
might require exploring an exponentially large search space.</p></li>
</ol>
<p>The exercises at the end of the section ask for proofs related to
complexity classes (like P and NP), reductions between problems, and
properties of specific languages (like SAT, 3-SAT, Clique, etc.). They
aim to deepen understanding of these concepts through practice.</p>
<p>The given text provides a summary of several key concepts in
computational theory, specifically focusing on different classes of
formal languages and their properties. Let’s break down the details:</p>
<ol type="1">
<li><p><strong>Regular Languages</strong>: These are the simplest class
of formal languages. A language is regular if there exists a
deterministic finite automaton (DFA), nondeterministic finite automaton
(NFA), or regular expression that describes it. Regular languages are
closed under union, concatenation, Kleene star, and subset operations.
The Pumping Lemma for Regular Languages can be used to prove that
certain languages aren’t regular.</p></li>
<li><p><strong>Context-Free Languages</strong>: These are more complex
than regular languages. A language is context-free if it can be
generated by a context-free grammar or accepted by a nondeterministic
pushdown automaton (NPDA). Every regular language is context-free, but
not every context-free language is regular (e.g., {anbn : n ≥ 0}).
Context-free languages are closed under union and concatenation, but not
necessarily for intersection and complement operations. The Pumping
Lemma for Context-Free Languages can be used to prove that certain
languages aren’t context-free.</p></li>
<li><p><strong>Decidable and Enumerable Languages</strong>: These
classes of languages are defined based on “reasonable” computational
devices like Turing machines or Java programs. Every context-free
language is decidable, but not all decidable languages are context-free
(e.g., {anbncn : n ≥ 0}). All enumerable languages are also decidable,
but not vice versa. The Halting Problem is an example of a language
that’s enumerable but not decidable. Moreover, there exist languages
that are decidable but not enumerable.</p></li>
<li><p><strong>Complexity Classes</strong>: These classes categorize
problems based on computational complexity.</p>
<ul>
<li><p>P: This class includes all decision problems that can be solved
by a deterministic Turing machine in polynomial time.</p></li>
<li><p>NP: This class includes all decision problems for which a
solution can be verified by a deterministic Turing machine in polynomial
time, given a certificate (or “proof”) that is polynomial-length. The
key point here is that while we don’t know if there are problems in NP
that aren’t in P, it’s widely believed that such problems
exist.</p></li>
<li><p>NP-Complete: These are the hardest problems in NP. A problem is
NP-complete if it’s in NP and every problem in NP can be reduced to it
in polynomial time. In other words, an NP-complete problem is at least
as hard as any other problem in NP.</p></li>
</ul></li>
<li><p><strong>Church-Turing Thesis</strong>: This is a hypothesis that
states any function computable by an algorithm can also be computed by a
Turing machine. Essentially, all reasonable models of computation are
equivalent to Turing machines.</p></li>
</ol>
<p>The summary concludes with a diagram illustrating the relationships
among these language classes and complexity classes. Regular languages
are a subset of context-free languages, which are a subset of decidable
languages, which in turn are a subset of enumerable languages. All these
subsets are contained within the class of all languages. NP is a subset
of P, but it’s unknown if they’re equal (i.e., whether P = NP).
Furthermore, NP-complete problems sit atop the NP class in terms of
complexity.</p>
<h3 id="book">book</h3>
<p>The chapter “Congruences” introduces the concept of modular
arithmetic, often referred to as ‘clock arithmetic’. It lays the
foundation for understanding groups by exploring integers modulo n. If n
is a prime number, these integers form a field. This chapter covers the
following topics:</p>
<ol type="1">
<li>Basic Properties (Section 1.1):
<ul>
<li>Definition of congruence modulo n: a ≡ b (mod n) if a − b is
divisible by n.</li>
<li>Proof that congruence is an equivalence relation, meaning it’s
reflexive, symmetric, and transitive.</li>
</ul></li>
<li>Divisibility Tests (Section 1.2):
<ul>
<li>Explanation of simple tests for divisibility by small numbers using
congruences:
<ul>
<li>Test for divisibility by 2 or 5: a is even/divisible by 5 if its
last digit a₀ is even/0 or 5, respectively.</li>
<li>Test for divisibility by 3 or 9: sum of digits of the number is
divisible by 3 or 9.</li>
<li>Test for divisibility by 7: b − 2a₀ is divisible by 7, where a = 10b
+ a₀.</li>
</ul></li>
</ul></li>
<li>Common Divisors (Section 1.3):
<ul>
<li>Explanation of greatest common divisor (gcd) and least common
multiple (lcm).</li>
<li>The Euclidean algorithm to compute gcd(a, b), showing it involves
successive division with remainder until reaching zero, and expressing
the gcd as a linear combination of a and b.</li>
</ul></li>
<li>Solving Congruences (Section 1.4):
<ul>
<li>Theorem stating that if (a, n) = 1, then the congruence ax ≡ b (mod
n) has a unique solution modulo n.</li>
<li>Example demonstrating how to solve a specific linear congruence
using the Euclidean algorithm.</li>
</ul></li>
<li>The Integers Modulo n (Section 1.5):
<ul>
<li>Discussion on the integers modulo n and their properties, setting
the stage for understanding groups.</li>
</ul></li>
<li>Introduction to Software (Section 1.6):
<ul>
<li>Brief mention of computational tools that can be used in group
theory, preparing students for integrating technology into mathematical
problem-solving.</li>
</ul></li>
</ol>
<p>This text discusses the concept of permutations and permutation
groups, focusing on their properties, notation, and generation. Here’s a
detailed summary:</p>
<ol type="1">
<li><p><strong>Permutations</strong>: A permutation is a bijective
(one-to-one and onto) mapping of a set to itself. The number of
permutations in S_n (the set of all permutations of n elements) is n! (n
factorial). Permutations can be represented using mapping notation or
cycle notation, which lists the images of each element under the
permutation in a cyclic order.</p></li>
<li><p><strong>Cycles</strong>: An r-cycle is a special type of
permutation that maps the first number to the second, the second to the
third, and so on, until the r-th number maps back to the first, while
leaving all other elements fixed. Any permutation can be written as a
product (composition) of disjoint cycles, and any cycle can be expressed
as a product of transpositions (2-cycles).</p></li>
<li><p><strong>Sign of a Permutation</strong>: The sign of a permutation
is defined based on its determinant when represented as a matrix
operation. A permutation is even if it’s the product of an even number
of transpositions and odd otherwise. Even permutations commute with each
other, as do odd ones.</p></li>
<li><p><strong>Permutation Groups</strong>: A permutation group G is a
non-empty subset of Sn (the set of all permutations of n elements)
that’s closed under multiplication (composition) and inverses. The
symmetric group Sn itself is a permutation group, known as the full or
symmetric group. Other examples include alternating groups An (even
permutations), cyclic groups generated by a single permutation, and
groups generated by a set of permutations.</p></li>
<li><p><strong>Cyclic Groups</strong>: Cyclic groups are special types
of permutation groups generated by a single permutation α. The order of
α (the smallest positive integer r such that α^r = identity) equals the
number of elements in the group ⟨α⟩ generated by α.</p></li>
<li><p><strong>Generators</strong>: A subset g of permutations generates
a permutation group G if every element in G can be written as a product
of elements from g. The smallest permutation group containing g is
called the group generated by g. This group consists of all products of
finitely many elements from g, not just finite products.</p></li>
<li><p><strong>Software and Calculations</strong>: The ‘Groups.m’
package provides tools for calculations in permutation groups using
Mathematica. Permutations are represented as lists of their images, with
a specific header (M) for the package to interpret them correctly.
Various operations and properties can be computed using this
software.</p></li>
</ol>
<p>The provided text discusses linear groups, which are sets of
invertible matrices with specific algebraic properties. These groups are
defined over a field F, and they must satisfy two conditions: (i) if α,
β ∈ G, then αβ ∈ G; (ii) if α ∈ G, then α^-1 ∈ G. The general linear
group GL(n, F) is an example of a linear group, where F can be any field
and n represents the size of the matrices.</p>
<p>The special linear group SL(n, F) is another important linear group,
consisting of all n × n matrices with determinant 1, also defined over
field F. The order of a linear group G (denoted as |G|) is the number of
elements in G if it’s finite; otherwise, it’s denoted as ∞.</p>
<p>A key aspect of linear groups is the concept of generators, which are
similar to permutations. For an element α in GL(n, F), its order |α| is
defined as the smallest integer n such that α^n = I (the identity
matrix). If no such n exists, we say that α has infinite order.</p>
<p>A cyclic linear group generated by α is {α^b | b ∈ Z}, which means it
consists of all powers of α, including negative ones if its order is
infinite. A linear group G is said to be generated by a subset g ⊂ G if
every element in G can be expressed as a product of elements from g and
their inverses.</p>
<p>The text provides several examples of linear groups over different
fields: 1. T = {(1 b 0 1) | b ∈ F}, where there’s a one-to-one
correspondence between F and T, under matrix multiplication. 2. N(p) =
{(a b 0 d) | a, b, d ∈ F_p, ad ≠ 0}, upper triangular matrices in GL(2,
F_p). 3. G(p) = {(a b br a) | a, b ∈ F_p, a^2 - b^2r ≠ 0}, where r is
not a square in F_p. 4. GL(2, F_p), the general linear group over finite
field F_p, with order (p-1)^2<em>p</em>(p+1).</p>
<p>The package ‘Groups.m’ allows for calculations involving linear
groups over a finite field F_p. It uses a chosen prime p, and matrices
are represented differently to account for modulo arithmetic. Functions
similar to those used in permutation groups (like Inverse, .) apply here
as well. The package provides a mechanism for reducing calculations
modulo the chosen prime at each step.</p>
<p>The text concludes by introducing an algorithm for expressing a
matrix in GL(2, F) using a set of generators. It uses row and column
operations to transform a given matrix into a product of these generator
matrices. This process is applicable over any field F, but the specific
number of required generators might vary depending on whether F is
finite or infinite.</p>
<p>The text discusses several concepts related to group theory, a
fundamental area of abstract algebra. Here’s a detailed summary and
explanation of the key points:</p>
<ol type="1">
<li><p><strong>Group</strong>: A set G with a binary operation
(multiplication) that is associative, has an identity element, and every
element has an inverse. Examples include permutation groups, linear
groups, integers under addition, non-zero elements in a field under
multiplication, and more.</p></li>
<li><p><strong>Homomorphism</strong>: A mapping between two groups
preserving the group operation. For example, the mapping from
permutations to their corresponding permutation matrices is a
homomorphism, as is the determinant map from GL(n, F) to F^x (the
multiplicative group of non-zero elements in field F).</p></li>
<li><p><strong>Isomorphism</strong>: A bijective homomorphism; it’s like
an equality sign for groups. If there exists an isomorphism between two
groups G and H, we say they are isomorphic, written G ≅ H. For instance,
the exponential mapping exp: R → S (where S is the positive real numbers
under multiplication) is an isomorphism.</p></li>
<li><p><strong>Direct Product</strong>: Given two groups G and H, their
direct product G × H is a group with operation defined component-wise.
The projections onto each factor are homomorphisms. An example is R^2
with vector addition.</p></li>
<li><p><strong>Subgroups</strong>: Non-empty subsets of a group that
themselves form a group under the same operation. Conditions include
closure under the operation and inverses being within the subset. Cyclic
subgroups are generated by a single element, and any subgroup can be
generated by a set of elements (called generators).</p></li>
<li><p><strong>Orthogonal Groups</strong>: Subgroups of linear groups
consisting of orthogonal matrices – those preserving the Euclidean
distance. Examples include SO(n), the special orthogonal group (matrices
with determinant 1), and O(n), the orthogonal group (including both
oriented and unoriented rotations).</p></li>
<li><p><strong>Cyclic Subgroups and Generators</strong>: If G is a group
and α ∈G, then ⟨α⟩ = {α^n | n ∈Z} is the cyclic subgroup generated by α.
A group is cyclic if it equals such a subgroup. Infinite cyclic groups
are isomorphic to Z (integers), while finite ones of order n are
isomorphic to Z/nZ.</p></li>
<li><p><strong>Subgroups Generated by Sets</strong>: Given a subset g ⊂
G, the subgroup ⟨g⟩ consists of all elements expressible as products of
elements from g and their inverses, following specific rules. For GL(n,
Z), SL(n, Z) is a significant example: matrices with determinant
1.</p></li>
</ol>
<p>The text also includes several exercises to deepen understanding of
these concepts, such as proving properties of cyclic groups,
demonstrating that certain mappings are homomorphisms, and exploring the
structure of specific groups like braid groups and orthogonal
groups.</p>
<p>The text discusses several topics related to group theory, symmetry
groups, and group actions. Here’s a detailed summary of each
section:</p>
<ol type="1">
<li><strong>Symmetries of Regular Polygons</strong>:
<ul>
<li>The symmetry group Sym(X) of an object X in Euclidean n-space is the
set of all isometries that map X to itself.</li>
<li>For regular polygons, symmetries include rotations and
reflections.</li>
<li>For P3 (equilateral triangle), there are 6 symmetries: 3 rotations
(2π/3, 4π/3, 2π) and 3 reflections. This group is isomorphic to S3.</li>
<li>For P4 (square), there are 8 rotations (π/2, π, 3π/2, 2π) and 4
reflections, totaling 12 symmetries. The group D4, isomorphic to
Sym(P4).</li>
</ul></li>
<li><strong>Symmetries of Platonic Solids</strong>:
<ul>
<li>Proper symmetries (isometries in SO(3)) are considered for Platonic
solids.</li>
<li>For the tetrahedron, there are 12 proper symmetries, forming the
alternating group A4.</li>
<li>The cube and its dual, the octahedron, have 24 proper symmetries,
forming the group O (or SO(3) × {±1}).</li>
<li>The dodecahedron and its dual, the icosahedron, have 60 proper
symmetries, forming the group A5.</li>
</ul></li>
<li><strong>Improper Symmetries</strong>:
<ul>
<li>Improper symmetries are isometries with det = -1 (e.g.,
reflections).</li>
<li>For the cube, there are 9 reflections and 15 rotatory reflections,
totaling 24 improper symmetries.</li>
</ul></li>
<li><strong>Symmetries of Equations</strong>:
<ul>
<li>The symmetry group of a polynomial equation is its Galois group,
reflecting symmetries among roots.</li>
<li>Examples are given for specific equations (x^4 + x^3 + x^2 + x + 1
and x^4 - 2x^2 - 2 = 0) to illustrate how the symmetry group is
determined by preserving algebraic relations among roots.</li>
</ul></li>
<li><strong>Group Actions</strong>:
<ul>
<li>Definition: A group G acts on a set X if there’s a mapping G × X → X
such that (αβ)·x = α·(β·x) and 1·x = x for all α, β ∈G and x ∈X.</li>
<li>Examples: G acting on itself by multiplication or conjugation are
provided.</li>
</ul></li>
<li><strong>Orbits and Stabilizers</strong>:
<ul>
<li>The orbit of a point x ∈ X is the set {α·x | α ∈ G}.</li>
<li>The stabilizer of x is the subgroup {α ∈G | α·x = x}.</li>
<li>Example: For G = V’ (a subgroup of S4) acting on X = {1, 2, 3, 4},
orbits and stabilizers are computed.</li>
</ul></li>
<li><strong>Conjugacy Classes</strong>:
<ul>
<li>In a group action by conjugation, the orbit of an element ξ ∈ G is
called its conjugacy class (Cξ).</li>
<li>The stabilizer of ξ, also known as the centralizer Zξ(G), consists
of elements commuting with ξ.</li>
<li>Conjugacy classes in Sn are defined by cycle types; for example,
possible cycle types in S5 are {1}, {2}, …, {5}.</li>
</ul></li>
<li><strong>Fractional Linear Transformations</strong>:
<ul>
<li>GL(2, F) (2x2 invertible matrices over a field F) acts on the
projective line P^1(F).</li>
<li>This action is defined by [a b; c d] · [x : y] = [ax + by : cx +
dy].</li>
<li>This action will be further explored in chapter 10 and will play a
significant role in chapter 12.</li>
</ul></li>
</ol>
<p>The text provides a foundation for understanding group actions,
symmetry groups, and their applications to various mathematical objects
like polygons, Platonic solids, and polynomial equations. It also
introduces the concept of conjugacy classes and sets the stage for
further exploration of fractional linear transformations in subsequent
chapters.</p>
<p>Summary of Key Points and Explanation from Section 9.4 on Finite
Subgroups of SO(3):</p>
<p>This section explores the classification of finite subgroups within
the group of rotations in three dimensions, denoted as SO(3). The goal
is to determine which types of groups can be finite subgroups of
SO(3).</p>
<ol type="1">
<li><p><strong>Action on S2</strong>: Any group G acting on S2 (the unit
sphere) induces a set X of fixed points, where each point x ∈ X
represents a pair of antipodal points that are invariant under the
action of some non-identity element α ∈ G.</p></li>
<li><p><strong>Orbit Decomposition</strong>: The orbits O1, …, Os of G’s
action on X divide S2 into regions with equivalent symmetry properties.
Each orbit Oj consists of nj pairs of antipodal points, where |G|/nj =
|Stab(x)| for any x ∈ Oj (a property from equation 9.1).</p></li>
<li><p><strong>Burnside’s Formula Application</strong>: Using Burnside’s
counting lemma, we establish the relationship between the number of
orbits s and the sizes nj of these orbits:</p>
<p>Sum_j=1^s (1/nj) = s - 2 + 2|G|</p></li>
<li><p><strong>Inequality Analysis</strong>: By analyzing this equation,
we deduce that s (the number of orbits) must be less than or equal to 3
due to the inequality derived from the terms’ non-negativity and upper
limit of 1/2:</p>
<p>2 &gt; s / 2 ⇒ s &lt; 4</p></li>
<li><p><strong>Case Analysis</strong>: Given s ≤ 3, we consider three
cases:</p>
<ul>
<li><ol type="i">
<li>If s = 1, then there is only one orbit O1 containing all fixed
points. This implies that G must act transitively and regularly on S2,
meaning it must be either cyclic (G ≅ Cn), dihedral (Dn), the full group
of rotations (T), or the identity group (O).</li>
</ol></li>
<li><ol start="2" type="i">
<li>If s = 2, then there are two distinct orbits O1 and O2. The
inequality suggests that n1 and n2 must both be equal to 2. In this
scenario, G’s structure is more complex, involving subgroups of SO(3)
that contain rotations through different axes (like axis-reflection
pairs).</li>
</ol></li>
<li><ol start="3" type="i">
<li>If s = 3, it implies three orbits O1, O2, and O3 with nj ≥ 2. The
specific group structures in this case are yet to be fully determined
but may involve more complicated subgroup configurations within
SO(3).</li>
</ol></li>
</ul></li>
</ol>
<p>In conclusion, Section 9.4 demonstrates that finite subgroups of
SO(3) can be classified into cyclic groups (Cn), dihedral groups (Dn),
the full group of rotations (T), or the identity group (O). The method
involves studying the fixed points under these group actions and
applying Burnside’s counting lemma to derive essential constraints on
possible orbit structures. Further exploration is needed for a complete
classification in the case where s = 3.</p>
<p>The Sylow Theorems are fundamental results in group theory that
provide crucial information about the structure of finite groups,
especially those with a large order. These theorems were developed by
Ludwig Sylow in 1872 and consist of three main statements regarding
Sylow p-subgroups (p being a prime divisor of the order of the
group).</p>
<ol type="1">
<li><p><strong>Existence Theorem</strong>: For every finite group G, for
each prime p dividing |G|, there exists at least one subgroup H ⊂ G
whose order is the highest power of p that divides |G|. Such a subgroup
H is called a Sylow p-subgroup.</p>
<p>Proof: Let n = |G| and write n = apr where (a, p) = 1. The number of
subsets T with pr elements in G is given by (n choose pr), i.e., |X| =
(n choose pr). G acts on X by (α, T) → αT. This action decomposes X into
disjoint orbits. By Lemma 11.2, p does not divide |X|, so one of these
orbits has order a, meaning it consists of right cosets of Sylow
p-subgroups.</p></li>
<li><p><strong>Congruence Theorem</strong>: If np denotes the number of
Sylow p-subgroups in G, then np ≡ 1 (mod p).</p>
<p>Proof: As mentioned earlier, each orbit of length a contains elements
from right cosets of different Sylow p-subgroups. Since p does not
divide |G/H| where H is any Sylow p-subgroup, there must be an orbit
whose order isn’t divisible by p. By applying the formula (9.2), we
deduce that np ≡ 1 (mod p).</p></li>
<li><p><strong>Conjugacy Theorem</strong>: All Sylow p-subgroups of G
are conjugate to each other.</p>
<p>Proof: Let H be a Sylow p-subgroup, and consider the set of left
cosets G/H. The group K acts on this set by left multiplication,
resulting in disjoint orbits. Since p does not divide |G/H|, there
exists an orbit whose order is not divisible by p. For any α ∈ G, if αH
belongs to such an orbit, then for every κ ∈ K, α−1κα ∈ H, implying that
α−1Kα ⊂ H and ultimately α−1Kα = H. Hence, K is conjugate to H via the
element α.</p></li>
</ol>
<p>These Sylow Theorems have significant implications for understanding
finite groups:</p>
<ul>
<li>They help in identifying important subgroups of a given group, which
can simplify the task of determining its structure.</li>
<li>In combination with other results (e.g., Lagrange’s Theorem), they
enable us to determine the order and structure of various subgroups
within a larger group.</li>
<li>Sylow p-subgroups play an essential role in classifying finite
simple groups, which are fundamental building blocks in group
theory.</li>
</ul>
<p>The normalizer NG(H) (the largest subgroup of G containing H such
that every element of NG(H) commutes with H) is also crucial in Sylow
theory: The number of conjugates of a Sylow p-subgroup H is given by
|G|/|NG(H)|, and this quantity divides the index a = |G|/|H|.</p>
<p>The text discusses several key concepts related to Abelian groups,
specifically focusing on free abelian groups and their structure. Here’s
a summary and explanation of the main points:</p>
<ol type="1">
<li><p>Free Abelian Groups: A group G is finitely generated if there
exists a finite subset g ⊂G such that G = ⟨g⟩. In an abelian group, a
basis of G is a set of generators {α₁, …, αₙ} with no non-trivial
relations among them (i.e., a₁α₁ + … + aᵢαᵢ = 0 implies a₁ = … = aᵢ =
0). If such a basis exists, the group G is called free abelian, and its
rank is the number of elements in the basis.</p></li>
<li><p>Torsion Subgroup: For any abelian group G, define Gₖ as {α ∈G |
nα = 0 for some n ∈Z}. This subgroup Gₖ is called the torsion subgroup
of G. Finite Abelian groups are not free because they have non-trivial
elements that satisfy nα = 0 for a fixed n &gt; 1.</p></li>
<li><p>Rank and Classification: Every finitely generated abelian group G
has a decomposition into its free part (Gₖ) and torsion part (Gₖ). The
rank of G is the number of elements in a basis of the free part Gₖ. This
rank determines essential properties of the group, such as isomorphism
classes.</p></li>
<li><p>Kernel and Homomorphisms: For an abelian group G with generators
{α₁, …, αₙ} and a homomorphism f : Zⁿ →G given by (a₁, …, aₙ) ↦ aᵢαᵢ,
the kernel of f is a subgroup of Zⁿ. By analyzing this kernel using row
and column reduction of integer matrices, one can obtain a basis for the
kernel in terms of the original generators {α₁, …, αₙ}.</p></li>
<li><p>Row and Column Reduction: Given an n × m integer matrix A =
(aij), row and column operations are applied to diagonalize A using only
integral operations. This process involves multiplying rows/columns by
-1, swapping rows/columns, and adding a multiple of one row/column to
another. The resulting diagonalized matrix B reveals the rank and
structure of the original subgroup H in G.</p></li>
</ol>
<p>In essence, this text lays the foundation for understanding the
structure of finite Abelian groups by introducing free Abelian groups,
their bases, torsion subgroups, and the classification theorem that
connects these concepts. Row and column reduction of integer matrices is
presented as a tool to analyze subgroups within free Abelian groups,
providing insight into their structure and rank.</p>
<p>This text discusses polynomial rings, focusing on properties of
polynomials with coefficients in an arbitrary field F. Key points
include:</p>
<ol type="1">
<li><p>Polynomial definition: A polynomial f(x) in F[x] is expressed as
amxm + … + a1x + a0, where an ∈F and m ≥ 0. The degree of the polynomial
(deg f) is determined by the highest monomial’s coefficient with a
non-zero value.</p></li>
<li><p>Polynomial operations: Polynomials can be added and multiplied
similarly to integers. For two polynomials f and g, their degrees
satisfy deg(fg) = deg f + deg g.</p></li>
<li><p>Long division for polynomials (Theorem 14.1): Given non-zero
polynomials f and g in F[x], there exist unique q and r in F[x] such
that g = qf + r, where deg r &lt; deg f. This theorem is proved using
mathematical induction on n - m, with n and m being the degrees of g and
f, respectively.</p></li>
<li><p>Common divisors: A polynomial f divides another polynomial g
(denoted by f | g) if g = qf for some q ∈F[x]. The leading coefficient
determines whether a non-zero scalar divides any given polynomial in
F[x].</p></li>
<li><p>Greatest common divisor (GCD): For two polynomials f and g, their
greatest common divisor is defined as the monic (leading coefficient of
1) common divisor with the highest degree. It can be found using the
Euclidean algorithm similar to integers, which involves successive
division steps reducing the remainder’s degree until reaching
zero.</p></li>
</ol>
<p>These properties allow for a rich algebraic structure in F[x],
resembling that of the integers Z, enabling further study of polynomial
equations and their symmetries.</p>
<p>The provided text describes the Berlekamp algorithm for factoring
polynomials over a finite field F_p. Here’s a detailed explanation:</p>
<ol type="1">
<li><p><strong>Solving the congruence:</strong> The first step is to
solve the congruence g^p - g ≡ 0 (mod f), where f is the given
polynomial in F_p[x]. This congruence has solutions in the quotient ring
F_p[x]/(f).</p></li>
<li><p><strong>Finding a basis of solutions:</strong> Let’s denote the
solutions as {g_1, g_2, …, g_r}. These form a basis for the solution
space W of the congruence. The dimension r of this space is crucial
because it tells us how many relatively prime factors the original
polynomial f has.</p></li>
<li><p><strong>Determining coefficients (a_i):</strong> For each
solution g_k, find all a in F_p such that (g_k - a, f) ≠ 1. This means
that f and g_k - a share a common factor other than 1. The set of all
such a for each k corresponds to the factors of f.</p></li>
<li><p><strong>Identifying irreducible factors:</strong> If there are r
distinct a values for some k, then we have found r relatively prime
factors of f. If not, repeat the process with the next polynomial g
obtained by solving the congruence (g - a, f) ≠ 0 (mod f).</p></li>
<li><p><strong>Deriving irreducible polynomials:</strong> Once you’ve
identified the factors q_i = p^m_i i, where i is irreducible, determine
i as follows:</p>
<ul>
<li>If the derivative q’_i ≠ 0, then i = q_i / (q_i, q’_i).</li>
<li>If q’_i = 0, then q_i(x) = ˜q_i(x^p), where ˜q_i is irreducible in
F_p[x^p]. In this case, find its derivative and repeat the process.</li>
</ul></li>
</ol>
<p>The algorithm continues until all factors are found, and it
terminates because each step reduces the degree of the polynomial being
factored.</p>
<p>This algorithm is efficient for factoring polynomials over finite
fields, especially when compared to brute-force methods that check every
possible factor. It leverages properties of finite fields and polynomial
congruences to systematically identify irreducible factors.</p>
<p>The text discusses the concept of extension fields in algebra,
focusing on simple extensions. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Extension Fields</strong>: Let E and F be fields with E
containing F (i.e., E ⊃ F). In this context, E is called an extension
field of F, and F is considered a subfield of E. The notation E/F refers
to the extension of F by E. Examples include Q(√2) and Q(ω), which are
extensions of the rational numbers Q and are subfields of the complex
numbers C.</p></li>
<li><p><strong>Simple Extensions</strong>: A special type of extension
is a simple extension, where there exists an element ζ ∈ E such that E =
F(ζ). This means E is generated by adjoining ζ to F. The set F(ζ)
consists of all rational expressions in ζ with coefficients from
F.</p></li>
<li><p><strong>Theorem on Simple Extensions (16.3)</strong>: For a
simple extension E = F(ζ), the following two cases are possible:</p>
<ul>
<li><p><strong>Case 1</strong>: There exists an irreducible polynomial f
∈ F[x] such that F(ζ) is isomorphic to F[x]/(f). This means that
adjoining ζ to F effectively “splits” the polynomial f into linear
factors in the field extension. In other words, the minimal polynomial
of ζ over F is irreducible and generates the field extension.</p></li>
<li><p><strong>Case 2</strong>: F(ζ) is isomorphic to the field of
rational functions F(x). This occurs when ζ is transcendental over F,
meaning it is not a root of any non-zero polynomial with coefficients in
F. In this case, adjoining ζ to F results in a larger field that
contains all possible rational expressions in ζ.</p></li>
</ul></li>
<li><p><strong>Proof of Theorem 16.3</strong>:</p>
<ul>
<li><p>The theorem states that for E = F(ζ), either E is isomorphic to
F[x]/(f) for some irreducible polynomial f ∈ F[x], or E is isomorphic to
F(x).</p></li>
<li><p>To prove this, consider the evaluation map φ: F[x] → E defined by
φ(g) = g(ζ). This map sends a polynomial g to its value at ζ. The kernel
of this map consists of all polynomials in F[x] that have ζ as a root,
i.e., the ideal (f) generated by the minimal polynomial f of ζ over
F.</p></li>
<li><p>By the First Isomorphism Theorem for rings, F[x]/(f) is
isomorphic to the image of φ, which is F(ζ). Thus, F(ζ) ≅ F[x]/(f),
where f is irreducible because we chose it to be minimal.</p></li>
<li><p>If no such irreducible polynomial f exists (i.e., ζ is
transcendental over F), then the image of φ is all of E, and F(ζ) ≅
F(x).</p></li>
</ul></li>
</ol>
<p>This theorem provides a clear characterization of simple extensions,
making them easier to understand and work with in algebraic structures.
It shows that every simple extension either “splits” by adjoining a root
of an irreducible polynomial or remains as a larger field containing
rational expressions in the adjoined element.</p>
<p>The provided text discusses several concepts in abstract algebra,
specifically focusing on field extensions and cyclotomic polynomials.
Here’s a detailed summary of the key points:</p>
<ol type="1">
<li><p><strong>Field Extensions</strong>: A field extension E/F is a
pair of fields where F is a subfield of E. The notation F(ζ) means the
smallest subfield of E containing both F and ζ. If f(x) ∈ F[x] is a
polynomial, the field F(ζ) for some root ζ of f can be described as a
simple extension:</p>
<ul>
<li>If f(ζ) = 0, then ¯ϵ_ζ : F[x]/(f) → F(ζ) is an isomorphism.</li>
<li>If f(ζ) ≠ 0 for all ζ and f ∈ F[x], the map ϵ_ζ : F[x] → F(ζ),
defined by ϵ_ζ(g/h) = g(ζ)/h(ζ), is a homomorphism, and its image is an
extension of F containing ζ.</li>
</ul></li>
<li><p><strong>Algebraic and Transcendental Elements</strong>: An
element ζ ∈ E is algebraic over F if there exists a non-zero polynomial
f(x) ∈ F[x] such that f(ζ) = 0. If no such f exists, ζ is transcendental
over F. Algebraic numbers are those that are algebraic over Q (the
rational numbers), and transcendental numbers are not algebraic over
Q.</p></li>
<li><p><strong>Splitting Fields</strong>: Given a polynomial f(x) ∈
F[x], the splitting field of f is the smallest extension E/F such that f
splits into linear factors in E[x]. The degree [E : F] of an extension
is defined as the dimension of E as an F-vector space.</p></li>
<li><p><strong>Cyclotomic Polynomials</strong>: For a positive integer
n, the nth cyclotomic polynomial Φ_n(x) is the monic polynomial whose
roots are the primitive nth roots of unity. It’s defined recursively
using the formula x^n - 1 = ∏_d|n Φ_d(x). Cyclotomic polynomials have
integer coefficients and are irreducible over Q.</p></li>
<li><p><strong>Finite Fields</strong>: If F is a field of characteristic
p, then for any positive integer r, there exists a unique finite field
F_pr (called the Galois field of order pr) with pr elements. The number
of monic, irreducible polynomials in F_p[x] of degree r is denoted by
N(p, r), and they satisfy the relation ps = ∑_{r|s} N(p, r)r.</p></li>
</ol>
<p>The text also includes examples and plots to illustrate these
concepts: - The behavior of cubic polynomials based on their
discriminant. - A demonstration showing how the discriminant controls
the number of real roots for a family of real cubics. - Plots of
semi-cubical parabolas, illustrating regions where the discriminant is
positive or negative, corresponding to different root behaviors.</p>
<p>The Galois Correspondence is a fundamental theorem in Galois Theory
that establishes a one-to-one correspondence between subgroups of the
Galois group (Gal(E/F)) and intermediate fields (subfields K such that F
⊆ K ⊆ E). This relationship provides crucial insights into understanding
field extensions.</p>
<ol type="1">
<li><p><strong>Subgroup H &lt; Gal(E/F) implies Intermediate Field
Fix(H):</strong> Given a subgroup H of the Galois group, Fix(H) = {a ∈ E
| α(a) = a for all α ∈ H} is an intermediate field containing F. This
means that Fix(H) is a subset of E that forms a field and contains the
base field F.</p></li>
<li><p><strong>Intermediate Field K implies Subgroup Gal(E/K):</strong>
For any intermediate field K (F ⊆ K ⊆ E), the set Gal(E/K) = {α ∈
Gal(E/F) | α(a) = a for all a ∈ K} is a subgroup of Gal(E/F). In other
words, it’s the collection of automorphisms in the Galois group that
preserve elements in K.</p></li>
</ol>
<p>The key to understanding these relationships lies in Theorem
17.11:</p>
<p><strong>Theorem 17.11:</strong> Let E be a field and G a finite group
of automorphisms of E. Set F = Fix(G). Then [E : F] ≤ |G|.</p>
<p>This theorem essentially says that the degree of the extension (the
dimension of E as a vector space over F) is less than or equal to the
order of the group of automorphisms G. The proof involves showing that
any n elements in E are linearly dependent over F when n exceeds the
order of the group |G|.</p>
<p>The proof proceeds by considering an arbitrary set of n elements (ζ1,
…, ζn) ∈ E and constructing a system of equations based on these
elements under the action of each automorphism in G. By leveraging the
properties of groups and linear algebra, it is demonstrated that this
system of equations must have a non-trivial solution lying in F if n
&gt; |G|, implying the linear dependence over F.</p>
<p>This theorem, along with its corollaries (17.6 and 17.7), forms the
foundation of the Galois Correspondence. It asserts that for each
subgroup H of Gal(E/F), there exists an intermediate field Fix(H) = {a ∈
E | α(a) = a for all α ∈ H}, and conversely, for every intermediate
field K (with F ⊆ K ⊆ E), there is a corresponding subgroup Gal(E/K).
This one-to-one correspondence uncovers deep connections between the
algebraic structure of fields and their symmetry groups.</p>
<p>The text discusses the Galois theory of polynomials, focusing on
quartics (degree 4 polynomials). Here’s a summary and explanation of key
concepts and results:</p>
<ol type="1">
<li><p><strong>Galois Groups of Quartics</strong>: The text describes
how to determine the Galois group of an irreducible quartic polynomial
by examining its discriminant δ and the cubic resolvent r(x). Based on
whether δ is a square in F (the base field) or not, the Galois group can
be identified as:</p>
<ul>
<li>A4 (Alternating group): If δ ∈ F.</li>
<li>S4 (Symmetric group): If δ ̸∈ F and r(x) is irreducible.</li>
<li>V4 (Klein four-group): If δ ̸∈ F and r(x) is reducible, splitting
into a linear factor and an irreducible quadratic.</li>
<li>D4 or C4: If δ ̸∈ F and r(x) is reducible, splitting into two
irreducible quadratics.</li>
</ul></li>
<li><p><strong>Cubic Resolvent</strong>: The cubic resolvent of a
quartic f(x) = x^4 + b_2 x^2 + b_1 x + b_0 is defined as:</p>
<p>r(x) = x^3 - 2b_2 x^2 + (b_2^2 - 4b_0)x + b_2^1</p>
<p>Its discriminant, δr, is equal to the discriminant of f.</p></li>
<li><p><strong>Geometry of the Cubic Resolvent</strong>: The cubic
resolvent can be interpreted geometrically using conics in C^2 (complex
2-dimensional space). By substituting x^2 = y and considering the
intersection of two conics, one can determine the Galois group based on
the degeneracy of these conics.</p></li>
<li><p><strong>Software</strong>: The text mentions a Mathematica
package called ‘Quartics’ that computes the cubic resolvent and
determines the Galois group according to the criteria outlined in the
text.</p></li>
<li><p><strong>General Equation of the nth Degree (Theorem
19.1)</strong>: This section introduces a theorem stating that, for an
irreducible polynomial f(x) ∈ Q[x] of degree p (where p is prime), if f
has exactly two non-real roots, then its Galois group Gal(f) is
isomorphic to the symmetric group S_p. The proof uses properties of
Sylow subgroups and transitivity of the Galois group.</p></li>
<li><p><strong>Examples</strong>: Several examples of irreducible
polynomials with specific Galois groups are given, including:</p>
<ul>
<li>A cubic with negative discriminant (∆ &lt; 0) has Galois group
S_3.</li>
<li>f(x) = x^5 − 6x + 2 ∈ Q[x] has Galois group S_5.</li>
</ul></li>
<li><p><strong>Symmetric Functions</strong>: The text briefly mentions
how symmetric functions can sometimes help determine the Galois group of
a polynomial over Q, particularly when reductions modulo primes reveal
cycle types of elements in the Galois group.</p></li>
</ol>
<p>In summary, this text presents methods for determining Galois groups
of quartics using discriminants and cubic resolvents, as well as a
general result (Theorem 19.1) about the Galois group of irreducible
polynomials with specific root properties. It also touches on geometric
interpretations and computational tools for analyzing these groups.</p>
<p>The provided text discusses the concept of solving polynomial
equations by radicals, a method that involves expressing the roots of an
equation using arithmetic operations (addition, subtraction,
multiplication, division) and nth roots. The main focus is on cubic
equations, but the topic also extends to higher-degree polynomials.</p>
<ol type="1">
<li><p><strong>Formulas for a Cubic</strong>: Cardano’s formulas provide
a way to find the roots of a cubic equation in the form f(x) = x^3 +
a_1x + a_0, assuming a_1 ≠ 0. These formulas involve Lagrange resolvents
ξ2 and ξ3, which are expressed in terms of the coefficients a_0 and a_1,
and cube roots of expressions involving these coefficients. The roots
themselves can then be recovered using linear combinations of ξ2 and
ξ3.</p></li>
<li><p><strong>Cyclic Extensions</strong>: A cyclic extension is an
algebraic field extension E/F where Gal(E/F), the Galois group, is a
cyclic group. In characteristic 0, if F contains all nth roots of unity,
then the splitting field of x^n - a over F is a cyclic extension, and
can be written as F(n√a) for some a ∈ F. The proof involves constructing
a Lagrange resolvent ξ such that ξ^n ∈ F and E = F(ξ).</p></li>
<li><p><strong>Solution by Radicals in Higher Degrees</strong>: The
question of whether formulas similar to Cardano’s can be derived for
higher-degree polynomials is explored. It turns out that while this is
possible for quartics, it cannot be done for equations of degree 5 or
higher due to the nature of their Galois groups. Specifically, if the
Galois group is Sn (the symmetric group on n elements), then the
equation is not solvable by radicals when n ≥ 5.</p></li>
<li><p><strong>Galois Theory and Radical Solvability</strong>: The text
also touches upon the connection between a polynomial’s Galois group and
its solvability by radicals. It’s shown that if the Galois group is Sn
for n ≥ 5, then the polynomial cannot be solved by taking radicals. This
is established using the simplicity of An (the alternating group on n
elements) when n &gt; 4, and properties of normal subgroups in simple
groups.</p></li>
<li><p><strong>Mathematica Calculations</strong>: The text includes
examples using Mathematica’s Solve function to find roots of cubic and
quartic equations, demonstrating a more reliable alternative to
Cardano’s formulas that can handle complex numbers and other edge
cases.</p></li>
</ol>
<p>In summary, the text explores the methodology for solving cubic
polynomials via radicals, discusses cyclic extensions in the context of
field theory, and presents results from Galois theory showing why
higher-degree equations generally cannot be solved by radicals. It also
provides a computational aspect through Mathematica examples,
illustrating the practical side of these theoretical concepts.</p>
<p>The document provides an overview of ruler-and-compass constructions
and their relationship to algebraic problems involving polynomial
equations. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Algebraic Interpretation</strong>: The text introduces
the idea that geometric constructions can be translated into algebraic
problems, particularly in solving polynomial equations. For instance,
trisecting an angle τ is equivalent to finding a solution for the cubic
equation 4x³ - 3x - cos(τ) = 0.</p></li>
<li><p><strong>Field of Constructible Lengths (L)</strong>: The set L
consists of all lengths that can be constructed using ruler and compass,
given some initial points and lengths. It’s shown to be a field
containing Q(a₁, …, ar), where a₁, …, ar are the given real numbers.
Moreover, if b ∈ L, then √b ∈ L.</p></li>
<li><p><strong>Constructibility</strong>: A real number ζ is
constructible using ruler and compass if and only if it lies in an
extension E/Q(a₁, …, ar) that can be built up as a sequence of quadratic
extensions. This means [Q(a₁, …, ar, ζ) : Q(a₁, …, ar)] is a power of
2.</p></li>
<li><p><strong>Regular Polygons</strong>: The constructibility of
regular polygons is discussed. If Pn is a regular n-gon that can be
constructed, then φ(n), the Euler’s totient function, must be a power of
2. Conversely, if φ(n) is a power of 2, then Pn is
constructible.</p></li>
<li><p><strong>Periods</strong>: The concept of periods is introduced
for primitive pth roots of unity (ω), where p is prime. A period ωH is
defined as the sum of all elements in a subgroup H of Gal(Φp) applied to
ω. These periods play a crucial role in constructing cyclotomic fields
and regular polygons.</p></li>
<li><p><strong>Galois Theory</strong>: The text briefly mentions Galois
theory, which is used to study the solvability of polynomial equations
by radicals. It’s stated that if a polynomial is solvable by radicals,
its Galois group must be a solvable group.</p></li>
</ol>
<p>The exercises at the end of the document delve into specific problems
related to these concepts, such as proving irreducibility of certain
polynomials, constructing specific lengths using ruler and compass, and
finding minimal polynomials of trigonometric functions over Q.</p>
<p>The provided list consists of mathematical terms related to various
branches of algebra, number theory, and group theory. Here’s a detailed
explanation of some key concepts:</p>
<ol type="1">
<li><p><strong>Group</strong>: A group is a set equipped with an
operation that combines any two of its elements to form a third element
in such a way that four conditions called group axioms are satisfied.
These include closure, associativity, identity and invertibility.
Examples include the integers under addition or multiplication, where
the operations are associative, commutative (except for non-abelian
groups), have an identity element, and every element has an
inverse.</p></li>
<li><p><strong>Orthogonal Group</strong>: Specifically, the Orthogonal
Group O(n) is the group of n×n orthogonal matrices, which are square
matrices whose columns and rows are orthonormal vectors (meaning they
are unit vectors and orthogonal to each other). The determinant of such
a matrix can be ±1.</p></li>
<li><p><strong>Orthogonal Matrix</strong>: A square matrix Q with real
entries is orthogonal if its transpose is also its inverse, i.e., Q^T =
Q^(-1). This implies that the columns (and rows) form an orthonormal set
of vectors.</p></li>
<li><p><strong>Polynomial</strong>: A polynomial is a mathematical
expression involving a sum of powers in one or more variables multiplied
by coefficients. For example, 3x^2 + 2x - 5 is a polynomial in x. The
degree of the polynomial (highest power of the variable) plays an
important role in understanding its properties.</p></li>
<li><p><strong>Prime Field</strong>: In field theory, a prime field is
either the finite field with p elements, denoted as F_p, or the field of
rational numbers Q, depending on whether the characteristic is a prime
number p or 0, respectively.</p></li>
<li><p><strong>Projective Line</strong>: The projective line over a
field K (denoted as P^1(K)) is a geometric structure that generalizes
the concept of a line to include points at infinity. It can be thought
of as the set of all lines through the origin in the affine line over K,
along with a point at infinity for each direction.</p></li>
<li><p><strong>Ring</strong>: A ring is an algebraic structure
consisting of a set equipped with two binary operations usually called
addition and multiplication, where the set is an abelian group under
addition and associative under multiplication. Multiplication need not
be commutative. Examples include the integers (Z) and polynomials
(F[x]).</p></li>
<li><p><strong>Field</strong>: A field is a set on which addition,
subtraction, multiplication, and division are defined and behave as the
corresponding operations on rational and real numbers do. Every field
contains at least two numbers, 0 and 1, and every non-zero element has a
multiplicative inverse. Examples include the rational numbers (Q), real
numbers (R), and complex numbers (C).</p></li>
<li><p><strong>Symmetric Group</strong>: The symmetric group S_n on n
letters is the group whose elements are all the permutations of n items
and whose group operation is the composition of permutations. It’s a
fundamental example of a finite group.</p></li>
<li><p><strong>Polynomial Ring</strong>: In abstract algebra, a
polynomial ring in one variable over a commutative ring R is denoted by
R[x]. It consists of all polynomials in x with coefficients from
R.</p></li>
<li><p><strong>Separable Polynomial</strong>: A separable polynomial is
a nonconstant polynomial whose roots are distinct in its splitting
field. In characteristic 0 (like the rational numbers Q), every
irreducible polynomial is separable, but this isn’t true for fields of
positive characteristic.</p></li>
<li><p><strong>Simple Extension</strong>: If E is an extension field of
F and α is algebraic over F, then the set {β ∈ E | β is algebraic over
F(α)} forms a subfield of E called the simple transcendental extension
of F(α) over F, denoted by F(α).</p></li>
<li><p><strong>Sylow p-subgroup</strong>: In group theory, Sylow
p-subgroups are certain p-subgroups of a given finite group G; they are
named after Ludwig Sylow. The number of Sylow p-subgroups is congruent
to 1 modulo p and divides the order of G.</p></li>
<li><p><strong>Splitting Field</strong>: Given a field extension L/K, if
there exists a larger field E such that L is a subfield of E and the
minimal polynomial of every element of L splits into linear factors over
E, then E is called a splitting field of L/K.</p></li>
</ol>
<p>These terms are fundamental in understanding various aspects of
algebraic structures and their properties. They underpin many advanced
concepts in abstract algebra, number theory, and geometry.</p>
<h3 id="cmbook">cmbook</h3>
<p>Continued fractions are a fascinating representation of numbers,
expressing them as an “infinite” fraction where each term is itself a
fraction. The general form of a continued fraction is given by:</p>
<pre><code>a_0 + 1/(a_1 + 1/(a_2 + 1/(...)))</code></pre>
<p>Here’s a detailed explanation and example to illustrate their
concept:</p>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Basic Concept</strong>: A continued fraction represents a
number as the sum of an integer part followed by the reciprocal of
another fraction, which itself is composed similarly (i.e., having
another integer plus a reciprocal). This nesting can continue
indefinitely.</p></li>
<li><p><strong>Notation</strong>: The general form of a continued
fraction is written as:</p>
<p>[a_0 + ]</p>
<p>where (a_0, a_1, a_2, ) are integers.</p></li>
<li><p><strong>Representation</strong>: Each (a_n) represents the
numerator (or sometimes denominator) of the fraction in the nth
position. The overall value of the continued fraction is approached as
the number of terms increases.</p></li>
<li><p><strong>Convergence</strong>: For most continued fractions, the
sequence of partial sums converges to a specific real number or complex
number depending on whether (a_n) are real or complex.</p></li>
</ol>
<p><strong>Example: The Golden Ratio (φ)</strong>:</p>
<p>The golden ratio φ is famously represented by the continued
fraction:</p>
<pre><code>\[\phi = 1 + \frac{1}{1 + \frac{1}{1 + \frac{1}{1 + \dots}}}\]</code></pre>
<p>Let’s compute this for a few terms to see how it converges:</p>
<ul>
<li>( _0 = 1 )</li>
<li>( _1 = 1 + = 2 )</li>
<li>( _2 = 1 + = = 1.5 )</li>
<li>( _3 = 1 + = )</li>
<li>( _4 = 1 + )</li>
<li>( _5 = 1 + )</li>
<li>( _6 = 1 + )</li>
<li>( _7 = 1 + )</li>
<li>( _8 = 1 + )</li>
</ul>
<p>As we can see, the sequence of fractions (_n) approaches a limit
around 1.61803398875…, which is the golden ratio φ ≈ 1.61803398875
(rounded to 12 decimal places).</p>
<p><strong>Properties and Uses</strong>:</p>
<ul>
<li><p><strong>Irrational Numbers</strong>: Many irrational numbers,
like √2, e, and π, can be expressed as continued fractions, often
providing a way to approximate these constants.</p></li>
<li><p><strong>Algebraic Numbers</strong>: Even algebraic numbers (roots
of polynomials with integer coefficients) have continued fraction
representations.</p></li>
<li><p><strong>Approximation and Computation</strong>: Continued
fractions are useful for numerical approximations, allowing one to find
rational numbers that closely match irrational values, which can be
essential in computational mathematics and computer science
applications.</p></li>
</ul>
<p>Continued fractions bridge the gap between discrete sequences
(fractions) and continuous mathematical concepts (irrational numbers),
offering both theoretical insights and practical computational
tools.</p>
<ol type="1">
<li>Value of π Approximation using Recurrence Relation:</li>
</ol>
<p>The provided script implements a recurrence relation to approximate
the value of π based on the infinite series:</p>
<p>π ≈ 2 + ∑(1/(n-j)) for j = 0, …, n-1</p>
<p>Here’s a MATLAB implementation for this series approximation:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="va">pi_approx</span> <span class="op">=</span> <span class="va">pi_recurrence</span>(<span class="va">n</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">s</span> <span class="op">=</span> <span class="fl">1</span><span class="op">;</span> <span class="co">% Initializing the sum (s0)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">for</span> <span class="va">j</span> <span class="op">=</span> <span class="fl">0</span> <span class="op">:</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">1</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">term</span> <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> (<span class="va">j</span> <span class="op">+</span> <span class="fl">1</span>)<span class="op">;</span> <span class="co">% Calculating the next term of the series</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">s</span> <span class="op">=</span> <span class="va">s</span> <span class="op">+</span> <span class="va">term</span><span class="op">;</span> <span class="co">% Updating the sum (sj)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">pi_approx</span> <span class="op">=</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span><span class="op">/</span><span class="va">s</span><span class="op">;</span> <span class="co">% Final approximation of π</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>You can call this function with a desired value for <code>n</code> to
approximate π. For example, <code>pi_recurrence(100)</code> will compute
an approximation using the first 100 terms of the series.</p>
<ol start="2" type="1">
<li>Finite and Infinite Series:</li>
</ol>
<p>A finite series is a sum of a finite sequence of numbers, while an
infinite series is the sum of an infinite sequence. The provided
examples show how to approximate π using both finite (truncated) and
infinite series in MATLAB.</p>
<ol start="3" type="1">
<li>Principles of Programming: Arrays &amp; Functions:</li>
</ol>
<ul>
<li>Arrays are used for storing sequences or multiple variables with
integer length and indexed elements. In MATLAB, arrays can be created
and manipulated easily, allowing for efficient storage and manipulation
of data.</li>
<li>Functions in programming languages (including MATLAB) are ‘black
boxes’ that take inputs and produce outputs based on a set of
instructions. They allow modularizing code and improving reusability and
readability.</li>
</ul>
<ol start="4" type="1">
<li>Conditional Statements:</li>
</ol>
<p>Conditional statements, such as <code>if</code>, <code>elseif</code>,
and <code>else</code>, enable decision-making within MATLAB programs by
executing different actions depending on whether certain conditions are
met. Relational operators (<code>&lt;</code>, <code>&lt;=</code>,
<code>==</code>, <code>&gt;=</code>, <code>&gt;</code>, <code>~=</code>)
compare values to determine the truth or falsity of a condition, which
is stored as 1 (true) or 0 (false).</p>
<ol start="5" type="1">
<li>Root Finding: Bisection Method &amp; Newton’s Method:</li>
</ol>
<ul>
<li>The bisection method is an iterative algorithm for finding roots of
a continuous function within a given interval <code>[a, b]</code> where
the function changes sign (<code>f(a) * f(b) &lt; 0</code>). It works by
repeatedly dividing the interval in half and selecting a subinterval
that contains the root.</li>
<li>Newton’s method is another iterative approach for finding roots of a
differentiable function <code>f(x)</code> given an initial guess
<code>x0</code>. The method updates the guess using the formula:
<code>xn+1 = xn - f(xn)/f'(xn)</code>. Its convergence can be faster
than bisection but may also diverge under certain conditions.</li>
</ul>
<p>The probability of ending up at location j on a Galton board with n
rows of pegs is given by the formula:</p>
<pre><code>P(j) = (n!)/(2^n * j!(n - j)!) for j = 0, 1, ..., n</code></pre>
<p>Here’s a detailed explanation of this formula:</p>
<ol type="1">
<li><p><strong>Factorials</strong>: The factorial function, denoted by
“!”, is used to represent the product of all positive integers up to
that number. For example, 5! = 5 × 4 × 3 × 2 × 1 = 120.</p></li>
<li><p><strong>Combination</strong>: The term (n choose j), or C(n,j),
represents the number of ways to choose j items from a set of n distinct
items without regard to the order of selection. It is given by the
formula:</p>
<p>C(n, j) = n! / (j!(n - j)!)</p></li>
<li><p><strong>Probability calculation</strong>: On the Galton board, at
each peg, the ball has an equal probability (1/2) of going left or
right. The number of paths to location j is given by the combination
C(n, j), since you have n choices for pegs where you go left and then j
of those must be chosen to end up at location j.</p></li>
<li><p><strong>Final probability</strong>: Since each path has a
probability of (1/2)^n (because there are n decisions and each decision
has a 1/2 chance of being correct), the overall probability of ending up
at location j is:</p>
<p>P(j) = C(n, j) * (1/2)^n</p></li>
<li><p><strong>Simplification</strong>: The provided formula simplifies
this expression by rewriting it as:</p>
<p>P(j) = n! / (2^n * j!(n - j)!)</p></li>
</ol>
<p>This formula calculates the probability of the ball ending up at
location j after dropping through n rows of pegs on a Galton board. It
takes into account all possible paths and their probabilities, giving us
a comprehensive statistical model for predicting outcomes in such
experiments.</p>
<ol type="a">
<li>The given statement can be proved using combinatorial principles. We
start by considering a binary tree representing n coin tosses, where
each internal node has two children (representing heads or tails), and
the leaves represent the possible outcomes.</li>
</ol>
<p>For a single coin toss, there are 2 possible outcomes: Heads (H) or
Tails (T). So, for n tosses, we have 2^n possible outcomes.</p>
<p>Now, let’s analyze the number of ways to get exactly j heads in n
tosses:</p>
<ol type="1">
<li>For each sequence with j heads, there is a corresponding sequence
with (n-j) tails.</li>
<li>The position of these j heads can be chosen from n+1 possible
positions (including the beginning and the end of the sequence).</li>
<li>Thus, we select j positions for heads out of n+1 total positions,
which is given by the binomial coefficient C(n+1, j) = (n+1)! / [j! *
(n-j)!].</li>
<li>For each selection of j positions, there are 2^(n-j) ways to fill
the remaining (n-j) tails positions.</li>
<li>Combining these, we get the total number of sequences with exactly j
heads as C(n+1, j) * 2^(n-j).</li>
</ol>
<p>Summing this for all possible values of j from 0 to n gives us the
total number of outcomes:</p>
<p>∑<em>{j=0}^{n} C(n+1, j) * 2^(n-j) = ∑</em>{j=0}^{n} (n+1)! / [j! *
(n-j)!] * 2^(n-j).</p>
<p>To simplify this expression, we can use the binomial theorem. The
binomial theorem states that for any real numbers a and b and
non-negative integer n:</p>
<p>(a + b)^n = ∑_{j=0}^{n} C(n, j) * a^(n-j) * b^j.</p>
<p>Setting a = 1 and b = 2, we get:</p>
<p>2^(n+1) = (1 + 2)^(n+1) = ∑_{j=0}^{n+1} C(n+1, j) * 2^j.</p>
<p>This can be rewritten as:</p>
<p>2^(n+1) = C(n+1, 0) * 2^0 + ∑_{j=1}^{n} C(n+1, j) * 2^j + C(n+1, n+1)
* 2^(n+1).</p>
<p>Simplifying, we find:</p>
<p>2^(n+1) = 1 + ∑_{j=1}^{n} C(n+1, j) * 2^j + 2^(n+1),</p>
<p>which implies that:</p>
<p>∑_{j=1}^{n} C(n+1, j) * 2^j = 2^(n+1) - 1.</p>
<p>Recall our expression for the total number of outcomes: ∑_{j=0}^{n}
(n+1)! / [j! * (n-j)!] * 2^(n-j). We can rewrite this using the binomial
coefficient property C(n, j) = C(n, n-j):</p>
<p>∑<em>{j=0}^{n} (n+1)! / [j! * (n-j)!] * 2^(n-j) = ∑</em>{j=0}^{n}
(n+1)! / [(n-j)! * j!] * 2^(n-j).</p>
<p>This is equivalent to:</p>
<p>(n+1) * [∑_{j=1}^{n} C(n+1, j) * 2^j],</p>
<p>which we’ve shown equals (n+1) * [2^(n+1) - 1].</p>
<p>Thus, the total number of outcomes is:</p>
<p>n ∑_{j=1}^{n} C(n, j) = n * 2^n.</p>
<ol start="2" type="a">
<li>To modify the theory where the probability of a ball being deflected
left at each peg is p ≠ 1/2, we can adjust the initial conditions and
update rules accordingly:</li>
</ol>
<ol type="1">
<li><p>Initial Conditions: If initially, the ball has a probability q of
being deflected left (q ≠ 0.5), then at time step k = 1, u(1, j) = q for
the central cell and u(1, j) = 0 for all other cells j ≠ n/2.</p></li>
<li><p>Update Rules: For interior cells (1 &lt; j &lt; n-1), the update
rule becomes:</p>
<p>u(k+1,j) = f(u(k,j−1), u(k,j), u(k,j+1)), where f is now a function
that depends on p. Specifically, if we consider the case where the
deflection probability at each peg is p (with 0 &lt; p &lt; 1),
then:</p>
<ul>
<li>If all three neighboring cells are right-deflected (RRR): u(k+1,j) =
0 with probability p.</li>
<li>Otherwise, u(k+1,j) = 1 with probability (1-p).</li>
</ul></li>
</ol>
<p>For edge cells (j = 1 or j = n), the update rules are:</p>
<ul>
<li>u(k+1,1) = 0 and u(k+1,n) = 0 since there’s no left-deflecting
neighbor.</li>
<li>For all other edges (j = 2 to n-1), if both neighboring cells are
right-deflected: u(k+1,j) = 0 with probability p; otherwise, u(k+1,j) =
1 with probability (1-p).</li>
</ul>
<p>These adjustments allow for a more general model where the deflection
probabilities at each peg can be tuned using parameter p. The code
adaptation would involve modifying the initialization and update logic
to accommodate these changes.</p>
<p>The text provided discusses dynamical systems, which describe the
evolution of a system of variables through either differential equations
(continuous dynamical systems) or difference equations (discrete
dynamical systems). These systems are often derived from modeling
physical phenomena and can exhibit fascinating behavior.</p>
<p>11.1 Introduction to Dynamical Systems:</p>
<ul>
<li>Dynamical systems are a significant part of applied mathematics,
describing the evolution of variables through time, either continuously
or discretely. They are usually derived from modeling physical phenomena
and can display intriguing behaviors.</li>
</ul>
<p>11.1.1 Example of a Continuous Dynamical System: - A simple example
is a population model where the growth rate of a species’ population
p(t) is proportional to its current size, modeled by the first-order
ordinary differential equation (ODE):</p>
<p>dp/dt = αp, for t &gt; t0, with initial condition p(t0) = p0.</p>
<ul>
<li>The solution to this ODE can be found analytically as p(t) = p0 *
exp(αt), representing exponential growth if there are abundant resources
and no threats.</li>
</ul>
<p>11.1.2 Example of a Discrete Dynamical System: - A discrete version
of the above model is given by the recurrence relation:</p>
<p>pn = αpn−1, for n &gt; 0 with initial condition p0.</p>
<ul>
<li>The solution to this difference equation is easy to derive
analytically as pn = α^n * p0.</li>
</ul>
<p>11.1.3 Matlab’s ODE Solver (ode45): - To solve a first-order ODE like
the one in (11.1), Matlab provides the built-in solver ode45. The script
demonstrates using ode45 to solve the population model:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">global</span> <span class="va">alpha</span><span class="op">;</span> <span class="co">% shared value of alpha elsewhere in the code</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="va">alpha</span> <span class="op">=</span> <span class="fl">1</span><span class="op">;</span> <span class="co">% set value of alpha</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>[<span class="va">t</span><span class="op">,</span> <span class="va">y</span>] <span class="op">=</span> <span class="va">ode45</span>(<span class="op">@</span><span class="va">popfun</span><span class="op">,</span> [<span class="fl">0</span><span class="op">,</span> <span class="fl">1</span>]<span class="op">,</span> <span class="fl">1</span>)<span class="op">;</span> <span class="co">% solve ODE system over range 0 &lt; t &lt; 1 with initial condition y(1) = 1</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="va">plot</span>(<span class="va">t</span><span class="op">,</span> <span class="va">y</span>)<span class="op">;</span> <span class="co">% plot solution against time</span></span></code></pre></div>
<ul>
<li><p>This script calls a separate user-defined function popfun.m that
computes the derivative (dy/dt).</p></li>
<li><p>Key points:</p>
<ul>
<li>The ‘global’ keyword allows variables set in one part of the code to
be shared with other parts.</li>
<li>ode45 takes three arguments: function defining the derivative, time
range, and initial condition. It outputs arrays of discrete time steps
(t) and corresponding solution values (y).</li>
<li>A user-defined function is required for computing the derivative,
which ode45 calls during its execution.</li>
</ul></li>
</ul>
<p>11.2 Prey vs Predator Model: - The text then introduces a more
complex model involving two species—rabbits (r(t)) and foxes (f(t)).
Their population dynamics are described by coupled non-linear ODEs:</p>
<p>dr/dt = αr - βrf, d f/dt = δrf - γf,</p>
<p>for t &gt; 0 with initial conditions r(0) = r0 and f(0) = f0. Here,
α, β, γ, and δ are assumed to be positive constants.</p>
<ul>
<li>The model takes into account interactions between the two species:
<ul>
<li>Rabbit population grows exponentially (αr) but is reduced by
predation from foxes (−βrf).</li>
<li>Fox population grows due to consuming rabbits (δrf) but declines
naturally (−γf).</li>
</ul></li>
<li>The constants α, β, γ, and δ represent various factors such as birth
rates, death rates, predation efficiencies, etc. The model’s behavior
depends on these parameters’ values.</li>
</ul>
<p>The text concludes by hinting at the potential for studying
fascinating behaviors exhibited by dynamical systems, including the
prey-predator model mentioned above.</p>
<p>Summary:</p>
<ol type="1">
<li><p><strong>Predator-Prey Model (Lokta-Volterra Equations):</strong>
This is a system of two coupled, non-linear ordinary differential
equations that model the dynamics between a predator (foxes) and its
prey (rabbits). The model describes how populations grow or decline
based on their interactions.</p>
<p>Equations:</p>
<ul>
<li>dr/dt = αr - βrf (growth of rabbits, reduced by predation)</li>
<li>df/dt = δrf - γf (decline of foxes due to lack of food, exponential
decay when no rabbits present)</li>
</ul>
<p>Here, α is the rabbit growth rate, β is the predation rate, γ is the
fox death rate, and δ is the conversion efficiency.</p></li>
<li><p><strong>Equilibrium Solutions:</strong> The equilibrium solutions
are points where both populations remain constant over time (dr/dt = 0
and df/dt = 0). They are found by setting the right-hand side of each
equation to zero:</p>
<ul>
<li>f* = α/β, r* = γ/δ or f* = 0, r* = 0</li>
</ul>
<p>These equilibrium points represent scenarios where either predator or
prey populations dominate, or both are absent.</p></li>
<li><p><strong>Numerical Solution:</strong> Due to the non-linear nature
of these equations and the coupled ODE system (two equations for two
unknowns), analytical solutions aren’t possible in closed form. Instead,
numerical methods like ode45 in MATLAB can be used to solve them. The
provided code (lv.m) demonstrates how to do this, producing time series
plots of rabbit and fox populations over time as well as a phase
portrait illustrating their relationship.</p></li>
<li><p><strong>Logistic Map:</strong> A discrete-time model describing
the evolution of a population, here represented by p(n). It’s given
by:</p>
<ul>
<li>pn = αpn−1 (1 −pn−1), for n ≥ 1</li>
</ul>
<p>Here, α is a parameter controlling the growth rate. The equilibrium
solutions are found by setting pn = x* and solving x* = αx<em>(1
−x</em>), giving x* = 1 −1/α or x* = 0.</p>
<p>MATLAB code (lmap.m) numerically explores the long-term behavior of
this system for different values of α, revealing various dynamic regimes
including convergence to a constant value, periodicity, and
chaos.</p></li>
<li><p><strong>Lorenz Attractor:</strong> A set of chaotic solutions
arising from a simplified model of atmospheric convection. The system is
described by three non-linear ODEs:</p>
<ul>
<li>dx/dt = σ(y −x)</li>
<li>dy/dt = x(ρ −z) −y</li>
<li>dz/dt = xy −βz</li>
</ul>
<p>For specific parameter values (σ, β, ρ), this system exhibits chaotic
behavior where trajectories are sensitive to initial conditions. MATLAB
code (lorenz.m) generates plots illustrating the complex, aperiodic
dynamics of this system in phase space.</p></li>
<li><p><strong>Netflix Prize Problem:</strong> An optimization challenge
posed by Netflix in 2006 to improve its movie recommendation algorithm.
The goal was to develop a method that could predict user preferences for
movies with at least a 10% improvement over the existing system
(“Cinematch”). This involved predicting missing ratings in a sparse
matrix representing users’ evaluations of films on a 1-5 scale, using
techniques like matrix factorization.</p></li>
<li><p><strong>Matrix Factorization Approach:</strong> A key method
employed in solving the Netflix Prize problem. The central idea is to
approximate the original rating matrix R as the product of two
lower-dimensional matrices P and Q, capturing user preferences for
categories (P) and movie characteristics corresponding to these
categories (Q). Minimizing a cost function that quantifies the
prediction error guides the iterative refinement of P and Q towards
optimal values.</p></li>
<li><p><strong>MATLAB Code:</strong> Provides an implementation of the
matrix factorization approach using gradient descent optimization to
solve the Netflix Prize problem. The code iteratively updates matrices P
and Q, aiming to minimize the difference between predicted ratings
(P*Q^T) and actual ratings in R. It also includes regularization to
prevent overfitting and bias correction to isolate user and movie
preferences from overall averages.</p></li>
<li><p><strong>Improvements:</strong> Discusses enhancements to the
basic matrix factorization approach, including:</p>
<ul>
<li>Regularization: Adding a penalty term to discourage large values in
P and Q, helping to prevent overfitting.</li>
<li>Isolating Bias: Subtracting user and movie averages from the rating
matrix before applying matrix factorization, then re-incorporating these
averages after obtaining predictions. These modifications lead to more
robust and less sensitive solutions across different simulations.</li>
</ul></li>
</ol>
<p>The provided text contains solutions to various problems related to
numerical methods, geometrical arguments, and programming using MATLAB.
Here’s a detailed summary of each section:</p>
<ol type="1">
<li><p>Geometric Argument and Recurrence Relation: The geometric
argument involves the relationship between two terms in a sequence (dn
and dn+1), which is rearranged to express dn+1 in terms of dn. This
recurrence relation is then used to approximate a specific mathematical
constant.</p>
<p>The rearrangement goes as follows: ((dn/2)^2 - (dn+1/2)^2) =
(dn+1/2)^2 + ((s<em>(dn/2)^2 + 1) - 1)^2 This can be simplified to:
dn+1/2 = sqrt((dn/2)^2 + (s</em>(dn/2)^2 + 1 - 1)^2)</p></li>
<li><p>MATLAB Scripts for Sequences and Series:</p>
<ol type="a">
<li><p>A script to approximate π using a specific sequence is provided,
with varying convergence depending on the number of terms (n).</p></li>
<li><p>Another script calculates the sum of cubes of integers up to n
(denoted as Σj^3), comparing it to the given formula Σj=1..n j^3 =
(n*(n+1)/2)^2.</p></li>
<li><p>A third script explores the Euler-Mascheroni constant, γ, which
is the limit of the difference between the harmonic series and the
natural logarithm. The script visualizes this relationship using a plot
of sn - ln(n) against n.</p></li>
</ol></li>
<li><p>MATLAB Scripts for Various Problems: Several scripts are given to
solve different problems using MATLAB, including calculating factorials,
Stirling’s approximation for n!, Pascal’s triangle, sinc function,
square wave function, Newton’s method, bisection method, secant method,
bubble sort algorithm, and series convergence tests.</p></li>
<li><p>Explanations of Convergence and Error: The text explains the
concept of convergence in numerical methods and error analysis for
different integration techniques (rectangular midpoint rule, trapezium
rule, and Simpson’s Rule). It demonstrates how errors decrease as the
number of subintervals (n) increases, with specific rates depending on
the method used.</p>
<p>For example:</p>
<ul>
<li>The rectangular midpoint rule has an error approximately
proportional to 1/n when f’(a) = f’(b), meaning doubling n reduces the
error by a factor of four.</li>
<li>Simpson’s Rule has errors that decrease even faster, roughly
proportional to 1/n^4, implying quartic improvement in accuracy for each
doubling of n.</li>
</ul></li>
<li><p>Additional Problem Solutions: The text includes solutions for
various mathematical problems, such as computing Catalan numbers,
evaluating spherical Bessel functions (sph(x,n)), and simulating random
walks in one and two dimensions. These solutions are presented in MATLAB
code format, often accompanied by explanations of the underlying
mathematics or computational methods.</p></li>
</ol>
<p>The provided text appears to be a collection of problem solutions and
code snippets related to numerical methods, differential equations, and
MATLAB programming. Here’s a detailed summary and explanation of each
section:</p>
<h3 id="error-ratio-and-integration-techniques">1. Error Ratio and
Integration Techniques</h3>
<ul>
<li><strong>Error Ratio:</strong> The ratio of consecutive errors is
approximately 16, indicating that the error decreases by a factor of
1/16 as n (presumably step size or iteration count) doubles.</li>
<li><strong>Improper Integral:</strong> A technique for solving an
improper integral using substitution: t = u/(1 - u), leading to a
transformed integral and subsequent evaluation with MATLAB.</li>
</ul>
<h3 id="numerical-methods">2. Numerical Methods</h3>
<ul>
<li><strong>Euler’s Method vs Midpoint Method:</strong> Comparison of
Euler’s method and the Midpoint method, showing that the Midpoint method
(with a modified line in the code) produces similar results but may be
more accurate.</li>
<li><strong>Challenges with ln(sin x) Integration:</strong> Discussion
on the difficulty of numerically integrating <code>ln(sin(x))</code> due
to its behavior near x = 0 and suggestions for alternative methods, such
as using Maclaurin series approximations.</li>
</ul>
<h3 id="vector-calculations">3. Vector Calculations</h3>
<ul>
<li><strong>Cross Product and Dot Product Properties:</strong>
Demonstrates the use of cross product (cross) and dot product (dot)
operations on vectors to verify algebraic identities.</li>
</ul>
<h3 id="linear-algebra-problems">4. Linear Algebra Problems</h3>
<ul>
<li><strong>Matrix Inversion and Eigenvalue Computation:</strong>
Various exercises involving matrix inversion, determinant calculation,
and eigenvalue computation using MATLAB’s built-in functions.</li>
</ul>
<h3 id="numerical-methods-for-odes">5. Numerical Methods for ODEs</h3>
<ul>
<li><strong>Power Method for Eigenvalues:</strong> Implementation of the
power method to approximate the dominant eigenvalue and eigenvector of a
given matrix.</li>
</ul>
<h3 id="pagerank-algorithm">6. PageRank Algorithm</h3>
<ul>
<li><strong>Page Ranking Problem:</strong> Scripts implementing
different versions of the PageRank algorithm, involving matrix
operations to calculate the rankings of web pages based on link
structures.</li>
</ul>
<h3 id="dynamical-systems">7. Dynamical Systems</h3>
<ul>
<li><strong>Population Model and Chaos Theory:</strong> Code for solving
ordinary differential equations (ODEs) that model population dynamics,
showcasing chaotic behavior in different parameter regimes using Lorenz
attractor as an example.</li>
</ul>
<h3 id="additional-problems-and-solutions">8. Additional Problems and
Solutions</h3>
<ul>
<li><strong>Various Numerical Methods and Dynamical Systems:</strong>
Includes additional problems and solutions covering topics like
numerical integration techniques, solving systems of ODEs, and analyzing
dynamical systems’ stability and behavior.</li>
</ul>
<p>Each section provides detailed explanations and code snippets that
demonstrate how to implement and analyze various mathematical concepts
using MATLAB, focusing on numerical methods, differential equations,
linear algebra, and computational dynamics. The solutions also highlight
the importance of understanding algorithmic steps and parameter
sensitivity in numerical simulations.</p>
<h3 id="comsoc">comsoc</h3>
<p>The Handbook of Computational Social Choice is an authoritative
overview of the field that combines computer science and economics to
study collective decision-making processes. The book is divided into
four main parts, each focusing on a different aspect of computational
social choice:</p>
<ol type="1">
<li><p>Part I: Voting - This section covers various aspects of voting
rules and their analysis. It includes an introduction to the theory of
voting (Chapter 2), which discusses classical themes such as Condorcet
extensions, scoring rules, and run-offs. Fishburn’s classification of
voting rules is also presented, grouping them into C1, C2, and C3
classes based on their properties.</p>
<ul>
<li><p>Chapter 3: Tournament Solutions focuses on C1 functions (voting
rules that depend only on pairwise majority comparisons) represented by
directed graphs called tournaments. The chapter covers topics like
McGarvey’s Theorem, strategyproofness, implementation via binary
agendas, and extensions to weak tournaments.</p></li>
<li><p>Chapter 4: Weighted Tournament Solutions discusses C2 functions
(voting rules that depend on weighted pairwise majority comparisons)
represented by weighted directed graphs. It focuses on prominent voting
rules such as Kemeny’s rule, the maximin rule, and Schulze’s method,
analyzing their computation, approximation, and fixed-parameter
tractability, with a particular emphasis on Kemeny’s rule.</p></li>
<li><p>Chapter 5: Dodgson’s Rule and Young’s Rule examines C3 voting
rules (requiring strictly more information for winner determination)
such as Dodgson’s and Young’s rules. The complexity of their winner
determination problem is analyzed, along with methods for bypassing the
intractability through approximation algorithms, fixed-parameter
tractable algorithms, and heuristic algorithms.</p></li>
<li><p>The remaining chapters in Part I address specific methodologies
for analyzing voting rules, including topics like manipulation barriers
(Chapter 6) and further exploration of axiomatic, strategic, and
computational aspects of voting.</p></li>
</ul></li>
<li><p>Part II: Fair Allocation - This section focuses on the problem of
allocating indivisible goods to individuals with heterogeneous
preferences in a fair manner. It distinguishes between divisible
(Chapter 12) and indivisible (Chapter 13) goods, with an emphasis on
cake-cutting algorithms for dividing resources fairly among multiple
parties.</p>
<ul>
<li><p>Chapter 12: Fair Allocation of Indivisible Goods introduces
preference structures and the fairness vs. efficiency trade-off. It
covers methods for computing fair allocations and protocols designed to
achieve them.</p></li>
<li><p>Chapter 13: Cake Cutting Algorithms presents classic cake-cutting
algorithms, analyzing their complexity and discussing optimal methods
for fairly dividing a resource among several parties with different
preferences.</p></li>
</ul></li>
<li><p>Part III: Coalition Formation - This section addresses questions
arising when agents can form coalitions and each have preferences over
these coalitions. It includes two-sided matching problems (Chapter 14),
hedonic games (Chapter 15), and weighted voting games (Chapter 16).</p>
<ul>
<li><p>Chapter 14: Matching under Preferences focuses on two-sided
preferences in matching markets, discussing various solution concepts
like stable marriage and roommates.</p></li>
<li><p>Chapter 15: Hedonic Games examines the situation where agents’
preferences depend purely on the members of the coalition they are part
of, studying solution concepts and computational complexity.</p></li>
<li><p>Chapter 16: Weighted Voting Games explores scenarios where
coalitions emerge to achieve specific goals (e.g., passing a bill in
parliament), discussing voter weight versus voter power and simple
games.</p></li>
</ul></li>
<li><p>Part IV: Additional Topics - This section covers topics that do
not fit neatly into the first three thematic parts, including judgment
aggregation (Chapter 17), applications of the axiomatic method to
reputation systems on the Internet (Chapter 18), and knockout
tournaments (Chapter 19).</p></li>
</ol>
<p>In summary, this handbook provides a comprehensive introduction to
computational social choice by exploring various aspects such as voting
rules, fair allocation, coalition formation, and additional topics. It
aims to be accessible for students, scholars from computer science and
economics, and other researchers interested in mathematical and social
sciences. The book highlights the interdisciplinary nature of
computational social choice by incorporating theoretical computer
science concepts like computational complexity theory and approximation
algorithms into traditional social choice problems.</p>
<p>Chapter 2 of the Handbook of Computational Social Choice provides an
introduction to the theory of voting. It focuses on multicandidate
voting with ranked ballots, where each voter submits a linear ordering
of alternatives, specifying their preferences from most favored to least
favored.</p>
<p>The chapter discusses three fundamental results in voting theory:</p>
<ol type="1">
<li>Majority cycles (Condorcet’s principle): A situation where
collective preference violates individual rationality, with a majority
preferring alternative A over B, another majority preferring B over C,
and yet another majority preferring C over A.</li>
<li>Arrow’s Impossibility Theorem: Every voting rule for three or more
alternatives either violates Independence of Irrelevant Alternatives
(IIA) or is a dictatorship, where the election outcome depends solely on
one designated voter’s ballot.</li>
<li>Gibbard-Satterthwaite Theorem (GST): Every strategyproof social
choice function (SCF), other than a dictatorship, fails to be immune to
manipulation by individual voters.</li>
</ol>
<p>The chapter introduces several voting rules within the SCF context
and discusses their properties or axioms. It covers:</p>
<ul>
<li>Plurality voting: Selects the alternative with the most votes
(greatest number). Criticized for potentially electing an unpopular
candidate due to a mere plurality, not majority, of support.</li>
<li>Copeland: Assigns points based on pairwise majority victories and
defeats, disregarding margins. The Copeland score is the difference
between the number of alternatives preferred by strict majority and
those that prefer it. The alternative with the highest score wins.</li>
<li>Borda (both symmetric and asymmetric): Awards points to alternatives
according to their rankings on individual ballots, with higher ranks
receiving more points. The symmetric version gives equal weights to all
positions, while the asymmetric version assigns different weights (e.g.,
2 for first place, 1 for second place, and 0 for last).</li>
</ul>
<p>The chapter also discusses three axioms that distinguish voting
rules: anonymity, neutrality, and Pareto property:</p>
<ul>
<li>Anonymity: Each pair of voters plays interchangeable roles; swapping
the ballots of two voters does not affect the outcome.</li>
<li>Neutrality: Interchangeable alternatives (equivalent to swapping
their positions in every ballot) result in an equivalent outcome.</li>
<li>Pareto property (Pareto optimality): The winning set never contains
a dominated alternative; that is, no alternative exists that another
alternative beats in pairwise comparisons according to all voters’
preferences.</li>
</ul>
<p>These axioms are often uncontroversial and help identify interesting
SCFs while minimizing the risk of unintended consequences. The chapter
also mentions other groups of axioms (milder, stronger) and
strategyproofness, with impossibility results like Arrow’s Theorem and
IIA controversies.</p>
<p>In summary, this chapter offers an overview of voting theory,
focusing on SCFs using ranked ballots. It introduces fundamental results
such as majority cycles and the GST and discusses key voting rules
(plurality, Copeland, Borda) and their axiomatic properties, including
anonymity, neutrality, and Pareto property. The chapter lays the
groundwork for understanding various aspects of strategic manipulation
in voting systems.</p>
<p>The Gibbard-Satterthwaite Theorem states that any resolute
(non-imposed), nondictatorial, and strategyproof Social Choice Function
(SCF) for three or more alternatives must be a dictatorship. This means
that some voter’s preferences always determine the outcome of the
election.</p>
<p>The proof involves several lemmas: 1. Push-Down Lemma: If an SCF is
resolute and down monotonic, then there exists a profi le where voters
can manipulate their rankings to achieve their desired outcome. 2.
Adjustment Lemma: A Pareto, resolute, and down monotonic SCF must be
imposed (non-manipulable). 3. Splitting Lemma: If a dictating set is
split into disjoint subsets Y and Z, then either Y or Z must also be a
dictating set. 4. The proof of the Adjustment Lemma uses the Push-Down
Lemma to show that if an SCF is resolute, Pareto, and down monotonic, it
cannot have any manipulable voters.</p>
<p>Limitations of the Gibbard-Satterthwaite Theorem include: 1. It
applies only to resolute SCFs, while many real-world voting rules are
irresolute (allowing ties). 2. Real-life conditions often prevent a
single voter from knowing other voters’ ballots and predicting the
outcome of manipulations. 3. The theorem does not cover Social Decision
Schemes that use different types of inputs or outputs for elections.</p>
<p>A generalization, the Duggan-Schwartz Theorem, suggests that even
allowing a large number of ties (ties as numerous as the Omninominator
rule) may not be enough to achieve strategyproofness in irresolute
SCFs.</p>
<p>Black’s Theorem and Sen’s Possibility Theorem demonstrate that
certain domain restrictions can guarantee transitivity and
strategyproofness for specific types of profi les (single-peaked and
value-restricted, respectively).</p>
<p>This chapter discusses Tournament Solutions in the context of
aggregating binary preferences from individual agents to a group,
focusing on Social Choice Functions (SCFs) based solely on the dominance
relation, known as C1 functions. The majority rule is characterized by
May’s Theorem for two alternatives, and most common voting rules satisfy
its axioms in this case.</p>
<p>The chapter begins by introducing key concepts: a set of voters N, a
set of alternatives A, and a preference profile R = (≿1, …, ≿n), where
≿i is the preference relation of voter i. The majority relation ≿ for R
is defined such that a ≿ b if and only if |{i ∈ N : a ≿i b}| ≥ |{i ∈ N :
b ≿i a}|.</p>
<p>McGarvey’s Theorem states that for any complete relation over
alternatives, there exists a preference profile with at most n ≤ m(m-1)
voters that induces this relation. This theorem implies that every
majority relation can be obtained from some preference profile, and it
is used to demonstrate that, in odd-numbered voter cases, the dominance
relation (the asymmetric part of the majority relation) is
antisymmetric, connex, and irreflexive—thus, forming a tournament.</p>
<p>A tournament solution is a function S mapping each tournament T = (A,
≻) to a nonempty subset S(T) of its alternatives A, known as the choice
set. Tournament solutions are required not to distinguish between
isomorphic tournaments and should satisfy certain properties:
monotonicity, stability, and composition-consistency.</p>
<p>Monotonicity means that an alternative remains in the choice set when
its dominion is expanded without altering anything else. Stability
requires that a chosen subset of alternatives is selected from the union
of two other subsets if and only if it is selected from each subset
individually. Composition-consistency implies that the solution chooses
“best” alternatives from “best” components, as defined by decompositions
of tournaments.</p>
<p>The chapter then introduces several common tournament solutions:</p>
<ol type="1">
<li>Trivial (TRIV): Always selects all alternatives, satisfying
monotonicity, stability, and composition-consistency but is not
discriminatory.</li>
<li>Condorcet Non-losers (CNL): Selects all alternatives except for
Condorcet losers, violating stability and composition-consistency while
satisfying monotonicity.</li>
<li>Copeland Set (CO): Chooses alternatives with maximal dominion size.
It satisfies monotonicity but not stability or
composition-consistency.</li>
<li>Slater Set (SL): Selects alternatives that are maximal elements in
strict linear orders derived from the tournament by inverting minimal
edges. SL violates stability and composition-consistency, and membership
is NP-hard to decide.</li>
<li>Markov Set (MA): Determines alternatives based on their maximum
probability in a stationary distribution of a Markov chain defined by
the tournament’s adjacency matrix. MA satisfies monotonicity but not
stability or weak composition-consistency, with polynomial-time
computation.</li>
<li>Bipartisan Set (BP): Based on maximal lotteries over alternatives,
satisfying monotonicity, stability, and composition-consistency.
Computation is in polynomial time using a linear feasibility
problem.</li>
<li>Uncovered Set (UC): Selects maximal elements according to the
covering relation—a transitive subrelation of the dominance relation. UC
satisfies monotonicity and composition-consistency but not stability; it
can be computed in polynomial time via matrix multiplication.</li>
<li>Banks Set (BA): Chooses maximal alternatives from all
inclusion-maximal transitive subtournaments, violating stability and
composition-consistency. Deciding membership is NP-complete, and there’s
no known polynomial-time algorithm for its computation.</li>
</ol>
<p>The chapter concludes by discussing methods to refine tournament
solutions based on stability criteria, leading to new solutions like the
top cycle, minimal covering set, and minimal extending set. These
solutions aim to maintain or improve properties such as monotonicity,
stability, and composition-consistency while providing alternative ways
to aggregate preferences in diverse contexts.</p>
<p>Kemeny’s Rule is a social choice function that aggregates individual
preferences into a collective ranking by maximizing agreement with the
input profiles. It was first introduced by Kemeny (1959) as a method to
find the linear order that agrees with the maximum number of pairwise
comparisons from given preference profiles.</p>
<p>The rule is defined based on the concept of majority margins, which
are differences in the number of voters preferring one alternative over
another. A weighted tournament, represented by an antisymmetric matrix M
where entries (M)xx = 0 and (M)xy = mR(x, y) for x ≠ y, can be derived
from a preference profile R.</p>
<p>Kemeny’s Rule selects the linear order with the minimum score, which
is calculated as the sum of distances to each individual preference
order in terms of inversions (Kendall’s tau distance). This rule is a C2
function since it depends only on pairwise majority margins and not on
their absolute values.</p>
<p>The computational complexity of Kemeny’s Rule has been studied
extensively:</p>
<ol type="1">
<li><p>Kemeny Score: Given a preference profile R and an integer k, the
problem asks whether there exists a linear order with score at most k.
This decision problem is NP-complete for even n ≥ 4 and for odd n when n
is unbounded (Bartholdi et al., 1989a; Hudry, 1989). The case of odd n ≥
3 remains open.</p></li>
<li><p>Kemeny Winner: This decision problem asks whether there exists a
linear order that ranks a given alternative x as the first and has
minimum score with respect to R. Its complexity is also NP-complete for
even n ≥ 4 and for odd n when n is unbounded (Bartholdi et al., 1989b;
Hudry, 1989).</p></li>
<li><p>Kemeny Ranking: This problem asks whether there exists a linear
order that ranks one alternative x above another y with minimum score.
Its complexity is even harder and belongs to the class 𝜆P2 of problems
solvable via parallel access to NP (Hemaspaandra et al., 2005).</p></li>
<li><p>Kemeny Rank Aggregation: The optimization problem asks for
finding a linear order that has minimum score with respect to R. Its
computational complexity is also believed to be hard, as efficient
algorithms are unlikely to exist due to its membership in the class
𝜆P2.</p></li>
</ol>
<p>Due to these NP-completeness and 𝜆P2-completeness results, exact
polynomial-time algorithms for Kemeny’s Rule are generally not feasible.
Researchers have explored alternative approaches:</p>
<ol type="1">
<li><p>Exponential-Time Parameterized Algorithms: These methods aim to
solve the problem more efficiently by targeting specific parameters
within the input instance. However, their effectiveness is limited in
practice due to large parameter values.</p></li>
<li><p>Polynomial-Time Approximation Algorithms: These algorithms trade
solution quality for polynomial running time. A 5-approximation
algorithm for Kemeny Rank Aggregation was proposed by Coppersmith et
al. (2010), ordering alternatives based on increasing Borda
scores.</p></li>
<li><p>Practical Heuristics and Exact Methods: Various heuristic and
exact methods have been developed to address real-world applications of
Kemeny’s Rule, even though they may not guarantee optimality or
polynomial runtime in all cases. Examples include the minimum feedback
arc set approach, local search algorithms, and integer programming
formulations.</p></li>
</ol>
<p>In summary, while Kemeny’s Rule offers an attractive way to aggregate
individual preferences into a collective ranking based on agreement, its
computational complexity poses significant challenges for finding exact
solutions efficiently. Approximation algorithms and heuristic methods
provide practical alternatives for tackling these problems in various
applications.</p>
<p>The Dodgson winner problem, which determines the winner of an
election under Dodgson’s voting system, has been proven to be
𝑶p_2-complete by Hemaspaandra et al. (1997a). This result implies that
unless NP = coNP, the problem is not NP-complete and is considered
highly intractable.</p>
<p>The complexity of the Dodgson winner problem being 𝑶p_2-complete was
established through a complex proof structure. The core idea involves
proving several seemingly easy properties about Dodgson elections to
ultimately demonstrate hardness. These properties include:</p>
<ol type="1">
<li>Trapping potential scores within two adjacent values, using an
NP-hardness reduction (L1).</li>
<li>Creating a “double exposure” merging key information from two
elections in polynomial time (L2).</li>
<li>Summing the Dodgson scores of candidates across multiple elections
to equal the score of a designated candidate in a single election
(dodgsonsum, Lemma 5.2).</li>
</ol>
<p>The proof exploits the structure of these properties and uses them in
conjunction with Wagner’s technical lemma (Lemma 5.3) to establish
𝑶p_2-hardness for the Dodgson winner problem. This technique involves
demonstrating that, given k inputs satisfying χA(x1) ≥ · · · ≥ χA(x2k),
where A is an NP-complete set, the function f (x1, …, x2k) belongs to B
if and only if ∥{i | xi ∈A}∥≡1 (mod 2).</p>
<p>In essence, proving easy properties about Dodgson elections allows
for a more organized structure that can be harnessed by polynomial-time
many-one reductions to establish the problem’s high level of
computational difficulty under the 𝑶p_2 complexity class. This approach
contrasts with NP-hardness proofs, which typically focus on
demonstrating hardness without explicitly exploiting organized
structures within the problem.</p>
<p>The Gibbard-Satterthwaite Theorem is a fundamental result in voting
theory, which establishes impossibility results for voting rules with
unrestricted preference domains (m ≥ 3 alternatives). The theorem states
that any such voting rule must exhibit at least one of three undesirable
properties:</p>
<ol type="1">
<li><p>Dictatorial: There exists a single fixed voter whose preferred
alternative is always chosen, regardless of the other voters’
preferences. This means that the decision-making power lies solely with
this individual, disregarding the collective will of the group.</p></li>
<li><p>Imposing: At least one alternative does not win under any
possible preference profile (combination of individual rankings). In
other words, there is an alternative that can never be selected as the
winner, no matter how voters rank their preferences. This property
renders the voting rule useless since it excludes certain alternatives
from ever being elected.</p></li>
<li><p>Manipulable (not strategyproof): There exist preference profiles
in which at least one voter has an incentive to misreport her true
preferences, aiming for a better outcome. In other words, voters can
benefit by strategically casting their votes differently than their
genuine preferences, thus engaging in manipulation or strategic
voting.</p></li>
</ol>
<p>This theorem implies that, without restrictions on voter preferences,
it is impossible to design a voting rule that satisfies all desirable
properties such as non-dictatorship, non-imposition, and
strategyproofness simultaneously. As a result, various approaches have
been taken to address this challenge, including imposing restrictions on
the preference domain or employing computational complexity as a barrier
to manipulation, which is the main focus of the chapter.</p>
<p>The undesirability of strategic voting arises from the following
reasons: - It violates the principle of one person, one vote, as
manipulating voters effectively alter their weight in the election. -
Manipulation can lead to outcomes that do not reflect the true
preferences of the electorate, undermining democratic principles and
legitimacy. - Strategic voting introduces uncertainty and complexity
into the voting process, potentially causing confusion among voters and
increasing the likelihood of errors or mistakes.</p>
<p>In summary, the Gibbard-Satterthwaite Theorem highlights the inherent
difficulties in designing a perfect, manipulation-free voting rule with
unrestricted preferences. This motivates the exploration of alternative
approaches, such as computational complexity barriers to manipulation,
which this chapter delves into.</p>
<p>Table 7.1 presents three types of preference profiles required by
different voting rules, specifically focusing on the Borda election
system. The table outlines how voters rank candidates (A through F)
based on their preferences, assigning points to each ranking position as
follows: - Position 1: 5 points - Position 2: 4 points - Position 3: 3
points - Position 4: 2 points - Position 5: 1 point - Position 6: 0
points (last place)</p>
<p>For each voter profile, the table shows the specific ranking of
candidates.</p>
<ol type="1">
<li><p>Voter 1’s preference profile:</p>
<ul>
<li>First choice: a</li>
<li>Second choice: c</li>
<li>Third choice: b</li>
<li>Fourth choice: f</li>
<li>Fifth choice: e</li>
<li>Sixth (last) choice: d</li>
</ul>
<p>In this case, voter 1 assigns the highest preference to candidate
‘a,’ followed by ‘c,’ then ‘b,’ and so on. Candidate ‘d’ is ranked last
with 0 points.</p></li>
<li><p>Voter 2’s preference profile:</p>
<ul>
<li>First choice: b</li>
<li>Second choice: a</li>
<li>Third choice: f</li>
<li>Fourth choice: c</li>
<li>Fifth choice: e</li>
</ul>
<p>Here, voter 2 ranks candidate ‘b’ first, followed by ‘a,’ then ‘f,’
and so on. Candidate ‘c’ is ranked fourth with 3 points (4 - 1 = 3), ‘e’
gets 2 points (5 - 3 = 2), and candidate ‘a’ receives 4 points (5 - 1 =
4).</p></li>
</ol>
<p>This table illustrates how each voter’s preference profile affects
their ranking of candidates in a Borda election system, ultimately
influencing the overall score for each candidate. The scores are
calculated by summing up the points assigned to each rank across all
voters’ profiles.</p>
<p>The text discusses various types of control in the context of voting
systems, focusing on the manipulation of elections by a chair or
authority responsible for organizing the election. The four primary
control types are:</p>
<ol type="1">
<li>Constructive Control by Adding/Deleting Candidates/Voters: This
involves changing the set of candidates or voters to influence the
outcome of the election in favor of a designated candidate (p).
<ul>
<li><strong>Adding Spoilers (CCAUC)</strong>: The chair can add spoiler
candidates from a given set B, hoping that these candidates will weaken
p’s competitors. A variant with a bound k on the number of added
spoilers (CCAC) is also considered.</li>
<li><strong>Deleting Candidates/Voters (CCDC/CCDV)</strong>: The chair
can delete up to k candidates or votes to eliminate p’s worst rivals,
making p more likely to win.</li>
</ul></li>
<li>Constructive Control by Partitioning Candidates/Voters: This
involves dividing the set of candidates or voters into subsets and
organizing elections in stages.
<ul>
<li><strong>Runoff Partition (CCRPC-TE/TP)</strong>: The chair
partitions the candidates into two groups, and each group holds a
pre-election using the given voting rule. Depending on the tie-handling
rule (TE: Ties Eliminate; TP: Ties Promote), either unique or multiple
winners proceed to the final stage.</li>
<li><strong>Partition of Voters (CCPV-TE/TP)</strong>: The chair
partitions the set of voters into two groups, with each group voting
separately in a pre-election. Similar to CCRPC, the tie-handling rule
determines which winners proceed to the final stage.</li>
</ul></li>
<li>Destructive Control: These variants aim to prevent a designated
candidate (p) from becoming the unique winner of the election resulting
from the chair’s control action. They are denoted by replacing the
initial “C” with a “D,” e.g., DCDC for “destructive control by deleting
candidates.”</li>
</ol>
<p>The text also mentions various voting rules and their susceptibility
or resistance to these control types, as well as some immunity results.
Immunity means that it is impossible for the chair to manipulate the
election outcome through a given control type. Resistance indicates that
the voting rule remains robust against manipulation attempts, while
vulnerability implies that manipulation can be successful under certain
conditions.</p>
<p>The table (Table 7.3) in the text summarizes the complexity of
control problems for several prominent voting rules, indicating whether
each rule is immune, susceptible, vulnerable, or resistant to different
control types. Some rules, like Copelandα with 0 &lt; α &lt; 1, SP-AV,
fallback, Bucklin, and NRV, are resistant to all constructive control
types considered in the study. However, these rules may have
vulnerabilities when it comes to destructive control variants.</p>
<p>The text concludes by mentioning that while some natural voting rules
with P-time winner determination exist (resistant to most control
types), their practicality might be limited due to complexities in
understanding and implementing the rules. An alternative is to combine
well-known rules artificially to create resistant systems, although
these may not be appealing in practice. The study of control in voting
systems helps understand potential vulnerabilities and develop
strategies to ensure fair elections.</p>
<p>The Consensus-Based Framework for Rationalizing Voting Rules</p>
<p>The consensus-based approach to rationalizing voting rules is rooted
in the idea of reaching agreement among voters while minimizing changes
to their preferences. This framework involves two main components: a
notion of consensus (agreement) and a distance measure between
preference profiles. Here’s a detailed summary and explanation of the
key aspects:</p>
<ol type="1">
<li><p>Consensus Classes: A consensus class is a pair (X, w), where X is
a non-empty set of preference profiles over a candidate set A, and w is
a mapping that assigns a unique candidate to each profile in X. The
assigned candidate is called the consensus choice or winner. The
consensus class must be anonymous (identical under voter permutations)
and neutral (invariant under candidate renaming).</p>
<p>Examples of consensus classes include:</p>
<ul>
<li>Strong Unanimity (S): All voters report the same preference order;
the consensus choice is the top-ranked candidate.</li>
<li>Unanimity (U): Some candidate ranks first for all voters; the
consensus choice is that candidate.</li>
<li>Majority (M): More than half of the voters rank a common candidate
first; the consensus choice is that candidate.</li>
<li>Condorcet (C): A proﬁle with a Condorcet winner (a candidate who
beats all others in pairwise elections); the consensus choice is the
Condorcet winner.</li>
<li>Transitivity (T): Majority preferences form a transitive relation;
the consensus choice is the Condorcet winner.</li>
</ul></li>
<li><p>Distances: A distance on preference profiles measures the
magnitude of changes between two proﬁles. It satisfies non-negativity,
identity of indiscernibles, symmetry, and triangle inequality. For a
distance over individual votes (d), a profile distance (d^) can be
defined as d^((u1, …, un), (v1, …, vn)) = Σ(d(ui, vi)).</p>
<p>Examples of distances include:</p>
<ul>
<li>Discrete Distance: Measures the number of voters with different top
choices.</li>
<li>Swap Distance (Kendall Tau, Kemeny): Counts swaps of adjacent
candidates required to transform one profile into another.</li>
<li>Footrule Distance (Spearman): Sums absolute differences in candidate
positions across profiles.</li>
<li>Weighted Footrule Distance: Modifies the footrule distance by
assigning weights to positions.</li>
<li>ℓ∞-Sertel Distance: Measures the maximum position difference between
corresponding candidates in two profiles.</li>
<li>Edge Reversal (Pseudo)Distance: Counts edge reversals needed in
pairwise majority graphs to transform one proﬁle into another.</li>
</ul></li>
<li><p>Rationalization of Voting Rules: A voting rule can be
rationalized by identifying a consensus class and distance that explain
its behavior. For instance, Plurality can be explained using Unanimity
and Swap Distance, while Borda can be rationalized with Unanimity and
Footrule Distance. The Consensus-Based Framework is versatile, allowing
for the derivation of properties (e.g., monotonicity) from its
components and providing a systematic approach to constructing new
voting rules by combining known distances and consensus
classes.</p></li>
</ol>
<p>The text discusses two main approaches for rationalizing voting
rules: consensus-based and probabilistic (Maximum Likelihood Estimator
or MLE) methods.</p>
<ol type="1">
<li>Consensus-Based Approach: This approach, introduced by Elkind et
al. (2010a), focuses on the concept of distance rationalizability. It
involves defining a pseudo-distance (dins, dswap, ddiscr, dfr, dsert)
between preference profiles and using it to determine winners based on
consensus classes (C, S, U, M). The Kemeny rule, Dodgson rule,
Plurality, Borda rule, Copeland rule, and Maximin are rationalized
within this framework.</li>
</ol>
<p>The main advantage of the consensus-based approach is that it
provides a unified way to analyze various voting rules by defining a
pseudo-distance on preference profiles. However, as shown by Lerer and
Nitzan (1985), any voting rule can be distance rationalizable if we
don’t impose restrictions on the distance used, which makes this
framework too permissive for gaining insights into the properties of
specific rules.</p>
<ol start="2" type="1">
<li><p>Probabilistic Approach (Maximum Likelihood Estimator or MLE):
This approach, inspired by Condorcet’s probabilistic model and the
maximum likelihood estimation principle, represents voting rules as
maximum likelihood estimators (MLE) under specific noise models for
voters’ preferences. The MLE framework distinguishes between two main
types of MLE rules:</p>
<ol type="a">
<li><p>MLERIV Rules: These rules aim to estimate the most likely ranking
given a preference profile and then select winners based on this
estimated ranking. They are constructed by associating each voter’s
preference with a probability distribution and maximizing the likelihood
of the observed preference profile under these distributions. Examples
include the Kemeny rule, scoring rules, and refinements like MLE∞intr
and MLE1intr (Young’s interpretations of Condorcet’s proposal).</p></li>
<li><p>MLEWIV Rules: These rules estimate winners directly by maximizing
the likelihood of a candidate being the correct winner given their
position in each vote. Neutral MLEWIV rules are simply scoring rules, as
shown by Proposition 8.16 (Elkind et al., 2010b).</p></li>
</ol></li>
</ol>
<p>The probabilistic approach provides insights into the behavior of
voting rules under various noise models and allows for a more nuanced
understanding of how different factors (e.g., voter accuracy,
independence) affect rule outcomes. However, it also faces challenges in
characterizing all MLE rules axiomatically due to the complexity
introduced by noise models.</p>
<p>In summary, both consensus-based and probabilistic approaches offer
valuable perspectives on voting rule rationalization. The
consensus-based framework provides a unified way to analyze various
rules through distance rationalizability, while the probabilistic MLE
approach offers insights into how different noise models affect rule
outcomes and winner selection. Understanding these frameworks is
essential for studying the properties and behavior of election methods
in theoretical and applied contexts.</p>
<p>Sequential Voting in Combinatorial Domains:</p>
<p>Sequential voting is a method for preference aggregation in
combinatorial domains, where each variable (or issue) is addressed one
at a time. The process involves the following components:</p>
<ol type="1">
<li><p><strong>Order of Issues</strong>: An order O over the set of
variables X = {X1, …, Xp} determines the sequence in which voters
express their preferences for each variable. Without loss of generality,
we can assume O = X1 ▷ X2 ▷ … ▷ Xp.</p></li>
<li><p><strong>Local Voting Rules</strong>: For each issue i (i ≤ p),
there is a local voting rule ri that determines the outcome based on
voters’ preferences for that specific variable Di. These rules can be
resolute, where a clear winner is chosen, or irresolute, allowing for
ties or multiple outcomes.</p></li>
<li><p><strong>Sequential Voting Protocol (SeqO(r1, …, rp))</strong>:
This protocol outlines the steps of sequential voting:</p>
<ul>
<li>For each issue i in order O, ask voters to report their preferences
≻i_t over Di given the current values d1, …, dt-1 for previously decided
variables.</li>
<li>Collect all reported preferences and form a profile Pt = (&lt;≻1_t,
…, ≻n_t&gt;).</li>
<li>Apply the local voting rule ri to Pt, obtaining decision dt =
ri(Pt).</li>
<li>Communicate dt back to voters.</li>
</ul></li>
<li><p><strong>Voter’s Behavior</strong>: In each step t, voters provide
their preferences for issue Xt based on the current state of the other
variables (d1, …, dt-1). This introduces complexity because a preference
for one issue may depend on the results of previous issues. The
challenge lies in defining marginal or local preferences unambiguously
when they depend on undecided variables.</p></li>
<li><p><strong>O-Legality</strong>: To ensure voters can report their
preferences without ambiguity, the concept of O-legality is introduced.
A preference relation ≻ over A is O-legal if, given the order O = X1 ▷
X2 ▷ … ▷ Xp, voters’ preferences for each variable Xt only depend on the
current state (d1, …, dt-1) and not on future variables (Xt+1, …,
Xp).</p></li>
</ol>
<p>By following this sequential voting protocol, the method aims to
balance expressivity and cost in preference aggregation over
combinatorial domains. It allows voters to provide their preferences
issue by issue while maintaining a low communication burden. However,
designing appropriate local voting rules that handle dependency between
issues is crucial for its successful implementation.</p>
<p>This chapter, authored by Craig Boutilier and Jeffrey S. Rosenschein,
discusses incomplete information and communication requirements in
voting, focusing on methods for determining winners or making decisions
with partial preference knowledge. The key theme is the use of partial
preferences to reduce communication and informational burdens without
compromising decision quality.</p>
<p>10.2 Models of Partial Preferences: - <strong>Basic
Notation</strong>: Alternatives A = {a1, …, am} and voters N = {1, …,
n}. Each voter i has a preference order ≻i over A. - <strong>Partial
Votes/Profiles</strong>: Partial information πi (partial ordering) about
voter i’s preferences, with completions C(πi) being all complete votes
extending πi. - <strong>Probabilistic Preference Models</strong>:
Distributions over voter preferences, such as Impartial Culture (IC),
Impartial Anonymous Culture (IAC), Mallows φ-model, rifle independence
model, etc., used for probabilistic analysis of voting outcomes with
incomplete information.</p>
<p>10.3 Solution Concepts with Partial Preferences: - <strong>Possible
and Necessary Winners</strong>: - A is a possible winner under  if
there’s an R ∈C() such that f(R) = a. - A is a necessary winner under 
if f(R) = a for all R ∈C(). - Computation complexity varies with voting
rule and number of alternatives; generally NP-complete or coNP-complete
for various rules like STV, Condorcet, Borda, Copeland, maximin,
Bucklin, etc.</p>
<p>10.3.1 Possible Winners: - Sufficient partial information to
determine the winner (or rule out alternatives). - Related to
coalitional manipulation problem; if a is not a possible winner under ,
it cannot be manipulated by a constructive coalition.</p>
<p>10.3.2 Minimax Regret: - Measure the difference between the score of
alternative a and the optimal score in the worst case (given any
completion). - PMR(a, a’, ) = maxR∈C() [s(a’, R) - s(a, R)]; MMR() =
mina∈A MR(a, ), where MR(a, ) is the maximized regret under partial
profile . - Minimax optimal alternative may not be a possible winner but
provides a general method for selecting winners in incomplete
information scenarios.</p>
<p>10.3.3 Probabilistic Solution Concepts: - Utilize probabilistic
preference models to assess voting outcomes’ likelihood, considering
phenomena like Condorcet cycles, manipulation opportunities, and
expected loss under social welfare measures.</p>
<p>10.4 Communication and Query Complexity: - Formal models quantifying
the information needed in worst cases to determine winners using
specific voting rules. - Two varieties of models: 1. Communication
complexity (Yao, 1979; Kushilevitz and Nisan, 1996): Measures number of
bits communicated between voters and mechanism. * Upper bounds provided
by deterministic protocols (e.g., O(nm log m) for rank-based rules),
lower bounds using fooling sets technique. 2. Query complexity: Measures
the number of queries voters need to answer, sensitive to query form due
to varying information carried by different queries.</p>
<p>This chapter provides an overview of techniques and models for
dealing with incomplete preference information in voting scenarios,
enabling more efficient group decision-making while maintaining adequate
decision quality.</p>
<p>11.2 What Is a Resource Allocation Problem?</p>
<p>An allocation problem in the context of fair resource distribution
involves several key components that together define the economic
environment within which the allocation takes place. Here’s a detailed
explanation of these components:</p>
<ol type="1">
<li><p><strong>Set of Agents</strong>: This refers to the individuals or
entities participating in the allocation process. These can be
individual people, government agencies, firms, or other artificial
agents representing real-world entities. The number and characteristics
of these agents play a significant role in shaping the allocation
problem’s complexity and constraints.</p></li>
<li><p><strong>Resource Data</strong>: This component pertains to
unproduced endowments of goods that are available for distribution.
These goods can be either consumed directly or, when production
opportunities are defined, used as inputs in the production process. The
nature, quantity, and characteristics of these resources significantly
influence the allocation problem’s structure and potential
solutions.</p></li>
<li><p><strong>Production Opportunities</strong>: In some cases, these
unproduced endowments can be utilized not only for direct consumption
but also as inputs in the creation of new goods or services through
production processes. The availability and nature of these production
opportunities further expand the problem’s complexity by introducing
interdependencies among resources and potential outputs.</p></li>
<li><p><strong>Preferences</strong>: Each agent within the economy has a
set of preferences over the possible bundles of resources they might
receive as an allocation. These preferences can be complete, transitive,
and reflexive, or they may exhibit more complex structures like
satiation or diminishing marginal rates of substitution. Understanding
these preferences is crucial for assessing the fairness and efficiency
of potential allocations.</p></li>
<li><p><strong>Feasibility Constraints</strong>: These are the rules
governing which resource bundles can actually be produced or allocated
within the economy. They may include physical constraints (e.g., limited
availability of certain resources), technological constraints (e.g.,
production processes requiring specific inputs in particular ratios), or
social norms and regulations (e.g., prohibitions on allocating certain
resources to specific agents).</p></li>
<li><p><strong>Fairness Criteria</strong>: These are the principles that
guide the allocation process, ensuring that it aligns with notions of
fairness. Examples include Pareto efficiency (no agent can be made
better off without making someone else worse off), envy-freeness (no
agent prefers another’s bundle more than their own), proportionality
(agents receive a share proportional to some measure of their
entitlement or need), and various solidarity requirements (e.g., equal
opportunities, egalitarian-equivalence).</p></li>
</ol>
<p>By understanding these components—agents, resources, production
opportunities, preferences, feasibility constraints, and fairness
criteria—one can specify a resource allocation problem concretely within
an economic context. This specification allows for the application of
axiomatic approaches to fair allocation, as well as computational
methods for addressing various types of allocation problems.</p>
<p>This chapter of “The Theory of Fair Allocation” focuses on the fair
division of indivisible goods among agents with different preferences.
In this context, a resource is a set O = {o1, …, op} of objects (also
referred to as goods or items) that must be allocated whole and cannot
be divided or broken down into smaller parts. This assumption applies to
various real-world scenarios such as dividing physical objects like
houses or cars in divorce settlements, allocating courses or Earth
observation images among students, etc.</p>
<p>The chapter discusses several fair division problems under this
context:</p>
<ol type="1">
<li><p>Classical fair division problems: A social endowment of ℓ
indivisible goods needs to be distributed among a group N of agents,
each equipped with a preference relation ≿i over the commodity space
R^ℓ. Preferences satisfy classical assumptions like continuity,
monotonicity, and convexity.</p></li>
<li><p>Fair division problems with single-peaked preferences (Sprumont,
1991): A social endowment of a single commodity must be fully
distributed among a group N of agents with single-peaked preferences.
These preferences have a peak amount, p(≿i), up to which increasing
consumption increases welfare, and beyond that level, further increase
decreases welfare.</p></li>
<li><p>Claims problems (O’Neill, 1982): A social endowment of a single
good must be distributed among a group N of agents with incompatible
claims ci on it, where the total claim exceeds the endowment. Agents
have monotonic preferences. Typical applications include bankruptcy and
taxation problems.</p></li>
<li><p>Partitioning nonhomogeneous continua: A social endowment
consisting of an indivisible and nonhomogeneous continuum must be
partitioned among a group N of agents, with each agent i having
preferences ≿i over its measurable subsets. The base model assumes
monotonicity of preferences with respect to set inclusion.</p></li>
<li><p>Object allocation problems: A social endowment O of indivisible
goods has to be assigned to a group N of agents, where each agent i ∈N
can consume only one object and has preferences ≿i defined over O. This
is the base case for object allocation problems, which may involve
assigning offices, tasks, or other resources among individuals.</p></li>
<li><p>Objects-and-money allocation problems: An enriched version of
object allocation problems where each agent i ∈N can consume some amount
of money and one object. Consumptions of money may be unrestricted in
sign, or restricted to a lower bound (e.g., zero). Each agent has
preferences defined over R × O, perhaps [a, ∞[×O for some a ∈R.</p></li>
<li><p>Priority-augmented object allocation problems (Balinski and
S¨onmez, 1999; Abdulkadiro˘glu and S¨onmez, 2003): An extension of the
previous model where each object a ∈O is equipped with a priority order
πa over its possible recipients. Applications include school choice
problems, where objects are seats in schools, and priorities depend on
factors like sibling attendance, walk-zone proximity, waiting list
length, and academic records.</p></li>
<li><p>Matching agents to each other: A partition of the agent set into
two sets is made, with preferences over the agents in the component of
the partition to which they do not belong. The objective is to form
pairs containing one agent from each set. This can involve various
models like strict or indifferent preferences, one-to-one or
several-to-one matching, and even distribution of an infinitely
divisible good (e.g., money) among paired agents.</p></li>
</ol>
<p>Throughout the chapter, the authors emphasize the importance of
understanding fair allocation practices in real-world scenarios and
examining their desirable features as well as potential shortcomings.
They also introduce various axiomatic principles and concepts to define
fair solutions for these problems, such as no-envy,
egalitarian-equivalence, resource monotonicity, population monotonicity,
welfare dominance under preference replacement, consistency, and
model-specific requirements like duality in claims problems.</p>
<p>In the following chapters, this theoretical framework is applied to
solve specific fair division problems of indivisible goods using
algorithmic methods and axiomatic approaches, providing a comprehensive
treatment of the field.</p>
<p>The Adjusted Winner Procedure is an algorithm designed for fair
division problems involving two agents with additive utility functions.
It operates in two phases: the “winning phase” and the “adjusting
phase.”</p>
<ol type="1">
<li><p>Winning Phase:</p>
<ul>
<li>The items are allocated to each agent based on their individual
valuations, i.e., an item is given to the agent who values it the
most.</li>
<li>This results in a provisional allocation where potentially one agent
may have higher utility than the other.</li>
</ul></li>
<li><p>Adjusting Phase:</p>
<ul>
<li>If there’s a disparity in utilities after the winning phase, the
“adjusting” process begins. In this phase, items are transferred from
the richer agent (richest) to the poorer agent (poorest). The transfer
happens according to an increasing order of the ratio of their
valuations for each item.</li>
<li>The algorithm continues transferring items until either both agents
have equal utility or the richest agent becomes the poorest after a
transfer.</li>
</ul></li>
<li><p>Equitable Allocation:</p>
<ul>
<li>Once the disparity cannot be reduced further, the last transferred
item ‘g’ needs to be split between the two agents. The fraction of this
item allocated to the poorer agent is calculated as follows:</li>
</ul>
<p>[ = ]</p>
<ul>
<li>The rich agent receives the rest, ensuring that both agents end up
with equal utility.</li>
</ul></li>
</ol>
<p>Properties of the Adjusted Winner Procedure: - Equitable (Both agents
have the same utility at the end). - Envy-free (No agent envies
another’s allocation). - Pareto-optimal (There is no other allocation
where one agent could gain without causing harm to another).</p>
<p>This procedure provides a simple and efficient way for two agents to
divide items while ensuring fairness, even when the agents have
different valuations for the objects. It demonstrates that fair division
can be achieved through iterative processes involving direct comparisons
of individual utilities and adjustments based on those comparisons.</p>
<p>The text discusses various aspects of cake cutting algorithms, which
are used to divide a heterogeneous divisible resource among multiple
agents with different preferences. Here’s a summary and explanation of
the key points:</p>
<ol type="1">
<li><p><strong>Model</strong>: The problem involves n agents and a cake
represented by the interval [0, 1]. Each agent has a valuation function
Vi that assigns a value to any subinterval I ⊆[0, 1]. These functions
satisfy normalization (Vi(0, 1) = 1), divisibility, nonnegativity, and
additivity.</p></li>
<li><p><strong>Fairness properties</strong>: The fairness properties
considered are proportionality (each agent values their piece at least
1/n), envy-freeness (each agent prefers their own piece to any other
piece), and equitability (every two agents value their pieces
equally).</p></li>
<li><p><strong>Classic cake cutting algorithms</strong>:</p>
<ul>
<li><p><strong>Proportionality for n = 2: Cut and Choose</strong> -
Agent 1 cuts the cake into two equal-value pieces, and Agent 2 chooses
their preferred piece. This results in a proportional allocation (and
also envy-free).</p></li>
<li><p><strong>Proportionality for any n: Dubins-Spanier and
Even-Paz</strong> - These algorithms guarantee proportionality for any
number of agents. The Dubins-Spanier algorithm uses a continuously
moving knife, while the Even-Paz algorithm is more computationally
efficient, requiring O(n log n) queries in the Robertson-Webb
model.</p></li>
<li><p><strong>Envy-Freeness for n = 3: Selfridge-Conway</strong> - This
algorithm provides an envy-free allocation for three agents using a
series of cuts and selections by each agent.</p></li>
</ul></li>
<li><p><strong>Complexity of Cake Cutting</strong>: The computational
complexity of cake cutting is analyzed in terms of the number of queries
required to find a fair allocation. The Robertson-Webb model supports
two types of queries: evali(x, y) (asking agent i to evaluate interval
[x, y]) and cuti(x, α) (asking agent i to cut a piece of cake worth α
starting at x).</p>
<ul>
<li><p><strong>Lower Bound for Proportional Cake Cutting</strong>: Any
proportional cake-cutting algorithm requires Ω(n log n) queries in the
Robertson-Webb model (Edmonds and Pruhs, 2006b).</p></li>
<li><p><strong>The Complexity of Envy-Free Cake Cutting</strong>: While
there are finite envy-free algorithms for three agents
(Selfridge-Conway), extending these to any number of agents is
challenging. The Brams-Taylor algorithm has unbounded running time, and
current lower bounds show that envy-free cake cutting requires Ω(n^2)
queries in the Robertson-Webb model (Procaccia, 2009).</p></li>
</ul></li>
<li><p><strong>Optimal Cake Cutting</strong>: The text also covers
optimal fair allocations, focusing on piecewise constant valuation
functions with known parameters. Algorithms can compute envy-free and
equitable allocations efficiently by partitioning intervals between
marks reported by agents. However, when contiguous allocations are
required, the problem becomes NP-hard to approximate within a factor of
Ω(√n) (Bei et al., 2012).</p></li>
</ol>
<p>In summary, cake cutting algorithms deal with dividing a
heterogeneous resource among multiple agents with varying preferences.
Various fairness properties and complexity analyses are discussed, along
with specific algorithms for different numbers of agents. The challenge
lies in balancing fairness and computational efficiency, especially when
contiguous allocations are required.</p>
<p>The text provided discusses two types of matching problems under
preferences: bipartite with two-sided preferences (Section 14.2) and
one-sided preferences (Section 14.3).</p>
<p><strong>14.2 Two-Sided Preferences:</strong></p>
<p>This section focuses on the Hospitals/Residents problem (hr), which
is a bipartite matching problem where both hospitals and residents have
preferences over each other. The problem involves assigning residents to
hospitals while respecting capacity constraints of the hospitals.</p>
<ol type="1">
<li><strong>Introduction and Preliminary Deﬁnitions:</strong>
<ul>
<li>An instance I of hr consists of a set R of residents, a set H of
hospitals, acceptable pairs E ⊆ R × H, and capacities cj for each
hospital hj ∈ H.</li>
<li>Each resident ri ∈ R has a preference list ranking the acceptable
hospitals A(ri), and each hospital hj ∈ H has a preference list ranking
the acceptable residents A(hj).</li>
</ul></li>
<li><strong>Stable Matchings:</strong>
<ul>
<li>A matching M is stable if there’s no blocking pair (ri, hj) that
would improve both resident ri and hospital hj by forming a
partnership.</li>
<li>Stable matchings ensure that no resident-hospital pair can mutually
benefit from leaving their current assignment to form a new
partnership.</li>
</ul></li>
<li><strong>Gale-Shapley Algorithms:</strong>
<ul>
<li>The Resident-Oriented Gale-Shapley (RGS) algorithm involves
residents applying to hospitals in order of preference, with hospitals
accepting the best available candidates up to their capacity.</li>
<li>The Hospital-Oriented Gale-Shapley (HGS) algorithm has hospitals
offering posts to residents based on their preferences and
capacities.</li>
</ul></li>
<li><strong>Classical Results:</strong>
<ul>
<li>Every hr instance admits at least one stable matching, as proven by
Gale and Shapley (1962).</li>
<li>The RGS and HGS algorithms construct the unique resident-optimal and
hospital-optimal stable matchings, respectively, in O(m) time.</li>
</ul></li>
<li><strong>Strategic Results:</strong>
<ul>
<li>The Impossibility Theorem states that no mechanism can be both
stable and strategyproof (i.e., truthful reporting is a weakly dominant
strategy for all agents).</li>
<li>The RGS mechanism is strategyproof for residents, meaning that
truthfully reporting preferences is a weakly dominant strategy for
residents, while hospitals have no strategyproof mechanism.</li>
</ul></li>
</ol>
<p><strong>14.3 One-Sided Preferences:</strong></p>
<p>This section discusses the House Allocation problem (ha) and its
extension to Housing Markets (hm), where only applicants have
preferences over houses/items.</p>
<ol type="1">
<li><strong>Introduction and Preliminary Deﬁnitions:</strong>
<ul>
<li>An instance I of ha consists of a set A of applicants, a set H of
houses, and acceptable pairs E ⊆ A × H.</li>
<li>Each applicant ai ∈ A has a preference list ranking the acceptable
houses A(ai).</li>
</ul></li>
<li><strong>Pareto Optimality:</strong>
<ul>
<li>A matching M is Pareto optimal if no applicant can improve their
assignment without making another applicant worse off.</li>
<li>The Serial Dictatorship (SD) algorithm finds all Pareto optimal
matchings, assigning each applicant to their most-preferred available
house in a predetermined order.</li>
</ul></li>
<li><strong>Classical Results:</strong>
<ul>
<li>All Pareto optimal matchings can be constructed using the SD
algorithm, which runs in O(m) time.</li>
<li>The Random Serial Dictatorship (RSD) mechanism is a random version
of SD that produces a probability distribution over matchings, and
determining positive probabilities is #P-complete.</li>
</ul></li>
<li><strong>Housing Markets:</strong>
<ul>
<li>An instance I of hm includes an initial endowment M0, where each
applicant has one house initially.</li>
<li>The Top Trading Cycles (TTC) algorithm constructs a weak core
matching that can be implemented as the Core Mechanism for assigning
houses.</li>
</ul></li>
<li><strong>Strategic Results:</strong>
<ul>
<li>No mechanism can be both stable and strategyproof for housing
markets with strict preferences, as shown by Roth’s Impossibility
Theorem.</li>
</ul></li>
</ol>
<p>The Top Covering Algorithm (TCA) is designed to compute a core stable
partition for hedonic games that satisfy top responsiveness, as
discussed in Sections 15.3.4 and 15.4.3 of the text. Here’s a detailed
explanation of the algorithm:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: The TCA begins with an initial
partition π that is empty (π = ∅), and a set R1 containing all agents
N.</p></li>
<li><p><strong>Iterative process</strong>: For each round k from 1 to
|N| (the total number of agents):</p>
<ol type="a">
<li><p>Select agent i from the current set Rk such that the size of the
connected component CC(i, Rk) is less than or equal to the size of the
connected component CC(j, Rk) for every other agent j in Rk. This
selection ensures that the chosen coalition will be smaller or equal in
size to at least one other possible coalition.</p></li>
<li><p>Create a new coalition Sk by taking the connected component CC(i,
Rk). Add this coalition to the current partition π: π ←π ∪{Sk}.</p></li>
<li><p>Update the set of remaining agents Rk+1 by removing the newly
formed coalition Sk from Rk: Rk+1 ←Rk  Sk.</p></li>
</ol></li>
<li><p><strong>Termination</strong>: If there are no remaining agents
(i.e., Rk+1 = ∅), then the algorithm has found a core stable partition
and returns π. Otherwise, continue to the next round.</p></li>
<li><p><strong>Final output</strong>: Once all agents have been assigned
to coalitions, the TCA returns the partition π as the core stable
solution.</p></li>
</ol>
<p>The key idea behind the Top Covering Algorithm is to iteratively form
coalitions based on connected components in a graph where vertices
represent agents and edges indicate that two agents are neighbors (i.e.,
they prefer each other’s company). By always selecting an agent with the
smallest connected component, TCA ensures that the resulting partition
will be core stable for games satisfying top responsiveness.</p>
<p>The algorithm’s correctness and efficiency are supported by the
following facts:</p>
<ul>
<li>For top responsive preferences, TCA is strategyproof, meaning no
coalition of agents can benefit from misrepresenting their preferences
(Aziz and Brandl, 2012).</li>
<li>In addition to core stability, if the preference profile satisfies
certain natural constraints in addition to top responsiveness, TCA
returns a partition that satisfies stronger notions of stability (Aziz
and Brandl, 2012).</li>
</ul>
<p>The Top Covering Algorithm provides an efficient method for computing
core stable partitions in games with top responsive preferences. Its
performance is closely tied to the structure of connected components in
the underlying preference graph, making it well-suited for analyzing
hedonic games with specific preference structures like B-hedonic
games.</p>
<p>Weighted Voting Games are a class of cooperative games used to model
decision-making situations where a binary yes/no decision is made by a
set of voters, each with an assigned weight. The game’s value function
determines if a coalition (subset) of voters wins based on whether the
sum of their weights meets or exceeds a given quota. These games have
applications in various real-world scenarios, such as legislative bodies
and resource allocation problems.</p>
<p>The core, Shapley value, and Banzhaf index are key solution concepts
for cooperative games:</p>
<ol type="1">
<li>The Core: A set of imputations (payoff distributions) where no
coalition can improve its payoff by deviating from the imputation.</li>
<li>Shapley Value: A method to distribute the total value among players
based on their marginal contributions, measured as the average over all
possible orderings of players.</li>
<li>Banzhaf Index: A simpler measure of voting power that calculates the
probability a player can turn a losing coalition into a winning one by
joining it.</li>
</ol>
<p>The computational properties of weighted voting games reveal some
interesting results:</p>
<ol type="1">
<li>Veto Player problem: Identifying whether a given player is a veto
player (whose presence is necessary for any winning coalition) can be
solved in polynomial time.</li>
<li>Dummy Player problem: Determining if a player is a dummy (never
contributes to a winning coalition) is coNP-complete, meaning it’s
computationally hard unless P= NP. However, pseudopolynomial algorithms
exist for small weights.</li>
<li>Shapley Value and Banzhaf Index: Computing these indices in weighted
voting games is #P-complete and also pseudopolynomial when weights are
large or small, respectively.</li>
</ol>
<p>An important observation is that the weight assigned to a voter
doesn’t necessarily reflect their actual power (voting influence) within
the game. There can be instances where players with equal power have
significantly different weights and vice versa. This phenomenon, known
as “weighted voting doesn’t work,” has been observed in various
decision-making bodies like legislative systems.</p>
<p>Additionally, changing the quota or adding/dividing a player’s weight
can affect voting power unexpectedly:</p>
<ol type="1">
<li>Paradox of New Members: Adding players to a game may increase
existing players’ power instead of reducing it.</li>
<li>Paradox of Size: Splitting an existing player into multiple
identities with equal weights doesn’t necessarily preserve their
original power distribution. Instead, their combined power might
increase or decrease significantly, depending on the structure of the
game.</li>
</ol>
<p>These counterintuitive behaviors highlight the complex relationship
between voter weight and voting power in weighted voting games.</p>
<p>Judgment Aggregation (JA) is a formal framework used to model
collective decision-making processes where individual judgments about
propositional formulas are aggregated into a single, coherent set of
judgments. This concept was introduced by Christian List and Philip
Pettit in 2002 as a means to understand the challenges and potential
solutions for aggregating consistent judgments from multiple
individuals, particularly when dealing with interdependent
propositions.</p>
<p>The basic setup of JA involves:</p>
<ol type="1">
<li>An agenda () - a finite, nonempty subset of propositional formulas
that is closed under complementation and does not contain any doubly
negated formulas. The agenda represents the set of statements to be
judged by the group of individuals or “judges.”</li>
<li>A judgment set (J) - a subset of the agenda consisting of formulas
accepted by a judge, along with their complements if they are rejected.
Judgment sets are complete and consistent, meaning that for every
formula in the agenda, either it or its negation is included.</li>
<li>A profile - an n-tuple (n &gt; 1) of judgment sets, one for each
judge. The profile represents the individual assessments of the judges
regarding the formulas in the agenda.</li>
<li>A judgment aggregation rule (aggregator) - a function that maps
every profile into a single collective judgment set. This rule
aggregates the individual judgments to produce a coherent, possibly
complete and consistent, set of judgments for the group.</li>
</ol>
<p>Some crucial properties or axioms of an aggregator include:</p>
<ul>
<li>Unanimity: If all judges accept a formula ϕ, then the collective
decision also accepts it (ϕ ∈ f(J) if ϕ ∈ Ji for all i).</li>
<li>Anonymity: The collective decision is indifferent to permutations of
individual judges. In other words, the outcome depends only on the
pattern of acceptances and not on which judge made a particular
assessment (f(J) = f(Jπ), where π is any permutation of N).</li>
<li>Neutrality/Independence: The collective decision treats all formulas
symmetrically; if two formulas ϕ and ψ are accepted by the same set of
judges, then their collective acceptance depends only on that pattern (ϕ
∈ f(J) ⇔ ψ ∈ f(J) if N J ϕ = N J ψ).</li>
<li>Monotonicity: If a formula is accepted by an additional judge, it
should still be accepted in the collective decision.</li>
</ul>
<p>The List and Pettit impossibility theorem (2002) demonstrates that no
aggregator satisfying Anonymity, Neutrality/Independence, and
Independence can guarantee both Completeness and Consistency
simultaneously for any nontrivial agenda containing p, q, and ¬(p ∧q).
This highlights the fundamental challenge in JA – finding a reliable
aggregation method that ensures consistent collective decisions while
preserving essential properties like Anonymity, Neutrality/Independence,
and Monotonicity.</p>
<p>To address this impossibility, researchers have explored various
strategies:</p>
<ol type="1">
<li>Weakening or relaxing the axioms to find aggregators that provide
more lenient guarantees on consistency, completeness, or other desirable
properties.</li>
<li>Restricting the class of agendas for which consistent aggregation is
possible by imposing additional conditions or assumptions.</li>
<li>Introducing value restrictions on individual judgments, such as
requiring that no minimally inconsistent set contains two formulas
accepted by the same judge.</li>
<li>Employing alternative distance-based aggregation methods like the
Kemeny rule or Slater rule, which achieve consistency at the cost of
increased computational complexity.</li>
<li>Utilizing premise-based rules that partition the agenda into
premises and conclusions and ensure completeness and consistency under
specific conditions.</li>
</ol>
<p>These strategies help researchers better understand the possibilities
and limitations of judgment aggregation in various contexts, from legal
decision-making to artificial intelligence systems employing logical
reasoning for autonomous agents.</p>
<p>The chapter discusses the application of the axiomatic approach to
Internet-based multiagent systems, focusing on three illustrative
studies: graph ranking systems (PageRank algorithm), trust-based
recommendation systems, and multilevel marketing (affiliate
marketing).</p>
<ol type="1">
<li><p><strong>Graph Ranking Systems (PageRank Algorithm):</strong> The
classical theory of social choice deals with aggregating individual
rankings into a global or social ranking. In the Internet setting,
agents and alternatives coincide, leading to new axioms for graph
ranking systems. PageRank is a well-known algorithm used by search
engines like Google to rank web pages based on their importance,
determined by the structure of the link graph.</p>
<p>The authors present an axiomatic characterization of PageRank, which
involves defining a set of axioms that capture desired properties of a
ranking system for graphs. These axioms include:</p>
<ul>
<li><strong>Isomorphism:</strong> The ranking procedure should be
independent of vertex naming.</li>
<li><strong>Self Edge:</strong> If a vertex ranks another vertex higher
than itself without self-looping, adding a self-loop shouldn’t decrease
its rank relative to other vertices.</li>
<li><strong>Vote by Committee:</strong> Replacing direct links with
indirect ones through committees should maintain the relative
ranking.</li>
<li><strong>Collapsing:</strong> Collapsing identical vertices that have
disjoint predecessors and successors should preserve the relative
ranking of other vertices.</li>
<li><strong>Proxy:</strong> Distributing importance from multiple
sources through a proxy shouldn’t change the relative ranking of other
vertices.</li>
</ul>
<p>Proposition 18.8 states that PageRank satisfies all these axioms,
ensuring their soundness.</p></li>
<li><p><strong>Trust-Based Recommendation Systems:</strong> These
systems provide personalized rankings or recommendations to each agent
based on trust relationships and local rankings, instead of aggregating
preferences into a global ranking. Challenges include determining how to
aggregate both measures of trust and preference optimally. The axiomatic
approach can help characterize the desired properties of such systems
for design purposes.</p></li>
<li><p><strong>Multilevel Marketing (Affiliate Marketing):</strong> In
this form of marketing, products are sold through referrals generated by
previous customers, with credit offered in exchange for successful
referrals. The axiomatic approach can be applied to study the fair
distribution of credit among participants, addressing issues like how to
balance the contributions of direct and indirect referrers.</p></li>
</ol>
<p>The chapter emphasizes that while the “big data” approach is popular
in designing Internet systems due to available user-generated
information, it doesn’t provide a comprehensive tool for exploring the
vast design space. The axiomatic approach, combining formal speciﬁcation
of clear system requirements with implementation, offers an alternative
method for multiagent systems design on the Internet, augmented with
conceptual depth and evidence of its effectiveness in various
illustrative studies.</p>
<p>The text discusses knockout tournaments, a type of competition where
each match results in a single winner, and the loser is eliminated.
These tournaments are represented by binary trees, with players as
leaves and winners at internal nodes. The winner is determined
recursively based on matches between child nodes’ winners.</p>
<p>Properties of knockout tournaments include: 1. They are
Condorcet-consistent, meaning if a Condorcet winner (a candidate
preferred by a majority over all others) exists, they will win. 2.
Knockout tournaments are Smith-consistent; if candidates can be split
into two sets where each set’s members beat the other set’s members by
majority, then a candidate from the first set will always win regardless
of seeding. 3. They do not satisfy the reinforcement or participation
criteria (reinforcement ensures that if two groups elect the same winner
using subsets of voters, the entire group should also elect that winner;
participation states no voter has an incentive to abstain). 4. Knockout
tournaments are not neutral, as renaming candidates can change the
winner due to seeding dependence on candidate names. 5. They do not
guarantee Pareto-optimality when there are five or more candidates,
meaning there exists a voter profile where an alternative candidate is
unanimously preferred over the elected one.</p>
<p>Despite these drawbacks, knockout tournaments remain popular due to
their simplicity and efficiency (requiring no more matches than
players). They also discourage strategic losses since a single loss
eliminates a player.</p>
<p>The text also mentions an O(m^2) time algorithm for calculating the
probability of winning a knockout tournament given a probability matrix
P, where P[i, j] is the probability that i beats j (0 ≤ P[i, j] = 1 -
P[j, i] ≤ 1). This algorithm uses dynamic programming to compute q(j,
v), the probability that player j reaches tree node v of T. The base
case is q(j, S(j)) = 1, and for internal nodes with children r and l,
q(j, v) = q(j, r) * ∏ (1 - P[l, i]) if j is seeded at a descendant of
r.</p>
<p>The provided text discusses various aspects of agenda control in
knockout tournaments, focusing on different types of manipulation and
their complexity. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>General Agenda Control Problem</strong>: The problem
involves determining whether there exists a knockout tournament (T, S)
such that a given candidate i wins with probability at least p. This is
divided into four variants based on the control over tree T and seeding
S:</p>
<ul>
<li>Full Agenda Control: Chairman can modify both T and S.</li>
<li>P-Agenda Control: Chairman can modify only S, with probabilistic
input (a probability matrix P).</li>
<li>Deterministic Agenda Control: Similar to P-Agenda Control but with
deterministic input (tournament graph G).</li>
</ul></li>
<li><p><strong>Full Agenda Control</strong>: In a deterministic setting,
there’s a simple algorithm running in O(m) time that checks if the input
candidate i can reach all other nodes in the tournament graph G. This is
based on two claims:</p>
<ul>
<li>Claim 19.1: If i wins, then every node j is reachable from i in
G.</li>
<li>Claim 19.2: If all nodes are reachable from i, then there exists a
tree T and seeding S such that i wins.</li>
</ul></li>
<li><p><strong>P-Agenda Control</strong>: For the probabilistic case, Vu
et al. (2009a) showed that unbalanced tournaments with i only playing in
the last round are optimal, but no polynomial-time algorithm is known to
find this solution.</p></li>
<li><p><strong>Extensions and Further Reading</strong>:</p>
<ul>
<li>Real-world experiments by Russell (2010) show that agenda control
for generated tournaments can be easily solved under various
restrictions.</li>
<li>Vu et al. (2009b) analyze the performance of heuristics in solving
P-Agenda Control for both balanced and caterpillar voting trees using
real data from tennis and basketball.</li>
<li>Horen and Riezman (1985) study optimal seeding to maximize the
probability that the best player wins or improve the expected quality of
the winner in balanced knockout tournaments for m = 4 and m = 8.</li>
<li>Kr¨akel (2014) investigates game-theoretic scenarios where players
exert effort, aiming to maximize total expected effort or probability
that a strong player wins in four-player knockout tournaments.</li>
</ul></li>
<li><p><strong>Further Topics</strong>: Other extensions and further
readings include:</p>
<ul>
<li>Complexity of multiple elimination tournaments by Stanton and
Vassilevska Williams (2013).</li>
<li>Analysis of complex competition formats like the PGA TOUR’s FedExCup
by Connolly and Rendleman (2011) and the seeding and selection
efficiency in soccer world cup and Olympics by Pauly (2014).</li>
</ul></li>
</ol>
<p>The text concludes with acknowledgments to reviewers, editors, and
contributors who helped shape the content.</p>
<p>The provided text is a list of references related to various topics
in the field of social choice theory, preference aggregation, voting
systems, and coalition formation games. Here’s a detailed summary and
explanation of some key concepts and papers mentioned:</p>
<ol type="1">
<li><strong>Preference Aggregation and Voting Systems</strong>: Many
papers focus on aggregating individual preferences into a collective
decision, often through voting systems. Some notable works include:
<ul>
<li><em>Dagan (1996)</em>: A note on Thomson’s characterizations of the
uniform rule.</li>
<li><em>Cohen (1951)</em>: Introducing the concept of a social welfare
function.</li>
<li><em>Coppersmith, Fleischer, and Rudra (2006, 2010)</em>: Algorithms
for ordering weighted tournaments based on the number of wins.</li>
<li><em>Conitzer, Sandholm, and Tengin (2007)</em>: Complexity of
elections with few candidates hard to manipulate.</li>
</ul></li>
<li><strong>Condorcet Method</strong>: This is a voting system where
each voter ranks candidates, and the Condorcet winner is the candidate
who would beat every other candidate in pairwise comparisons. Some
papers discuss its properties and variants:
<ul>
<li><em>Debreu (1954)</em>: Representation of a preference ordering by a
numerical function.</li>
<li><em>Duggan and Le Breton (2001)</em>: Mixed re finements of
Shapley’s saddles and weak tournaments.</li>
</ul></li>
<li><strong>Coalition Formation Games</strong>: These games model
situations where agents form coalitions to achieve their goals. Papers
in this area include:
<ul>
<li><em>Drissi-Bakhkhat and Truchon (2004)</em>: Maximum likelihood
approach to vote aggregation with variable probabilities.</li>
<li><em>Dubins and Freedman (1981)</em>: Machiavelli and the
Gale-Shapley algorithm.</li>
</ul></li>
<li><strong>Manipulation and Strategyproofness</strong>: Many papers
investigate how agents can manipulate voting systems to achieve their
preferred outcomes, or the conditions under which a system is resistant
to manipulation:
<ul>
<li><em>Dubey and Shapley (1979)</em>: Mathematical properties of the
Banzhaf power index.</li>
<li><em>Duggan and Le Breton (2001)</em>: Strategic candidacy and voting
procedures.</li>
<li><em>Conitzer, Sandholm, and Tengin (2007)</em>: Complexity of
elections with few candidates hard to manipulate.</li>
</ul></li>
<li><strong>Fair Division</strong>: This area focuses on dividing
resources among agents while ensuring fairness. Papers include:
<ul>
<li><em>Dubins and Spanier (1961)</em>: How to cut a cake fairly.</li>
<li><em>Deng, Papadimitriou, and Safra (2003)</em>: On the complexity of
equilibria.</li>
</ul></li>
<li><strong>Multi-issue Domains</strong>: Some papers explore voting
systems in multi-issue domains, where each agent has preferences over
multiple issues:
<ul>
<li><em>Deegan, Katsirelos, Narodytska, and Walsh (2014)</em>:
Complexity of and algorithms for the manipulation of Borda, Nanson’s,
and Baldwin’s voting rules.</li>
</ul></li>
<li><strong>Judgment Aggregation</strong>: This is a field studying how
to combine individual judgments into collective judgments while
satisfying certain properties like consistency or independence:
<ul>
<li><em>Dietrich (2007)</em>: A generalised model of judgment
aggregation.</li>
<li><em>Dietrich and List (2004, 2007a, 2007b, 2010)</em>: Various works
on models of judgment aggregation, consistency, and stability.</li>
</ul></li>
<li><strong>Parameterized Complexity</strong>: Some papers apply
parameterized complexity theory to voting-related problems:
<ul>
<li><em>Downey and Fellows (1999, 2013)</em>: Parameterized
complexity.</li>
</ul></li>
</ol>
<p>These references provide a comprehensive overview of the
state-of-the-art in social choice theory, preference aggregation, voting
systems, coalition formation games, fair division, multi-issue domains,
judgment aggregation, and related topics.</p>
<p>The provided text is a list of references related to the field of
social choice theory, voting systems, and fair division of resources.
Here’s a summary of some key topics, authors, and papers mentioned:</p>
<ol type="1">
<li><p><strong>Voting Systems</strong>: The text covers various voting
rules such as Dodgson’s rule, Copeland’s rule, Borda count, Bucklin,
Approval voting, Schulze method, Range voting, Normalized Range Voting
(NRV), and Majority Judgment.</p></li>
<li><p><strong>Manipulation and Control</strong>: Many references focus
on the complexity of manipulation or control in different voting
systems. This includes works on bribery, strategic behavior, and
convergence to equilibria in plurality voting.</p></li>
<li><p><strong>Approximability and Inapproximability</strong>: Some
papers discuss the approximability and inapproximability results for
social welfare optimization in multiagent resource allocation, such as
stable matching problems.</p></li>
<li><p><strong>Fair Division</strong>: The text also covers fair
division problems, including the cake-cutting protocol, probabilistic
marriage problems, and probabilistic fair division with ordinal
preferences.</p></li>
<li><p><strong>Judgment Aggregation</strong>: Several references deal
with judgment aggregation, a method of combining individual judgments or
opinions into collective ones. Topics include distance-based models,
classics of social choice, and axioms for cooperative decision
making.</p></li>
<li><p><strong>Game Theory</strong>: The text includes papers on game
theory applications to voting systems, such as Nash equilibrium,
strategic behavior, and mechanism design.</p></li>
<li><p><strong>Complexity Theory</strong>: Many references touch upon
complexity theory, including fixed-parameter algorithms,
approximability, and inapproximability results for social welfare
optimization problems.</p></li>
<li><p><strong>Specific Authors and Papers</strong>: Some notable
authors mentioned include Arrow, Gibbard, Satterthwaite, Nanson, Moulin,
Myerson, Merrill, McKelvey &amp; Niemi, Meir et al., Mossel et al.,
Moulin &amp; Thomson, Munera et al., and others. Specific papers
include:</p>
<ul>
<li>Arrow (1950): “A Modification of the Condorcet Criterion”</li>
<li>Gibbard (1973): “Manipulating Votes”</li>
<li>Satterthwaite (1975): “Strategy-proofness and the Independence of
Irrelevant Alternatives Axiom”</li>
<li>Moulin (1988a): “Axioms of Cooperative Decision Making”</li>
<li>Moulin (2004): “Fair Division and Collective Welfare”</li>
<li>Meir et al. (2013): “Algorithms for Strategyproof
Classiﬁcation”</li>
<li>Narodytska &amp; Walsh (2013): “Manipulating Two Stage Voting
Rules”</li>
<li>Obraztsova et al. (2015a): “Analysis of Equilibria in Iterative
Voting Schemes”</li>
</ul></li>
</ol>
<p>This list provides a comprehensive overview of the literature on
voting systems, fair division, and related topics in social choice
theory and game theory.</p>
<p>The text provided is a list of terms, concepts, and theorems related
to social choice theory, cooperative game theory, and fair division.
Here’s a detailed summary and explanation of some key topics:</p>
<ol type="1">
<li><strong>Voting Rules</strong>:
<ul>
<li><strong>Approval Voting Rule (AV)</strong>: Each voter approves or
disapproves each candidate. The candidate with the most approvals
wins.</li>
<li><strong>Borda Count</strong>: Each voter ranks candidates, and
points are assigned based on rankings (e.g., 1st place gets n-1 points,
where n is the number of candidates). The candidate with the highest
total score wins.</li>
<li><strong>Condorcet Voting Rule</strong>: A candidate is a Condorcet
winner if they beat every other candidate in pairwise comparisons. If no
Condorcet winner exists, various methods (e.g., Copeland, Ranked Pairs)
are used to determine the winner.</li>
<li><strong>Copeland Voting Rule</strong>: A candidate beats another if
more voters rank them higher than the other candidate. The candidate
with the most “beats” wins.</li>
</ul></li>
<li><strong>Manipulation</strong>:
<ul>
<li><strong>Bribery</strong>: Influencing voters or candidates to change
their votes or preferences, respectively. This can occur in various
forms, such as priced bribery (voters pay money) or swap-bribery (voter
A swaps votes with voter B).</li>
<li><strong>Coalitional Manipulation</strong>: A group of voters
collectively manipulates the election outcome by coordinating their
votes.</li>
</ul></li>
<li><strong>Aggregators and Judgment Aggregation</strong>:
<ul>
<li>An aggregator takes individual judgments (e.g., preferences, binary
relations) as inputs and produces a single output (e.g., a social
welfare ordering).</li>
<li>Judgment aggregation is concerned with combining individuals’
judgments into collective judgments that satisfy certain axioms (e.g.,
unanimity, consistency).</li>
</ul></li>
<li><strong>Fair Division</strong>:
<ul>
<li>The cake division problem involves dividing a heterogeneous resource
among selfish agents with additive or single-peaked preferences.</li>
<li>Fairness criteria include proportionality, envy-freeness, and
equitability. Algorithms for fair division often involve procedures like
the Adjusted Winner, Last Diminisher, and the Envy-Free Cake Cutting
algorithm.</li>
</ul></li>
<li><strong>Game Theory</strong>:
<ul>
<li><strong>Cooperative Game</strong>: A game where players form
coalitions to maximize their collective payoff. Examples include the
core, Shapley value, and cooperative game with non-transferable
utility.</li>
<li><strong>Noncooperative Game</strong>: A game where players act
independently, aiming to maximize their individual payoffs. Examples
include strategic-form games and extensive-form games.</li>
</ul></li>
<li><strong>Axioms and Properties</strong>:
<ul>
<li><strong>Anonymity</strong>: The outcome of an election depends only
on voters’ preferences, not their identities.</li>
<li><strong>Neutrality/Independence of Irrelevant Alternatives
(IIA)</strong>: The ranking of candidates does not depend on the
presence or absence of other candidates.</li>
<li><strong>Monotonicity</strong>: If a candidate becomes more preferred
by some voters, their rank cannot decrease.</li>
<li><strong>Strategyproofness/Manipulability</strong>: Voters have no
incentive to misrepresent their preferences, as doing so would not
change the outcome (impossible) or might even harm their preferred
candidate (manipulable).</li>
</ul></li>
<li><strong>Complexity Theory</strong>:
<ul>
<li>The computational complexity of determining whether a given instance
of a problem (e.g., manipulation, control) has a solution is studied
using techniques from complexity theory.</li>
</ul></li>
<li><strong>Additional Concepts and Results</strong>:
<ul>
<li><strong>Agenda Control Problem</strong>: Determining the minimum
number of candidates or voters that need to be added, deleted, or
partitioned to ensure a desired outcome (e.g., a specific candidate
wins).</li>
<li><strong>Bribery Problem</strong>: The computational complexity of
finding the minimum amount of money needed to bribe voters to achieve a
desired outcome.</li>
<li><strong>Dichotomy Result</strong>: A result that distinguishes
between easy and hard cases for a given problem (e.g., control,
manipulation).</li>
<li><strong>Distance-based Aggregation</strong>: Combining judgments
based on their distance or similarity in some metric space.</li>
</ul></li>
</ol>
<p>The document provided is an index of terms related to social choice
theory, mechanism design, and game theory. Here’s a detailed summary and
explanation of some key concepts:</p>
<ol type="1">
<li><p><strong>Voting Rules</strong>: These are methods for collective
decision-making based on individual preferences. Examples include
Plurality Voting (choosing the candidate with the most votes), Instant
Runoff Voting (eliminating the least preferred candidate in each round
until a majority winner is found), and Approval Voting (voters can
approve multiple candidates, with the one approved by the most voters
winning).</p></li>
<li><p><strong>Tournament Solutions</strong>: These are methods for
ranking competitors based on pairwise comparisons. Examples include
Schulze Method and Kemeny’s Rule. They aim to find a ranking that is
transitive (if A beats B and B beats C, then A should beat C) and
Condorcet-winning if possible (a candidate who would win against every
other in pairwise comparisons).</p></li>
<li><p><strong>Stable Matching Problems</strong>: These are problems
where stable pairs (or groups) must be formed based on preferences of
multiple entities. The most famous example is the Stable Marriage
Problem, which finds a stable matching between two equally sized sets of
items (like students and colleges), each having their own preference
list.</p></li>
<li><p><strong>Manipulation</strong>: This refers to strategies used by
voters or candidates to influence the outcome of an election in their
favor by strategically misrepresenting their preferences.</p></li>
<li><p><strong>Judgment Aggregation</strong>: This involves combining
individual judgments (like “guilty” or “not guilty”) into a collective
judgment while satisfying certain axioms, such as non-dictatorship and
independence of irrelevant alternatives.</p></li>
<li><p><strong>Mechanism Design</strong>: This is the study of designing
rules for strategic agents to produce desirable outcomes. It often
involves creating incentives for truthful revelation of private
information.</p></li>
<li><p><strong>Core and Welfare</strong>: The core of a cooperative game
is a set of allocations where no coalition can improve upon their share
by deviating unilaterally or collectively. Welfare refers to the total
satisfaction or benefit achieved by all participants.</p></li>
<li><p><strong>Paradoxes</strong>: These are situations where different
outcomes seem equally reasonable based on individual preferences, yet a
societal choice rule leads to a contradictory or counterintuitive result
(e.g., Condorcet’s Paradox).</p></li>
<li><p><strong>Axioms</strong>: These are rules that a desirable voting
rule or mechanism should satisfy. Examples include Individual
Rationality (no participant is worse off under the outcome than they
would be under any other feasible outcome), Non-Dictatorship (no single
individual can always determine the outcome), and Independence of
Irrelevant Alternatives (the ranking between two candidates shouldn’t
change if we add or remove irrelevant alternatives).</p></li>
<li><p><strong>Computation Complexity</strong>: This field studies the
resources (like time, space) needed to solve computational problems. The
document mentions concepts like inapproximability (impossibility results
for approximation algorithms), parameterized complexity (studying the
complexity with respect to a specific parameter of the input), and PPAD
(a complexity class related to finding fixed points).</p></li>
</ol>
<p>This index is a rich source of information for researchers, students,
or anyone interested in understanding the theoretical foundations and
nuances of social choice theory, mechanism design, and game theory.</p>
<h3 id="fxtbook">fxtbook</h3>
<p>This chapter, “Bit Wizardry,” focuses on low-level algorithms related
to binary words (i.e., sequences of bits). It introduces various
functions for manipulating these binary representations, which are
essential in many computational tasks. Here’s a detailed explanation of
the key concepts discussed in this chapter:</p>
<p>1.1 Trivia</p>
<p>1.1.1 Little Endian vs Big Endian The order in which bytes within an
integer are stored in memory can differ between machine architectures.
In little-endian systems, the least significant byte comes first,
whereas big-endian systems store the most significant byte first. For
example, the hexadecimal value 0x0D0C0B0A would be stored as
follows:</p>
<ul>
<li>Big Endian: mem: 0D 0C 0B 0A</li>
<li>Little Endian: mem: 0A 0B 0C 0D</li>
</ul>
<p>This distinction is crucial when working with memory addresses that
increase from left to right. When casting pointers to char types, the
results will vary depending on the machine’s endianness. On a
little-endian machine, <em>(char </em>)(&amp;V) would yield the value
modulo 256 (0x0A), while on a big-endian machine, it yields the value
divided by 2^24 (0x0D).</p>
<p>For portable code, separate implementations for big-endian and
little-endian architectures are often required when dealing with
serialization or transferring data between systems. The C union type may
also necessitate distinct treatments depending on endianness.</p>
<p>1.1.2 Size of pointer is not size of int In programming for 32-bit
architectures, casting pointers to integers (and vice versa) often works
correctly because the sizes of int and long coincide. However, this
approach will fail on 64-bit machines where the size of a pointer
differs from that of an integer type. To avoid issues with different
machine architectures, it is advisable to cast pointers to larger types
or refrain from casting pointers to integers altogether for more
portable code.</p>
<p>1.1.3 Shifts and division With two’s complement arithmetic,
performing multiplication by a power of 2 corresponds to a left shift,
while division by a power of 2 is equivalent to a right shift for
unsigned types. For signed types, division by a power of 2 still results
in a right shift; however, the higher bits are filled with ones or zeros
depending on whether the original value was positive or negative. This
behavior corresponds to rounding towards negative infinity.</p>
<p>The compiler automatically optimizes these operations where possible.
Additionally, computing remainders modulo a power of 2 for unsigned
types can be achieved using bitwise AND operations (b % 32 == b &amp;
(32-1)).</p>
<p>These low-level optimizations are critical in various computational
tasks and should be well understood when working with binary
representations. The chapter also provides examples of assembly code for
x86 and AMD64 architectures, which can help readers understand how these
operations are implemented at the machine level.</p>
<p>The provided text discusses various bit manipulation techniques and
optimizations, primarily for use in low-level programming, such as C or
assembly language. Here’s a detailed summary and explanation of the key
points:</p>
<ol type="1">
<li><p><strong>Division by Constants</strong>: The text explains that
division by compile-time constants can be replaced with multiplication
and shifts during compilation to improve performance. For example,
<code>a / 10</code> is compiled into a sequence of instructions that
effectively multiply by a precomputed value (0xcccccccd) and then
perform a right shift by three bits. This optimization applies similarly
to the modulo operation when the modulus is a compile-time
constant.</p></li>
<li><p><strong>Pitfalls</strong>: The text highlights two pitfalls
related to bit manipulation in C:</p>
<ul>
<li><p><strong>Two’s Complement Pitfall</strong>: In two’s complement
representation, the most negative value (all bits set except the sign
bit) is equal to its negative. This can lead to unexpected behavior if
not accounted for, such as <code>if(x &lt; 0)</code>, where x could be
this special value.</p></li>
<li><p><strong>Shift Pitfall</strong>: The C standard does not define
the behavior of a right shift on signed integers when the shift count
exceeds the number of bits in the integer type (BITS_PER_LONG). Some
compilers may produce incorrect results if they implement a right shift
as a logical shift, which could lead to undefined behavior.</p></li>
</ul></li>
<li><p><strong>Bit Manipulation Functions</strong>: The text provides
several functions for bit manipulation:</p>
<ul>
<li><p><strong>Testing, Setting, and Deleting Bits</strong>: Functions
like <code>test_bit</code>, <code>set_bit</code>,
<code>clear_bit</code>, and <code>change_bit</code> allow for checking,
setting, or clearing individual bits within a word (long integer). These
functions are straightforward and assume that the indices used do not
exceed BITS_PER_LONG.</p></li>
<li><p><strong>Copying a Bit</strong>: The <code>copy_bit</code>
function copies a bit from one position to another in a word using XOR
operations. A more complex version, <code>mask_copy_bit</code>, allows
copying bits based on mask patterns.</p></li>
<li><p><strong>Swapping Two Bits</strong>: Functions like
<code>bit_swap</code> and <code>bit_swap_01</code> swap the values of
two specific bits within a word.</p></li>
</ul></li>
<li><p><strong>Operations on Low Bits or Blocks of a Word</strong>:
These functions focus on manipulating the lowest set bit (lowest one) or
blocks of ones/zeros at the low end of a word:</p>
<ul>
<li><p><strong>Isolating, Setting, and Deleting the Lowest One</strong>:
Functions like <code>lowest_one</code>, <code>clear_lowest_one</code>,
and <code>set_lowest_zero</code> isolate, clear, or set the lowest set
bit in a word.</p></li>
<li><p><strong>Computing the Index of the Lowest One</strong>: The
<code>lowest_one_idx</code> function calculates the index (position) of
the lowest set bit using an algorithm involving multiple AND operations
with shifted versions of the input word. An alternative, assembler-based
implementation (<code>asm_bsf</code>) is also provided for platforms
that support it.</p></li>
<li><p><strong>Isolating Blocks of Zeros or Ones at the Low
End</strong>: Functions like <code>low_ones</code>,
<code>low_zeros</code>, and <code>lowest_block</code> isolate blocks of
zeros or ones near the low end of a word.</p></li>
</ul></li>
<li><p><strong>Creating Transitions at the Lowest One</strong>: The
<code>lowest_one_10edge</code> and <code>lowest_one_01edge</code>
functions create rising or falling edges, respectively, at the position
of the lowest set bit in a word.</p></li>
<li><p><strong>Extraction of Ones, Zeros, or Blocks Near
Transitions</strong>: These functions, like <code>low_match</code>,
isolate values near transitions (places where adjacent bits have
different values) within two input words.</p></li>
<li><p><strong>Creating Blocks of Ones</strong>: The
<code>bit_block</code> and <code>cyclic_bit_block</code> functions
generate bit blocks of a specified length starting at a given position
within a word, with the latter allowing for wrapping around the word
boundary if necessary.</p></li>
<li><p><strong>Optimization Considerations</strong>: The text emphasizes
the importance of proper documentation, benchmarking, and understanding
platform-specific optimizations when working with low-level code. It
also warns against deleting unoptimized versions of critical functions
without keeping them as a fallback in case of issues during porting or
under specific conditions (e.g., extreme register pressure).</p></li>
</ol>
<p>These techniques and functions are useful for optimizing performance,
manipulating bit-level data, and understanding low-level programming
concepts. They can be applied in various domains, such as embedded
systems, cryptography, and computer graphics, where efficient
manipulation of individual bits or groups of bits is essential.</p>
<p>The provided text discusses various bit manipulation techniques and
functions, primarily focusing on C/C++ implementations. Here’s a
detailed summary and explanation of the key concepts:</p>
<ol type="1">
<li><p><strong>Bit Manipulation Functions:</strong></p>
<ul>
<li><strong><code>single_ones(ulong x)</code></strong>: Returns a word
with only the isolated ones (set bits) of <code>x</code> set. This is
achieved by bitwise ANDing <code>x</code> with its right-shifted version
and then with its left-shifted version, followed by an OR
operation.</li>
<li><strong><code>single_zeros_xi(ulong x)</code></strong>: Returns a
word with only the isolated zeros (unset bits) of <code>x</code> set. It
uses the <code>single_ones()</code> function by passing the bitwise NOT
of <code>x</code>.</li>
<li><strong><code>single_zeros(ulong x)</code></strong>: Returns a word
with only the isolated zeros of <code>x</code> set. It’s similar to
<code>single_zeros_xi()</code>, but it assumes that bits outside the
word are zero, so it directly uses bitwise operations without
considering them.</li>
<li><strong><code>single_values(ulong x)</code></strong>: Returns a word
where only the isolated ones and zeros of <code>x</code> are set. This
is done by XORing <code>x</code> with its left-shifted version and then
ANDing the result with the XOR of <code>x</code> and its right-shifted
version.</li>
<li><strong><code>single_values_xi(ulong x)</code></strong>: Similar to
<code>single_values()</code>, but it ignores outside values, meaning it
assumes all bits outside the word are zero.</li>
<li><strong><code>border_ones(ulong x)</code></strong>: Returns a word
where only those ones of <code>x</code> are set that lie next to a zero.
This is achieved by bitwise ANDing <code>x</code> with the bitwise OR of
its left-shifted and right-shifted versions, followed by a bitwise NOT
operation.</li>
<li><strong><code>border_values(ulong x)</code></strong>: Returns a word
where those bits of <code>x</code> are set that lie on a transition
(change from 0 to 1 or vice versa). This is done by XORing
<code>x</code> with its left-shifted version and then ORing the result
with the XOR of <code>x</code> and its right-shifted version.</li>
<li><strong><code>high_border_ones(ulong x)</code></strong>: Returns a
word where only those ones of <code>x</code> are set that lie right to
(next lower bin) a zero. This is achieved by bitwise ANDing
<code>x</code> with the result of XORing <code>x</code> with its
right-shifted version.</li>
<li><strong><code>low_border_ones(ulong x)</code></strong>: Returns a
word where only those ones of <code>x</code> are set that lie left to
(next higher bin) a zero. This is done by bitwise ANDing <code>x</code>
with the result of XORing <code>x</code> with its left-shifted
version.</li>
<li><strong><code>block_border_ones(ulong x)</code></strong>: Returns a
word where only those ones of <code>x</code> are set that are at the
border of a block of at least 2 bits. This is achieved by bitwise ANDing
<code>x</code> with the result of XORing its left-shifted and
right-shifted versions.</li>
<li><strong><code>low_block_border_ones(ulong x)</code></strong>:
Returns a word where only those bits of <code>x</code> are set that are
at the left of a border of a block of at least 2 bits. This is done by
first applying <code>block_border_ones()</code> and then ANDing the
result with <code>x</code> right-shifted by one.</li>
<li><strong><code>high_block_border_ones(ulong x)</code></strong>:
Returns a word where only those bits of <code>x</code> are set that are
at the right of a border of a block of at least 2 bits. This is done by
first applying <code>block_border_ones()</code> and then ANDing the
result with <code>x</code> left-shifted by one.</li>
<li><strong><code>block_ones(ulong x)</code></strong>: Returns a word
where only those bits of <code>x</code> are set that are part of a block
of at least 2 bits. This is achieved by bitwise ANDing <code>x</code>
with the result of ORing its left-shifted and right-shifted
versions.</li>
</ul></li>
<li><p><strong>Computing the Index of a Single Set Bit:</strong></p>
<ul>
<li><strong>Cohen’s Trick</strong>: Uses a modulus <code>m</code> such
that all powers of 2 are different modulo <code>m</code>. A table
<code>mt[]</code> is created with size <code>m</code>, where
<code>mt[(2^j) mod m] = j</code> for <code>j &gt; 0</code>. The index of
the lowest set bit in <code>x</code> can be found by reducing
<code>x</code> modulo <code>m</code> and then looking up the result in
<code>mt[].</code></li>
<li><strong>De Bruijn Sequence</strong>: Uses a binary De Bruijn
sequence of size <code>N</code>. A table <code>dbt[]</code> is created
such that the entry with index <code>wi</code> points to <code>i</code>.
The index of the lowest set bit in <code>x</code> can be found by
multiplying <code>x</code> with the De Bruijn sequence, right-shifting
the result, and then looking up the result in <code>dbt[].</code></li>
<li><strong>Floating-Point Numbers</strong>: This method converts an
integer into a floating-point number, reads off the position of the
highest set bit from the exponent, and isolates the lowest bit before
conversion. However, this technique is slow and machine-dependent.</li>
</ul></li>
<li><p><strong>Operations on High Bits or Blocks of a Word:</strong></p>
<ul>
<li><strong>Isolating the Highest One and Finding Its Index</strong>:
The highest set bit in a word can be isolated using various methods,
including a bit-scan instruction if available or auxiliary functions
like <code>highest_one_01edge()</code> and
<code>highest_one_idx()</code>.</li>
<li><strong>Isolating the Highest Block of Ones or Zeros</strong>:
Functions like <code>high_zeros(ulong x)</code> and
<code>high_ones(ulong x)</code> isolate the highest block of zeros or
ones, respectively. These functions use bitwise OR operations combined
with right shifts to set the corresponding bits in the result.</li>
</ul></li>
<li><p><strong>Functions Related to the Base-2 Logarithm:</strong></p>
<ul>
<li><strong><code>ld(ulong x)</code></strong>: Returns the floor value
of the base-2 logarithm of <code>x</code>, i.e., the largest integer
<code>k</code> such that <code>2^k &lt;= x &lt; 2^(k+1)</code>.</li>
<li><strong><code>one_bit_q(ulong x)</code> and
<code>is_pow_of_2(ulong x)</code></strong>: Determine whether
<code>x</code> is a power of two (including zero).</li>
<li><strong><code>next_pow_of_2(ulong x)</code> and
<code>next_exp_of_2(ulong x)</code></strong>: Return the next power of
two or exponent, respectively, greater than or equal to <code>x</code>.
These functions are useful in algorithms involving powers of two, such
as Fast Fourier Transform (FFT) with lengths restricted to powers of
two.</li>
</ul></li>
<li><p><strong>Counting the Bits and Blocks of a Word:</strong></p>
<ul>
<li><strong><code>bit_count(ulong x)</code></strong>: Counts the number
of set bits (ones) in <code>x</code> using a bitwise search via masks,
requiring O(log2(BITS PER LONG)) operations. The function uses multiple
AND and OR operations with specific masks to count bits in groups of 2,
4, 8, 16, 32, and 64 bits, respectively.</li>
</ul></li>
</ol>
<p>These functions and techniques provide a comprehensive set of tools
for bit manipulation and analysis, which can be useful in various
applications, such as image processing, data compression, and algorithms
involving binary representations.</p>
<p>The provided text discusses various methods for counting bits in a
binary word, optimizing these methods for specific scenarios, and using
bit manipulation techniques. Here’s a detailed summary and explanation
of the key concepts:</p>
<ol type="1">
<li><p><strong>Bit Counting Methods</strong>: The text presents several
algorithms for counting the number of set (i.e., 1) bits in a binary
word. These methods vary in their efficiency depending on the average
number of set bits in the input data.</p>
<ul>
<li><p><strong>Sparse Counting</strong> (<code>bit_count_sparse</code>):
This method is efficient when the input word has only a few set bits. It
iteratively clears the least significant set bit and increments a
counter until no more set bits are left.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> bit_count_sparse<span class="op">(</span><span class="ex">ulong</span> x<span class="op">)</span> <span class="op">{</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ulong</span> n <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>x<span class="op">)</span> <span class="op">{</span> <span class="op">++</span>n<span class="op">;</span> x <span class="op">&amp;=</span> <span class="op">(</span>x<span class="op">-</span><span class="dv">1</span><span class="op">);</span> <span class="op">}</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Dense Counting</strong> (<code>bit_count_dense</code>):
This method is used when the input word has many set bits. It first
applies sparse counting to the complement of the input word and
subtracts the result from the total number of bits (i.e.,
<code>BITS_PER_LONG</code>).</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> bit_count_dense<span class="op">(</span><span class="ex">ulong</span> x<span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> BITS_PER_LONG <span class="op">-</span> bit_count_sparse<span class="op">(~</span>x<span class="op">);</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Specialized Counting</strong> (<code>bit_count_3</code>,
<code>bit_count_15</code>): These methods are optimized for words with a
limited number of set bits. For example, <code>bit_count_3</code> is
designed for words with up to 3 set bits and uses a combination of
bitwise operations and multiplication by a magic constant.</p></li>
</ul></li>
<li><p><strong>Bit Block Counting</strong>: The text introduces
functions for counting the number of bit blocks (contiguous sequences of
set bits) in a binary word. For instance, <code>bit_block_count</code>
returns the total number of bit blocks, while
<code>bit_block_ge2_count</code> counts only those with two or more set
bits.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> bit_block_count<span class="op">(</span><span class="ex">ulong</span> x<span class="op">)</span> <span class="op">{</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>x <span class="op">&amp;</span> <span class="dv">1</span><span class="op">)</span> <span class="op">+</span> bit_count<span class="op">((</span>x<span class="op">^(</span>x<span class="op">&gt;&gt;</span><span class="dv">1</span><span class="op">))</span> <span class="op">/</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> bit_block_ge2_count<span class="op">(</span><span class="ex">ulong</span> x<span class="op">)</span> <span class="op">{</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bit_block_count<span class="op">(</span>x <span class="op">&amp;</span> <span class="op">((</span>x<span class="op">&lt;&lt;</span><span class="dv">1</span><span class="op">)&amp;(</span>x<span class="op">&gt;&gt;</span><span class="dv">1</span><span class="op">)));</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Vertical Addition</strong>: This technique optimizes bit
counting by processing multiple words simultaneously using a “vertical”
addition approach, which reduces the number of required bitwise
operations compared to a straightforward loop. The provided example
demonstrates this method for counting bits in an array of binary
words.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> bit_count_v<span class="op">(</span><span class="at">const</span> <span class="ex">ulong</span> <span class="op">*</span>x<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ulong</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="ex">ulong</span> <span class="op">*</span>xe <span class="op">=</span> x <span class="op">+</span> n <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>x<span class="op">+</span><span class="dv">15</span> <span class="op">&lt;</span> xe<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        b <span class="op">+=</span> bit_count_v15<span class="op">(</span>x<span class="op">);</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> <span class="dv">15</span><span class="op">;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Process remaining elements...</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Bitwise Operations for Set Testing</strong>: The text
discusses using bitwise operations to test whether a given word
<code>u</code> is a subset of another word <code>e</code> (i.e., whether
all set bits in <code>u</code> are also set in <code>e</code>). This can
be done efficiently using AND, OR, and NOT operations.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="dt">bool</span> is_subset<span class="op">(</span><span class="ex">ulong</span> u<span class="op">,</span> <span class="ex">ulong</span> e<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>u <span class="op">&amp;</span> e<span class="op">)</span> <span class="op">==</span> u<span class="op">;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Bit Indexing</strong>: The text presents a method for
finding the index of the i-th set bit in a binary word <code>x</code>.
This is achieved by iteratively narrowing down the search range using
bitwise operations and multiplication by magic constants.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> <span class="ex">ulong</span> ith_one_idx<span class="op">(</span><span class="ex">ulong</span> x<span class="op">,</span> <span class="ex">ulong</span> i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ... (implementation details omitted for brevity)</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
</ol>
<p>In summary, the text covers various bit manipulation techniques
tailored to different scenarios, optimizing performance by leveraging
properties of binary representations and employing efficient algorithms.
These methods find applications in areas such as data compression,
cryptography, and parallel processing.</p>
<p>This text discusses several topics related to bit manipulation,
focusing on branchless algorithms and binary necklaces. Let’s break it
down into sections for clarity:</p>
<h3 id="avoiding-branches">Avoiding Branches</h3>
<p>The text begins by explaining how branches can be expensive
operations, particularly on CPUs with long pipelines. To mitigate this,
the author suggests replacing certain branch conditions with branchless
alternatives. For example, instead of:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span> <span class="op">(</span>x<span class="op">&lt;</span><span class="dv">0</span><span class="op">)</span> <span class="op">||</span> <span class="op">(</span>x<span class="op">&gt;</span>m<span class="op">)</span> <span class="op">)</span> <span class="op">{</span> <span class="op">...</span> <span class="op">}</span></span></code></pre></div>
<p>You could use:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span> <span class="op">(</span><span class="dt">unsigned</span><span class="op">)</span>x <span class="op">&gt;</span> m <span class="op">)</span> <span class="op">{</span> <span class="op">...</span> <span class="op">}</span></span></code></pre></div>
<p>If <code>m</code> is a power of 2, an even more efficient approach is
to use:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span> <span class="op">(</span> <span class="op">(</span>ulong<span class="op">)</span>x <span class="op">|</span> <span class="op">(</span>ulong<span class="op">)</span>y <span class="op">)</span> <span class="op">&gt;</span> <span class="op">(</span><span class="dt">unsigned</span><span class="op">)</span>m <span class="op">)</span> <span class="op">{</span> <span class="op">...</span> <span class="op">}</span></span></code></pre></div>
<p>The text also provides examples of branchless functions for computing
maximum and minimum values. For instance, the <code>max0</code> function
returns <code>x</code> if it’s non-negative or zero otherwise:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="kw">inline</span> <span class="dt">long</span> max0<span class="op">(</span><span class="dt">long</span> x<span class="op">)</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">&amp;</span> <span class="op">~(</span>x <span class="op">&gt;&gt;</span> <span class="op">(</span>BITS_PER_LONG <span class="op">-</span> <span class="dv">1</span><span class="op">));</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This works by leveraging arithmetic right shift and bitwise AND
operations to clear all bits when <code>x</code> is negative. The
complementary function, <code>min0</code>, returns zero for positive
inputs:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="kw">inline</span> <span class="dt">long</span> min0<span class="op">(</span><span class="dt">long</span> x<span class="op">)</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">&amp;</span> <span class="op">(</span>x <span class="op">&gt;&gt;</span> <span class="op">(</span>BITS_PER_LONG <span class="op">-</span> <span class="dv">1</span><span class="op">));</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<h3 id="bit-wise-rotation-of-a-word">Bit-wise Rotation of a Word</h3>
<p>The text discusses bit-wise rotation functions, which are not
natively supported in C or C++. These operations can be emulated using
bitwise operators. For example, left rotation by <code>r</code> bits can
be achieved with:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="kw">inline</span> ulong bit_rotate_left<span class="op">(</span>ulong x<span class="op">,</span> ulong r<span class="op">)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">(</span>x <span class="op">&lt;&lt;</span> r<span class="op">)</span> <span class="op">|</span> <span class="op">(</span>x <span class="op">&gt;&gt;</span> <span class="op">(</span>BITS_PER_LONG <span class="op">-</span> r<span class="op">));</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Right rotation is similar but with the arguments swapped. If your CPU
supports it, you can use an assembler instruction for better
performance:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="kw">inline</span> ulong bit_rotate_right<span class="op">(</span>ulong x<span class="op">,</span> ulong r<span class="op">)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="pp">#if defined BITS_USE_ASM</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> asm_ror<span class="op">(</span>x<span class="op">,</span> r<span class="op">);</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="pp">#else</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">(</span>x <span class="op">&gt;&gt;</span> r<span class="op">)</span> <span class="op">|</span> <span class="op">(</span>x <span class="op">&lt;&lt;</span> <span class="op">(</span>BITS_PER_LONG <span class="op">-</span> r<span class="op">));</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="pp">#endif</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Here’s an example of rotation using only a part of the word
length:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="kw">inline</span> ulong bit_rotate_left<span class="op">(</span>ulong x<span class="op">,</span> ulong r<span class="op">,</span> ulong ldn<span class="op">)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    ulong m <span class="op">=</span> <span class="op">~</span><span class="dv">0</span><span class="bu">UL</span> <span class="op">&gt;&gt;</span> <span class="op">(</span> BITS_PER_LONG <span class="op">-</span> ldn <span class="op">);</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">&amp;=</span> m<span class="op">;</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="op">(</span>x <span class="op">&lt;&lt;</span> r<span class="op">)</span> <span class="op">|</span> <span class="op">(</span>x <span class="op">&gt;&gt;</span> <span class="op">(</span>ldn <span class="op">-</span> r<span class="op">));</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">&amp;=</span> m<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">;</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Finally, the text provides functions for signed rotation:
<code>bit_rotate_sgn</code>.</p>
<h3 id="binary-necklaces">Binary Necklaces</h3>
<p>This section discusses binary necklaces, which are cyclic
permutations of bit words. The author presents several related functions
and a class to generate these necklaces. Here’s an overview:</p>
<h4 id="cyclic-matching-minimum-and-maximum">Cyclic Matching, Minimum,
and Maximum</h4>
<p>The <code>bit_cyclic_match</code> function checks how often the
second argument needs to be rotated right to match the first. It returns
the rotation count or ~0UL if no match is found within the word length.
The function can also be adapted to work with a specified number of
leading bits (<code>ldn</code>).</p>
<p>The <code>bit_cyclic_min</code> function computes the cyclic minimum
(i.e., the lexicographically smallest cyclic permutation) of a given bit
word.</p>
<h4 id="cyclic-period-and-binary-necklaces">Cyclic Period and Binary
Necklaces</h4>
<p>The <code>bit_cyclic_period</code> function calculates the minimal
positive rotation that transforms a word into itself, which corresponds
to the period of the binary necklace. The implementation uses efficient
methods to avoid unnecessary rotations and checks only for periods that
are divisors of the word length.</p>
<h4 id="generating-all-binary-necklaces">Generating All Binary
Necklaces</h4>
<p>The author presents a class, <code>bit_necklace</code>, for
generating all binary necklaces using the FKM algorithm. This class has
properties like the necklace itself (<code>a_</code>), its period
(<code>j_</code>), and the number of bits in the words
(<code>n_</code>). The <code>next()</code> method generates successive
necklaces, while <code>is_lyndon_word()</code> checks if a necklace is a
Lyndon word.</p>
<h3 id="computing-cyclic-distance">Computing Cyclic Distance</h3>
<p>The <code>bit_cyclic_dist</code> function computes the minimal bit
count of <code>(t ^ b)</code> where <code>t</code> ranges over all
cyclic rotations of <code>a</code>. If the arguments are cyclic shifts
of each other, it returns zero. A version for partial words is also
provided.</p>
<p>In summary, this text delves into various techniques for efficient
bit manipulation, focusing on branchless algorithms and binary necklace
generation. These methods can be valuable in performance-critical
applications where minimizing branch mispredictions and optimizing
bitwise operations are essential.</p>
<p>The text discusses various operations related to binary words (i.e.,
integers represented in binary format) in the context of bit wizardry, a
field focused on efficient manipulation of bits within these words.
Here’s a detailed summary and explanation of the key concepts and
functions presented:</p>
<ol type="1">
<li>Cyclic XOR and Inverse (Section 1.13.5):
<ul>
<li><code>bit_cyclic_rxor(ulong x)</code>: This function performs a
cyclic XOR operation on an input word <code>x</code>. The result is
obtained by XORing the original word with its right-rotated version by
one bit. The number of set bits (ones) in the resulting word will be
even, and it will be the same as the complement of the original
word.</li>
<li><code>bit_cyclic_inv_rxor(ulong x)</code>: This is the inverse
function of <code>bit_cyclic_rxor()</code>. It takes an input word
<code>x</code> with an even number of set bits, then returns a word
<code>v</code> such that applying <code>bit_cyclic_rxor()</code> to
<code>v</code> will yield the original <code>x</code>.</li>
</ul></li>
<li>Reversing Bits (Section 1.14):
<ul>
<li>Bit reversal is the process of swapping the positions of the bits in
a binary word, resulting in a new order where the least significant bit
becomes the most significant, and vice versa.</li>
<li>Various auxiliary functions are provided to help reverse adjacent
blocks or groups of bits within a word efficiently:
<ol type="1">
<li><code>bit_swap_1(ulong x)</code>: Swaps pairs of adjacent bits.</li>
<li><code>bit_swap_2(ulong x)</code>, <code>bit_swap_4(ulong x)</code>,
and <code>bit_swap_8(ulong x)</code>: Swap groups of 2, 4, or 8 bits,
respectively.</li>
<li><code>bit_swap_16(ulong x)</code>: Swaps groups of 16 bits (for
32-bit architectures).</li>
</ol></li>
<li>The primary function for reversing the bit order in a word is
<code>revbin(ulong x)</code>, which applies the auxiliary functions
successively to achieve full bit reversal. On 64-bit architectures, it
may utilize an inline assembler (<code>asm_bswap</code>) for more
efficient byte swapping.</li>
</ul></li>
<li>Generating Bit-Reversed Words (Section 1.14):
<ul>
<li>The text presents several methods for generating bit-reversed words
in a specific order:
<ol type="1">
<li><code>revbin(ulong x)</code>: Reverses the entire bit order of an
input word <code>x</code>.</li>
<li><code>revbin_t(ulong x, ulong ldn)</code>: Reverses only the least
significant <code>ldn</code> bits of the input word <code>x</code>,
padding the remaining bits with zeros. This function uses a table-lookup
approach for efficiency when <code>ldn</code> is not too small.</li>
<li><code>revbin_upd(ulong r, ulong h)</code>: Updates the bit-reversed
word <code>r</code> by one position while maintaining the counting
order, using a lookup table (<code>utab</code>) and bit manipulations to
minimize branching.</li>
</ol></li>
</ul></li>
<li>Bit-wise Zip (Section 1.15):
<ul>
<li>The <code>bit_zip(ulong x)</code> function rearranges the bits in a
binary word such that even-indexed bits from the lower half become
odd-indexed, while odd-indexed bits from the upper half become
even-indexed. This operation is also called bit zip or bit unzip
(inverse of bit zip).</li>
<li>The text presents multiple implementations of
<code>bit_zip(ulong x)</code> and its inverse
(<code>bit_unzip(ulong x)</code>) to demonstrate different optimization
techniques and trade-offs between readability, performance, and code
size.</li>
</ul></li>
<li>Alternative Techniques for In-Order Generation (Section 1.14.4):
<ul>
<li>The text briefly mentions alternative approaches to generating
bit-reversed words in a specific order:
<ol type="1">
<li>A branchless loop from Brent Lehmann, involving divisions that can
be expensive on some architectures.</li>
<li>A recursive algorithm for generating all N-bit bit-reversed words in
order.</li>
<li>Generating revbin pairs in a pseudo-random order for certain
applications.</li>
</ol></li>
</ul></li>
</ol>
<p>Throughout the text, various optimizations and trade-offs are
explored to achieve efficient bit manipulations in C/C++ code. These
techniques can be useful when working with binary representations of
integers in performance-critical scenarios such as digital signal
processing, cryptography, or computer graphics.</p>
<p>The text discusses several concepts related to bit manipulation and
number theory, specifically focusing on Gray code, parity, reversed Gray
code, and sequency of binary words. Here’s a detailed summary and
explanation of each concept:</p>
<ol type="1">
<li><p><strong>Gray Code</strong>: The Gray code is a binary numeral
system where two successive values differ in only one bit. It is useful
for applications requiring minimal changes between adjacent values. The
given C++ function <code>gray_code(ulong x)</code> computes the Gray
code of a 64-bit unsigned integer <code>x</code>. The inverse Gray code
can be computed using three methods:</p>
<ul>
<li>Version 1: Integration modulo 2, which uses a loop to calculate the
inverse by XORing with shifted versions of itself.</li>
<li>Version 2: Applying the Gray code computation BITS_PER_LONG-1 times,
which is more efficient and faster than Version 1.</li>
<li>Version 3: Using the property that gray ** (2^n) = id for n-bit
words, this version applies the Gray code operation multiple times to
achieve the inverse.</li>
</ul></li>
<li><p><strong>Parity</strong>: The parity of a binary word is its
bit-count modulo 2, indicating whether the number of set bits is even or
odd. Functions <code>parity(ulong x)</code> and
<code>asm_parity(ulong x)</code> compute the parity of a 64-bit unsigned
integer <code>x</code>. The latter uses assembly code for better
performance on specific CPU architectures (x86 and AMD64).</p></li>
<li><p><strong>Byte-wise Gray Code and Parity</strong>: These functions
(<code>byte_gray_code(ulong x)</code>,
<code>byte_inverse_gray_code(ulong x)</code>, and
<code>byte_parity(ulong x)</code>) compute the Gray code, inverse Gray
code, and parities of 8-bit segments within a larger binary
word.</p></li>
<li><p><strong>Incrementing (counting) in Gray Code</strong>: This
section introduces methods for efficiently incrementing Gray codes while
preserving their properties. The <code>next_gray2(ulong x)</code>
function generates the next even-numbered Gray code value, starting from
an even input. To increment odd-numbered values, a modified approach is
provided.</p></li>
<li><p><strong>Thue-Morse Sequence</strong>: This sequence is generated
by counting the bit-pairs modulo 2 for binary words. The given C++ class
<code>thue_morse</code> computes this sequence using efficient methods
based on bitwise operations and parity checks.</p></li>
<li><p><strong>Golay-Rudin-Shapiro (GRS) Sequence</strong>: This
sequence is defined by counting the bit-pairs modulo 2 for specific
binary words. The provided C++ function
<code>grs_negative_q(ulong x)</code> returns +1 for indices where the
GRS sequence has the value −1, based on a method that counts overlapping
bit-pairs and applies parity checks.</p></li>
<li><p><strong>Reversed Gray Code</strong>: This concept involves
computing the Gray code of the bit-reversed word and then reversing the
result again. The given C++ functions
<code>rev_gray_code(ulong x)</code> and
<code>inverse_rev_gray_code(ulong x)</code> efficiently compute the
reversed Gray code and its inverse, respectively.</p></li>
<li><p><strong>Bit Sequency</strong>: The sequency of a binary word is
the number of zero-one transitions in the word. The provided C++
function <code>bit_sequency(ulong x)</code> calculates this value by
counting the transitions in the Gray code of the input word. To account
for the lowest bit, add 1 to the result.</p></li>
</ol>
<p>These concepts and functions are valuable tools in various
applications involving bit manipulation, error-correcting codes, and
combinatorial problems, among others.</p>
<p>The text describes several invertible binary word transforms, named
‘blue’, ‘yellow’, ‘red’, and ‘green’. These transforms are self-inverse
or third roots of identity, meaning they can be applied twice to return
the original value (involutions) or three times to do so. The transforms
scramble binary words while preserving certain properties like bit count
and parity.</p>
<ol type="1">
<li><p>Blue Code: This code is an involution, denoted as B. It uses a
specific mask ‘m’ that shifts right over iterations until the mask
reaches zero. The blue code maps any range [0…2^k - 1] onto itself. It
can be used for fast composition of binary polynomials with x + 1 and
has potential applications in randomization of binary words.</p></li>
<li><p>Yellow Code: Also an involution (denoted as Y), the yellow code
uses similar masks to blue but shifts left instead of right. Its
scrambling might be beneficial for randomizing binary words, though it’s
not explicitly stated.</p></li>
<li><p>Red Code: Denoted as R, this transform is a third root of
identity, meaning applying it three times results in the original word
(R³ = id). It uses masks that shift left or right based on iteration,
and it can be computed using bit-reversal of blue code (rB).</p></li>
<li><p>Green Code: Denoted as E, this is another third root of identity
like red, with the property that E² = R and E³ = id. It’s computed by
applying bit-reversal to yellow code (rY).</p></li>
</ol>
<p>The relations between these transforms are detailed in equations
(1.19-1a) to (1.19-6), showing how they can be composed or reversed. For
instance, Blue and Yellow are related as B = YRY and Y = BRB, while Red
and Green are interconnected via R = ERE and E = RRR. Bit-reversal (r)
plays a significant role in defining these relationships.</p>
<p>The transforms’ effects on the Gray code (g) are described by
equations (1.19-9a) to (1.19-9d), showing how applying Gray code
followed or preceded by another transform (B, Y, R, E) can lead back to
the original word or result in a specific transformation.</p>
<p>These transforms have potential applications in various computational
tasks involving binary words, such as randomization, polynomial
composition, and bit manipulation. However, the text does not explicitly
detail these use cases; it primarily focuses on their definitions,
properties, and relationships.</p>
<p>The Radix-2 (Minus Two) representation is a unique way to represent
non-negative integers using powers of -2 instead of 2. The
representation takes the form of an infinite series, where each term is
either 0 or -2 raised to some power k. For integers, this series
terminates after a finite number of terms, with the highest nonzero tk
being at most two positions beyond the highest bit in the binary
representation of the absolute value of the integer (using two’s
complement).</p>
<p>Key points and functions related to Radix-2 representation are:</p>
<ol type="1">
<li><p>Conversion from Binary to Radix-2: The function
<code>bin2neg(x)</code> converts a binary number x into its Radix-2
representation. This algorithm involves adding and XORing the input with
a specific constant (0xaaaaaaaaUL for 32 bits) to produce the Radix-2
representation. The inverse of this conversion, i.e., converting back
from Radix-2 to binary, is achieved through <code>neg2bin(x)</code>,
which reverses these steps by subtracting and XORing with the same
constant.</p></li>
<li><p>Fixed Points: The sequence of fixed points for the Radix-2
conversion starts as 0, 1, 4, 5, 16, 17, 20, 21, 64, 65, 68, 69, 80, 81,
84, 85, …, where each fixed point has ones only at even positions in its
binary representation. This sequence is known as the Moser-De Bruijn
sequence (A000695).</p></li>
<li><p>Gray Code of Radix-2: The Gray code for the numbers in the range
0 to k (where k = (4n - 1)/3, n ∈ ℕ) can be generated using the Radix-2
representation. The sequence of these Gray codes forms a Gray code for
the specified set of integers.</p></li>
<li><p>Generating Radix-2 words in order: To generate Radix-2 words in
order, one can start with the largest 111…111 (with n ones) and
systematically move bits to even positions by summing them up. This
process will produce all Radix-2 words in ascending order.</p></li>
</ol>
<p>The Radix-2 representation provides an alternative way of
representing non-negative integers, offering unique properties such as
fixed points and connections to Gray codes. It is particularly
interesting for its relationship with the Moser-De Bruijn sequence and
the possibility to generate Radix-2 words systematically using bit
manipulation techniques.</p>
<p>The provided text discusses three different orders for generating bit
combinations (binary words with a specific number of set bits):
Co-lexicographic (colex) order, Lexicographic (lex) order, and
Shifts-order.</p>
<ol type="1">
<li><p><strong>Co-lexicographic (colex) order</strong>: In this order,
the reversed sets are sorted. The method to find the next combination
involves determining the lowest block of ones and moving its highest bit
one position up, then shifting the rest of the block to the low end of
the word. The program [FXT: bits/bitcombcolex.h] provides a routine
(next_colex_comb) to compute the next combination in co-lexicographic
order. The predecessor can be found using the inverse method, which
involves finding the successor in co-lexicographic order and then taking
its bitwise complement if it’s not zero. The first and last combinations
can be computed using [FXT: bits/bitcombcolex-demo.cc].</p></li>
<li><p><strong>Lexicographic (lex) order</strong>: In this order, the
sets are sorted. The binary words corresponding to combinations in lex
order are the bit-reversed complements of the words for combinations in
co-lexicographic order. The program [FXT: bits/bitcomblex-demo.cc]
demonstrates how to compute the subset-lex sequence efficiently by
iterating through combinations in co-lexicographic order, reversing them
using the revbin() function (section 1.14), and applying an auxiliary
mask (m) and last combination (l).</p></li>
<li><p><strong>Shifts-order</strong>: This order is obtained from the
shifts-order for subsets by discarding all subsets whose number of
elements are not equal to k and reversing the list order. The first
combination is [1k0n−k], and the successor is computed by shifting the
bits in a specific way (as shown in figure 1.24-D). Figure 1.24-C
illustrates combinations in shifts-order for k = 1, 2, 3, 4.</p></li>
</ol>
<p>The text also mentions nonadjacent form (NAF) representations of
signed binary numbers and provides routines to convert between binary
and NAF representations. The Fibbinary numbers are defined as the
sequence of values whose negative part in the NAF representation is
zero, which corresponds to the sequence [0, 1, 2, 4, 5, 8, 9, 10, 16,
…]. These numbers are also known as the Fibonacci binary numbers.</p>
<p>The provided text discusses various methods for generating bit
combinations, minimal-change orders, and subsets of a given binary word.
Here’s a detailed summary and explanation of each section:</p>
<ol type="1">
<li><p><strong>Generating Bit Combinations</strong></p>
<ul>
<li><strong>Simple Split (S)</strong>: If the rightmost one is not in
position zero (least significant bit), shift the word to the right and
return the combination.</li>
<li><strong>Finished?</strong>: If the combination is the last one
([0n], [0n−11], [10n−k1k−1]), then return zero.</li>
<li><strong>Shift Back</strong>: Shift the word to the left such that
the leftmost one is in the leftmost position (this can be a no-op).</li>
<li><strong>Simple Split (S-2)</strong>: If the rightmost one is not the
least significant bit, move it one position to the right and return the
combination.</li>
<li><strong>Split Second Block</strong>: Move the rightmost bit of the
second block (from the right) of ones one position to the right and
attach the lowest block of ones, then return the combination.</li>
</ul>
<p>An implementation for these methods is provided in [FXT:
bits/bitcombshifts.h]. The generated combinations’ speed varies
depending on the parameters <code>n</code> and <code>k</code>.</p></li>
<li><p><strong>Minimal-Change Order</strong></p>
<ul>
<li><strong>igc_next_minchange_comb(ulong x)</strong>: A routine to
return the inverse Gray code of the next combination in minimal-change
order. The input must be the inverse Gray code of the current
combination.</li>
<li><strong>next_minchange_comb(ulong x, ulong last)</strong>: An
alternative method that uses <code>igc_next_minchange_comb()</code> and
returns the next combination in minimal-change order.</li>
</ul>
<p>Successive combinations differ in exactly two positions, as shown in
Figure 1.24-E. The algorithm is efficient, generating combinations at a
high rate (e.g., about 96 million per second for
<code>32 12 </code>).</p></li>
<li><p><strong>Generating Bit Subsets of a Given Word</strong></p>
<ul>
<li><strong>Counting Order</strong>: To generate all subsets of the set
of ones of a binary word, use sparse counting ideas from section 1.8.1.
The implementation is provided in [FXT: class bit_subset in
bits/bitsubset.h]. With a word like […11.1.], subsequent
<code>next()</code> and <code>prev()</code> calls produce subsets in
counting order. About 1.1 billion subsets per second are generated with
both methods.</li>
<li><strong>Minimal-Change Order</strong>: A method to isolate the
changing bit from counting order that does not depend on shifting is
introduced. The implementation is provided in [FXT: class
bit_subset_gray in bits/bitsubset-gray.h]. With a word like […11.1.],
subsequent <code>next()</code> and <code>prev()</code> calls produce
subsets in minimal-change order, generating about 365 million subsets
per second with both methods.</li>
</ul></li>
<li><p><strong>Binary Words in Lexicographic Order for
Subsets</strong></p>
<ul>
<li>The text provides an example of binary words corresponding to
nonempty subsets of the 4-element set in lexicographic order with
respect to subsets. It also shows a table of binary words and their
corresponding subsets.</li>
<li>The <code>next()</code> and <code>prev()</code> methods are
discussed for generating the next and previous word in lexicographic
order, respectively.</li>
</ul></li>
</ol>
<p>In summary, the text covers various techniques for generating bit
combinations, minimal-change orders, and subsets of given binary words.
It presents implementations and discusses their efficiency based on the
provided examples and figures.</p>
<p>The given text discusses several topics related to binary words,
their representation, and algorithms for generating them. Here’s a
detailed summary and explanation of each section:</p>
<ol type="1">
<li><p><strong>Binary Words in Lexicographic Order (Section
1.26):</strong></p>
<ul>
<li>The text introduces two functions (<code>next_lexrev()</code> and
<code>prev_lexrev()</code>) that generate binary words in lexicographic
order, starting from the one-bit word at position n−1.</li>
<li>These functions isolate the lowest bit for manipulation to create
the next subset, making the process efficient.</li>
<li>The function <code>negidx2lexrev(k)</code> converts a rank (index)
to its corresponding binary word in lexicographic order, and
<code>lexrev2negidx(x)</code> does the reverse conversion.</li>
<li>The number of bits required to represent a lex-word equals the
sequence’s entry A108918 in [312].</li>
</ul></li>
<li><p><strong>Minimal Decompositions into Terms 2^k −1 (Section
1.26.3):</strong></p>
<ul>
<li>This section describes how binary words can be represented as sums
of distinct powers of 2 minus one (<code>2^k −1</code>).</li>
<li>It provides examples and a formula to compute the minimal number of
terms required for such decompositions.</li>
</ul></li>
<li><p><strong>Fixed Points (Section 1.26.4):</strong></p>
<ul>
<li>The text discusses fixed points in the conversion between binary
words and their lexicographic order indices.</li>
<li>A sequence of fixed points is presented, along with a generating
function (equation 1.26-2).</li>
<li>It provides an alternative method to compute the number of terms
needed for a decomposition using the
<code>is_lexrev_fixed_point(x)</code> function.</li>
</ul></li>
<li><p><strong>Recursive Generation and Relation to Power Series
(Section 1.26.5):</strong></p>
<ul>
<li>The section introduces two functions (<code>C()</code> and
<code>A()</code>) that generate bit-reversed binary words in reversed
lexicographic order and a power series, respectively.</li>
<li>It explains the relationship between these generated sequences and
power series, showing how the lowest bits of the k-th word correspond to
coefficients in the power series.</li>
</ul></li>
<li><p><strong>Fibonacci Words (Section 1.27):</strong></p>
<ul>
<li>Fibonacci words are binary words that do not contain consecutive
ones. The text provides functions (<code>is_fibrep()</code>,
<code>bin2fibrep()</code>, and <code>fibrep2bin()</code>) for testing,
converting to, and from Fibonacci representations.</li>
<li>Two functions, <code>next_fibrep()</code> and
<code>prev_fibrep()</code>, are introduced to generate all n-bit
Fibonacci words in lexicographic order and its reverse.</li>
</ul></li>
<li><p><strong>Gray Code Order (Section 1.27.2):</strong></p>
<ul>
<li>The text discusses generating a Gray code for binary Fibonacci
words, based on the radix −2 representations’ minimal-change
combination.</li>
<li>It provides an implementation of the <code>bit_fibgray</code> class
to generate and manipulate Fibonacci Gray codes.</li>
</ul></li>
</ol>
<p>In summary, this text presents various algorithms and methods related
to binary word representation, lexicographic order generation, Fibonacci
words, and their conversions. These techniques are useful in different
areas, including combinatorics, information theory, and fast Walsh
transforms.</p>
<p>The Hilbert curve is a continuous fractal space-filling curve
introduced by David Hilbert. It is named after him due to his work on
this mathematical concept. The Hilbert curve is significant because it
can fill any two-dimensional space without overlapping, making it useful
in various fields such as computer graphics, data analysis, and image
processing.</p>
<p>The curve’s path is determined by a sequence of moves (dx, dy) where
dx and dy are integer values that can be either -1, 0, or +1. At each
step n of the Hilbert curve, exactly one of dx and dy is zero. This
means that at each step, the curve either moves horizontally (+1 or -1
for dx) or vertically (+1 or -1 for dy), but not diagonally.</p>
<p>The function <code>hilbert_p(t)</code> computes the direction of the
n-th move (dx + dy) of the Hilbert curve using the parity of the number
of threes in the radix-4 representation of t. The radix-4 representation
is a way to express numbers using base 4 digits, where each digit can be
one of the values {0, 1, 2, 3}.</p>
<p>Here’s how <code>hilbert_p(t)</code> works:</p>
<ol type="1">
<li>It first computes the value ‘d’ by performing bitwise AND operations
on ‘t’ with the hexadecimal masks <code>0x5555555555555555UL</code> and
<code>(t &amp; 0xaaaaaaaaaaaaaaaaUL) &gt;&gt; 1</code>. This operation
effectively isolates the bits corresponding to the threes in the radix-4
representation of ‘t’.</li>
<li>The function then returns the parity (odd or even) of ‘d’, which
indicates whether dx + dy equals -1 (return 0) or +1 (return 1).</li>
</ol>
<p>The optimized version of <code>hilbert_p(t)</code> uses a series of
bitwise XOR operations to isolate and compute the parity more
efficiently:</p>
<ol type="1">
<li>It performs a bitwise AND operation with the mask
<code>0xaaaaaaaaaaaaaaaaUL</code> and right-shifts ‘t’ by one position,
effectively isolating the bits corresponding to the threes in the
radix-4 representation.</li>
<li>The function then applies a series of bitwise XOR operations with
‘t’ shifted by powers of 2 (2, 4, 8, 16, and 32) to further isolate and
compute the parity more efficiently.</li>
<li>Finally, it returns the least significant bit of the result, which
indicates whether dx + dy equals -1 (return 0) or +1 (return 1).</li>
</ol>
<p>Once you have computed p using <code>hilbert_p(t)</code>, you can
determine m (dx - dy) by using the relationship between p and m. Since
exactly one of dx and dy is zero, m will be either -p or p, depending on
whether dx is positive or negative (similar for dy).</p>
<p>The Hilbert curve’s moves and turns are shown in Figure 1.31-B, where
‘dir’ indicates the direction
(&gt;<sup>&lt;&gt;v&gt;</sup>&gt;vv<v>&gt;<sup>&gt;v&gt;&gt;</sup>&lt;<sup>&gt;</sup>&lt;&lt;v&lt;^^<sup>&gt;v&gt;&gt;</sup>&lt;<sup>&gt;</sup>&lt;&lt;v&lt;<sup>&lt;<v>vv&lt;</sup>&lt;v&lt;^<sup>&gt;</sup>&lt;)
and ‘turn’ represents the change in direction
(0–+0++–++0+–0-++-0–++–0-++00++-0–++–0-++-0–+0++–++0+–). These patterns
help visualize how the Hilbert curve navigates through two-dimensional
space.</p>
<p>The provided text discusses several space-filling curves, including
the Hilbert curve, Z-order curve, dragon curve, alternate paper-folding
sequence, terdragon, and hexdragon. Here’s a detailed summary of
each:</p>
<ol type="1">
<li><p><strong>Hilbert Curve</strong>: This is a continuous fractal
curve that passes through every point in a square grid exactly once. It
has four possible moves (directions): right (+x), down (-y), up (+y),
and left (-x). The direction and turn between steps are determined by
the functions <code>hilbert_dir()</code> and
<code>hilbert_turn()</code>, respectively.</p>
<ul>
<li><p><code>hilbert_dir(t)</code>: Returns a 2-bit value
(<code>d</code>) encoding the direction of the move with step
<code>t</code>. The values are:</p>
<pre><code>d : direction
0 : right (+x: dx=+1, dy= 0)
1 : down (-y: dx= 0, dy=-1)
2 : up (+y: dx= 0, dy=+1)
3 : left (-x: dx=-1, dy= 0)</code></pre></li>
<li><p><code>hilbert_turn(t)</code>: Returns the turn (left or right)
between steps <code>t</code> and <code>t-1</code>. The returned value is
0 for no turn, +1 for a right turn, and -1 for a left turn.</p></li>
</ul></li>
<li><p><strong>Z-order Curve</strong>: This curve visits all points in
each quadrant before moving to the next one. It’s generated by
separating bits at even and odd indices of a linear parameter
<code>t</code>. The conversion between the linear coordinate and pair of
coordinates (x, y) is done using bit manipulation functions
<code>lin2zorder()</code> and <code>zorder2lin()</code>.</p>
<ul>
<li><code>lin2zorder(t, x, y)</code>: Transforms the linear coordinate
<code>t</code> to Z-order coordinates <code>x</code> and
<code>y</code>.</li>
<li><code>zorder2lin(x, y)</code>: Transforms Z-order coordinates
<code>x</code> and <code>y</code> back to a linear parameter
<code>t</code>.</li>
</ul></li>
<li><p><strong>Dragon Curve</strong>: This curve is generated by
interpreting ones as ‘turn left’ and zeros as ‘turn right’ in the
paper-folding sequence. The curve has two types of turns: left
(<code>^</code>) and right (<code>v</code>). The net rotation after
<code>k</code> steps, modulo 4 (to ignore multiples of 360 degrees), is
computed by counting ones in the Gray code of <code>k</code>.</p></li>
<li><p><strong>Alternate Paper-Folding Sequence</strong>: This sequence
is generated by folding a strip of paper alternately from the left and
right. The resulting curve has turns interpreted as ‘left’ (L) and
‘right’ (R). The net rotation after <code>k</code> steps, modulo 4, is
computed similarly to the dragon curve.</p></li>
<li><p><strong>Terdragon Curve</strong>: This curve turns to the left or
right by 120 degrees depending on the sequence 0, 1, 0, 0, 1, 1, 0, 1,
0, 0, 1, 0, 0, 1, 1, 0, 1.</p></li>
<li><p><strong>Hexdragon Curve</strong>: This curve is similar to the
terdragon but with a more complex turning sequence (not provided in the
text). The first 729 segments of this curve are shown in the
text.</p></li>
</ol>
<p>Each of these curves has its unique generation process and rules,
leading to different patterns and properties. They are used in various
fields, including computer graphics, data structures, and fractal
geometry.</p>
<p>The provided text discusses various aspects of permutations, a
concept from combinatorial mathematics. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Permutations</strong>: A permutation of n elements is an
arrangement of these elements into a specific order. It can be
represented as an array, where the element at position k is moved to
position xk when the permutation X is applied to F = [f0, f1, …, fn-1].
The identical permutation leaves all elements in their
positions.</p></li>
<li><p><strong>Basic Operations</strong>:</p>
<ul>
<li><strong>Application of Permutation (apply_permutation)</strong>:
This function applies a given permutation to an array.</li>
<li><strong>Identity Check (is_identity)</strong>: Determines if a
permutation is the identity permutation, i.e., whether f[k]==k for all
k=0…n-1.</li>
<li><strong>Fixed Points Count (count_fixed_points)</strong>: Returns
the number of fixed points in a permutation, where a fixed point is an
index where the element is not moved.</li>
<li><strong>Derangement Check (is_derangement)</strong>: Determines if a
permutation is a derangement, i.e., whether f[k]!=k for all k. Two
arrays are mutual derangements if fk ̸= gk for all k.</li>
<li><strong>Connected Permutation Check (is_connected)</strong>: A
connected permutation contains no proper preﬁx mapped to itself. This
function checks if max(f0, f1, …, fk) &gt; k for all k &lt; n −1.</li>
<li><strong>Valid Permutation Check (is_valid_permutation)</strong>:
Verifies that each index in the valid range appears exactly once.</li>
</ul></li>
<li><p><strong>Representation as Disjoint Cycles</strong>: Every
permutation consists entirely of disjoint cycles. A cycle is a subset of
indices rotated by one position by the permutation. The inverse of a
permutation can be found by reversing every arrow in each
cycle.</p></li>
<li><p><strong>Cyclic Permutations</strong>: A permutation consisting of
exactly one cycle is called cyclic. There are (n −1)! cyclic
permutations of n elements.</p></li>
<li><p><strong>Sign and Parity of a Permutation</strong>: The sign of a
permutation is +1 if the number of transpositions (cycles of length 2)
is even, and -1 if it’s odd. This parity is unique modulo 2.</p></li>
<li><p><strong>Compositions of Permutations</strong>: Applying several
permutations to an array, one by one, results in a composition of
permutations. The operation of composition is not commutative.</p></li>
<li><p><strong>Inverse of a Permutation</strong>: A permutation f is the
inverse of g if f · g = id (identity permutation). The routine
make_inverse computes the inverse of a given permutation. For in-place
computation, each cycle is inverted.</p></li>
<li><p><strong>Involutions</strong>: A permutation which is its own
inverse is called an involution. This can be checked by verifying if max
cycle length is &lt;= 2.</p></li>
</ol>
<p>The text also mentions routines and functions for generating random
permutations, cyclic permutations, involutions, and derangements, as
well as methods for applying special permutations like the revbin
permutation, the Gray permutation, and matrix transposition.
Additionally, it discusses dragon curves generated by string
substitution based on radix-R counting, where R is a base (5, 7, 9,
13).</p>
<p>The given text discusses various methods for generating random
permutations and applying them to arrays of data. Here’s a detailed
summary:</p>
<ol type="1">
<li><p><strong>Random Permutation</strong>: The random_permute function
(line 3) randomly permutes an array f of n elements using the
Fisher-Yates shuffle algorithm, also known as Knuth shuﬄe. It
iteratively selects a random index i between 0 and k (inclusive), then
swaps the element at position k with the element at position i. This
process continues until all elements have been visited.</p></li>
<li><p><strong>Random Cyclic Permutation</strong>: The
random_permute_cyclic function generates a random cyclic permutation of
an array f. It works similarly to the previous function but only
permutes the last k-1 elements, leaving the 0th element unchanged. This
is known as Sattolo’s algorithm.</p></li>
<li><p><strong>Random Prefix Permutation</strong>: The
random_permute_pref function generates a prefix of a random permutation.
It fills the first m elements of an array f with a random permutation of
the first m+1, m+2, …, n elements. This is achieved by iterating over
the desired positions k and selecting a random index i from the
remaining n-k elements.</p></li>
<li><p><strong>Random Permutation with Prescribed Parity</strong>: The
random_permute_parity function generates a random permutation with a
specified parity (even or odd number of transpositions). It does this by
initially generating a random permutation and then adjusting it if
necessary to achieve the desired parity.</p></li>
<li><p><strong>Random Permutation with Preserved Order of m Smallest
Elements</strong>: The random_ordm_permutation function generates a
random permutation where the first m smallest elements are in ascending
order. This is done by first generating a random permutation and then
rearranging these m elements accordingly.</p></li>
<li><p><strong>Random Permutation with Specified Cycle Type</strong>:
The random_permute_cycle_type function generates a random permutation
with a specified cycle type (i.e., the number of cycles of each length).
This is achieved by iteratively creating cycles of the desired lengths
using the random_cycle helper function.</p></li>
</ol>
<p>These functions provide various ways to generate and apply random
permutations, which are useful in many applications such as simulations,
cryptography, and combinatorial algorithms. The efficiency and
effectiveness of these methods depend on the specific requirements of
the task at hand.</p>
<p>The text discusses several methods for generating random
permutations, focusing on specific types such as self-inverse
(involution) permutations, derangements (permutations without fixed
points), connected permutations, and the revbin permutation. Here’s a
detailed summary of each method:</p>
<ol type="1">
<li><strong>Random Self-Inverse Permutation:</strong>
<ul>
<li>A self-inverse permutation, or involution, is a permutation that is
its own inverse.</li>
<li>The probability of generating a 2-cycle (a pair of elements swapped)
at each step is R(n) = I(n−1)/I(n), where I(n) is the number of
involutions of n elements.</li>
<li>To avoid overflow issues with large n, R(n) is updated using a
numerically stable recurrence relation: R(n + 1) = 1 / (1 + n *
R(n)).</li>
<li>The routine <code>random_permute_self_inverse</code> generates such
permutations by randomly deciding whether to create a 2-cycle or a fixed
point based on the probability R(n).</li>
</ul></li>
<li><strong>Random Derangement:</strong>
<ul>
<li>A derangement is a permutation without any fixed points (elements
that map to themselves).</li>
<li>The probability of generating a cycle closure at each step is B(n) =
(n−1) D(n−1)/D(n), where D(n) is the number of derangements of n
elements.</li>
<li>For large n, B(n) is close to 1/n, and a table of precomputed values
can be used for efficiency.</li>
<li>The routine <code>random_derange</code> generates such permutations
by randomly deciding whether to join two cycles or create a fixed point
based on the probability B(n).</li>
</ul></li>
<li><strong>Random Connected Permutation:</strong>
<ul>
<li>A connected permutation is an indecomposable permutation, meaning it
cannot be split into smaller non-trivial permutations.</li>
<li>The routine <code>random_connected_permutation</code> generates such
permutations using the rejection method: it repeatedly generates random
permutations until a connected one is obtained. This method is efficient
because the probability of generating a disconnected permutation
decreases with n.</li>
</ul></li>
<li><strong>Revbin Permutation:</strong>
<ul>
<li>The revbin permutation swaps elements whose binary indices are
mutual reversals (bit-reversal or bitrev permutation). For example, for
length n = 256, element x = 4310 = 001010112 is swapped with the element
whose index is ˜x = 110101002 = 21210.</li>
<li>The key optimization for generating this permutation is updating
only the bit-reversed values, using the revbin_upd function to compute
]x + 1 efficiently.</li>
<li>Further optimizations can be made by exploiting symmetries in the
permutation, such as recognizing that swaps for certain pairs are
independent and can be processed simultaneously.</li>
</ul></li>
</ol>
<p>Each of these methods has its applications in various fields,
including numerical analysis, cryptography, and random number
generation. The revbin permutation, in particular, is useful in fast
Fourier transform (FFT) algorithms due to its ability to rearrange data
efficiently.</p>
<p>The text discusses several array permutations, which are operations
that rearrange elements within an array according to specific rules.
Here’s a detailed explanation of each permutation:</p>
<ol type="1">
<li><p><strong>Radix Permutation</strong>: This is a generalization of
the revbin permutation (discussed later) to arbitrary radices. In radix
r, pairs of indices with reversed values are swapped. For example, in
radix 10 and n = 1000, elements with indices 123 and 321 would be
swapped. The radix permutation is self-inverse; meaning applying it
twice will return the original array. The code for this permutation is
provided in [FXT: perm/radixpermute.h], but it must be called with n as
a perfect power of r.</p></li>
<li><p><strong>In-place Matrix Transposition</strong>: This involves
swapping elements in an array to transpose a matrix without using extra
memory. For non-square matrices, this gets complex due to the need to
identify cycles of the underlying permutation. When n (the total number
of elements) is a power of 2, multiplications modulo n - 1 are cyclic
shifts, making computations simpler and avoiding overflows.</p></li>
<li><p><strong>Rotation by Triple Reversal</strong>: This technique
rotates an array by s positions using three reversals without temporary
memory. For left rotation (move elements towards the start), it reverses
the first s elements, then reverses the remaining n-s elements, and
finally reverses the whole array. Right rotation is achieved similarly
but with reversed steps. This trick can also be used to swap two blocks
in an array by reversing four ranges (first block, range between blocks,
last block, and the entire array).</p></li>
<li><p><strong>Zip Permutation</strong>: This permutation moves elements
from the lower half of the array to even indices and those from the
upper half to odd indices. It requires an even-sized array. The routine
is straightforward, with one loop filling even indices and another for
odd ones. The inverse, unzip, performs the reverse operation. If the
array size n is a power of 2, zip can be computed as a transposition of
a 2 × n/2 matrix using revbin permutations.</p></li>
<li><p><strong>XOR Permutation</strong>: This permutation swaps an
element at index k with another at index x XOR k. It’s self-inverse and
requires that the array length n is divisible by the smallest power of 2
greater than x for correct operation (e.g., if x = 1, n must be even; if
x = 2 or 3, n must be divisible by 4).</p></li>
</ol>
<p>Each of these permutations has its use cases depending on the
specific requirements of the task at hand, such as memory constraints,
computational efficiency, and the desired end result of rearranging
array elements.</p>
<p>Selection sort is a simple sorting algorithm that has a time
complexity of O(n^2), making it less efficient for large datasets
compared to other algorithms like quicksort or mergesort. However, its
simplicity makes it suitable for small arrays or educational
purposes.</p>
<p>Here’s how selection sort works:</p>
<ol type="1">
<li><p>The array is divided into two parts: the sorted section at the
left end and the unsorted section at the right end. Initially, the
sorted section contains only the first element of the array, while the
rest forms the unsorted section.</p></li>
<li><p>In each iteration (from i = 0 to n-1), the algorithm searches for
the minimum value in the unsorted part of the array (i.e., from index i
to n-1).</p></li>
<li><p>Once found, this minimum element is swapped with the first
element of the unsorted section (index i). This effectively extends the
sorted section by one element.</p></li>
<li><p>The process repeats for the remaining elements in the unsorted
section until the entire array is sorted.</p></li>
</ol>
<p>Here’s a step-by-step breakdown using the example string
‘nowsortme’:</p>
<ol type="1">
<li><p>Start with the first character, ‘n’. This becomes the minimum so
far, and it’s placed at position 0 (the sorted section).</p></li>
<li><p>Move to the next unsorted element (‘o’). Since ‘o’ is less than
‘n’, we update our minimum and its position to (‘o’, 1).</p></li>
<li><p>Continue this process for all elements:</p>
<ul>
<li>For ‘w’ at index 2, it’s greater than the current minimum ‘o’, so no
change.</li>
<li>‘s’ at index 3 is less than ‘o’, update minimum to ‘s’ and position
to 2.</li>
<li>‘r’ at index 4 is greater than ‘s’, no change.</li>
<li>‘t’ at index 5 is less than ‘s’, update minimum to ‘t’ and position
to 4.</li>
<li>‘m’ at index 6 is less than ‘t’, update minimum to ‘m’ and position
to 5.</li>
<li>‘e’ at index 7 is greater than ‘m’, no change.</li>
<li>‘o’ at index 8 is equal to the current minimum, so no change
(duplicate elements can be handled differently based on
requirements).</li>
</ul></li>
<li><p>After iterating through all indices, the sorted array is [‘e’,
‘m’, ‘n’, ‘o’, ‘o’, ‘r’, ‘s’, ‘t’, ‘w’].</p></li>
</ol>
<p>The FXT library implementation for selection sort in C++ is as
follows:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> selection_sort<span class="op">(</span>Type <span class="op">*</span>f<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Sort f[] (ascending order).</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Algorithm is O(n*n), use for short arrays only.</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="ex">ulong</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        Type v <span class="op">=</span> f<span class="op">[</span>i<span class="op">];</span> <span class="co">// Store the current minimum value</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="ex">ulong</span> m <span class="op">=</span> i<span class="op">;</span>   <span class="co">// Position of the minimum</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="ex">ulong</span> j <span class="op">=</span> n<span class="op">;</span>    <span class="co">// Search from the end of the unsorted section</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="op">(--</span>j <span class="op">&gt;</span> i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>f<span class="op">[</span>j<span class="op">]</span> <span class="op">&lt;</span> v<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>                m <span class="op">=</span> j<span class="op">;</span>  <span class="co">// Update position of minimum</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>                v <span class="op">=</span> f<span class="op">[</span>m<span class="op">];</span> <span class="co">// Update current minimum value</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Swap minimum with first element of unsorted section</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>m <span class="op">!=</span> i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            Type temp <span class="op">=</span> f<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            f<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> v<span class="op">;</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>            v <span class="op">=</span> temp<span class="op">;</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            m <span class="op">=</span> i<span class="op">;</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Key points to remember: - Selection sort has a time complexity of
O(n^2), making it inefficient for large datasets. - It’s simple and easy
to understand, with minimal memory overhead (O(1) additional space). -
The algorithm is stable, meaning that equal elements maintain their
relative order. - Use selection sort when dealing with small arrays or
educational purposes due to its simplicity.</p>
<p>The text discusses several sorting algorithms, their complexities,
and variations. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Verification Routines</strong>: Two functions are
provided for checking the order of sequences:</p>
<ul>
<li><code>is_sorted&lt;Type&gt;(const Type *f, ulong n)</code>: Checks
if a sequence is in ascending order.</li>
<li><code>is_falling&lt;Type&gt;(const Type *f, ulong n)</code>: Checks
if a sequence is in descending order.</li>
</ul></li>
<li><p><strong>Quicksort</strong>: This algorithm has an average-case
complexity of O(n log(n)). It works by partitioning the array and
recursively sorting the sub-arrays on either side of the partition
index. The partition function rearranges the array such that all
elements to the left of the pivot are less than or equal to it, and all
elements to the right are greater than or equal to it. To avoid
worst-case scenarios with already sorted input, a median-of-three method
is used to select the pivot.</p></li>
<li><p><strong>Counting Sort</strong>: This algorithm sorts an n-element
array of unsigned 8-bit values in two passes through the data. It
allocates an array C and counts occurrences of each value in the input
array F. Then, it reorders F based on these counts, preserving the
relative order of equal elements (a stable sort). The method can be
extended to sort larger integer variables by using a bit mask and a
mapping function M.</p></li>
<li><p><strong>Radix Sort</strong>: This algorithm sorts an n-element
array of unsigned integers by processing each bit position in turn. It
uses counting, cumulating, and copying stages to sort the data based on
specific bit ranges. The routine <code>radix_sort</code> implements this
method for 8-bit integers, while <code>counting_sort_core</code> handles
the core counting sort logic for arbitrary bit ranges.</p></li>
<li><p><strong>Merge Sort</strong>: Merge sort is a divide-and-conquer
algorithm with a complexity of O(n log(n)). It sorts an array by
recursively dividing it into smaller sub-arrays, sorting those
sub-arrays, and merging them back together. The routine
<code>merge_sort</code> performs this operation, while
<code>merge_sort_rec4</code> implements a 4-way merge sort for improved
locality of reference.</p></li>
<li><p><strong>Heapsort</strong>: Heapsort is another O(n log(n))
algorithm that uses a heap data structure to sort an array. It builds a
heap from the input array, repeatedly extracts the maximum element (or
minimum for descending order), and restores the heap property. The
routine <code>heap_sort</code> implements this method.</p></li>
<li><p><strong>Binary Search</strong>: Binary search is an efficient
O(log(n)) algorithm for searching in sorted arrays. It repeatedly
divides the search interval in half, comparing the target value with the
middle element to determine which half to continue searching in. The
function <code>bsearch</code> performs this operation, while
<code>bsearch_geq</code> and <code>bsearch_approx</code> offer
variations that search for elements greater than or equal to a given
value and approximate matches within a certain tolerance,
respectively.</p></li>
<li><p><strong>Variants of Sorting Methods</strong>:</p>
<ul>
<li><strong>Index Sorting</strong>: This involves sorting an array of
indices x such that f[x[k]] is in ascending order. The routine
<code>idx_selection_sort</code> demonstrates this using the selection
sort algorithm for short arrays.</li>
</ul></li>
</ol>
<p>These algorithms and routines provide various options for sorting and
searching data, each with its strengths and trade-offs depending on the
specific use case and data characteristics.</p>
<p>Title: Summary and Explanation of Equivalence Classes Algorithm and
Examples</p>
<ol type="1">
<li><p><strong>Algorithm for Decomposition into Equivalence
Classes</strong></p>
<p>The provided algorithm aims to determine the equivalence classes of a
set S under a given equivalence relation. Here’s a detailed
breakdown:</p>
<ul>
<li><strong>Initialization</strong>: Each element in S is initially
considered its own equivalence class (Qk = k for all 0 ≤k &lt; n).</li>
<li><strong>Search for Equivalent Element</strong>: The algorithm
searches for an equivalent element to the current one by comparing it
with previously processed elements (j) and setting their equivalence
class index (Qk) to that of the equivalent element (Qj).</li>
<li><strong>Iterate</strong>: The process continues until all elements
have been checked, at which point each element is assigned to its
respective equivalence class.</li>
</ul>
<p>The algorithm’s time complexity depends on the number of equivalence
tests required: n-1 tests for when all elements are in the same class
and n(n-1)/2 tests for when each element is alone in its own
class.</p></li>
<li><p><strong>Equivalence Relation as a Function</strong></p>
<p>In this context, an equivalence relation (equiv_q) is treated as a
function that returns true if two input arguments are equivalent
according to the given relation. This function is supplied by the user
and must conform to the properties of reflexivity, symmetry, and
transitivity for it to qualify as an equivalence relation.</p></li>
<li><p><strong>Examples of Equivalence Classes</strong></p>
<p>The provided examples illustrate various applications of the
equivalence classes concept:</p>
<ul>
<li><strong>Integers Modulo m</strong>: Two integers a and b are
equivalent if (a-b) is a multiple of some fixed integer m &gt; 0. The
set Z of natural numbers partitions into m subsets, with x ≡ y if and
only if x ≡ y (mod m).</li>
<li><strong>Binary Necklaces</strong>: In this case, two n-bit binary
words are equivalent if there exists a cyclic shift by k positions (0 ≤k
&lt; n) such that the shifted word equals the other. The equivalence
relation is captured by the function <code>n_equiv_q()</code>. For
example, with n = 4, we obtain six distinct classes.</li>
<li><strong>Unlabeled Binary Necklaces</strong>: Similar to binary
necklaces but also considers complements of words. Two words are
equivalent if one can be obtained from the other through a cyclic shift
or bitwise complementation. This is represented by
<code>nu_equiv_q()</code>. For n = 4, there are four such classes.</li>
<li><strong>Binary Bracelets</strong>: Two bracelets (n-bit binary
words) are equivalent if they match after rotation and possible
reversal. The equivalence relation is captured by the function
<code>b_equiv_q()</code>, yielding six distinct classes for n = 4.</li>
<li><strong>Unlabeled Binary Bracelets</strong>: Similar to binary
bracelets but allows bitwise complementation in addition to rotation and
reversal. This results in four equivalence classes for n = 4, as
represented by the function <code>bu_equiv_q()</code>.</li>
</ul></li>
</ol>
<p>Each example demonstrates how the concept of equivalence classes can
be applied to different types of data, providing a way to group similar
elements based on a user-defined relation while preserving essential
properties.</p>
<p>A queue (FIFO - First In, First Out) is a linear data structure that
follows the principle of “first come, first served.” It has two primary
operations:</p>
<ol type="1">
<li><p><strong>Push()</strong>: This operation adds an element to the
rear (back) of the queue. If the queue is not full, the new element is
inserted, and the position counter (often referred to as ‘wpos’ for
write position) is incremented.</p></li>
<li><p><strong>Pop()</strong>: This operation removes the front (first)
element from the queue. It retrieves this element and updates the
position counter (often referred to as ‘rpos’ for read position). If the
queue becomes empty after removal, ‘rpos’ is reset to zero or an
appropriate value indicating an empty state.</p></li>
<li><p><strong>Peek()</strong>: This operation allows you to examine the
front element without removing it. Like pop(), it retrieves this element
and updates the ‘rpos’, but does not alter the queue’s size.</p></li>
</ol>
<p>Here is a simple implementation of a Queue using C++:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;vector&gt;</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdexcept&gt;</span><span class="pp"> </span><span class="co">// for std::out_of_range</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Queue <span class="op">{</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">private</span><span class="op">:</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">std::</span>vector<span class="op">&lt;</span>T<span class="op">&gt;</span> data<span class="op">;</span> <span class="co">// The queue&#39;s storage</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">public</span><span class="op">:</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="dt">void</span> push<span class="op">(</span><span class="at">const</span> T<span class="op">&amp;</span> value<span class="op">)</span> <span class="op">{</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>            data<span class="op">.</span>push_back<span class="op">(</span>value<span class="op">);</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        T pop<span class="op">()</span> <span class="op">{</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>data<span class="op">.</span>empty<span class="op">())</span> <span class="op">{</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>                <span class="cf">throw</span> <span class="bu">std::</span>out_of_range<span class="op">(</span><span class="st">&quot;Queue is empty&quot;</span><span class="op">);</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            T front <span class="op">=</span> data<span class="op">.</span>front<span class="op">();</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            data<span class="op">.</span>erase<span class="op">(</span>data<span class="op">.</span>begin<span class="op">());</span> <span class="co">// Remove the first element</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> front<span class="op">;</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="at">const</span> T<span class="op">&amp;</span> peek<span class="op">()</span> <span class="at">const</span> <span class="op">{</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>data<span class="op">.</span>empty<span class="op">())</span> <span class="op">{</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">throw</span> <span class="bu">std::</span>out_of_range<span class="op">(</span><span class="st">&quot;Queue is empty&quot;</span><span class="op">);</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> data<span class="op">.</span>front<span class="op">();</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div>
<p>In this implementation:</p>
<ul>
<li><code>push(const T&amp; value)</code> adds an element to the end of
the vector, which acts as our queue.</li>
<li><code>pop()</code> removes and returns the first element in the
vector. If the queue is empty, it throws an exception
(<code>std::out_of_range</code>).</li>
<li><code>peek() const</code> returns a constant reference to the front
element without removing it. Again, if the queue is empty, it throws an
exception.</li>
</ul>
<p>This simple Queue uses a dynamic array (vector) to store elements,
allowing for efficient insertion and removal of elements from either
end. The ‘front’ of the queue is always the first element in this
vector, and the ‘rear’ (or ‘back’) is the last position where an element
was inserted.</p>
<p>The provided text discusses two data structures: Queue and Deque,
along with a brief overview of Heap and Priority Queue. I will summarize
and explain each section in detail.</p>
<ol type="1">
<li><p>Queue: A queue is a First-In-First-Out (FIFO) data structure
where elements are added at the rear (push operation) and removed from
the front (pop operation). The given code implements a dynamic queue
that grows when necessary, with an optional growth feature controlled by
<code>gq_</code> variable.</p>
<ul>
<li><p>Class definition:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> queue <span class="op">{</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span><span class="op">:</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  Type <span class="op">*</span><span class="va">x_</span><span class="op">;</span> <span class="co">// pointer to data</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">s_</span><span class="op">;</span>  <span class="co">// allocated size (# of elements)</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">n_</span><span class="op">;</span>  <span class="co">// current number of entries in buffer</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">wpos_</span><span class="op">;</span><span class="co">// next position to write in buffer</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">rpos_</span><span class="op">;</span><span class="co">// next position to read in buffer</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">gq_</span><span class="op">;</span>  <span class="co">// grow gq elements if necessary, 0 for &quot;never grow&quot;</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div></li>
<li><p>Constructor initializes the queue with a given size and growth
parameter:</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">explicit</span> queue<span class="op">(</span><span class="ex">ulong</span> n<span class="op">,</span> <span class="ex">ulong</span> growq<span class="op">=</span><span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="va">s_</span> <span class="op">=</span> n<span class="op">;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="va">x_</span> <span class="op">=</span> <span class="kw">new</span> Type<span class="op">[</span><span class="va">s_</span><span class="op">];</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="va">n_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="va">wpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">rpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>  <span class="va">gq_</span> <span class="op">=</span> growq<span class="op">;</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>push operation inserts an element at the rear and increments the
write position (<code>wpos_</code>):</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> push<span class="op">(</span><span class="at">const</span> Type <span class="op">&amp;</span>z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">n_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">gq_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// growing disabled</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    grow<span class="op">();</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">x_</span><span class="op">[</span><span class="va">wpos_</span><span class="op">]</span> <span class="op">=</span> z<span class="op">;</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">wpos_</span><span class="op">;</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">wpos_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="va">wpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>peek operation reads from the front without removing it:</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> peek<span class="op">(</span>Type <span class="op">&amp;</span>z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span><span class="va">rpos_</span><span class="op">];</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>pop operation removes and returns the element at the front:</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> pop<span class="op">(</span>Type <span class="op">&amp;</span>z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> ret <span class="op">=</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">!=</span> <span class="va">n_</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span><span class="va">rpos_</span><span class="op">];</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">++</span><span class="va">rpos_</span><span class="op">;</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="va">rpos_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="va">rpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> ret<span class="op">;</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>grow method resizes the internal buffer by doubling its size
(<code>gq_</code>):</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">private</span><span class="op">:</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">void</span> grow<span class="op">()</span> <span class="op">{</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ulong</span> ns <span class="op">=</span> <span class="va">s_</span> <span class="op">+</span> <span class="va">gq_</span><span class="op">;</span> <span class="co">// new size</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    rotate_left<span class="op">(</span><span class="va">x_</span><span class="op">,</span> <span class="va">s_</span><span class="op">,</span> <span class="va">rpos_</span><span class="op">);</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">x_</span> <span class="op">=</span> ReAlloc<span class="op">&lt;</span>Type<span class="op">&gt;(</span><span class="va">x_</span><span class="op">,</span> ns<span class="op">,</span> <span class="va">s_</span><span class="op">);</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">wpos_</span> <span class="op">=</span> <span class="va">s_</span><span class="op">;</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">rpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">s_</span> <span class="op">=</span> ns<span class="op">;</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span></code></pre></div></li>
</ul></li>
<li><p>Deque (Double-Ended Queue): A deque allows insertion and deletion
at both ends efficiently (O(1) time complexity). The provided code
includes a dynamic deque that grows when necessary.</p>
<ul>
<li><p>Class definition:</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> deque <span class="op">{</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span><span class="op">:</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  Type <span class="op">*</span><span class="va">x_</span><span class="op">;</span> <span class="co">// data (ring buffer)</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">s_</span><span class="op">;</span>  <span class="co">// allocated size (# of elements)</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">n_</span><span class="op">;</span>  <span class="co">// current number of entries in buffer</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">fpos_</span><span class="op">;</span><span class="co">// position of first element in buffer</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">lpos_</span><span class="op">;</span><span class="co">// position of last element in buffer plus one</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> <span class="va">gq_</span><span class="op">;</span>  <span class="co">// grow gq elements if necessary, 0 for &quot;never grow&quot;</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div></li>
<li><p>Constructors and destructors are similar to the queue
implementation.</p></li>
<li><p>insert_first operation adds an element at the front:</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> insert_first<span class="op">(</span><span class="at">const</span> Type <span class="op">&amp;</span>z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">n_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">gq_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// growing disabled</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    grow<span class="op">();</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">--</span><span class="va">fpos_</span><span class="op">;</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">fpos_</span> <span class="op">==</span> <span class="op">-</span><span class="dv">1</span><span class="bu">UL</span><span class="op">)</span> <span class="va">fpos_</span> <span class="op">=</span> <span class="va">s_</span> <span class="op">-</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>  <span class="va">x_</span><span class="op">[</span><span class="va">fpos_</span><span class="op">]</span> <span class="op">=</span> z<span class="op">;</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>insert_last operation adds an element at the rear:</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> insert_last<span class="op">(</span><span class="at">const</span> Type <span class="op">&amp;</span>z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">n_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">gq_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// growing disabled</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    grow<span class="op">();</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  <span class="va">x_</span><span class="op">[</span><span class="va">lpos_</span><span class="op">]</span> <span class="op">=</span> z<span class="op">;</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">lpos_</span><span class="op">;</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">lpos_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="va">lpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>extract_first and extract_last operations remove and return
elements from the front and rear, respectively:</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> extract_first<span class="op">(</span>Type <span class="op">&amp;</span> z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">n_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span><span class="va">fpos_</span><span class="op">];</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">++</span><span class="va">fpos_</span><span class="op">;</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">fpos_</span> <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> <span class="va">fpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">--</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> extract_last<span class="op">(</span>Type <span class="op">&amp;</span> z<span class="op">)</span> <span class="op">{</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">n_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>  <span class="op">--</span><span class="va">lpos_</span><span class="op">;</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="va">lpos_</span> <span class="op">==</span> <span class="op">-</span><span class="dv">1</span><span class="bu">UL</span><span class="op">)</span> <span class="va">lpos_</span> <span class="op">=</span> <span class="va">s_</span> <span class="op">-</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>  z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span><span class="va">lpos_</span><span class="op">];</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>  <span class="op">--</span><span class="va">n_</span><span class="op">;</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>read operations allow reading elements without removing them:</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> read_first<span class="op">(</span>Type <span class="op">&amp;</span> z<span class="op">)</span> <span class="at">const</span> <span class="op">{</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span><span class="dv">0</span> <span class="op">==</span> <span class="va">n_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span><span class="va">fpos_</span><span class="op">];</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> read_last<span class="op">(</span>Type <span class="op">&amp;</span> z<span class="op">)</span> <span class="at">const</span> <span class="op">{</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> read<span class="op">(</span><span class="va">n_</span><span class="op">-</span><span class="dv">1</span><span class="op">,</span> z<span class="op">);</span> <span class="co">// ok for n_==0</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> read<span class="op">(</span><span class="ex">ulong</span> k<span class="op">,</span> Type <span class="op">&amp;</span> z<span class="op">)</span> <span class="at">const</span> <span class="op">{</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>k <span class="op">&gt;=</span> <span class="va">n_</span><span class="op">)</span> <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> j <span class="op">=</span> <span class="va">fpos_</span> <span class="op">+</span> k<span class="op">;</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>j <span class="op">&gt;=</span> <span class="va">s_</span><span class="op">)</span> j <span class="op">-=</span> <span class="va">s_</span><span class="op">;</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>  z <span class="op">=</span> <span class="va">x_</span><span class="op">[</span>j<span class="op">];</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> k <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>grow method resizes the internal buffer by doubling its size
(<code>gq_</code>):</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">private</span><span class="op">:</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">void</span> grow<span class="op">()</span> <span class="op">{</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ulong</span> ns <span class="op">=</span> <span class="va">s_</span> <span class="op">+</span> <span class="va">gq_</span><span class="op">;</span> <span class="co">// new size</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    rotate_left<span class="op">(</span><span class="va">x_</span><span class="op">,</span> <span class="va">s_</span><span class="op">,</span> <span class="va">fpos_</span><span class="op">);</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">x_</span> <span class="op">=</span> ReAlloc<span class="op">&lt;</span>Type<span class="op">&gt;(</span><span class="va">x_</span><span class="op">,</span> ns<span class="op">,</span> <span class="va">s_</span><span class="op">);</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">fpos_</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">lpos_</span> <span class="op">=</span> <span class="va">n_</span><span class="op">;</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">s_</span> <span class="op">=</span> ns<span class="op">;</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span></code></pre></div></li>
</ul></li>
<li><p>Heap and Priority Queue: Heaps are binary trees where the parent
node is less than or equal to its children (max-heap). The provided code
includes a function to test if an array represents a heap, as well as
functions for heapify and build_heap operations.</p>
<ul>
<li><p>Test heap function checks whether an array has heap property:</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="ex">ulong</span> test_heap<span class="op">(</span><span class="at">const</span> Type <span class="op">*</span>x<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> Type <span class="op">*</span>p <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> <span class="co">// make one-based</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="ex">ulong</span> k <span class="op">=</span> n<span class="op">;</span> k <span class="op">&gt;</span> <span class="dv">1</span><span class="op">;</span> <span class="op">--</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="ex">ulong</span> t <span class="op">=</span> <span class="op">(</span>k <span class="op">&gt;&gt;</span> <span class="dv">1</span><span class="op">);</span> <span class="co">// parent(k)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>p<span class="op">[</span>t<span class="op">]</span> <span class="op">&lt;</span> p<span class="op">[</span>k<span class="op">])</span> <span class="cf">return</span> k <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> <span class="co">// in {1, 2, ..., n}</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// has heap property</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>Heapify function ensures the heap property between a node and its
children:</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> heapify<span class="op">(</span>Type <span class="op">*</span>z<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">,</span> <span class="ex">ulong</span> k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> m <span class="op">=</span> k<span class="op">;</span> <span class="co">// index of max of k, left(k), and right(k)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="ex">ulong</span> l <span class="op">=</span> <span class="op">(</span>k <span class="op">&lt;&lt;</span> <span class="dv">1</span><span class="op">);</span> <span class="co">// left(k);</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">((</span>l <span class="op">&lt;=</span> n<span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="op">(</span>z<span class="op">[</span>l<span class="op">]</span> <span class="op">&gt;</span> z<span class="op">[</span>k<span class="op">]))</span> m <span class="op">=</span> l<span class="op">;</span> <span class="co">// left child (exists and) greater than k</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="ex">ulong</span> r <span class="op">=</span> <span class="op">(</span>k <span class="op">&lt;&lt;</span> <span class="dv">1</span><span class="op">)</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span> <span class="co">// right(k);</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">((</span>r <span class="op">&lt;=</span> n<span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="op">(</span>z<span class="op">[</span>r<span class="op">]</span> <span class="op">&gt;</span> z<span class="op">[</span>m<span class="op">]))</span> m <span class="op">=</span> r<span class="op">;</span> <span class="co">// right child (ex. and) greater than max(k,l)</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>m <span class="op">!=</span> k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    swap2<span class="op">(</span>z<span class="op">[</span>k<span class="op">],</span> z<span class="op">[</span>m<span class="op">]);</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    heapify<span class="op">(</span>z<span class="op">,</span> n<span class="op">,</span> m<span class="op">);</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p>Build heap function reorders an array into a heap from the bottom
up:</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> build_heap<span class="op">(</span>Type <span class="op">*</span>x<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  Type <span class="op">*</span>z <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> <span class="co">// make one-based</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">ulong</span> j <span class="op">=</span> <span class="op">(</span>n <span class="op">&gt;&gt;</span> <span class="dv">1</span><span class="op">);</span> <span class="co">// max index such that node has at least one child</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> <span class="op">(</span>j <span class="op">&gt;</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    j <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">1</span><span class="op">)</span> <span class="op">?</span> <span class="dv">0</span> <span class="op">:</span> <span class="op">((</span>j <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    heapify<span class="op">(</span>z<span class="op">,</span> n<span class="op">,</span> j<span class="op">);</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
</ul></li>
</ol>
<p>The provided text discusses several data structures and their
implementations: Heap, Priority Queue, Bit-array, and Left-right
array.</p>
<ol type="1">
<li><p><strong>Heap</strong>: A heap is a specialized tree-based data
structure that satisfies the heap property. The root of the tree is the
largest (max-heap) or smallest (min-heap) element. The given text
describes a max-heap implementation in C++. The <code>heapify</code>
function ensures the heap property by comparing and swapping elements,
with a time complexity of O(log n). Insertion into a heap takes O(log n)
time, as does extraction of the maximum (or minimum) element.</p></li>
<li><p><strong>Priority Queue</strong>: A priority queue is an abstract
data type that supports insertion and removal of elements according to
their priority. It can be implemented using a binary heap. The given
text describes a generic priority queue class in C++ that can extract
either the maximum or minimum element, depending on compile-time
options. This priority queue maintains two arrays: one for storing times
(t1_) and another for storing associated events (e1_).</p></li>
<li><p><strong>Bit-array</strong>: A bit-array is an array of Boolean
values used to save memory when dealing with small data types. The
provided text describes a C++ class <code>bitarray</code> that offers
methods for testing, setting, and clearing bits. It also includes
optimized versions using CPU instructions (for AMD64 architecture) or
macros for other architectures.</p></li>
<li><p><strong>Left-right array</strong>: A left-right array (LR-array)
is a data structure used to keep track of free and set indices in an
array. It supports marking the k-th free index as set, marking the k-th
set index as free, and finding how many indices of the same type are
left or right to any given index. The given text presents a C++ class
<code>left_right_array</code> that uses binary search to implement these
operations in O(log n) time.</p></li>
</ol>
<p>The LR-array’s key feature is its ability to efficiently mark free
and set indices using a recursive initialization method
(<code>init_rec</code>) and two main methods:</p>
<ul>
<li><code>get_free_idx(ulong k)</code> returns the k-th free index, with
a time complexity of O(log n).</li>
<li><code>get_set_idx_chg(ulong k)</code> returns the k-th set index
while changing it to a free index, also with a time complexity of O(log
n).</li>
</ul>
<p>These operations are essential for managing dynamic arrays or other
data structures that require efficient manipulation of free and set
indices.</p>
<p>The text discusses the generation of combinations, which are subsets
of a set containing a specific number of elements. Combinations are also
known as “k-subsets” or “k-combinations” of an n-set (a set with n
elements). The number of ways to choose k elements from a set of n
elements is given by the binomial coefficient, denoted as “n choose k”
or “k out of n”:</p>
<p>(n choose k) = n! / (k!(n - k)!)</p>
<p>or equivalently, using falling factorial notation:</p>
<p>(n choose k) = n^(k) / k!</p>
<p>This coefficient is also known as the binomial coefficient and can be
calculated using the formula:</p>
<p>(n choose k) = [(n - k + 1) * (n - k + 2) * … * n] / k!</p>
<p>Two common orders for generating combinations are lexicographic (lex)
and co-lexicographic (colex).</p>
<p>Lexicographic order generates the combinations in increasing
dictionary order, where each combination is smaller than the next one.
For example, when generating 3-combinations from {0,1,2}, the sequence
would be: {{0,1,2}},
{{0,1,2},{0,2,1},{1,0,2},{1,2,0},{2,0,1},{2,1,0}}.</p>
<p>Co-lexicographic order generates combinations in decreasing
dictionary order when the elements are reversed. For example, in
co-lexicographic order, 3-combinations from {0,1,2} would be:
{{0,1,2}},{{0,1,2},{0,2,1},{1,2,0},{2,1,0},{2,0,1},{1,0,2}}.</p>
<p>The text provides C++ class implementations for generating
combinations in both lexicographic and co-lexicographic orders. These
classes store the combination as an array of integers, where each
integer is less than or equal to k (the number of elements in the
combination) and greater than or equal to 0. The classes also include
methods for initializing a new combination, computing successors and
predecessors, and accessing the current combination.</p>
<p>The lexicographic order class (<code>combination_lex</code>)
generates combinations by incrementing the smallest possible element
that can be increased without violating the order constraints. If no
such element exists (i.e., the current combination is the last one), it
resets to the first combination.</p>
<p>The co-lexicographic order class (<code>combination_colex</code>)
works similarly but increments elements from right to left, reversing
the order before applying the increment and then reverting it back
afterward. This process ensures that combinations are generated in
decreasing lexicographic order when viewed as arrays of integers.</p>
<p>These implementations provide efficient ways to generate all possible
combinations of a given size from an n-element set, which can be useful
for various combinatorial problems and applications.</p>
<p>The Eades-McKay strong minimal-change order (SMMC) is an improvement
over the standard Gray code for generating combinations. In a standard
Gray code, only one element changes position between two successive
combinations, but when an element moves across another, multiple
elements in the set representation change. The SMMC aims to minimize
these additional changes.</p>
<p>In the SMMC order, each combination is represented as a “delta-set,”
which is a binary vector where 1s denote elements included and 0s denote
excluded elements. Two consecutive combinations differ by exactly one
bit flip (from 0 to 1 or from 1 to 0). This property ensures that only
two elements change in the set representation for any pair of adjacent
combinations, hence the “strong minimal-change” name.</p>
<p>To illustrate this order, let’s consider an example with n = 6 and k
= 3:</p>
<table>
<thead>
<tr class="header">
<th>Delta-set</th>
<th>Set Representation</th>
<th>Elements Changed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>{0,1,2}</td>
<td>{0,1,2}</td>
<td>{3,4,5}</td>
</tr>
<tr class="even">
<td>{0,1,3}</td>
<td>{0,1,3}</td>
<td>{2,4,5}</td>
</tr>
<tr class="odd">
<td>{0,2,3}</td>
<td>{0,2,3}</td>
<td>{1,4,5}</td>
</tr>
<tr class="even">
<td>{1,2,3}</td>
<td>{1,2,3}</td>
<td>{0,4,5}</td>
</tr>
<tr class="odd">
<td>{0,1,4}</td>
<td>{0,1,4}</td>
<td>{2,3,5}</td>
</tr>
<tr class="even">
<td>{0,2,4}</td>
<td>{0,2,4}</td>
<td>{1,3,5}</td>
</tr>
<tr class="odd">
<td>{1,2,4}</td>
<td>{1,2,4}</td>
<td>{0,3,5}</td>
</tr>
<tr class="even">
<td>{0,3,4}</td>
<td>{0,3,4}</td>
<td>{1,2,5}</td>
</tr>
<tr class="odd">
<td>{1,3,4}</td>
<td>{1,3,4}</td>
<td>{0,2,5}</td>
</tr>
<tr class="even">
<td>{2,3,4}</td>
<td>{2,3,4}</td>
<td>{0,1,5}</td>
</tr>
</tbody>
</table>
<p>As shown in the table, each pair of consecutive combinations differs
by exactly one bit flip. This is why SMMC is called “strong
minimal-change,” as it ensures that only two elements change for any
adjacent combination pairs.</p>
<p>The Eades-McKay algorithm generates these delta-sets systematically
to ensure this property holds. The generation process involves a careful
manipulation of the set sizes and element placements, ensuring that each
flip results in the minimal possible changes across adjacent
combinations. This order is valuable for applications where minimizing
changes between successive elements is crucial, such as in optimization
algorithms or when simulating dynamic systems.</p>
<p>The text discusses two specific ordering methods for combinations of
numbers, known as the Eades-McKay (EMK) order and two-close orderings
via endo/enup moves.</p>
<ol type="1">
<li><p><strong>Eades-McKay (EMK) Order</strong>: This is a strong
minimal-change order where only one entry in the set representation
changes per step, making it a Gray code with homogeneous moves. The EMK
sequence can be generated recursively or iteratively.</p>
<ul>
<li><p><strong>Recursive Generation</strong>: The recursive algorithm
for generating combinations in EMK order is provided. It involves three
cases: increasing the last element, decreasing the second-to-last while
keeping the last fixed, and keeping the second-to-last fixed while
increasing the last.</p></li>
<li><p><strong>Iterative Generation via Modulo Moves</strong>: An
iterative algorithm for generating combinations in EMK order is also
presented. This method uses modulo steps to compute successors. The rate
of generation varies depending on the number of elements (n) and
combination size (k).</p></li>
</ul></li>
<li><p><strong>Two-close Orderings via Endo/Enup Moves</strong>: These
are two different ways to generate sequences with specific properties
related to even and odd numbers.</p>
<ul>
<li><p><strong>Endo Sequence</strong>: This order starts with all odd
numbers from 1 up to m in increasing order, followed by all even numbers
from m down to 0 in decreasing order. A routine for generating the
successor in endo order is provided.</p></li>
<li><p><strong>Enup Sequence (Even Numbers Up, Odd Numbers
Down)</strong>: This sequence starts with all even numbers from 0 up to
m in increasing order, followed by all odd numbers from m down to 1 in
decreasing order. The routine for generating the successor in enup order
is also provided. These two sequences are reversals of each
other.</p></li>
</ul></li>
</ol>
<p>The text concludes by defining functions to compute the x-th number
in enup order with a maximal digit (m) and the previous number in both
endo and enup orders for a given number (x).</p>
<p>The provided text describes four different orderings for generating
combinations, which are visualized in Figure 6.7-A. Here’s a detailed
explanation of each ordering:</p>
<ol type="1">
<li><p><strong>Lexicographic Order</strong>: This is the natural,
increasing order where elements are arranged from smallest to largest.
In other words, it follows the dictionary order. For example, for the
set {0, 1, 2}, the lexicographic combinations are (0, 1, 2), (0, 2, 1),
(1, 0, 2), etc.</p></li>
<li><p><strong>Gray Code</strong>: Gray code is a binary numeral system
where two successive values differ in only one bit. This property makes
it useful for applications requiring minimal change between adjacent
values, like in error correction and digital communications. In the
context of combinations, the Gray code ordering ensures that each
combination differs from its neighbors by just one element.</p></li>
<li><p><strong>Complemented Enup Order</strong>: Enup stands for
“even-numbered up.” This order is generated using an algorithm called
<code>endo_num</code>. It maps numbers to combinations in a way that
moves right on even positions. The term “complemented” means that the
sequence considers complements of sets (i.e., all possible subsets
except the set itself) when generating combinations.</p></li>
<li><p><strong>Complemented Eades-McKay Sequence</strong>: This is
another ordering for generating combinations, similar to the
Complemented Enup Order but following a different algorithm. The
Eades-McKay sequence was developed by David Eades and John McKay in
their 1992 paper “A new Gray code for combinations.” Like the
Complemented Enup Order, it considers complements of sets when
generating combinations.</p></li>
</ol>
<p>The text also presents a recursive algorithm to generate these orders
using a class named <code>comb_rec</code>. This algorithm accepts a
visitor function that gets called with each generated combination and
parameters determining which order (<code>rq_</code>) to use
(lexicographic, Gray code, complemented enup, or complemented
Eades-McKay) and whether to reverse the order (<code>nq_</code>).</p>
<p>The recursion is implemented in the <code>next_rec()</code> function.
It uses an array <code>rv_</code> to store the current combination and
updates it based on the chosen order. The conditions for updating the
combination are controlled by the <code>rq_</code> parameter, which can
take values 0 (lexicographic), 1 (Gray code), 2 (complemented enup), or
3 (complemented Eades-McKay).</p>
<p>In summary, this section of the text discusses four different ways to
order combinations and provides a flexible recursive algorithm for
generating these orders using customizable parameters.</p>
<p>The provided text discusses two topics related to combinatorics,
specifically compositions and combinations, along with their respective
generation methods and transformations between them.</p>
<p><strong>Compositions:</strong></p>
<p>A composition of a non-negative integer <code>n</code> into at most
<code>k</code> parts is an ordered sequence of non-negative integers
<code>(x0, x1, ..., xk-1)</code> such that
<code>x0 + x1 + ... + xk-1 = n</code>, and each part <code>xi</code> is
less than or equal to <code>n</code>. Order matters in compositions;
different sequences are considered distinct even if they sum up to the
same value. For instance, <code>(0, 1, 5, 1)</code> and
<code>(5, 0, 1, 1)</code> are separate 4-compositions of 7.</p>
<p>Two algorithms for generating compositions in co-lexicographic order
(colex) were presented:</p>
<ol type="1">
<li><p><code>composition_colex</code>: This algorithm uses an array to
store the composition’s parts and has methods to set the first
(<code>first()</code>) and last (<code>last()</code>) composition, as
well as to compute successor (<code>next()</code>) and predecessor
(<code>prev()</code>) compositions. The complexity of this
implementation is efficient for dense cases where <code>k</code> (number
of parts) is much greater than <code>n</code> (sum).</p></li>
<li><p><code>composition_ex_colex</code>: This optimized algorithm
handles sparse cases efficiently by introducing an additional variable
(<code>p0_</code>) to track the position of the first non-zero entry.
The methods for computing successor and predecessor are adaptations from
the <code>composition_colex</code> class, making it equally fast for
dense or sparse compositions.</p></li>
</ol>
<p><strong>Combinations:</strong></p>
<p>A combination is a selection of items without regard to their order.
In this context, the text refers to combinations in terms of choosing
<code>K</code> elements out of <code>N</code>, denoted as
<code>B(N, K)</code>. The relationship between compositions and
combinations is established through delta sets:</p>
<ul>
<li>A run of <code>r</code> consecutive ones in a binary representation
corresponds to an entry <code>r</code> in a composition at the
corresponding position.</li>
</ul>
<p>The text mentions that while the sequence of combinations can be
represented as a Gray code (a binary reflected Gray code), the sequence
of compositions is not a Gray code.</p>
<p><strong>Conversion between Compositions and
Combinations:</strong></p>
<p>The provided text includes a conversion routine
<code>comp2comb</code> to transform a given composition into its
corresponding combination:</p>
<ol type="1">
<li>Input:
<ul>
<li><code>p[]</code>: Array representing the composition.</li>
<li><code>k</code>: Number of parts in the composition.</li>
<li><code>b[]</code>: Output array for storing the combination
(initialized with enough space).</li>
</ul></li>
<li>Process:
<ul>
<li>Iterate through each part of the composition (<code>j</code> from 0
to <code>k-1</code>).</li>
<li>For each part (<code>pj</code>), add that number of consecutive
values starting from <code>z</code> to the output combination array
<code>b[]</code>.</li>
<li>Increment <code>z</code> to point to the next available value.</li>
</ul></li>
</ol>
<p>This routine transforms a composition into its equivalent combination
by assigning values in the output array based on the length and position
of each part within the input composition.</p>
<p>The text discusses two methods for generating all subsets of a set
with n elements, namely lexicographic order with sets and delta
sets.</p>
<ol type="1">
<li>Lexicographic Order with Sets:
<ul>
<li>This method represents each subset as a binary array where 1s denote
the presence of an element in the subset and 0s denote its absence.</li>
<li>The class <code>subset_lex</code> in the FXT library implements this
approach. It initializes an array <code>x_</code> of size n, with the
first call to <code>first()</code> setting all elements to zero
(representing the empty set).</li>
<li>The <code>next()</code> function increments the last non-zero
element in the binary representation and returns its index plus one,
which corresponds to the number of elements in the new subset.</li>
<li>The generation speed is around 176 million subsets per second using
standard arrays or approximately 192 M/s with optimized array
usage.</li>
</ul></li>
<li>Lexicographic Order with Delta Sets:
<ul>
<li>In this method, each subset is represented as a delta set, where the
i-th element indicates whether element i is included in the subset
(non-zero) or not (zero).</li>
<li>The class <code>subset_deltalex</code> in the FXT library implements
this approach. It initializes an array <code>d_</code> of size n+1 with
sentinel value 0 at the end.</li>
<li>The <code>first()</code> function sets all elements to zero,
representing the empty set.</li>
<li>The <code>next()</code> function uses binary counting to find the
next non-zero element, increments it by one, and returns its index plus
one (representing the number of elements in the new subset).</li>
<li>This method generates around 176 million subsets per second using
standard arrays or approximately 192 M/s with optimized array
usage.</li>
</ul></li>
</ol>
<p>Both methods generate all nonempty subsets of a set with n elements,
totaling 2^n - 1 subsets. The main difference lies in the
representation: sets use binary arrays (0s and 1s), while delta sets
represent each subset as an array where non-zero values indicate
included elements.</p>
<p>The lexicographic order ensures that subsets are generated in a
systematic, ascending manner. By changing the direction of increments at
odd or even positions in the recursion formula for generating
combinations (as shown in Figure 7.4-A and B), different minimal-change
orders (Gray codes) can be obtained to generate compositions and
combinations with specific properties.</p>
<p>The provided text discusses different methods for generating subsets
of a given set in an order that minimizes changes between consecutive
subsets. This is known as the “minimal-change order”. The following are
the three main approaches outlined:</p>
<ol type="1">
<li><strong>Gray Code with Delta Sets:</strong>
<ul>
<li>This method represents each subset as a delta-set, where only the
difference from the previous subset is stored.</li>
<li>The class <code>subset_gray_delta</code> implements this approach.
It uses the Gray code of binary words to determine the order in which
subsets are generated.</li>
<li>When generating the next subset, it finds the position where the
Gray code changes and toggles that bit. The position of change
(<code>j_</code>) is then returned.</li>
<li>The <code>prev()</code> function works similarly but in reverse,
decrementing the counter until it reaches zero.</li>
<li>This method can generate about 180 million subsets per second.</li>
</ul></li>
<li><strong>Gray Code with Set Representation:</strong>
<ul>
<li>In this approach, each subset is represented as a set of numbers
from 1 to n. The class <code>subset_gray</code> implements this.</li>
<li>It uses the Gray code for minimal-change order but represents
subsets differently: <code>x_[k_] = n_</code> means that element
<code>n_</code> is included in position <code>k_</code>.</li>
<li>The <code>next()</code> and <code>prev()</code> methods adjust the
subset by either removing or adding elements based on specific
conditions.</li>
<li>This method can generate about 241 million subsets per second with
<code>next()</code>, and around 167 M/s with <code>prev()</code>.</li>
</ul></li>
<li><strong>Computing Positions of Change (Ruler Function):</strong>
<ul>
<li>Unlike the previous methods that generate full subsets, this class
<code>ruler_func</code> only computes the positions where changes occur
in generating subsets in Gray code order.</li>
<li>It uses a technique called the “Ruler function sequence”, which
produces a sequence of numbers where each number indicates the position
of change when generating a Gray code.</li>
<li>The <code>next()</code> method returns these positions of change,
allowing users to generate subsets or perform other operations based on
these positions.</li>
<li>This method can generate about 244 M/s with pointers and 293 M/s
with arrays.</li>
</ul></li>
</ol>
<p>Each method has its own trade-offs in terms of memory usage,
computational complexity, and speed. The choice between them would
depend on the specific requirements and constraints of the application
at hand.</p>
<p>This text discusses various methods for ordering subsets of a given
set using different sequences or algorithms, with a focus on De Bruijn
sequences and shift-based orders. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>De Bruijn Sequence Ordering</strong>: This method
generates an ordering for all subsets of a given set based on a cyclic
binary sequence (De Bruijn sequence). Each subset is the result of
shifting its predecessor to the right and adding the next element from
the De Bruijn sequence. The example provided is for a 5-element set,
with the De Bruijn sequence being:</p>
<p><code>10001100101011101000</code></p>
<p>This ordering has the property that each column in the delta set
representation (a visualization of subsets) is a circular shift of the
first column.</p></li>
<li><p><strong>Sequency-Complemented Subsets</strong>: By complementing
elements at even indices, an alternative ordering can be obtained. This
approach generates more frequent transitions between subsets with
smaller sequencies (a measure of the distance between binary sequences).
The resulting order corresponds to a complement-shift sequence.</p></li>
<li><p><strong>Shifts-Order for Subsets</strong>: Figure 8.4-A
illustrates an ordering where all linear shifts of a word appear in
succession. This order is generated using a simple recursive algorithm
(line 4-15). The transitions not involving shifts change just one bit,
resulting in a minimal-change shifts-order as shown in Figure
8.4-B.</p></li>
<li><p><strong>Minimal-Change Shifts-Order</strong>: This order ensures
that transitions not involving shifts change only one bit (Figure
8.4-B). The recursion functions <code>F</code> and <code>G</code> are
used to generate this ordering, with <code>F(2*x)</code> for non-shift
transitions and <code>G(2*x+1)</code> for shift transitions.</p></li>
<li><p><strong>Fibonacci Words in Shifts-Order</strong>: A simple
variation of the previous algorithm can be employed to generate
Fibonacci words (infinite sequences where each finite prefix appears as
a substring) in an order where all shifts appear in succession (Figure
8.4-C). The recursion function <code>B(ulong x)</code> is used for this
purpose, with transitions that generally change more than one
bit.</p></li>
<li><p><strong>k-subsets within a Range</strong>: The text also covers
algorithms to generate k-subsets of an n-set where the value of k lies
in a given range (kmin ≤ k ≤ kmax). It introduces a recursive algorithm
(<code>class ksubset_rec</code>) that generates subsets in one of
sixteen possible orders, determined by the variable <code>rq</code>. The
order can be lexicographic or Gray codes, and it accepts a user-defined
function to process each subset.</p></li>
</ol>
<p>In summary, this text explores different ways to order subsets of a
set using sequences (De Bruijn) and recursive algorithms that generate
patterns like linear shifts or minimal bit changes between consecutive
subsets. These methods find applications in combinatorics, coding
theory, and other areas requiring systematic exploration of subset
combinations.</p>
<p>The provided text describes two algorithms for generating k-subsets
of a given set, where the size of the subsets (k) lies within a
specified range. Both algorithms aim to produce these subsets in an
order that minimizes changes between consecutive subsets, known as
“minimal-change” or “Gray code” order.</p>
<ol type="1">
<li>Recursive Algorithm:</li>
</ol>
<p>The recursive algorithm is defined by the <code>ksubset_rec</code>
class and its methods. Here’s a detailed explanation of the key
components:</p>
<ul>
<li><p><strong>Initialization (lines 2-18)</strong>: The constructor
initializes the parameters for subset generation, including the total
number of elements (<code>n_</code>), the minimum (<code>kmin_</code>)
and maximum (<code>kmax_</code>) allowed subset sizes, and some
auxiliary variables. It ensures <code>kmin</code> is less than or equal
to <code>kmax</code>, and if not, it swaps their values. If
<code>kmin</code> is greater than <code>n</code>, it’s set to
<code>n</code>. Similarly, if <code>kmax</code> exceeds <code>n</code>,
it’s clipped at <code>n</code>.</p></li>
<li><p><strong>Next Subset Generation (lines 19-39)</strong>: The
<code>next_rec</code> method generates the next k-subset in the
minimal-change order. It uses a helper function
<code>next_rec(long d)</code> that navigates through the subsets level
by level, incrementing or decrementing elements based on certain
conditions determined by <code>rq_</code> and <code>pq_</code>
variables. The changes depend on whether the current position is even or
odd, and if a nonzero <code>nq</code> is provided, it reverses the
order.</p></li>
</ul>
<ol start="2" type="1">
<li>Iterative Algorithm:</li>
</ol>
<p>The iterative algorithm generates k-subsets in minimal-change order
using the <code>ksubset_gray</code> class. Here’s an explanation of its
key components:</p>
<ul>
<li><p><strong>Initialization (lines 11-39)</strong>: The constructor
initializes parameters similarly to the recursive algorithm, with
additional variables for tracking the current subset (<code>S_</code>)
and position within it (<code>j_</code>). It sets up the initial state,
ensuring <code>kmin</code> is at least 1.</p></li>
<li><p><strong>First Subset (lines 22-48)</strong>: The
<code>first()</code> method initializes the first k-subset in
minimal-change order by populating the <code>S_</code> array with values
from <code>n_ - kmin + i</code>, where <code>i</code> ranges from 1 to
<code>kmin</code>.</p></li>
<li><p><strong>Last Subset (lines 30-39)</strong>: The
<code>last()</code> method generates the last k-subset in minimal-change
order by setting <code>S_[1]</code> to 1 and adjusting subsequent values
accordingly. It also updates the position counter <code>j_</code> based
on the current value of <code>kmin</code>.</p></li>
<li><p><strong>Prev_Even/Odd Methods (lines 49-67)</strong>: These
private helper methods manage moving backward through the k-subsets,
updating the <code>S_</code> array and adjusting the position counter
<code>j_</code> as necessary. They handle cases where elements can touch
the sentinel (<code>S[0]</code>) or not, and whether the current
position is even or odd.</p></li>
</ul>
<p>Both algorithms aim to generate k-subsets in a minimal-change order,
which is useful for various applications such as combinatorial testing,
data encoding, and more. The provided code snippets show how these
algorithms can be implemented in C++.</p>
<p>The provided text describes several concepts related to number
theory, specifically focusing on mixed radix numbers and their orders.
Here’s a detailed explanation:</p>
<ol type="1">
<li><p><strong>Mixed Radix Numbers</strong>: A mixed radix
representation of a number x is given by equation (9.0-1):</p>
<p>x = ∑(k=0 to n-1) ak * ∏(j=0 to k-1) mj</p>
<p>where 0 ≤ ak &lt; mj, and the total sum should not exceed Qn⁻¹ j=0
mj, ensuring that n digits are sufficient to represent x. When all radii
(mj) are equal (M = [r, r, …, r]), this simplifies to the standard
radix-r representation (9.0-2).</p></li>
<li><p><strong>Two-Close Order</strong>: This is a specific ordering of
k-subsets where one element is inserted or removed, or moves by at most
two positions. The changes are homogenous, meaning they only cross a
zero and all changes are either insertions or deletions, not a mix of
both.</p></li>
<li><p><strong>Mixed Radix Lexicographic Order
(mixedradix_lex)</strong>: This class generates mixed radix numbers in
lexicographic order. It uses an array ‘a_’ for digits and another array
‘m1_’ for the radices minus one. The constructor initializes these
arrays, and a recursive function <code>next_rec()</code> is used to
generate the next number in the sequence.</p></li>
<li><p><strong>Initialization (mixedradix_init)</strong>: This is an
auxiliary function that initializes the vector of nines (digits equal to
the respective radix minus one) for mixed radix classes. If all radices
are given, it directly assigns each radix-1 value; otherwise, if no
radices are provided, it assumes a uniform radix r = m0 + 1.</p></li>
<li><p><strong>Various Orders</strong>: The text mentions different
visual orders for mixed radix numbers: counting (lexicographic), Gray
code, modular Gray code, gslex, endo, and endo Gray. These orders
represent the same set of numbers but in different sequences, which can
be useful for various applications like error detection and correction
in data transmission.</p></li>
<li><p><strong>Performance</strong>: The text also provides performance
metrics for generating subsets using these algorithms: about 150 million
subsets per second with ‘next()’, 130 million with ‘prev()’, and 75
million with the two-close order algorithm.</p></li>
</ol>
<p>In summary, this text discusses mixed radix numbers, their
representations, and different ordering methods. It also introduces a
C++ class <code>mixedradix_lex</code> to generate mixed radix numbers in
lexicographic order and provides context about related orders and
performance metrics.</p>
<p>The text discusses various algorithms for generating mixed radix
numbers, which are numbers represented in a non-standard positional
numeral system. Mixed radix systems use different bases (radices) for
each digit position rather than the uniform base we’re used to in
decimal (base 10).</p>
<p><strong>9.1: Lexicographic Order Generation</strong></p>
<p>The lexicographic order (dictionary order, alphabetical order) is a
natural way to arrange mixed radix numbers. The given C++ code snippet
provides two functions, <code>next()</code> and <code>prev()</code>, for
generating the next and previous mixed radix numbers in lexicographic
order, respectively.</p>
<ol type="1">
<li><p><strong>Initialization</strong>: The number of digits
<code>n_</code> is initialized, and arrays <code>a_</code>,
<code>m1_</code>, and <code>j_</code> are created to store the current
number, its radices (minus one), and the position of the last change,
respectively.</p></li>
<li><p><strong><code>next()</code> function</strong>: This increments
the mixed radix number by setting all digits equal to their respective
base minus one (<code>m1_[k] - 1</code>) at the lower end to zero and
incrementing the next digit. It does this until it finds a digit that is
less than its base, then increments it.</p></li>
<li><p><strong><code>prev()</code> function</strong>: This decrements
the mixed radix number by setting all zero digits at the lower end to
their respective base minus one and decrementing the next
digit.</p></li>
</ol>
<p>The time complexity for generating numbers in lexicographic order
varies with the radix vector <code>M</code>. For a uniform radix (all
elements of <code>M</code> are equal), the number of carries when
incrementing is maximal, leading to slower generation rates.</p>
<p><strong>9.2: Gray Code Order Generation</strong></p>
<p>Gray code, also known as reflected binary code, is a binary numeral
system where two successive values differ in only one digit. The text
describes two algorithms for generating mixed radix numbers in Gray code
order, which minimizes the number of changes between consecutive
numbers.</p>
<ol type="1">
<li><p><strong>Constant Amortized Time (CAT) Algorithm</strong>: This
algorithm uses an array <code>i_</code> to store the ‘directions’ (+1 or
-1) for each digit. The direction is flipped when incrementing a digit
would cause it to exceed its base, and the digit is updated
accordingly.</p></li>
<li><p><strong>Loopless Algorithm</strong>: This is an optimized version
of the Gray code generation algorithm. It uses arrays <code>f_</code>
(focus pointer) and <code>d_</code> (direction). When incrementing, if
the new value for a digit would exceed its base, it changes direction by
looking up the next position in <code>f_</code>.</p></li>
</ol>
<p><strong>9.3: gslex Order Generation</strong></p>
<p>gslex order is another way to generate mixed radix numbers. The
provided code snippet describes an algorithm for this order but doesn’t
go into detail on how it works.</p>
<p>The key points are: - It uses radices [2, 3, 4] and [4, 3, 2] as
examples. - Numbers are represented as arrays where each element
corresponds to a digit in the mixed radix system. - The positions of
changes between consecutive numbers follow a specific pattern, shown in
Figures 9.2-A and 9.2-B for Gray code order, and presumably different
for gslex order.</p>
<p>The performance metrics (generation rates in M/s) are provided for
each algorithm under various radix conditions (radix 2, 4, and 8),
highlighting that the speed can vary significantly depending on the
radix vector used.</p>
<p>In summary, these algorithms provide methods for generating mixed
radix numbers in different orders, each with its own advantages in terms
of the pattern of changes between consecutive numbers, which can be
crucial for various applications like combinatorial search or
simulation.</p>
<p>The text describes four distinct methods for ordering mixed radix
numbers, each with its own algorithm and implementation.</p>
<ol type="1">
<li><p><strong>Generalized Subset Lexicographic (gslex)
Order:</strong></p>
<p>This order is a generalization of the subset lexicographic order to
mixed radix numbers. It’s generated by incrementing digits starting from
zero until a digit different from one is encountered, then increment
that digit. If all subsequent digits are also one, decrement them before
moving to the next higher digit. The generator works in constant
amortized time and generates approximately 123 million objects per
second for radix 2, increasing to around 273 M/s for radix 16.</p>
<p>Implementation details include an array <code>a_</code> for storing
digits, another array <code>m1_</code> that holds the radix minus one at
each position, and a sentinel at <code>a_[n_]</code>. The method
<code>next()</code> computes successors using this algorithm.</p></li>
<li><p><strong>Alternative gslex Order:</strong></p>
<p>This variant of the gslex order is achieved by reversing the list of
numbers, reversing the words within each number, and replacing non-zero
digits with their corresponding radii minus the digit value. The
generation rate remains similar to the regular gslex order.</p></li>
<li><p><strong>Endo Order:</strong></p>
<p>In endo order, successors are generated by incrementing elements in a
way that reflects the nature of the mixed radix system. This method uses
an additional array <code>le_</code> that stores the last nonzero
element for each position in endo order (2 if the radix is greater than
1, else 1). The generation rate is between approximately 115 million and
180 million numbers per second, depending on the radix.</p>
<p>Implementation involves an extra array <code>le_</code> to store
these values, with methods <code>first()</code> and <code>last()</code>
setting up initial and final states respectively. The method
<code>next_endo()</code> handles the increment operation for endo
order.</p></li>
<li><p><strong>Endo Gray Code:</strong></p>
<p>This is a modification of the CAT algorithm for Gray code applied to
mixed radix numbers in endo order. It introduces an array
<code>i_</code> that tracks the direction of movement (+1 for forward,
-1 for backward).</p>
<p>In the computation of the successor, the algorithm checks if it needs
to increment or decrement based on the current digit and its
corresponding radix. If there’s an overflow (when incrementing), it
changes direction. The method <code>next()</code> uses this
strategy.</p></li>
</ol>
<p>The ‘Gray code’ version ensures that only one digit (bit) flips
between successive numbers, preserving a property of standard Gray codes
in binary systems for mixed-radix settings.</p>
<p>The provided text discusses methods for representing permutations
using factorial number systems, specifically falling factorial base and
rising factorial base. These representations are known as Lehmer codes
(or inversion tables) and their inverses.</p>
<ol type="1">
<li><p><strong>Lehmer Code/Inversion Table</strong>: Each permutation of
n elements can be represented uniquely by an (n-1)-digit number in the
falling factorial base. This representation, called the Lehmer code or
inversion table, is computed by counting the number of elements to the
right of each index k that are less than the element at that index. For
example, for permutation P = [3, 0, 1, 4, 2], the inversion table I =
[3, 0, 0, 1] is obtained by counting how many elements with larger
indices are smaller than each position’s element.</p></li>
<li><p><strong>Conversion Routines</strong>:</p>
<ul>
<li><code>perm2ffact</code>: Converts a permutation to its Lehmer code
(falling factorial base).</li>
<li><code>ffact2perm</code>: Converts a Lehmer code back to the original
permutation.</li>
<li><code>perm2rfact</code>: Converts a permutation to its rising
factorial representation.</li>
<li><code>rfact2perm</code>: Converts a rising factorial representation
back to the original permutation.</li>
</ul></li>
<li><p><strong>Inverse Permutations</strong>: Routines for computing
inverse permutations from Lehmer codes (<code>ffact2invperm</code>) and
rising factorial representations (<code>rfact2invperm</code>).</p></li>
<li><p><strong>Large Arrays</strong>: For handling large arrays (e.g.,
millions of entries), left-right array data structures are used to
optimize the conversion process, reducing computational complexity to
O(n log n).</p></li>
<li><p><strong>Falling vs Rising Factorial Bases</strong>: Falling
factorial base uses radices in descending order (2, 3, 4, …), while
rising factorial base uses ascending radices (2, 3, 4, …). Permutations
corresponding to Lehmer codes and their reversed/complemented versions
are shown for both bases.</p></li>
<li><p><strong>Applications</strong>: These factorial representations of
permutations have applications in various fields, including
combinatorics, computer science, and cryptography.</p></li>
</ol>
<p>The text discusses various methods to represent permutations using
factorial numbers, focusing on the conversion of routines that compute
permutations from factorial numbers into those that compute inverse
permutations. The changes required for this conversion are
straightforward: swapping ‘x[a] = b’ with ‘x[b] = a’.</p>
<ol type="1">
<li><p><strong>Falling Factorial Base (ffact)</strong>: This method uses
rotations in the computation of a permutation from its Lehmer code. The
routine <code>perm2ffact_rev(const ulong *x, ulong n, ulong *fc)</code>
computes the factorial representation of a given permutation using this
method. Its inverse is
<code>ffact2perm_rev(const ulong *fc, ulong n, ulong *x)</code>.</p></li>
<li><p><strong>Rising Factorial Base (rfact)</strong>: This method also
uses rotations but in a different manner than the falling factorial
base. The routine
<code>perm2rfact_rev(const ulong *x, ulong n, ulong *fc)</code> computes
the rising factorial representation of a given permutation, and its
inverse is
<code>rfact2perm_rev(const ulong *fc, ulong n, ulong *x)</code>.</p></li>
<li><p><strong>Representation via Swaps</strong>: This method represents
permutations using swaps. The complexity of this direct implementation
is O(n). The routines
<code>perm2ffact_swp(const ulong *x, ulong n, ulong *fc)</code> and
<code>perm2rfact_swp(const ulong *x, ulong n, ulong *fc)</code> compute
the factorial representations using swaps for falling and rising
factorial bases, respectively. The inverse routines are not explicitly
mentioned in this text but can be inferred as ‘ffact2perm_swp’ and
‘rfact2perm_swp’.</p></li>
<li><p><strong>Representation via Rotations</strong>: There are two
types of rotations used here: fixed length with variable rotation amount
(ffact2perm_rot, rfact2perm_rot) and fixed rotation amount with variable
length (perm2ffact_rot, perm2rfact_rot). These methods provide
alternative ways to compute permutations from factorial
numbers.</p></li>
</ol>
<p>The text also includes algorithms for calculating the number of
inversions in a permutation using both O(n^2) and O(n log n) methods. It
concludes with visual representations (Figure 10.1-C and Figure 10.1-D)
that depict falling and rising factorial numbers alongside their
corresponding permutations, illustrating how changes in the code lead to
different permutation representations.</p>
<p>The provided text describes four different methods for generating
permutations of a set of n elements, each following a specific order.
Here’s a detailed summary and explanation of these methods:</p>
<ol type="1">
<li><strong>Falling Factorial (ffact) Order</strong>:
<ul>
<li>The falling factorial representation encodes a permutation as an
array where the element at position k is the difference between the
original value at that position and k.</li>
<li>The corresponding inverse permutation has the rising factorial
representation, which is digit-reversed.</li>
<li>The routines <code>perm2ffact_swp</code> (for falling base) and
<code>ffact2perm_swp</code> generate permutations in this order, while
<code>ffact2invperm_swp</code> computes the inverse permutation.</li>
</ul></li>
<li><strong>Rising Factorial (rfact) Order</strong>:
<ul>
<li>The rising factorial representation encodes a permutation as an
array where the element at position k is the difference between the
original value at that position and n-1-k.</li>
<li>The corresponding inverse permutation has the falling factorial
representation, which is digit-reversed.</li>
<li>The routines <code>perm2rfact_swp</code> (for rising base) and
<code>rfact2perm_swp</code> generate permutations in this order, while
<code>rfact2invperm_swp</code> computes the inverse permutation.</li>
</ul></li>
<li><strong>Lexicographic Order</strong>:
<ul>
<li>In lexicographic order, permutations are sorted as if they were
numbers written in ascending order.</li>
<li>The class <code>perm_lex</code> implements an iterative algorithm
for generating permutations in this order, with the method
<code>next()</code> computing the next permutation with each call.</li>
</ul></li>
<li><strong>Co-Lexicographic Order</strong>:
<ul>
<li>In co-lexicographic order (colex), permutations are sorted such
that, at any position, the value decreases as we move rightward across
the permutation.</li>
<li>The class <code>perm_colex</code> implements an algorithm for
generating permutations in this order, using rising factorial numbers to
update the permutation when necessary.</li>
</ul></li>
<li><strong>Reversing Prefixes Order</strong>:
<ul>
<li>In this order, whenever the first j digits of a mixed radix number
(with radii [2, 3, 4, …]) change with an increment, the permutation is
updated by reversing the first j+1 elements.</li>
<li>The class <code>perm_rev</code> generates permutations in this order
using the method described above.</li>
</ul></li>
</ol>
<p>Each method has its own advantages and applications, such as finding
interesting orders for permutations or serving as a basis for generating
other permutation orders. These routines can be used to efficiently
generate permutations for various combinatorial problems and
algorithms.</p>
<p>The provided text discusses several methods for generating
permutations in a minimal-change order, which is a strategy that
minimizes the number of elements swapped during each update to generate
the next permutation. This approach is useful in various applications
such as combinatorial algorithms and statistical simulations where the
order of generation can significantly impact performance.</p>
<ol type="1">
<li><p><strong>Heap’s Algorithm</strong>: Heap’s algorithm generates
permutations in a minimal-change order using mixed radix representation
with rising factorial base. The idea is to increment the mixed radix
number while keeping it in ascending order, swapping elements only when
necessary to maintain this property. This method ensures that each
update changes at most one digit in the mixed radix representation,
resulting in a Gray code for permutations up to four elements.</p>
<ul>
<li><strong>Class Implementation</strong>: <code>perm_heap</code></li>
<li><strong>Next Permutation Generation</strong>: The algorithm
increments the mixed radix number by finding the rightmost
non-increasing element and swapping it with the subsequent element if
needed. It uses an auxiliary counter to keep track of the swaps and
reset it when certain conditions are met, optimizing for common
cases.</li>
<li><strong>Performance</strong>: Generates about 133 million
permutations per second on average.</li>
</ul></li>
<li><p><strong>Optimized Heap’s Algorithm (Heap’s Algorithm with
Counter)</strong>: This is a further optimization of Heap’s algorithm
that recognizes and handles five specific swap patterns (0,1), (0,2),
(0,1), (0,2), and (0,1) separately. By doing so, it reduces the number
of comparisons needed, leading to faster permutation generation.</p>
<ul>
<li><strong>Class Implementation</strong>: <code>perm_rev2</code></li>
<li><strong>Next Permutation Generation</strong>: The counter is used to
quickly handle common swap patterns, significantly speeding up the
algorithm. It generates around 275 million permutations per second on
average.</li>
</ul></li>
<li><p><strong>Lipski’s Minimal-Change Orders</strong>: Lipski
introduced several variants of Heap’s method, each with a unique way of
incrementing mixed radix numbers to generate minimal-change orders.
These variations aim to improve the generation rate and adaptability for
various permutation sizes.</p>
<ul>
<li><strong>Class Implementation</strong>:
<code>perm_gray_lipski</code></li>
<li><strong>Next Permutation Generation</strong>: The algorithm
increments the mixed radix number based on different strategies
controlled by a parameter <code>r</code>. Each value of <code>r</code>
corresponds to a specific minimal-change order, offering flexibility in
choosing the most efficient variant for given use cases. The generation
rate varies depending on the chosen order but typically ranges from 100
million to 280 million permutations per second.</li>
</ul></li>
<li><p><strong>Wells’ Algorithm</strong>: Wells’ algorithm is another
method for generating permutations, which differs from Heap’s by using a
different strategy for incrementing mixed radix numbers and handling
swaps. It offers various variants controlled by a parameter
<code>d</code>.</p>
<ul>
<li><strong>Next Permutation Generation</strong>: The algorithm
increments the mixed radix number according to specific rules based on
the parameter <code>d</code>, resulting in different minimal-change
orders. The provided text does not detail the full implementation or
performance metrics of Wells’ algorithm, but it’s mentioned that
optimizations similar to those for Heap’s method should be
apparent.</li>
</ul></li>
</ol>
<p>In summary, these algorithms focus on generating permutations with
minimal changes (swaps) between consecutive permutations by utilizing
mixed radix representations and various strategies for incrementing the
representation. The choice of algorithm depends on factors such as the
desired permutation size, performance requirements, and flexibility in
choosing different minimal-change orders. Optimizations like counters
and handling specific swap patterns can significantly improve generation
rates across these methods.</p>
<p>The text discusses two permutation orders, Trotter’s strong
minimal-change order and Star-transposition order, along with their
implementations.</p>
<ol type="1">
<li><p><strong>Trotter’s Strong Minimal-Change Order (Section
10.7):</strong></p>
<p>This ordering ensures that in each step of generating the next
permutation, only adjacent elements are swapped, making it a “minimal
change” order. The algorithm is based on H. F. Trotter’s construction
from 1962 and has been optimized for efficiency.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li>The algorithm generates permutations of n elements.</li>
<li>It uses sentinel elements (0 and n) at the lower and upper ends of
the array to simplify boundary conditions.</li>
<li>An auxiliary array <code>d_</code> stores direction flags (+1 or
-1), initially set to +1, indicating whether to move an element up (-1)
or down (+1).</li>
<li>The <code>next()</code> method finds the smallest element whose
neighbor is greater (in the case of moving up) and swaps them, changing
the direction of all elements that couldn’t be moved.</li>
<li>The <code>prev()</code> method is almost identical but negates the
direction to find the previous permutation.</li>
</ul>
<p><strong>Optimizations:</strong></p>
<ul>
<li>A special case is handled for the element zero, which moves most
often. This optimization speeds up computations significantly.</li>
</ul></li>
<li><p><strong>Star-transposition Order (Section 10.8):</strong></p>
<p>In this ordering, successive permutations differ by a swap of the
first element with another element (star transposition). The list of
inverse permutations always includes the movement of zero.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li>Each permutation differs from the previous one by swapping the first
element with some other element.</li>
<li>The inverse permutations’ sequence of positions swapped with the
first position is entry A123400 in [312].</li>
<li>The sequence of positions of the element zero is entry A159880,
which can be constructed by considering permutations described in
section 10.4 on page 245 and computing inverse permutations as shown in
figure 10.8-C.</li>
</ul>
<p><strong>Implementation:</strong></p>
<ul>
<li>The class <code>perm_star</code> generates the star-transposition
order and is implemented in [FXT: comb/perm-star.h].</li>
<li>The demonstration program to generate listings of star-transposition
permutations is [FXT: comb/perm-star-demo.cc].</li>
</ul></li>
</ol>
<p>Both algorithms generate a significant number of permutations per
second, with Trotter’s algorithm generating around 145 million and
Star-transposition order generating about 190 million permutations per
second. The Star-transposition order is particularly interesting for
studying the movement patterns of elements within permutations.</p>
<p>This text discusses three different minimal-change orders for
generating permutations, each using a unique base for mixed radix
numbers. These methods are based on falling factorial numbers, rising
factorial numbers, and permuted factorial numbers, respectively.</p>
<ol type="1">
<li><p><strong>Permutations with Falling Factorial Numbers (Section
10.9.1)</strong>:</p>
<p>The Gray code for mixed radix numbers with a falling factorial base
is used to generate permutations in Trotter’s minimal-change order. This
method involves swapping elements based on changes in the mixed radix
sequence, where a digit ‘p’ (position) changes by ‘d’ (= ±1) in the
sequence, implying that element ‘p’ of the permutation is swapped with
its right neighbor for d = +1 or left neighbor for d = -1.</p>
<p>The class <code>perm_gray_ffact2</code> implements this algorithm,
using a loopless mixed-radix Gray code routine and storing current
permutations in arrays <code>x_</code> and their inverse permutations in
<code>ix_</code>. The most recently swapped elements are tracked by
<code>sw1_</code> and <code>sw2_</code>.</p></li>
<li><p><strong>Permutations with Rising Factorial Numbers (Section
10.9.2)</strong>:</p>
<p>This method uses a Gray code for numbers in rising factorial base to
generate permutations. A recursive construction for this order is shown
in Figure 10.9-C, and the corresponding Gray code is displayed in Figure
10.9-B. The class <code>perm_gray_rfact</code> implements a constant
amortized time (CAT) algorithm for generating these permutations using
mixed radix Gray codes with rising factorial base.</p>
<p>To compute the next permutation, elements are swapped based on the
position ‘j’ of the digit changed and its direction ‘d’ (= ±1). The
element to be swapped is determined by searching for the greatest
smaller (for d &gt; 0) or smallest greater (for d &lt; 0) element
relative to the changing element.</p></li>
<li><p><strong>Permutations with Permuted Factorial Numbers (Section
10.9.3)</strong>:</p>
<p>This method generates permutations using a base of permuted factorial
numbers. The swaps are determined by positions and directions similar to
the previous methods, but here, elements are indexed using factorial
numbers that have been permuted in some way. The class responsible for
this algorithm is not explicitly mentioned, but it likely follows a
similar structure as <code>perm_gray_ffact2</code> or
<code>perm_gray_rfact</code>.</p></li>
</ol>
<p>Each of these methods generates permutations with minimal changes
between consecutive permutations, making them efficient for generating
sequences of permutations in various contexts. The speeds at which they
generate permutations are provided: around 155 million per second for a
simple implementation, 80-90 million per second for the loopless
versions based on falling and rising factorial bases. These algorithms
offer alternatives to standard permutation generation methods and can be
tailored depending on specific use cases or requirements.</p>
<p>The text discusses several permutation orders, each with its unique
characteristics. Here’s a detailed explanation of the four orders
mentioned:</p>
<ol type="1">
<li><strong>Mixed Radix Numbers (Gray Code):</strong>
<ul>
<li>This order is derived from Gray codes for factorial numbers.</li>
<li>The radix vector used here is [2, 3, 5, 4].</li>
<li>For even n, the last permutation in this order is a cyclic shift of
one position from the first.</li>
<li>An implementation of this order is provided in FXT (Fast eXact
Algorithms and Tools) library as <code>perm_gray_rot1</code>.</li>
</ul></li>
<li><strong>Derangement Order:</strong>
<ul>
<li>In this order, no two successive permutations share any element at
the same position.</li>
<li>There’s no derangement order for n = 3.</li>
<li>An implementation of the underlying algorithm is provided in FXT
library as <code>perm_derange</code>.</li>
<li>This order ensures that each permutation differs from its
predecessor by at least one element, maximizing transitions between
permutations.</li>
</ul></li>
<li><strong>Cyclic Shift Order (Falling Factorial Base):</strong>
<ul>
<li>This order generates permutations via cyclic shifts.</li>
<li>It creates a derangement order if n is even but not for odd n.</li>
<li>The algorithm for generating all permutations with k transitions
(where 2 ≤k ≤n and k ̸= 3) is given in [297].</li>
<li>An implementation of this order is provided in FXT library as
<code>perm_rot</code>.</li>
</ul></li>
<li><strong>Orders where the Smallest Element Always Moves
Right:</strong>
<ul>
<li>The text discusses a variant of Trotter’s construction, specifically
an ordering where the first element always moves right.</li>
<li>This order can be generated using an interleaving process (shown in
Figure 10.11-A).</li>
<li>The second half of permutations is the reversed list of the reversed
permutations in the first half.</li>
<li>An implementation of this order is provided in FXT library as
<code>perm_mv0</code>.</li>
</ul></li>
</ol>
<p>The derangement order ensures that each permutation differs from its
predecessor by at least one element, making it an order with maximum
transitions between successive permutations. The mixed radix numbers
(Gray Code) ensure minimal changes between successive permutations.
Cyclic shift orders generate permutations via cyclic shifts and can
create derangement orders for even n but not for odd n. Lastly, the
orders where the smallest element always moves right are generated
through an interleaving process, ensuring that the first element moves
to the right in each step.</p>
<p>The text discusses two permutation generation algorithms, Ives’
algorithm and single track orders, along with a specific type of Gray
code for permutations known as single track Gray codes.</p>
<p><strong>Ives’ Algorithm:</strong></p>
<ul>
<li>This algorithm generates permutations in an order where most updates
involve moving the smallest element to the right by one position.</li>
<li>The rate of generation is approximately 180 million permutations per
second when using pointers and around 190 M/s with arrays.</li>
<li>Optimizations include a special case for when only the first element
needs to be moved, which can be handled more efficiently, and modifying
certain conditions in the loop to improve performance.</li>
<li>The algorithm is implemented in the <code>perm_ives</code> class,
with additional optimizations like counters (<code>ctm_</code> and
<code>ctm0_</code>) to handle the easy case (moving only the first
element) and bitwise operations for checking equality.</li>
</ul>
<p><strong>Single Track Orders:</strong></p>
<ul>
<li>This method generates permutations in a single track order, where
each column is a cyclic shift of the previous one.</li>
<li>The order is constructed recursively using mixed radix counting with
rising factorial base.</li>
<li>The algorithm is implemented in the <code>perm_st</code> class,
generating around 123 million permutations per second.</li>
<li>A single track Gray code can be derived from a Gray code for n-1
elements by ensuring that the first and last permutation are cyclic
shifts of each other (applicable only to even lengths). This results in
a single track order with just n-1 extra transpositions for all
permutations of n elements.</li>
<li>The number of distinct single track orders is ((n - 1)!)!, and there
are ((n - 1)! - 1)! single track orders starting with the identity
permutation, where each run of (n - 1)! elements begins at position
k.</li>
</ul>
<p><strong>Gray Code for Permutations:</strong></p>
<ul>
<li>A Gray code is a binary code in which adjacent codewords differ by
only one bit. In the context of permutations, this means that
consecutive permutations differ by a single transposition (swap of two
elements).</li>
<li>Single track Gray codes have the additional property where each
column (or row) is a cyclic shift of the previous one. These codes are
constructed using a Gray code for n - 1 elements and ensuring that the
first and last permutation are cyclic shifts of each other.</li>
</ul>
<p>These algorithms and methods provide efficient ways to generate
permutations in specific orders, with varying levels of optimization and
unique properties suitable for different applications.</p>
<p>The text discusses various types of permutations, their properties,
and generating functions associated with them. Here’s a summary:</p>
<ol type="1">
<li><p><strong>Stirling Cycle Numbers (s(n, m))</strong>: These numbers
represent the number of ways to partition n elements into m cycles. They
satisfy the recurrence relation s(n, m) = s(n - 1, m - 1) + (n - 1) *
s(n - 1, m). The exponential generating function for these numbers is
exp(L(z)), where L(z) is defined as a summation of terms with base
(k-1)! * t_k * z^k/k.</p></li>
<li><p><strong>Permutations with Prescribed Cycle Type</strong>: If a
permutation has c_i cycles of length i, the number of such permutations
is given by Z_n,C = n! / (c_1! c_2! … c_n! 1^(c_1) 2^(c_2) …
n^(c_n)).</p></li>
<li><p><strong>Prefix Conditions</strong>: Certain types of permutations
can be generated efficiently using routines that produce
lexicographically ordered lists subject to conditions for all prefixes.
The condition must be supplied as a function pointer during the creation
of a class instance. Examples include involutions, up-down permutations,
connected permutations, and derangements.</p>
<ul>
<li><p><strong>Involutions</strong>: These are self-inverse permutations
(a permutation that is its own inverse). The sequence I(n) starts as 1,
2, 4, 10, … and satisfies the recurrence relation I(n) = I(n - 1) + (n -
1) * I(n - 2).</p></li>
<li><p><strong>Derangements</strong>: These are permutations where no
element appears in its original position. The sequence D(n) starts as 0,
1, 2, 9, … and satisfies the recurrence relation D(n) = (n - 1) * [D(n -
1) + D(n - 2)].</p></li>
<li><p><strong>Connected Permutations</strong>: These permutations
cannot be split into two non-empty sub-permutations that are themselves
permutations. The sequence C(n) starts as 1, 1, 3, 13, … and satisfies
the recurrence relation C(n) = n! - Σ (k=1 to n-1) k! * C(n -
k).</p></li>
<li><p><strong>Alternating Permutations</strong>: These permutations
alternate between increasing and decreasing subsequences. The sequence
A(n) starts as 1, 1, 2, 5, … and satisfies the recurrence relation A(n)
= 1/2 * Σ (k=0 to n-1) binomial(n - 1, k) * A(k) * A(n - 1 -
k).</p></li>
</ul></li>
</ol>
<p>The text also mentions generating functions for these sequences,
which are useful in combinatorics and number theory. These functions
allow us to encode information about sequences in a compact way and
provide tools for manipulating and analyzing the sequences.</p>
<p>The text discusses different types of permutations with specific
restrictions on the movement of elements. Here’s a detailed summary:</p>
<ol type="1">
<li><strong>Permutations where no element can move more than one place
to the right (M(n))</strong>:
<ul>
<li>M(n) = 2^n - 1, as listed in A000079 in [312].</li>
<li>These permutations are also known as self-inverse permutations or
involutions.</li>
<li>Figure 11.2-A shows a Gray code for these permutations when n=5,
where no element lies more than one place to the right of its position
in the identical permutation.</li>
</ul></li>
<li><strong>Permutations where no element can move more than one place
to the left (F(n))</strong>:
<ul>
<li>F(n) is the (n + 1)-st Fibonacci number.</li>
<li>These permutations are self-inverse and also exhibit the property
that no element moves more than one place to the right.</li>
<li>Figure 11.2-B displays a Gray code for these permutations when
n=7.</li>
</ul></li>
<li><strong>Permutations where an element can move k − 1 ≤ p(k) ≤ k + d
places (both left and right)</strong>:
<ul>
<li>This is a generalization of the previous two cases.</li>
<li>A Gray code for such permutations can be generated using binary
words with at most d consecutive ones, as demonstrated in Figure 11.2-C
for n=6 and d=2.</li>
</ul></li>
</ol>
<p>The algorithms for generating these permutations involve recursive
functions:</p>
<p><strong>Recursive Algorithm for Cyclic Permutations</strong>: - This
algorithm generates all permutations of n elements by placing each
element in the first position and recursively generating permutations of
the remaining elements. - The order of generation corresponds to the
alternative factorial representation with a falling base, as shown in
Figure 11.4-A.</p>
<p><strong>Minimal-Change Order for Cyclic Permutations</strong>: - All
cyclic permutations can be generated using a mixed radix Gray code with
a falling factorial base (as depicted in Figure 11.4-C). - Two
successive permutations differ at three positions, indicating a constant
amortized time (CAT) implementation for generating these
permutations.</p>
<p>The provided C++ classes and functions implement these
algorithms:</p>
<ul>
<li><code>perm_rec</code> class generates all permutations of n elements
using the recursive algorithm described above.</li>
<li><code>perm_involution</code> class specializes in generating
self-inverse permutations or involutions.</li>
<li><code>cyclic_perm</code> class generates cyclic permutations, with
an option to generate only these specific permutations by modifying the
recursive function’s loop condition.</li>
</ul>
<p>The text discusses two methods for generating k-permutations of n
elements, specifically focusing on the lexicographic order and
minimal-change (Gray code) order.</p>
<p><strong>Lexicographic Order:</strong></p>
<ol type="1">
<li>The class <code>kperm_lex</code> generates k-permutations in
lexicographic order using mixed radix numbers, where changes are
restricted to the first k elements.</li>
<li>The constructor initializes the permutation array (<code>p_</code>),
its inverse (<code>ip_</code>), and a falling factorial number array
(<code>d_</code>).</li>
<li>The <code>first()</code> method sets up the initial state by
populating <code>p_</code> with numbers from 0 to n-1, <code>ip_</code>
with the same values, and <code>d_</code> with zeros.</li>
<li>In the <code>next()</code> method:
<ul>
<li>It identifies the leftmost changed position (<code>i</code>) within
the first k elements.</li>
<li>It increments the mixed radix number at position <code>i</code>
until a change is found.</li>
<li>It swaps the element at position <code>i</code> with the smallest
element to its right that is greater than <code>p[i]</code>.</li>
<li>It then updates the positions of the swapped elements in both
<code>p_</code> and <code>ip_</code>.</li>
</ul></li>
<li>This method generates k-permutations efficiently for small k, with a
rate of around 80 million permutations per second for k=4 and n=100
(best case), and about 30 million permutations per second for k=n=12
(worst case).</li>
</ol>
<p><strong>Minimal-Change Order (Gray Code):</strong></p>
<ol type="1">
<li>The class <code>kperm_gray</code> generates k-permutations in
minimal-change order, specifically using the first inverse permutations
in Trotter’s order.</li>
<li>The update routine differs from the lexicographic order generator by
checking whether the left element of the swapped pair lies within the
k-prefix.</li>
<li>This method ensures that each new permutation differs from the
previous one by only one element swap, creating a Gray code for
k-permutations.</li>
<li>The generation rate for this method is not explicitly provided but
is expected to be comparable to the lexicographic order generator,
depending on the values of k and n.</li>
</ol>
<p>Both methods allow for the efficient generation of k-permutations,
with the choice between them depending on whether lexicographic order or
minimal-change order (Gray code) is preferred for a given
application.</p>
<p>The text discusses the permutations of multisets, which are
collections of elements where repetition is allowed but order does not
matter. A multiset can be represented as (r0, r1, …, rk-1), indicating
that there are r0 elements of type 0, r1 elements of type 1, and so on,
up to rk-1 elements of type k-1. The total number of elements in the
multiset is n = Σ(rj from j=0 to k-1).</p>
<p>The text covers three methods for generating permutations of a
multiset: recursive generation, iterative generation using lexicographic
order, and iterative generation using an ordering based on prefix shifts
(also known as cool-lex order).</p>
<ol type="1">
<li>Recursive Generation:
<ul>
<li>The given routine generates all permutations in lexicographic order
when called with argument zero.</li>
<li>It uses a recursive approach to generate each permutation by
selecting elements from different “buckets” or types, represented by the
rj values.</li>
<li>The efficiency of this method can be improved by maintaining a list
of pointers to the next nonzero bucket (nk[]), reducing work for regular
permutations to less than e (Euler’s number) times the number of
generated permutations.</li>
</ul></li>
<li>Iterative Generation (Lexicographic Order):
<ul>
<li>This method generates the next permutation in lexicographic order
given an initial state.</li>
<li>The algorithm involves finding the rightmost pair with a smaller
element before a larger one, swapping elements to maintain a falling
sequence to the right of the swap position, and reversing the order of
elements to the right.</li>
</ul></li>
<li>Iterative Generation (Prefix Shifts/Cool-Lex Order):
<ul>
<li>This ordering is described in [360], and it involves a cyclic shift
of a prefix for each transition.</li>
<li>Each permutation is related to the previous one by shifting a
contiguous subsequence of elements to the right or left, maintaining the
relative order within the shifted subsequence.</li>
</ul></li>
</ol>
<p>The text also provides examples and code snippets for implementing
these methods in C++, including classes like
<code>mset_perm_lex_rec</code>, <code>mset_perm_lex</code>, and
<code>mset_perm_pref</code>. The performance of these implementations is
also discussed, with generation rates ranging from tens to hundreds of
millions of permutations per second, depending on the size and
composition of the multiset.</p>
<p>The text discusses two methods for generating Gray codes, which are
binary sequences that change by only one bit at a time, for permutations
of multisets and strings with certain restrictions.</p>
<ol type="1">
<li><p><strong>Permutation of Multisets (Section 13.2):</strong></p>
<p>The paper presents an algorithm for generating permutations of
multisets in “cool-lex” order. A multiset is a collection that allows
multiple instances of the same element. Here’s a summary:</p>
<ul>
<li>The algorithm uses an array to represent the multiset and determines
the length of the longest non-increasing prefix in a simple manner.</li>
<li>It initializes with a ‘first()’ function, which assigns indices
based on the frequency of elements in the multiset.</li>
<li>The ‘next()’ function scans for the longest non-increasing prefix
and rotates it to generate the next permutation.</li>
<li>If the number of elements in the prefix equals the total minus one
(indicating the last permutation), it rotates the entire array and
checks if this is indeed the last permutation.</li>
<li>Otherwise, it compares the last element of the prefix with the
element two positions rightward, rotates accordingly, and updates the
length of the next longest non-increasing prefix.</li>
</ul>
<p>The performance of the algorithm varies depending on the multiset: 68
M/s for permutations of 12 elements, 46 M/s for combinations (6 choose
2), and 62 M/s for permutations of (2, 2, 2, 3, 3, 3).</p></li>
<li><p><strong>Gray Code for Multiset Permutations (Section
13.2.4):</strong></p>
<p>This section introduces an alternative method using a linked list for
multiset permutations, as suggested by the original paper. The key
points are:</p>
<ul>
<li>The algorithm calculates the length of the next longest
non-increasing prefix with just one comparison.</li>
<li>It stores this length in a variable ‘ln_’ and uses fast update when
enabled via a preprocessor directive (#define MSET_PERM_PREF_LEN).</li>
<li>The initialization is modified to account for the new computation
method.</li>
</ul>
<p>The performance improves slightly: 71 M/s for permutations of 12
elements, 62 M/s for combinations (30 choose 15), and 69 M/s for
permutations of (2, 2, 2, 3, 3, 3).</p></li>
<li><p><strong>Minimal-Change Order (Section 13.2.4):</strong></p>
<p>This section presents a Gray code algorithm for multiset permutations
proposed by Fred Lunnon. It’s a generalization of Trotter’s order for
permutations. The key aspects are:</p>
<ul>
<li>The algorithm generates a sequence where each successive permutation
differs from the previous one by swapping two adjacent elements only if
necessary.</li>
<li>The implementation uses classes ‘mset_perm_gray’ in the file
‘comb/mset-perm-gray.h’.</li>
<li>It has functions for initialization, data retrieval, and swap
position extraction.</li>
</ul>
<p>The performance is about 40 M/s when generating the Gray code for
multiset permutations (FXT: comb/mset-perm-gray-demo.cc).</p></li>
<li><p><strong>List Recursions (Section 14.1):</strong></p>
<p>This section discusses a method for generating Gray codes using list
recursions, where a relation like W(n) = [0 0 . W(n −2)] + [1 0 . W R(n
−2)] + [1 2 0 . W(n −3)] implies another version obtained by reversing
the order of sublists and additionally reversing each sublist.</p>
<ul>
<li>The recursion relation leads to a linear recurrence for the number
of strings (w(n)) with certain properties, which in this case are
Fibonacci numbers.</li>
<li>An implementation of such an algorithm is provided in
‘comb/fib-alt-gray-demo.cc’.</li>
</ul></li>
<li><p><strong>Fibonacci Words (Section 14.2):</strong></p>
<p>This section introduces Fibonacci words, binary sequences where each
word is either “0”, “1”, or “01”. The key points are:</p>
<ul>
<li>Fibonacci words follow a specific pattern based on the Fibonacci
sequence, where w(n) represents the number of n-digit Fibonacci
words.</li>
<li>The recursion relation for w(n) is w(n) = 2w(n −2) + w(n −3),
starting with w(0) = 1 (an empty string).</li>
<li>This results in the Fibonacci sequence for w(n): 1, 1, 2, 3, 5, 8,
13, 21, …</li>
</ul>
<p>The text also provides a recursive function ‘X_rec’ to generate
Fibonacci words based on these rules.</p></li>
</ol>
<p>This text discusses several topics related to string generation,
specifically focusing on Gray codes for strings with certain
restrictions.</p>
<ol type="1">
<li><p><strong>Fibonacci Words</strong>: These are binary words (strings
of 0s and 1s) that do not contain two consecutive ones. A recursive
routine is provided to generate these words, with a modification to
create a Gray code through the Fibonacci words. The algorithm operates
in constant amortized time and can generate about 70 million objects per
second.</p></li>
<li><p><strong>Generalized Fibonacci Words</strong>: This generalization
allows for a fixed maximum number (r) of consecutive ones in binary
words. The list recursion for generating these words is provided, along
with examples for r=1 through r=5.</p></li>
<li><p><strong>Gray Codes for Generalized Fibonacci Words</strong>: A
Gray code is a sequence of integers where each term differs from the
previous one by only one bit (in binary representation). For generalized
Fibonacci words (with up to r consecutive ones), a list recursion is
given to generate such a Gray code. The text also mentions an
alternative Gray code for words without substrings 111 (r=2) and another
for words without substrings 1111 (r=3).</p></li>
<li><p><strong>Run-Length Limited (RLL) Words</strong>: These are binary
strings where the number of consecutive zeros or ones is at most r, with
r ≥2. The RLL(2) words in lexicographic order correspond to Fibonacci
words in minimal change order. A recursive routine is provided for
generating these words.</p></li>
<li><p><strong>Digit x followed by at least x zeros</strong>: This
section presents a table showing the lexicographic order of RLL(2) words
and their corresponding changes in Fibonacci words. The changes indicate
whether a bit stays the same (no change), or if it flips (indicated by
1).</p></li>
</ol>
<p>In summary, this text explores various methods for generating
specific types of binary strings, including Fibonacci words, generalized
Fibonacci words, and RLL words. It also details techniques for creating
Gray codes for these sequences, providing recursive algorithms and
visual examples to illustrate the concepts.</p>
<p>The text discusses several topics related to Gray codes for specific
types of binary strings, focusing on their definitions, properties, and
algorithms for generating these codes. Here’s a detailed summary and
explanation of each section:</p>
<p>14.6 Generalized Pell words</p>
<p>14.6.1 Gray code for Pell words - Definition: Pell words are ternary
(base 3) strings without the substrings “21” or “22.” A Gray code is a
minimal-change order of such strings. - Figure 14.6-A shows the start
and end of the lists in counting order and Gray code order for 5-digit
Pell words. - The recursive algorithm for generating this Gray code is
provided, along with an implementation in C++. - The computation of a
power series related to the Pell Gray code is mentioned.</p>
<p>14.6.2 Gray code for generalized Pell words - Definition: Generalized
Pell words are radix-(r+1) strings where the substring rx (with x ≠ 0)
is forbidden. - The recursion formula for generating a Gray code of such
words is given, depending on whether r is even or odd. - Figure 14.6-B
displays a Gray code for 4-digit radix-3 strings with no substring 3x
(where x ≠ 0). - An implementation of the algorithm is provided, along
with generating functions for the number of words in these
sequences.</p>
<p>14.7 Sparse signed binary words</p>
<p>14.7 Sparse signed binary words - Definition: These are binary
strings where zeros can be replaced by either +1 or -1, but there cannot
be two consecutive nonzero digits. - Figure 14.7-A shows a Gray code for
6-digit sparse signed binary words, with ‘P’ and ‘M’ denoting +1 and -1,
respectively. - The recursive algorithm for generating this Gray code is
provided, along with an implementation in C++. - The number of n-digit
sparse signed binary numbers (S(n)) and positive n-digit sparse signed
binary numbers (P(n)) are given, along with recurrence relations and
initial conditions.</p>
<p>14.8 Strings with no two consecutive nonzero digits</p>
<p>14.8 Strings with no two consecutive nonzero digits - Definition:
These strings consist of zeros and ones, where no two consecutive digits
are nonzero (i.e., there cannot be two consecutive 1s). - Figures 14.8
provide examples of such strings for different lengths (n = 1 to n =
30). - Two recursive algorithms for generating orders close to Gray
codes are presented: a) pos_rec() generates an almost Gray code, with N
non-Gray transitions and X excess digit changes from a true Gray code.
b) pos_AAA() and pos_BBB() generate a more refined ordering with
approximately n/2 non-Gray transitions for larger n.</p>
<p>In summary, these sections discuss various types of binary string
Gray codes and near-Gray codes, focusing on Pell words, generalized Pell
words, sparse signed binary words, and strings with no two consecutive
nonzero digits. They provide definitions, examples, recursive
algorithms, and implementations in C++, along with generating functions
for the number of words in these sequences. The text also explores
properties like minimal-change orders and the relationship between
different types of binary string orders.</p>
<p>The text describes various types of well-formed parentheses strings,
also known as paren strings, which are lists of n pairs of parentheses
that adhere to specific rules. These rules ensure the strings are valid
and non-nested. Here’s a detailed explanation of each type
discussed:</p>
<ol type="1">
<li>Co-lexicographic order (Figure 15.1):
<ul>
<li>This is the standard lexicographic (dictionary) order for
parentheses strings, where the comparison starts from the leftmost
parenthesis.</li>
<li>Strings are listed in ascending order based on their
co-lexicographic value.</li>
<li>For example, the first few lines of Figure 15.1 show the following
strings: 1: (((((())))) 22: ()()(()())</li>
</ul></li>
<li>Gray codes for specific restrictions (Figures 14.8-A, 14.9-A,
14.10-A):
<ul>
<li>These figures illustrate Gray codes for parentheses strings with
certain restrictions on the digits or substrings.
<ul>
<li>Figure 14.8-A: Length-4 radix-4 strings with no two consecutive
nonzero digits.</li>
<li>Figure 14.9-A: Binary strings with no two consecutive zeros (with r
= 1, 2, 3).</li>
<li>Figure 14.10-A: Binary strings without substring 1x1 (where x is
either 0 or 1).</li>
</ul></li>
<li>Gray codes are a sequence of codes in which two successive code
words differ by only one bit (or digit).</li>
</ul></li>
<li>Recursion for generating lists (Equations 14.8-1, 14.9-1, 14.10-1):
<ul>
<li>These equations define recursive structures for generating the
respective lists of parentheses strings based on specific rules:
<ul>
<li>Equation 14.8-1: For length-n strings with radix (r+1) and no two
consecutive nonzero digits.</li>
<li>Equation 14.9-1: For length-n strings with radix (r+1) and no two
consecutive zeros, with different versions for even and odd r.</li>
<li>Equation 14.10-1: For length-n binary strings without substring
1x1.</li>
</ul></li>
</ul></li>
<li>Recurrence relations for counting elements (Equations 14.8-2,
14.9-2, 14.10-2):
<ul>
<li>These equations provide recurrence relations to calculate the number
of valid parentheses strings based on specific restrictions:
<ul>
<li>Equation 14.8-2: For length-n strings with no two consecutive
nonzero digits (denoted as dr(n)).</li>
<li>Equation 14.9-2: For length-n binary strings without two consecutive
zeros (denoted as zr(n)).</li>
<li>Equation 14.10-2: For length-n binary strings without substring 1x1
(denoted as v(n)).</li>
</ul></li>
</ul></li>
<li>Generating functions for counting elements (Equations 14.8-3,
14.9-3, 14.10-3):
<ul>
<li>These equations provide generating functions that can be used to
calculate the number of valid parentheses strings based on specific
restrictions:
<ul>
<li>Equation 14.8-3: For length-n strings with no two consecutive
nonzero digits (denoted as dr(n)).</li>
<li>Equation 14.9-3: For length-n binary strings without two consecutive
zeros (denoted as zr(n)).</li>
<li>Equation 14.10-3: For length-n binary strings without substring 1x1
(denoted as v(n)).</li>
</ul></li>
</ul></li>
<li>Generalization to k-ary Dyck words (at the end of Section 15):
<ul>
<li>The text briefly mentions a generalization of parentheses strings
called k-ary Dyck words, where each parenthesis can be replaced by any
of k symbols, resulting in a broader family of combinatorial
objects.</li>
</ul></li>
</ol>
<p>In summary, this text describes various well-formed parentheses
string lists (co-lexicographic order and Gray codes) and their
respective recurrence relations and generating functions based on
specific restrictions such as no two consecutive nonzero digits, no two
consecutive zeros, or absence of certain substrings like 1x1 or 1xy1.
The text also mentions a generalization to k-ary Dyck words for broader
combinatorial applications.</p>
<p>The provided text describes a method for generating valid strings of
parentheses, which are also known as Dyck words. These strings follow
certain rules: they start and end with the same type of parenthesis,
every opening parenthesis “(” must be followed by a closing parenthesis
“)”, and at no point can the number of closing parentheses exceed the
number of opening ones.</p>
<p>The method uses a restricted growth string (RGS) representation for
these valid strings. A RGS is a sequence where each term ak satisfies 0
≤ ak ≤ ak-1 + 1, meaning that each term increases by at most one from
the previous term.</p>
<p>For generating the next valid parentheses string, two operations are
performed:</p>
<ol type="1">
<li><p><strong>Increment Operation</strong>: The highest digit
(rightmost in the sequence) where this digit is less than or equal to
its predecessor (ak ≤ ak-1) is incremented. All subsequent digits (ai
for i &gt; k) are then reset to 0. This operation ensures that only one
value changes in each step, maintaining a Gray code property which means
that consecutive strings differ by a single parenthesis swap.</p></li>
<li><p><strong>Decrement Operation</strong>: The highest non-zero digit
aj is decremented, and all subsequent digits ai for i &gt; j are set to
ai-1 + 1. This operation moves the “highest” increase point leftwards,
again ensuring only one change per step.</p></li>
</ol>
<p>The class <code>catalan</code> in the provided C++ code implements
this method. It takes as input the number of pairs (n), and by default
generates RGSs in near-perfect minimal-change order, where exactly two
symbols change with each step.</p>
<p>The class has three main components: - <code>as_</code>: an array
storing the digits of the RGS, ensuring that each element is less than
or equal to its predecessor plus one. - <code>d_</code>: an array
indicating the direction (+1 or -1) for the next recursive step. -
<code>n_</code> : The number of digits (paren pairs).</p>
<p>The method also includes a string representation of the generated
parentheses, which is constructed on demand using a char array
(<code>str_</code>).</p>
<p>Finally, Figure 15.2-A to C illustrate this concept with examples for
n=4 and n=5. These figures show the RGSs, their corresponding
parenthesis strings, delta sets (difference between consecutive
strings), and difference strings in lexicographic order. The
minimal-change order ensures that the distance between consecutive
strings is minimized, making the sequence more uniform and
predictable.</p>
<p>The text discusses various algorithms for generating minimal-change
orders of parenthesis strings, also known as Dyck words. Here’s a
detailed summary and explanation:</p>
<ol type="1">
<li>Recursion (next_rec):
<ul>
<li>The function <code>next_rec(k-1)</code> generates the next Catalan
number string by recursion. It uses a helper variable <code>d</code> to
store the current digit (0 or 1) and <code>as[k]</code> to keep track of
consecutive zeros.</li>
<li>If <code>ns1</code> (next significant bit) is zero, it means no
valid continuation exists, so the function returns false. Otherwise, it
updates <code>d_[k]</code> with the new digit and increments
<code>as[k]</code>.</li>
<li>The base case for this recursion isn’t shown in the provided code
snippet but would be where <code>k</code> reaches a predefined limit or
condition.</li>
</ul></li>
<li>Gray Code Order:
<ul>
<li>This order generates parenthesis strings while ensuring that each
string differs from its successor by only one bit change (ignoring the
first bit, always 1).</li>
<li>The algorithm uses recursion with two cases: forward
(<code>0==z</code>) and backward (<code>!z</code>). For forward, it
iterates through possible values for <code>rv[d]</code>, while for
backward, it iterates in reverse.</li>
</ul></li>
<li>Prefix Shift Order (Cool-Lex):
<ul>
<li>This order generates parenthesis strings where each string differs
from its successor by a cyclic shift of a prefix, ignoring the first bit
which is always 1.</li>
<li>The algorithm generates binary words corresponding to parenthesis
strings and then maps them back to the parenthesis representation.</li>
</ul></li>
<li>Catalan Numbers:
<ul>
<li>The number of valid combinations of n parentheses pairs (Dyck words)
is given by the nth Catalan number, Cn = (2n choose n)/(n+1).</li>
<li>These numbers are also represented as sequence A000108 in the Online
Encyclopedia of Integer Sequences (OEIS).</li>
<li>The generating function for Catalan numbers is C(x) = 1 + x + 2x^2 +
5x^3 + …</li>
<li>The Catalan numbers satisfy a convolution property: Cn = Σ[k=0 to
n-1] C_k * C_(n-1-k).</li>
</ul></li>
<li>Restricted Growth Strings (RGS):
<ul>
<li>RGS is another order for generating parenthesis strings where each
string differs from its predecessor by changing exactly two positions
and maintaining the order of non-changing positions.</li>
<li>The table provided shows examples of RGS for n = 1 to 5, listing the
binary representation of Dyck words along with their positions in the
RGS sequence.</li>
</ul></li>
</ol>
<p>The different orders (recursion, Gray code, prefix shift, and RGS)
serve various purposes such as efficient string generation, minimizing
changes between adjacent strings, or generating strings with specific
properties like strong minimal-change order where changes occur only in
adjacent positions for even values of n.</p>
<p>The text discusses three related topics: Increment-i Restricted
Growth Strings (RGS), k-ary Dyck words, and k-ary trees.</p>
<ol type="1">
<li><p><strong>Increment-i RGS</strong>: These are sequences where the
first element is 0, and each subsequent element does not exceed the
previous one by more than i. For example, a 4-length Increment-2 RGS
could be [0, 2, 3, 6]. The case where i=1 corresponds to regular
Restricted Growth Strings (RGS).</p></li>
<li><p><strong>k-ary Dyck Words</strong>: These are binary words where
any prefix contains at least k-1 ones for every zero. A correspondence
exists between Increment-i RGS and k-ary Dyck words, where k equals
i+1.</p></li>
<li><p><strong>k-ary Trees</strong>: The length-n increment-i RGS also
describe k-ary trees with n internal nodes. Starting from the root,
moving outwards by i positions for each ‘1’ (increment) and then
following back by one position for each ‘0’, constructs these
trees.</p></li>
</ol>
<p><strong>Generation in Lexicographic Order</strong>: The text provides
an algorithm to generate increment-i RGS in lexicographic order. This is
achieved through a class named <code>dyck_rgs</code> with methods like
<code>next()</code> that returns the index of the first changed element
in the restricted growth string (RGS).</p>
<p><strong>Gray Codes with Homogeneous Moves</strong>: The text mentions
a loopless algorithm for generating a Gray code where all moves are
homogeneous. This is visualized in Figure 15.5-B, showing the positions
of ones in delta sets for 3-ary Dyck words. The corresponding
implementation is given in the <code>dyck_gray</code> class.</p>
<p><strong>Gray Codes with Homogeneous and Two-Close Moves</strong>: A
more specific Gray code is presented where all transitions are both
homogeneous (all ’1’s or ’0’s change by the same amount) and two-close
(the difference between the old and new values is either +2 or -2). This
is shown in Figure 15.5-C, with its implementation provided in the
<code>dyck_gray2</code> class.</p>
<p>In summary, this text presents methods for generating specific types
of strings (Increment-i RGS) that have applications in constructing Dyck
words and k-ary trees. It also details algorithms to generate Gray
codes—sequences used to enumerate combinatorial objects without
repetition while maintaining a close relationship between successive
elements—for these structures with constraints on the type of moves
allowed.</p>
<p>The text discusses several aspects related to integer partitions,
which are the different ways a number can be expressed as a sum of
smaller positive integers. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>General Integer Partitions</strong>: The problem of
finding all integer partitions of a given number x (Fig 16.0-A) is
solved by an iterative algorithm presented in [FXT: class partition in
comb/partition.h]. This algorithm generates the partitions in
lexicographic order.</p></li>
<li><p><strong>Partitions into m parts</strong>: A different algorithm
is provided for generating all integer partitions of n into exactly m
parts (Fig 16.3-A). This method, described in [FXT: class mpartition in
comb/mpartition.h], starts with an initial partition containing m−1
units and the element n−m+1, then modifies this partition by increasing
elements to satisfy the condition that no part is less than the final
part by at least two units.</p></li>
<li><p><strong>Number of Integer Partitions</strong>: The text presents
generating functions for various types of partitions:</p>
<ul>
<li><p>Unrestricted partitions: This is given by η(x), where η(x) =
∏(1-x^n) for all n &gt; 0 (Eq 16.4-6a).</p></li>
<li><p>Partitions into an even or odd number of parts: These are derived
using Jacobi’s identity (Eq 16.4-4), which relates two products
involving q-series. The special cases a = -1, b = 0 and a = 0, b = 1
correspond to partitions into even/odd numbers of distinct parts (Eqs
16.4-2a and 16.4-2b).</p></li>
<li><p>Partitions into exactly m parts: This is derived from Jacobi’s
identity with a = 0 and b = 1, leading to the generating function Q_m(x)
= ∏((1 - x^k)/(1 - x^(k+1))) for k from 1 to m-1 (Eq 16.4-3b).</p></li>
<li><p>Partitions into distinct parts: This is derived using Cauchy’s
identity (Eq 16.4-5) with a = -1 and b = 0, resulting in the generating
function η(x)/η+(x), where η+(x) = ∏(1/(1 - x^n)) for all n &gt; 0 (Eq
16.4-6b).</p></li>
<li><p>Partitions into square-free parts: This is a more complex topic,
not directly addressed in the provided text.</p></li>
</ul></li>
</ol>
<p>These generating functions allow one to study properties of integer
partitions using techniques from combinatorics and analysis. They can be
used, for example, to derive closed forms or asymptotic estimates for
the number of partitions of a given size.</p>
<p>The text discusses the topic of integer partitions, focusing on two
types: unrestricted partitions and partitions into distinct parts.</p>
<p><strong>Unrestricted Partitions (Section 16.4.1):</strong></p>
<ul>
<li><p>The number of integer partitions of n is denoted by Pn, which
forms sequence A000041 in [312]. Figure 16.4-A provides values for Pn
when n ≤50.</p></li>
<li><p>P(n, m) represents the number of partitions of n into exactly m
parts. It satisfies the recurrence relation:</p>
<p>P(n, m) = P(n - 1, m - 1) + P(n - m, m), for n &gt; 0 and m &gt;
0.</p>
<p>With the initial condition P(0, 0) = 1.</p></li>
<li><p>The generating function for partitions into exactly m parts
is:</p>
<p>∞∑<em>{n=1} P(n,m) x^n = (x^m Q</em>{k=1}^{∞} (1 -
x<sup>k))</sup>{-1}</p>
<p>The rows in Figure 16.4-B correspond to fixed powers of x.</p></li>
<li><p>The generating function for the number Pn of integer partitions
of n is found by setting u=1 in:</p>
<p>∞∑_{n=0} P_n x^n = (1 - x)^(-1) = η(x), where η(x) is known as
Euler’s function.</p></li>
<li><p>The generating function for partitions into parts r + j (for j =
0, 1, …) with maximal part r can be expressed using the Rogers-Ramanujan
identities.</p></li>
</ul>
<p><strong>Partitions into Distinct Parts (Section 16.4.2):</strong></p>
<ul>
<li><p>The generating function for Dn, the number of partitions of n
into distinct parts, is:</p>
<p>∞∑<em>{n=0} D_n x^n = ∏</em>{n=1}^∞ (1 + x^n) = η+(x), where η+(x) is
Euler’s function for distinct parts.</p></li>
<li><p>The number of partitions into distinct parts equals the number of
partitions into odd parts:</p>
<p>η+(x) = η(x^2)/(η(x)) = ∏_{k=1}^∞ (1 -
x<sup>{2k-1})</sup>{-1}.</p></li>
<li><p>D(n, m), the number of partitions of n into exactly m distinct
parts, has a generating function:</p>
<p>∞∑<em>{n=0} D(n,m) x^n = x^(m(m+1)/2) ∏</em>{k=1}^m (1 -
x^k).</p></li>
<li><p>The connection between relations 16.4-24 and 16.4-13 can be seen
by decomposing the Ferrers diagram of a partition into m distinct parts
into a triangle of size m(m+1)/2 and a partition into at most m
elements.</p></li>
</ul>
<p>In summary, integer partitions are ways to express an integer as a
sum of positive integers, possibly with restrictions on the parts (e.g.,
distinct or bounded). The text explores generating functions for these
partitions, recurrence relations, and various identities connecting
different types of partitions.</p>
<p>The text discusses the concept of set partitions, focusing on two
methods for generating all possible set partitions of a given set size
(n).</p>
<ol type="1">
<li>Recursive Generation:
<ul>
<li>The list Z_n contains all set partitions of an n-element set S_n =
{1,2,…,n}.</li>
<li>To generate Z_n, we use a complete list of partitions Z_{n-1} for
the (n-1)-element set. For each partition P in Z_{n-1}, new partitions
are created by appending n to the first, second, …, last subset, or as a
single-element set {n}. This process is repeated n-1 times, starting
with the only partition {{1}} of the 1-element set.</li>
<li>Figure 17.1-A illustrates this recursive construction for n = 4,
while the right column shows all set partitions of 4 elements.</li>
</ul></li>
<li>Minimal-Change Order:
<ul>
<li>A modified version of the recursive construction generates set
partitions in a minimal-change order. This can be achieved by
incrementing (adding an element) from left to right or right to
left.</li>
<li>The interleaving process is depicted in Figure 17.1-B, which
demonstrates how this method works similarly to Trotter’s construction
for permutations. By changing the direction of incrementation in each
recursive step, we obtain a minimal-change order (Figure 17.1-C).</li>
<li>The C++ class <code>setpart</code> stores set partitions in an array
of signed characters, where negated values indicate the last element in
a subset. The generation work is proportional to P_n^k * B_k, where B_k
represents the k-th Bell number, and P_n^k denotes the sum of the first
n natural numbers raised to the power k.</li>
</ul></li>
</ol>
<p>This class <code>setpart</code> includes parameters like
<code>xdr</code>, which determines the order in which partitions are
created (minimal-change by default), and <code>dr0</code>, which sets
the initial direction for each recursive step—either starting with a
single partition containing all elements or beginning with individual
singleton sets for each element.</p>
<p>The provided text discusses various aspects of set partitions,
including their representation as restricted growth strings (RGS),
generating functions for the number of set partitions (Stirling numbers
and Bell numbers), and algorithms for generating RGSs in different
orders. Here’s a detailed summary:</p>
<ol type="1">
<li><strong>Set Partitions and Restricted Growth Strings (RGS):</strong>
<ul>
<li>A set partition is a division of a finite set into non-empty
subsets, called blocks or parts. The number of ways to partition an
n-set into k subsets is given by the Stirling numbers of the second kind
(S(n, k)).</li>
<li>RGSs are a compact way of representing set partitions. In an RGS s =
[s0, s1, …, sn−1], each si represents the size of the i-th block plus
one. The condition aj ≤ 1 + maxi&lt;j(ai) ensures that the RGS
corresponds to a valid set partition.</li>
</ul></li>
<li><strong>Generating Functions:</strong>
<ul>
<li><p>The ordinary generating function (OGF) for Bell numbers (the sum
of Stirling numbers in each row) is given by:</p>
<pre><code>∞
X
n=0
Bn xn = ∞
X
k=0
xk Qk j=1 (1 −j x)</code></pre></li>
<li><p>The exponential generating function (EGF) for Bell numbers is
exp[exp(x) - 1].</p></li>
</ul></li>
<li><strong>Stirling Numbers and Bell Numbers:</strong>
<ul>
<li><p>Stirling numbers of the second kind, S(n, k), can be computed
using the recurrence relation:</p>
<pre><code>S(n, k) = k * S(n −1, k) + S(n −1, k −1)</code></pre></li>
<li><p>Bell numbers (sum of Stirling numbers in each row) can be
computed using the recurrence relation:</p>
<pre><code>Bn+1 = Σ(k=0 to n) binomial(n-2, k-1) * Bk</code></pre></li>
</ul></li>
<li><strong>RGS Generation Algorithms:</strong>
<ul>
<li><p>The text describes three classes for generating RGSs in different
orders:</p>
<ol type="a">
<li><strong>Lexicographic Order (setpart_rgs_lex):</strong>
<ul>
<li>Generates RGSs for set partitions in lexicographic order.</li>
<li>Uses an array m[] to keep track of the maximum value allowed for
each position in the RGS.</li>
<li>The successor method finds the first digit that can be incremented,
increments it, and adjusts the maxima accordingly.</li>
</ul></li>
<li><strong>Set Partitions into p Parts (setpart_p_rgs_lex):</strong>
<ul>
<li>Generates RGSs for set partitions of an n-set into exactly p
parts.</li>
<li>Initializes the RGS with a specific pattern that ensures p
subsets.</li>
<li>The successor method checks if the digit is less than p and repairs
the rightmost digits when needed.</li>
</ul></li>
<li><strong>Minimal-Change Order (setpart_rgs_gray):</strong>
<ul>
<li>Generates RGSs for set partitions in minimal-change order, which
corresponds to a Gray code for set partitions.</li>
<li>Uses an additional array of directions to track changes during
recursion.</li>
<li>The successor method adjusts the RGS based on the direction array
and repairs the tail when necessary.</li>
</ul></li>
<li><strong>Max-Increment RGS (rgs_maxincr):</strong>
<ul>
<li>A generalization of RGS for set partitions where sk ≤ maxj&lt;k(sj)
+ i, with i being a parameter.</li>
<li>The provided class generates these RGSs in lexicographic order.</li>
</ul></li>
</ol></li>
</ul></li>
</ol>
<p>These algorithms allow for efficient generation and manipulation of
restricted growth strings representing set partitions, which is crucial
for various combinatorial problems and applications.</p>
<p>This text discusses a specific type of restricted growth strings
(RGS), which are sequences used to represent set partitions, along with
their properties and generating functions. Here’s a detailed
summary:</p>
<ol type="1">
<li><p><strong>Restricted Growth Strings (RGS)</strong>: RGSs are
sequences where each element is non-decreasing and the difference
between consecutive elements is bounded by a fixed integer ‘i’. The text
focuses on two types of RGSs: max-increment RGSs and F-increment
RGSs.</p>
<ul>
<li><p><strong>Max-Increment RGSs (sk ≤ i + maxj&lt;k(sj))</strong>:
These are the standard RGSs, where each element sk is less than or equal
to ‘i’ plus the maximum of all previous elements (sj for j &lt; k). The
text provides sequences for various values of ‘i’ (1, 2, 3, and 4) and
mentions that these sequences correspond to specific entries in the
Online Encyclopedia of Integer Sequences (OEIS).</p></li>
<li><p><strong>F-Increment RGSs</strong>: These are a generalization of
max-increment RGSs. In an F-increment RGS (sk ≤ F(k) + i), the ‘maximum’
function F(k) increases only when the last increment (sk - sk-1) is
equal to ‘i’. The text also provides sequences for various values of ‘i’
and mentions their OEIS entries.</p></li>
</ul></li>
<li><p><strong>Generating Functions</strong>:</p>
<ul>
<li><p><strong>Exponential Generating Function (EGF)</strong>: The EGF
for RGSs with a given increment ‘i’ is provided in the text as exp[x +
Σ(exp(j*x)/j! from j=1 to i) - 1]. This formula generates sequences that
correspond to the number of RGSs of a given length.</p></li>
<li><p><strong>Ordinary Generating Function (OGF)</strong>: The OGF for
F-increment RGSs is stated as exp[Σ(exp(j*x)/j! from j=1 to i) - 1].
This function generates sequences that correspond to the total number of
RGSs up to a given length.</p></li>
</ul></li>
<li><p><strong>Sequences and their properties</strong>: The text
mentions several integer sequences related to RGSs, such as Bell numbers
(A000110 in OEIS), and provides their EGFs and OGFs. It also states that
the number of F-increment RGSs of length ‘n’ with increment ‘i’, denoted
Fn,i, can be calculated using Stirling numbers of the second kind S(n,
k) via the formula Fn,i = Σ(in-k * S(n, k)) from k=0 to n.</p></li>
<li><p><strong>Visual Representations</strong>: The text includes
figures that visually represent RGSs for different lengths and
increments, along with their corresponding arrays of ‘max’ values (for
max-increment RGSs) or ‘F’ values (for F-increment RGSs). These figures
help in understanding the structure of these sequences.</p></li>
</ol>
<p>In summary, this text discusses two types of generalized restricted
growth strings and provides their generating functions, related integer
sequences, and visual representations to understand their properties
better.</p>
<p>The FKM (Fredericksen, Kessler, Maiorana) algorithm is an efficient
method for generating all length-n k-ary pre-necklaces, which are a
subset of the necklaces. Pre-necklaces are strings that serve as
prefixes to some necklace. Not every pre-necklace is a necklace itself;
however, they provide an effective way to generate all possible
necklaces.</p>
<p>The algorithm works by iteratively modifying a zero-initialized
string of length ‘n’ with values from 0 to k-1 (where k is the number of
possible values for each element in the string). Here’s a detailed
explanation of its steps:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Start with an all-zero string F
= [f₁, f₂, …, fn] and set j = 1.</p></li>
<li><p><strong>Visit pre-necklace F</strong>: Check whether the current
string F is a necklace or a Lyndon word based on specific
conditions:</p>
<ul>
<li>If ‘j’ divides ‘n’, then F is a necklace.</li>
<li>If ‘j’ equals ‘n’, then F is a Lyndon word.</li>
</ul></li>
<li><p><strong>Find largest index for increment</strong>: Locate the
largest index ‘j’ where f_j &lt; k-1.</p>
<ul>
<li>If no such index exists (meaning all elements in F are k-1),
terminate the algorithm as we’ve reached the last pre-necklace, which is
[k-1, k-1, …, k-1].</li>
</ul></li>
<li><p><strong>Increment and copy periodically</strong>:</p>
<ul>
<li>Increment f_j.</li>
<li>Fill the suffix starting at position f_j+1 with copies of [f₁, …,
f_j], effectively extending the current pre-necklace.</li>
</ul></li>
<li><p><strong>Repeat</strong>: Go back to step 2 with the updated
string F.</p></li>
</ol>
<p>Key points in the implementation are: - The use of a delta sequence
(dv_) to quickly determine whether the current pre-necklace is also a
necklace or Lyndon word by checking divisors of ‘n’. - Optimizations
such as skipping unnecessary increment checks when j equals n.</p>
<p>The provided C++ class <code>necklace</code> in FXT implements this
algorithm, with additional methods for generating necklaces and Lyndon
words directly without producing pre-necklaces first. The rate of
generation for binary necklaces using this implementation is
approximately 128 M/s.</p>
<p>The FKM algorithm is highly efficient for generating k-ary strings,
making it suitable for applications involving necklace enumeration in
combinatorics and other fields where the concept of necklaces plays a
crucial role.</p>
<p>Title: Generating Binary Lyndon Words with Mersenne Exponent
Length</p>
<p>This text discusses various algorithms to generate binary Lyndon
words (BLWs) of lengths that are exponents of Mersenne primes, denoted
as Mn = 2^n - 1.</p>
<ol type="1">
<li><p><strong>Binary Lyndon Words Generation via Primitive
Roots</strong>: The first method described generates BLWs by using the
binary expansions of powers of a primitive root r of Mn. A primitive
root is an integer that generates all non-zero residues modulo Mn. For
example, with n = 7 and M7 = 127, the primitive root r = 3 generates a
sequence of BLWs as shown in Figure 18.1-B.</p></li>
<li><p><strong>Constant Amortized Time (CAT) Algorithm</strong>: This
algorithm is for generating k-ary pre-necklaces of length N. The core
function, <code>crsms_gen</code>, uses recursion to fill an array
<code>f</code> with values and visit the pre-necklaces when the length
exceeds N. This method can generate binary, ternary, or 5-ary
pre-necklaces efficiently.</p></li>
<li><p><strong>Order with Fewer Transitions</strong>: Another approach
generates binary BLWs in an order that minimizes transitions between
successive words by selecting valid words from a modified binary Gray
code. The routine <code>xgen</code> performs this generation, and Figure
18.1-C illustrates the output for 8-bit Lyndon words.</p></li>
<li><p><strong>Order with At Most Three Changes Per Transition</strong>:
This algorithm generates necklaces in an order where at most three
elements change with each update. The provided recursion
(<code>gen3</code>) produces binary length-8 necklaces, as shown in
Figure 18.1-E. Selecting the necklaces from complemented Gray codes of
n-bit binary words yields the same list.</p></li>
<li><p><strong>Binary Necklaces via Gray-Cycle Leaders</strong>: This
method generates BLWs by using cycle leaders of the Gray permutation,
which can be obtained through Reed-Muller transforms of these leaders.
Figure 18.1-G displays the correspondence between cycles and cyclic
shifts for length-8 binary necklaces.</p></li>
<li><p><strong>Binary Necklaces via Cyclic Shifts and
Complements</strong>: This algorithm generates all nonzero binary
necklaces using cyclic shifts and complements of the lowest bit. The
provided code snippet uses functions <code>sigma</code> (cyclic shift)
and <code>tau</code> (complement) to recursively generate BLWs in
lexicographic order, as shown in Figure 18.1-H for lengths n = 3 to
8.</p></li>
<li><p><strong>Lex-min De Bruijn Sequence from Necklaces</strong>: The
text also describes obtaining the lexicographically minimal De Bruijn
sequence by concatenating primitive parts of necklaces in lex order. An
implementation class <code>debruijn</code> is provided for this
purpose.</p></li>
</ol>
<p>In summary, this text presents multiple algorithms to generate binary
Lyndon words with lengths being Mersenne exponents and discuss various
ways to order these BLWs to minimize transitions between successive
words. It also explains how to derive the lex-min De Bruijn sequence
from necklaces and showcases different methods for generating necklaces
themselves using cyclic shifts, complements, and Gray permutation cycle
leaders.</p>
<p>The provided text discusses two main topics related to necklaces,
specifically binary necklaces, and Lyndon words (aperiodic necklaces),
focusing on their count and properties. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Binary Necklaces:</strong></p>
<ul>
<li><p>A binary necklace is a circular sequence of 0s and 1s. For
example, “0011” and “1100” are binary necklaces of length 4.</p></li>
<li><p>The total number of binary necklaces of length <code>n</code>
(denoted as <code>Nn</code>) can be calculated using the formula:</p>
<p>Nn = (1/n) Σ φ(d) * 2^(n/d), where d divides n, and φ is Euler’s
totient function.</p></li>
<li><p>This sequence (A000031) lists the number of binary necklaces for
various lengths (<code>n</code>). The formula takes into account that
each divisor <code>d</code> of <code>n</code> contributes φ(d) * 2^(n/d)
unique necklaces, normalized by the total number of possible sequences
(which is 2^n).</p></li>
</ul></li>
<li><p><strong>Binary Lyndon Words:</strong></p>
<ul>
<li><p>A binary Lyndon word is an aperiodic (non-repeating) circular
sequence of 0s and 1s.</p></li>
<li><p>The count of binary Lyndon words of length <code>n</code>
(denoted as <code>Ln</code>) is given by:</p>
<p>Ln = (1/n) Σ µ(d) * 2^(n/d), where d divides n, and µ is the Möbius
function.</p></li>
<li><p>This sequence (A001037) lists the number of binary Lyndon words
for various lengths (<code>n</code>). Similar to necklaces, each divisor
<code>d</code> of <code>n</code> contributes µ(d) * 2^(n/d) unique
Lyndon words.</p></li>
</ul></li>
<li><p><strong>Special Cases:</strong></p>
<ul>
<li>For prime length <code>p</code>, the number of binary Lyndon words
(and irreducible polynomials of degree <code>p</code>) equals L_p = N_p
- 2. This is because there are exactly <code>(p-1)/p * p</code> Lyndon
words with one ‘1’, and so on, up to ‘(p-1)/p * p’ Lyndon words with all
‘1s’.</li>
<li>The difference of 2 accounts for the necklaces consisting entirely
of 0s or 1s.</li>
</ul></li>
<li><p><strong>Relations between Necklace Counts:</strong></p>
<ul>
<li><p>There’s a relationship between the total count (<code>Nn</code>),
necklaces with fixed zeros (<code>N(n, n0)</code>), and Lyndon words
with fixed zeros (<code>L(n, n0)</code>) given by:</p>
<p>Nn = Σ N(n, d), where the sum is over all divisors <code>d</code> of
<code>n</code>.</p></li>
<li><p>This can be derived using Möbius inversion.</p></li>
<li><p>Similarly, Lyndon words satisfy Ln = Σ L(n, d).</p></li>
</ul></li>
<li><p><strong>Binary Necklaces with Fixed Density:</strong></p>
<ul>
<li><p>N(n, n0) is the count of binary necklaces of length
<code>n</code> with exactly <code>n0</code> zeros (and <code>n-n0</code>
ones).</p></li>
<li><p>The formula for N(n, n0) is:</p>
<p>N(n, n0) = (1/n) Σ φ(j) * (n/j choose n0/j), where j divides both n
and n0.</p></li>
</ul></li>
<li><p><strong>Binary Lyndon Words with Fixed Density:</strong></p>
<ul>
<li><p>L(n, n0) is the count of binary Lyndon words of length
<code>n</code> with exactly <code>n0</code> zeros.</p></li>
<li><p>The formula for L(n, n0) is:</p>
<p>L(n, n0) = (1/n) Σ µ(j) * (n/j choose n0/j), where j divides both n
and n0.</p></li>
</ul></li>
</ol>
<p>The text also provides tables of these counts for small values of
<code>n</code> and various fixed densities or zeros. These formulas and
relationships provide ways to calculate the number of necklaces and
Lyndon words with specific properties, which can be useful in
combinatorics, coding theory, and other fields involving circular
sequences.</p>
<p>The text discusses Hadamard matrices and conference matrices, which
are special types of matrices with significant applications in coding
theory, combinatorics, and signal processing. Here’s a detailed summary
and explanation:</p>
<ol type="1">
<li><strong>Hadamard Matrices</strong>:
<ul>
<li>A Hadamard matrix is an N × N orthogonal matrix with entries ±1,
where N is typically a power of 2. The property that makes it Hadamard
is that the dot product of any two distinct rows (or columns) is zero
and the dot product of any row (column) with itself is N/2.</li>
<li>For even N = 2n, an explicit construction using the Kronecker
product of smaller Hadamard matrices exists: H_N = [H_(N/2) | H_(N/2);
H_(N/2) | -H_(N/2)]. This construction doubles both the size and the
number of rows/columns.</li>
<li>The determinant of an N × N Hadamard matrix is ±N^(N/2).</li>
</ul></li>
<li><strong>Conference Matrices</strong>:
<ul>
<li>A conference matrix C_Q (where Q = q + 1 for odd prime q) is a Q × Q
matrix with zero diagonal and entries ±1, satisfying C_Q * C_Q^T = (Q-1)
* I, where I is the identity matrix. If Q ≡ 1 mod 4, then C_Q is
symmetric; if Q ≡ 3 mod 4, it’s antisymmetric.</li>
<li>A Hadamard matrix can be obtained from a conference matrix: for
symmetric matrices, simply add 1 to the diagonal. For antisymmetric
matrices, construct a 2Q × 2Q matrix using [C_Q | I; -I | C_Q].</li>
</ul></li>
<li><strong>Construction via Finite Fields</strong>:
<ul>
<li>Conference and Hadamard matrices can also be constructed using
finite fields GF(q^n), where q is an odd prime and n is a positive
integer. The elements z0, …, z_(q^n-1) are ordered, and the quadratic
character of their differences determines the entries of C_Q.</li>
<li>To compute these characters efficiently, precompute a table using
polynomial operations in GF(q).</li>
</ul></li>
<li><strong>Hadamard Matrices via LFSR (Linear Feedback Shift
Register)</strong>:
<ul>
<li>This method constructs Hadamard matrices using maximum length binary
shift register sequences (SRS), signed appropriately (-1 for 1s and +1
for 0s). The N × N matrix H is then filled with cyclic shifts of this
sequence, with the first row/column set to all ones.</li>
</ul></li>
<li><strong>Applications</strong>:
<ul>
<li>Hadamard matrices are used in various areas such as signal
processing (e.g., Walsh-Hadamard transforms), error-correcting codes,
and combinatorial designs. Conference matrices are related to Hadamard
matrices and have applications in coding theory and combinatorics.</li>
</ul></li>
</ol>
<p>In essence, the text discusses different methods for constructing
special matrices with specific properties (±1 entries and orthogonality)
that find applications in various fields of mathematics and
engineering.</p>
<p>The text discusses the representation of combinatorial structures as
paths or cycles in directed graphs. It uses the example of Gray codes of
n-bit binary words, which are sequences where only one bit changes
between consecutive words. A convenient way to visualize this search
space is through a directed graph, with nodes representing the binary
words and edges connecting nodes if their values differ by exactly one
bit.</p>
<p>The text introduces terminology for working with graphs: “node”
instead of “vertex,” “edge” (sometimes called arc), and terms like
simple path, full path, cycle (or circuit), Hamiltonian cycle,
Hamiltonian graph, loops (edges starting and ending at the same node),
pseudo-graphs (graphs that may contain loops), and multigraphs (graphs
with multiple edges between nodes).</p>
<p>The text also describes a simple representation of directed graphs
using arrays for storing nodes and outgoing edges. An example is given
for initializing the complete graph, where every node has an edge
connecting it to all other nodes.</p>
<p>To search full paths starting from some position p0, additional data
structures are required: arrays to record visited nodes (rv_[]) and tags
indicating whether a node has been visited (qq_[]). A recursive
algorithm is employed for the path search, which can find at least one
object, generate all objects, or show that no such object exists. The
method used is called backtracking.</p>
<p>The text provides examples of using this graph representation to
search for permutations in a complete graph and De Bruijn sequences in a
De Bruijn graph. It also mentions a modified De Bruijn graph for
generating complement-shift sequences.</p>
<p>In summary, the text outlines how directed graphs can be used as a
tool for searching combinatorial structures, with examples involving
permutations and sequences like Gray codes and De Bruijn sequences. The
algorithms presented are based on backtracking and can be optimized to
handle larger graph sizes efficiently.</p>
<p>The provided text discusses two types of Gray codes, Modular Adjacent
Changes (MAC) and Adjacent Changes (AC), and their search using a
conditional graph traversal algorithm.</p>
<ol type="1">
<li><p><strong>Modular Adjacent Changes (MAC) Gray Codes</strong>: These
are Gray codes where the difference between successive elements in the
delta sequence can only change by ±1 modulo n. The text presents an
algorithm to find such paths, with the condition that canonical paths
start as 0–&gt;1–&gt;3. The search is performed using
<code>all_cond_paths()</code> function, which takes a condition-imposing
function (<code>cfunc_mac</code> in this case) and other parameters.</p>
<p>The condition function (<code>cfunc_mac</code>) checks if the
difference between successive delta values (modulo n) equals ±1. It does
this by comparing the current value (<code>p</code>), previous value
(<code>p1</code>), and the value before that (<code>p2</code>). If the
bitwise AND of <code>c</code> (the difference between <code>p</code> and
<code>p1</code>) and <code>c1</code> (the difference between
<code>p1</code> and <code>p2</code>, rotated left by one) is true, or
vice versa, the condition is met.</p></li>
<li><p><strong>Adjacent Changes (AC) Gray Codes</strong>: These are Gray
codes where the difference between successive elements in the delta
sequence equals ±1. The search algorithm for AC paths discards
track-reflected solutions and considers canonical paths to start with a
value less than or equal to ⌈n/2⌉.</p>
<p>The condition function (<code>cfunc_ac</code>) checks if the
difference between successive delta values equals ±1, but avoids
track-reflected solutions by ensuring the first value is less than
<code>cf_mt</code> (mid track). It performs similar bitwise operations
as <code>cfunc_mac</code>, comparing the current value (<code>p</code>),
previous value (<code>p1</code>), and the value before that
(<code>p2</code>).</p></li>
</ol>
<p>The text also mentions that MAC Gray codes exist for n ≤ 7, while AC
Gray codes exist for n ≤ 6. No MAC or AC Gray codes are known to exist
for n ≥ 8 and n = 7 respectively. The search times indicate the
computational complexity of these problems increases significantly with
n.</p>
<p>The text concludes by mentioning an ad-hoc algorithm to compute delta
sequences for AC Gray codes for n ≤ 6, though no specific details about
this algorithm are provided in the given excerpt.</p>
<p>The text discusses the application of Gray codes to Lyndon words,
which are non-repeating binary strings that are lexicographically
smaller than all their rotations (cyclic permutations).</p>
<p>In the context of graph theory, a directed graph is constructed where
nodes represent n-bit Lyndon words. Edges connect nodes if one word can
be transformed into another by flipping exactly one bit (a single
transition). The goal is to find paths through this graph that traverse
all nodes exactly once and return to the starting node, known as
Hamiltonian cycles in directed graphs or Eulerian circuits when each
node has equal in-degree and out-degree.</p>
<ol type="1">
<li><p><strong>Gray Codes for Lyndon Words</strong>: The text starts by
discussing Gray codes for n-bit binary Lyndon words where n is a prime
number. A Gray code is a sequence of n-bit binary numbers such that each
pair of consecutive numbers differs in only one bit. For Lyndon words,
this means that the sequence visits all non-repeating binary strings
exactly once while changing only one bit at a time. The text mentions a
5-bit example and shows how to extend this concept to larger prime
numbers like 7 and 11 using graph search methods.</p></li>
<li><p><strong>Graph Search with Edge Sorting</strong>: To find such
paths, an algorithm is employed that traverses the graph nodes in an
arbitrary order but sorts the outgoing edges of each node according to a
specific comparison function (lexicographic order in this case). This
sorting can significantly impact the search efficiency, as demonstrated
by comparing the time it takes to find the first path for both sorted
and unsorted graphs.</p></li>
<li><p><strong>Lucky Paths</strong>: The concept of “lucky paths” is
introduced. A lucky path is one where no U-turns (backtracking) occur
during traversal—meaning that once a node is visited, its neighbors are
always traversed in the correct order without needing to revisit any
previously explored paths. If such a lucky path exists for a given graph
and starting point, it is typically found very quickly, as the number of
operations scales linearly with the number of edges.</p></li>
<li><p><strong>Edge Sorting Impact</strong>: The example provided shows
that sorting the edges can drastically reduce the time needed to find
the first path in the graph. For an 8-bit problem, the unsorted graph
takes 1.14 seconds, whereas the sorted one requires only 0.03
seconds.</p></li>
<li><p><strong>Lyndon Words Graph Construction</strong>: Nodes in this
graph correspond to n-bit Lyndon words, and edges exist between two
nodes if their corresponding Lyndon words differ by a single bit flip
(one transition). The path through the graph represents a Gray code for
the Lyndon words.</p></li>
<li><p><strong>Finding Gray Codes via Graph Search</strong>: The
algorithm searches for paths that visit each node exactly once
(Hamiltonian cycle) or, in some cases, allows for arbitrary rotations of
the Lyndon words (Eulerian circuit with degree equality). The search is
performed using a recursive function <code>next_path()</code>, which
explores possible transitions from the current node.</p></li>
<li><p><strong>Challenges with Larger Primes</strong>: The text mentions
that while finding Gray codes for smaller primes like 7 is feasible,
larger ones (like 11 and 13) pose significant computational challenges
due to the exponential growth in the number of nodes and edges as n
increases. Even after sorting the edges, the search times become
prohibitive for very large graphs.</p></li>
</ol>
<p>In summary, this text explores the application of Gray codes to
Lyndon words represented as directed graphs. It highlights how edge
sorting can significantly impact search efficiency and introduces the
concept of “lucky paths”—traversals without backtracking that can be
found rapidly if they exist. The challenges in applying these methods to
larger graphs due to exponential growth in complexity are also
discussed.</p>
<p>The Decimation in Time (DIT) FFT algorithm, also known as the
Cooley-Tukey FFT algorithm, is a method for efficiently computing the
Discrete Fourier Transform (DFT) of a sequence. This algorithm leverages
the property that the k-th element of the DFT can be expressed as a
combination of the DFTs of the even and odd indexed subsequences of the
original sequence.</p>
<p>Here’s a detailed explanation:</p>
<ol type="1">
<li><p><strong>Notation</strong>:</p>
<ul>
<li>Let <code>a</code> be a length-n sequence, where n is a power of
2.</li>
<li><code>a(even)</code> and <code>a(odd)</code> denote the length-n/2
subsequences of even and odd indexed elements, respectively.</li>
<li><code>a(left)</code> and <code>a(right)</code> denote the left and
right subsequences, respectively.</li>
</ul></li>
<li><p><strong>Key Identity</strong>: The identity that forms the basis
of the DIT FFT is:</p>
<p>F[a]k = Σ x=0^n-1 ax * zx k = Σ x=0^n/2-1 a2x * z2x k + Σ x=0^n/2-1
a2x+1 * z(2x+1) k</p>
<p>Here, <code>z</code> is an n-th primitive root of unity (zn = 1 and
zj ≠ 1 for 0 &lt; j &lt; n), and <code>σ</code> is the sign of the
transform.</p></li>
<li><p><strong>Rewriting the DFT</strong>: The above identity can be
rewritten in terms of j and δ, where k ∈{0, 1, …, n −1} = j + δ *
n/2:</p>
<p>F[a]j+δ<em>n/2 = Σ x=0^n/2-1 a(even)x </em> z2x (j+δ<em>n/2) +
zj</em>n/2 * Σ x=0^n/2-1 a(odd)x * z2x (j+δ*n/2)</p>
<p>This shows how to compute the j+δ*n/2-th element of the DFT from the
DFTs of the even and odd indexed subsequences.</p></li>
<li><p><strong>Algorithm</strong>: The DIT FFT algorithm proceeds
recursively by dividing the sequence into even and odd indexed
subsequences, computing their DFTs, and then combining these results to
obtain the DFT of the original sequence. This process is known as
decimation in time because it involves reducing the sample rate
(decimating) by a factor of 2 at each step.</p></li>
<li><p><strong>Time Complexity</strong>: The DIT FFT algorithm has a
time complexity of O(n log(n)), which is a significant improvement over
the naive O(n^2) algorithm for computing the DFT. This makes it a
crucial tool in many areas of signal processing, image processing, and
other fields where Fourier transforms are used.</p></li>
<li><p><strong>Sign of Transform</strong>: The operator <code>S</code>
in the algorithm depends on the sign of the transform (σ = ±1). It
represents a shift operation that multiplies each element by
z^σ<em>2π</em>i*x/n, where x is the index and n is the length of the
sequence.</p></li>
</ol>
<p>The provided text describes various aspects of Fast Fourier Transform
(FFT) algorithms, focusing on higher radix methods to reduce
trigonometric computations. Here’s a summary and explanation of the key
points:</p>
<ol type="1">
<li><p><strong>Radix-2 Decimation in Time (DIT) FFT</strong>: This is
the basic form of FFT, which recursively divides the input sequence into
even and odd indexed elements, applies a twiddle factor (S0/2 or S1/2),
and combines the results. The complexity is O(n log2 n).</p>
<ul>
<li><strong>Recursive implementation</strong>: Uses multiple function
calls to compute the transform, with each step having O(n) work.</li>
<li><strong>Iterative implementation</strong>: Avoids function call
overhead by using a single loop, but still requires workspace. It can be
made in-place and non-recursive.</li>
</ul></li>
<li><p><strong>Radix-4 DIT FFT</strong>: This extends the radix-2 method
to use four subsequences (0%4, 1%4, 2%4, 3%4) instead of two. It reduces
trigonometric computations by simplifying multiplications with complex
factors (0, ±i) into simpler operations.</p>
<ul>
<li><strong>General radix-r DIT FFT</strong>: The formula for the
general case is provided, allowing for any radix r that divides n.</li>
</ul></li>
<li><p><strong>Radix-2 Decimation in Frequency (DIF) FFT</strong>: This
method splits the Fourier sum into left and right halves, recursively
applying the transform to these halves with twiddle factors.</p>
<ul>
<li><strong>Recursive implementation</strong>: Similar to DIT, but with
a different memory access pattern that can be less efficient for large
n.</li>
<li><strong>Iterative implementation</strong>: Non-recursive version
that works in-place and saves trigonometric computations by swapping
inner loops.</li>
</ul></li>
<li><p><strong>Saving Trigonometric Computations</strong>: The text
discusses two methods to reduce the computational cost of sine and
cosine calculations:</p>
<ul>
<li><strong>Lookup Tables</strong>: Precompute and store necessary
values, reducing the need for real-time calculations. This is effective
for FFTs of small lengths but can lead to cache problems for large
sequences.</li>
<li><strong>Recursive Generation</strong>: Use trigonometric recursion
to compute values, with a stable version provided that loses less than 3
bits of precision even for very long FFTs.</li>
</ul></li>
<li><p><strong>Higher Radix FFT Algorithms (Radix-4 and General
r)</strong>: These methods further reduce trigonometric computations by
using more subsequences (r instead of 2). They also simplify special
cases where sines and cosines equal ±√1/2. The general radix-r DIT and
DIF FFT steps are provided.</p></li>
<li><p><strong>Implementation of Radix-r FFTs</strong>: For higher
radices, the revbin_permute routine is replaced by a radix_permute
function that swaps elements based on reversing their radix-r expansion.
The pseudocode for a radix r = px DIT FFT is given.</p></li>
</ol>
<p>In summary, the text presents various FFT algorithms and methods to
optimize their performance, focusing on higher radix techniques to
reduce trigonometric computations and improve efficiency.</p>
<p>The Split-Radix Algorithm is a method used in Fast Fourier Transform
(FFT) computation that combines both radix-2 and radix-4 decompositions
to achieve a lower operation count compared to the radix-4 FFT. This
algorithm splits the length-N=2^n FFT into one length-N/2 and two
length-N/4 FFTs, using relations from both radix-2 (Decimation in
Frequency, DIF) and radix-4 decompositions.</p>
<p>The key idea behind the Split-Radix FFT is to use a radix-2 relation
for even indices and a radix-4 splitting for odd indices, albeit in a
slightly reordered form.</p>
<p>For the Decimation In Time (DIT) version of the split-radix FFT:</p>
<ol type="1">
<li><p>The length-N/2 FFT is calculated using a radix-2 decomposition:
F<a href="0/2">a</a>^n = F[a(0%2)]^n + S1/2 * F[a(1%2)]^n, where S1/2 is
the sign factor (±1 or ±i) depending on the FFT sign (is).</p></li>
<li><p>The length-N/4 FFTs are calculated using a radix-4 decomposition:
F<a href="1/4">a</a>^n = F[a(0%4)]^n - S2/4 * F[a(2%4)]^n + i<em>σ </em>
S1/4 * F[a(1%4)]^n - S2/4 * F[a(3%4)]^n, F<a href="3/4">a</a>^n =
F[a(0%4)]^n - S2/4 * F[a(2%4)]^n - i<em>σ </em> S1/4 * F[a(1%4)]^n -
S2/4 * F[a(3%4)]^n, where σ is the sign of the transform (+1 or -1), and
S2/4 and S1/4 are sign factors (±1 or ±i).</p></li>
</ol>
<p>The operation count for Split-Radix FFT is lower than that of radix-4
FFT due to this efficient use of both radix-2 and radix-4
decompositions. It’s particularly advantageous when N is large, as it
reduces the number of complex multiplications compared to a pure radix-4
approach.</p>
<p>In practical implementations, the split-radix algorithm would involve
procedures for calculating each FFT step (length-N/2 and length-N/4),
with these procedures being further optimized for specific cases (e.g.,
hard-coded for σ=±1 or using complex numbers). The actual splitting of
work into radix-2 and radix-4 steps, as well as the handling of sign
factors, would be implemented within these procedures.</p>
<p>The Split-Radix DIF algorithm can be seen as an extension of this
concept, where both even and odd indices are handled with a combination
of radix-2 and radix-4 relations, ultimately achieving a similar
reduction in operation count while offering flexibility in handling
different FFT signs.</p>
<p>The provided code snippets appear to be a C++ implementation of the
Split-Radix algorithm for computing the Discrete Fourier Transform
(DFT), specifically designed for real-valued inputs. This algorithm is
efficient and takes advantage of the symmetries of the DFT for real
inputs to reduce computational complexity.</p>
<p>Here’s a summary and explanation of the key sections:</p>
<ol type="1">
<li><p><strong>Complex Multiplication and Splitting:</strong></p>
<ul>
<li>Lines 43-52: These lines define complex multiplication for two pairs
of complex numbers (r1, s1) and (ss1, cc1), and (r2, s2) and (cc3, ss3).
The results are split into real and imaginary parts. This is used in the
subsequent butterfly operations of the FFT algorithm.</li>
</ul></li>
<li><p><strong>Butterfly Operations:</strong></p>
<ul>
<li>Lines 54-60: These lines perform complex multiplications followed by
summation/subtraction (represented by <code>sumdiff3</code>) for
processing pairs of data points. This is a key step in the FFT
algorithm, known as the “butterfly operation.”</li>
</ul></li>
<li><p><strong>Decrement and Loop Control:</strong></p>
<ul>
<li>Lines 61-82: These lines handle loop control and decrementing the
index ‘id’ to traverse through all necessary data points in the FFT
computation. The variable ‘i0’ is used to keep track of the current data
point being processed.</li>
</ul></li>
<li><p><strong>Permute and Swap Operations:</strong></p>
<ul>
<li>Lines 83-92: These lines perform permutations and swaps on the data
arrays, which are essential for arranging the output in the correct
order (revbin_permuted). This step is necessary because the FFT
algorithm naturally produces results in a different order than the
standard Fourier Transform.</li>
</ul></li>
<li><p><strong>Additional Utility Functions:</strong></p>
<ul>
<li>The code uses several utility functions
(<code>revbin_permute</code>, <code>swap</code>, etc.) which are not
detailed in these snippets but are likely defined elsewhere in the
source file or library, handling tasks like array permutation and
swapping elements within arrays.</li>
</ul></li>
<li><p><strong>DIF Core Implementation:</strong></p>
<ul>
<li>The DIF (Decimation In Frequency) core is implemented starting from
line 21 in the provided code. This part of the FFT algorithm processes
the data in frequency space by dividing it into even and odd indexed
components, then recursively applying the butterfly operations.</li>
</ul></li>
</ol>
<p>The Split-Radix FFT algorithm described here is a more complex
version of the Cooley-Tukey radix-2 FFT, designed to reduce
computational complexity for certain types of inputs (like real-valued
sequences) by exploiting their symmetries. The provided pseudocode and
C++ implementations serve as a guideline for understanding and
implementing this algorithm in various programming languages.</p>
<p>The text provided discusses various aspects of convolution,
correlation, and related algorithms using the Fast Fourier Transform
(FFT). Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Cyclic Convolution</strong>: This is the type of
convolution considered in the text. For two sequences <code>a</code> and
<code>b</code> of length <code>n</code>, the cyclic convolution
<code>h = a ⊛ b</code> is defined as
<code>h_τ = Σ_{x+y ≡ τ (mod n)} a_x * b_(τ-x)</code>. This means that
indices wrap around, creating a circular or periodic
convolution.</p></li>
<li><p><strong>Direct Computation</strong>: The text provides a C++
implementation of the direct computation of cyclic convolution using
nested loops. This method, while straightforward, has a time complexity
of O(n^2), making it inefficient for large sequences.</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> Type<span class="op">&gt;</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> slow_convolution<span class="op">(</span><span class="at">const</span> Type <span class="op">*</span>f<span class="op">,</span> <span class="at">const</span> Type <span class="op">*</span>g<span class="op">,</span> Type <span class="op">*</span>h<span class="op">,</span> <span class="ex">ulong</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="ex">ulong</span> tau<span class="op">=</span><span class="dv">0</span><span class="op">;</span> tau<span class="op">&lt;</span>n<span class="op">;</span> <span class="op">++</span>tau<span class="op">)</span> <span class="op">{</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        Type s <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="ex">ulong</span> k<span class="op">=</span><span class="dv">0</span><span class="op">;</span> k<span class="op">&lt;</span>n<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            s <span class="op">+=</span> f<span class="op">[</span>k<span class="op">]</span> <span class="op">*</span> g<span class="op">[(</span>tau<span class="op">-</span>k<span class="op">+</span>n<span class="op">)%</span>n<span class="op">];</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        h<span class="op">[</span>tau<span class="op">]</span> <span class="op">=</span> s<span class="op">;</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div></li>
<li><p><strong>Fast Fourier Transform (FFT) Based Convolution</strong>:
The text focuses on algorithms that leverage the FFT to compute
convolution more efficiently. These methods have a time complexity of
O(n log n), making them suitable for large sequences.</p>
<ul>
<li><p><strong>One-dimensional Convolution via FFT</strong>: This method
involves transforming both sequences using FFT, element-wise
multiplication in the frequency domain, and inverse FFT to obtain the
convolution result. The complex conjugate symmetry property is used to
reduce computational cost by only computing half of the frequency
domain.</p></li>
<li><p><strong>Multi-dimensional Convolution via FFT</strong>: For
multi-dimensional arrays (e.g., matrices), the text discusses applying
1D FFTs first along rows, then columns (row-column algorithm) or vice
versa. Transposing the array before the column pass can improve
performance due to better memory access patterns.</p></li>
<li><p><strong>Matrix Fourier Algorithm (MFA)</strong>: This is a
variant of the row-column method for 1D FFTs, optimized for data lengths
<code>n = R * C</code>. It involves applying FFTs on columns and rows,
followed by element-wise multiplication with trigonometric factors and a
final matrix transposition.</p></li>
<li><p><strong>Transposed Matrix Fourier Algorithm (TMFA)</strong>: This
is a variation of the MFA that accesses memory in consecutive address
ranges, avoiding the need for a final transpose if an inverse transform
follows immediately.</p></li>
</ul></li>
<li><p><strong>Weighted Convolution</strong>: The text introduces
weighted convolution, where each element of one sequence is multiplied
by a corresponding weight before convolution. This can be computed
efficiently using FFT-based methods by incorporating the weights into
the frequency domain multiplication step.</p></li>
<li><p><strong>Convolution for Z-Transform Computation</strong>: The
text explains how fast convolution algorithms can be used to compute the
Z-transform of sequences of arbitrary length, which is a crucial
operation in signal processing and control theory.</p></li>
<li><p><strong>Rader’s Algorithm</strong>: For arrays of prime length,
Rader’s algorithm provides an efficient method for computing the
discrete Fourier transform (DFT) using convolution. This algorithm
leverages the properties of cyclotomic polynomials to reduce the
computational complexity compared to direct DFT computation.</p></li>
</ol>
<p>In summary, the text presents various FFT-based algorithms for
efficient convolution and related operations, emphasizing their time
complexity advantages over direct computation methods. It covers
one-dimensional and multi-dimensional cases, weighted convolutions,
Z-transform computation, and specialized algorithms like Rader’s
algorithm for prime lengths.</p>
<p>The provided text discusses various aspects of convolution and
correlation, including both direct computation methods and Fast Fourier
Transform (FFT)-based approaches. Here’s a detailed summary:</p>
<ol type="1">
<li><p><strong>Cyclic Convolution</strong>: This is the operation of two
sequences where the output is wrapped around, meaning that elements from
the end of one sequence can pair with elements from the beginning of the
other. The complexity for direct computation is O(n^2), making it slow
for large n but suitable for small lengths.</p></li>
<li><p><strong>FFT-based Cyclic Convolution</strong>: Using FFT allows
convolution to be computed more efficiently, requiring only O(n log(n))
operations. This method exploits the convolution property of the Fourier
transform (F[a ⊛b] = F[a] · F[b]). The algorithm involves three main
steps:</p>
<ul>
<li>Transform both input sequences using a Forward FFT.</li>
<li>Perform element-wise multiplication in the transformed domain.</li>
<li>Apply an Inverse FFT to get the result.</li>
</ul></li>
<li><p><strong>Linear (Acyclic) Convolution</strong>: This is similar to
cyclic convolution but without wrapping around indices, resulting in a
sequence of twice the length. It can be computed using zero-padded
cyclic convolution. The semi-symbolic table for linear correlation
differs from that of cyclic correlation, with elements arranged in a
full matrix rather than a triangular one.</p></li>
<li><p><strong>Cyclic Correlation</strong>: This measures the similarity
between two sequences as a function of the displacement of one relative
to the other. It’s also known as circular correlation. The semi-symbolic
table for this operation is provided, showing non-wrapping index
relationships.</p></li>
<li><p><strong>Direct Computation Methods</strong>: For both cyclic and
linear correlations, direct computation methods are given in C++ code.
These involve nested loops that compute sums of products between
elements from different shifts of the sequences. To avoid index
overflow/underflow checks, an alternative version is provided that uses
modulo arithmetic within the loops.</p></li>
<li><p><strong>Zero-padded Data for Linear Correlation</strong>: When
computing linear correlation (which requires zero-padding), specific
elements (f[k], g[k] for k=n/2…n-1) are assumed to be zero, optimizing
the computation by avoiding unnecessary multiplications with
zeros.</p></li>
<li><p><strong>Auto-correlation and Cross-correlation</strong>:
Auto-correlation refers to correlating a sequence with itself, while
cross-correlation involves distinct sequences. The term
“auto-correlation function” (ACF) is often used for auto-correlation
sequences.</p></li>
</ol>
<p>In summary, the text covers both theoretical aspects (definitions,
semi-symbolic tables) and practical computational methods for cyclic and
linear convolutions/correlations, with a focus on optimization
techniques like zero-padding and FFT-based approaches to improve
efficiency.</p>
<p>The provided text discusses various aspects of correlation,
convolution, and related algorithms. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Correlation Algorithms</strong>: Two correlation
algorithms are presented. The first algorithm (lines 12-14) computes the
cyclic correlation between two sequences <code>f[]</code> and
<code>g[]</code>, storing the result in an array <code>h[]</code>. It
iterates over the sequences, multiplying corresponding elements and
summing the results. This algorithm has a time complexity of O(n^2).</p>
<p>The second algorithm (lines 17-22) is similar but designed for
self-correlation (when one sequence is the same as the other), which can
be more efficiently computed using Fast Fourier Transform (FFT)
techniques.</p></li>
<li><p><strong>Fast Correlation via FFT</strong>: This method leverages
the Fast Fourier Transform to calculate correlation more efficiently.
The algorithm (lines 7-21) first transforms both input sequences using
an FHT (Fast Hartley Transform), then multiplies each element by its
complex conjugate (or real conjugate for real sequences), and finally
transforms back using another FHT. This method significantly reduces
computational complexity compared to the O(n^2) algorithm.</p></li>
<li><p><strong>Correlation, Convolution, and Circulant
Matrices</strong>: The text explains how cyclic correlation and
convolution correspond to multiplication with circulant matrices. For a
given sequence <code>a[]</code>, its cyclic correlation with itself
(autocorrelation) can be represented as a matrix-vector product with a
specific circulant matrix <code>Ra</code>. Similarly, the cyclic
convolution is represented by another circulant matrix <code>Ca</code>.
The Fourier transform diagonalizes these circulant matrices, simplifying
their analysis and computation.</p></li>
<li><p><strong>Weighted Fourier Transforms and Convolutions</strong>:
These are variations of standard Fourier transforms and convolutions
that involve a weight sequence <code>v[]</code>. The weighted Fourier
transform (lines 1-8) scales each input element by its corresponding
weight before applying the FFT. Its inverse operation (lines 9-16)
reverses this scaling after the inverse FFT. Weighted convolution (lines
22.4.2) is a modified form of convolution where each summand in the
standard convolution definition is multiplied by a weight. The weighted
cyclic convolution can be computed efficiently using the FFT, as shown
in the provided code snippets.</p></li>
<li><p><strong>Negacyclic Convolution</strong>: This is a specific type
of weighted cyclic convolution where weights are chosen such that
<code>V_n = -1</code>. It results in a negacyclic (or skew circular)
convolution, which separates the cyclic and wrapped parts of the
standard convolution more distinctly for real sequences. The negacyclic
convolution can be computed efficiently using FFT-based methods, as
demonstrated by the provided code snippets.</p></li>
</ol>
<p>In summary, the text covers various algorithms for computing
correlation and convolution, including naive O(n^2) methods, FFT-based
fast correlation, and weighted/negacyclic convolutions. These techniques
offer trade-offs between simplicity and computational efficiency, with
FFT-based methods generally providing significant speedups for large
sequences.</p>
<p>The provided text discusses the Walsh Transform, a discrete transform
similar to the Fourier Transform but without involving multiplications.
It’s also known as the Walsh-Hadamard Transform or simply Hadamard
Transform. The Walsh Transform uses a Walsh-Kronecker basis, which
consists of functions that take on values +1 and -1.</p>
<p>The text presents a 2D visualization of these basis functions in
Figure 23.1-A, where asterisks denote the value +1 and blank entries
denote -1. The Walsh Transform can be derived from a Fast Fourier
Transform (FFT) by removing multiplications with sines and cosines.</p>
<p>The text also mentions a specific implementation of the Walsh
Transform using a radix-2 Decimation in Time (DIT) FFT, which is
presented as a C++ function <code>slow_walsh_wak_dit2</code>. This
function takes a double pointer <code>f</code> to the input data and its
length <code>ldn</code>, and it seems to perform the Walsh Transform
using a nested loop structure.</p>
<p>The outer loop iterates over different bit lengths (<code>ldm</code>)
from 1 to <code>ldn</code>, while the inner loops handle the actual
transformation process. The variable <code>m</code> represents the
current bit length, and <code>mh</code> is half of that. The variable
<code>r</code> iterates over the input data in steps of
<code>m</code>.</p>
<p>However, the text notes a problem with this implementation without
providing further details. Despite this issue, understanding the
structure of this code can give insights into how to implement a Walsh
Transform using an FFT-like approach.</p>
<p>The Walsh Transform has applications in various fields, including
signal processing and error-correcting codes, due to its efficiency and
lack of multiplication operations. It’s also used in the computation of
XOR (dyadic) convolutions, which can be done efficiently by the Walsh
Transform. Other related transforms mentioned in the text include the
Slant Transform, Reed-Muller Transform, and Arithmetic Transform.</p>
<p>The text discusses various aspects of the Walsh transform, a type of
discrete Fourier-like transform used for signal processing. Here’s a
detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Walsh Transform Basics</strong>: The Walsh transform is a
real-valued, orthogonal transform that operates on sequences of
integers. It has two main properties: it’s self-inverse (its eigenvalues
are ±1), and it doesn’t involve multiplication operations, making it
computationally efficient.</p></li>
<li><p><strong>Radix-2 Decimation in Time (DIT) Algorithm</strong>: The
given code snippet implements a radix-2 DIT algorithm for the Walsh
transform. This algorithm performs n log2(n) additions (and
subtractions), making it faster than the naive approach but still
suffers from non-local memory access patterns, which can degrade
performance on modern processors due to cache inefficiency.</p></li>
<li><p><strong>Improved Algorithm</strong>: A more efficient version of
the Walsh transform is presented, which achieves a speedup of about 8x
for n = 2^21 (with double precision) compared to the original algorithm.
This improvement is achieved by reordering the memory access pattern,
making it more cache-friendly.</p></li>
<li><p><strong>Eigenvectors of the Walsh Transform</strong>: The text
explains how to compute eigenvectors of the Walsh transform. Given a
sequence ‘a’, two eigenvectors can be computed: u+ = W(a) + a (with
eigenvalue +1) and u- = W(a) - a (with eigenvalue -1). The routine
<code>walsh_wak_eigen</code> generates an eigenvector by adding a scaled
delta peak to the corresponding basis function.</p></li>
<li><p><strong>Kronecker Product</strong>: This mathematical concept is
introduced as a powerful tool for dealing with orthogonal transforms and
their fast algorithms. It allows expressing complex matrix operations
(like the Walsh transform) in terms of simpler ones, facilitating
algorithm development and analysis.</p></li>
<li><p><strong>Higher Radix Walsh Transforms</strong>: The text
discusses methods for generating short-length Walsh transforms (DIF and
DIT variants) using a generator approach. This allows creating custom
transform routines for various lengths, including higher radix (e.g.,
radix-4, radix-8) algorithms.</p></li>
<li><p><strong>Performance Comparison</strong>: The text presents
performance comparisons of different Walsh transform implementations,
including a matrix variant. It highlights that the choice of algorithm
depends on factors like cache size and transform length, with the
localized Walsh transform (avoiding expensive transposition operations)
generally outperforming the matrix version for large datasets.</p></li>
</ol>
<p>In summary, the text covers the theoretical aspects and practical
implementations of the Walsh transform, including improvements in memory
access patterns, eigenvector computation, and higher-radix algorithms.
It also introduces the Kronecker product as a useful mathematical tool
for analyzing and developing fast transforms.</p>
<p>The text discusses the Walsh-Hadamard Transform (WHT) and its
variants, focusing on localized versions that provide improved
performance for large data sets by optimizing memory access patterns.
Here’s a detailed summary and explanation of the key points:</p>
<ol type="1">
<li><p><strong>Localized Walsh Transforms</strong>: These are optimized
implementations of the Walsh-Hadamard Transform (WHT) designed to
minimize cache misses and improve performance on large data sets. They
are based on a decimation-in-frequency (DIF) or decimation-in-time (DIT)
approach, depending on whether the transform is executed in a forward
(DIF) or reverse (DIT) order.</p></li>
<li><p><strong>Memory Access Pattern</strong>: The localized algorithms
use a specific memory access pattern that results in excellent
performance for large transforms. This pattern involves processing
smaller chunks of data at a time, keeping more data in the cache when
larger sub-arrays are accessed.</p></li>
<li><p><strong>Comparison with Matrix Algorithm</strong>: Figure 23.5-A
compares the speed of localized Walsh transforms (both DIF and DIT) with
the matrix algorithm. For small sizes, the localized algorithms have
similar speed to the radix-4 version they fall back to. However, for
larger sizes, the localized algorithms outperform the matrix algorithm,
even when considering only one transposition. The DIF version is
slightly faster for very large transforms due to its starting with
smaller chunks of data.</p></li>
<li><p><strong>Recursive Implementation</strong>: Both DIF and DIT
versions are implemented recursively, with base cases handling small
transform sizes directly (without falling back to full Walsh
transforms). This recursive structure allows for efficient execution of
larger transforms by breaking them down into smaller
sub-problems.</p></li>
<li><p><strong>Iterative Versions</strong>: Iterative versions of the
algorithms are also discussed, with the DIF algorithm executing Haar
transforms at positions f + 2, f + 4, f + 6, …, and the length of the
transform at position f + s determined by the lowest set bit in s. The
DIT algorithm follows a similar pattern but with reversed binary words
in reversed lexicographic order.</p></li>
<li><p><strong>Walsh-Paley Transform</strong>: A different ordering of
the Walsh basis, known as the Walsh-Paley basis (figure 23.6-A), is also
mentioned. This transform can be computed using the
<code>walsh_pal</code> function provided in [FXT:
walsh/walshpal.h].</p></li>
</ol>
<p>In summary, the text presents optimized localized Walsh transform
algorithms that leverage efficient memory access patterns to achieve
better performance for large data sets. It compares these localized
transforms with a matrix algorithm and discusses iterative versions of
both decimation-in-frequency (DIF) and decimation-in-time (DIT)
algorithms. Additionally, it briefly mentions the Walsh-Paley transform
as an alternative basis ordering for the Walsh Transform.</p>
<p>The text discusses the Sequency-ordered Walsh transforms, focusing on
two functions to compute the k-th basis function of the transform:
walsh_wal_basis() and walsh_wak().</p>
<ol type="1">
<li><p><strong>Walsh-Paley Basis (walsh_pal_basis())</strong>: This
function generates a Walsh-Paley basis, which is ordered by frequency.
The sequence starts with the first element having a sequency of 0, and
each subsequent line represents an increment in sequency value.
Asterisks denote a value of +1, while blank entries represent
-1.</p></li>
<li><p><strong>Walsh-Kacmarz Basis (walsh_wal_basis())</strong>: This
function generates a Walsh-Kacmarz basis, which is sequency-ordered. The
sequency here refers to the number of sign changes in each Walsh
function. Unlike the Walsh-Paley basis, this one doesn’t increase
linearly with sequence numbers; instead, it jumps between values.
Asterisks denote +1, and blanks represent -1.</p></li>
</ol>
<p>The sequency ordering is important because in many applications,
particularly signal processing, it’s beneficial to order the basis
functions by their ‘speed’ or frequency content of the signal they
represent.</p>
<p>To achieve this ordering, a three-step process is described:</p>
<ul>
<li><code>n = (1UL&lt;&lt;ldn);</code> sets <code>n</code> to 2 raised
to the power of <code>ldn</code>. This computes the total number of
basis functions for the given depth <code>ldn</code>.</li>
<li><code>walsh_wak(f, ldn);</code> applies a Walsh-Kacmarz transform,
ordering the basis functions according to their binary
representation.</li>
<li><code>revbin_permute(f, n);</code> and
<code>inverse_gray_permute(f, n);</code> apply permutations to rearrange
the order of the transformed data into sequency-order. The first
permutation (<code>revbin_permute</code>) is a reverse bit-reversal (or
bit-reverse) permutation, which is commonly used for fast Fourier
transforms. The second permutation (<code>inverse_gray_code</code>),
inverts the Gray code ordering applied by <code>walsh_wak</code>.</li>
</ul>
<p>The sequency-ordered transform can be computed more efficiently using
the radix-2 decimation in frequency (DIF) algorithm, detailed in
<code>walsh_wal_dif2_core()</code>. This core routine is then wrapped in
a function (<code>walsh_wal</code>) that applies the necessary
permutations before and after the DIF computation.</p>
<p>An alternative ordering of sequencies, first even ascending then odd
descending, is also presented in <code>walsh_wal_rev()</code>, using a
decimation-in-time (DIT) approach instead of DIF. This alternative order
can be useful for certain applications, such as wavelet transforms.</p>
<p>In summary, the text discusses different methods to order Walsh basis
functions based on their sequency (frequency content), providing source
code and visual representations of these ordered bases. It also explains
the procedures used to achieve this ordering in the context of
computational efficiency for transforms like the Fast Walsh-Hadamard
Transform.</p>
<p>The provided text describes various forms of the Walsh Transform, a
discrete transform named after the Polish mathematician Joseph Walsh.
The Walsh Transform is used in signal processing and related fields for
analyzing functions or sequences into their frequency components. Here’s
a detailed summary and explanation:</p>
<ol type="1">
<li><strong>Reversed Sequency-Ordered Walsh Transform (Figure
23.7-B):</strong>
<ul>
<li>This transform uses basis functions with specific sequencies
(patterns of ‘1’s) as shown in the figure. The sequencies range from n/2
down to 1, where ’n’ is the length of the sequence.</li>
<li>Basis functions are represented by asterisks (*) for +1 and blank
spaces for -1.</li>
<li>The transform can be computed using three methods: a combination of
reverse binary permutation (revbin_permute), Gray code permutation
(gray_permute), and Walsh-Wak (walsh_wak).</li>
</ul></li>
<li><strong>Self-Inverse Walsh Transform with Sequencies n/2 or n/2 - 1
(Figure 23.7-C):</strong>
<ul>
<li>This transform’s basis functions have sequencies of either n/2 or
n/2 - 1 at even and odd indices, respectively.</li>
<li>It is self-inverse, meaning applying the transform twice to a
function yields the original function.</li>
<li>The transform can be computed using two methods: first by applying
Gray code permutation (grs_negate), reverse binary permutation
(revbin_permute), and Walsh-Gray (walsh_gray), then reversing these
operations; or vice versa.</li>
</ul></li>
<li><strong>Walsh Transform with Sequencies n/2 for Even Indices, n/2 -
1 for Odd Indices (Figure 23.7-E):</strong>
<ul>
<li>Similar to the previous variant but swaps even and odd indices’
sequency types.</li>
<li>This transform can also be computed using two methods: first
applying reverse binary permutation, negating the sequence, Walsh-Gray,
then reversing these operations; or vice versa.</li>
</ul></li>
</ol>
<p>The provided C++ code snippets (using templates for type generality)
define functions to compute these variants of the Walsh Transform. The
basis function computations use bitwise operations and logical functions
like ‘parity’ and ‘revbin’, with the gray_code function generating Gray
codes, which are used in permutations.</p>
<p>These transforms have applications in areas like signal processing,
error-correcting codes, and data compression due to their fast
computation (O(n log n) using a radix-2 decimation-in-time Fast Walsh
Transform algorithm) and ability to represent signals in frequency space
efficiently.</p>
<p>The provided code snippets are related to various transform
algorithms used in digital signal processing, specifically focusing on
the Walsh Transform and its relatives. Here’s a detailed explanation of
each section:</p>
<ol type="1">
<li><p><strong>Walsh-Hadamard Transform (WHT)</strong>: The first part
discusses two variations of the Walsh-Hadamard Transform (WHT), which is
a type of orthogonal transform named after Joseph L. Walsh and Marshall
H. Stone (Hadamard).</p>
<ul>
<li><p><strong>walsh_q2_basis()</strong>: This function computes the
Walsh-Q transform, also known as the dyadic convolution. It takes an
array <code>f</code>, its length <code>n</code>, and an index
<code>k</code> as input. The function uses a semi-symbolic scheme
(Figure 23.8-A) to calculate the result. The key steps include
calculating <code>qk</code> using <code>grs_negative_q(k)</code>,
reversing and gray coding the index <code>k</code>, and then iterating
over <code>i</code> from 0 to <code>n-1</code>. For each <code>i</code>,
it calculates <code>x</code> as the XOR of <code>i</code> and
<code>k</code>, applies parity function, calculates <code>qi</code>,
performs an XOR operation with <code>qk</code> and <code>qi</code>, and
finally assigns <code>-1</code> or <code>+1</code> to <code>f[i]</code>
based on whether <code>x</code> is zero.</p></li>
<li><p><strong>dyadic_convolution()</strong>: This function computes the
dyadic convolution using a fast algorithm involving Walsh Transform. It
takes two arrays <code>f</code> and <code>g</code>, along with their
length <code>ldn</code>. The function performs Walsh Transforms on both
input arrays, multiplies corresponding elements, and then applies an
inverse Walsh Transform to obtain the result in array
<code>g</code>.</p></li>
</ul></li>
<li><p><strong>Slant Transform</strong>: This is a variant of the
discrete Fourier transform (DFT) or the fast Fourier transform (FFT). It
involves pre- and post-processing steps in addition to a Walsh
Transform.</p>
<ul>
<li><p><strong>slant()</strong>: The slant function performs the slant
transform on an input array <code>f</code> with length <code>ldn</code>.
After applying a Walsh Transform, it applies a series of operations
(inverse gray permutation, unzip_rev(), and revbin_permute()) to
rearrange the elements.</p></li>
<li><p><strong>inverse_slant()</strong>: This function computes the
inverse slant transform. It involves post-processing steps in reverse
order: <code>revbin_permute()</code>, <code>zip_rev()</code>, and
<code>gray_permute()</code>, followed by a Walsh Transform.</p></li>
</ul></li>
<li><p><strong>Arithmetic Transform (Y+ and Y-)</strong>: These are two
forms of an arithmetic transform, which are mutually inverse. Their
basis functions are shown in Figure 23.10-A.</p>
<ul>
<li><strong>arith_transform_plus()</strong> and
<strong>arith_transform_minus()</strong>: These functions compute the
positive and negative signs of the arithmetic transform, respectively.
They use a radix-2 decimation-in-frequency (DIF) algorithm similar to
the WHT but with different operations in the inner loop. The length-2
transforms are also provided for both Y+ and Y-.</li>
</ul></li>
</ol>
<p>In summary, these code snippets demonstrate various transform
algorithms used for different purposes, such as convolution, slanting
data, and arithmetic transformations. They showcase how these transforms
can be implemented efficiently using techniques like fast algorithms
(Walsh Transform) and permutation operations. Each transform has its
specific use case depending on the application’s requirements.</p>
<p>The text discusses several discrete transforms, including the
Walsh-Hadamard (W), arithmetic (Y), Reed-Muller (R), and reversed
Reed-Muller (E) transforms. These transforms are self-inverse and have
basis functions that can be visualized as matrices or bit arrays.</p>
<ol type="1">
<li><p><strong>Walsh-Hadamard Transform (W):</strong> This transform is
defined using addition operations (+). The basis functions for Wn are
obtained by the Kronecker product of smaller W2 matrices. The inverse Wn
is also the transpose of Wn.</p></li>
<li><p><strong>Arithmetic Transform (Y):</strong> Similar to the
Walsh-Hadamard transform, but with subtraction (-) instead of addition.
The basis functions of Yn are identical to those of Wn. The self-inverse
property of Yn can be verified by checking that Yn * Yn = I (identity
matrix).</p></li>
<li><p><strong>Reed-Muller Transform (R):</strong> Derived from the
arithmetic transform by replacing addition and subtraction with XOR
operations. The Reed-Muller transform is also self-inverse, and its
basis functions are identical to those of Y+.</p></li>
<li><p><strong>Reverse Reed-Muller Transform (E):</strong> Another name
for the Reed-Muller transform when it’s viewed as a self-inverse
operation. Its basis functions are the same as R.</p></li>
</ol>
<p>The text also introduces two convolutions: OR-convolution and
AND-convolution, which operate on binary sequences using bitwise
operations. The OR-convolution hτ of sequences a and b is defined as hτ
= ∑(i∨j=τ ai * bj), where ∨ denotes bitwise OR. The AND-convolution
follows a similar definition but uses bitwise AND instead.</p>
<p>Fast algorithms for computing these convolutions are provided, along
with explanations of their implementations in code snippets. These
transforms and convolutions find applications in various areas of
computer science and engineering, including signal processing, coding
theory, and cryptography.</p>
<p>The text discusses several types of convolutions related to
sequences, focusing on the Fast Walsh-Hadamon Transform (FWHT), which is
an efficient method for computing these convolutions. Here’s a detailed
summary:</p>
<ol type="1">
<li><p><strong>OR-convolution</strong>: This operation involves
element-wise bitwise OR between two sequences and then applying a Fast
Walsh-Hadamard Transform (FWHT). The key relation used in its
computation is h = Y - * (Y +[a] * Y +[b]), where Y + and Y - denote the
arithmetic transforms. The FWHT implementation for OR-convolution is
provided, with an overall time complexity of O(n log n).</p></li>
<li><p><strong>AND-convolution</strong>: This operation involves
element-wise bitwise AND between two sequences, followed by applying a
reversed arithmetic transform. The key relation used in its computation
is h = B - * (B +[a] * B +[b]), where B + and B - denote the reversed
arithmetic transforms. Similar to OR-convolution, the FWHT
implementation for AND-convolution is given, with an overall time
complexity of O(n log n).</p></li>
<li><p><strong>MAX-convolution</strong>: This operation involves finding
the maximum value between pairs of indices in two sequences and then
performing an element-wise multiplication before summing up these
products. The naive implementation has a quadratic time complexity
(O(n^2)). However, it’s shown that this convolution can be computed
linearly (O(n)) using cumulative sums: h[k] = f[k]<em>g[k] + sf</em>g[k]
+ sg*f[k], where sf and sg are cumulative sums of f and g
respectively.</p></li>
<li><p><strong>Weighted Arithmetic Transform</strong>: This is a
generalization of the Walsh-Hadamard transform, where each element in
the transformed sequence is a weighted sum of the original elements
based on bitwise relationships. The basis functions for this transform
are shown in Figure 23.14-A, with weights depending on the number of
ones (binary expansion) in the indices.</p></li>
<li><p><strong>Subset Convolution</strong>: This operation involves
finding pairs of indices where the binary OR equals a given index and
the binary AND equals zero. It’s computationally intensive due to many
missing products compared to other convolutions. A method with time
complexity O(n (log n)^2) is presented, which computes a weighted
version of this convolution first, then extracts the desired result via
a series of operations.</p></li>
</ol>
<p>In summary, the Fast Walsh-Hadamard Transform (FWHT) is a powerful
tool for efficiently computing various types of sequence convolutions by
exploiting properties of binary representations and symmetries in the
operations involved. The methods presented offer improvements over naive
implementations, reducing time complexities from quadratic to linear or
logarithmic, depending on the specific operation.</p>
<p>The Haar Transform is a type of discrete wavelet transform that does
not involve trigonometric functions. It’s an invertible transformation
used to decompose signals or data into different frequency components.
The standard Haar Transform works on sequences (or arrays) whose length
is a power of 2, decomposing the signal into approximation and detail
coefficients at each level of the decomposition.</p>
<ol type="1">
<li><p><strong>Standard Haar Transform</strong>:</p>
<ul>
<li>The transform consists of log2(n) steps where n is the length of the
sequence.</li>
<li>In each step, adjacent pairs of elements in the sequence are summed
(for approximations) or differenced (for detail coefficients).</li>
<li>Sums go to the lower half of the array, and differences go to the
upper half.</li>
<li>The process repeats for successively halved segments of the array
until individual elements remain.</li>
</ul>
<p>Mathematically, if <code>f</code> is the input sequence:</p>
<pre><code>For i = 0 to log2(n)-1
  For j = 0 to n/2^i - 1
     A[j] = Sum from k=0 to 1 of f[2j+k] * W[i, k]
  End for
  For j = n/2^i to n-1
     D[j] = Diff from k=0 to 1 of f[2j+k] * W[i, k]
  End for
Next i</code></pre>
<p>Here, <code>A</code> represents the approximation coefficients and
<code>D</code> represents the detail coefficients. <code>W[i,k]</code>
are the transformation matrices (for standard Haar, they’re simple
±1).</p></li>
<li><p><strong>Orthogonality</strong>:</p>
<ul>
<li>The standard Haar transform can be made orthogonal by scaling the
transformation matrices appropriately (usually by √2). This means that
the inverse transform can be directly obtained by multiplying with the
transpose of the forward transform matrix.</li>
</ul></li>
<li><p><strong>In-place Variants</strong>:</p>
<ul>
<li>Standard Haar Transform requires additional memory for temporary
storage of intermediate results. In-place variants aim to perform the
transform without this extra space, typically at the cost of increased
computational complexity.</li>
</ul></li>
<li><p><strong>Applications</strong>:</p>
<ul>
<li>The Haar Transform is fundamental in wavelet theory and has
applications in signal processing, image compression (like JPEG 2000),
data analysis, and more due to its ability to capture both frequency and
spatial information effectively.</li>
</ul></li>
</ol>
<p>The provided code snippets implement the standard Haar transform in
C++. The first version allocates extra memory for intermediate results,
while the second version uses an optional workspace parameter supplied
by the caller to reduce memory usage, making it an in-place variant. The
inverse transform is similarly implemented but in reverse order.</p>
<p>The provided text discusses various versions of the Haar Transform, a
mathematical operation used in signal processing and data compression.
The Haar Transform decomposes a function or signal into a sum of a
series of orthonormal basis functions, which are step-like functions in
the case of the Haar Transform.</p>
<ol type="1">
<li><p><strong>Standard (Non-Inplace) Haar Transform</strong>: This is
the basic version where a temporary storage array (<code>g</code>) is
used to store intermediate results during the transformation process.
It’s defined by the following operations:</p>
<ul>
<li><code>x = f[k]</code> (assigning the value at index k in array
<code>f</code> to variable x)</li>
<li><code>y = f[mh+k] * v</code> (assigning the product of the value at
index mh+k in array <code>f</code> and scalar v to y)</li>
<li><code>g[j] = x + y</code> (storing the sum of x and y into g at
index j)</li>
<li><code>g[j+1] = x - y</code> (storing the difference of x and y into
g at index j+1)</li>
</ul></li>
<li><p><strong>In-place Haar Transform</strong>: This version performs
the transformation without requiring additional storage, thus saving
memory. It achieves this by cleverly rearranging (or permuting) the data
in place before and after each level of the transform. The permutation
is handled by two routines: <code>haar_permute()</code> and its inverse
<code>inverse_haar_permute()</code>. The actual transform is performed
by <code>haar_inplace()</code>, which is equivalent to the sequence
<code>haar_inplace(); haar_permute();</code>.</p></li>
<li><p><strong>Non-normalized Haar Transform</strong>: This version does
not normalize the basis functions, meaning the nonzero entries have
different absolute values compared to the normalized version. The
forward transform (<code>haar_nn()</code>) and its inverse
(<code>inverse_haar_nn()</code>) are straightforward implementations
without normalization factors. An in-place version of this transform is
also provided (<code>haar_inplace_nn()</code>), along with its inverse
(<code>inverse_haar_inplace_nn()</code>).</p></li>
<li><p><strong>Transposed Haar Transform</strong>: This version involves
a different set of basis functions (shown in Figure 24.4-A). Despite the
change in basis functions, an unnormalized transposed Haar transform can
be performed using routines <code>transposed_haar_nn()</code> and its
inverse <code>inverse_transposed_haar_nn()</code>.</p></li>
</ol>
<p>Each of these versions has its use cases depending on the specific
requirements of a task, such as memory constraints or computational
efficiency. The text also mentions relationships between different
transformations via permutations (<code>PH</code>), which can be
programmed using routines like <code>haar_permute()</code> and
<code>inverse_haar_permute()</code>.</p>
<p>The provided text discusses various aspects of the Haar Transform,
focusing on its reversed and transposed versions, as well as their
relations with Walsh Transforms. Here’s a detailed summary:</p>
<ol type="1">
<li><strong>Reversed Haar Transform (H_rev)</strong>:
<ul>
<li>Basis functions are depicted in Figure 24.5-A. They resemble those
of the original Haar Transform but are reversed.</li>
<li>The routine <code>haar_rev_nn</code> computes this transform
in-place without using a temporary array.</li>
<li>It’s based on a radix-2 DIF (Difference-in-Few) implementation
similar to Walsh transforms, with modifications to handle the reverse
operation.</li>
</ul></li>
<li><strong>Transposed Reversed Haar Transform (Ht_rev)</strong>:
<ul>
<li>This is essentially the inverse of the reversed Haar Transform
(<code>haar_rev_nn</code>). It’s computed via
<code>transposed_haar_rev_nn</code>.</li>
<li>The routine follows a similar radix-2 DIT (Difference-in-Twos)
structure as Walsh transforms but with the necessary modifications for
inversion.</li>
</ul></li>
<li><strong>Relating Haar and Walsh Transforms</strong>:
<ul>
<li><strong>Algorithm WH1 and WH1T</strong>: These show how to compute a
Walsh transform using one length-n Haar Transform (reversed), followed
by multiple smaller Haar Transforms (reversed). The difference lies in
whether these smaller transforms are applied before or after the larger
ones.</li>
<li><strong>Algorithm WH2 and WH2T</strong>: Similar, but here the Walsh
Transform is computed via the inverse of the reversed Haar
Transform.</li>
</ul></li>
<li><strong>Relating Walsh and Haar Transforms (via algorithms HW1,
HW1I, HW2, and HW2I)</strong>:
<ul>
<li>These algorithms illustrate how to compute Haar Transforms using
Walsh Transforms. They demonstrate that certain Walsh Transforms can be
used as building blocks for Haar Transforms, although this method is not
efficient for computing the Haar Transform directly due to its O(n log
n) complexity compared to the O(n) complexity of direct Haar Transform
computation.</li>
</ul></li>
<li><strong>Prefix Transform and Prefix Convolution</strong>:
<ul>
<li>The text concludes by introducing the concept of the prefix
transform and prefix convolution, which are likely related to discrete
signal processing operations (like convolution). However, specific
details about these concepts aren’t provided in the given excerpt.</li>
</ul></li>
</ol>
<p>The Haar and Walsh transforms are integral parts of signal
processing, data compression, and image analysis, with their basis
functions playing a crucial role in understanding and manipulating
signals at different scales or frequency bands.</p>
<p>The text discusses various aspects of the Hartley transform, a
trigonometric transform that maps real data to real data. Here’s a
detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Definition and Symmetries</strong>: The discrete Hartley
Transform (H[a]) of a length-n sequence ‘a’ is defined as ck = (1/√n) *
Σ(ax * [cos(2πkx/n) + sin(2πkx/n)]), where k ranges from 0 to n-1. It’s
similar to the Discrete Fourier Transform but uses cosine and sine
instead of cosine and imaginary sine. The Hartley transform of a real
sequence remains real, and it is its own inverse (H[H[a]] = a). It also
preserves symmetry properties: symmetric sequences yield symmetric
transforms, and anti-symmetric sequences yield anti-symmetric
transforms.</p></li>
<li><p><strong>Fast Hartley Transform (FHT)</strong>: An efficient
algorithm for computing the Hartley transform in O(n log n) time is
called the Fast Hartley Transform (FHT).</p></li>
<li><p><strong>Radix-2 FHT Algorithms - Decimation in Time
(DIT)</strong>: The radix-2 DIT FHT involves recursively splitting the
input sequence into even and odd indexed elements, applying the
transform to each half, and then combining the results using a specific
operator X 1/2. This operator is equivalent to S1/2 in Fourier Transform
algorithms.</p>
<ul>
<li><p><strong>Pseudocode for Recursive Radix-2 DIT FHT</strong>: The
provided pseudocode outlines the steps for computing the FHT recursively
using DIT. It involves creating temporary arrays for even and odd
indexed elements, applying the transform to each half, and then
combining the results using the hartley_shift function.</p></li>
<li><p><strong>hartley_shift Function</strong>: This function implements
the X 1/2 operator, which transforms each element ck of the input
sequence c by ck cos(πk/n) + cn-k sin(πk/n).</p></li>
</ul></li>
<li><p><strong>Non-recursive Radix-2 DIT FHT</strong>: An alternative
approach to compute the FHT non-recursively is also outlined in the
text, although specific pseudocode isn’t provided. This method would
involve a depth-first traversal of the binary tree representation of the
sequence length, applying the transform at each node, and combining
results similarly to the recursive case.</p></li>
<li><p><strong>Optimization of hartley_shift</strong>: An optimized
version of the hartley_shift function is presented, which leverages
symmetry properties of trigonometric functions for improved efficiency.
This version uses precomputed constants and parallel assignments to
reduce computational overhead.</p></li>
</ol>
<p>In summary, this text introduces the Hartley transform, its
properties, and efficient algorithms (FHT) for its computation using
radix-2 decimation in time (DIT). It also provides pseudocode for both
recursive and non-recursive implementations of the DIT FHT and discusses
an optimized version of a crucial function (hartley_shift) involved in
these algorithms.</p>
<p>The provided text discusses several algorithms related to the Hartley
Transform (HT), which is a discrete Fourier-like transform, but uses
real coefficients. The text covers both the Decimation in Time (DIT) and
Decimation in Frequency (DIF) methods for computing the FHT (Fast
Hartley Transform).</p>
<ol type="1">
<li><p><strong>Radix-2 DIT FHT Algorithm</strong>: This algorithm begins
by reordering (permuting) the input data, then recursively divides the
transform size into halves until a base case is reached (a size of 1),
and finally combines the results using a sum/diff operation. The base
case computes the Hartley transform for a single element
directly.</p></li>
<li><p><strong>Radix-2 DIF FHT Algorithm</strong>: This algorithm also
starts with data reordering but uses a different approach to compute the
transform. It performs a series of sum and difference operations
followed by a Hartley shift, which is a rotation in the frequency
domain. The process is recursive, reducing the size of the problem at
each step until it reaches the base case.</p></li>
<li><p><strong>Complex FFT by FHT</strong>: The relationship between
Complex Fourier Transform (CFT) and Hartley Transform (HT) is
established. It’s shown that a complex CFT can be computed using two
separate real HTs, or vice versa. This conversion involves rearranging
and scaling the input data according to specific rules.</p></li>
<li><p><strong>Real FFT by FHT</strong>: A method for computing Real
Fast Fourier Transform (FFT) via Hartley Transform is detailed. The
process involves transforming the real sequence into its Hartley
representation, then applying a post-processing step to obtain the
desired real FFT result. The inverse operation, transforming an FFT back
to a real sequence using FHT, is also described.</p></li>
<li><p><strong>Higher Radix FHT Algorithms</strong>: The text mentions
that higher radix versions of FHT can be derived from existing FFT
algorithms by wrapping the FFT steps with conversion operators (T). This
method inherently takes advantage of trigonometric factor symmetry.
However, detailed splitting methods for specific radices like 4 or
split-radix are referenced to other works.</p></li>
<li><p><strong>Convolution via FHT</strong>: The convolution property of
Hartley Transform is used to create an algorithm for cyclic convolution
of two real sequences. This method leverages the efficiency of FHT by
performing multiplication and addition in a transformed domain, reducing
computational complexity compared to direct time-domain
convolution.</p></li>
</ol>
<p>In summary, these algorithms provide efficient methods for computing
various types of discrete Fourier transforms (DFT, FFT, CFT) using the
Hartley Transform. They achieve this by leveraging symmetry properties
and recurrence relations inherent to these transforms, leading to
significant computational savings compared to direct
implementations.</p>
<p>The given text discusses various aspects of the Fast Hartley
Transform (FHT), a computational method related to the Discrete Fourier
Transform (DFT). Here’s a detailed summary and explanation of the key
points:</p>
<ol type="1">
<li><p><strong>Cyclic Convolution using FHT</strong>: The cyclic
convolution of two real sequences can be computed using FHT, as shown in
the pseudocode provided. This involves transforming both input
sequences, performing element-wise multiplication in the transformed
domain (convolution), and then transforming back to the original domain.
Normalization is applied at the end.</p></li>
<li><p><strong>C++ Implementations</strong>: The text presents C++
implementations for cyclic convolution (<code>fht_convolution</code>)
and self-convolution (<code>fht_auto_convolution</code>). These
implementations use auxiliary functions like <code>fht_mul</code> and
<code>fht_sqr</code> to handle the element-wise multiplication in the
transformed domain.</p></li>
<li><p><strong>Avoiding Revbin Permutations</strong>: The text mentions
that, similar to FFT-based convolutions, revbin permutations can be
omitted with FHT-based convolutions for improved performance. This is
achieved by accessing data in reverse binary order during computation
and post-processing.</p></li>
<li><p><strong>Negacyclic Convolution via FHT</strong>: A pseudocode is
provided for the negacyclic auto-convolution using FHT. This involves
preprocessing (Hartley shift), transforming data, performing convolution
in the transformed domain, transforming back, and postprocessing
(another Hartley shift). C++ implementations for this are given as
well.</p></li>
<li><p><strong>Localized FHT Algorithms</strong>: The text introduces
localized versions of the FHT, which aim to improve cache performance by
processing smaller chunks of data at a time. Two versions are presented:
Decimation in Time (DIT) and Decimation in Frequency (DIF). These
algorithms use recursion to handle larger lengths by breaking them down
into smaller, manageable subproblems.</p></li>
<li><p><strong>2-dimensional FHTs</strong>: The text explains how to
compute 2-dimensional FHTs using a row-column algorithm similar to the
2D FFT. Post-processing is required to transform the row-column FHT
result into a true 2D FHT.</p></li>
<li><p><strong>Automatic Generation of Transform Code</strong>: The text
discusses the concept of generating FFT and FHT code automatically. This
involves creating a program that reads existing FFT/FHT code as input
and generates optimized, localized versions tailored for specific
hardware characteristics (like cache size). The process includes partial
evaluation, where current loop variable values are printed as comments
to help identify corresponding parts in the generated code.</p></li>
<li><p><strong>Eigenvectors of Fourier and Hartley Transforms</strong>:
The text briefly mentions eigenvectors of both Fourier and Hartley
transforms. It provides a relationship between a sequence’s symmetric
part (aS) and its Fourier transform, which could be useful in
understanding the properties of these transforms.</p></li>
</ol>
<p>In summary, the text covers various aspects of the Fast Hartley
Transform, including efficient convolution algorithms, C++
implementations, optimization techniques like avoiding revbin
permutations, 2D FHT computation, and automated code generation for
improved performance tailored to specific hardware characteristics. It
also briefly touches on eigenvectors of Fourier and Hartley
transforms.</p>
<p>The text discusses Number Theoretic Transforms (NTTs), a variant of
the Fast Fourier Transform (FFT) that operates over finite fields,
specifically modular arithmetic. NTTs are used for exact computations
without rounding errors, making them suitable for high-precision
applications like multiplication algorithms.</p>
<p><strong>Key Points:</strong></p>
<ol type="1">
<li><p><strong>Prime Moduli</strong>: The choice of a prime modulus m is
crucial for implementing NTTs. A primitive n-th root of unity (r) exists
in Z/mZ if and only if n divides the maximal order R = p - 1, where p is
the prime. This means that suitable primes are of the form p = v*n +
1.</p></li>
<li><p><strong>Conditions for NTTs</strong>: Two conditions must be
met:</p>
<ul>
<li>The transform length n must divide the maximal order R (condition
26.1-1).</li>
<li>The modulus m must be coprime to the transform length n (condition
26.1-2). This is automatically satisfied if m is prime, as all primes
are coprime to any integer less than themselves.</li>
</ul></li>
<li><p><strong>Implementation</strong>: NTTs can be implemented using
similar algorithms as FFTs but with modular arithmetic replacing complex
arithmetic. The text provides pseudocode and C++ implementations for
Radix-2 Decimation in Time (DIT) and Decimation in Frequency (DIF) NTTs,
as well as Radix-4 versions of these transforms.</p></li>
<li><p><strong>Convolution with NTTs</strong>: One significant
application of NTTs is the computation of exact integer convolutions,
which are essential in high-precision multiplication algorithms. When
dealing with large numbers that don’t fit into a single machine word,
it’s recommended to use multiple smaller primes and apply the Chinese
Remainder Theorem to combine results.</p></li>
</ol>
<p><strong>Pseudocode Examples</strong>:</p>
<ul>
<li>Radix-2 DIT NTT:
<ul>
<li>Pseudocode involves finding a primitive n-th root of unity (rn),
performing a bit reversal permutation, then iterating over decreasing
powers of 2 (ldm) to perform butterfly operations.</li>
</ul></li>
<li>Radix-2 DIF NTT:
<ul>
<li>Similar to DIT but in reverse order, starting from the highest power
of 2 and moving downwards. The final step involves a bit reversal
permutation.</li>
</ul></li>
<li>Radix-4 NTTs:
<ul>
<li>These are extensions of the Radix-2 transforms, involving additional
steps for handling four elements at a time. Both DIT and DIF versions
are provided.</li>
</ul></li>
</ul>
<p>The text also includes examples of suitable prime numbers for various
transform lengths, generated using a program called
<code>FXT: mod/fftprimes-demo.cc</code>. These primes are selected to
allow transforms of lengths that divide specific numbers (e.g., 2^44,
2^40, etc.), ensuring the existence of primitive roots required for
NTTs.</p>
<p>The Karatsuba algorithm is an efficient method for multiplying large
numbers, reducing the number of operations from O(N^2) (as in
traditional multiplication) to O(N^log_2(3)) ≈ O(N^1.585). This speedup
is achieved by splitting each input number into two parts and
recursively applying the Karatsuba formula.</p>
<p>Here’s a step-by-step explanation of the 2-way splitting (Karatsuba)
algorithm:</p>
<ol type="1">
<li><p>Divide both input numbers A and B into two parts, such that their
lengths are approximately equal:</p>
<ul>
<li>A = x * a_high + a_low</li>
<li>B = x * b_high + b_low</li>
</ul>
<p>Here, ‘x’ is chosen to be the square root of the base (e.g., 10 for
decimal numbers) rounded to the nearest integer.</p></li>
<li><p>Compute three intermediate products:</p>
<ul>
<li>P1 = a_low * b_low</li>
<li>P2 = x * (a_low * b_high + a_high * b_low)</li>
<li>P3 = x^2 * a_high * b_high</li>
</ul></li>
<li><p>Combine the intermediate results to obtain the final product
using the following formula: A * B = P1 + x * P2 + P3</p></li>
</ol>
<p>This 2-way splitting scheme reduces the number of multiplications
required from four (in traditional multiplication) to three, thus
improving efficiency for large numbers. The algorithm can be further
optimized by choosing an optimal value for ‘x’ that minimizes the number
of operations.</p>
<p>The Karatsuba algorithm has a time complexity of O(N^log_2(3)) ≈
O(N^1.585), making it more efficient than traditional multiplication for
sufficiently large input sizes. It forms the basis for more advanced
multiplication algorithms like Toom-Cook and FFT-based methods.</p>
<p>The text discusses fast multiplication algorithms, focusing on
splitting schemes and Fast Fourier Transform (FFT) based methods.</p>
<p><strong>Splitting Schemes for Multiplication:</strong></p>
<ol type="1">
<li><p><strong>Two-way Splitting (Karatsuba Algorithm):</strong> This
method reduces the number of multiplications needed to compute the
product of two numbers by recursively splitting them into half-precision
parts. The Karatsuba algorithm requires three multiplications of half
precision for one full precision multiplication, yielding a time
complexity of O(N^1.585).</p>
<p>Formula: A * B = (1 + x) * a0 * b0 + x * (a1 - a0) * (b0 - b1) + (x +
x^2) * a1 * b1 Here, ‘x’ is a power of the radix.</p></li>
<li><p><strong>Three-way Splitting:</strong> This scheme splits each
number into three parts rather than two, further reducing the number of
multiplications needed. Two notable three-way splitting methods are:</p>
<ul>
<li><strong>Zimmermann’s 3-way multiplication:</strong> This scheme
requires five multiplications of length N/3 and has a time complexity of
approximately O(N^1.465).</li>
<li><strong>Bodrato and Zanoni’s 3-way multiplication:</strong> This
method also needs five multiplications of length N/3, but it only
involves one division by 2 instead of two divisions by 3 found in
Zimmermann’s scheme. Its time complexity is similar to Zimmermann’s,
approximately O(N^1.465).</li>
</ul></li>
<li><p><strong>Four-way and Five-way Splitting:</strong> These methods
extend the splitting concept to four or five parts, respectively,
providing even more efficient multiplication algorithms with higher
complexity (O(N log_4(7) ≈ N^1.403) for 4-way and O(N log_5(7) ≈
N^1.3928) for 5-way).</p></li>
</ol>
<p><strong>Fast Multiplication via FFT:</strong></p>
<p>The Fast Fourier Transform (FFT)-based algorithm is another efficient
method to multiply two numbers represented in a given radix ‘R’. This
approach treats the multiplication of integers as a polynomial
multiplication, followed by linear convolution and carry operations.</p>
<ol type="1">
<li><p><strong>Numbers as Almost Polynomials:</strong> An N-digit
integer A can be viewed as a polynomial of degree (N - 1) with
coefficients representing its digits in radix R. The product of two such
numbers is nearly their polynomial product.</p></li>
<li><p><strong>Polynomial Multiplication as Linear Convolution:</strong>
The coefficients ck of the resulting polynomial can be computed via
linear convolution, which involves multiplying sequences A and B at
various points (roots of unity). This process transforms both sequences
using FFTs, multiplies them element-wise, and then inverse transforms to
obtain the polynomial C.</p></li>
<li><p><strong>Complexity and Radix/Precision Considerations:</strong>
The FFT-based algorithm has a time complexity of O(N log N) due to the
two FFT operations (for forward and inverse transformations). However,
it requires at least 2N - 2 points for evaluation (zero-padding), which
limits the radix R based on the desired precision. For large precisions,
the cumulative sums (ck) must be representable as integer numbers with
the data type used for the FFTs, imposing restrictions on the maximum
value L that can appear in the product.</p></li>
</ol>
<p>In summary, both splitting schemes and FFT-based multiplication
methods significantly reduce the computational complexity of multiplying
two large integers by dividing them into smaller parts or treating them
as polynomials. These techniques are essential in developing fast
algorithms for high-precision arithmetic, cryptography, and numerical
computations involving large numbers.</p>
<p>Title: Division, Square Root, and Cube Root - Root Extraction
Methods</p>
<ol type="1">
<li><p><strong>Inverse and Division</strong>: The standard division
operation is computationally expensive for high-precision numbers.
Instead, the inverse of a divisor (d) is computed using Newton’s method,
also known as the Newton-Raphson iteration:</p>
<p>x_(k+1) = x_k + x_k * (1 - d * x_k)</p>
<p>This iterative process starts with an initial approximation (x0 ≈
1/d), and each subsequent step doubles the number of accurate digits.
The multiplication operation in this method requires only half the
precision of the current value, making it efficient.</p></li>
<li><p><strong>Convergence and Precision</strong>: The convergence rate
of this method is quadratic, meaning that with each iteration, the
number of correct digits approximately doubles. This results in a total
computational cost less than three full-precision multiplications,
including the final multiplication to obtain the quotient
(a/d).</p></li>
<li><p><strong>Third-Order Correction</strong>: For higher precision
requirements, a third-order correction can be applied:</p>
<p>x_(k+1) = x_k + x_k * (1 - d * x_k) + x_k * (1 - d * x_k)^2</p>
<p>This ensures maximum precision in the final result.</p></li>
<li><p><strong>Long and Short Divisions</strong>: If both operands have
equal precision, it is called a long division; if one operand fits
within a machine word, it’s known as short division. Similarly,
multiplication with full-precision numbers is referred to as long
multiplication, while short multiplication occurs when one operand fits
into a machine word.</p></li>
<li><p><strong>Example</strong>: The given figure (29.1-A) illustrates
the first few iterations of computing the inverse of 3.1415926 starting
from an initial two-digit approximation (0.31). Each step improves the
precision until the desired level is achieved.</p></li>
</ol>
<p>Title: Initial Approximations for Iterations in High-Precision
Arithmetic</p>
<p>In high-precision arithmetic, direct conversion to machine
floating-point numbers and subsequent use of the Floating-Point Unit
(FPU) may not always be feasible due to limitations in representable
values. This section discusses methods for generating initial
approximations suitable for iterative computations without relying on
FPU operations.</p>
<ol type="1">
<li><p><strong>Taylor Series Expansion:</strong></p>
<p>The first method involves expanding the target function around a
nearby point where a high-precision approximation is known or can be
computed easily. For instance, consider computing √d with d being a
large number:</p>
<ul>
<li>Step 1: Obtain an initial guess x0 for √d by using simpler
approximations like Newton’s method or other suitable techniques.</li>
<li>Step 2: Use the Taylor series expansion of f(x) = (√x - x0)^2 around
x0: <span class="math display">\[f(x) \approx \frac{1}{4x_0} (x -
x_0)^2\]</span></li>
<li>Step 3: Solve for √d: <span class="math display">\[\sqrt{d} \approx
x_0 + \frac{(d - x_0^2)}{2x_0}\]</span></li>
</ul>
<p>This method is simple and can be extended to other functions by
adjusting the Taylor series expansion accordingly.</p></li>
<li><p><strong>Pade Approximants:</strong></p>
<p>Padé approximants are rational functions that provide increasingly
accurate approximations of a given function around a specific point.
They have the advantage of being more efficient than traditional Taylor
series expansions for certain functions, especially when dealing with
large arguments or high-precision arithmetic.</p>
<ul>
<li>Step 1: Identify the Padé approximant [m,n] for f(x) around x0,
where m and n are integers specifying the orders of the numerator and
denominator polynomials, respectively.</li>
<li>Step 2: Evaluate the approximant at d to obtain an initial
approximation for √d: <span class="math display">\[\sqrt{d} \approx
\frac{\text{Numerator}[m,n](d)}{\text{Denominator}[m,n](d)}\]</span></li>
</ul>
<p>The choice of [m, n] depends on the desired balance between accuracy
and computational complexity. Higher-order Padé approximants generally
yield better precision but at the cost of increased complexity.</p></li>
<li><p><strong>Continued Fractions:</strong></p>
<p>Continued fractions offer another approach for generating initial
approximations in high-precision arithmetic. They provide increasingly
accurate rational approximations to irrational numbers, making them
suitable for computing square roots and other functions.</p>
<ul>
<li>Step 1: Generate the continued fraction representation of √d using
well-known algorithms like the Gauss-Legendre algorithm or the modified
Lagrange inversion formula.</li>
<li>Step 2: Truncate the continued fraction at a suitable depth to
obtain an initial approximation for √d as a rational number.</li>
</ul>
<p>This method is particularly useful when dealing with transcendental
functions and irrational numbers, providing rapid convergence even for
large arguments.</p></li>
</ol>
<p>In summary, these methods for generating initial approximations in
high-precision arithmetic aim to bypass limitations associated with
machine floating-point representations. By leveraging Taylor series
expansions, Padé approximants, or continued fractions, one can
efficiently obtain accurate starting values for iterative computations
without relying on FPU operations. The choice of method depends on the
target function, desired accuracy, and available computational
resources.</p>
<p>The text discusses various mathematical techniques for root
extraction, focusing on inverse roots, the exponential function, and
applications of matrix square roots. Here’s a detailed summary and
explanation of each section:</p>
<ol type="1">
<li><p>Inverse Roots: The technique presented involves expressing a
number d in the form d = M · RX, where M is the mantissa (0 ≤ M &lt; 1),
R is the radix (base), and X is the exponent (X ∈ Z). The inverse root
d^(-1/a) can then be computed as M^(1/a) * R^(Y/a) * R^(Z), where Y = X
% a, Z = ⌊X/a⌋, and X = a*Z + Y.</p>
<p>The provided C++ code (approx_invpow) implements this technique:</p>
<ul>
<li>Line 4-5: Convert the hfloat number d to double dd for easier
computation.</li>
<li>Line 6: Compute M^(1/a) using pow(dd, 1.0/(double)a).</li>
<li>Lines 7-9: Calculate Z and Y based on the exponent of d.</li>
<li>Line 10: Compute R^(Y/a) using pow((double)d.radix(),
(double)Y/a).</li>
<li>Line 11: Multiply M^(1/a) * R^(Y/a) to get dd.</li>
<li>Lines 12-13: Convert the result back to hfloat c and adjust its
exponent.</li>
</ul></li>
<li><p>Exponential Function: For the exponential function f(d) = exp(d),
the input d is expressed as M · RX, where X = ⌊d/log(R)⌋ and M = exp(d -
X * log(R)). The initial approximation for exp(d) is computed directly
using these values.</p>
<p>The provided C++ code (approx_exp) implements this technique:</p>
<ul>
<li>Line 4-5: Convert the hfloat number d to double dd for easier
computation.</li>
<li>Lines 6-8: Calculate X and M based on the input d and the radix
R.</li>
<li>Line 9: Assign M as the initial approximation of exp(d).</li>
<li>Line 10: Adjust the exponent of c to match the computed value of
exp(d).</li>
</ul></li>
<li><p>Applications of Matrix Square Root:</p>
<ol type="a">
<li><p>Re-orthogonalization: This method is used to transform a rotation
matrix A, which has deviated from being orthogonal (due to cumulative
errors), to the closest orthogonal matrix E. The transformation uses the
inverse square root iteration with d = AT * A and x = 1/(2<em>(1 + AT
</em> A)).</p></li>
<li><p>Polar Decomposition: The polar decomposition of a matrix A is
represented as A = ER, where E is an orthogonal matrix and R is a
positive semi-definite matrix. This decomposition can be computed using
the inverse square root iteration for both E and R matrices.</p></li>
<li><p>Sign Decomposition: The sign decomposition represents a matrix A
as S * N, where S is its own inverse (eigenvalues are ±1) and N has
positive eigenvalues. This decomposition can be obtained through an
iterative process similar to the polar decomposition.</p></li>
<li><p>Pseudo-Inverse: The pseudo-inverse of a matrix A, denoted by A+,
is defined as (A^T * A)^(-1) * A^T. It exists even when A^(-1) does not
and provides the best possible solution (in a least-squares sense) for
Ax = b. The pseudo-inverse can be computed using an iteration similar to
that of matrix inversion.</p></li>
</ol></li>
</ol>
<p>These mathematical techniques are valuable in various fields, such as
numerical analysis, linear algebra, and graphics applications, for tasks
like orthogonalization, decomposition, and solving systems of linear
equations.</p>
<p>Schröder’s formula is an nth-order iteration for a simple root r of a
function f(x), given by the expression:</p>
<p>Sn(x) = x - ∑[n-1]_k=1 (−1)^k * f(x)^k / k! * (1/f’(x))^(k-1)</p>
<p>This formula provides an efficient way to compute higher-order
iterations for finding the roots of a function. It was introduced by
Ernst Schröder in 1870 and is derived using power series reversion
techniques or systematically through a recursive relation.</p>
<p>Schröder’s formula can be used to construct various higher-order
iterations, such as Newton’s method (n=2) and Halley’s method (n=3). The
recursion relation for the coefficients Un in Schröder’s formula is:</p>
<p>Un = (2n - 3) * f’(x) * U_(n-1) - f(x) * U’_n-1,</p>
<p>where U_0 = 1 and U’_n denotes the derivative of U_n with respect to
x.</p>
<p>The formula also has applications in other areas, such as finding
divisionless iterations for polynomial roots (29.7.3). Schröder’s
iteration can be seen as a generalization of Newton’s method by
including higher-order derivatives of f(x) and its inverse.</p>
<p>In summary, Schröder’s formula provides an elegant way to create
higher-order iterations for finding the roots of a function. It is
derived from power series reversion techniques and offers a flexible
framework for generating various high-order methods like Newton’s
method, Halley’s method, and others. This formula is essential in
numerical analysis and can be applied in various areas, including
polynomial root finding and other optimization problems.</p>
<p>The Complete Elliptic Integral of the First Kind, denoted as K(k), is
a fundamental function in mathematics, particularly in the study of
elliptic curves and special functions. It’s defined for the parameter k
(0 &lt; k ≤ 1) as follows:</p>
<p>K(k) = ∫_0^π/2 [1 / √(1 - k²sin²θ)] dθ</p>
<p>This integral represents the arc length of a portion of an
ellipse.</p>
<p>The AGM-based algorithm for computing K(k) is as follows:</p>
<ol type="1">
<li><p>Initialize two variables, <code>a0 = 1</code> and
<code>b0 = sqrt((1 - k) / (1 + k))</code>.</p></li>
<li><p>Perform the AGM iteration until convergence:</p>
<p>For n from 0 to a pre-determined number of iterations:</p>
<ul>
<li>Compute <code>an+1 = (an + bn) / 2</code></li>
<li>Compute <code>bn+1 = sqrt(an * bn)</code></li>
</ul></li>
<li><p>Once the iteration converges, compute <code>K(k)</code> as:</p>
<p>K(k) = π / (2 AGM(1, √((1 - k) / (1 + k))))</p></li>
</ol>
<p>Where <code>AGM</code> refers to the Arithmetic-Geometric Mean
function discussed in Section 31.1. This method has super-linear
convergence and is highly efficient for calculating K(k) with high
precision.</p>
<ol start="4" type="1">
<li><p>The value of K(k) can be extended to k &gt; 1 using the symmetry
relation:</p>
<p>K(1/k) = K(k)</p></li>
<li><p>The derivative of K(k) with respect to k can be expressed in
terms of complete elliptic integrals of the second and third kind,
denoted as E(k) and Π(n; k), respectively:</p>
<p>dK(k)/dk = (E(k) - K(k)) / (2k)</p></li>
<li><p>At k = 1, K(1) is a logarithmic singularity, meaning it
approaches infinity as k approaches 1 from below:</p>
<p>lim_(k→1^-) K(k) = ∞</p></li>
<li><p>The complementary form of the complete elliptic integral of the
first kind is given by:</p>
<p>K’(k) = (π / 2) * Σ[(−1)^n * (2n − 1)!] / [n! * (n + 1)! * (1 -
k^(2n))]</p></li>
</ol>
<p>This series converges for all k in the interval (0, 1].</p>
<p>The complete elliptic integral of the first kind, K(k), appears in
many areas of mathematics and physics, including number theory,
differential equations, and quantum mechanics. Its efficient computation
is crucial for various high-precision numerical applications.</p>
<p>The given text discusses various mathematical concepts related to
elliptic integrals, the Arithmetic-Geometric Mean (AGM), theta
functions, eta functions, and singular values. Here’s a detailed
summary:</p>
<ol type="1">
<li><p><strong>Elliptic Integral K(k)</strong>:</p>
<ul>
<li>Definition: The complete elliptic integral of the first kind,
denoted by K(k), is defined as
<code>K(k) = ∫_0^(π/2) dϑ / sqrt(1-k^2 sin^2 ϑ)</code>.</li>
<li>Special Values: <code>K(0) = π/2</code> and
<code>lim_{k→1-} K(k) = +∞</code>.</li>
<li>Formula: It can be expressed as
<code>K(k) = (π/2) F((1/2), (1/2); 1; k^2)</code>, where F is the
Gaussian hypergeometric function. Alternative series expansions are also
provided.</li>
</ul></li>
<li><p><strong>AGM-based Computation</strong>:</p>
<ul>
<li>The connection to AGM: <code>K(k) = (π/2) * AGM(1, √(1-k^2))</code>,
where AGM is the Arithmetic-Geometric Mean function.</li>
<li>Approximation for k close to 1:
<code>K(k) ≈ log(4 / √(1-k^2))</code>.</li>
</ul></li>
<li><p><strong>Product Forms</strong>: Two product forms are provided
for efficient computation of K’(k) and E’(k), which are related to K(k)
and E(k) respectively. These product forms involve iterative
calculations with square roots and inverse square roots.</p></li>
<li><p><strong>Higher-order Products</strong>: The text also discusses
higher-order products for K and E, which can be used for more accurate
computations but require more complex iterations.</p></li>
<li><p><strong>Elliptic Integral of the Second Kind (E(k))</strong>:</p>
<ul>
<li>Definition:
<code>E(k) = ∫_0^(π/2) sqrt(1-k^2 sin^2 ϑ) dϑ</code>.</li>
<li>Special Values: <code>E(0) = π/2</code> and
<code>E(1) = 1</code>.</li>
<li>Relation to K(k): <code>E(k) = (π/2) * F(-1/2, 1/2; 1; k^2)</code>,
where F is the Gaussian hypergeometric function.</li>
</ul></li>
<li><p><strong>Theta Functions</strong>: Theta functions Θ2, Θ3, and Θ4
are defined as infinite series involving powers of a variable ‘q’. They
satisfy certain identities and relations that allow them to be connected
to elliptic integrals via the AGM.</p></li>
<li><p><strong>Eta Function</strong>: Although not explicitly defined in
the given text, eta functions (denoted by η(z)) are related to theta
functions and play a significant role in number theory, particularly in
the study of modular forms.</p></li>
<li><p><strong>Singular Values</strong>: These are not explicitly
discussed in the provided text but are likely related to singular values
in linear algebra or singular value decomposition (SVD), which can be
connected to elliptic integrals through certain
transformations.</p></li>
</ol>
<p>These concepts are crucial in various areas of mathematics, including
number theory, complex analysis, and computational methods for
approximating mathematical constants like π. The AGM-based computations
provide efficient ways to calculate these elliptic integrals, while the
product forms and higher-order products offer alternative methods with
varying levels of complexity and accuracy.</p>
<p>The text presents algorithms for calculating specific hypergeometric
functions using the Arithmetic-Geometric Mean (AGM) iteration method.
Here’s a detailed explanation of the given algorithms:</p>
<ol type="1">
<li><strong>Hypergeometric Function F(1/2, 1/2; z)</strong></li>
</ol>
<p>This algorithm calculates the hypergeometric function F(1/2, 1/2; z),
where |z| &lt; 1/2. The AGM iteration is used to compute this function
efficiently with quadratic convergence:</p>
<pre><code>a. Initialize `a_0 = √z` and `b_0 = 1 - z`.

b. Perform the following iterations until convergence (i.e., until `a_n - b_n` is sufficiently small):
    
    i. Compute the arithmetic mean `c_n = (a_n + b_n) / 2`.
    ii. Compute the geometric mean `d_n = √(a_n * b_n)`.
    iii. Update `a_{n+1} = c_n - d_n` and `b_{n+1} = c_n + d_n`.

c. The desired hypergeometric function F(1/2, 1/2; z) is then given by:

    F(1/2, 1/2; z) = (2 / π) * arctan(√z) - ∑_{k=0}^{N-1} (1 / (2k + 1)) * ((a_N - b_N)^(2k+1))</code></pre>
<ol start="2" type="1">
<li><strong>Transformations for Hypergeometric Functions</strong></li>
</ol>
<p>The text also provides two transformations that can be applied to the
hypergeometric functions F(1/2 ± s, 1/2 ∓ s; z) and F(1/4 ± t, 1/4 ∓ t;
z):</p>
<pre><code>a. For F(1/2 ± s, 1/2 ∓ s; z), where |z| &lt; 1/2:

    F(1/2 + s, 1/2 - s; z) = (4 / π) * arctan(√(1 - z) / √z) * F(1/4 + s/2, 1/4 - s/2; 4z(1 - z))

b. For F(1/4 ± t, 1/4 ∓ t; z), where |z| &lt; 1:

    F(1/4 + t, 1/4 - t; z) = (1 / π) * arctan(√(1 - z^2)) * F(1/2 + 2t, 1/2 - 2t; (1 - √(1 - z))/2)</code></pre>
<p>These transformations allow the calculation of different
hypergeometric functions by relating them to previously computed
values.</p>
<p>The text provides several algorithms for computing the value of
specific hypergeometric functions, which are closely related to the
computation of the mathematical constant π. These algorithms use a
method known as Arithmetic-Geometric Mean (AGM), a numerical method that
converges superlinearly (faster than linear convergence).</p>
<ol type="1">
<li><p><strong>Basic AGM Algorithm for Computing π</strong>: The first
algorithm is based on Gauss’s original formulation of the AGM, and it
involves initializing two variables <code>a0</code> and <code>b0</code>,
then iteratively updating them according to the rules:</p>
<ul>
<li><code>ak+1 = (ak + bk) / 2</code></li>
<li><code>bk+1 = sqrt(ak * bk)</code></li>
</ul>
<p>The computation of π is derived from these iterations by using a
formula involving the arithmetic-geometric mean and an error term. This
algorithm has second-order convergence (#FPM=98.4 for computing π to 4
million digits).</p></li>
<li><p><strong>Schönhage’s Variant</strong>: Another AGM variant, due to
Schönhage, is also presented. It uses a different initial condition
(<code>a0 = 1</code>, <code>b0 = sqrt(6 + sqrt(2)) / 4</code>) and has
fewer full precision multiplications (#FPM=78.424 for the same π
computation).</p></li>
<li><p><strong>AGM Variants by Borwein</strong>: Two additional
AGM-based iterations are introduced, both with fourth-order convergence
(faster than the second-order methods above). They use different initial
conditions and transformations:</p>
<ul>
<li>Variant 1 uses <code>a0 = 1</code>,
<code>b0 = sqrt(6 + sqrt(2)) / 4</code> (#FPM=99.5 for the bilinear
variant, #FPM=155.3 for the quartic).</li>
<li>Variant 2 uses <code>a0 = 1</code>,
<code>b0 = sqrt(6 - sqrt(2)) / 4</code> (#FPM=108.2 for the bilinear
variant, #FPM=169.5 for the quartic).</li>
</ul></li>
<li><p><strong>Second-Order Iteration</strong>: A simpler iteration with
quadratic convergence is presented:</p>
<ul>
<li><code>y0 = sqrt(2) - 1</code>,</li>
<li><code>a0 = 1 / 2</code>,</li>
<li><code>yk+1 = (1 - yk^2)^(-1/2) - 1</code>,</li>
<li><code>ak+1 = ak * (1 + yk+1)^2 - 2^(k+1) * yk+1</code>.</li>
</ul>
<p>This iteration (#FPM=255.7 for the same π computation) demonstrates a
technique to save operations by reusing intermediate results.</p></li>
<li><p><strong>Borwein’s Quartic Iterations</strong>: Two quartic
(fourth-order convergence) iterations are described:</p>
<ul>
<li>Variant r = 4 uses <code>y0 = sqrt(2) - 1</code>,
<code>a0 = 6 - 4*sqrt(2)</code>,</li>
<li>Variant r = 16 uses a specific initial condition and provides
approximately double the precision for each step.</li>
</ul></li>
</ol>
<p>These algorithms serve as efficient ways to compute π, with varying
numbers of full precision multiplications (#FPM), depending on their
order of convergence and the specific implementation chosen. They are
all based on the AGM method, which has been known since Gauss’s time and
has been rediscovered and refined by various mathematicians over the
years.</p>
<p>The text discusses methods for computing the natural logarithm using
the Arithmetic-Geometric Mean (AGM) method, as well as through inverting
the exponential function.</p>
<ol type="1">
<li><p><strong>AGM-based computation</strong>: The AGM method is used to
compute the logarithm by relating it to the AGM iteration. The formula
given is:</p>
<p>|log(d) - R’(10^-n) + R’(10^-n * d)| &lt;= n / 10^(2<em>(n-1))
(32.1-1a) log(d) ≈ R’(10^-n) - R’(10^-n </em> d) (32.1-1b)</p>
<p>Here, ‘R’ likely refers to some function derived from the AGM
iteration, and n is a parameter determining the precision. The term
R’(10^-n) can be computed once and reused for subsequent logarithm
calculations.</p>
<p>The argument of the logarithm (d) is first normalized so that it
falls within the interval [1/2, 1]. If it’s outside this range, an
argument reduction is performed using log(M sf - f * log(s)), where M is
the mantissa, s = sqrt(2), and f is an integer chosen such that M*s^f is
in the desired interval.</p>
<p>The logarithm of the radix (r) can be computed using Θ3(q) and Θ2(q),
which are functions related to the AGM iteration. Once π and log(r) are
known, log(d) can be computed using the above relations.</p></li>
<li><p><strong>Computation by inverting the exponential
function</strong>: This method involves using the inverse relationship
between the logarithm and the exponential function. Given an efficient
algorithm for the exponential function (exp), we can compute the
logarithm as follows:</p>
<p>y := 1 - d * exp(-x) log(d) = x + log(1 - y)</p>
<p>The last term, log(1 - y), is expanded as a power series and
truncated after the n-th power of y to yield an iteration of order
n:</p>
<p>x_(k+1) = Φ_n(x_k) := x_k - (y + y^2/2 + y^3/3 + … + y^(n-1)/n-1)</p>
<p>Alternatively, Padé approximants can be used to construct higher
order iterations. These are rational function approximations of log(1 -
z), which can save computational effort compared to the power series
method by requiring only one division instead of multiple
exponentiations for high-order terms. The Padé approximants P<a
href="z">i,j</a> produce iterations of order i + j + 1.</p>
<p>In summary, both methods presented leverage properties of logarithmic
and exponential functions to compute the natural logarithm. The
AGM-based method uses relationships derived from the AGM iteration,
while the inversion method relies on the inverse relationship between
logarithms and exponents, with options for either power series or Padé
approximants to enhance precision.</p></li>
</ol>
<p>The text discusses several methods for computing special functions,
specifically focusing on logarithms, exponentials, and their related
series. Here’s a summary of the key points:</p>
<ol type="1">
<li><p><strong>Padé Approximants</strong>: These are rational function
approximations to analytic functions, often used when dealing with
functions that are difficult to compute directly. For example, Padé
approximants for log(1+z) and arctan(z) are provided, along with methods
for their computation using recurrence relations.</p></li>
<li><p><strong>Argument Reduction</strong>: This technique is employed
to improve the efficiency of computing special functions when the
argument (input value) is large. For logarithms, this involves using the
functional equation log(za) = a*log(z) and reducing z to a smaller value
close to 1 by setting a = 1/N, where N is a large integer. Similar
techniques are applied for arctan.</p></li>
<li><p><strong>Power Series</strong>: The exponential function, inverse
trigonometric functions (arcsin and arccos), and logarithms can be
computed using power series. These methods involve iterating over the
series terms until a desired precision is reached. High-order iterations
are preferred due to the expense of computing logarithms.</p></li>
<li><p><strong>AGM-based Computation</strong>: The Arithmetic-Geometric
Mean (AGM) method can be used for high-precision computation of the
exponential function. This involves solving for k and k’ such that x = π
K’/K, then using a relation involving 2n-th roots to compute
exp(-x).</p></li>
<li><p><strong>Curious Series</strong>: The text also presents some
intriguing series representations for logarithms, including relations
involving Fibonacci numbers (F_k) and Pell numbers (P_k).</p></li>
<li><p><strong>Simultaneous Computation of Logarithms</strong>: A method
is described to compute the logarithms of a given set of small primes
simultaneously using a function L(z) = 2*arccoth(z). This allows for
faster computation by expressing each prime’s logarithm as a linear
combination of terms L(Xi), where Xi are large integers, and the series
converges quickly.</p></li>
</ol>
<p>In summary, this text provides various methods to compute special
functions (logarithms, exponentials, arctan) efficiently using
techniques like Padé approximants, argument reduction, power series,
AGM-based computation, and intriguing series representations. These
methods cater to different scenarios based on the required precision and
computational resources available.</p>
<p>The text describes a shift-and-add algorithm for computing the base-b
logarithm (log_b(x)) with limited resources, which only requires shifts,
additions, comparisons, and table lookups. Here’s a detailed explanation
of the algorithm:</p>
<ol type="1">
<li><p><strong>Table Initialization</strong>: First, create a lookup
table (<code>shiftadd_ltab</code>) containing values
<code>Ak = log_b(1 + 1/2^k)</code>. This table is generated by the
function <code>make_shiftadd_ltab(double b)</code>, which calculates
these values and stores them for future use.</p></li>
<li><p><strong>Algorithm Input</strong>: The algorithm takes two
inputs:</p>
<ul>
<li>Argument <code>x</code> (≥ 1): For which we want to compute
log_b(x).</li>
<li>Number of iterations <code>n</code>: Determines the precision of the
result.</li>
</ul></li>
<li><p><strong>Initialization</strong>: Set initial values for
variables:</p>
<ul>
<li><code>t0 = 0</code>: Accumulator for the final result.</li>
<li><code>e0 = 1</code>: A value used in the computation, initially set
to 1.</li>
<li><code>k = 1</code>: Iteration counter, starting from 1.</li>
</ul></li>
<li><p><strong>Main Loop</strong>: The algorithm then enters a loop that
iterates <code>n</code> times (or until the desired precision is
reached).</p>
<p>In each iteration (<code>k</code>):</p>
<ul>
<li>Compute <code>uk = ek * (1 + 2^(-k))</code>. This calculates an
intermediate value using the current exponent <code>ek</code> and the
iteration index <code>k</code>.</li>
<li>Compare <code>uk</code> with <code>x</code>:
<ul>
<li>If <code>uk ≤ x</code>, set <code>dk = 1</code>; otherwise, set
<code>dk = 0</code>. This determines whether to add the corresponding
table value.</li>
</ul></li>
<li>If <code>dk ≠ 0</code>, update:
<ul>
<li><code>tk+1 = tk + Ak</code>: Add the lookup table value to the
accumulator.</li>
<li><code>ek+1 = uk</code>: Update the exponent for the next
iteration.</li>
</ul></li>
<li>If <code>dk = 0</code>, keep the current values:
<code>tk+1 = tk</code> and <code>ek+1 = ek</code>.</li>
<li>Increment <code>k</code>.</li>
</ul></li>
<li><p><strong>Termination</strong>: The loop exits when
<code>k = n</code> (or the desired precision is reached), and the final
result is stored in <code>tk</code>.</p></li>
</ol>
<p>This shift-and-add algorithm for logarithm computation offers several
advantages, such as: - It only requires basic arithmetic operations
(shifts, additions, comparisons) and table lookups. - It can be adapted
to use integer arithmetic by scaling the values appropriately. - The
algorithm is particularly useful in hardware implementations with
limited resources or when precise logarithms are needed for specific
applications like binary search algorithms.</p>
<p>The provided text discusses two types of algorithms for computing
mathematical functions, specifically logarithms (log_b(x)) and
exponentials (b^x), using limited computational resources. The methods
are shift-and-add and CORDIC (Coordinate Rotation Digital Computer).</p>
<ol type="1">
<li><p>Shift-and-Add Algorithms:</p>
<ol type="a">
<li>Logarithm Computation (log_b(x)):</li>
</ol>
<ul>
<li>This algorithm computes the logarithm of x to base b, where b &gt; 0
and b ≠ 1. It uses a precomputed lookup table, shiftadd_ltab[],
containing values Ak = log_b((2<sup>k)/(2</sup>k-1)).</li>
<li>The main loop iterates k times (n &lt;= ltab_n), updating variables
t (accumulated result), e (multiplier), and v (power of 1/2). It checks
if the current estimate u is less than or equal to x, and if so, adds
the corresponding table value Ak to the accumulator.</li>
<li>The process continues until the desired precision is reached or a
maximum number of iterations is exceeded.</li>
</ul>
<ol start="2" type="a">
<li>Exponential Computation (b^x):</li>
</ol>
<ul>
<li>This algorithm computes b^x for b &gt; 1 and x ∈ R. It also uses the
same lookup table, shiftadd_ltab[], but with values Ak =
log_b((2<sup>k)/(2</sup>(k-1))).</li>
<li>The main loop iterates k times (n &lt;= ltab_n), updating variables
t (accumulated result), e (multiplier), and v (power of 1/2). It checks
if the current estimate u is less than or equal to x, and if so, adds
the corresponding table value Ak to the accumulator.</li>
<li>The process continues until the desired precision is reached or a
maximum number of iterations is exceeded.</li>
</ul></li>
<li><p>CORDIC Algorithms:</p>
<p>CORDIC algorithms are used for computing functions like sine, cosine,
exp, and log using only multiplication by powers of 2 (shifts),
additions, subtractions, and comparisons. They require a precomputed
lookup table with as many entries as the desired accuracy in bits.</p>
<p>The circular case (sine and cosine computation) is discussed:</p>
<ul>
<li>Initialize x0 = K, y0 = 0, z0 = θ, where K is a scaling
constant.</li>
<li>Iterate using the formulas for xk+1, yk+1, and zk+1, which involve
the precomputed arctan values from cordic_ctab[].</li>
<li>The process converges if -r ≤ z0 ≤ r, where r is the sum of all
arctan(2^(-k)).</li>
</ul>
<p>Both shift-and-add and CORDIC algorithms are designed to minimize the
use of multiplications, relying instead on shifts and additions, making
them suitable for environments with limited computational
resources.</p></li>
</ol>
<p>The text discusses the Binary Splitting (binsplit) algorithm, a
method for efficiently computing power series, particularly useful when
the coefficients are rational numbers. Here’s a detailed explanation of
the key points:</p>
<ol type="1">
<li><p><strong>Binary Splitting Algorithm</strong>: This is an efficient
way to compute sums and products by recursively splitting them into
smaller parts. It’s based on the idea of dividing the sum or product
into two parts, evaluating each part recursively, and then combining the
results.</p></li>
<li><p><strong>Factorial Computation</strong>: The algorithm is first
illustrated using factorials (n!). For example, to compute 8!, it breaks
down as follows: F(1,8) -&gt; F(1,4) -&gt; F(1,2) -&gt; F(1,1), then
combining the results. This depth-first approach localizes memory access
and avoids cache problems associated with pairwise processing.</p></li>
<li><p><strong>Polynomial Computation</strong>: The same principle can
be applied to compute polynomials from their roots. Each term of the
polynomial is a product of factors (x - root_i), and the algorithm
recursively computes these products.</p></li>
<li><p><strong>Summation Scheme for Power Series</strong>: For power
series PN-1 k=0 ak, the algorithm uses ratios Rk = ak/ak-1 to
recursively compute the sum. It defines Rm,n as the sum from m to n, and
expresses it in terms of smaller sub-sums.</p></li>
<li><p><strong>Rational Implementation</strong>: For rational
coefficients, the algorithm can be implemented using rationals directly.
The function R(m, n) computes the sum from m to n recursively, using the
relation Rm,n = Rm + Rm · Rm+1 + … + Rm · … · Rx + Rm · … · Rx · [Rx+1 +
… + Rx+1 · … · Rn].</p></li>
<li><p><strong>Integer Implementation</strong>: In languages without
rational support, the algorithm can be adapted to use integers. It
involves separate computations for numerators and denominators, with a
reduction step to simplify fractions if desired.</p></li>
<li><p><strong>Performance</strong>: The binary splitting algorithm
significantly outperforms naive methods for power series, especially
those with good convergence properties (like arctan(1/10)). Its
complexity is O(log N · M(N)), where M(N) is the complexity of a
multiplication of two N-bit numbers.</p></li>
<li><p><strong>Extending Prior Computations</strong>: The algorithm can
be extended to compute sums to higher precision by reusing previously
computed ratios, making it efficient for progressive
refinement.</p></li>
<li><p><strong>Radix Conversion</strong>: A variation of the binary
splitting algorithm is used for fast radix conversion (changing between
number systems). This involves recursively applying a relation that
splits a sum into two parts based on powers of 2.</p></li>
<li><p><strong>AGM vs Binary Splitting for π Computation</strong>: While
both methods can compute π, binary splitting generally outperforms
AGM-based iterations due to better memory access patterns. However, it
may require more memory if the coefficients grow rapidly, which can be
mitigated by converting to floating-point numbers at appropriate
stages.</p></li>
</ol>
<p>Recurrences are mathematical relationships that define a sequence
based on its previous terms. Specifically, a k-th order recurrence
relation is given by:</p>
<p>an = Σ(j=1 to k) mj * an-j</p>
<p>where ‘mj’ are constants, and ‘n’ represents the term number in the
sequence. The sequence is defined not just by this recurrence relation
but also by its initial k terms (a0, a1, …, ak-1).</p>
<p>The Fibonacci numbers and Lucas numbers are examples of sequences
defined by linear, homogeneous recurrences with constant
coefficients.</p>
<p>For instance, the Fibonacci sequence (Fn) is defined by:</p>
<p>Fn = Fn-1 + Fn-2, for n &gt; 1, with initial conditions F0 = 0 and F1
= 1.</p>
<p>Similarly, the Lucas numbers (Ln) are defined as:</p>
<p>Ln = Ln-1 + Ln-2, for n &gt; 1, but with different initial conditions
L0 = 2 and L1 = 1.</p>
<p>The term ‘linear’ in this context means that each term an is a linear
combination of the previous k terms. The term ‘homogeneous’ signifies
that there’s no term independent of the previous ones (i.e., no constant
term). Lastly, ‘with constant coefficients’ implies that the multipliers
mj are not functions of n but constants.</p>
<p>Solving recurrence relations can be complex, especially for
higher-order recurrences or non-constant coefficient cases. However,
they’re fundamental in many areas of mathematics and computer science,
including algorithm analysis, dynamic programming, numerical methods,
and more.</p>
<p>In the context of the provided code snippet (Ri(m, n, i=0)), it
appears to be a recursive function implementing a variant of the
recurrence relation for generating numbers based on their radix
representation. The function takes three arguments: m (the starting
number), n (the end number), and i (an indentation level used for
printing). It calculates and returns the sum of terms in a series
related to the radix conversion process, which might be part of an
algorithm for fast multiplication or similar numerical tasks.</p>
<p>It’s important to note that understanding this specific code snippet
requires additional context, as it seems to be a piece of a larger
program or library. Without knowing the exact purpose and surrounding
code, a comprehensive explanation isn’t possible.</p>
<p>The text discusses several aspects of recurrence relations, including
their computation using matrix powers and polynomial arithmetic, as well
as methods for handling inhomogeneous recurrences and generating
functions.</p>
<ol type="1">
<li><p><strong>Fast Computation Using Matrix Powers</strong>: This
method uses the characteristic polynomial p(x) to create a companion
matrix M. The k-th term of a recurrence can be computed by calculating
the leftmost column of M^k (mod p(x)) and multiplying it with the
initial values vector v. This approach is efficient when powering
algorithms are used, especially for high-order recurrences.</p></li>
<li><p><strong>Faster Computation Using Polynomial Arithmetic</strong>:
This method further optimizes the matrix power approach by exploiting
the structure of polynomial multiplication. It uses modular polynomial
multiplications, reducing the complexity from O(log k * n^3) to O(log k
* n^2) or even O(log k * n * log n), depending on the multiplication
algorithm used.</p></li>
<li><p><strong>Inhomogeneous Recurrences</strong>: The text explains how
to transform inhomogeneous recurrence relations (those with an
additional polynomial term P(n)) into homogeneous ones of higher order.
This is done by repeatedly subtracting shifted versions of the original
relation until the constant or polynomial term vanishes.</p></li>
<li><p><strong>Recurrence Relations for Subsequences</strong>: The text
describes methods for finding recurrences for subsequences of a given
sequence. For two-term recurrences, it provides closed forms for the
coefficients Ak using Chebyshev polynomials. For general order n
recurrences, it suggests using the companion matrix and its powers to
find the characteristic polynomial of the stride-s subsequence
recurrence.</p></li>
<li><p><strong>Generating Functions for Recurrences</strong>: A
generating function is a power series where the k-th coefficient equals
the k-th term of the recurrence. The text explains how to construct such
functions using the characteristic polynomial and initial terms of the
sequence.</p></li>
<li><p><strong>Binet Forms for Recurrences</strong>: These are
closed-form expressions for terms in a linear recurrence relation. For a
two-term recurrence, it provides a formula involving the roots of the
characteristic polynomial. The text also mentions Binet forms for n-term
recurrences and conditions under which they hold.</p></li>
</ol>
<p>The main advantage of these methods is their efficiency, especially
for high-order recurrences and when modular arithmetic or polynomial
multiplication algorithms optimized by Fast Fourier Transform (FFT) are
used. These techniques find extensive applications in number theory,
combinatorics, and other areas of mathematics and computer science.</p>
<p>The text discusses Chebyshev polynomials of the first and second
kinds, denoted as Tn(x) and Un(x), respectively. These are defined using
trigonometric functions or through explicit formulas involving binomial
coefficients.</p>
<ol type="1">
<li><p><strong>Definitions</strong>:</p>
<ul>
<li>Tn(x) = cos[n arccos(x)]</li>
<li>Un(x) = sin[(n+1)arccos(x)] / sqrt[1-x^2]</li>
</ul>
<p>For integer n, both are polynomials. The first few are provided in
figures 35.2-A and 35.2-B.</p></li>
<li><p><strong>Explicit Formulas</strong>:</p>
<ul>
<li>Tn(x) = Σ [(-1)^k * (n-k-1)! / (k! * (n-2k)! * (2x)^(n-2k))] for k=0
to ⌊n/2⌋</li>
<li>Un(x) = Σ [(-1)^k * (n-k)! / (k! * (n-2k)! * (2x)^(n-2k))] for k=0
to ⌊n/2⌋</li>
</ul></li>
<li><p><strong>Properties</strong>:</p>
<ul>
<li>The n+1 extrema of Tn(x) are at x_k = cos(kπ/n), where −1 ≤ x_k ≤
+1, with values ±1.</li>
<li>The n zeros of Tn(x) are at x_k = cos((k-1/2)π/n), for k=1 to
n.</li>
</ul></li>
<li><p><strong>Expansion</strong>:</p>
<ul>
<li>For even n: x^n = 1/(2n) * (n choose n/2) + Σ [(n choose 2k-n) *
T_(n-2k)(x)] for k=0 to ⌊n/2⌋ - 1</li>
<li>For odd n: x^n = Σ [(n choose (2k-n)) * T_(n-2k)(x)] for k=0 to
(n-1)/2</li>
</ul></li>
<li><p><strong>Recurrence Relations</strong>:</p>
<ul>
<li>Nn = 2xNn−1 −Nn−2, where N can be either T or U</li>
<li>Subsequence recurrences: Nn+1 = [2x] * Nn - Nn−1, Nn+2 = [2(2x^2
-1)] * Nn - Nn−2, etc.</li>
</ul></li>
<li><p><strong>Generating Functions</strong>:</p>
<ul>
<li>1 −xt / (1 −2xt + t^2) = Σ tn Tn(x), and similarly for Un(x).</li>
</ul></li>
<li><p><strong>Binet Formulas</strong>:</p>
<ul>
<li>Tn(x) = (x + sqrt[x^2 -1])^n + (x - sqrt[x^2 -1])^n / 2, and Un(x) =
sqrt[(x^2 -1)/(4x)] * [(x + sqrt[x^2 -1])^n - (x - sqrt[x^2
-1])^n].</li>
</ul></li>
<li><p><strong>Composition Law</strong>:</p>
<ul>
<li>T_m(T_n(x)) = T_(mn)(x).</li>
</ul></li>
<li><p><strong>Fast Computation Algorithms</strong>:</p>
<ul>
<li>The text presents efficient algorithms for computing these
polynomials, particularly for large n. These methods exploit the
recurrence relations and avoid expensive operations like multiplication
by x or division.</li>
</ul>
<p>For Chebyshev polynomials of the first kind (Tn), the algorithm
recursively computes pairs [Tn-1, Tn] using a binary splitting method.
Similarly, an efficient method for Un is described using the relation Un
= (Tn - xTn+1) / (1 - x^2).</p></li>
</ol>
<p>These Chebyshev polynomials have numerous applications in numerical
analysis and approximation theory, particularly for approximating
functions or solving differential equations over intervals. They are
named after Pafnuty Chebyshev, a 19th-century Russian mathematician who
introduced them.</p>
<p>This section discusses hypergeometric series and functions, which are
a set of special functions that include many useful mathematical
functions like logarithms and sines as special cases.</p>
<p><strong>Deﬁnition and Basic Operations (36.1)</strong></p>
<p>The hypergeometric series F(a, b; c | z) is defined as:</p>
<p>F(a, b; c | z) = ∑_{k=0}^∞ [(a)__k (b)__k] / [(c)__k k!] * z^k</p>
<p>where (x)__n denotes the Pochhammer symbol or rising factorial power:
x(x+1)(x+2)…(x+n-1), and (x)__0 = 1. The variable z is called the
argument, while a, b, and c are parameters. ‘a’ and ‘b’ are upper
parameters, and ‘c’ is the lower parameter.</p>
<p>Hypergeometric series can have any number of parameters, and they
correspond to hypergeometric functions when they converge.
Hypergeometric functions with rational arguments can be computed using
binary splitting methods (34.1.2).</p>
<p><strong>Derivative and Differential Equation (36.1)</strong></p>
<p>The n-th derivative of a hypergeometric function f(z) = F(a, b; c |
z) is:</p>
<p>d<sup>n/dz</sup>n [F(a, b; c | z)] = [(a)<strong>(n+1)
(b)</strong>(n+1)] / [(c)__(n+1) n!] * F(a+n, b+n; c+n | z)</p>
<p>The function f(z) = F(a, b; c | z) is a solution of the differential
equation:</p>
<p>z (1 - z) d^2 f/dz^2 + [c - (1 + a + b) z] df/dz - a b f = 0</p>
<p><strong>Evaluations for Fixed Argument (36.1)</strong></p>
<p>Closed-form evaluations of hypergeometric functions at specific
points are given for certain conditions. For example, the evaluation at
z=1 for F(a, b; c | z) is:</p>
<p>F(a, b; c | 1) = Γ(c) * Γ(c - a - b) / [Γ(c - a) * Γ(c - b)] if Re(c
- a - b) &gt; 0 or b ∈ N and b &lt; 0</p>
<p><strong>Extraction of Even and Odd Part (36.1)</strong></p>
<p>The even part E[f(z)] and odd part O[f(z)] of a hypergeometric series
are given by:</p>
<p>E[F(a, b; c | z)] = F(a/2, (a+1)/2, b/2, (b+1)/2; c/2, (c+1)/2, 1/2 |
z^2) O[F(a, b; c | z)] = a * b / c * z * F((a+1)/2, (a+2)/2, (b+1)/2,
(b+2)/2; (c+1)/2, (c+2)/2, 3/2 | z^2)</p>
<p><strong>Multisection by Selecting Terms with Exponents s mod M
(36.1)</strong></p>
<p>For a power series H(z), the multisection operation extracts terms
whose exponent of z is congruent to s modulo M:</p>
<p>H<a href="z">s,M</a> = 1/M ∑_{k=0}^{M-1} ω^(-s k) * H(ω^k z)</p>
<p>where ω = exp(2πi/M). For hypergeometric functions, this operation
involves replacing each upper and lower parameter A with (A+s)/M, …,
(A+s+M-1)/M, the argument z with X*z^M, and multiplying by z^s.</p>
<p><strong>Transformations of Hypergeometric Series (36.2)</strong></p>
<p>Hypergeometric series parameters in the upper or lower row can be
swapped: F(a, b, c | z) = F(b, a, c | z). Identical elements in the
lower and upper row can be canceled: F(a, b, C | z) = F(a, b | z) if C
appears in both rows.</p>
<p><strong>Elementary and Contiguous Relations (36.2)</strong></p>
<p>Identities of contiguous relations are given by:</p>
<p>(a - b) * F(a, b; c | z) = a * F(a+1, b; c | z) - b * F(a, b+1; c |
z) (a - c) * F(a, b; c+1 | z) = a * F(a+1, b; c+1 | z) - c * F(a, b; c |
z)</p>
<p><strong>Pfaﬀ’s Reﬂection Law and Euler’s Identity (36.2)</strong></p>
<p>Pfaﬀ’s reﬂection law relates the value of a hypergeometric function
at z to its value at -z:</p>
<p>1/(1-z)^a * F(a, b; c | -z / (1-z)) = F(a, c-b; c | z)</p>
<p>The provided text discusses various transformations of hypergeometric
series, which are special functions that appear frequently in
mathematical physics, combinatorics, and other areas. These
transformations allow for the manipulation and simplification of
expressions involving these functions. Here’s a detailed summary and
explanation of some key points:</p>
<ol type="1">
<li><p><strong>Euler’s Transformation</strong>: Euler’s transformation
is given by equation (36.2-9), which expresses the ratio of two
hypergeometric functions as a Padé approximant for the r-th root,
provided both series terminate. This means that under certain
conditions, the ratio simplifies to a rational function
approximation.</p></li>
<li><p><strong>Generalizations of Euler’s Transformation</strong>: The
text presents two generalized forms of Euler’s transformation (36.2-10
and 36.2-11) for hypergeometric functions of type 3F2, where one upper
parameter exceeds a lower parameter by 1. These transformations involve
specific constants (f and g) derived from the parameters a, b, and
c.</p></li>
<li><p><strong>Gauss’ Transformations</strong>: Gauss provided two
quadratic transformations for hypergeometric series, as seen in
equations (36.2-12a) and (36.2-12b). These transformations relate 2F1
series with different arguments through the variable z. A rewritten form
of these transformations is also given (36.2-13), which connects 2F1
series for the argument 1−z/2.</p></li>
<li><p><strong>Whipple’s Identity</strong>: Whipple’s identity (36.2-14)
establishes a connection between two specific hypergeometric functions
of type 3F2. This identity is useful in simplifying or relating
different expressions involving these functions, particularly when
certain parameters are equal or related.</p></li>
<li><p><strong>Special Cases and Further Transformations</strong>: The
text presents several special cases and additional transformations
derived from Gauss’ and Whipple’s identities, such as (36.2-15),
(36.2-16), (36.2-18a/b), (36.2-19a/b), and others. These relations
provide further simplification or manipulation possibilities for
hypergeometric series expressions.</p></li>
<li><p><strong>Clausen’s Product Formula</strong>: This formula
(36.2-24) connects two different hypergeometric functions of type 2F1
and 3F2, offering a way to express one in terms of the other under
specific conditions. Goursat’s relation (36.2-25) is another similar
product-type identity for hypergeometric series.</p></li>
<li><p><strong>Kummer Transformation</strong>: The Kummer transformation
(36.2-33) relates two 1F1 (confluent hypergeometric functions) by
expressing one in terms of the other, multiplied by an exponential
function. This transformation is particularly useful for manipulating
expressions involving confluent hypergeometric series.</p></li>
<li><p><strong>Additional Transformations</strong>: Various other
transformations are presented, connecting hypergeometric functions of
different types (e.g., 1F1 to 2F3) and involving relationships between
parameters (e.g., (36.2-34), (36.2-37), and (36.2-40)). These
transformations offer further flexibility in working with hypergeometric
series expressions.</p></li>
</ol>
<p>These transformations, identities, and relationships are essential
tools for manipulating, simplifying, or evaluating complex expressions
involving hypergeometric functions. They find applications in diverse
fields such as physics, engineering, and mathematics, allowing
researchers to tackle problems that would otherwise be challenging or
impossible to solve directly.</p>
<p>The provided text is a section from a mathematical reference on
Hypergeometric series, specifically focusing on elementary functions
expressed as hypergeometric functions. Here’s a detailed summary and
explanation of the content:</p>
<ol type="1">
<li><strong>Power, Root, and Binomial Series (36.3.1):</strong>
<ul>
<li>The reciprocal of (1-z)^a can be represented as a Hypergeometric
function F(a; ; z), and similarly for (1+z)^a. These are given by
relations 36.3-1a and 36.3-1b, respectively.</li>
<li>An important special case is 1/(1-z) = F(1; ; z).</li>
</ul></li>
<li><strong>Additional Identities:</strong>
<ul>
<li>The text provides several identities involving Hypergeometric
functions with specific parameters. For example, F((-n); n+1; n; -z) =
(1-2z)(1-z)^(n-1), showing a relationship between these functions and
polynomial expressions.</li>
</ul></li>
<li><strong>Chebyshev Polynomials:</strong>
<ul>
<li>The Chebyshev polynomials T_n(x) can be expressed using
Hypergeometric functions, as shown in 36.3-7a, 36.3-7b, and
36.3-7c.</li>
</ul></li>
<li><strong>Hermite Polynomials (36.3.3):</strong>
<ul>
<li>The Hermite polynomials H_n(z) are defined by a recurrence relation
involving the previous two terms. The Hypergeometric representation of
H_n(z) for non-negative integer n is given in 36.3-10.</li>
</ul></li>
<li><strong>Exponential and Logarithm (36.3.4):</strong>
<ul>
<li>The exponential function exp(z) equals F(; ; z), and the natural
logarithm log(1+z) can be expressed as a Hypergeometric series in
36.3-12a.</li>
</ul></li>
<li><strong>Bessel Functions and Error Function (36.3.5):</strong>
<ul>
<li>The Bessel functions of the first kind J_n(z) and modified Bessel
functions of the second kind I_n(z) are expressed using Hypergeometric
functions in 36.3-14a and 36.3-14b, respectively.</li>
<li>The Error function erf(z), defined as √π/2 * z * F(1/2; 3/2; -z^2),
is given in 36.3-15a.</li>
</ul></li>
<li><strong>Trigonometric and Hyperbolic Functions (36.3.6):</strong>
<ul>
<li>Series expansions for sine, hyperbolic sine, cosine, and hyperbolic
cosine are provided as Hypergeometric functions. For example, sin(z) = z
* F(3/2; ; -z^2/4).</li>
</ul></li>
<li><strong>Inverse Trigonometric and Hyperbolic Functions
(36.3.7):</strong>
<ul>
<li>Series expansions for arctan(z), arctanh(z), arccoth(z), and the
inverse of logarithm are given using Hypergeometric functions. For
example, arctan(z) = z * F(1/2; 1/3; -z^2).</li>
</ul></li>
</ol>
<p>These relations provide powerful tools for expressing elementary
functions as Hypergeometric series, enabling a wide range of
mathematical manipulations and approximations.</p>
<p>The provided text discusses several mathematical concepts, primarily
focusing on Cyclotomic Polynomials, Möbius Inversion Principle, Lambert
Series, and continued fractions.</p>
<ol type="1">
<li><p><strong>Cyclotomic Polynomials</strong>: These are polynomials
with integer coefficients whose roots are the primitive nth roots of
unity (complex numbers that satisfy x^n = 1). The degree of these
polynomials equals Euler’s totient function φ(n), which counts the
positive integers up to n that are coprime to n. For example, the 63rd
cyclotomic polynomial is Y63(x) = x^36 - x^33 + x^27 - x^24 + x^18 -
x^12 + x^9 - x^3 + 1.</p>
<p>The algorithm to compute these polynomials involves prime
factorization of n, then recursively dividing the variable by each prime
factor and finally adjusting the resultant polynomial with respect to
the highest power of primes in the factorization.</p></li>
<li><p><strong>Möbius Inversion Principle</strong>: This principle
relates two arithmetic functions through a summation process. The Möbius
function µ(n) is central to this principle. It’s defined such that it
equals 0 for numbers with square factors, +1 for 1, and (-1)^k for n
being the product of k distinct primes.</p>
<p>The key property of the Möbius function is that the sum over all
divisors d of a number n equals 1 if n = 1, and 0 otherwise: ∑_{d|n}
µ(d) = {1 if n=1; 0 otherwise}.</p>
<p>Interestingly, this function can also be expressed as a sum of the
primitive nth roots of unity.</p></li>
<li><p><strong>Lambert Series</strong>: The text mentions Lambert series
but does not provide explicit definitions or formulas. Generally, a
Lambert series is a power series whose terms are the values of an
arithmetic function at integer arguments, weighted by the reciprocals of
consecutive powers of some base. They’re often used in number theory for
summing up divisors.</p></li>
<li><p><strong>Continued Fractions</strong>: The text briefly mentions
continued fractions but does not provide details about their computation
algorithms. A continued fraction is an expression obtained through a
recursive process of representing a number as the sum of its integer
part and the reciprocal of another number, then repeating this process
with the new number. They’re useful in various areas of mathematics
including number theory and approximation theory.</p></li>
</ol>
<p>In summary, the text provides insights into cyclotomic polynomials,
an essential concept in algebraic number theory, and introduces related
mathematical principles like Möbius Inversion and Lambert series. It
also hints at continued fractions without delving into their specific
computation methods.</p>
<p>The text discusses various mathematical concepts related to power
series, Lambert series, and infinite products. Here’s a detailed summary
and explanation of the key points:</p>
<ol type="1">
<li><p><strong>Multiplicative Functions and Möbius Inversion</strong>:
The Möbius function (μ(n)) is multiplicative, meaning it satisfies
certain conditions for prime factorization. A multiplicative function
has values that can be computed using prime factors of n. The Möbius
inversion principle provides a way to switch between two functions, f(n)
and g(n), such that f(n) = Σ_{d|n} g(d)μ(n/d).</p></li>
<li><p><strong>Cyclotomic Polynomials</strong>: These are polynomials
with integer coefficients whose roots are primitive nth roots of unity.
They play a significant role in number theory and have a product form
involving the Möbius function. The cyclotomic polynomial Φ_n(x) can be
expressed as Σ_{d|n} μ(d) x^{n/d}.</p></li>
<li><p><strong>Lambert Series</strong>: These are series of the form
L(x) = Σ_{k&gt;0} a_k x^k / (1 - x^k). They have conversions to power
series and can be used to represent certain mathematical objects, like
the number of divisors function d(n).</p></li>
<li><p><strong>Conversion from Power Series to Infinite
Products</strong>: The text describes algorithms to convert power series
into infinite products of specific forms:</p>
<ul>
<li><p><strong>Q_k&gt;0 (1 - x<sup>k)</sup>b_k</strong>: This involves
finding coefficients b_k such that the power series’ derivative divided
by the series itself equals a Lambert series with coefficients -k * b_k.
The product form is then obtained by applying the inverse of this
transformation to the Lambert series.</p></li>
<li><p><strong>Q_k&gt;0 (1 + x<sup>k)</sup>c_k</strong>: For these, a
greedy algorithm is used to subtract and add terms from the power series
to obtain a Lambert series-like form, which can then be converted into a
product.</p></li>
<li><p><strong>Eta Products</strong>: The eta function η(x) = ∏_{j&gt;0}
(1 - x^j) can be used to represent certain power series as infinite
products. Algorithms are provided to convert between power series and
eta/eta+ products.</p></li>
</ul></li>
</ol>
<p>These conversions are useful in various areas of mathematics,
including number theory, combinatorics, and the study of integer
partitions. They allow researchers to switch between different
representations of mathematical objects, potentially simplifying
computations or revealing underlying structures.</p>
<p>The text discusses continued fractions, their properties, and methods
for computing them. Here’s a detailed summary:</p>
<p><strong>Continued Fractions:</strong></p>
<p>A continued fraction is an expression of the form K(a, b) = a0 +
b1/(a1 + b2/(a2 + b3/(…))). The bk are called partial numerators, and ak
are partial denominators. If bk+1 is set to zero, the value obtained is
called the k-th convergent of the continued fraction: Pk/Qk = a0 +
b1/(a1 + b2/(a2 + … + bk-1/(ak-1 + bk/ak))).</p>
<p><strong>Simple Continued Fractions:</strong></p>
<p>A simple continued fraction has all bk equal to 1. Rational numbers
have terminating simple continued fractions. Solutions of quadratic
equations with non-rational roots have eventually periodic simple
continued fractions.</p>
<p><strong>Convergents and Approximation:</strong></p>
<p>The k-th convergent (in lowest terms) is the best rational
approximation to a real number x, where the inequality |x - Pk/Qk| &lt;
1/(Q_k * Q_(k-1)) holds. The equality can only occur for terminating
continued fractions.</p>
<p><strong>Computing Simple Continued Fractions:</strong></p>
<p>The simple continued fraction of a real number x can be computed
using an iterative process that repeatedly applies the floor function to
x and stores the integer part as the next term in the continued
fraction.</p>
<p><strong>Continued Fractions of Polynomial Roots:</strong></p>
<p>For algebraic numbers (roots of polynomial equations with integer
coefficients), simple continued fractions can be computed by finding
successive real roots of shifted and negated reciprocal versions of the
original polynomial.</p>
<p><strong>Computation of Convergents:</strong></p>
<p>The sequences of partial numerators Pk and denominators Qk for a
given continued fraction can be computed using recurrence relations: Pk
= ak * P_(k-1) + bk * P_(k-2), Qk = ak * Q_(k-1) + bk * Q_(k-2). These
sequences give the convergents of the continued fraction.</p>
<p><strong>Evaluation:</strong></p>
<p>A function to compute the numerical value x from a simple continued
fraction uses an iterative process that starts with the last term and
moves backward through the sequence, adding 1 divided by each term. For
general (non-simple) continued fractions, this process needs to be
adapted to account for the partial numerators bk as well.</p>
<p>The text also mentions methods for computing continued fractions for
algebraic numbers and references resources for further study.</p>
<p>The text discusses a variation of the iteration for the inverse
function, focusing on two specific functions I(y) and J(y).</p>
<ol type="1">
<li><p><strong>Function I(y):</strong> This is defined as the infinite
series 1/(1 - y), which can also be expressed as (1 + y)(1 + y^2)(1 +
y^4)…(1 + y^k)…. The sign of each term in this product alternates,
starting with a positive one.</p></li>
<li><p><strong>Function J(y):</strong> This is defined similarly to
I(y), but the signs are reversed: (1 - y)(1 - y^2)(1 - y^4)…(1 - y^k)….
The sequence of zeros and ones in the binary expansion of this function
is known as the Thue-Morse sequence (A106400).</p></li>
</ol>
<p>The parity number, P, is a constant derived from J(y) when y = 1/2.
It can be computed using the iteration for K(y), which is defined as
half the square of the difference between I(y) and J(y).</p>
<p>Several relations are provided that connect these functions:</p>
<ul>
<li>I(y) * I(-y) = I(y^2) / 2</li>
<li>I(y) = J(y^2) / J(y)</li>
<li>I(-y) = (1 - y) / (1 + y) * I(y)</li>
<li>J(-y) = (1 + y) / (1 - y) * J(y)</li>
</ul>
<p>These relations show how the functions I and J are related to each
other and to their inverses.</p>
<p>The text also provides algorithms for computing I(y) and J(y), as
well as for the parity number P. These algorithms involve iterative
processes that manipulate the variable y according to specific
rules.</p>
<p>Finally, an inverse function for J(y), denoted as binpart(y), is
introduced. This function can be computed without division, making it
useful in contexts where division operations are costly or
problematic.</p>
<p>The provided text discusses several iterations related to various
mathematical sequences, each with its unique properties and functional
equations. Here’s a detailed explanation of each:</p>
<ol type="1">
<li><strong>Period-Doubling Sequence (Section 38.5):</strong>
<ul>
<li>Definition: T(y) is defined as the sum of
y<sup>(2n)/(1+(-1)</sup>n), for all non-negative integers n. This
results in an infinite series: T(y) = Σ (−1)^n * y^(2n) / (1 -
y^(2n)).</li>
<li>Representation: The sequence starts with 0, and each subsequent term
is generated by replacing every 0 with 11 and every 1 with 10. This can
be visualized through a string substitution process (Figure
38.5-A).</li>
<li>Generating Function: T(y) = y + y^3 + y^4 + y^5 + y^7 + y^9 + y^11 +
…, which represents the power series of the period-doubling
sequence.</li>
<li>Functional Equation: The key equation governing T(y) is T(y) +
T(y^2) = y / (1 - y), connecting the function at two different powers of
y.</li>
<li>Constant: The constant T := T(1/2) is a transcendental number with
various base representations and continued fraction expansion.</li>
</ul></li>
<li><strong>Golay-Rudin-Shapiro Sequence (Section 38.3):</strong>
<ul>
<li>Definition: Q(y) is generated by an iterative process that starts
with y and involves three operations in each step: updating the sequence
(Ln), calculating a new term (Rn = Ln + Rn^2 * (Ln - Rn)), and squaring
the current term (Yn+1 = Y_n^2).</li>
<li>Function: Q(y) is defined as an infinite series: 1 + y + y^2 - y^3 +
y^4 + y^5 - y^6 + …, representing a specific pattern of positive and
negative y-powers.</li>
<li>Constant: The Golay-Rudin-Shapiro constant Q, calculated as 1/2 *
Q(1/2), is also transcendental with various base representations
(hexadecimal in this context).</li>
<li>Functional Equations: Several functional equations govern the
behavior of Q(y), such as Q(y^2) = Q(y) + Q(-y)/2, and more complex
relations involving powers of y.</li>
</ul></li>
<li><strong>Thue Constant (Section 38.2):</strong>
<ul>
<li>Definition: The Thue constant T is constructed via a substitution
process where every ‘0’ becomes ‘111’, and every ‘1’ becomes ‘110’. This
results in an infinite string, from which the power series T(y) = Σ
y^(3n), for n = 0, 1, …, can be derived.</li>
<li>Functional Equation: The Thue constant obeys a specific equation
involving y, y^2, and y^3 terms, linking its value at different powers
of y.</li>
</ul></li>
<li><strong>Ruler Function (Section 38.4):</strong>
<ul>
<li>Definition: The ruler function R(y) is defined through an iterative
process starting with y and using a series of updates involving squaring
the current term (Yn+1 = Y_n^2), updating sequence values based on these
squared terms, and adding specific contributions based on n.</li>
<li>Constant: The constant R := R(1/2)/2 is transcendental, with its
binary representation detailing a pattern of ones and zeros.</li>
<li>Functional Equation: A key equation governing R(y) involves y, y^2,
and y^4 terms.</li>
</ul></li>
</ol>
<p>Each of these iterations and constants has unique properties and
plays significant roles in various mathematical contexts, such as number
theory, dynamical systems, and symbolic dynamics. They often exhibit
intriguing patterns and connections that are still objects of ongoing
study and exploration.</p>
<p>The text discusses several iterations related to various mathematical
sequences, specifically focusing on the period-doubling sequence,
generalizations of this sequence, and iterations connected with binary
digit summations and Gray codes.</p>
<ol type="1">
<li><p>Period-Doubling Sequence: The period-doubling sequence is a
series of 0s and 1s where each term (starting from the second) is formed
by doubling the position of the last non-zero digit in the previous
term, then replacing all zeros in between with ones. This results in a
sequence that has a pattern of repeating at intervals of 2^n for n ≥
0.</p></li>
<li><p>Generalizations:</p>
<ul>
<li>The functional equation F(y) + F(y3) = y/(1-y) leads to a new
function F(y), which is a Lambert series with coefficients given by R(k)
depending on the divisibility of k by 2 and 3.</li>
<li>Another generalization involves the functional equation F(y) +
F(y^2) + F(y^3) = y/(1-y). This can be solved recursively to produce a
function F(y), which is also shown to satisfy this equation through
power series analysis.</li>
</ul></li>
<li><p>Iterations related to Binary Digit Sum:</p>
<ul>
<li>A string substitution rule generates the “1’s-counting sequence”
(A000120 in OEIS), where each term represents the sum of binary digits
of natural numbers up to that point. The corresponding function S(y) is
defined by recursive rules involving powers and sums of previous terms,
leading to a power series with coefficients given by the sum-of-digits
sequence.</li>
<li>A similar approach generates a weighted sum of binary digits (revbin
constant), denoted W(y). The resulting function W(y) satisfies a
functional equation involving y and its powers.</li>
</ul></li>
<li><p>Iterations related to Binary Gray Code:</p>
<ul>
<li>The Gray code is an ordering of binary numbers such that each
successive value differs by only one bit. An iteration is defined where
the power series coefficients are the binary Gray code of the exponent
of y, leading to a function G(y). This function has associated
functional equations and its constant value at y=1/2 (Gray code
constant) is also discussed.</li>
</ul></li>
</ol>
<p>In all these iterations, the focus is on defining sequences or
functions recursively based on string substitution rules or arithmetic
operations, then exploring their properties through power series
analysis and identifying underlying patterns and functional
relationships. These mathematical constructions provide insights into
number theory, combinatorics, and sequence analysis.</p>
<p>The text discusses several iterations related to binary Gray code,
focusing on functions G(y), F(y), R(y), E(y), H(y) that generate
sequences with specific properties. Here’s a detailed summary of
each:</p>
<ol type="1">
<li><p><strong>G(y) - Gray Code Series</strong>: This is defined by the
infinite series (38.8-5). It can be computed everywhere except on the
unit circle, and its coefficients are powers of 2 in magnitude. The
function G(y) relates to the Gray code sequence, a binary numeral system
where two successive values differ by only one bit.</p></li>
<li><p><strong>F(y) - Differences of the Gray Code</strong>: F(y) = (1 -
y)G(y), which gives the power series whose coefficients are the
successive differences of the Gray code. Its sequence corresponds to the
ruler function, which outputs the highest power of 2 that divides a
given number.</p></li>
<li><p><strong>R(y) - Sum of Gray Code Digits</strong>: This is defined
by the infinite series (38.8-12). It represents the sum of binary digits
in the Gray code sequence for k ≥0. The constant R := R(1/2)/2 has a
specific value (38.8-10) and a binary expansion that corresponds to the
paper-folding sequence.</p></li>
<li><p><strong>P - Paper-Folding Constant</strong>: Defined as P = (R +
1)/2, this constant has various representations in different bases. It
is related to the paper-folding process used to create origami
figures.</p></li>
<li><p><strong>E(y) - Differences of Sum of Gray Code Digits</strong>:
This function is derived from F(y) by (38.8-17), with all power series
coefficients except for the constant term being ±1. It’s defined and
computable everywhere except on the unit circle, with a specific
iteration given in (38.8-19).</p></li>
<li><p><strong>H(y) - Hilbert Curve Encoding Function</strong>: This
function is defined through an iterative process (38.9-1), generating a
sequence of complex numbers that, when interpreted as movement
instructions, trace out the Hilbert curve in the complex plane. The real
and imaginary parts alternate, representing right/left turns and up/down
movements.</p></li>
</ol>
<p>The text also introduces a simplified algorithm for computing H(y)
(38.9-5), using two auxiliary functions P(y) and M(y). These are
computed via their respective iterations (38.9-3 and 38.9-4), making the
overall computation more efficient.</p>
<p>In addition, the text discusses a sparse power series iteration
(38.10) for computing the reciprocal of (1 - y), with applications in
number theory and continued fractions. The function F(y) generated by
this iteration has interesting properties concerning its binary and
decimal expansions.</p>
<p>The text discusses two iterations related to mathematical sequences,
specifically the Fibonacci numbers and Pell numbers.</p>
<p><strong>Fibonacci Iteration (Section 38.11):</strong></p>
<ol type="1">
<li><p><strong>Function Definition</strong>: The function A(y) is
defined through an iterative process that involves Fibonacci numbers.
It’s initialized with L0=0, R0=1, l0=1, and r0=y.</p></li>
<li><p><strong>Iteration Rules</strong>:</p>
<ul>
<li>ln+1 = rn (where rn = yFn+1)</li>
<li>rn+1 = rn ln (where ln = yFn+2)</li>
<li>Ln+1 = Rn</li>
<li>Rn+1 = Rn + rn+1 Ln + rn+1 ln</li>
</ul></li>
<li><p><strong>Power Series</strong>: After the n-th step, the series in
y is correct up to order Fn+2 −1, with a convergence rate of √(5+1)/2 ≈
1.6180. The power series representation of A(y) is given by:</p>
<p>A(y) = 1 + y^2 + y^3 + y^5 + y^7 + y^8 + y^10 + y^11 + y^13 + y^15 +
y^16 + y^18 + y^20 + …</p></li>
<li><p><strong>Relation to Fibonacci Numbers</strong>: The coefficients
of the power series correspond to the Fibonacci numbers where the index
is even (A022342).</p></li>
<li><p><strong>Continued Fraction Representation</strong>: There’s a
continued fraction representation for (1 − 1/q) A(1/q), which grows
doubly exponentially with large q values.</p></li>
</ol>
<p><strong>Pell Iteration (Section 38.12):</strong></p>
<ol type="1">
<li><p><strong>Function Definition</strong>: The function B(y) is
defined using an iterative process involving Pell numbers. It starts
with L0=1, R0=1+y, l0=y, and r0=y.</p></li>
<li><p><strong>Iteration Rules</strong>:</p>
<ul>
<li>ln+1 = rn (where rn = yFn+1)</li>
<li>rn+1 = r^2_n ln (where ln = yFn+2)</li>
<li>Ln+1 = Rn</li>
<li>Rn+1 = Rn + rn+1 Rn + r^2_n+1 Ln</li>
</ul></li>
<li><p><strong>Power Series</strong>: After the n-th step, the series in
y is correct up to order pn (where pn are Pell numbers), with a
convergence rate of √2 + 1 ≈ 2.4142. The power series representation of
B(y) is:</p>
<p>B(y) = 1 + y + y^3 + y^4 + y^6 + y^7 + y^8 + y^10 + y^11 + y^13 +
y^14 + y^15 + y^17 + y^18 + …</p></li>
<li><p><strong>Pell Palindromic Constant</strong>: The function P(y) is
defined similarly but results in a palindromic sequence, specifically
when R0=1+y+y^2, it computes 1-y.</p></li>
<li><p><strong>Continued Fraction Representation</strong>: Like the
Fibonacci case, there’s a continued fraction representation for (1 -
1/q) B(1/q), which also grows doubly exponentially with large q
values.</p></li>
</ol>
<p>Both iterations have applications in various areas of mathematics and
computer science, particularly in sequence generation and digital signal
processing due to their recursive nature and palindromic properties.</p>
<p>The text discusses the implementation of arithmetic operations modulo
m, which are essential in various fields such as cryptography, error
correcting codes, and digital signal processing. The modular arithmetic
functions include addition, subtraction, multiplication, power,
inversion, and division.</p>
<ol type="1">
<li><p><strong>Addition and Subtraction</strong>: These are
straightforward to implement using the modulus operation. If
<code>a &gt;= b</code>, then <code>sub_mod(a, b, m)</code> returns
<code>a - b</code>. Otherwise, it returns <code>m - b + a</code>. The
addition table for moduli 13 and 9 is provided in Figure
39.1-A.</p></li>
<li><p><strong>Multiplication</strong>: Multiplication modulo m is more
complex. A naive approach would limit the modulus to half of the word
size. However, a more efficient method involves using the formula:
<code>a · b = (⌊a·b/m⌋ * m + ⟨a·b⟩m) mod z</code>, where <code>z</code>
is the word size. This technique allows for larger moduli by using
floating-point arithmetic to compute the most significant bits and
integer arithmetic for the least significant bits.</p>
<p>The implementation uses a 64-bit unsigned integer type
(<code>uint64</code>) for both operands and result, and a 64-bit
floating-point type (<code>float64</code>, typically
<code>long double</code>) for intermediate calculations. It computes
<code>y = ⌊a·b/m⌋ * m</code> using floating-point arithmetic, then
calculates the residue <code>r = a·b - y</code>. If <code>r</code> is
negative, it adds <code>m</code> to get the positive residue.</p>
<p>The normalization step (line 10) is optional and can be omitted if
the modulus is less than <code>2^62</code>. For fixed moduli, the
division by <code>m</code> can be replaced with multiplication by the
modular inverse of <code>m</code>, which only needs to be computed
once.</p></li>
<li><p><strong>Power</strong>: The power operation modulo m can be
implemented using exponentiation by squaring, a method that reduces the
number of multiplications needed. This algorithm works by repeatedly
squaring the base and multiplying into the result when the current bit
of the exponent is 1.</p></li>
<li><p><strong>Inversion and Division</strong>: Modular inversion
(finding the modular multiplicative inverse) can be computed using the
extended Euclidean algorithm. Division modulo m can then be performed by
multiplying the dividend by the modular inverse of the divisor.</p></li>
</ol>
<p>These operations are crucial in various applications, including
primality tests and cryptographic algorithms that rely on modular
arithmetic. The choice of data types (64-bit integers and 64-bit
floating-point numbers) ensures efficient computation while handling
large moduli.</p>
<p>The order of an element a modulo m, denoted as ord_m(a), is the
smallest positive integer k such that a^k ≡ 1 (mod m). If no such k
exists, we say that the order is infinite. The order of any finite group
element divides the order of the group, which in modular arithmetic is
Euler’s totient function φ(m) for coprime a and m.</p>
<p>For prime moduli p, the possible orders are 1, 2, …, φ(p). For
composite moduli, the situation is more complex. The order of an element
modulo a composite number n can be found by factoring n into its prime
factors and applying the Chinese Remainder Theorem (CRT) to compute the
order modulo each prime power factor.</p>
<p>The order of an element modulo m can be computed using the following
algorithm:</p>
<ol type="1">
<li>Factorize m into its prime powers, i.e., m = p1^e1 * p2^e2 * … *
pk^ek where pi are distinct primes and ei are positive integers.</li>
<li>For each prime power pj^ej, compute ord_pj^ej(a) using the following
steps:
<ol type="a">
<li>If pj is odd, set k = 1 while (a^k) mod pj^ej ≠ 1; then ord_pj^ej(a)
= k.</li>
<li>If pj = 2 and ej &gt; 1, set k = 1 while (a^k) mod 2^(ej+1) ≠ 1;
then ord_pj^ej(a) = k * 2^(ej-1).</li>
</ol></li>
<li>Using the CRT, compute the order of a modulo m as: ord_m(a) =
lcm(ord_p1^e1(a), …, ord_pk^ek(a))</li>
</ol>
<p>This algorithm relies on the fact that the order of an element modulo
a prime power pj^ej is related to its order modulo pj. For odd primes,
the order modulo pj^ej is simply the smallest k such that a^k ≡ 1 (mod
pj^ej). For even primes, particularly 2, the situation is more
complicated due to the presence of small subgroups. In this case, we
look at the order modulo a larger power of 2 and then adjust
accordingly.</p>
<p>The orders of elements play a crucial role in number theory and
cryptography. They are used, for instance, in the Diffie-Hellman key
exchange and the ElGamal encryption system. Moreover, the order of an
element modulo m determines the size of the subgroup generated by that
element, which is essential in understanding the structure of
multiplicative groups modulo m.</p>
<p>The given text discusses the concept of order in modular arithmetic,
particularly within the context of groups. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Order (r)</strong>: For an element ‘a’ modulo ‘m’, the
order r = ord(a) is defined as the smallest positive integer such that
ar ≡ 1 (mod m). If no such integer exists, the order is undefined. The
order essentially tells us how many times we need to multiply ‘a’ by
itself to get back to 1, modulo ‘m’.</p></li>
<li><p><strong>Roots of Unity</strong>: An element ‘a’ whose r-th power
equals 1 (i.e., ar ≡ 1 (mod m)) is called an r-th root of unity. If ax ≢
1 for all x &lt; r but ar ≡ 1, then ‘a’ is a primitive r-th root of
unity. A primitive r-th root of unity generates the cyclic group of
order r under multiplication modulo m.</p></li>
<li><p><strong>Maximal Order (R(m))</strong>: The maximal order R(m) for
an element modulo m is the highest order among all elements. For prime
modulus p, R(p) = p-1. Elements of this maximal order are known as
primitive roots or generators of the multiplicative group.</p></li>
<li><p><strong>Index (i)</strong>: The factor by which the order of an
element falls short of the maximal order is called its index. If ‘r’ is
the order and ‘i’ is the index, then i * r = R(m).</p></li>
<li><p><strong>Group Theory Connection</strong>: These concepts come
from group theory. In the multiplicative group modulo m (consisting of
invertible elements under multiplication), the order refers to how often
we need to multiply an element to reach the identity (1) in this group.
The additive group (all elements under addition) has simpler orders,
being the smallest integer ‘m’ such that ma ≡ 0 (mod m).</p></li>
<li><p><strong>Prime Modulus</strong>: If the modulus m is prime (p),
then Z/pZ forms a finite field Fp = GF(p), where all non-zero elements
have inverses and division is possible. The maximal order R(p) = p - 1,
and elements of this order are called primitive roots modulo p or
generators modulo p. To test if an element ‘g’ is a primitive root, we
check whether g^((p-1)/q) ≠ 1 (mod p) for all prime divisors q of p -
1.</p></li>
<li><p><strong>Composite Modulus</strong>: For composite modulus m, the
concept becomes more complex due to the presence of non-invertible
elements. The totient function ϕ(m) counts numbers less than ‘m’ and
coprime to ‘m’. This function is multiplicative: ϕ(x1 * x2) = ϕ(x1) *
ϕ(x2) for coprime x1, x2. If m has prime factorization m = p₁^e₁ * … *
pₖ^eₖ, then |(Z/mZ)<em>| = ϕ(m). The multiplicative group (Z/mZ)</em> is
isomorphic to the direct product of groups modulo prime powers.</p></li>
<li><p><strong>Cyclic vs Non-cyclic Groups</strong>: Not all
multiplicative groups modulo m are cyclic (i.e., generated by a single
element). A group is cyclic if and only if ϕ(m) is even, thanks to a
theorem by Gauss. For example, (Z/8Z)* is not cyclic because ϕ(8) = 4,
which is even, but (Z/10Z)* is non-cyclic despite ϕ(10) = 4, due to the
presence of multiple generators of order 2.</p></li>
</ol>
<p>In essence, this text provides a comprehensive overview of order in
modular arithmetic, its relation to group theory, and how these concepts
apply differently for prime versus composite moduli. It also introduces
key functions like the totient function ϕ(m) and discusses properties of
cyclic groups within this context.</p>
<p>The text discusses various aspects of modular arithmetic, focusing on
cyclic groups, noncyclic groups, generators, and quadratic residues.
Here’s a summary of the main points:</p>
<ol type="1">
<li><p><strong>Cyclic Groups</strong>: A group (Z/mZ)* is called cyclic
if it contains an element whose powers generate all invertible elements
in the group. This maximal order is denoted as R(m). If R(m) equals the
number of invertible elements, |(Z/mZ)*| = ϕ(m), then the group is
cyclic; otherwise, it’s noncyclic.</p></li>
<li><p><strong>Non-Cyclic Groups</strong>: In noncyclic groups, no
single element generates all invertible elements. The given example
(Z/15Z)* shows such a case where no element of maximal order can
generate all units.</p></li>
<li><p><strong>Maximal Order Calculation</strong>: Algorithms are
provided for computing the maximal order R(m) and finding an element of
maximal order in a group (Z/mZ)*:</p>
<ul>
<li><code>maxorder(m)</code> computes R(m).</li>
<li><code>order(x, m)</code> calculates the order of a given element x
in (Z/mZ)*.</li>
<li><code>maxorder_element(m)</code> finds an element in (Z/mZ)* with
maximal order.</li>
</ul></li>
<li><p><strong>Generators</strong>: In cyclic groups, generators are
elements whose powers cycle through all group elements. The number of
generators is ϕ(ϕ(n)). For prime moduli p, any generator modulo p is
also a generator modulo 2^k * p for k ≥ 1 if it’s odd; otherwise, g + pk
is a generator.</p></li>
<li><p><strong>Quadratic Residues</strong>: These are values ‘a’ that
satisfy the equation x² ≡ a (mod p), where p is prime. The quadratic
residues can be determined using the Legendre symbol, which has
properties like the law of quadratic reciprocity and relations for
specific arguments.</p></li>
<li><p><strong>Kronecker Symbol</strong>: A generalization of the
Legendre symbol for composite moduli, allowing us to determine whether a
number is a quadratic residue modulo a composite modulus.</p></li>
<li><p><strong>Square Roots Modulo m</strong>: Algorithms are presented
for computing square roots modulo primes (p = 4k + 3), prime powers
(pe), and composites:</p>
<ul>
<li>For primes p = 4k + 3, the square root is given by ±a((p+1)/4) mod
p.</li>
<li>For prime powers pe with e ≥ 2, Newton’s iteration can be used. The
case p = 2 requires a separate treatment.</li>
</ul></li>
</ol>
<p>These concepts and algorithms are crucial in various areas of number
theory and cryptography.</p>
<p>The text discusses the Rabin-Miller test, a probabilistic method used
to prove the compositeness of an integer. The test is based on strong
pseudoprimes (SPPs), composite numbers for which certain properties hold
regarding modular exponentiation with respect to a base ‘a’.</p>
<ol type="1">
<li><p><strong>Pseudoprimes and Strong Pseudoprimes:</strong> A number n
is called a pseudoprime to base a if <code>an-1 ≠ 1 (mod n)</code>. If
<code>an-1 = 1 (mod n)</code>, then n is said to be a strong pseudoprime
to base a. Carmichael numbers are composite numbers that are
pseudoprimes to all bases relatively prime to them.</p></li>
<li><p><strong>Rabin-Miller Test for Compositeness:</strong> This test
uses the concept of strong pseudoprimes to prove compositeness with high
probability. It involves examining the sequence
<code>b, b^2, ..., b^(2t)</code>, where <code>n-1 = q*2^t</code> and
<code>q</code> is odd. If neither of the conditions (1)
<code>b ≡ 1 (mod n)</code> nor (2) <code>b^e ≡ -1 (mod n)</code> for
some <code>0 &lt;= e &lt; t</code> hold, then n is composite.</p></li>
<li><p><strong>Algorithm Steps:</strong></p>
<ol type="a">
<li>Compute <code>n-1 = q * 2^t</code>.</li>
<li>Initialize <code>b = a^q (mod n)</code>.</li>
<li>If <code>b == 1</code>, return true (n is probably prime).</li>
<li>For <code>e</code> from 0 to <code>t-1</code>:
<ul>
<li>Update <code>b = b^2 (mod n)</code>.</li>
<li>If <code>b == n-1</code>, break the loop.</li>
</ul></li>
<li>If <code>b != n-1</code>, return false (n is composite).</li>
</ol></li>
<li><p><strong>Probability of Error:</strong> For a composite number,
the probability that it passes the Rabin-Miller test for a single base
is at most 1/4. Therefore, by testing multiple bases, we can
significantly increase our confidence in the compositeness of a
number.</p></li>
<li><p><strong>Efficiency and Restrictions:</strong> While the test does
not guarantee primality, it effectively rules out compositeness with
high probability. To speed up the test for small numbers (e.g., n &lt;
2^32), one can restrict base testing to common bases like 2, 3, and 5,
and use lookup tables for known composite SPPs.</p></li>
<li><p><strong>Extensions:</strong> The concept of strong pseudoprimes
extends beyond two bases. Numbers that are SPP to several chosen bases
or even all prime bases up to a certain limit can be constructed.
However, a number being an SPP to bases <code>a1</code> and
<code>a2</code> does not necessarily imply it’s an SPP for the base
<code>a1*a2</code>.</p></li>
</ol>
<p>In summary, the Rabin-Miller test is a powerful tool in computational
number theory for distinguishing prime from composite numbers
probabilistically. Its effectiveness lies in its ability to rule out
compositeness with high confidence using modular exponentiation and
carefully chosen base sequences.</p>
<p>The text describes several methods for proving primality, which are
primarily used to demonstrate the primality of numbers that satisfy
specific conditions. Here’s a detailed explanation of each method:</p>
<ol type="1">
<li><p><strong>Pratt’s Certiﬁcate of Primality</strong>: This method
requires the factorization of n-1 and the determination of a primitive
root modulo p (where p is prime). The certiﬁcate is represented as a
tree, where each level corresponds to a prime factor of n-1. Each leaf
node provides primality proof for its parent, with increasing levels
representing successive factorizations. This method is only practical
for numbers of special forms due to the complexity of computing large
factorizations.</p></li>
<li><p><strong>Pocklington-Lehmer Test</strong>: This test requires
knowledge of a partial factorization of n-1. It checks whether, for each
prime factor q of F (where F = n - 1), there exists an integer a such
that a^((n-1)/q) ≡ 1 (mod n) and gcd(a^((p-1)/q) - 1, p) = 1. If this
condition holds for all prime factors, then n is prime. This test can be
applied to numbers of the form p = F * U + 1, where F’s factorization is
known, and U &lt; F.</p></li>
<li><p><strong>Tests for n = k 2^t + 1 (Proth’s Theorem)</strong>: If
there exists an integer a such that a^((n-1)/2) ≡ -1 (mod n), then n
must be prime (Proth’s theorem). This test is particularly useful for
“Fermat numbers” of the form Fn = 2<sup>(2</sup>n) + 1, where it
suffices to check if 3^x ≡ -1 (mod Fn) with x = 2^(t-1).</p></li>
<li><p><strong>Tests for n = k 2^t −1 (Lucas-Lehmer Test)</strong>: The
Lucas-Lehmer test is used to prove primality of Mersenne numbers, i.e.,
numbers of the form n = 2^p - 1 where p is prime. It states that if
H_(2^p - 2) ≡ 0 (mod n), then n is prime, where H is a sequence defined
recursively as H_0 = 1, H_1 = 2 and H_k = 4H_(k-1) - H_(k-2). The test
can be efficiently computed using the index doubling formula.</p></li>
<li><p><strong>Lucas Test</strong>: This generalization of the
Lucas-Lehmer test applies to numbers n = k*2^t - 1 where k is odd, 2^t
&gt; k, and k ≠ 0 (mod 3), ensuring that n ≡ 1 (mod 3). If H_(n+1)/4 ≡ 0
(mod n), then n is prime. The sequence H can be computed using the index
doubling formula, similar to the Lucas-Lehmer test.</p></li>
</ol>
<p>Each of these methods has its own strengths and limitations, and
their applicability depends on the specific form of the number being
tested for primality. These tests build upon the properties of modular
arithmetic and number theory, particularly concerning orders of elements
in modular rings and cyclotomic polynomials.</p>
<p>Title: Complex Modulus: The Field GF(p^2)</p>
<p>This section discusses the concept of complex modulus in the context
of finite fields, specifically GF(p^2), where p is a prime number. It
explores the construction and properties of this field, focusing on
efficient reduction methods for certain quadratic polynomials.</p>
<ol type="1">
<li><p><strong>Construction of Complex Numbers:</strong> The standard
way to construct complex numbers involves taking pairs of real numbers
(a, b) = a + bi, with component-wise addition and multiplication defined
by:</p>
<ul>
<li>Addition: (a, b) + (c, d) = (a + c, b + d)</li>
<li>Multiplication: (a, b)(c, d) = (ac - bd, ad + bc)</li>
</ul>
<p>This construction forms a field.</p></li>
<li><p><strong>Complex Finite Fields GF(p^2):</strong> When the ground
field is the integers modulo a prime p (GF(p)), and an irreducible
polynomial c(x) of degree n with coefficients in GF(p), we obtain an
extension field GF(p^n). For binary fields, GF(2^n) is treated in
Chapter 42.</p></li>
<li><p><strong>Irreducible Polynomials:</strong> The construction
depends on the choice of irreducible polynomial c(x). In particular, for
primes p = 4k + 3 (where -1 is a quadratic non-residue), x^2 + 1 is
irreducible, leading to GF(p^2) being denoted by GF((4k+3)^2). For p =
4k + 1, the polynomial x^2 + x + 1 can be used instead.</p></li>
<li><p><strong>Multiplication Efficiency:</strong> Certain quadratic
polynomials allow for efficient multiplication methods that require
fewer scalar multiplications and additions than standard multiplication
algorithms. These include:</p>
<ul>
<li>C = x^2 + 1 (reducible modulo p = 4k + 3): Multiplication costs
three real multiplications and five real additions.</li>
<li>C = x^2 + d (irreducible if −d is not a square): Multiplication also
costs three scalar multiplications when multiplication by d is
cheap.</li>
</ul></li>
<li><p><strong>Primitive Roots:</strong> For primes p with the lowest k
bits set, the maximal order in GF(p^2) equals N = 2^(k+1). An algorithm
exists for constructing primitive 2^j-th roots in GF(p^2) for j = 2, 3,
…, a (where 2a is the largest power of 2 dividing p^2 - 1), outlined in
[149].</p></li>
<li><p><strong>Mersenne Primes:</strong> For Mersenne primes p = 2^e -
1, an element of order 2^(e+1) (in GF(p^2) with field polynomial x^2 +
1) can be constructed more directly by computing √-3 and 1/√2 without
modular reduction.</p></li>
</ol>
<p>In summary, this section introduces the concept of complex modulus in
finite fields, specifically GF(p^2), and discusses efficient
multiplication methods for certain quadratic polynomials. It also
provides algorithms for constructing primitive roots within these
fields, with special consideration given to Mersenne primes.</p>
<p>The text discusses methods to solve the Pell equation, which is a
Diophantine equation of the form x^2 - D*y^2 = ±1, where D is a positive
nonsquare integer. The solutions are sought for integers x and y.</p>
<p><strong>39.13.1 Solution via Continued Fractions:</strong></p>
<p>This method uses simple continued fractions to find the solutions of
the Pell equation (x^2 - D<em>y^2 = +1) and similar equations with
negative signs. The key idea is that the convergents of the continued
fraction of √D are close approximations to √D, and their squares (P_k^2
- D</em>Q_k^2) give the values e_k.</p>
<p>For a given D, the continued fraction of √D can be calculated using
an efficient algorithm for square roots (mentioned but not detailed in
the text). Once we have the convergents P_k/Q_k and their corresponding
e_k = P_k^2 - D*Q_k^2, we look for the smallest non-trivial k such that
e_k equals +1 or -1.</p>
<p>For example, if D = 53: - CF(√53) = [7, 3, 1, 1, 3, 14, …] (with
period length 6), - The smallest non-trivial solution for x^2 -
53<em>y^2 = +1 is at k=9: P_8^2 - 53</em>Q_8^2 = +1.</p>
<p>This method works for all nonsquare positive integers D, and it’s
particularly efficient when D is a prime of the form 4k+1 or has no
factors of the form 4k+3.</p>
<p><strong>39.13.2 Multiplying and Powering Solutions:</strong></p>
<p>Given two solutions (x, y) and (r, s) to Pell equations with signs e
and f: - x^2 - D<em>y^2 = e, - r^2 - D</em>s^2 = f,</p>
<p>we can derive new solutions by multiplying these pairs: - (U, V) :=
(x<em>r + D</em>y<em>s, x</em>s + y*r), which is also a solution with
sign ef.</p>
<p>We can also raise solutions to any power using matrix exponentiation.
Define the matrix M = [x, D*y; y, x] and its k-th power M^k. Then the
k-th power of (x, y) is given by (X_k, Y_k) = M^k * [1; 0].</p>
<p>For example, if r^2 - D<em>s^2 = +1 (the smallest nontrivial
solution), then x_k^2 - D</em>y_k^2 = e for all odd k. Moreover, if r^2
- D<em>s^2 = -1, then x_k^2 - D</em>y_k^2 = -1 for all odd k.</p>
<p>This method allows us to generate an infinite sequence of solutions
from a single pair by multiplication and matrix exponentiation,
providing a systematic way to find higher-index solutions to the Pell
equation.</p>
<p>The text discusses various aspects related to hypercomplex numbers,
specifically focusing on algebras, multiplication tables, and the
Cayley-Dickson construction. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Algebra</strong>: An algebra is an n-dimensional vector
space (over a field) equipped with component-wise addition and a
multiplication table that defines the product of any two components. The
product of elements x = ∑αk ek and y = ∑βj ej is given by x · y = ∑k,j
[(αk · βj) mk,j], where mk,j = ek ej are arbitrary elements from the
algebra (potentially linear combinations of components ei).</p></li>
<li><p><strong>Multiplication Table</strong>: A multiplication table for
an algebra can be arbitrary, but in this context, we’re interested in
algebras over the real numbers where the product of two components
equals ±1 times another component. For example, complex numbers are a
2-dimensional algebra with the familiar multiplication table:</p>
<p>0 1 0: +0 +1 1: +1 -0</p></li>
<li><p><strong>Cayley-Dickson Construction</strong>: This construction
recursively defines multiplication tables for certain algebras where the
dimension is a power of 2. Given elements a, A, b, and B from a
(2n−1)-dimensional algebra U, define the multiplication rule for an
algebra V (of dimension 2n) as:</p>
<p>(a, b) · (A, B) := (a · A - B · b<em>, a</em> · B + A · b), where b*
is the conjugate of b.</p>
<p>The construction ensures that ek· ej = ±ek for any two nonzero
components k and j (with k ≠ j).</p></li>
<li><p><strong>Signs in Multiplication Table</strong>: Figures 39.14-A
and B illustrate multiplication tables and sign patterns for sedenions,
an example of a 16-dimensional algebra obtained via the Cayley-Dickson
construction. The multiplication table exhibits partial antisymmetry,
meaning ek· ej = -ej· ek whenever both k and j are nonzero (and k ≠
j).</p></li>
<li><p><strong>Fast Quaternion Multiplication</strong>: The text
mentions a method for multiplying quaternions using the dyadic
convolution scheme with eight real multiplications. This involves
initializing four temporary results (c0, c1, c2, c3), performing a
length-4 dyadic convolution, and then correcting the results through
normalization and additional multiplications.</p></li>
<li><p><strong>Eight-Square Identity</strong>: This identity relates to
matrices C, A, and B as shown in Figure 39.14-F. The sum of products of
squared components from vectors A and B equals the sum of squared
components of vector P (obtained by matrix multiplication CA).</p>
<p>Σk=0^(n-1) Ak^2 · Σk=0^(n-1) Bk^2 = Σk=0^(n-1) Pk^2</p></li>
</ol>
<p>This identity is useful in simplifying certain mathematical
expressions and algorithms involving squared vectors or matrices.</p>
<p>The text discusses various arithmetic operations with Binary
Polynomials, which are polynomials with coefficients in the finite field
GF(2) (also known as Z/2Z). Here’s a detailed summary and explanation of
the key points:</p>
<ol type="1">
<li><p><strong>Basic Arithmetic Operations:</strong></p>
<ul>
<li>Addition is performed using the XOR operation, identical to regular
polynomial addition but with coefficients reduced modulo 2.</li>
<li>Subtraction is also equivalent to addition in this context.</li>
<li>Multiplication by x (the independent variable) is a left shift
operation, similar to integer multiplication but without carry.</li>
</ul></li>
<li><p><strong>Multiplication and Squaring:</strong></p>
<ul>
<li>Multiplication of two binary polynomials A and B is identical to
usual polynomial multiplication except that no carry occurs. The
provided code snippet performs this operation using bitwise
operations.</li>
<li>Squaring a binary polynomial involves shifting the bits from
position k to position 2k, effectively doubling each exponent.</li>
</ul></li>
<li><p><strong>Optimization:</strong></p>
<ul>
<li>The routines for squaring and multiplication can be optimized by
partially unrolling loops to avoid branches, which speeds up
execution.</li>
<li>A bit-zip function (bit_zip0) can also be used for squaring,
provided that the upper half of the bits of the argument are zero.</li>
</ul></li>
<li><p><strong>Exponentiation:</strong></p>
<ul>
<li>Binary exponentiation is implemented using a loop that repeatedly
squares the base and multiplies when necessary. Overflow may occur for
large exponents.</li>
</ul></li>
<li><p><strong>Quotient and Remainder:</strong></p>
<ul>
<li>The remainder (R) of A modulo B can be computed by initializing A =
a, then subtracting B multiplied by the highest power of x less than or
equal to deg(A). This process continues until deg(B) &gt; deg(A).</li>
<li>The quotient (Q) is computed similarly but without discarding the
remainder.</li>
</ul></li>
<li><p><strong>Greatest Common Divisor (GCD):</strong></p>
<ul>
<li>The binary GCD algorithm uses a loop to repeatedly reduce both
numbers by their highest power of 2 until they become odd, then applies
a series of shifts and subtractions to find the GCD.</li>
</ul></li>
<li><p><strong>Exact Division:</strong></p>
<ul>
<li>This method leverages the relation C = (1 + Y)(1 + Y^2)…(1 + Y^(2n))
mod x^(2n+1) for binary polynomials C with constant term 1. It uses
shifts and subtractions to compute R = A/C when A is an exact multiple
of C, making it efficient when the number of nonzero coefficients in C-1
(k) is small.</li>
</ul></li>
</ol>
<p>The primary application of these operations is in the study and
implementation of Linear Feedback Shift Registers (LFSRs), which will be
covered in Chapter 41. The binary polynomial arithmetic forms the
foundation for computations within binary finite fields, as discussed in
Chapter 42.</p>
<p>The text discusses various algorithms for performing arithmetic
operations with binary polynomials over the field GF(2), where
arithmetic is performed using XOR (exclusive or) for addition and
subtraction, and a bitwise reduction for multiplication. Here’s a
detailed summary and explanation of the key points:</p>
<ol type="1">
<li><p><strong>Binary Polynomial Representation</strong>: Binary
polynomials are represented as a sum of powers of x, where each
coefficient is either 0 or 1. For example, A = a3<em>x^3 + a2</em>x^2 +
a1*x + a0.</p></li>
<li><p><strong>Bitwise Operations for Arithmetic</strong>:</p>
<ul>
<li>Addition and subtraction are performed using XOR (⊕): A ⊕ B = C
(i.e., a ⊕ b = c).</li>
<li>Multiplication is performed by shifting left and subtracting the
modulus if the shifted-out bit is 1, utilizing an auxiliary variable ‘h’
representing the highest set bit of the modulus.</li>
</ul></li>
<li><p><strong>Multiplication Routine</strong>: The multiplication
routine <code>bitpolmod_mult(a, b, c, h)</code> computes (A * B) mod C
by repeatedly performing XOR and shifting operations based on the bits
of ‘b’.</p></li>
<li><p><strong>Squaring Optimization</strong>: Squaring can be optimized
using a precomputed table of residues x^2k mod C for various k, reducing
the number of multiplications required.</p></li>
<li><p><strong>Exponentiation Algorithms</strong>:</p>
<ul>
<li>Right-to-left powering algorithm
(<code>bitpolmod_power(a, e, c, h)</code>) uses repeated squaring and
multiplication to compute (A ^ e) mod C.</li>
<li>Left-to-right powering algorithm provides an alternative approach
using a different loop structure.</li>
</ul></li>
<li><p><strong>Division by x</strong>: Division by x is possible if the
modulus has a nonzero constant term, allowing for efficient computation
of (A / x) mod C using bitwise operations. The inverse of x can also be
computed efficiently under this condition.</p></li>
<li><p><strong>Extended GCD (EGCD) Method</strong>: The method to
compute EGCD (bitpol_egcd(u, v, iu, iv)) remains the same as in section
39.1.4 on page 767, using a standard Euclidean algorithm with bitwise
operations for polynomial division and coefficient updates.</p></li>
<li><p><strong>High-degree Polynomials</strong>: For very high-degree
binary polynomials, FFT-based methods can be used, but simpler splitting
schemes (Karatsuba, Toom-Cook) are generally more efficient unless the
cost of multiplication is significantly higher than addition.</p></li>
<li><p><strong>Toom-Cook Algorithms for Binary Polynomials</strong>:
These algorithms use only constants 0 and 1 in their splitting schemes,
making them suitable for binary polynomial arithmetic over GF(2). The
3-way and 4-way splitting methods are provided for multiplication of
binary polynomials with degrees 3N and 4N, respectively.</p></li>
</ol>
<p>These techniques enable efficient computation with binary
polynomials, particularly when working with high-degree polynomials
where the naive O(N^2) approach becomes impractical due to its quadratic
time complexity.</p>
<p>The text discusses various aspects related to binary polynomials,
focusing on irreducible and primitive polynomials, their properties, and
methods for testing their irreducibility and primitivity.</p>
<ol type="1">
<li><p><strong>Binary Polynomials</strong>: These are polynomials with
coefficients in the binary field GF(2), where addition is equivalent to
XOR, and multiplication is equivalent to AND or bitwise shift
operations.</p></li>
<li><p><strong>Irreducible Polynomials</strong>: A polynomial is
irreducible if it has no nontrivial factors (factors other than 1 and
itself). For binary polynomials, having a nonzero constant term makes
the polynomial reducible because x is always a factor. Similarly, if the
number of nonzero coefficients (of odd degree) is even for binary
polynomials, they are reducible due to the presence of the factor x +
1.</p></li>
<li><p><strong>Testing for Irreducibility</strong>:</p>
<ul>
<li><strong>Ben-Or Test</strong>: This test checks if
gcd(x<sup>(2</sup>k)-x mod C, C) ≠ 1 for any k &lt; d, where d is the
degree of the polynomial. It suffices to check only the first ⌊d/2⌋
tests because a factor of degree f implies another one of degree
d-f.</li>
<li><strong>Rabin’s Test</strong>: This test checks two conditions:
x<sup>(2</sup>d) ≡ x (mod C) and, for all prime divisors pi of d,
gcd(x<sup>(2</sup>d/pi)-x mod C, C) = 1. The test involves GCD
computations and squarings to update the power of x.</li>
<li><strong>Strong Pseudo-Irreducible (SPI) Test</strong>: This test
checks if a polynomial has no linear factors, x<sup>(2</sup>k) ≠ x for
all k &lt; d, and x<sup>(2</sup>d) = x without needing GCD
computations.</li>
</ul></li>
<li><p><strong>Primitive Polynomials</strong>: A polynomial is primitive
if its order (period) is maximal, which means the powers of x generate
all nonzero binary polynomials of degree less than the polynomial’s
degree. Primitivity implies irreducibility but not vice versa.</p></li>
<li><p><strong>Testing for Primitivity</strong>: The text mentions an
algorithm to determine whether a given irreducible binary polynomial is
primitive by computing the order of its root using modular
exponentiation. The provided GP language code and C++ implementations
demonstrate this method.</p></li>
<li><p><strong>Möbius Function (µ)</strong>: This function appears in
the formula for calculating the number of irreducible polynomials of
degree n, which involves summing over all divisors d of n weighted by
µ(n/d). The Möbius function is defined via a specific recurrence
relation.</p></li>
</ol>
<p>The text also includes tables and figures displaying the number of
irreducible and primitive binary polynomials up to degrees 40. These
counts are essential in various applications, such as coding theory and
finite field arithmetic.</p>
<p>This text discusses various aspects of binary polynomials, focusing
on irreducible, primitive, self-reciprocal, and trinomial polynomials.
Here’s a detailed summary:</p>
<ol type="1">
<li><strong>Lyndon Words and Irreducible Polynomials:</strong>
<ul>
<li>Lyndon words are non-increasing sequences of symbols that are
strictly smaller than any of their proper prefixes. The formula for the
number of Lyndon words (In) is given as In = 2^(n-2)/n when n is prime.
This relationship is based on relation 18.3-2 from page 380, and the
sequence is entry A001037 in [312].</li>
<li>For large degrees n, the probability that a randomly chosen
polynomial is irreducible is about 1/n. However, for polynomials in two
or more variables, this probability tends to 1 as n increases.</li>
</ul></li>
<li><strong>Primitive Binary Polynomials:</strong>
<ul>
<li>The number of primitive binary polynomials (Pn) of degree n is given
by Pn = ϕ(2^n - 1)/n, where ϕ denotes Euler’s totient function. If n is
the exponent of a Mersenne prime, then Pn equals In (the number of
irreducible polynomials).</li>
<li>Values for Pn up to degree 40 are provided in Figure 40.6-B and are
entry A011260 in [312].</li>
</ul></li>
<li><strong>Irreducible Non-Primitive Polynomials:</strong>
<ul>
<li>The difference Dn = In - Pn represents the number of irreducible
non-primitive polynomials. If n is a Mersenne prime exponent, then Dn
equals 0. A list of these polynomials up to degree 12 is provided in
[FXT: data/all-nonprim-irredpoly.txt].</li>
</ul></li>
<li><strong>Transformations Preserving Irreducibility:</strong>
<ul>
<li>Reciprocal Polynomial: The reciprocal of an irreducible polynomial
remains irreducible, with the order preserved. The reciprocal can be
calculated using bit manipulation operations.</li>
<li>Composition with x + 1: If a polynomial p(x) is irreducible, then
p(x + 1) (the composition with x + 1) is also irreducible, although this
transformation does not necessarily preserve the order of the
polynomial.</li>
</ul></li>
<li><strong>Self-Reciprocal Polynomials:</strong>
<ul>
<li>Self-reciprocal polynomials are those that are equal to their own
reciprocals. These can be generated from irreducible polynomials with
nonzero linear coefficients using specific transformations. The
self-reciprocal polynomials of degree 2n are factors of x<sup>(2</sup>n)
+ 1, and the order of a self-reciprocal polynomial of degree 2n is a
divisor of 2^n + 1.</li>
</ul></li>
<li><strong>Irreducible and Primitive Polynomials of Special
Forms:</strong>
<ul>
<li>The text provides lists of irreducible and primitive polynomials for
low degrees (up to 11) and trinomials (polynomials with exactly three
non-zero coefficients) for even lower degrees (up to 49). These lists
are available in the associated data files mentioned.</li>
</ul></li>
<li><strong>Notation:</strong>
<ul>
<li>PP: Primitive Polynomial</li>
<li>SRP: Self-Reciprocal Polynomial</li>
<li>µ(d): Möbius function, a multiplicative function in number theory
used to count the square-free integers up to a given integer.</li>
</ul></li>
</ol>
<p>The text discusses various types of irreducible polynomials over the
Galois Field GF(2), which consists of two elements, 0 and 1. These
polynomials have applications in error-correcting codes, cryptography,
and digital signal processing. Here’s a detailed explanation of the
topics discussed:</p>
<ol type="1">
<li><p><strong>Irreducible Trinomials</strong>: A trinomial is an
expression with three terms. In this context, we’re looking at
polynomials of the form <code>x^n + x^k + 1</code>, where <code>n</code>
and <code>k</code> are integers. The text provides a list of irreducible
trinomials for degrees up to 49. It also mentions Swan’s theorem
regarding conditions under which such trinomials are reducible.</p></li>
<li><p><strong>Primitive Trinomials</strong>: A primitive polynomial is
an irreducible polynomial whose roots generate the multiplicative group
of the finite field it defines. In other words, all non-zero elements in
the field can be expressed as a power of a root of the polynomial. The
text provides lists of primitive trinomials for different degrees and
mentions that no irreducible trinomial exists for primes
<code>n ≡ 13 (mod 24)</code> or <code>n ≡ 19 (mod 24)</code>.</p></li>
<li><p><strong>Irreducible Pentanomials</strong>: A pentanomial is a
polynomial with exactly five non-zero coefficients. The text mentions
that irreducible primitive pentanomials exist for all degrees
<code>n &gt;= 5</code>, but this has not been proven yet. It provides
examples of such polynomials for small degrees.</p></li>
<li><p><strong>Primitive Minimum-Weight and Low-Bit
Polynomials</strong>: These are special types of primitive polynomials
with certain properties related to their weights (sum of absolute values
of coefficients) and the positions of non-zero coefficients. The text
provides lists and descriptions of such polynomials for different
conditions.</p></li>
<li><p><strong>All Primitive Low-Bit Polynomials for Certain
Degrees</strong>: These are primitive polynomials where only a few bits
(coefficients) are set to 1, and these bits are as low as possible in
the representation of the degree. The text provides lists for specific
degrees, such as 256.</p></li>
<li><p><strong>Primitive Low-Block Polynomials</strong>: A low-block
polynomial has a special form where most coefficients are zero except
for a block of consecutive non-zero coefficients at the end. These
polynomials exist for many degrees and are easy to store in an array due
to their structure.</p></li>
<li><p><strong>Irreducible All-Ones Polynomials</strong>: An all-ones
polynomial is one where every coefficient (except possibly the leading
one) is 1. These polynomials are irreducible when the degree plus one is
a prime number for which 2 is a primitive root. The text provides a list
of such primes and degrees up to 400 where these polynomials are
irreducible.</p></li>
<li><p><strong>Irreducible Alternating Polynomials</strong>: These are
polynomials of the form <code>1 + x + x^3 + ... + x^(2d+1)</code>, which
can be irreducible only when <code>d</code> is odd. The text provides a
list up to degree 1000.</p></li>
<li><p><strong>Primitive Polynomials with Uniformly Distributed
Coefficients</strong>: These are primitive polynomials where the
coefficients are (roughly) equally spaced. Lists for degrees from 9 to
660 are provided in [278].</p></li>
<li><p><strong>Irreducible Self-Reciprocal Polynomials</strong>: A
self-reciprocal polynomial is one that remains unchanged when its
coefficients are reversed and <code>n</code> (the degree) is replaced by
<code>(n+1)/2</code>. The text provides a list of all irreducible
self-reciprocal polynomials up to degree 22.</p></li>
<li><p><strong>Generating Irreducible Polynomials from Lyndon
Words</strong>: This section discusses an algorithm that generates
irreducible polynomials from Lyndon words, which are special types of
strings with certain properties. The algorithm uses a primitive
polynomial and a generator to construct a new polynomial based on powers
of the generator modulo the given polynomial.</p></li>
</ol>
<p>The text also mentions various files containing lists of these
polynomials for different conditions, such as [FXT:
data/all-trinomial-primpoly.txt], [FXT: data/pentanomial-primpoly.txt],
etc., which can be found in the referenced sources.</p>
<p>Linear Feedback Shift Registers (LFSR) are a type of shift register
used for generating pseudorandom sequences, particularly in digital
systems. They operate based on the principles of modular arithmetic with
binary polynomials, as discussed in section 40.3.</p>
<p>The fundamental structure of an LFSR consists of a shift register
with feedback connections. The register contains bits (usually denoted
as ‘w’) that shift leftward with each clock pulse. The rightmost bit is
typically set to zero during initialization.</p>
<p>In an LFSR, the feedback connection involves combining (or “tapping”)
certain bits from the shift register and feeding them back into the
input. This combination is performed using exclusive-OR (XOR)
operations, symbolized by the plus sign (+). The tap positions and the
coefficients used in this process define the specific type of LFSR.</p>
<p>The crucial component of an LFSR is its characteristic polynomial,
represented as c = 1 + aT + a^2 T^2 + … + a^k T^k, where ‘a’ represents
the taps (feedback positions), and ‘T’ signifies the left shift
operator. For instance, in Fig. 41.1, the degree of the polynomial is 4,
with coefficients c = {1, 0, 1, 1}, corresponding to tap positions at
bits 1, 2, and 4 from the right.</p>
<p>The sequence generated by an LFSR repeats after a certain number of
steps, known as its period. The maximum possible length for an LFSR of
degree ‘n’ is 2^n - 1. Sequences with this maximal length are called
m-sequences and exhibit optimal statistical properties.</p>
<p>LFSRs are versatile in their applications, including random number
generation, error detection (through Cyclic Redundancy Check, CRC),
spread spectrum communication protocols, and hardware testing. Their
simplicity allows for efficient hardware implementation, making them
popular choices in digital systems where pseudorandom sequences are
required.</p>
<p>This text discusses Linear Feedback Shift Registers (LFSRs), a
mechanism used to generate shift register sequences (SRS). LFSRs are
based on the principle of shifting bits and conditionally feeding back
certain bits, determined by a primitive polynomial C. This polynomial is
also known as the connection polynomial.</p>
<ol type="1">
<li><strong>Linear Feedback Shift Register (LFSR) Basics</strong>:
<ul>
<li>An SRS is generated by computing Ak = x^k mod C for k = 0, 1, …,
2n-1 and setting bit k of the SRS to the least significant bit of
Ak.</li>
<li>If C is a primitive polynomial of degree n, the SRS will contain all
nonzero words of length n and will also cycle through all nonzero words
when a word W is updated by left shifting and adding the bit of the
SRS.</li>
</ul></li>
<li><strong>Efficient Generation of SRS</strong>:
<ul>
<li>A simple way to generate an SRS is by repeatedly dividing (XOR’ing)
by x, shifting left, and conditionally XOR-ing with C.</li>
</ul></li>
<li><strong>LFSR Implementation in C++</strong>:
<ul>
<li>The provided code shows a class <code>lfsr</code> that implements
the LFSR mechanism, generating a shift register sequence based on a
primitive polynomial of degree n. The period of the SRS is 2^n - 1.</li>
</ul></li>
<li><strong>Galois and Fibonacci Setup</strong>:
<ul>
<li>Galois setup involves shifting left and conditionally XOR-ing with C
if the shifted out bit is 1.</li>
<li>Fibonacci setup involves shifting right and conditionally XOR-ing
with a shifted version of C (C &gt;&gt; 1) if the rightmost bit was
originally 1.</li>
</ul></li>
<li><strong>Cyclic Redundancy Check (CRC)</strong>:
<ul>
<li>CRCs are hash functions that generate binary words of fixed length
as hash values. They are used for error detection in data transmission
and storage.</li>
<li>The CRC function computes h = s mod c, where s is the binary
polynomial corresponding to the input sequence and c is a binary
primitive polynomial.</li>
<li>A C++ implementation of 64-bit CRC (class <code>crc64</code>) is
provided, with methods for initializing, resetting, setting internal
state, feeding bits or bytes into the CRC, and computing the CRC
value.</li>
</ul></li>
<li><strong>Optimization via Lookup Tables</strong>:
<ul>
<li>To speed up CRC computation, especially for larger word sizes,
lookup tables can be used. These tables store precomputed values of
specific shifts and XOR operations with C. The size of the table is 2^n
(for n bits), and its entries are computed during initialization based
on the polynomial C.</li>
</ul></li>
</ol>
<p>In summary, this text explains the Linear Feedback Shift Register
mechanism, its implementation in C++, and the use of CRCs for error
detection. It also discusses how lookup tables can optimize CRC
computation by precomputing shift and XOR operations with the primitive
polynomial.</p>
<p>The text discusses various aspects of shift register sequences,
focusing on Linear Feedback Shift Registers (LFSRs), Feedback Carry
Shift Registers (FCSRs), and their applications in generating cyclic
redundancy checks (CRCs) and De Bruijn Sequences (DBS).</p>
<ol type="1">
<li><strong>Linear Feedback Shift Registers (LFSRs):</strong>
<ul>
<li>LFSRs are used for generating pseudo-random sequences based on a
linear recurrence relation. The generation is done using a primitive
polynomial of degree n, which ensures the maximal length sequence.</li>
<li>The primitive polynomial’s coefficients determine the feedback
connections in the register. For example, with a 4-bit register and
primitive polynomial x^4 + x + 1 (or 1001), bits are fed into the
leftmost position using XOR operations based on the polynomial’s
coefficients.</li>
<li>The text presents an optimized routine for CRC computation using
lookup tables in LFSRs, significantly improving speed when table sizes
of 16 or 256 words are used.</li>
</ul></li>
<li><strong>Parallel CRCs:</strong>
<ul>
<li>A more efficient method for checksumming is computing CRCs for each
bit of the input word in parallel. The ‘pcrc64’ class example
demonstrates this approach using an array of 64 words, with a Fibonacci
setup and the pentanomial x^64 + x^4 + x^3 + x^2 + 1 as the primitive
polynomial.</li>
</ul></li>
<li><strong>Generating all revbin pairs:</strong>
<ul>
<li>The text explains how to generate all nonzero revbin (reflected
binary) pairs using an LFSR with a primitive polynomial c and its
reciprocal cr. This is achieved by computing the next pair using a
function ‘revbin_next’.</li>
</ul></li>
<li><strong>The number of m-sequences and De Bruijn sequences:</strong>
<ul>
<li>An m-sequence (maximal length sequence) is generated when the LFSR’s
polynomial is primitive. The total number of such sequences equals Pn =
ϕ(2^n − 1)/n, where ϕ denotes Euler’s totient function.</li>
<li>De Bruijn Sequences (DBS) are obtained by inserting a single zero in
the longest run of zeros from an m-sequence. The number of DBSs for
given n is Sn = 2^(2n−1 − n).</li>
</ul></li>
<li><strong>Auto-correlation of m-sequences:</strong>
<ul>
<li>An auto-correlation function (ACF) is defined for a sequence M,
where Sk = +1 if Mk = 1 and -1 otherwise. For an m-sequence, this ACF
has Cτ = L (length of the sequence) when τ = 0 and -1 for other values
of τ. Most truncated De Bruijn sequences do not have this property.</li>
<li>Specific sequences constructed with primes q can have an
auto-correlation satisfying Cτ = L−1 if τ = 0 and −1 otherwise. These
sequences can be used in Hadamard matrix construction (Chapter 19).</li>
</ul></li>
<li><strong>Feedback Carry Shift Registers (FCSRs):</strong>
<ul>
<li>FCSRs are the modulo counterpart of LFSRs, using a prime c with
primitive root 2. The powers of 2 modulo c run through all nonzero
values less than c.</li>
<li>An FCSR’s internal state (a) and output word (w) evolve according to
the provided code snippet, with w not necessarily covering all possible
values &lt; c but rather a subset of c−1 distinct values &lt;
2^(word_size).</li>
</ul></li>
</ol>
<p>The text provides insights into various applications of shift
register sequences in error-correcting codes, pseudorandom number
generation, and combinatorial sequence construction. It also offers code
snippets for implementing LFSRs and FCSRs in C++.</p>
<p>Linear Hybrid Cellular Automata (LHCA) are a type of one-dimensional
cellular automaton where two different rules are applied based on the
position of each cell. The next state of an LHCA is determined by Rule
150 for cells where a certain rule condition (r) is met, and Rule 90
otherwise. This is implemented in a branch-free manner, making hardware
implementation straightforward.</p>
<p>The rules are defined by examining the eight possible states of a
cell with its neighbors and interpreting the resulting pattern as a
binary number. For example, Rule 90 corresponds to the binary number
01011010 (90 in decimal), while Rule 150 corresponds to 10010110 (150 in
decimal).</p>
<p>The computation of the next state with an LHCA can be summarized as
follows:</p>
<ol type="1">
<li>Compute <code>r &amp; x</code> (bitwise AND between rule and current
cell state).</li>
<li>Compute <code>t = (x &gt;&gt; 1) ^ (x &lt;&lt; 1)</code> (bit-shift
operations on current cell state).</li>
<li>Apply the XOR operation with the result of step 1:
<code>t ^= r</code>.</li>
<li>Perform bitwise AND with the mask <code>m</code>:
<code>t &amp;= m</code>.</li>
<li>Return the resulting value as the next state.</li>
</ol>
<p>The length of the automaton is defined by <code>m</code>, which must
be a burst of the n lowest bits (n being the length of the
automaton).</p>
<p>Certain rule vectors <code>r</code> can lead to maximal periods in
LHCA, where all nonzero values occur for m = 2^n - 1. Minimum-weight
rules that result in maximal period are listed and can be found in [FXT:
bpol/lhcarule-minweight.cc].</p>
<p>LHCA rules can be converted to binary polynomials using a recursive
formula, where <code>pk</code> is calculated based on previous values of
<code>pk−1</code> and <code>pk−2</code>, with the rule bits
(<code>ri</code>) taken into account. This process generates a
polynomial of degree <code>n</code>.</p>
<p>Additive Linear Hybrid Cellular Automata (ALHCA) are a broader
category that includes LHCA, where the next state of any two words
<code>a</code> and <code>b</code> satisfies
<code>N(a) + N(b) = N(a + b)</code>. The action of such an automaton can
be described by a matrix over GF(2), with the binary polynomial
corresponding to the automaton being the characteristic polynomial of
that matrix.</p>
<p>The conversion from ALHCA rules to binary polynomials involves
determining the matrix whose k-th row corresponds to <code>N(ek)</code>,
where <code>ek</code> is a word with only bit <code>k</code> set. The
resulting polynomial can be computed using the characteristic polynomial
algorithm for matrices over GF(2). This method generalizes the LHCA
conversion process described earlier, allowing for the analysis of
various additive cellular automata beyond just LHCAs.</p>
<p>This text discusses the binary finite fields GF(2^n), focusing on
their arithmetic properties, particularly for n not greater than
BITS_PER_LONG (a constant defining the maximum number of bits that can
be represented by a long integer). The following points summarize the
key details and explanations:</p>
<ol type="1">
<li><p><strong>Field Representation</strong>: In binary finite fields
GF(2^n), elements are polynomials modulo an irreducible polynomial C
with coefficients in the ground field GF(2). The arithmetic is
polynomial arithmetic modulo C. Different irreducible polynomials of
degree n represent the same 42.1: Arithmetic and basic properties 893
field, making them isomorphic to each other.</p></li>
<li><p><strong>Characteristic</strong>: The characteristic of GF(2^n) is
2. Two elements are identified if their difference is a multiple of 2
(equivalently, adding any element p times results in zero).</p></li>
<li><p><strong>Linear Functions</strong>: Linear functions f(x) in
GF(2^n) have the form ∑(0 ≤ k &lt; n) uk * x^(2k), where u_k are
elements from the ground field GF(2). These linear functions can be
computed using lookup tables, known as square-and-multiply
algorithms.</p></li>
<li><p><strong>Squaring</strong>: Squaring (and raising to any power
2^k) is a linear operation in GF(2^n), meaning that the square of an
element can be calculated efficiently by precomputing and utilizing the
powers of x modulo C.</p></li>
<li><p><strong>Trace Function</strong>: The trace Tr(a) of an element a
in GF(2^n) is defined as the sum of its powers: Tr(a) = ∑(0 ≤ j &lt; n)
a^(2j). The trace function is linear and can be computed using lookup
tables based on a precomputed trace vector tv.</p></li>
<li><p><strong>Inverse and Square Root</strong>:</p>
<ul>
<li>The inverse of a nonzero element a in GF(2^n) is given by a^(Q-2),
where Q = 2^n.</li>
<li>Every element has a unique square root s, which can be computed
using an efficient lookup table method with precomputed √x values.</li>
</ul></li>
<li><p><strong>Order and Primitive Roots</strong>: The order of an
element in GF(2^n) is the smallest positive exponent r such that a^r =
1. An element of maximal order (2^n - 1) is called a generator or
primitive root, as its powers generate all nonzero elements in the
field.</p></li>
<li><p><strong>Implementation</strong>: A C++ class GF2n provides an
implementation for computations in GF(2^n), including constructors to
create objects from integers and other types, initialization methods,
and arithmetic operations like addition, multiplication, squaring, trace
calculation, and finding the inverse or square root of elements. The
irreducible polynomial is chosen by default, but can be specified if
needed.</p></li>
</ol>
<p>In summary, this text presents the basics of working with binary
finite fields GF(2^n), including their representation as polynomials
modulo an irreducible polynomial, arithmetic operations, and efficient
methods for tasks like squaring, computing traces, inverses, and square
roots using lookup tables. The provided C++ class GF2n serves as a
practical tool to perform computations within these fields, taking
advantage of precomputed data structures (such as trace vectors) to
optimize performance.</p>
<p>The text discusses the topic of binary finite fields, specifically
GF(2^n), and provides information about various aspects such as
generators, minimal polynomials, trace vectors, and solving quadratic
equations within this field. Here’s a detailed summary and
explanation:</p>
<ol type="1">
<li><p><strong>Generators</strong>: The document introduces two examples
of generating elements (g) in GF(2^4). These are visualized using tables
showing the powers of g under both primitive and non-primitive
polynomial moduli. The class GF2n provides operations like addition,
subtraction, multiplication, division, inverse calculation, powering,
order determination, and trace computation for these elements.</p></li>
<li><p><strong>Minimal Polynomials</strong>: A minimal polynomial of an
element in GF(2^n) is defined as the lowest-degree polynomial with that
element as a root. The algorithm to compute this polynomial involves
finding the smallest positive integer r such that a<sup>(2</sup>r) = a,
then constructing a product of binomial factors (x - a<sup>(2</sup>k))
for k from 0 to r-1. This result is guaranteed to have coefficients in
GF(2^n), but they will be binary (either 0 or 1).</p></li>
<li><p><strong>Fast Computation of Trace Vector</strong>: The trace
vector, which records the trace (sum of elements in GF(2^n)) for each
power of a generator element, can be computed using two methods:</p>
<ul>
<li><p><strong>Newton’s Formula</strong>: This method uses recursion to
compute trace values based on previously calculated ones and
coefficients of the irreducible polynomial. It does not involve any
polynomial modular reductions, making it efficient even for calculating
just one trace.</p></li>
<li><p><strong>Division of Power Series</strong>: This variant of the
algorithm treats trace vector computation as a division operation
between two power series. While conceptually elegant, its practical
efficiency depends on the method used for high-order multiplication or
division of power series, which should ideally be FFT-based methods for
large n.</p></li>
</ul></li>
<li><p><strong>Properties of Trace Vector</strong>: Certain properties
of the trace vector are discussed:</p>
<ul>
<li>For binary polynomials of odd degree with nonzero coefficients only
at odd indices, t0 = 1 and all other ti are zero unless i equals a
multiple of n.</li>
<li>For even-degree binary polynomials with nonzero odd coefficients
only at positions less than halfway through the degree, only tn−i
components are non-zero where i &lt; n/2.</li>
</ul></li>
<li><p><strong>Solving Quadratic Equations</strong>: The document points
out that while extracting a square root in GF(2^n) is straightforward,
solving quadratic equations (ax^2 + bx + c = 0) is not as simple due to
the lack of a standard division operation by 2. Therefore, standard
quadratic solution formulas from real or complex number algebra do not
apply directly in this context.</p></li>
</ol>
<p>Throughout these discussions, the focus is on binary finite fields
(GF(2^n)), with special attention given to the structure and properties
that arise due to their binary nature. This includes unique
characteristics of generators, minimal polynomials, trace vectors, and
the challenges presented when attempting to apply familiar algebraic
operations or formulas from other number systems within this binary
field context.</p>
<p>This text discusses the representation of elements in a binary finite
field GF(2^n) using normal bases. Normal bases offer an alternative way
to represent elements, which can simplify certain arithmetic operations.
Here’s a detailed summary and explanation:</p>
<ol type="1">
<li><p><strong>Normal Basis Representation</strong>: An element
<code>a</code> in GF(2^n) can be represented as a sum of powers of a
normal basis, i.e., <code>a = ∑_{k=0}^{n-1} ak x^(2k)</code>. This is
different from the standard representation where elements are
represented as sums of powers of the generator ‘x’.</p></li>
<li><p><strong>Multiplication Algorithm</strong>: Multiplication in
normal basis can be done using a multiplication matrix M. Given two
elements <code>a</code> and <code>b</code>, their product
<code>p = a · b</code> can be computed by performing dot products
between cyclically shifted versions of <code>a</code> and <code>b</code>
with the matrix M.</p></li>
<li><p><strong>Multiplication Matrix</strong>: The multiplication matrix
M is derived from the companion matrix C of the field polynomial (which
is irreducible). To find M, we compute matrices A and D, where:</p>
<ul>
<li>A has rows equal to x^(2k) mod C,</li>
<li>D = A * CT * A^(-1),</li>
<li>M’s entries Mi,j are given by D_j’,i’ where i’ = -i mod n and j’ = j
- i mod n.</li>
</ul></li>
<li><p><strong>Normal Polynomial Test</strong>: A polynomial c is normal
if the nullspace of the matrix whose k-th row equals x^(2k) mod c is
empty. This means that the roots of c are linearly independent.</p></li>
<li><p><strong>Solving Reduced Quadratic Equation</strong>: The reduced
quadratic equation x^2 + x = f has two solutions if Tr(f) = 0 (i.e., the
number of ones in the normal representation of f is even). One solution
can be computed using the inverse of the reversed Gray code of
f.</p></li>
<li><p><strong>Number of Binary Normal Bases</strong>: The text also
provides a table showing the number of binary normal bases for different
values of n, which grows rapidly with n.</p></li>
</ol>
<p>The use of normal bases simplifies multiplication and trace
computation but may complicate other operations like addition and
exponentiation. They are particularly beneficial in hardware
implementations due to their regular structure, making them suitable for
pipelined arithmetic circuits.</p>
<p>Title: Summary of Normal Bases in GF(2^n)</p>
<p>Normal bases are a special type of basis for the finite field
GF(2^n), which play a significant role in various areas of mathematics,
including coding theory and cryptography. This summary focuses on key
aspects related to normal bases in binary finite fields (GF(2^n)), their
properties, computation methods, and related concepts like dual bases
and self-dual bases.</p>
<ol type="1">
<li><p><strong>Normal Bases</strong>: A normal basis for GF(2^n) is a
basis where the minimal polynomial of each element is a normal
polynomial—an irreducible polynomial that divides x<sup>(2</sup>n) - 1
but not its proper factors. Normal bases have several desirable
properties, such as fast multiplication algorithms and simple field
arithmetic.</p></li>
<li><p><strong>Number of Normal Bases (An)</strong>: The number of
degree-n binary normal polynomials is denoted by An. This sequence is
A027362 in the Online Encyclopedia of Integer Sequences ([312]). No
explicit formula for An exists, but it can be computed using
factorizations or efficient algorithms.</p></li>
<li><p><strong>Computation Methods</strong>:</p>
<ul>
<li>Exhaustive search: For small degrees (n &lt; 25), normal polynomials
can be generated by selecting irreducible polynomials that are also
normal. Using the Lyndon word-based mechanism for generating all
irreducible polynomials, the computation is relatively fast.</li>
<li>Invertible circulant matrices: The number of binary normal bases
equals the number of invertible circulant n×n matrices over GF(2).
Circulant matrices can be constructed from Lyndon words with odd weight,
and their invertibility is determined by testing coprimality with x^n -
1.</li>
</ul></li>
<li><p><strong>De Bruijn Graph Connection</strong>: Interestingly, the
number of degree-n binary normal polynomials (An) equals the number of
cycles in the De Bruijn graph B(2, n), which are binary sequences of
length 2^n without repeated subsequences of length less than n.</p></li>
<li><p><strong>Dual and Self-dual Bases</strong>: For a basis A = {a0,
a1, …, an−1}, a dual (or complementary) basis B = {b0, b1, …, bn−1}
satisfies Tr(ak bj) = δk,j for 0 ≤ k, j &lt; n. Self-dual bases are
normal bases that are also their own duals.</p>
<ul>
<li>To check if the roots of an irreducible polynomial C form a
self-dual basis: compute its trace polynomial T (using gf2n_xx2k_trace),
and test if T = 1 mod x^n - 1 using bitpol_normal2_q.</li>
<li>Computing dual bases involves finding the minimal polynomial CS of
the dual (normal) basis β, which can be done with gf2n_dual_normal
routine.</li>
</ul></li>
<li><p><strong>Self-dual Normal Bases</strong>: A self-dual normal basis
is a normal basis that’s also its own dual. The number of such bases for
GF(2^n), denoted Sn (and Zn if the field polynomial is primitive), forms
sequences A135488 and A135498 in [312]. No formula exists for these
values, but they can be computed using specialized algorithms like the
one provided by Max Alekseyev.</p></li>
</ol>
<p>In conclusion, normal bases are a vital concept in GF(2^n) with deep
connections to combinatorics (De Bruijn graphs), linear algebra
(circulant matrices), and finite field theory (dual and self-dual
bases). Although no explicit formula exists for the number of normal or
self-dual normal bases, efficient algorithms allow their computation up
to relatively large degrees. Understanding these concepts is crucial for
applications in various areas of mathematics and computer science,
including coding theory, cryptography, and algorithmic algebra.</p>
<p>The text discusses various types of normal bases for binary finite
fields GF(2^n), which are crucial for efficient computations within
these fields. Here’s a detailed summary:</p>
<ol type="1">
<li><strong>Self-Dual Normal Basis (SDNB)</strong>:
<ul>
<li>No self-dual normal basis exists when n is a multiple of 4.</li>
<li>The function <code>sdn(m,p)</code> calculates the number of distinct
self-dual normal bases for GF(p^m) over GF(p), where p is prime.</li>
<li>If m is divisible by p, it uses a different formula; otherwise, it
factors (x^m - 1)/(x - 1) mod p and iterates through the factors to find
self-dual pairs.</li>
</ul></li>
<li><strong>Conversion Between Normal and Polynomial
Representation</strong>:
<ul>
<li>For a normal field polynomial C, conversion between representations
is possible using matrices Z and Z^(-1).</li>
<li>Matrix Z’s k-th column contains x^(2k) mod C, and the normal
representation b can be obtained as b = Z^(-1) * a (where a is the
polynomial representation).</li>
</ul></li>
<li><strong>Optimal Normal Bases (ONB)</strong>:
<ul>
<li>Type-1 ONBs exist when p := n + 1 is prime, and 2 is a primitive
root modulo p (also for n = 0 and n = 1). The sequence of such n is
A071642.</li>
<li>Type-2 ONBs exist when p := 2n + 1 is prime, satisfying certain
conditions related to the order of 2 modulo p. These n values are listed
in A054639.</li>
<li>For both types, the field polynomial is the all-ones polynomial (1 +
x + … + x^n). Multiplication matrices for ONBs have sparse structures
with at most two nonzero entries per row/column.</li>
</ul></li>
<li><strong>Gaussian Normal Bases (GNB)</strong>:
<ul>
<li>GNB generalize ONBs and have multiplication matrices with more
nonzero entries as t (the type parameter) increases.</li>
<li>A type-t GNB exists for n if p := t n + 1 is prime, and gcd(n, t
n/r2) = 1 where r2 is the order of 2 modulo p. No GNB exist when n is
divisible by 8.</li>
</ul></li>
<li><strong>Computation of Multiplication Matrix</strong>:
<ul>
<li>For type-t GNBs (t &gt; 2), an algorithm to compute the
multiplication matrix involves setting up a vector F, iterating through
it with powers of r (an element of order t modulo p), and populating the
multiplication matrix M based on F values.</li>
</ul></li>
</ol>
<p>The text also mentions algorithms for constructing self-dual normal
bases ([229]), conversion functions between representations in C++ class
GF2n, and the use of Pascal’s triangle to compute certain polynomials.
It concludes by mentioning the Lucas Correspondence Theorem for
computing binomial coefficients modulo a prime q.</p>
<p>The provided text is an excerpt from the appendix of a book about
finite fields, specifically discussing Gaussian normal bases (GNB) over
GF(2^n). Here’s a detailed summary and explanation of the content:</p>
<p><strong>Gaussian Normal Bases (GNB):</strong> A GNB is a special
basis for the finite field GF(2^n), which allows for efficient
arithmetic operations. The text focuses on two aspects related to GNBs:
determining whether they exist for given n and t, and computing the
corresponding field polynomial when they do.</p>
<ol type="1">
<li><p><strong>Existence of Gaussian Normal Bases:</strong></p>
<ul>
<li>The file <code>[FXT: data/gauss-normal-types.txt]</code> lists, for
each 2 ≤ n ≤ 1032, the smallest ten values of t such that a type-t GNB
exists over GF(2^n).</li>
<li>Different values of t may not necessarily lead to different
multiplication matrices, especially for small values of n.</li>
</ul></li>
<li><p><strong>Computing the Field Polynomial:</strong></p>
<p>The field polynomial (also called normal polynomial) corresponding to
a given pair (n, t) can be computed using two algorithms: one that works
with complex numbers and another that operates within GF(2).</p>
<p><strong>Algorithm using Complex Numbers:</strong></p>
<ol type="1">
<li>Set p = tn + 1 and determine r such that the order of r modulo p
equals t.</li>
<li>For 1 ≤ k ≤ n, compute w_k = ∑_{j=0}^{t-1} exp(a_k * 2πi/p), where
a_k = 2^k * r^j mod p.</li>
<li>Let z(x) = Π_{k=1}^n (x - w_k), which is a polynomial with real
integer coefficients.</li>
<li>Return the polynomial with coeﬃcients reduced modulo 2.</li>
</ol>
<p><strong>Algorithm working in GF(2):</strong></p>
<ol type="1">
<li>Set p = tn + 1 and determine r such that the order of r modulo p
equals t.</li>
<li>Initialize M = ∑_{k=0}^{p-1} x^k (all computations are done modulo
M).</li>
<li>If t equals 1, return M.</li>
<li>Set F_0 = 1 (modulo M).</li>
<li>For 1 ≤ k ≤ n:
<ul>
<li>Compute Z_k = ∑_{j=0}^{t-1} x<sup>(2</sup>k * r^j) mod M.</li>
<li>Update F_k = (x + Z_k) * F_(k-1) mod M.</li>
</ul></li>
<li>Return F_n.</li>
</ol>
<p>The second algorithm avoids inexact arithmetic but has a larger
polynomial modulus of degree p − 1 = n t, making it slower for large t
compared to the complex number method.</p></li>
<li><p><strong>Optimization and Symmetry Exploitation:</strong></p>
<ul>
<li>The computation can be optimized using trigonometric recursion
(mentioned but not detailed in the provided text).</li>
<li>For even types t, real values can be used instead of complex
numbers, reducing computational cost without loss of accuracy.</li>
</ul></li>
<li><p><strong>Verification:</strong></p>
<p>Results for type-1 bases can be verified using specific relations,
while results with type-2 bases can be checked using different relations
(42.8-2a and 42.8-2b).</p></li>
<li><p><strong>GP Implementation:</strong></p>
<p>The text provides GP routines for computing the field polynomials
using both algorithms mentioned above (<code>gauss_zpoly</code> and
<code>gauss_poly</code>, <code>gauss_poly2</code>). These functions use
complex arithmetic or work entirely within GF(2) to compute the normal
polynomial of a type-t Gaussian normal basis.</p></li>
</ol>
<p>In summary, this section delves into the existence and computation of
Gaussian normal bases over GF(2^n), presenting two algorithms for
determining whether such bases exist and computing their corresponding
field polynomials when they do. The text also discusses optimizations
and verification methods along with GP implementations of these
computations.</p>
<p>The paper “Time- and Space-Efficient Evaluation of Some
Hypergeometric Constants” by Howard Cheng, Guillaume Hanrot, Emmanuel
Thomé, Eugene Zima, and Paul Zimmermann presents an algorithm for
evaluating hypergeometric constants with high precision. The authors
focus on the calculation of values like π, log(2), and Catalan’s
constant (G) using a combination of symbolic manipulation and numerical
methods.</p>
<p>The algorithm employs a technique called “series acceleration” to
speed up the convergence of series representations for these constants.
This method leverages techniques such as Euler-Maclaurin summation, Pade
approximation, and Richardson extrapolation to achieve high accuracy
with fewer terms in the series.</p>
<p>The paper also discusses an implementation of this algorithm using
the GNU Multiple Precision Arithmetic Library (GMP) and MPFR libraries
for arbitrary precision arithmetic. The authors demonstrate that their
method outperforms previous techniques in both time and space
complexity, making it a practical choice for computing hypergeometric
constants with high precision.</p>
<p>In summary, “Time- and Space-Efficient Evaluation of Some
Hypergeometric Constants” by Cheng et al. presents an efficient
algorithm for calculating hypergeometric constants such as π, log(2),
and Catalan’s constant (G) using series acceleration techniques. The
authors provide both theoretical analysis and practical implementation
details, showcasing the advantages of their approach in terms of speed
and memory usage.</p>
<p>Title: Fast Fourier Transform (FFT) Algorithms</p>
<p>The Fast Fourier Transform (FFT) is a computationally efficient
algorithm for computing the Discrete Fourier Transform (DFT), which is
widely used in signal processing, digital image processing, and many
other areas of mathematics and engineering. The main idea behind FFT is
to reduce the number of complex multiplications required by exploiting
the symmetries and periodicities of the DFT.</p>
<ol type="1">
<li>Background:
<ul>
<li>Discrete Fourier Transform (DFT): Given a sequence of N complex
numbers x[0], x[1], …, x[N-1], the DFT is defined as X[k] =
∑_{n=0}^{N-1} x[n] * exp(-j2πkn/N), where j is the imaginary unit, and k
ranges from 0 to N-1.</li>
<li>Complex Multiplication: A complex multiplication involves two real
multiplications and one real addition (or subtraction).</li>
</ul></li>
<li>Cooley-Tukey FFT Algorithm:
<ul>
<li>The Cooley-Tukey algorithm, introduced in 1965, is the most widely
used FFT algorithm. It recursively divides the DFT of size N into
smaller DFTs of size N/2, reducing the number of complex multiplications
from O(N^2) to O(N log N).</li>
<li>The algorithm relies on the fact that any even-length DFT can be
computed by combining two half-length DFTs using twiddle factors
(complex exponentials).</li>
</ul></li>
<li>Radix-2 Decimation in Time (DIT) and Decimation in Frequency (DIF):
<ul>
<li>The radix-2 Cooley-Tukey FFT algorithm uses either decimation in
time (DIT) or decimation in frequency (DIF) to divide the DFT into
smaller subproblems.</li>
<li>In DIT, the input sequence is divided into two halves, and each half
is transformed separately. Then, the even-indexed coefficients are
combined using twiddle factors, followed by the odd-indexed
coefficients.</li>
<li>In DIF, the input sequence is divided into two sets of coefficients
with even and odd indices. Each set is then transformed separately, and
the results are combined using twiddle factors.</li>
</ul></li>
<li>Split-Radix FFT:
<ul>
<li>The split-radix algorithm extends the radix-2 Cooley-Tukey FFT to
handle prime or composite sizes other than powers of 2. It combines
radix-2 and radix-3 (or higher) decompositions, reducing the number of
complex multiplications further.</li>
</ul></li>
<li>FFT Applications:
<ul>
<li>Signal Processing: FFT is used for analyzing and manipulating
signals in various domains, such as audio processing, image compression,
and communication systems.</li>
<li>Digital Image Processing: In image processing, FFT is applied to
perform tasks like filtering, edge detection, and pattern
recognition.</li>
<li>Solving Linear Systems: The Cooley-Tukey algorithm can be used to
solve linear systems by computing the eigenvalues of a matrix through
the DFT of its coefficients.</li>
</ul></li>
<li>Challenges and Optimizations:
<ul>
<li>Memory Access Patterns: FFT algorithms can have irregular memory
access patterns, leading to cache misses and performance degradation.
Techniques like bit-reversal permutations and data reordering aim to
mitigate this issue.</li>
<li>Parallelism: Modern hardware architectures support parallel
processing, allowing for efficient implementations of FFT on multi-core
processors, GPUs, and other specialized platforms (e.g., FPGAs).</li>
</ul></li>
</ol>
<p>In summary, the Fast Fourier Transform (FFT) is a crucial algorithm
in signal processing, digital image processing, and many other fields
due to its ability to efficiently compute Discrete Fourier Transforms
with significantly reduced computational complexity compared to direct
evaluation. The Cooley-Tukey algorithm, along with various extensions
like split-radix FFT, forms the basis for most modern FFT
implementations.</p>
<p>The provided text appears to be an index of terms related to various
mathematical, computational, and algorithmic concepts. Here’s a detailed
summary and explanation of some key topics:</p>
<ol type="1">
<li><strong>Fourier Transform and Related Concepts</strong>:
<ul>
<li><strong>Fourier Transform (FT)</strong>: A mathematical technique
used for transforming a time signal or function into the frequency
domain. It is widely applied in signal processing, image analysis, and
solving partial differential equations. The discrete version of FT,
known as Discrete Fourier Transform (DFT), is a fundamental operation in
digital signal processing.</li>
<li><strong>Fast Fourier Transform (FFT)</strong>: An efficient
algorithm to compute the DFT and its inverse. FFT algorithms reduce the
computational complexity from O(n^2) for naive methods to O(n log n).
Different variants of FFT, such as Cooley-Tukey and Prime Factor
Algorithm (PFA), are used based on the problem’s specifics.</li>
<li><strong>Convolution Property</strong>: Describes how convolution in
the time domain corresponds to multiplication in the frequency domain,
which is crucial for various applications like filtering and image
processing.</li>
<li><strong>Fractional Fourier Transform (FrFT)</strong>: An extension
of the standard Fourier transform that allows for a continuous rotation
angle, providing more flexibility compared to traditional FFT.</li>
</ul></li>
<li><strong>Graph Theory</strong>:
<ul>
<li><strong>Path in Graph</strong>: A sequence of adjacent nodes
connecting two specific nodes within a graph. Finding paths is essential
in many applications, such as network routing and finding shortest
connections between points.</li>
<li><strong>Loopless Algorithm</strong>: An approach that avoids cycles
or loops during the execution process to prevent redundant computations
or infinite recursions.</li>
</ul></li>
<li><strong>Number Theory and Algebraic Structures</strong>:
<ul>
<li><strong>Galois Fields (GF)</strong>: Finite fields introduced by
Évariste Galois, where the number of elements is a power of a prime, and
operations are performed modulo an irreducible polynomial of degree n
over GF(p). They play a crucial role in coding theory and
cryptography.</li>
<li><strong>Binary Finite Fields (GF(2^n))</strong>: Special cases of
Galois fields where the characteristic is 2, with elements represented
as binary words. These are fundamental in error-correcting codes and
cryptography.</li>
</ul></li>
<li><strong>Combinatorics and Permutations</strong>:
<ul>
<li><strong>Gray Codes</strong>: A sequence of integers where each
number differs from its neighbors by one in terms of binary
representation. They minimize the number of bit changes, useful for
applications like error correction and data compression. Different
variants exist, such as reflected Gray code and recursive Gray
codes.</li>
<li><strong>Permutations and Combinations</strong>: Arrangements or
selections of elements from a set, respectively. Various representations
(e.g., lexicographic order, factorial base) and algorithms (e.g., Heap’s
algorithm for permutations) are discussed in the index.</li>
</ul></li>
<li><strong>Algorithmic Techniques and Optimizations</strong>:
<ul>
<li><strong>In-place Routine</strong>: Algorithms that perform
computations without requiring additional memory, optimizing space
complexity.</li>
<li><strong>Interpolation Binary Search and Linear
Interpolation</strong>: Efficient search algorithms combining binary
search with interpolation to achieve faster searches in sorted
arrays.</li>
<li><strong>Parallel Assignment (with Pseudocode)</strong>: A
programming technique where multiple variables are updated
simultaneously using a single statement, improving efficiency on
parallel architectures.</li>
</ul></li>
<li><strong>Number Sequences and Series</strong>:
<ul>
<li>The index lists numerous integer sequences, including well-known
ones like Fibonacci numbers, Catalan numbers, and Bell numbers. These
sequences often have connections to combinatorics, number theory, and
various mathematical constants.</li>
</ul></li>
</ol>
<p>The provided index serves as a comprehensive reference for diverse
mathematical and computational concepts, their applications, and related
algorithms, which can be valuable resources for researchers, engineers,
and students in fields like computer science, mathematics, and
engineering.</p>
<p>The text provided is an index from a technical or mathematical
context, listing various terms related to algorithms, number theory,
combinatorics, and other areas of mathematics. Here’s a detailed
explanation of some key topics:</p>
<ol type="1">
<li><p><strong>Run-Length Limited (RLL) words</strong>: RLL codes are a
type of data compression technique used in digital systems. In these
codes, contiguous sequences of data values are replaced by pairs
consisting of the count (run length) and value. The number 310 refers to
the specific constraint or code rate used in this context.</p></li>
<li><p><strong>Rogers-Ramanujan identities</strong>: These are two
beautiful infinite series identities discovered independently by Leonard
James Rogers and Srinivasa Ramanujan, both formulated in the early 20th
century. They describe relationships between certain partitions of
integers. The number 347 likely refers to a specific reference or page
number where these identities are discussed.</p></li>
<li><p><strong>Roots</strong>:</p>
<ul>
<li><strong>Extraction</strong>: Finding roots (or zeros) of polynomials
or functions.</li>
<li><strong>Inverse</strong>: The multiplicative inverse in a given ring
(e.g., modulo arithmetic).</li>
<li><strong>Modulo pn (p-adic)</strong>: Computing roots in p-adic
number fields.</li>
<li><strong>Of a Polynomial, Divisionless Iterations</strong>: Methods
for finding roots without using polynomial division.</li>
<li><strong>Primitive</strong>: A root that generates all other roots
when raised to appropriate powers. The term is used in different
contexts like GF(2^n) (Galois Field) and modulo m (modular
arithmetic).</li>
<li><strong>Of Mersenne Primes</strong>: Roots of unity related to
Mersenne primes, which are prime numbers that can be written in the form
2^p − 1 for some prime p.</li>
</ul></li>
<li><p><strong>Roots of Unity</strong>: Complex numbers whose nth power
is 1 (where n is a positive integer). When they sum to zero, it’s
related to specific properties of these numbers and their cyclotomic
polynomials.</p></li>
<li><p><strong>Rotation</strong>:</p>
<ul>
<li><strong>Bit-wise</strong>: A shift operation where bits are moved
circularly within a fixed-size container.</li>
<li><strong>By Triple Reversal</strong>: A technique in combinatorics
involving permutations, which can be visualized as a rotation through
three axes.</li>
</ul></li>
<li><p><strong>Row-Column Algorithm</strong>: A method used for matrix
multiplication or other operations, where elements are processed row by
row and column by column in a systematic manner.</p></li>
<li><p><strong>Ruler Constant &amp; Ruler Function</strong>: The ruler
constant is the smallest positive real number such that there exists a
ruler of that length with no two marks at a distance less than the
constant from each other. The ruler function generates the sequence of
lengths of all possible rulers for a given integer.</p></li>
<li><p><strong>Sedenions</strong>: An 16-dimensional algebraic system,
extending the concept of complex numbers and quaternions. They have
applications in areas like theoretical physics and geometry.</p></li>
<li><p><strong>Selection Sort</strong>: A simple sorting algorithm that
repeatedly selects the smallest (or largest) element from the unsorted
part of the array and moves it to its correct position in the sorted
part.</p></li>
<li><p><strong>Sentinel Element</strong>: A special value used to
simplify array or list manipulation algorithms, such as searching or
inserting elements. It acts as a marker for the end of valid
data.</p></li>
<li><p><strong>Self-Correlation</strong>: A measure of similarity
between a signal and a delayed copy of itself regarding amplitude at
different points in time.</p></li>
<li><p><strong>Self-Dual (Basis over GF(2^n))</strong>: In Galois Field
theory, self-dual bases are special types of basis elements for GF(2^n)
that have specific symmetry properties.</p></li>
<li><p><strong>Self-Inverse Permutation, Random</strong>: A permutation
that is its own inverse, chosen randomly from the set of all possible
permutations of a given size.</p></li>
<li><p><strong>Self-Reciprocal Polynomial</strong>: A polynomial that
remains unchanged under the substitution n ↦ 1/n (except for constant
terms).</p></li>
<li><p><strong>Set Partition &amp; Subset</strong>: Concepts in
combinatorics where elements are divided into non-empty subsets such
that each element appears in exactly one subset, and all possible
subsets of a given set, respectively.</p></li>
<li><p><strong>Shift Operator</strong>: Used in various transforms
(e.g., Fourier or Hartley) to represent the shift operation. It’s often
denoted by ‘e’ raised to the power of ‘i2πft’, where f is the frequency
and t is time.</p></li>
<li><p><strong>Shift Register Sequence (SRS)</strong>: A type of
pseudorandom number generator using a shift register, cycling through a
sequence determined by its initial state and feedback
polynomial.</p></li>
<li><p><strong>Short Division &amp; Short Multiplication</strong>:
Efficient algorithms for division and multiplication in specific numeric
systems or with particular constraints on the operands.</p></li>
<li><p><strong>Sieve of Eratosthenes</strong>: An ancient algorithm used
to find all prime numbers up to any given limit by iteratively marking
as composite (i.e., not prime) the multiples of each prime, starting
from 2.</p></li>
<li><p><strong>Sign Decomposition of a Matrix</strong>: Breaking down a
matrix into components related to its positive and negative eigenvalues
or singular values.</p></li>
<li><p><strong>Sign of a Permutation/Fourier Transform</strong>: The
sign (or parity) of a permutation or the phase factor in the Fourier
transform, indicating whether the transformation preserves or reverses
orientation.</p></li>
<li><p><strong>Signed Binary Representation</strong>: A way to represent
signed integers using binary digits, often including a separate bit for
the sign and adjusting magnitude accordingly.</p></li>
<li><p><strong>Simple Continued Fraction</strong>: A representation of
real numbers as the limit of a sequence of simpler fractions, where each
fraction’s numerator is one more than its denominator, and so
on.</p></li>
<li><p><strong>Single Track</strong>: Refers to different encoding
schemes for combinatorial objects (like permutations or subsets) that
can be represented as sequences of ’1’s and ’0’s along a single
line.</p></li>
<li><p><strong>Singular Value Decomposition (SVD)</strong>: A
factorization of a real or complex matrix, expressing it as the product
of three matrices: an orthogonal/unitary matrix, a diagonal matrix, and
another orthogonal/unitary matrix. It has applications in data
compression, noise reduction, and solving linear systems.</p></li>
<li><p><strong>Skew Circular Convolution</strong>: A generalization of
circular convolution where the roles of input sequences are swapped but
with a phase shift between them.</p></li>
<li><p><strong>Slant Transform &amp; Sequency-Ordered Slant
Transform</strong>: Methods for analyzing or transforming signals/data,
particularly in the context of wavelets and multiresolution analysis,
where ‘sequency’ refers to the order of nonzero coefficients in a binary
sequence representation.</p></li>
<li><p><strong>Smart Compiler</strong>: A compiler that employs advanced
techniques (like optimization, parallelization, etc.) to generate more
efficient machine code from high-level source code.</p></li>
<li><p><strong>Stable Sort</strong>: A sorting algorithm that maintains
the relative order of records with equal keys (i.e., it’s stable under
equality).</p></li>
<li><p><strong>Stack (LIFO)</strong>: A Last-In-First-Out data structure
where elements are added and removed from one end, often used for
function calls in programming languages or undo operations.</p></li>
</ol>
<p>These topics represent a mix of fundamental concepts and specific
algorithms/techniques within mathematics and computer science. They span
areas like algebra, number theory, combinatorics, and algorithm design,
with applications ranging from data compression to signal processing and
cryptography.</p>
