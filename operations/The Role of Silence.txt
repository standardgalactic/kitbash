The Role of Silence in Knowledge Systems
Flyxion
September 22, 2025
1
Introduction
Silence is typically considered the absence of speech or sound, a void in com-
munication. Yet within knowledge systems—whether philosophical, scien-
tific, or cultural—silence often plays a constitutive role. This essay explores
silence not as emptiness but as an active medium that structures discourse,
regulates entropy in meaning, and anchors the boundaries of knowledge.
2
Silence as Epistemic Boundary
Every system of knowledge presupposes limits: things that cannot be said,
measured, or known within its framework.
In mathematics, Gödel's in-
completeness theorems demonstrate that no consistent system can prove all
truths within itself [Gödel, 1931]. In physics, Heisenberg's uncertainty prin-
ciple [Heisenberg, 1927] similarly defines a domain of silence, where predic-
tion collapses into probability. Silence thus functions not as ignorance but
as the marker of epistemic humility—the recognition that the scaffolding of
explanation rests on foundations that cannot themselves be fully explained.
3
Cultural and Linguistic Silences
Across cultures, silence has been encoded as a form of communication in its
own right. In Japanese aesthetics, ma denotes the meaningful interval, the
space between actions or sounds that makes them intelligible [Isozaki, 2006].
In many Indigenous North American traditions, silence is not passivity but
respect—a gesture that creates space for collective resonance [Basso, 1970].
Linguistically, silence structures dialogue: pauses signal boundaries, hesita-
tion conveys doubt, and unspoken assumptions carry as much meaning as
explicit words [Jaworski, 1993].
1

4
Silence and Entropy in Knowledge
If speech generates informational entropy, silence regulates it. Just as gaps in
a lattice stabilize the whole structure, silences prevent overload in meaning
systems.
Scientific paradigms maintain silences by bracketing anomalies
until they can be reframed [Kuhn, 1962].
In philosophy, silence is often
invoked at the limit of metaphysics—as in Wittgenstein's dictum: "Whereof
one cannot speak, thereof one must be silent" [Wittgenstein, 1961]. Rather
than obstructing thought, these silences guide inquiry by highlighting the
contours of the unsayable.
5
Technological Silences
In digital systems, silence manifests as latency, bandwidth limits, or deliber-
ate omission. Algorithmic outputs are shaped not only by what they include
but also by what they leave out: the silenced variables, filtered data, or un-
sampled distributions [Gitelman, 2013]. These silences are often invisible
but decisive, shaping the epistemic horizon of artificial intelligence. Recog-
nizing them reveals the ethical stakes of information systems, where silence
can embody bias, exclusion, or intentional restraint [Benjamin, 2019].
6
Conclusion
Silence is not the absence of knowledge but a structuring principle within it.
It sets limits, enables meaning through spacing, regulates entropic overload,
and marks ethical responsibility in technological systems. To study silence
is to confront the paradox of knowledge: that what cannot be said often
does more to shape understanding than what is spoken. Knowledge systems
that fail to reckon with their silences risk mistaking noise for truth.
A
Silence in RSVP and Entropic Geometry
Silence as Zero Mode in RSVP Fields
Within the Relativistic Scalar Vector Plenum (RSVP) framework, si-
lence can be formalized as the zero mode of entropy flow.
If the scalar
field Φ encodes capacity for meaning, the vector field v encodes directional
flow, and the entropy field S encodes informational dissipation, then silence
corresponds to regions where ∇· S = 0. These are structured pauses in
negentropic flow, stabilizing Φ against runaway divergence. Silence is thus
a physical boundary condition: not erasure, but equilibrium.
Obstruction Theory and Derived Geometry
In the language of derived stacks and shifted symplectic structures, si-
lence corresponds to the obstruction class of knowledge. Just as Gödel's
2

incompleteness theorems expose undecidable truths, silence in RSVP marks
the degrees where global sections fail to glue, leaving only cohomological
shadows. Silence is not an absence of structure but the antifield of knowledge
in the AKSZ-BV formalism: a necessary ghost variable ensuring consistency
of the master equation.
Entropic Smoothing and Cultural Silence
Entropy regulation is the throughline. In RSVP cosmology, lamphron-
lamphrodyne smoothing prevents divergence by redistributing entropic gra-
dients. Analogously, cultural silences—ma in Japanese aesthetics, respectful
pauses in Apache dialogue—act as local tiling constraints. These structured
absences preserve coherence in semantic lattices, preventing runaway en-
tropic drift. Silence here functions as a negentropic operator, giving shape
by omission.
Algorithmic Silence as Hidden Entropy Reservoir
Technological silences—unmeasured variables, excluded datasets, algo-
rithmic filters—map directly to RSVP's hidden entropy reservoirs. These
reservoirs shape emergent epistemic horizons just as unobservable cosmo-
logical modes constrain field evolution. Acknowledging them reframes algo-
rithmic silence not as a bug but as a structural feature: the unspoken that
makes systemic coherence possible.
Synthesis
Across epistemic, cultural, and technological domains, silence emerges
as an entropy-regulating invariant. In RSVP terms:
Epistemic silence →obstruction class in derived geometry.
Cultural silence →local tiling gap stabilizing the semantic lattice.
Entropy-regulating silence →zero-divergence negentropic pause.
Technological silence →hidden entropy reservoir shaping horizons.
Thus, silence is not passive but constitutive: the very absence that en-
ables coherence across scales, from cosmology to cognition to computation.
B
Silence, Constraints, and Null Fronts in RSVP
B.1 Constraints in Causative Graphs
Let G = (V, E) be a directed acyclic graph (DAG) representing a causal
structure in the sense of Pearl (2009). A silence constraint is defined as the
enforced absence of an edge:
(x ̸→y) ∈C
where C is the set of constraints. In RSVP field terms:
V corresponds to scalar sites Φi on the plenum lattice.
E corresponds to vector flows vij linking nodes.
Silence corresponds to null divergence conditions on S:
∇· S|C = 0
3

Thus, a missing edge in the causal graph encodes an entropic boundary
condition: the forbidden channel for informational flow.
B.2 Counterfactuals and Null Assignments
Counterfactuals are modeled by intervention operators do(X = x′) that
generate alternative distributions P(Y | do(X = x′)). A silence-based null
assignment arises when a variable is set to the null symbol ∅, following
Subrahmanian's (1987) null convention logic.
Formally, define:
X = ∅⇒∀f ∈F, f(X) = ∅
where F is the set of structural equations. The null assignment is not a
contradiction but a propagating indeterminate.
In RSVP terms:
Null assignment ∅corresponds to a vanishing section of the sheaf S of
entropy flows.
Counterfactual silence = shadow trajectory in derived geometry:
Hk(S) ̸= 0 for some k > 0
indicating the existence of obstruction classes (unrealizable but struc-
turally necessary).
B.3 Propagating Null Wave Fronts
In null convention logic, the null wave propagates through inference
rules. Analogously, in RSVP dynamics, define a null wave front ψ∅with
propagation speed c∅across the lattice:
∂ψ∅
∂t + v · ∇ψ∅= 0
subject to initial condition ψ∅(x, 0) = 1 on the boundary of silence.
Interpretation:
ψ∅= 1 marks the front of indeterminacy.
Its propagation carries constraint information but no content.
In lattice simulations, this behaves like a shock wave of absence, ensuring
global consistency by extending local null conditions nonlocally.
B.4 Synthesis in RSVP Formalism
Constraint silence:
(x ̸→y) ↔∇· S = 0 on edge (x, y)
Counterfactual silence:
X = ∅↔Hk(S) ̸= 0
Null wave front silence:
∂ψ∅
∂t + v · ∇ψ∅= 0
Together, these establish silence as a technical operator in RSVP: con-
straining causal graphs, encoding counterfactual shadows, and propagating
entropic null fronts across the field.
B.5 Worked Example: 1D Propagation of a Null Wave Front
Model. We model the null wave front ψ∅(x, t) as passive advection by a
prescribed RSVP vector field v = (v, 0, 0) with v > 0:
∂ψ∅
∂t + v∂xψ∅= 0, ψ∅(x, 0) = 1[x0,L)(x),
4

on a 1D domain x ∈[0, L] with absorbing boundary at x = 0. Here, ψ∅=
1 indicates the front of indeterminacy (the propagating "silence" condition),
which carries constraint information but no content (§B.3).
Discretization. Use an upwind scheme (stable for CFL ≤1):
ψn+1
i
=
{
ψn
i −c(ψn
i −ψn
i−1),
v > 0,
ψn
i −c(ψn
i+1 −ψn
i ),
v < 0, c ≡v∆t/∆x.
We enforce an absorbing left boundary by setting ψn
−1 = 0. Numerical
clipping ψ ∈[0, 1] suppresses small oscillations.
Parameter choice (example). N = 64 cells, L = 1, v = 0.8, CFL = 0.8
⇒∆t = CFL · ∆x/v. Initial front at x0 = 0.25.
time
frontcellindex
frontpositionx
0.0000
16
0.25000
0.1562
24
0.37500
0.2969
31
0.48438
0.4531
39
0.60938
0.5938
47
0.73438
Table 1: Null wavefront: front position over time
time
psi[i=8]
psi[i=16]
psi[i=24]
psi[i=32]
psi[i=40]
psi[i=48]
psi[i=56]
0.0000
0.000
1.000
1.000
1.000
1.000
1.000
1.000
0.1562
0.000
0.000
0.624
1.000
1.000
1.000
1.000
0.2969
0.000
0.000
0.000
0.763
1.000
1.000
1.000
0.4531
0.000
0.000
0.000
0.002
0.716
1.000
1.000
0.5938
0.000
0.000
0.000
0.000
0.012
0.800
1.000
Table 2: Null wavefront: probe values across space
Interpreting the output
The front position increases linearly with time at speed ≈v, up to dis-
cretization error dictated by ∆x and the chosen CFL.
Probe values remain at 0 until the front arrives, then relax toward 1
under the upwind step, demonstrating a physically meaningful "wave of
silence" that propagates the constraint without injecting new content.
C
The Omission Principle of Entropic Prolifera-
tion
C.1 Definitions
5

Causal substrate. Let G = (V, E) be a causal DAG in the sense of Pearl.
A world-model is a parameterization θ ∈Θ of structural equations Fθ over
G, inducing a distribution Pθ on observables Y .
Disclosure policy.
A message M reveals a set of propositions R =
{r1, . . . , rm} and omits a set O = {o1, . . . , ok}. Each oj is a well-formed
proposition whose truth value is defined under Pθ.
Counterfactual set. Given the revealed content R, define the counterfac-
tual interpretation set
Ω(R) = {θ ∈Θ : Pθ is consistent with R}.
Attack surface (semantic).
For a prior π(θ) and likelihood threshold
λ ∈(0, 1], define
Aλ(R) = ∑
θ∈Ω(R) 1[L(θ|R) ≥λ],
H(R) = H(Θ|R),
where H is Shannon entropy under the posterior proportional to π(θ)L(θ|R).
RSVP embedding. In the RSVP field picture, let Φ be scalar capacity,
v vector flow, and S entropy density. A null assignment associated to an
omission seeds a null indicator ψ∅(x, t) ∈{0, 1} that obeys passive transport
∂tψ∅+ v · ∇ψ∅= 0,
with ψ∅= 1 on the omission locus at t = 0 (cf. App. B).
C.2 The Omission Principle (formal statement)
Theorem C.1 (Omission increases counterfactual multiplicity and en-
tropy). Let R be the revealed set and O = {o1, . . . , ok} a set of omitted
propositions. Suppose each oj ranges over a finite alphabet of size mj ≥2
and, conditional on R, the {oj} are mutually nonredundant (no oj is mea-
surable with respect to the sigma-algebra generated by R∪O \{oj}). Then:
(Multiplicity lower bound)
|Ω(R)| ≥∏k
j=1 mj · |Ω(R ∪O)|.
(Entropy lower bound) For the posterior over Θ,
H(R) = H(Θ|R) ≥H(Θ|R ∪O) + ∑k
j=1 I(Θ; oj|R ∪O \ {oj}),
and in particular, if each oj is (conditionally) near-uniform and informa-
tive,
H(R) ≥H(Θ|R ∪O) + ∑k
j=1 log mj.
(Attack-surface monotonicity) For any fixed λ ∈(0, 1],
Aλ(R) ≥Aλ(R ∪O).
Proof sketch. (1) Each omitted oj multiplies the number of consistent
completions by at least mj under nonredundancy; product rule yields the
bound. (2) Chain rule for entropy plus nonnegativity of conditional mutual
information gives the inequality; near-uniformity realizes the log mj lower
bound. (3) Since adding constraints can only prune models above any fixed
likelihood threshold, cardinality is monotone nonincreasing. □
Corollary C.2 (Binary omission bound). If oj ∈{0, 1} for all j and are
nonredundant, then omission of k binary facts increases posterior entropy
by at least k bits and multiplies |Ω(R)| by at least 2k.
6

Corollary C.3 (Commission vs. omission). Disclosing (commission) a
true proposition o reduces H(Θ|R) by I(Θ; o|R). Withholding (omission)
increases the same entropy by at least that amount relative to the fully
disclosed baseline.
C.3 Graph-theoretic formulation
Let G = (V, E) be a causal graph with structural equations F. A delib-
erate omission can be modeled as:
Edge omission: Removing an edge (u →v) from the message graph
EM (not necessarily from the true generative graph), thereby withholding a
dependence claim.
Node attribute omission: Withholding the value of a node X ∈V .
Proposition C.4 (Counterfactual branching via omitted edges). Assume a
family of interventions do(X = x) is admissible on G. If a message withholds
(u →v), then the number of counterfactual causal stories consistent with R
that differ only in the presence/absence or strength of that edge is at least
m, where m is the number of discretized levels for the u →v effect allowed
by prior model class Θ.
Consequently, |Ω(R)| scales by ≥m relative to
disclosing the specific edge model.
Proof sketch.
The withheld edge induces a model family over effect
sizes β ∈{β1, . . . , βm} (or continuous counterpart). Each β yields distinct
interventional distributions P(Y |do(u)) that are observationally indistin-
guishable under R; hence multiplicity ≥m. □
C.4 RSVP dynamics: omission as a null seed
Let ψ∅(x, t) be the null indicator field from App. B. An omission at
spacetime locus Γ0 seeds
ψ∅(x, 0) = 1Γ0(x), ∂tψ∅+ v · ∇ψ∅= 0.
Define the local counterfactual density κ(x, t) as the expected number
of consistent model completions per unit volume around x at time t. Then,
under mild regularity and a local product prior over omitted propositions,
κ(x, t) ≥
(∏
j:x∈inﬂ(oj,t) mj
)
· κbase(x, t),
where inﬂ(oj, t) is the domain reached by characteristics emanating from
the seed of oj under flow v.
Lemma C.5 (Front-entropy coupling). If ψ∅(x, t) = 1, then the posterior
entropy density h(x, t) satisfies
h(x, t) ≥hbase(x, t) + ∑
j:x∈inﬂ(oj,t) log mj.
Proof sketch. Transport of the null set along characteristics preserves
the local multiplicative factor from Theorem C.1; entropy adds in logs. □
C.5 Worked micro-example (binary)
Let O = {o1, . . . , ok} be k binary omissions and R fix all other relevant
facts. Then:
|Ω(R)| ≥2k |Ω(R ∪O)|.
H(Θ|R) ≥H(Θ|R ∪O) + k bits.
7

In RSVP, seeding ψ∅= 1 at k loci induces k fronts whose union raises
local h(x, t) by at least the number of fronts that have arrived at x.
This quantifies the intuitive asymmetry: a single sentence withheld can
spawn an exponential counterfactual tree, whereas a single sentence revealed
collapses only one branch.
C.6 Confessional mechanics (operational)
Your confession generator can enforce these bounds algorithmically:
Quota-aware omission budget. Track k omitted slots; estimate H↑≈
∑
j log mj. Refuse generation if H↑exceeds a policy threshold.
Entropy-aware redaction. When redacting, prefer propositions with high
redundancy given R (minimize I(Θ; oj|R)) to limit ∆H.
Front simulation. Use App. B's advection step to visualize (numeri-
cally) where omission-induced uncertainty is propagating in a document or
dialogue flow, guiding targeted clarification.
C.7 Limits and caveats
Redundancy and constraints. If omitted items are redundant given R,
the multiplicative bound weakens to the rank of independent omissions.
Replace ∑log mj by log det of an appropriate information matrix or by the
sum over an independent basis.
Continuous alphabets. For continuous oj, replace log mj by differential
entropy or by a metric entropy bound (e.g., log N(ε, Oj, ∥· ∥)).
Adversarial priors. Highly concentrated or dogmatic priors can cap ∆H;
the theorem is tight under broad (e.g., nearly uniform) priors over the omit-
ted coordinates.
C.8 One-line summary
Deliberate omission provably expands the counterfactual model set and
posterior entropy at least multiplicatively (in mj) and additively (in log mj),
and in RSVP dynamics this expansion propagates as a null wave front car-
rying constraints but no content.
D
The Second Law as Constraint Reduction
D.1 Setup and Notation
Let (X, µ) be a microstate space with reference measure µ (counting
measure in the discrete case, Liouville measure in Hamiltonian mechanics).
A constraint set C is a finite collection of measurable conditions of either
type:
Hard (microcanonical) constraints: gi(x) = 0 or gi(x) ∈Ii.
The admissible microstate set is Ω(C) = {x ∈X : x satisfies all C}.
Soft (canonical/MaxEnt) constraints: moment/expectation conditions
Ep[fi] = mi for a probability density p on X.
The feasible family is P(C) = {p ∈P(X) : Ep[fi] = mi, ∀i}.
Entropy functionals:
8

Microcanonical: Sµ(C) = kB ln µ(Ω(C)) (assuming 0 < µ(Ω) < ∞).
MaxEnt (Gibbs-Shannon): S[p] = −kB
∫
X p ln p dµ, and the maximal
entropy under C is
S∗(C) = supp∈P(C) S[p].
Write C2 ⪯C1 if C2 is no stronger than C1 (i.e., C2 removes some
constraints present in C1).
D.2 Second Law as Constraint Monotonicity
Theorem D.1 (Constraint reduction increases entropy). If C2 ⪯C1, then:
Microcanonical monotonicity
Ω(C1) ⊆Ω(C2) ⇒Sµ(C2) ≥Sµ(C1).
MaxEnt monotonicity
P(C1) ⊆P(C2) ⇒S∗(C2) ≥S∗(C1).
Proof. (1) If the admissible set enlarges, its µ-measure cannot decrease;
ln is monotone. (2) The supremum of a concave functional over a larger
feasible set cannot be smaller (standard convex analysis). □
Equality conditions.
Equality holds iff the removed constraints were
inactive/redundant: µ(Ω(C2)\Ω(C1)) = 0 in (1), or the added distributions
in P(C2) \ P(C1) do not raise S beyond the old maximizer in (2).
D.3 Slice Equivariance, Homogenization, and Symmetry Growth
Let a slice be a coarse-graining π : X →Y (macrostate map). A con-
straint set C determines a symmetry group GC ⊆Aut(X, µ) that preserves
both µ and C.
Define slice equivariance as invariance of the maximizer p∗
C under GC
modulo π: p∗
C ◦g induces the same macro-distribution for all g ∈GC.
Proposition D.2 (Constraint reduction grows effective symmetry).
If
C2 ⪯C1, then GC1 ⊆GC2 generically, and the corresponding MaxEnt
solution becomes more equivariant (more homogeneous across micro-orbits),
i.e., the macro-level distribution under π flattens.
Sketch. Fewer constraints identify more microstates as equivalent; the
MaxEnt solution spreads probability across the enlarged orbits.
This formalizes "homogenization": entropy increase can be viewed as
the growth of symmetry/equivariance of the slice induced by constraint re-
duction.
D.4 Thermodynamic Corollaries
Free energy. At fixed temperature T and mean energy U,
F = U −TS∗(C) ⇒C2 ⪯C1 =⇒F(C2) ≤F(C1).
Removing constraints lowers Helmholtz free energy (at fixed U, T) via
increased entropy.
H-theorem (coarse-grained). Many autonomous relaxations (elastic colli-
sions, diffusion) can be recast as dynamical shedding of effective constraints
(correlations, gradients), hence ˙S ≥0.
Isolated systems (Second Law). In an isolated system, admissible mi-
crotrajectories tend to reduce effective constraints (e.g., decorrelate dof's),
9

thus S is non-decreasing by Theorem D.1. Apparent violations arise when
hidden constraints (e.g., long-lived correlations) are reinstated.
D.5 RSVP and Communication Mappings
RSVP fields. Let S(x, t) be entropy density and κ(x, t) the local con-
straint count (number/strength). Then a constraint release rate σ(x, t) ≥0
yields
∂tS(x, t) = ∇· (JS) + α σ(x, t), α > 0,
with σ monotone in reductions of κ. This encodes the Second Law locally
as constraint-release-driven entropy production.
Null fronts (App.
B). An omission seeds a null indicator ψ∅whose
propagation expands the feasible set of inferences downstream; this is the
communication-theoretic analogue of constraint reduction.
Omission vs. commission (App. C). Disclosing a fact adds a constraint,
shrinking Ω(decreasing S∗); omitting removes a constraint, enlarging Ω
(increasing S∗). This is the same monotonicity expressed by Theorem D.1.
D.6 One-Line Restatement
Second Law (constraint form).
Entropy increases because dynamics
shed effective constraints: removing constraints enlarges the admissible mi-
crostate set or the feasible MaxEnt family, and maximal entropy is monotone
under this enlargement.
D.7 Caveats and Scope
Open systems: Energy/matter exchange can add constraints (driving,
feedback) and locally reduce entropy; global accounting (system+environment)
restores monotonicity.
Nonergodicity: If dynamics cannot explore Ω(C2), the accessible set may
remain small; the principle applies to the reachable constraint class.
Strong integrals of motion: Conserved quantities are irreducible con-
straints; entropy increases only within their invariant manifolds.
This appendix makes precise your reframing: "maximization of slice
equivariance and homogenization" is the effect of reducing the number/strength
of constraints; the Second Law follows as an entropy monotonicity theorem
under feasible-set enlargement.
E
Socio-Statistical Corollaries of the Second Law
E.1 Goodhart's Law as Constraint Reduction
Setup. Let a system have a performance vector X = (x1, . . . , xn) with
true objective utility U(X). A proxy measure M(X) is used for evaluation.
Initially, constraints C enforce correlations between M and U.
Once M becomes the sole target, constraints orthogonal to M are relaxed
or dropped.
Theorem E.1 (Goodhart-type entropy expansion). If a proxy M becomes
a direct optimization target, then:
10

The feasible set of strategies expands:
Ω(C, M) ⊆Ω(C′, M),
where C′ ⪯C (weaker constraints).
The counterfactual multiplicity of high-M but low-U states grows su-
perlinearly:
|{X : M(X) ≥τ, U(X) small}| ↑as constraints relax.
Entropy increases in the joint distribution over (M, U):
H(M, U|C′) ≥H(M, U|C).
Interpretation. Optimizing a measure is equivalent to removing multi-
dimensional constraints. This increases the entropy of possible outcomes
consistent with the measure, ensuring Goodhart-type pathologies.
E.2 Pareto Distributions as Entropic Outcomes
Setup. Consider a resource allocation process among agents/events {1, . . . , N}.
Each allocation step respects a constraint set C (e.g., conservation of total
resource). When most micro-constraints (e.g., equal sharing, bounded trans-
fer rates) are relaxed, entropy maximization governs the distribution.
Theorem E.2 (Pareto heavy-tail under weak constraints). Let C impose
only global conservation: ∑
i xi = R, xi ≥0. Then the maximum entropy
distribution over shares is asymptotically heavy-tailed:
Pr(X ≥x) ∼Cx−α, α > 1.
Sketch of proof. Under conservation alone, the uniform measure on the
simplex { xi ≥0, ∑xi = R } induces a marginal distribution on any
coordinate with a power-law tail.
Constraint reduction from equal-share
(Gaussian-like) regimes to minimal-constraint regimes yields Pareto heavy
tails. □
Corollary E.3 (80/20 law). For α in the empirical range [1.5, 2], about
20% of agents control
80% of the resource. This is the canonical Pareto
phenomenon arising from entropy maximization under minimal constraints.
E.3 Unified Restatement
Taken together, these literatures show three gaps that our work ad-
dresses: Goodhart's Law: When a measure becomes a target, omitted con-
straints expand the feasible space of behaviors. Entropy in the measure-
utility relation grows.
Pareto Distributions: When allocation constraints are relaxed to global
conservation alone, entropy maximization produces heavy-tailed distribu-
tions.
Corollary of the Second Law (socio-statistical form): In both measure-
ment systems and allocation systems, entropy rises as constraints are re-
duced. The observed laws (Goodhart, Pareto) are concrete manifestations
of the same underlying principle: the Second Law as constraint erosion.
E.4 Worked Toy Models
E.4.1 Goodhart's Law (proxy optimization distorts utility)
Model. True utility U = 0.7x1 + 0.3x2 + ϵ. Proxy M = x1 + η. Select
the top q = 10% by M.
11

metric
value
corr(M,U)
0.869402
mean U (all)
0.003070
mean M (all)
0.008905
Table 3: Goodhart baseline stats
metric
value
q (selected)
0.100000
corr(M,U) after selection
0.567302
mean U (selected)
1.205907
mean M (selected)
1.792632
Table 4: Goodhart after-selection stats
Interpretation: Treating a proxy as a target removes orthogonal con-
straints. This expands the feasible outcome set and inflates entropy in the
measure-utility relationship, manifesting Goodhart's law.
E.4.2 Pareto-like Tail via Preferential Attachment (minimal constraints)
Model.
N = 5000 agents start near zero.
Allocate 200,000 resource
quanta one-by-one via rich-get-richer dynamics. Only global conservation is
enforced.
Interpretation: Relaxing micro-constraints (fairness caps, bounded trans-
fers) allows entropy maximization to push the system into a Pareto regime:
scale-free, inequality-dominated, constraint-light.
E.5 Technical Takeaway
Both toy models concretely illustrate the Second Law as constraint re-
duction:
Goodhart: Dropping multidimensional constraints →entropy prolifer-
ates in the proxy-utility relation.
Pareto: Relaxing allocation constraints →entropy proliferates into heavy-
tailed distributions.
In each case, the observed pathology (Goodhart distortion, Pareto skew)
is not an anomaly but a corollary of the same universal law: entropy in-
creases whenever constraints are shed.
E.6 A General Constraint-Entropy Law
E.6.1 Setup
Let C denote a (possibly soft) set of constraints on states X or distribu-
tions p.
Define the feasible set F(C) (microcanonical: states; MaxEnt: distribu-
tions).
Let S be microcanonical Sµ = kB ln µ(F) or Gibbs-Shannon S[p] =
−kB
∫
p ln p.
12

group
meantrueU
meanproxyM
corrMUingroup
Top-q by M
1.205907
1.792632
0.567302
Top-q by U
1.379006
1.538616
0.587847
Table 5: Goodhart: Top-q by M vs Top-q by U
metric
value
agents
5000
total quanta
200000
Gini approx
0.732
top 1% share
0.304
top 10% share
0.705
Table 6: Preferential Attachment summary stats
Define an effective constraint complexity Ceﬀ(C) (larger means stricter)
by any monotone choice, e.g.:
microcanonical: Ceﬀ:= −ln µ(F(C)) (up to an additive constant),
MaxEnt: Ceﬀ:= −supp∈F(C) S[p]/kB,
information view: Ceﬀ:= rank IC for an information/regularization op-
erator IC.
E.6.2 Law (discrete form)
Theorem E.4 (Constraint-Entropy Monotonicity).
If C2 ⪯C1 (i.e.,
F(C1) ⊆F(C2)), then
∆S ≡S∗(C2) −S∗(C1) ≥kB ln |F(C2)|
|F(C1)| = kB ∆log |F|.
In particular, any reduction of effective constraint complexity (∆Ceﬀ≤
0) yields ∆S ≥0.
Sketch.
Supremum of a concave functional over a larger feasible set
cannot decrease; counting measure gives the log-ratio bound.
E.6.3 Law (differential form)
Assume a smooth constraint path C(t) with feasible sets F(t). Define
the constraint-release rate
˙Ceﬀ(t) := −d
dt
(
S∗(C(t))
kB
)
.
Then along any admissible (autonomous, coarse-graining-respecting) evo-
lution,
d
dtS∗(C(t)) = −kB ˙Ceﬀ(t) + Π(t),
where Π(t) ≥0 aggregates internal production (mixing, decorrelation).
In isolation, ˙Ceﬀ(t) ≤0 and Π(t) ≥0, so dS∗/dt ≥0.
E.6.4 Instantiations
(i) Goodhart's selection on a proxy
Let U be true utility, M a proxy, and C encode multi-dimensional behav-
ioral constraints (norms, costs, penalties). Turning the target to "maximize
13

M" relaxes orthogonal constraints:
C′ ⪯C, ⇒∆S ≥kB ln |F(C′)|
|F(C)| .
Conditioning on the selection event M ≥τ,
H(U|M ≥τ, C′) ≥H(U|M ≥τ, C),
and the Goodhart distortion (mass of high-M, low-U states) expands at
least proportionally to the feasible-set growth.
Equivalently, with k omitted constraints each admitting mj behaviors,
∆S ≥kB
∑k
j=1 ln mj.
(ii) Pareto heavy tails under minimal constraints
Consider allocation x ∈RN
≥0 with only conservation ∑
i xi = R. Then
F is the simplex; adding micro-constraints (caps, frictions) shrinks F. Un-
der preferential/scale-free kernels (constraint-light dynamics), feasible-set
enlargement toward the simplex yields a heavy-tailed stationary law with
CCDF ∼x−α (α > 1), consistent with
∆S ≥kB ln
|simplex|
|constrained polytope|,
and empirically α decreases as constraints weaken (heavier tails).
E.6.5 Practical corollaries
Entropy budget for targeting:
To avoid Goodhart, require that any
change of objective obey ∆S ≤ϵ, i.e., cap the feasible-set expansion (equiv-
alently, keep ∑
j ln mj ≤ϵ/kB).
Tail-risk guardrails: In allocation systems, enforce micro-constraints (rate
limits, caps, taxation) to keep |F| bounded away from the simplex and
thereby bound tail exponents α.
E.6.6 One-line unifier
Constraint-Entropy Law.
Any relaxation of effective constraints in-
creases maximal entropy by at least the log-ratio of feasible sets. Goodhart
distortions and Pareto heavy tails are two faces of this same mechanism.
References
Keith H. Basso. To give up on words: Silence in western apache culture.
Southwestern Journal of Anthropology, 26(3):213-230, 1970.
Ruha Benjamin. Race after technology: Abolitionist tools for the new jim
code. Polity Press, 2019.
Lisa Gitelman, editor. "Raw Data" Is an Oxymoron. MIT Press, 2013.
Kurt Gödel. Über formal unentscheidbare sätze der principia mathematica
und verwandter systeme i. Monatshefte für Mathematik und Physik, 38:
173-198, 1931.
Werner Heisenberg. Über den anschaulichen inhalt der quantentheoretischen
kinematik und mechanik. Zeitschrift für Physik, 43(3-4):172-198, 1927.
14

Arata Isozaki. Ma: Space-Time in Japan. Japan House, 2006.
Adam Jaworski. The Power of Silence: Social and Pragmatic Perspectives.
SAGE, 1993.
Thomas S. Kuhn. The Structure of Scientific Revolutions. University of
Chicago Press, 1962.
Ludwig Wittgenstein.
Tractatus Logico-Philosophicus.
Routledge, 1961.
(Original work published 1921).
15

