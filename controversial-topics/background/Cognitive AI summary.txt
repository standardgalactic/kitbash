Cognitive AI summary
Saltar al contenido
Abrir barra lateral
ChatGPT
5
ChatGPT
5
Compartir
ChatGPT dijo:
Pensado durante 45s
Want a quick executive summary of the Bradford talk you dropped in? Here's the crisp version.
Executive summary
Thesis.
Today's "generative AI" is
unsustainable
(energy, cost, reliability) and structurally ill-suited to true language understanding. The proposed successor is
Cognitive AI
, built from
brain science + linguistics
(specifically, Role & Reference Grammar, RRG) rather than big statistics alone.
[English (auto-generated)] Brad...
[English (auto-generated)] Brad...
Core claim.
Cognitive AI separates
linguistic skill
(mapping words→meaning) from
world knowledge
(context, facts), aiming to eliminate combinatorial blow-ups and reduce hallucinations. Predicates "determine their arguments," constraining what can sensibly combine.
[English (auto-generated)] Brad...
Why now / "crisis."
Generative AI overpromises (robots, universal AGI) yet still fails at robust understanding; governments and tech giants are pouring money into energy-hungry data centers—seen here as the wrong path.
[English (auto-generated)] Brad...
[English (auto-generated)] Brad...
What the demo showed
Context tracking & Q/A.
Add entities to context and the system answers "Who arrived?" correctly as context evolves (Beth → Beth & John → Beth, John, Sue, Dana), and handles tense/pronominal answers ("She did," "He did").
[English (auto-generated)] Brad...
Meaning generalization.
From specific events ("John ate cereal") the system answers passives ("Who was the cereal eaten by?"), multi-wh questions ("Who ate what?"), and contrasts ("Did Sue eat the cereal? — No, John did").
[English (auto-generated)] Brad...
Ambiguity & prepositions.
Location/state depends on meaning ("at the beach," "on the mat," "in the kitchen")—not mere word patterns.
[English (auto-generated)] Brad...
Nominalizations & event structure.
From "the destruction of the city by the vandals," it still recovers
who destroyed what
via predicate meaning (destruction ↔ destroy).
[English (auto-generated)] Brad...
The linguistic engine (RRG) in a nutshell
Move from
triples
and parts-of-speech-first parsing to
meaning representations
where predicates have 0/1/2/3+ arguments, causal structure, and event/state transformations (e.g.,
die
= "become dead";
kill
= "cause(become dead)"). This supports principled generalization and cleaner knowledge graphs.
[English (auto-generated)] Brad...
[English (auto-generated)] Brad...
RRG + semiotics: map
form → meaning → interpretation in context
, not just syntax trees.
[English (auto-generated)] Brad...
Engineering posture
Treat meanings as
typed predicates + roles
; build a
tested repository
per language (unit & integration tests) so new vocabulary can be added without regressions. Aim for deterministic, transparent behavior.
[English (auto-generated)] Brad...
Where they want this used
Enterprise assistants
(deterministic, auditable language understanding) and
language learning
(brain-inspired "brain soaking" & "face phonics" for easier L2 acquisition).
[English (auto-generated)] Brad...
[English (auto-generated)] Brad...
Why it matters (according to the talk)
Sustainability.
Reduce dependence on giant pattern-matching stacks; do more with meaning + context rather than brute force compute.
[English (auto-generated)] Brad...
Trust/controllability.
If the system always builds an explicit meaning structure, you can verify
why
an answer holds (and correct it) rather than shrugging at a latent vector.
If you want, I can convert this into:
a one-page brief for stakeholders, or
a side-by-side "Cognitive AI vs. GenAI" checklist you can drop into your RSVP-TARTAN docs.
Fuentes
ChatGPT puede cometer errores. Comprueba la información importante. Consulta
Preferencias de cookies
.

