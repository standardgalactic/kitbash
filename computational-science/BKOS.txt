
Computational Geometry
Third Edition

Mark de Berg · Otfried Cheong
Marc van Kreveld · Mark Overmars
Computational Geometry
Algorithms and Applications
Third Edition
123

Prof. Dr. Mark de Berg
Department of Mathematics
and Computer Science
TU Eindhoven
P.O. Box 513
5600 MB Eindhoven
The Netherlands
mdberg@win.tue.nl
Dr. Otfried Cheong, n´e Schwarzkopf
Department of Computer Science
KAIST
Gwahangno 335, Yuseong-gu
Daejeon 305-701
Korea
otfried@kaist.edu
Dr. Marc van Kreveld
Department of Information
and Computing Sciences
Utrecht University
P.O. Box 80.089
3508 TB Utrecht
The Netherlands
marc@cs.uu.nl
Prof. Dr. Mark Overmars
Department of Information
and Computing Sciences
Utrecht University
P.O. Box 80.089
3508 TB Utrecht
The Netherlands
markov@cs.uu.nl
ISBN 978-3-540-77973-5
DOI 10.1007/978-3-540-77974-2
e-ISBN 978-3-540-77974-2
ACM Computing Classiﬁcation (1998): F.2.2, I.3.5
Library of Congress Control Number: 2008921564
© 2008, 2000, 1997 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlm or in any other way, and storage in data banks. Duplication
of this publication or parts thereof is permitted only under the provisions of the German Copyright
Law of September 9, 1965, in its current version, and permission for use must always be obtained
from Springer. Violations are liable for prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Cover design: KünkelLopka, Heidelberg
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

Preface
Computational geometry emerged from the ﬁeld of algorithms design and
analysis in the late 1970s. It has grown into a recognized discipline with its
own journals, conferences, and a large community of active researchers. The
success of the ﬁeld as a research discipline can on the one hand be explained
from the beauty of the problems studied and the solutions obtained, and, on the
other hand, by the many application domains—computer graphics, geographic
information systems (GIS), robotics, and others—in which geometric algorithms
play a fundamental role.
For many geometric problems the early algorithmic solutions were either
slow or difﬁcult to understand and implement. In recent years a number of new
algorithmic techniques have been developed that improved and simpliﬁed many
of the previous approaches. In this textbook we have tried to make these modern
algorithmic solutions accessible to a large audience. The book has been written
as a textbook for a course in computational geometry, but it can also be used for
self-study.
Structure of the book.
Each of the sixteen chapters (except the introductory
chapter) starts with a problem arising in one of the application domains. This
problem is then transformed into a purely geometric one, which is solved
using techniques from computational geometry. The geometric problem and the
concepts and techniques needed to solve it are the real topic of each chapter. The
choice of the applications was guided by the topics in computational geometry
we wanted to cover; they are not meant to provide a good coverage of the
application domains. The purpose of the applications is to motivate the reader;
the goal of the chapters is not to provide ready-to-use solutions for them. Having
said this, we believe that knowledge of computational geometry is important
to solve geometric problems in application areas efﬁciently. We hope that our
book will not only raise the interest of people from the algorithms community,
but also from people in the application areas.
For most geometric problems treated we give just one solution, even when
a number of different solutions exist. In general we have chosen the solution
that is easiest to understand and implement. This is not necessarily the most
efﬁcient solution. We also took care that the book contains a good mixture of
techniques like divide-and-conquer, plane sweep, and randomized algorithms.
We decided not to treat all sorts of variations to the problems; we felt it is more
important to introduce all main topics in computational geometry than to give
more detailed information about a smaller number of topics.
v

PREFACE
Several chapters contain one or more sections marked with a star. They con-
tain improvements of the solution, extensions, or explain the relation between
various problems. They are not essential for understanding the remainder of the
book.
Every chapter concludes with a section that is entitled Notes and Comments.
These sections indicate where the results described in the chapter originated,
mention other solutions, generalizations, and improvements, and provide refer-
ences. They can be skipped, but do contain useful material for those who want
to know more about the topic of the chapter.
At the end of each chapter a number of exercises is provided. These range
from easy tests to check whether the reader understands the material to more
elaborate questions that extend the material covered. Difﬁcult exercises and
exercises about starred sections are indicated with a star.
A course outline.
Even though the chapters in this book are largely indepen-
dent, they should preferably not be treated in an arbitrary order. For instance,
Chapter 2 introduces plane sweep algorithms, and it is best to read this chapter
before any of the other chapters that use this technique. Similarly, Chapter 4
should be read before any other chapter that uses randomized algorithms.
For a ﬁrst course on computational geometry, we advise treating Chapters 1-
10 in the given order. They cover the concepts and techniques that, according
to us, should be present in any course on computational geometry. When more
material can be covered, a selection can be made from the remaining chapters.
Prerequisites.
The book can be used as a textbook for a high-level under-
graduate course or a low-level graduate course, depending on the rest of the
curriculum. Readers are assumed to have a basic knowledge of the design and
analysis of algorithms and data structures: they should be familiar with big-Oh
notations and simple algorithmic techniques like sorting, binary search, and
balanced search trees. No knowledge of the application domains is required, and
hardly any knowledge of geometry. The analysis of the randomized algorithms
uses some very elementary probability theory.
Implementations.
The algorithms in this book are presented in a pseudo-
code that, although rather high-level, is detailed enough to make it relatively
easy to implement them. In particular we have tried to indicate how to handle
degenerate cases, which are often a source of frustration when it comes to
implementing.
We believe that it is very useful to implement one or more of the algorithms;
it will give a feeling for the complexity of the algorithms in practice. Each
chapter can be seen as a programming project. Depending on the amount of
time available one can either just implement the plain geometric algorithms, or
implement the application as well.
To implement a geometric algorithm a number of basic data types—points,
lines, polygons, and so on—and basic routines that operate on them are needed.
Implementing these basic routines in a robust manner is not easy, and takes a lot
vi

PREFACE
of time. Although it is good to do this at least once, it is useful to have a software
library available that contains the basic data types and routines. Pointers to such
libraries can be found on our Web site.
Web site.
This book is accompanied by a Web site, which contains a list of
errata collected for each edition of the book, all ﬁgures and the pseudo code for
all algorithms, as well as some other resources. The address is
http://www.cs.uu.nl/geobook/
You can also use the address given on our Web site to send us errors you
have found, or any other comments you have about the book.
About the third edition.
This third edition contains two major additions: In
Chapter 7, on Voronoi diagrams, we now also discuss Voronoi diagrams of line
segments and farthest-point Voronoi diagrams. In Chapter 12, we have included
an extra section on binary space partition trees for low-density scenes, as an
introduction to realistic input models. In addition, a large number of small and
some larger errors have been corrected (see the list of errata for the second
edition on the Web site). We have also updated the notes and comments of every
chapter to include references to recent results and recent literature. We have
tried not to change the numbering of sections and exercises, so that it should be
possible for students in a course to still use the second edition.
Acknowledgments.
Writing a textbook is a long process, even with four
authors. Many people contributed to the original ﬁrst edition by providing
useful advice on what to put in the book and what not, by reading chapters and
suggesting changes, and by ﬁnding and correcting errors. Many more provided
feedback and found errors in the ﬁrst two editions. We would like to thank all of
them, in particular Pankaj Agarwal, Helmut Alt, Marshall Bern, Jit Bose, Hazel
Everett, Gerald Farin, Steve Fortune, Geert-Jan Giezeman, Mordecai Golin, Dan
Halperin, Richard Karp, Matthew Katz, Klara Kedem, Nelson Max, Joseph S. B.
Mitchell, Ren´e van Oostrum, G¨unter Rote, Henry Shapiro, Sven Skyum, Jack
Snoeyink, Gert Vegter, Peter Widmayer, Chee Yap, and G¨unther Ziegler. We
also would like to thank Springer-Verlag for their advice and support during the
creation of this book, its new editions, and the translations into other languages
(at the time of writing, Japanese, Chinese, and Polish).
Finally we would like to acknowledge the support of the Netherlands' Or-
ganization for Scientiﬁc Research (N.W.O.) and the Korea Research Founda-
tion (KRF).
January 2008
Mark de Berg
Otfried Cheong
Marc van Kreveld
Mark Overmars
vii

Contents
1
Computational Geometry
1
Introduction
1.1
An Example: Convex Hulls
2
1.2
Degeneracies and Robustness
8
1.3
Application Domains
10
1.4
Notes and Comments
13
1.5
Exercises
15
2
Line Segment Intersection
19
Thematic Map Overlay
2.1
Line Segment Intersection
20
2.2
The Doubly-Connected Edge List
29
2.3
Computing the Overlay of Two Subdivisions
33
2.4
Boolean Operations
39
2.5
Notes and Comments
40
2.6
Exercises
41
3
Polygon Triangulation
45
Guarding an Art Gallery
3.1
Guarding and Triangulations
46
3.2
Partitioning a Polygon into Monotone Pieces
49
3.3
Triangulating a Monotone Polygon
55
3.4
Notes and Comments
59
3.5
Exercises
60
4
Linear Programming
63
Manufacturing with Molds
4.1
The Geometry of Casting
64
4.2
Half-Plane Intersection
66
4.3
Incremental Linear Programming
71
4.4
Randomized Linear Programming
76
ix

CONTENTS
4.5
Unbounded Linear Programs
79
4.6* Linear Programming in Higher Dimensions
82
4.7* Smallest Enclosing Discs
86
4.8
Notes and Comments
89
4.9
Exercises
91
5
Orthogonal Range Searching
95
Querying a Database
5.1
1-Dimensional Range Searching
96
5.2
Kd-Trees
99
5.3
Range Trees
105
5.4
Higher-Dimensional Range Trees
109
5.5
General Sets of Points
110
5.6* Fractional Cascading
111
5.7
Notes and Comments
115
5.8
Exercises
117
6
Point Location
121
Knowing Where You Are
6.1
Point Location and Trapezoidal Maps
122
6.2
A Randomized Incremental Algorithm
128
6.3
Dealing with Degenerate Cases
137
6.4* A Tail Estimate
140
6.5
Notes and Comments
143
6.6
Exercises
144
7
Voronoi Diagrams
147
The Post Ofﬁce Problem
7.1
Deﬁnition and Basic Properties
148
7.2
Computing the Voronoi Diagram
151
7.3
Voronoi Diagrams of Line Segments
160
7.4
Farthest-Point Voronoi Diagrams
163
7.5
Notes and Comments
167
7.6
Exercises
170
8
Arrangements and Duality
173
Supersampling in Ray Tracing
8.1
Computing the Discrepancy
175
8.2
Duality
177
8.3
Arrangements of Lines
179
8.4
Levels and Discrepancy
185
x

CONTENTS
8.5
Notes and Comments
186
8.6
Exercises
188
9
Delaunay Triangulations
191
Height Interpolation
9.1
Triangulations of Planar Point Sets
193
9.2
The Delaunay Triangulation
196
9.3
Computing the Delaunay Triangulation
199
9.4
The Analysis
205
9.5* A Framework for Randomized Algorithms
208
9.6
Notes and Comments
214
9.7
Exercises
215
10 More Geometric Data Structures
219
Windowing
10.1 Interval Trees
220
10.2 Priority Search Trees
226
10.3 Segment Trees
231
10.4 Notes and Comments
237
10.5 Exercises
239
11 Convex Hulls
243
Mixing Things
11.1 The Complexity of Convex Hulls in 3-Space
244
11.2 Computing Convex Hulls in 3-Space
246
11.3* The Analysis
250
11.4* Convex Hulls and Half-Space Intersection
253
11.5* Voronoi Diagrams Revisited
254
11.6 Notes and Comments
256
11.7 Exercises
257
12 Binary Space Partitions
259
The Painter's Algorithm
12.1 The Deﬁnition of BSP Trees
261
12.2 BSP Trees and the Painter's Algorithm
263
12.3 Constructing a BSP Tree
264
12.4* The Size of BSP Trees in 3-Space
268
12.5 BSP Trees for Low-Density Scenes
271
12.6 Notes and Comments
278
12.7 Exercises
279
xi

CONTENTS
13 Robot Motion Planning
283
Getting Where You Want to Be
13.1 Work Space and Conﬁguration Space
284
13.2 A Point Robot
286
13.3 Minkowski Sums
290
13.4 Translational Motion Planning
297
13.5* Motion Planning with Rotations
299
13.6 Notes and Comments
303
13.7 Exercises
305
14 Quadtrees
307
Non-Uniform Mesh Generation
14.1 Uniform and Non-Uniform Meshes
308
14.2 Quadtrees for Point Sets
309
14.3 From Quadtrees to Meshes
315
14.4 Notes and Comments
318
14.5 Exercises
320
15 Visibility Graphs
323
Finding the Shortest Route
15.1 Shortest Paths for a Point Robot
324
15.2 Computing the Visibility Graph
326
15.3 Shortest Paths for a Translating Polygonal Robot
330
15.4 Notes and Comments
331
15.5 Exercises
332
16 Simplex Range Searching
335
Windowing Revisited
16.1 Partition Trees
336
16.2 Multi-Level Partition Trees
343
16.3 Cutting Trees
346
16.4 Notes and Comments
352
16.5 Exercises
353
Bibliography
357
Index
377
xii

1
Computational Geometry
Introduction
Imagine you are walking on the campus of a university and suddenly you realize
you have to make an urgent phone call. There are many public phones on
campus and of course you want to go to the nearest one. But which one is the
nearest? It would be helpful to have a map on which you could look up the
nearest public phone, wherever on campus you are. The map should show a
subdivision of the campus into regions, and for each region indicate the nearest
public phone. What would these regions look like? And how could we compute
them?
Even though this is not such a terribly important issue, it describes the basics
of a fundamental geometric concept, which plays a role in many applications.
The subdivision of the campus is a so-called Voronoi diagram, and it will be
studied in Chapter 7 in this book. It can be used to model trading areas of
different cities, to guide robots, and even to describe and simulate the growth
of crystals. Computing a geometric structure like a Voronoi diagram requires
geometric algorithms. Such algorithms form the topic of this book.
A second example. Assume you located the closest public phone. With
a campus map in hand you will probably have little problem in getting to the
phone along a reasonably short path, without hitting walls and other objects.
But programming a robot to perform the same task is a lot more difﬁcult. Again,
the heart of the problem is geometric: given a collection of geometric obstacles,
we have to ﬁnd a short connection between two points, avoiding collisions with
the obstacles. Solving this so-called motion planning problem is of crucial
importance in robotics. Chapters 13 and 15 deal with geometric algorithms
required for motion planning.
A third example. Assume you don't have one map but two: one with
a description of the various buildings, including the public phones, and one
indicating the roads on the campus. To plan a motion to the public phone we
have to overlay these maps, that is, we have to combine the information in
the two maps. Overlaying maps is one of the basic operations of geographic
information systems. It involves locating the position of objects from one map
in the other, computing the intersection of various features, and so on. Chapter 2
deals with this problem.
1

Chapter 1
COMPUTATIONAL GEOMETRY
These are just three examples of geometric problems requiring carefully de-
signed geometric algorithms for their solution. In the 1970s the ﬁeld of compu-
tational geometry emerged, dealing with such geometric problems. It can be
deﬁned as the systematic study of algorithms and data structures for geometric
objects, with a focus on exact algorithms that are asymptotically fast. Many
researchers were attracted by the challenges posed by the geometric problems.
The road from problem formulation to efﬁcient and elegant solutions has often
been long, with many difﬁcult and sub-optimal intermediate results. Today there
is a rich collection of geometric algorithms that are efﬁcient, and relatively easy
to understand and implement.
This book describes the most important notions, techniques, algorithms,
and data structures from computational geometry in a way that we hope will be
attractive to readers who are interested in applying results from computational
geometry. Each chapter is motivated with a real computational problem that
requires geometric algorithms for its solution. To show the wide applicability
of computational geometry, the problems were taken from various application
areas: robotics, computer graphics, CAD/CAM, and geographic information
systems.
You should not expect ready-to-implement software solutions for major
problems in the application areas. Every chapter deals with a single concept in
computational geometry; the applications only serve to introduce and motivate
the concepts. They also illustrate the process of modeling an engineering
problem and ﬁnding an exact solution.
1.1
An Example: Convex Hulls
Good solutions to algorithmic problems of a geometric nature are mostly based
on two ingredients. One is a thorough understanding of the geometric properties
of the problem, the other is a proper application of algorithmic techniques and
data structures. If you don't understand the geometry of the problem, all the
algorithms of the world won't help you to solve it efﬁciently. On the other hand,
even if you perfectly understand the geometry of the problem, it is hard to solve
it effectively if you don't know the right algorithmic techniques. This book will
give you a thorough understanding of the most important geometric concepts
and algorithmic techniques.
To illustrate the issues that arise in developing a geometric algorithm, this
section deals with one of the ﬁrst problems that was studied in computational
geometry: the computation of planar convex hulls. We'll skip the motivation
for this problem here; if you are interested you can read the introduction to
Chapter 11, where we study convex hulls in 3-dimensional space.
A subset S of the plane is called convex if and only if for any pair of points
convex
not convex
pq
p
q
pq
p
q
p,q ∈S the line segment pq is completely contained in S. The convex hull
CH(S) of a set S is the smallest convex set that contains S. To be more precise,
it is the intersection of all convex sets that contain S.
2

Section 1.1
AN EXAMPLE: CONVEX HULLS
We will study the problem of computing the convex hull of a ﬁnite set P
of n points in the plane. We can visualize what the convex hull looks like by a
thought experiment. Imagine that the points are nails sticking out of the plane,
take an elastic rubber band, hold it around the nails, and let it go. It will snap
around the nails, minimizing its length. The area enclosed by the rubber band
is the convex hull of P. This leads to an alternative deﬁnition of the convex
hull of a ﬁnite set P of points in the plane: it is the unique convex polygon
whose vertices are points from P and that contains all points of P. Of course
we should prove rigorously that this is well deﬁned—that is, that the polygon is
unique—and that the deﬁnition is equivalent to the one given earlier, but let's
skip that in this introductory chapter.
How do we compute the convex hull? Before we can answer this question we
must ask another question: what does it mean to compute the convex hull?
As we have seen, the convex hull of P is a convex polygon. A natural way
to represent a polygon is by listing its vertices in clockwise order, starting
with an arbitrary one. So the problem we want to solve is this: given a set
P = {p1, p2,..., pn} of points in the plane, compute a list that contains those
points from P that are the vertices of CH(P), listed in clockwise order.
p1
p2
p3
p4
p5
p6
p7
p8
p9
p1, p2, p3, p4, p5, p6, p7, p8, p9
input = set of points:
output = representation of the convex hull:
p4, p5, p8, p2, p9
Figure 1.1
Computing a convex hull
The ﬁrst deﬁnition of convex hulls is of little help when we want to design
an algorithm to compute the convex hull. It talks about the intersection of all
convex sets containing P, of which there are inﬁnitely many. The observation
that CH(P) is a convex polygon is more useful. Let's see what the edges of
CH(P) are. Both endpoints p and q of such an edge are points of P, and if we
direct the line through p and q such that CH(P) lies to the right, then all the
points of P must lie to the right of this line. The reverse is also true: if all points
of P\{p,q} lie to the right of the directed line through p and q, then pq is an
edge of CH(P).
p
q
Now that we understand the geometry of the problem a little bit better we can
develop an algorithm. We will describe it in a style of pseudocode we will use
throughout this book.
Algorithm SLOWCONVEXHULL(P)
Input. A set P of points in the plane.
Output. A list L containing the vertices of CH(P) in clockwise order.
1.
E ←/0.
2.
for all ordered pairs (p,q) ∈P×P with p not equal to q
3.
do valid ←true
3

Chapter 1
COMPUTATIONAL GEOMETRY
4.
for all points r ∈P not equal to p or q
5.
do if r lies to the left of the directed line from p to q
6.
then valid ←false.
7.
if valid then Add the directed edge
→
pq to E.
8.
From the set E of edges construct a list L of vertices of CH(P), sorted in
clockwise order.
Two steps in the algorithm are perhaps not entirely clear.
The ﬁrst one is line 5: how do we test whether a point lies to the left or to the
right of a directed line? This is one of the primitive operations required in most
geometric algorithms. Throughout this book we assume that such operations
are available. It is clear that they can be performed in constant time so the
actual implementation will not affect the asymptotic running time in order of
magnitude. This is not to say that such primitive operations are unimportant or
trivial. They are not easy to implement correctly and their implementation will
affect the actual running time of the algorithm. Fortunately, software libraries
containing such primitive operations are nowadays available. We conclude that
we don't have to worry about the test in line 5; we may assume that we have a
function available performing the test for us in constant time.
The other step of the algorithm that requires some explanation is the last one.
In the loop of lines 2-7 we determine the set E of convex hull edges. From E we
can construct the list L as follows. The edges in E are directed, so we can speak
about the origin and the destination of an edge. Because the edges are directed
⃗e1
⃗e2
destination of ⃗e1
= origin of ⃗e2
origin of ⃗e1
such that the other points lie to their right, the destination of an edge comes
after its origin when the vertices are listed in clockwise order. Now remove
an arbitrary edge ⃗e1 from E. Put the origin of ⃗e1 as the ﬁrst point into L, and
the destination as the second point. Find the edge ⃗e2 in E whose origin is the
destination of ⃗e1, remove it from E, and append its destination to L. Next, ﬁnd
the edge ⃗e3 whose origin is the destination of ⃗e2, remove it from E, and append
its destination to L. We continue in this manner until there is only one edge left
in E. Then we are done; the destination of the remaining edge is necessarily the
origin of ⃗e1, which is already the ﬁrst point in L. A simple implementation of
this procedure takes O(n2) time. This can easily be improved to O(nlogn), but
the time required for the rest of the algorithm dominates the total running time
anyway.
Analyzing the time complexity of SLOWCONVEXHULL is easy. We check
n2 −n pairs of points. For each pair we look at the n −2 other points to see
whether they all lie on the right side. This will take O(n3) time in total. The
ﬁnal step takes O(n2) time, so the total running time is O(n3). An algorithm
with a cubic running time is too slow to be of practical use for anything but the
smallest input sets. The problem is that we did not use any clever algorithmic
design techniques; we just translated the geometric insight into an algorithm in
a brute-force manner. But before we try to do better, it is useful to make several
observations about this algorithm.
We have been a bit careless when deriving the criterion of when a pair p,q
deﬁnes an edge of CH(P). A point r does not always lie to the right or to the
4

Section 1.1
AN EXAMPLE: CONVEX HULLS
left of the line through p and q, it can also happen that it lies on this line. What
should we do then? This is what we call a degenerate case, or a degeneracy for
short. We prefer to ignore such situations when we ﬁrst think about a problem,
so that we don't get confused when we try to ﬁgure out the geometric properties
of a problem. However, these situations do arise in practice. For instance, if
we create the points by clicking on a screen with a mouse, all points will have
small integer coordinates, and it is quite likely that we will create three points
on a line.
To make the algorithm correct in the presence of degeneracies we must
reformulate the criterion above as follows: a directed edge
→
pq is an edge of
CH(P) if and only if all other points r ∈P lie either strictly to the right of the
directed line through p and q, or they lie on the open line segment pq. (We
assume that there are no coinciding points in P.) So we have to replace line 5 of
the algorithm by this more complicated test.
We have been ignoring another important issue that can inﬂuence the correctness
of the result of our algorithm. We implicitly assumed that we can somehow
test exactly whether a point lies to the right or to the left of a given line. This
is not necessarily true: if the points are given in ﬂoating point coordinates and
the computations are done using ﬂoating point arithmetic, then there will be
rounding errors that may distort the outcome of tests.
p
q
r
p
q
r
Imagine that there are three points p, q, and r, that are nearly collinear, and
that all other points lie far to the right of them. Our algorithm tests the pairs
(p,q), (r,q), and (p,r). Since these points are nearly collinear, it is possible that
the rounding errors lead us to decide that r lies to the right of the line from p to
q, that p lies to the right of the line from r to q, and that q lies to the right of the
line from p to r. Of course this is geometrically impossible—but the ﬂoating
point arithmetic doesn't know that! In this case the algorithm will accept all
three edges. Even worse, all three tests could give the opposite answer, in which
case the algorithm rejects all three edges, leading to a gap in the boundary of
the convex hull. And this leads to a serious problem when we try to construct
the sorted list of convex hull vertices in the last step of our algorithm. This step
assumes that there is exactly one edge starting in every convex hull vertex, and
exactly one edge ending there. Due to the rounding errors there can suddenly be
two, or no, edges starting in vertex p. This can cause the program implementing
our simple algorithm to crash, since the last step has not been designed to deal
with such inconsistent data.
Although we have proven the algorithm to be correct and to handle all
special cases, it is not robust: small errors in the computations can make it
fail in completely unexpected ways. The problem is that we have proven the
correctness assuming that we can compute exactly with real numbers.
We have designed our ﬁrst geometric algorithm. It computes the convex hull
of a set of points in the plane. However, it is quite slow—its running time is
O(n3)—, it deals with degenerate cases in an awkward way, and it is not robust.
We should try to do better.
5

Chapter 1
COMPUTATIONAL GEOMETRY
To this end we apply a standard algorithmic design technique: we will
develop an incremental algorithm. This means that we will add the points in P
one by one, updating our solution after each addition. We give this incremental
approach a geometric ﬂavor by adding the points from left to right. So we ﬁrst
sort the points by x-coordinate, obtaining a sorted sequence p1,..., pn, and then
we add them in that order. Because we are working from left to right, it would
be convenient if the convex hull vertices were also ordered from left to right
as they occur along the boundary. But this is not the case. Therefore we ﬁrst
p1
pn
upper hull
lower hull
compute only those convex hull vertices that lie on the upper hull, which is the
part of the convex hull running from the leftmost point p1 to the rightmost point
pn when the vertices are listed in clockwise order. In other words, the upper
hull contains the convex hull edges bounding the convex hull from above. In a
second scan, which is performed from right to left, we compute the remaining
part of the convex hull, the lower hull.
The basic step in the incremental algorithm is the update of the upper hull
after adding a point pi. In other words, given the upper hull of the points
p1,..., pi−1, we have to compute the upper hull of p1,..., pi. This can be done
as follows. When we walk around the boundary of a polygon in clockwise order,
we make a turn at every vertex. For an arbitrary polygon this can be both a
right turn and a left turn, but for a convex polygon every turn must be a right
turn. This suggests handling the addition of pi in the following way. Let Lupper
pi
points deleted
be a list that stores the upper vertices in left-to-right order. We ﬁrst append pi
to Lupper. This is correct because pi is the rightmost point of the ones added so
far, so it must be on the upper hull. Next, we check whether the last three points
in Lupper make a right turn. If this is the case there is nothing more to do; Lupper
contains the vertices of the upper hull of p1,..., pi, and we can proceed to the
next point, pi+1. But if the last three points make a left turn, we have to delete
the middle one from the upper hull. In this case we are not ﬁnished yet: it could
be that the new last three points still do not make a right turn, in which case we
again have to delete the middle one. We continue in this manner until the last
three points make a right turn, or until there are only two points left.
We now give the algorithm in pseudocode. The pseudocode computes both the
upper hull and the lower hull. The latter is done by treating the points from right
to left, analogous to the computation of the upper hull.
Algorithm CONVEXHULL(P)
Input. A set P of points in the plane.
Output. A list containing the vertices of CH(P) in clockwise order.
1.
Sort the points by x-coordinate, resulting in a sequence p1,..., pn.
2.
Put the points p1 and p2 in a list Lupper, with p1 as the ﬁrst point.
3.
for i ←3 to n
4.
do Append pi to Lupper.
5.
while Lupper contains more than two points and the last three points
in Lupper do not make a right turn
6.
do Delete the middle of the last three points from Lupper.
7.
Put the points pn and pn−1 in a list Llower, with pn as the ﬁrst point.
6

Section 1.1
AN EXAMPLE: CONVEX HULLS
8.
for i ←n−2 downto 1
9.
do Append pi to Llower.
10.
while Llower contains more than 2 points and the last three points
in Llower do not make a right turn
11.
do Delete the middle of the last three points from Llower.
12. Remove the ﬁrst and the last point from Llower to avoid duplication of the
points where the upper and lower hull meet.
13. Append Llower to Lupper, and call the resulting list L.
14. return L
Once again, when we look closer we realize that the above algorithm is not
correct. Without mentioning it, we made the assumption that no two points have
the same x-coordinate. If this assumption is not valid the order on x-coordinate
is not well deﬁned. Fortunately, this turns out not to be a serious problem.
We only have to generalize the ordering in a suitable way: rather than using
only the x-coordinate of the points to deﬁne the order, we use the lexicographic
not a right turn
order. This means that we ﬁrst sort by x-coordinate, and if points have the same
x-coordinate we sort them by y-coordinate.
Another special case we have ignored is that the three points for which we
have to determine whether they make a left or a right turn lie on a straight line.
In this case the middle point should not occur on the convex hull, so collinear
points must be treated as if they make a left turn. In other words, we should use
a test that returns true if the three points make a right turn, and false otherwise.
(Note that this is simpler than the test required in the previous algorithm when
there were collinear points.)
With these modiﬁcations the algorithm correctly computes the convex hull:
the ﬁrst scan computes the upper hull, which is now deﬁned as the part of the
convex hull running from the lexicographically smallest vertex to the lexico-
graphically largest vertex, and the second scan computes the remaining part of
the convex hull.
What does our algorithm do in the presence of rounding errors in the ﬂoating
point arithmetic? When such errors occur, it can happen that a point is removed
from the convex hull although it should be there, or that a point inside the real
convex hull is not removed. But the structural integrity of the algorithm is
unharmed: it will compute a closed polygonal chain. After all, the output is
a list of points that we can interpret as the clockwise listing of the vertices of
a polygon, and any three consecutive points form a right turn or, because of
the rounding errors, they almost form a right turn. Moreover, no point in P
can be far outside the computed hull. The only problem that can still occur is
that, when three points lie very close together, a turn that is actually a sharp
left turn can be interpretated as a right turn. This might result in a dent in the
resulting polygon. A way out of this is to make sure that points in the input
that are very close together are considered as being the same point, for example
by rounding. Hence, although the result need not be exactly correct—but then,
we cannot hope for an exact result if we use inexact arithmetic—it does make
sense. For many applications this is good enough. Still, it is wise to be careful
in the implementation of the basic test to avoid errors as much as possible.
7

Chapter 1
COMPUTATIONAL GEOMETRY
We conclude with the following theorem:
Theorem 1.1 The convex hull of a set of n points in the plane can be computed
in O(nlogn) time.
Proof. We will prove the correctness of the computation of the upper hull; the
lower hull computation can be proved correct using similar arguments. The
proof is by induction on the number of point treated. Before the for-loop starts,
the list Lupper contains the points p1 and p2, which trivially form the upper
hull of {p1, p2}. Now suppose that Lupper contains the upper hull vertices
of {p1,..., pi−1} and consider the addition of pi. After the execution of the
while-loop and because of the induction hypothesis, we know that the points in
Lupper form a chain that only makes right turns. Moreover, the chain starts at the
lexicographically smallest point of {p1,..., pi} and ends at the lexicographically
largest point, namely pi. If we can show that all points of {p1,..., pi} that are
not in Lupper are below the chain, then Lupper contains the correct points. By
pi
pi−1
empty region
induction we know there is no point above the chain that we had before pi was
added. Since the old chain lies below the new chain, the only possibility for a
point to lie above the new chain is if it lies in the vertical slab between pi−1 and
pi. But this is not possible, since such a point would be in between pi−1 and pi
in the lexicographical order. (You should verify that a similar argument holds if
pi−1 and pi, or any other points, have the same x-coordinate.)
To prove the time bound, we note that sorting the points lexicographically
can be done in O(nlogn) time. Now consider the computation of the upper hull.
The for-loop is executed a linear number of times. The question that remains
is how often the while-loop inside it is executed. For each execution of the
for-loop the while-loop is executed at least once. For any extra execution a
point is deleted from the current hull. As each point can be deleted only once
during the construction of the upper hull, the total number of extra executions
over all for-loops is bounded by n. Similarly, the computation of the lower hull
takes O(n) time. Due to the sorting step, the total time required for computing
the convex hull is O(nlogn).
The ﬁnal convex hull algorithm is simple to describe and easy to implement.
It only requires lexicographic sorting and a test whether three consecutive points
make a right turn. From the original deﬁnition of the problem it was far from
obvious that such an easy and efﬁcient solution would exist.
1.2
Degeneracies and Robustness
As we have seen in the previous section, the development of a geometric
algorithm often goes through three phases.
In the ﬁrst phase, we try to ignore everything that will clutter our understanding
of the geometric concepts we are dealing with. Sometimes collinear points are
a nuisance, sometimes vertical line segments are. When ﬁrst trying to design or
understand an algorithm, it is often helpful to ignore these degenerate cases.
8

Section 1.2
DEGENERACIES AND ROBUSTNESS
In the second phase, we have to adjust the algorithm designed in the ﬁrst phase
to be correct in the presence of degenerate cases. Beginners tend to do this
by adding a huge number of case distinctions to their algorithms. In many
situations there is a better way. By considering the geometry of the problem
again, one can often integrate special cases with the general case. For example,
in the convex hull algorithm we only had to use the lexicographical order instead
of the order on x-coordinate to deal with points with equal x-coordinate. For
most algorithms in this book we have tried to take this integrated approach to
deal with special cases. Still, it is easier not to think about such cases upon ﬁrst
reading. Only after understanding how the algorithm works in the general case
should you think about degeneracies.
If you study the computational geometry literature, you will ﬁnd that many
authors ignore special cases, often by formulating speciﬁc assumptions on the
input. For example, in the convex hull problem we could have ignored special
cases by simply stating that we assume that the input is such that no three
points are collinear and no two points have the same x-coordinate. From a
theoretical point of view, such assumptions are usually justiﬁed: the goal is
then to establish the computational complexity of a problem and, although it is
tedious to work out the details, degenerate cases can almost always be handled
without increasing the asymptotic complexity of the algorithm. But special cases
deﬁnitely increase the complexity of the implementations. Most researchers in
computational geometry today are aware that their general position assumptions
are not satisﬁed in practical applications and that an integrated treatment of the
special cases is normally the best way to handle them. Furthermore, there are
general techniques—so-called symbolic perturbation schemes—that allow one
to ignore special cases during the design and implementation, and still have an
algorithm that is correct in the presence of degeneracies.
The third phase is the actual implementation. Now one needs to think about
the primitive operations, like testing whether a point lies to the left, to the right,
or on a directed line. If you are lucky you have a geometric software library
available that contains the operations you need, otherwise you must implement
them yourself.
Another issue that arises in the implementation phase is that the assumption
of doing exact arithmetic with real numbers breaks down, and it is necessary
to understand the consequences. Robustness problems are often a cause of
frustration when implementing geometric algorithms. Solving robustness prob-
lems is not easy. One solution is to use a package providing exact arithmetic
(using integers, rationals, or even algebraic numbers, depending on the type
of problem) but this will be slow. Alternatively, one can adapt the algorithm
to detect inconsistencies and take appropriate actions to avoid crashing the
program. In this case it is not guaranteed that the algorithm produces the correct
output, and it is important to establish the exact properties that the output has.
This is what we did in the previous section, when we developed the convex
hull algorithm: the result might not be a convex polygon but we know that the
structure of the output is correct and that the output polygon is very close to the
convex hull. Finally, it is possible to predict, based on the input, the precision in
9

Chapter 1
COMPUTATIONAL GEOMETRY
the number representation required to solve the problem correctly.
Which approach is best depends on the application. If speed is not an issue,
exact arithmetic is preferred. In other cases it is not so important that the result
of the algorithm is precise. For example, when displaying the convex hull of a
set of points, it is most likely not noticeable when the polygon deviates slightly
from the true convex hull. In this case we can use a careful implementation
based on ﬂoating point arithmetic.
In the rest of this book we focus on the design phase of geometric algorithms;
we won't say much about the problems that arise in the implementation phase.
1.3
Application Domains
As indicated before, we have chosen a motivating example application for every
geometric concept, algorithm, or data structure introduced in this book. Most of
the applications stem from the areas of computer graphics, robotics, geographic
information systems, and CAD/CAM. For those not familiar with these ﬁelds,
we give a brief description of the areas and indicate some of the geometric
problems that arise in them.
Computer graphics.
Computer graphics is concerned with creating images
of modeled scenes for display on a computer screen, a printer, or other output
device. The scenes vary from simple two-dimensional drawings—consisting of
lines, polygons, and other primitive objects—to realistic-looking 3-dimensional
scenes including light sources, textures, and so on. The latter type of scene can
easily contain over a million polygons or curved surface patches.
Because scenes consist of geometric objects, geometric algorithms play an
important role in computer graphics.
For 2-dimensional graphics, typical questions involve the intersection of
certain primitives, determining the primitive pointed to with the mouse, or deter-
mining the subset of primitives that lie within a particular region. Chapters 6, 10,
and 16 describe techniques useful for some of these problems.
When dealing with 3-dimensional problems the geometric questions be-
come more complex. A crucial step in displaying a 3-dimensional scene is
hidden surface removal: determine the part of a scene visible from a particular
viewpoint or, in other words, discard the parts that lie behind other objects. In
Chapter 12 we study one approach to this problem.
To create realistic-looking scenes we have to take light into account. This
creates many new problems, such as the computation of shadows. Hence,
realistic image synthesis requires complicated display techniques, like ray
tracing and radiosity. When dealing with moving objects and in virtual reality
applications, it is important to detect collisions between objects. All these
situations involve geometric problems.
Robotics.
The ﬁeld of robotics studies the design and use of robots. As robots
are geometric objects that operate in a 3-dimensional space—the real world—it
10

Section 1.3
APPLICATION DOMAINS
is obvious that geometric problems arise at many places. At the beginning of
this chapter we already introduced the motion planning problem, where a robot
has to ﬁnd a path in an environment with obstacles. In Chapters 13 and 15 we
study some simple cases of motion planning. Motion planning is one aspect
of the more general problem of task planning. One would like to give a robot
high-level tasks—"vacuum the room"—and let the robot ﬁgure out the best way
to execute the task. This involves planning motions, planning the order in which
to perform subtasks, and so on.
Other geometric problems occur in the design of robots and work cells in
which the robot has to operate. Most industrial robots are robot arms with a
ﬁxed base. The parts operated on by the robot arm have to be supplied in such
a way that the robot can easily grasp them. Some of the parts may have to be
immobilized so that the robot can work on them. They may also have to be
turned to a known orientation before the robot can work on them. These are
all geometric problems, sometimes with a kinematic component. Some of the
algorithms described in this book are applicable in such problems. For example,
the smallest enclosing disc problem, treated in Section 4.7, can be used for
optimal placement of robot arms.
Geographic information systems.
A geographic information system, or GIS
for short, stores geographical data like the shape of countries, the height of
mountains, the course of rivers, the type of vegetation at different locations,
population density, or rainfall. They can also store human-made structures
such as cities, roads, railways, electricity lines, or gas pipes. A GIS can be
used to extract information about certain regions and, in particular, to obtain
information about the relation between different types of data. For example,
a biologist may want to relate the average rainfall to the existence of certain
plants, and a civil engineer may need to query a GIS to determine whether there
are any gas pipes underneath a lot where excavation works are to be performed.
As most geographic information concerns properties of points and regions
on the earth's surface, geometric problems occur in abundance here. Moreover,
the amount of data is so large that efﬁcient algorithms are a must. Below we
mention the GIS-related problems treated in this book.
A ﬁrst question is how to store geographic data. Suppose that we want to
develop a car guidance system, which shows the driver at any moment where she
is. This requires storing a huge map of roads and other data. At every moment
we have to be able to determine the position of the car on the map and to quickly
select a small portion of the map for display on the on-board computer. Efﬁcient
data structures are needed for these operations. Chapters 6, 10, and 16 describe
computational geometry solutions to these problems.
The information about the height in some mountainous terrain is usually
only available at certain sample points. For other positions we have to obtain
the heights by interpolating between nearby sample points. But which sample
points should we choose? Chapter 9 deals with this problem.
The combination of different types of data is one of the most important
operations in a GIS. For example, we may want to check which houses lie in
11

Chapter 1
COMPUTATIONAL GEOMETRY
a forest, locate all bridges by checking where roads cross rivers, or determine
a good location for a new golf course by ﬁnding a slightly hilly, rather cheap
area not too far from a particular town. A GIS usually stores different types of
data in separate maps. To combine the data we have to overlay different maps.
Chapter 2 deals with a problem arising when we want to compute the overlay.
Finally, we mention the same example we gave at the beginning of this
chapter: the location of the nearest public phone (or hospital, or any other
facility). This requires the computation of a Voronoi diagram, a structure
studied in detail in Chapter 7.
CAD/CAM.
Computer aided design (CAD) concerns itself with the design
of products with a computer. The products can vary from printed circuit boards,
machine parts, or furniture, to complete buildings. In all cases the resulting
product is a geometric entity and, hence, it is to be expected that all sorts of
geometric problems appear. Indeed, CAD packages have to deal with intersec-
tions and unions of objects, with decomposing objects and object boundaries
into simpler shapes, and with visualizing the designed products.
To decide whether a design meets the speciﬁcations certain tests are needed.
Often one does not need to build a prototype for these tests, and a simulation
sufﬁces. Chapter 14 deals with a problem arising in the simulation of heat
emission by a printed circuit board.
Once an object has been designed and tested, it has to be manufactured.
Computer aided manufacturing (CAM) packages can be of assistance here.
CAM involves many geometric problems. Chapter 4 studies one of them.
A recent trend is design for assembly, where assembly decisions are already
taken into account during the design stage. A CAD system supporting this
would allow designers to test their design for feasibility, answering questions
like: can the product be built easily using a certain manufacturing process?
Many of these questions require geometric algorithms to be answered.
Other applications domains.
There are many more application domains
where geometric problems occur and geometric algorithms and data structures
can be used to solve them.
For example, in molecular modeling, molecules are often represented by
collections of intersecting balls in space, one ball for each atom. Typical
questions are to compute the union of the atom balls to obtain the molecule
surface, or to compute where two molecules can touch each other.
caffeine
Another area is pattern recognition. Consider for example an optical char-
acter recognition system. Such a system scans a paper with text on it with the
goal of recognizing the text characters. A basic step is to match the image of a
character against a collection of stored characters to ﬁnd the one that best ﬁts it.
This leads to a geometric problem: given two geometric objects, determine how
well they resemble each other.
Even certain areas that at ﬁrst sight do not seem to be geometric can ben-
eﬁt from geometric algorithms, because it is often possible to formulate non-
geometric problem in geometric terms. In Chapter 5, for instance, we will see
12

Section 1.4
NOTES AND COMMENTS
how records in a database can be interpreted as points in a higher-dimensional
space, and we will present a geometric data structure such that certain queries
on the records can be answered efﬁciently.
We hope that the above collection of geometric problems makes it clear that
computational geometry plays a role in many different areas of computer sci-
ence. The algorithms, data structures, and techniques described in this book
will provide you with the tools needed to attack such geometric problems
successfully.
1.4
Notes and Comments
Every chapter of this book ends with a section entitled Notes and Comments.
These sections indicate where the results described in the chapter came from,
indicate generalizations and improvements, and provide references. They can
be skipped but do contain useful material for those who want to know more
about the topic of the chapter. More information can also be found in the
Handbook of Computational Geometry [331] and the Handbook of Discrete and
Computational Geometry [191].
In this chapter the geometric problem treated in detail was the computation
of the convex hull of a set of points in the plane. This is a classic topic in
computational geometry and the amount of literature about it is huge. The
algorithm described in this chapter is commonly known as Graham's scan, and
is based on a modiﬁcation by Andrew [17] of one of the earliest algorithms by
Graham [192]. This is only one of the many O(nlogn) algorithms available for
solving the problem. A divide-and-conquer approach was given by Preparata
and Hong [322]. Also an incremental method exists that inserts the points
one by one in O(logn) time per insertion [321]. Overmars and van Leeuwen
generalized this to a method in which points could be both inserted and deleted
in O(log2 n) time [305]. Other results on dynamic convex hulls were obtained
by Hershberger and Suri [211], Chan [83], and Brodal and Jacob [73].
Even though an Ω(nlogn) lower bound is known for the problem [393]
many authors have tried to improve the result. This makes sense because in many
applications the number of points that appear on the convex hull is relatively
small, while the lower bound result assumes that (almost) all points show up
on the convex hull. Hence, it is useful to look at algorithms whose running
time depends on the complexity of the convex hull. Jarvis [221] introduced
a wrapping technique, often referred to as Jarvis's march, that computes the
convex hull in O(h·n) time where h is the complexity of the convex hull. The
same worst-case performance is achieved by the algorithm of Overmars and
van Leeuwen [303], based on earlier work by Bykat [79], Eddy [156], and
Green and Silverman [193]. This algorithm has the advantage that its expected
running time is linear for many distributions of points. Finally, Kirkpatrick
and Seidel [238] improved the result to O(nlogh), and recently Chan [82]
discovered a much simpler algorithm to achieve the same result.
13

Chapter 1
COMPUTATIONAL GEOMETRY
The convex hull can be deﬁned in any dimension. Convex hulls in 3-
dimensional space can still be computed in O(nlogn) time, as we will see in
Chapter 11. For dimensions higher than 3, however, the complexity of the
convex hull is no longer linear in the number of points. See the notes and
comments of Chapter 11 for more details.
In the past years a number of general methods for handling special cases have
been suggested. These symbolic perturbation schemes perturb the input in such
a way that all degeneracies disappear. However, the perturbation is only done
symbolically. This technique was introduced by Edelsbrunner and M¨ucke [164]
and later reﬁned by Yap [397] and Emiris and Canny [172, 171]. Symbolic
perturbation relieves the programmer of the burden of degeneracies, but it
has some drawbacks: the use of a symbolic perturbation library slows down
the algorithm, and sometimes one needs to recover the "real result" from the
"perturbed result", which is not always easy. These drawbacks led Burnikel et
al. [78] to claim that it is both simpler (in terms of programming effort) and
more efﬁcient (in terms of running time) to deal directly with degenerate inputs.
Robustness in geometric algorithms is a topic that has recently received a lot of
interest. Most geometric comparisons can be formulated as computing the sign
of some determinant. A possible way to deal with the inexactness in ﬂoating
point arithmetic when evaluating this sign is to choose a small threshold value
ε and to say that the determinant is zero when the outcome of the ﬂoating
point computation is less than ε. When implemented naively, this can lead to
inconsistencies (for instance, for three points a,b,c we may decide that a = b
and b = c but a ̸= c) that cause the program to fail. Guibas et al. [198] showed
that combining such an approach with interval arithmetic and backwards error
analysis can give robust algorithms. Another option is to use exact arithmetic.
Here one computes as many bits of the determinant as are needed to determine its
sign. This will slow down the computation, but techniques have been developed
to keep the performance penalty relatively small [182, 256, 395]. Besides
these general approaches, there have been a number papers dealing with robust
computation in speciﬁc problems [34, 37, 81, 145, 180, 181, 219, 279].
We gave a brief overview of the application domains from which we took our
examples, which serve to show the motivation behind the various geometric
notions and algorithms studied in this book. Below are some references to
textbooks you can consult if you want to know more about the application
domains. Of course there are many more good books about these domains than
the few we mention.
There is a large number of books on computer graphics. The book by Foley
et al. [179] is very extensive and generally considered one of the best books on
the topic. Other good books are the ones by Shirley et al. [359] and Watt [381].
An extensive overview of robotics and the motion planning problem can be
found in the book of Choset et al. [127], and in the somewhat older books of
Latombe [243] and Hopcroft, Schwartz, and Sharir [217]. More information on
geometric aspects of robotics is provided by the book of Selig [348].
14

Section 1.5
EXERCISES
There is a large collection of books about geographic information systems,
but most of them do not consider algorithmic issues in much detail. Some
general textbooks are the ones by DeMers [140], Longley et al. [257], and
Worboys and Duckham [392]. Data structures for spatial data are described
extensively in the book of Samet [335].
The books by Faux and Pratt [175], Mortenson [285], and Hoffmann [216]
are good introductory texts on CAD/CAM and geometric modeling.
1.5
Exercises
1.1
The convex hull of a set S is deﬁned to be the intersection of all convex
sets that contain S. For the convex hull of a set of points it was indicated
that the convex hull is the convex set with smallest perimeter. We want to
show that these are equivalent deﬁnitions.
a. Prove that the intersection of two convex sets is again convex. This
implies that the intersection of a ﬁnite family of convex sets is convex
as well.
b. Prove that the smallest perimeter polygon P containing a set of points
P is convex.
c. Prove that any convex set containing the set of points P contains the
smallest perimeter polygon P.
1.2
Let P be a set of points in the plane. Let P be the convex polygon whose
vertices are points from P and that contains all points in P. Prove that this
polygon P is uniquely deﬁned, and that it is the intersection of all convex
sets containing P.
1.3
Let E be an unsorted set of n segments that are the edges of a convex
polygon. Describe an O(nlogn) algorithm that computes from E a list
containing all vertices of the polygon, sorted in clockwise order.
1.4
For the convex hull algorithm we have to be able to test whether a point
r lies left or right of the directed line through two points p and q. Let
p = (px, py), q = (qx,qy), and r = (rx,ry).
a. Show that the sign of the determinant
D =
!!!!!!
1
px
py
1
qx
qy
1
rx
ry
!!!!!!
determines whether r lies left or right of the line.
b. Show that |D| in fact is twice the surface of the triangle determined by
p, q, and r.
c. Why is this an attractive way to implement the basic test in algorithm
CONVEXHULL? Give an argument for both integer and ﬂoating point
coordinates.
15

Chapter 1
COMPUTATIONAL GEOMETRY
1.5
Verify that the algorithm CONVEXHULL with the indicated modiﬁcations
correctly computes the convex hull, also of degenerate sets of points.
Consider for example such nasty cases as a set of points that all lie on one
(vertical) line.
1.6
In many situations we need to compute convex hulls of objects other than
points.
a.
Let S be a set of n line segments in the plane. Prove that the convex
hull of S is exactly the same as the convex hull of the 2n endpoints of
the segments.
b.* Let P be a non-convex polygon. Describe an algorithm that computes
the convex hull of P in O(n) time. Hint: Use a variant of algorithm
CONVEXHULL where the vertices are not treated in lexicographical
order, but in some other order.
1.7
Consider the following alternative approach to computing the convex hull
of a set of points in the plane: We start with the rightmost point. This is
the ﬁrst point p1 of the convex hull. Now imagine that we start with a
vertical line and rotate it clockwise until it hits another point p2. This is
the second point on the convex hull. We continue rotating the line but this
time around p2 until we hit a point p3. In this way we continue until we
reach p1 again.
a. Give pseudocode for this algorithm.
b. What degenerate cases can occur and how can we deal with them?
c. Prove that the algorithm correctly computes the convex hull.
d. Prove that the algorithm can be implemented to run in time O(n·h),
where h is the complexity of the convex hull.
e. What problems might occur when we deal with inexact ﬂoating point
arithmetic?
1.8
The O(nlogn) algorithm to compute the convex hull of a set of n points
in the plane that was described in this chapter is based on the paradigm
of incremental construction: add the points one by one, and update the
convex hull after each addition. In this exercise we shall develop an
algorithm based on another paradigm, namely divide-and-conquer.
a. Let P1 and P2 be two disjoint convex polygons with n vertices in total.
Give an O(n) time algorithm that computes the convex hull of P1 ∪P2.
b. Use the algorithm from part a to develop an O(nlogn) time divide-and-
conquer algorithm to compute the convex hull of a set of n points in
the plane.
1.9
Suppose that we have a subroutine CONVEXHULL available for comput-
ing the convex hull of a set of points in the plane. Its output is a list of con-
vex hull vertices, sorted in clockwise order. Now let S = {x1,x2,...,xn}
be a set of n numbers. Show that S can be sorted in O(n) time plus the
time needed for one call to CONVEXHULL. Since the sorting problem
has an Ω(nlogn) lower bound, this implies that the convex hull problem
16

Section 1.5
EXERCISES
has an Ω(nlogn) lower bound as well. Hence, the algorithm presented in
this chapter is asymptotically optimal.
1.10 Let S be a set of n (possibly intersecting) unit circles in the plane. We
want to compute the convex hull of S.
a.
Show that the boundary of the convex hull of S consists of straight
line segments and pieces of circles in S.
b.
Show that each circle can occur at most once on the boundary of the
convex hull.
c.
Let S′ be the set of points that are the centers of the circles in S. Show
that a circle in S appears on the boundary of the convex hull if and
only if the center of the circle lies on the convex hull of S′.
d.
Give an O(nlogn) algorithm for computing the convex hull of S.
e.* Give an O(nlogn) algorithm for the case in which the circles in S
have different radii.
17

2
Line Segment Intersection
Thematic Map Overlay
When you are visiting a country, maps are an invaluable source of information.
They tell you where tourist attractions are located, they indicate the roads and
railway lines to get there, they show small lakes, and so on. Unfortunately,
they can also be a source of frustration, as it is often difﬁcult to ﬁnd the right
information: even when you know the approximate position of a small town,
Victoria
Vancouver
Saskatoon
Calgary
Edmonton
Regina
Ft. Simpson
Whitehorse
Yellowknife
Echo
Bay
Dawson
Lynn
Lake
Victoria
Vancouver
Saskatoon
Calgary
Edmonton
Regina
Ft. Simpson
Whitehorse
Yellowknife
Echo
Bay
Dawson
Lynn
Lake
Prince
Rupert
Dawson
Creek
Uranium
City
Uranium
City
Prince
Rupert
Dawson
Creek
Figure 2.1
Cities, rivers, railroads, and their
overlay in western Canada
it can still be difﬁcult to spot it on the map. To make maps more readable,
geographic information systems split them into several layers. Each layer is a
thematic map, that is, it stores only one type of information. Thus there will
be a layer storing the roads, a layer storing the cities, a layer storing the rivers,
19

Chapter 2
LINE SEGMENT INTERSECTION
and so on. The theme of a layer can also be more abstract. For instance, there
could be a layer for the population density, for average precipitation, habitat of
the grizzly bear, or for vegetation. The type of geometric information stored
grizzly bear
in a layer can be very different: the layer for a road map could store the roads
as collections of line segments (or curves, perhaps), the layer for cities could
contain points labeled with city names, and the layer for vegetation could store
a subdivision of the map into regions labeled with the type of vegetation.
Users of a geographic information system can select one of the thematic
maps for display. To ﬁnd a small town you would select the layer storing cities,
and you would not be distracted by information such as the names of rivers
and lakes. After you have spotted the town, you probably want to know how to
get there. To this end geographic information systems allow users to view an
overlay of several maps—see Figure 2.1. Using an overlay of the road map and
the map storing cities you can now ﬁgure out how to get to the town. When two
or more thematic map layers are shown together, intersections in the overlay
are positions of special interest. For example, when viewing the overlay of
the layer for the roads and the layer for the rivers, it would be useful if the
intersections were clearly marked. In this example the two maps are basically
networks, and the intersections are points. In other cases one is interested in
the intersection of complete regions. For instance, geographers studying the
climate could be interested in ﬁnding regions where there is pine forest and the
annual precipitation is between 1000 mm and 1500 mm. These regions are the
intersections of the regions labeled "pine forest" in the vegetation map and the
regions labeled "1000-1500" in the precipitation map.
2.1
Line Segment Intersection
We ﬁrst study the simplest form of the map overlay problem, where the two map
layers are networks represented as collections of line segments. For example, a
map layer storing roads, railroads, or rivers at a small scale. Note that curves can
be approximated by a number of small segments. At ﬁrst we won't be interested
in the regions induced by these line segments. Later we shall look at the more
complex situation where the maps are not just networks, but subdivisions of
the plane into regions that have an explicit meaning. To solve the network
overlay problem we ﬁrst have to state it in a geometric setting. For the overlay
of two networks the geometric situation is the following: given two sets of
line segments, compute all intersections between a segment from one set and a
segment from the other. This problem speciﬁcation is not quite precise enough
yet, as we didn't deﬁne when two segments intersect. In particular, do two
segments intersect when an endpoint of one of them lies on the other? In other
words, we have to specify whether the input segments are open or closed. To
make this decision we should go back to the application, the network overlay
problem. Roads in a road map and rivers in a river map are represented by
chains of segments, so a crossing of a road and a river corresponds to the interior
of one chain intersecting the interior of another chain. This does not mean that
20

Section 2.1
LINE SEGMENT INTERSECTION
there is an intersection between the interior of two segments: the intersection
point could happen to coincide with an endpoint of a segment of a chain. In
fact, this situation is not uncommon because windy rivers are represented by
many small segments and coordinates of endpoints may have been rounded
when maps are digitized. We conclude that we should deﬁne the segments to be
closed, so that an endpoint of one segment lying on another segment counts as
an intersection.
To simplify the description somewhat we shall put the segments from the two
sets into one set, and compute all intersections among the segments in that set.
This way we certainly ﬁnd all the intersections we want. We may also ﬁnd
intersections between segments from the same set. Actually, we certainly will,
because in our application the segments from one set form a number of chains,
and we count coinciding endpoints as intersections. These other intersections
can be ﬁltered out afterwards by simply checking for each reported intersection
whether the two segments involved belong to the same set. So our problem
speciﬁcation is as follows: given a set S of n closed segments in the plane, report
all intersection points among the segments in S.
This doesn't seem like a challenging problem: we can simply take each pair
of segments, compute whether they intersect, and, if so, report their intersection
point. This brute-force algorithm clearly requires O(n2) time. In a sense this is
optimal: when each pair of segments intersects any algorithm must take Ω(n2)
time, because it has to report all intersections. A similar example can be given
when the overlay of two networks is considered. In practical situations, however,
most segments intersect no or only a few other segments, so the total number of
intersection points is much smaller than quadratic. It would be nice to have an
algorithm that is faster in such situations. In other words, we want an algorithm
whose running time depends not only on the number of segments in the input,
but also on the number of intersection points. Such an algorithm is called an
output-sensitive algorithm: the running time of the algorithm is sensitive to the
size of the output. We could also call such an algorithm intersection-sensitive,
since the number of intersections is what determines the size of the output.
How can we avoid testing all pairs of segments for intersection? Here we
must make use of the geometry of the situation: segments that are close together
are candidates for intersection, unlike segments that are far apart. Below we
shall see how we can use this observation to obtain an output-sensitive algorithm
for the line segment intersection problem.
Let S := {s1,s2,...,sn} be the set of segments for which we want to compute
all intersections. We want to avoid testing pairs of segments that are far apart.
But how can we do this? Let's ﬁrst try to rule out an easy case. Deﬁne the
y
x
y-interval of a segment to be its orthogonal projection onto the y-axis. When the
y-intervals of a pair of segments do not overlap—we could say that they are far
apart in the y-direction—then they cannot intersect. Hence, we only need to test
pairs of segments whose y-intervals overlap, that is, pairs for which there exists
a horizontal line that intersects both segments. To ﬁnd these pairs we imagine
sweeping a line ℓdownwards over the plane, starting from a position above all
21

Chapter 2
LINE SEGMENT INTERSECTION
segments. While we sweep the imaginary line, we keep track of all segments
intersecting it—the details of this will be explained later—so that we can ﬁnd
the pairs we need.
This type of algorithm is called a plane sweep algorithm and the line ℓis called
ℓ
event point
the sweep line. The status of the sweep line is the set of segments intersecting it.
The status changes while the sweep line moves downwards, but not continuously.
Only at particular points is an update of the status required. We call these points
the event points of the plane sweep algorithm. In this algorithm the event points
are the endpoints of the segments.
The moments at which the sweep line reaches an event point are the only
moments when the algorithm actually does something: it updates the status of
the sweep line and performs some intersection tests. In particular, if the event
point is the upper endpoint of a segment, then a new segment starts intersecting
the sweep line and must be added to the status. This segment is tested for
intersection against the ones already intersecting the sweep line. If the event
point is a lower endpoint, a segment stops intersecting the sweep line and must
be deleted from the status. This way we only test pairs of segments for which
there is a horizontal line that intersects both segments. Unfortunately, this is
not enough: there are still situations where we test a quadratic number of pairs,
whereas there is only a small number of intersection points. A simple example
is a set of vertical segments that all intersect the x-axis. So the algorithm is not
output-sensitive. The problem is that two segments that intersect the sweep line
can still be far apart in the horizontal direction.
Let's order the segments from left to right as they intersect the sweep line,
to include the idea of being close in the horizontal direction. We shall only
test segments when they are adjacent in the horizontal ordering. This means
that we only test any new segment against two segments, namely, the ones
immediately left and right of the upper endpoint. Later, when the sweep line has
moved downwards to another position, a segment can become adjacent to other
segments against which it will be tested. Our new strategy should be reﬂected in
the status of our algorithm: the status now corresponds to the ordered sequence
of segments intersecting the sweep line. The new status not only changes at
endpoints of segments; it also changes at intersection points, where the order
of the intersected segments changes. When this happens we must test the two
s j
sk
sl
sm
ℓ
new neighbors
segments that change position against their new neighbors. This is a new type
of event point.
Before trying to turn these ideas into an efﬁcient algorithm, we should
convince ourselves that the approach is correct. We have reduced the number
of pairs to be tested, but do we still ﬁnd all intersections? In other words, if
two segments si and sj intersect, is there always a position of the sweep line ℓ
where si and sj are adjacent along ℓ? Let's ﬁrst ignore some nasty cases: assume
that no segment is horizontal, that any two segments intersect in at most one
point—they do not overlap—, and that no three segments meet in a common
point. Later we shall see that these cases are easy to handle, but for now it
is convenient to forget about them. The intersections where an endpoint of a
segment lies on another segment can easily be detected when the sweep line
22

Section 2.1
LINE SEGMENT INTERSECTION
reaches the endpoint. So the only question is whether intersections between the
interiors of segments are always detected.
Lemma 2.1 Let si and sj be two non-horizontal segments whose interiors
intersect in a single point p, and assume there is no third segment passing
through p. Then there is an event point above p where si and sj become
adjacent and are tested for intersection.
Proof. Let ℓbe a horizontal line slightly above p. If ℓis close enough to p then
p
ℓ
si
s j
si and sj must be adjacent along ℓ. (To be precise, we should take ℓsuch that
there is no event point on ℓ, nor in between ℓand the horizontal line through
p.) In other words, there is a position of the sweep line where si and sj are
adjacent. On the other hand, si and sj are not yet adjacent when the algorithm
starts, because the sweep line starts above all line segments and the status is
empty. Hence, there must be an event point q where si and s j become adjacent
and are tested for intersection.
So our approach is correct, at least when we forget about the nasty cases
mentioned earlier. Now we can proceed with the development of the plane
sweep algorithm. Let's brieﬂy recap the overall approach. We imagine moving
a horizontal sweep line ℓdownwards over the plane. The sweep line halts at
certain event points; in our case these are the endpoints of the segments, which
we know beforehand, and the intersection points, which are computed on the
ﬂy. While the sweep line moves we maintain the ordered sequence of segments
intersected by it. When the sweep line halts at an event point the sequence of
segments changes and, depending on the type of event point, we have to take
several actions to update the status and detect intersections.
When the event point is the upper endpoint of a segment, there is a new segment
intersecting the sweep line. This segment must be tested for intersection against
its two neighbors along the sweep line. Only intersection points below the
sweep line are important; the ones above the sweep line have been detected
already. For example, if segments si and sk are adjacent on the sweep line, and
a new upper endpoint of a segment sj appears in between, then we have to test
s j for intersection with si and sk. If we ﬁnd an intersection below the sweep
line, we have found a new event point. After the upper endpoint is handled we
ℓ
si
s j
sk
intersection
detected
continue to the next event point.
When the event point is an intersection, the two segments that intersect
change their order. Each of them gets (at most) one new neighbor against which
it is tested for intersection. Again, only intersections below the sweep line are
still interesting. Suppose that four segments sj, sk, sl, and sm appear in this
order on the sweep line when the intersection point of sk and sl is reached. Then
sk and sl switch position and we must test sl and sj for intersection below the
sweep line, and also sk and sm. The new intersections that we ﬁnd are, of course,
s j sk
sl
sm
ℓ
also event points for the algorithm. Note, however, that it is possible that these
events have already been detected earlier, namely if a pair becoming adjacent
has been adjacent before.
23

Chapter 2
LINE SEGMENT INTERSECTION
When the event point is the lower endpoint of a segment, its two neighbors
now become adjacent and must be tested for intersection. If they intersect below
the sweep line, then their intersection point is an event point. (Again, this event
could have been detected already.) Assume three segments sk, sl, and sm appear
in this order on the sweep line when the lower endpoint of sl is encountered.
Then sk and sm will become adjacent and we test them for intersection.
sk
sl
sm
ℓ
After we have swept the whole plane—more precisely, after we have treated
the last event point—we have computed all intersection points. This is guaran-
teed by the following invariant, which holds at any time during the plane sweep:
all intersection points above the sweep line have been computed correctly.
After this sketch of the algorithm, it's time to go into more detail. It's also
time to look at the degenerate cases that can arise, like three or more segments
meeting in a point. We should ﬁrst specify what we expect from the algorithm
in these cases. We could require the algorithm to simply report each intersection
point once, but it seems more useful if it reports for each intersection point a
list of segments that pass through it or have it as an endpoint. There is another
special case for which we should deﬁne the required output more carefully,
namely that of two partially overlapping segments, but for simplicity we shall
ignore this case in the rest of this section.
We start by describing the data structures the algorithm uses.
First of all we need a data structure—called the event queue—that stores the
events. We denote the event queue by Q. We need an operation that removes the
next event that will occur from Q, and returns it so that it can be treated. This
event is the highest event below the sweep line. If two event points have the same
y-coordinate, then the one with smaller x-coordinate will be returned. In other
words, event points on the same horizontal line are treated from left to right.
This implies that we should consider the left endpoint of a horizontal segment
to be its upper endpoint, and its right endpoint to be its lower endpoint. You
can also think about our convention as follows: instead of having a horizontal
sweep line, imagine it is sloping just a tiny bit upward. As a result the sweep
line reaches the left endpoint of a horizontal segment just before reaching the
right endpoint. The event queue must allow insertions, because new events will
ℓ
be computed on the ﬂy. Notice that two event points can coincide. For example,
the upper endpoints of two distinct segments may coincide. It is convenient to
treat this as one event point. Hence, an insertion must be able to check whether
an event is already present in Q.
We implement the event queue as follows. Deﬁne an order ≺on the event
points that represents the order in which they will be handled. Hence, if p and q
are two event points then we have p ≺q if and only if py > qy holds or py = qy
and px < qx holds. We store the event points in a balanced binary search tree,
ordered according to ≺. With each event point p in Q we will store the segments
starting at p, that is, the segments whose upper endpoint is p. This information
will be needed to handle the event. Both operations—fetching the next event
and inserting an event—take O(logm) time, where m is the number of events
24

Section 2.1
LINE SEGMENT INTERSECTION
in Q. (We do not use a heap to implement the event queue, because we have to
be able to test whether a given event is already present in Q.)
Second, we need to maintain the status of the algorithm. This is the ordered
sequence of segments intersecting the sweep line. The status structure, denoted
by T, is used to access the neighbors of a given segment s, so that they can be
tested for intersection with s. The status structure must be dynamic: as segments
start or stop to intersect the sweep line, they must be inserted into or deleted
from the structure. Because there is a well-deﬁned order on the segments in
the status structure we can use a balanced binary search tree as status structure.
When you are only used to binary search trees that store numbers, this may be
surprising. But binary search trees can store any set of elements, as long as
there is an order on the elements.
ℓ
si
sk
sl
sm
si
s j
sk
sl
sm
sk
si
sl
s j
sj
T
In more detail, we store the segments intersecting the sweep line ordered
in the leaves of a balanced binary search tree T. The left-to-right order of
the segments along the sweep line corresponds to the left-to-right order of the
leaves in T. We must also store information in the internal nodes to guide the
search down the tree to the leaves. At each internal node, we store the segment
from the rightmost leaf in its left subtree. (Alternatively, we could store the
segments only in interior nodes. This will save some storage. However, it is
conceptually simpler to think about the segments in interior nodes as values
to guide the search, not as data items. Storing the segments in the leaves also
makes some algorithms simpler to describe.) Suppose we search in T for the
segment immediately to the left of some point p that lies on the sweep line. At
each internal node ν we test whether p lies left or right of the segment stored
at ν. Depending on the outcome we descend to the left or right subtree of ν,
eventually ending up in a leaf. Either this leaf, or the leaf immediately to the left
of it, stores the segment we are searching for. In a similar way we can ﬁnd the
segment immediately to the right of p, or the segments containing p. It follows
that each update and neighbor search operation takes O(logn) time.
The event queue Q and the status structure T are the only two data structures
we need. The global algorithm can now be described as follows.
Algorithm FINDINTERSECTIONS(S)
Input. A set S of line segments in the plane.
Output. The set of intersection points among the segments in S, with for each
intersection point the segments that contain it.
1.
Initialize an empty event queue Q. Next, insert the segment endpoints into
Q; when an upper endpoint is inserted, the corresponding segment should
be stored with it.
2.
Initialize an empty status structure T.
3.
while Q is not empty
4.
do Determine the next event point p in Q and delete it.
5.
HANDLEEVENTPOINT(p)
We have already seen how events are handled: at endpoints of segments we
have to insert or delete segments from the status structure T, and at intersection
points we have to change the order of two segments. In both cases we also
have to do intersection tests between segments that become neighbors after the
25

Chapter 2
LINE SEGMENT INTERSECTION
event. In degenerate cases—where several segments are involved in one event
point—the details are a little bit more tricky. The next procedure describes how
to handle event points correctly; it is illustrated in Figure 2.2.
Figure 2.2
An event point and the changes in the
status structure
T
s3
T
s8
s3
s1
s7
s8
s3
s1
s4
s5
s1
s2
s3
ℓ
s7
s5
s4
s1
s8
s7
s7
s7
s3
s5
s4
s1
s2
s3
s1
s2
HANDLEEVENTPOINT(p)
1.
Let U(p) be the set of segments whose upper endpoint is p; these segments
are stored with the event point p. (For horizontal segments, the upper
endpoint is by deﬁnition the left endpoint.)
2.
Find all segments stored in T that contain p; they are adjacent in T. Let
L(p) denote the subset of segments found whose lower endpoint is p, and
let C(p) denote the subset of segments found that contain p in their interior.
3.
if L(p)∪U(p)∪C(p) contains more than one segment
4.
then Report p as an intersection, together with L(p), U(p), and C(p).
5.
Delete the segments in L(p)∪C(p) from T.
6.
Insert the segments in U(p)∪C(p) into T. The order of the segments in T
should correspond to the order in which they are intersected by a sweep
line just below p. If there is a horizontal segment, it comes last among all
segments containing p.
7.
(∗Deleting and re-inserting the segments of C(p) reverses their order. ∗)
8.
if U(p)∪C(p) = /0
9.
then Let sl and sr be the left and right neighbors of p in T.
10.
FINDNEWEVENT(sl,sr, p)
11.
else Let s′ be the leftmost segment of U(p)∪C(p) in T.
12.
Let sl be the left neighbor of s′ in T.
13.
FINDNEWEVENT(sl,s′, p)
14.
Let s′′ be the rightmost segment of U(p)∪C(p) in T.
15.
Let sr be the right neighbor of s′′ in T.
16.
FINDNEWEVENT(s′′,sr, p)
Note that in lines 8-16 we assume that sl and sr actually exist. If they do not
exist the corresponding steps should obviously not be performed.
26

Section 2.1
LINE SEGMENT INTERSECTION
The procedures for ﬁnding the new intersections are easy: they simply test
two segments for intersection. The only thing we need to be careful about is,
when we ﬁnd an intersection, whether this intersection has already been handled
earlier or not. When there are no horizontal segments, then the intersection
has not been handled yet when the intersection point lies below the sweep line.
But how should we deal with horizontal segments? Recall our convention that
events with the same y-coordinate are treated from left to right. This implies
that we are still interested in intersection points lying to the right of the current
event point. Hence, the procedure FINDNEWEVENT is deﬁned as follows.
FINDNEWEVENT(sl,sr, p)
1.
if sl and sr intersect below the sweep line, or on it and to the right of the
current event point p, and the intersection is not yet present as an
event in Q
2.
then Insert the intersection point as an event into Q.
What about the correctness of our algorithm? It is clear that FINDINTERSEC-
TIONS only reports true intersection points, but does it ﬁnd all of them? The
next lemma states that this is indeed the case.
Lemma 2.2 Algorithm FINDINTERSECTIONS computes all intersection points
and the segments that contain it correctly.
Proof. Recall that the priority of an event is given by its y-coordinate, and that
when two events have the same y-coordinate the one with smaller x-coordinate
is given higher priority. We shall prove the lemma by induction on the priority
of the event points.
Let p be an intersection point and assume that all intersection points q with
a higher priority have been computed correctly. We shall prove that p and
the segments that contain p are computed correctly. Let U(p) be the set of
segments that have p as their upper endpoint (or, for horizontal segments, their
left endpoint), let L(p) be the set of segments having p as their lower endpoint
(or, for horizontal segments, their right endpoint), and let C(p) be the set of
segments having p in their interior.
First, assume that p is an endpoint of one or more of the segments. In that
case p is stored in the event queue Q at the start of the algorithm. The segments
from U(p) are stored with p, so they will be found. The segments from L(p)
and C(p) are stored in T when p is handled, so they will be found in line 2 of
HANDLEEVENTPOINT. Hence, p and all the segments involved are determined
correctly when p is an endpoint of one or more of the segments.
Now assume that p is not an endpoint of a segment. All we need to show is
that p will be inserted into Q at some moment. Note that all segments that are
involved have p in their interior. Order these segments by angle around p, and
let si and sj be two neighboring segments. Following the proof of Lemma 2.1
we see that there is an event point with a higher priority than p such that si and
sj become adjacent when q is passed. In Lemma 2.1 we assumed for simplicity
that si and sj are non-horizontal, but it is straightforward to adapt the proof for
27

Chapter 2
LINE SEGMENT INTERSECTION
horizontal segments. By induction, the event point q was handled correctly,
which means that p is detected and stored into Q.
So we have a correct algorithm. But did we succeed in developing an output-
sensitive algorithm? The answer is yes: the running time of the algorithm is
O((n+k)logn), where k is the size of the output. The following lemma states
an even stronger result: the running time is O((n + I)logn), where I is the
number of intersections. This is stronger, because for one intersection point the
output can consist of a large number of segments, namely in the case where
many segments intersect in a common point.
Lemma 2.3 The running time of Algorithm FINDINTERSECTIONS for a set S
of n line segments in the plane is O(nlogn+I logn), where I is the number of
intersection points of segments in S.
Proof. The algorithm starts by constructing the event queue on the segment
endpoints. Because we implemented the event queue as a balanced binary
search tree, this takes O(nlogn) time. Initializing the status structure takes
constant time. Then the plane sweep starts and all the events are handled. To
handle an event we perform three operations on the event queue Q: the event
itself is deleted from Q in line 4 of FINDINTERSECTIONS, and there can be one
or two calls to FINDNEWEVENT, which may cause at most two new events to
be inserted into Q. Deletions and insertions on Q take O(logn) time each. We
also perform operations—insertions, deletions, and neighbor ﬁnding—on the
status structure T, which take O(logn) time each. The number of operations
is linear in the number m(p) := card(L(p)∪U(p)∪C(p)) of segments that are
involved in the event. If we denote the sum of all m(p), over all event points p,
by m, the running time of the algorithm is O(mlogn).
It is clear that m = O(n + k), where k is the size of the output; after all,
whenever m(p) > 1 we report all segments involved in the event, and the only
events involving one segment are the endpoints of segments. But we want
to prove that m = O(n + I), where I is the number of intersection points. To
show this, we will interpret the set of segments as a planar graph embedded in
the plane. (If you are not familiar with planar graph terminology, you should
read the ﬁrst paragraphs of Section 2.2 ﬁrst.) Its vertices are the endpoints of
segments and intersection points of segments, and its edges are the pieces of
the segments connecting vertices. Consider an event point p. It is a vertex of
the graph, and m(p) is bounded by the degree of the vertex. Consequently, m is
bounded by the sum of the degrees of all vertices of our graph. Every edge of
the graph contributes one to the degree of exactly two vertices (its endpoints),
so m is bounded by 2ne, where ne is the number of edges of the graph. Let's
bound ne in terms of n and I. By deﬁnition, nv, the number of vertices, is at
most 2n+I. It is well known that in planar graphs ne = O(nv), which proves our
claim. But, for completeness, let us give the argument here. Every face of the
planar graph is bounded by at least three edges—provided that there are at least
three segments—and an edge can bound at most two different faces. Therefore
nf , the number of faces, is at most 2ne/3. We now use Euler's formula, which
states that for any planar graph with nv vertices, ne edges, and nf faces, the
28

Section 2.2
THE DOUBLY-CONNECTED EDGE LIST
following relation holds:
nv −ne +nf ⩾2.
Equality holds if and only if the graph is connected. Plugging the bounds on nv
and n f into this formula, we get
2 ⩽(2n+I)−ne + 2ne
3 = (2n+I)−ne/3.
So ne ⩽6n+3I −6, and m ⩽12n+6I −12, and the bound on the running time
follows.
We still have to analyze the other complexity aspect, the amount of storage
used by the algorithm. The tree T stores a segment at most once, so it uses O(n)
storage. The size of Q can be larger, however. The algorithm inserts intersection
points in Q when they are detected and it removes them when they are handled.
When it takes a long time before intersections are handled, it could happen that
Q gets very large. Of course its size is always bounded by O(n+I), but it would
be better if the working storage were always linear.
s1
s2
s3
s4
s5
ℓ
There is a relatively simple way to achieve this: only store intersection
points of pairs of segments that are currently adjacent on the sweep line. The
algorithm given above also stores intersection points of segments that have
been horizontally adjacent, but aren't anymore. By storing only intersections
among adjacent segments, the number of event points in Q is never more than
linear. The modiﬁcation required in the algorithm is that the intersection point
of two segments must be deleted when they stop being adjacent. These segments
must become adjacent again before the intersection point is reached, so the
intersection point will still be reported correctly. The total time taken by the
algorithm remains O(nlogn+I logn). We obtain the following theorem:
Theorem 2.4 Let S be a set of n line segments in the plane. All intersection
points in S, with for each intersection point the segments involved in it, can be
reported in O(nlogn+I logn) time and O(n) space, where I is the number of
intersection points.
2.2
The Doubly-Connected Edge List
We have solved the easiest case of the map overlay problem, where the two
maps are networks represented as collections of line segments. In general,
maps have a more complicated structure: they are subdivisions of the plane into
labeled regions. A thematic map of forests in Canada, for instance, would be
a subdivision of Canada into regions with labels such as "pine", "deciduous",
"birch", and "mixed".
Before we can give an algorithm for computing the overlay of two subdivi-
sions, we must develop a suitable representation for a subdivision. Storing a
subdivision as a collection of line segments is not such a good idea. Operations
like reporting the boundary of a region would be rather complicated. It is better
29

Chapter 2
LINE SEGMENT INTERSECTION
Figure 2.3
Types of forest in Canada
to incorporate structural, topological information: which segments bound a
given region, which regions are adjacent, and so on.
The maps we consider are planar subdivisions induced by planar embeddings
of graphs. Such a subdivision is connected if the underlying graph is connected.
The embedding of a node of the graph is called a vertex, and the embedding of
an arc is called an edge. We only consider embeddings where every edge is a
straight line segment. In principle, edges in a subdivision need not be straight.
A subdivision need not even be a planar embedding of a graph, as it may have
unbounded edges. In this section, however, we don't consider such more general
subdivisions. We consider an edge to be open, that is, its endpoints—which are
vertices of the subdivision—are not part of it. A face of the subdivision is a
face
vertex
edge
disconnected
subdivision
maximal connected subset of the plane that doesn't contain a point on an edge
or a vertex. Thus a face is an open polygonal region whose boundary is formed
by edges and vertices from the subdivision. The complexity of a subdivision
is deﬁned as the sum of the number of vertices, the number of edges, and the
number of faces it consists of. If a vertex is the endpoint of an edge, then we
say that the vertex and the edge are incident. Similarly, a face and an edge on
its boundary are incident, and a face and a vertex of its boundary are incident.
What should we require from a representation of a subdivision? An opera-
tion one could ask for is to determine the face containing a given point. This
is deﬁnitely useful in some applications—indeed, in a later chapter we shall
design a data structure for this—but it is a bit too much to ask from a basic
representation. The things we can ask for should be more local. For example, it
is reasonable to require that we can walk around the boundary of a given face,
or that we can access one face from an adjacent one if we are given a common
edge. Another operation that could be useful is to visit all the edges around a
given vertex. The representation that we shall discuss supports these operations.
It is called the doubly-connected edge list.
A doubly-connected edge list contains a record for each face, edge, and vertex
30

Section 2.2
THE DOUBLY-CONNECTED EDGE LIST
of the subdivision. Besides the geometric and topological information—to
be described shortly—each record may also store additional information. For
instance, if the subdivision represents a thematic map for vegetation, the doubly-
connected edge list would store in each face record the type of vegetation of
the corresponding region. The additional information is also called attribute
information. The geometric and topological information stored in the doubly-
connected edge list should enable us to perform the basic operations mentioned
earlier. To be able to walk around a face in counterclockwise order we store a
pointer from each edge to the next. It can also come in handy to walk around
a face the other way, so we also store a pointer to the previous edge. An edge
usually bounds two faces, so we need two pairs of pointers for it. It is convenient
to view the different sides of an edge as two distinct half-edges, so that we have
a unique next half-edge and previous half-edge for every half-edge. This also
means that a half-edge bounds only one face. The two half-edges we get for a
given edge are called twins. Deﬁning the next half-edge of a given half-edge
with respect to a counterclockwise traversal of a face induces an orientation on
each half-edge: it is oriented such that the face that it bounds lies to its left for
an observer walking along the edge. Because half-edges are oriented we can
speak of the origin and the destination of a half-edge. If a half-edge⃗e has v as its
origin and w as its destination, then its twin Twin(⃗e) has w as its origin and v as
its destination. To reach the boundary of a face we just need to store one pointer
in the face record to an arbitrary half-edge bounding the face. Starting from
⃗e
v
w
Twin(⃗e)
that half-edge, we can step from each half-edge to the next and walk around the
face.
What we just said does not quite hold for the boundaries of holes in a face:
if they are traversed in counterclockwise order then the face lies to the right. It
will be convenient to orient half-edges such that their face always lies to the
same side, so we change the direction of traversal for the boundary of a hole to
clockwise. Now a face always lies to the left of any half-edge on its boundary.
Another consequence is that twin half-edges always have opposite orientations.
The presence of holes in a face also means that one pointer from the face to an
arbitrary half-edge on its boundary is not enough to visit the whole boundary:
we need a pointer to a half-edge in every boundary component. If a face has
isolated vertices that don't have any incident edge, we can store pointers to them
as well. For simplicity we'll ignore this case.
Let's summarize. The doubly-connected edge list consists of three collections
of records: one for the vertices, one for the faces, and one for the half-edges.
These records store the following geometric and topological information:
The vertex record of a vertex v stores the coordinates of v in a ﬁeld called
Coordinates(v). It also stores a pointer IncidentEdge(v) to an arbitrary
half-edge that has v as its origin.
The face record of a face f stores a pointer OuterComponent(f) to some
half-edge on its outer boundary. For the unbounded face this pointer is nil.
It also stores a list InnerComponents(f), which contains for each hole in
the face a pointer to some half-edge on the boundary of the hole.
31

Chapter 2
LINE SEGMENT INTERSECTION
The half-edge record of a half-edge⃗e stores a pointer Origin(⃗e) to its origin,
a pointer Twin(⃗e) to its twin half-edge, and a pointer IncidentFace(⃗e) to
the face that it bounds. We don't need to store the destination of an edge,
because it is equal to Origin(Twin(⃗e)). The origin is chosen such that
⃗e
Prev(⃗e)
Next(⃗e)
IncidentFace(⃗e)
Origin(⃗e)
Twin(⃗e)
IncidentFace(⃗e) lies to the left of ⃗e when it is traversed from origin to
destination. The half-edge record also stores pointers Next(⃗e) and Prev(⃗e)
to the next and previous edge on the boundary of IncidentFace(⃗e). Thus
Next(⃗e) is the unique half-edge on the boundary of IncidentFace(⃗e) that has
the destination of⃗e as its origin, and Prev(⃗e) is the unique half-edge on the
boundary of IncidentFace(⃗e) that has Origin(⃗e) as its destination.
A constant amount of information is used for each vertex and edge. A face may
require more storage, since the list InnerComponents(f) has as many elements
as there are holes in the face. Because any half-edge is pointed to at most once
from all InnerComponents(f) lists together, we conclude that the amount of
storage is linear in the complexity of the subdivision. An example of a doubly-
connected edge list for a simple subdivision is given below. The two half-edges
corresponding to an edge ei are labeled⃗ei,1 and⃗ei,2.
Vertex
Coordinates
IncidentEdge
v1
(0,4)
⃗e1,1
v2
(2,4)
⃗e4,2
v3
(2,2)
⃗e2,1
v4
(1,1)
⃗e2,2
⃗e1,1
⃗e1,2
⃗e2,1
⃗e2,2
⃗e3,1
⃗e3,2
⃗e4,2
⃗e4,1
v1
v2
v3
v4
f1
f2
Face
OuterComponent
InnerComponents
f1
nil
⃗e1,1
f2
⃗e4,1
nil
Half-edge
Origin
Twin
IncidentFace
Next
Prev
⃗e1,1
v1
⃗e1,2
f1
⃗e4,2
⃗e3,1
⃗e1,2
v2
⃗e1,1
f2
⃗e3,2
⃗e4,1
⃗e2,1
v3
⃗e2,2
f1
⃗e2,2
⃗e4,2
⃗e2,2
v4
⃗e2,1
f1
⃗e3,1
⃗e2,1
⃗e3,1
v3
⃗e3,2
f1
⃗e1,1
⃗e2,2
⃗e3,2
v1
⃗e3,1
f2
⃗e4,1
⃗e1,2
⃗e4,1
v3
⃗e4,2
f2
⃗e1,2
⃗e3,2
⃗e4,2
v2
⃗e4,1
f1
⃗e2,1
⃗e1,1
The information stored in the doubly-connected edge list is enough to perform
the basic operations. For example, we can walk around the outer boundary
of a given face f by following Next(⃗e) pointers, starting from the half-edge
OuterComponent(f). We can also visit all edges incident to a vertex v. It is a
good exercise to ﬁgure out for yourself how to do this.
We described a fairly general version of the doubly-connected edge list. In
applications where the vertices carry no attribute information we could store
32

Section 2.3
COMPUTING THE OVERLAY OF TWO
SUBDIVISIONS
their coordinates directly in the Origin() ﬁeld of the edge; there is no strict need
for a separate type of vertex record. Even more important is to realize that in
many applications the faces of the subdivision carry no interesting meaning
(think of the network of rivers or roads that we looked at before). If that is the
case, we can completely forget about the face records, and the IncidentFace()
ﬁeld of half-edges. As we will see, the algorithm of the next section doesn't
need these ﬁelds (and is actually simpler to implement if we don't need to
update them). Some implementations of doubly-connected edge lists may also
insist that the graph formed by the vertices and edges of the subdivision be
connected. This can always be achieved by introducing dummy edges, and
has two advantages. Firstly, a simple graph transversal can be used to visit all
half-edges, and secondly, the InnerComponents() list for faces is not necessary.
2.3
Computing the Overlay of Two Subdivisions
Now that we have designed a good representation of a subdivision, we can tackle
the general map overlay problem. We deﬁne the overlay of two subdivisions S1
and S2 to be the subdivision O(S1,S2) such that there is a face f in O(S1,S2)
if and only if there are faces f1 in S1 and f2 in S2 such that f is a maximal
connected subset of f1 ∩f2. This sounds more complicated than it is: what it
means is that the overlay is the subdivision of the plane induced by the edges
from S1 and S2. Figure 2.4 illustrates this. The general map overlay problem
=⇒
Figure 2.4
Overlaying two subdivisions
is to compute a doubly-connected edge list for O(S1,S2), given the doubly-
connected edge lists of S1 and S2. We require that each face in O(S1,S2) be
labeled with the labels of the faces in S1 and S2 that contain it. This way we
have access to the attribute information stored for these faces. In an overlay of a
vegetation map and a precipitation map this would mean that we know for each
region in the overlay the type of vegetation and the amount of precipitation.
Let's ﬁrst see how much information from the doubly-connected edge lists
for S1 and S2 we can re-use in the doubly-connected edge list for O(S1,S2).
Consider the network of edges and vertices of S1. This network is cut into pieces
by the edges of S2. These pieces are for a large part re-usable; only the edges
that have been cut by the edges of S2 should be renewed. But does this also
33

Chapter 2
LINE SEGMENT INTERSECTION
hold for the half-edge records in the doubly-connected edge list that correspond
to the pieces? If the orientation of a half-edge would change, we would still
have to change the information in these records. Fortunately, this is not the case.
The half-edges are oriented such that the face that they bound lies to the left;
the shape of the face may change in the overlay, but it will remain to the same
side of the half-edge. Hence, we can re-use half-edge records corresponding to
edges that are not intersected by edges from the other map. Stated differently,
the only half-edge records in the doubly-connected edge list for O(S1,S2) that
we cannot borrow from S1 or S2 are the ones that are incident to an intersection
between edges from different maps.
This suggests the following approach. First, copy the doubly-connected
edge lists of S1 and S2 into one new doubly-connected edge list. The new
doubly-connected edge list is not a valid doubly-connected edge list, of course,
in the sense that it does not yet represent a planar subdivision. This is the task
of the overlay algorithm: it must transform the doubly-connected edge list into
a valid doubly-connected edge list for O(S1,S2) by computing the intersections
between the two networks of edges, and linking together the appropriate parts
of the two doubly-connected edge lists.
We did not talk about the new face records yet. The information for these
records is more difﬁcult to compute, so we leave this for later. We ﬁrst describe
in a little more detail how the vertex and half-edge records of the doubly-
connected edge list for O(S1,S2) are computed.
Our algorithm is based on the plane sweep algorithm of Section 2.1 for com-
puting the intersections in a set of line segments. We run this algorithm on the
set of segments that is the union of the sets of edges of the two subdivisions
S1 and S2. Here we consider the edges to be closed. Recall that the algorithm
is supported by two data structures: an event queue Q, which stores the event
points, and the status structure T, which is a balanced binary search tree storing
the segments intersecting the sweep line, ordered from left to right. We now
also maintain a doubly-connected edge list D. Initially, D contains a copy
of the doubly-connected edge list for S1 and a copy of the doubly-connected
edge list for S2. During the plane sweep we shall transform D to a correct
doubly-connected edge list for O(S1,S2). That is to say, as far as the vertex
and half-edge records are concerned; the face information will be computed
later. We keep cross pointers between the edges in the status structure T and
the half-edge records in D that correspond to them. This way we can access the
part of D that needs to be changed when we encounter an intersection point.
The invariant that we maintain is that at any time during the sweep, the part of
the overlay above the sweep line has been computed correctly.
Now, let's consider what we must do when we reach an event point. First
of all, we update T and Q as in the line segment intersection algorithm. If the
event involves only edges from one of the two subdivisions, this is all; the event
point is a vertex that can be re-used. If the event involves edges from both
subdivisions, we must make local changes to D to link the doubly-connected
edge lists of the two original subdivisions at the intersection point. This is
tedious but not difﬁcult.
34

Section 2.3
COMPUTING THE OVERLAY OF TWO
SUBDIVISIONS
the geometric situation and the
two doubly-connected edge lists
before handling the intersection
e
v
the doubly-connected edge list
after handling the intersection
Figure 2.5
An edge of one subdivision passing
through a vertex of the other
We describe the details for one of the possible cases, namely when an edge
e of S1 passes through a vertex v of S2, see Figure 2.5. The edge e must be
replaced by two edges denoted e′ and e′′. In the doubly-connected edge list, the
two half-edges for e must become four. We create two new half-edge records,
both with v as the origin. The two existing half-edges for e keep the endpoints of
e as their origin, as shown in Figure 2.5. Then we pair up the existing half-edges
with the new half-edges by setting their Twin() pointers. So e′ is represented
by one new and one existing half-edge, and the same holds for e′′. Now we
must set a number of Prev() and Next() pointers. We ﬁrst deal with the situation
around the endpoints of e; later we'll worry about the situation around v. The
e′
e′′
Next() pointers of the two new half-edges each copy the Next() pointer of the
old half-edge that is not its twin. The half-edges to which these pointers point
must also update their Prev() pointer and set it to the new half-edges. The
correctness of this step can be veriﬁed best by looking at a ﬁgure.
It remains to correct the situation around vertex v. We must set the Next()
and Prev() pointers of the four half-edges representing e′ and e′′, and of the four
half-edges incident from S2 to v. We locate these four half-edges from S2 by
testing where e′ and e′′ should be in the cyclic order of the edges around vertex
v. There are four pairs of half-edges that become linked by a Next() pointer
from the one and a Prev() pointer from the other. Consider the half-edge for
e′ that has v as its destination. It must be linked to the ﬁrst half-edge, seen
clockwise from e′, with v as its origin. The half-edge for e′ with v as its origin
e′
e′′
ﬁrst clockwise half-edge
from e′ with v as its origin
must be linked to the ﬁrst counterclockwise half-edge with v as its destination.
The same statements hold for e′′.
Most of the steps in the description above take only constant time. Only
locating where e′ and e′′ appear in the cyclic order around v may take longer:
it will take time linear in the degree of v. The other cases that can arise—
crossings of two edges from different maps, and coinciding vertices—are not
more difﬁcult than the case we just discussed. These cases also take time O(m),
where m is the number of edges incident to the event point. This means that
updating D does not increase the running time of the line segment intersection
algorithm asymptotically. Notice that every intersection that we ﬁnd is a vertex
of the overlay. It follows that the vertex records and the half-edge records of the
doubly-connected edge list for O(S1,S2) can be computed in O(nlogn+klogn)
time, where n denotes the sum of the complexities of S1 and S2, and k is the
complexity of the overlay.
35

Chapter 2
LINE SEGMENT INTERSECTION
After the ﬁelds involving vertex and half-edge records have been set, it remains
to compute the information about the faces of O(S1,S2). More precisely, we
have to create a face record for each face f in O(S1,S2), we have to make
OuterComponent(f) point to a half-edge on the outer boundary of f, and we
have to make a list InnerComponents(f) of pointers to half-edges on the bound-
aries of the holes inside f. Furthermore, we must set the IncidentFace() ﬁelds
of the half-edges on the boundary of f so that they point to the face record of f.
Finally, each of the new faces must be labeled with the names of the faces in
the old subdivisions that contain it.
How many face records will there be? Well, except for the unbounded face,
every face has a unique outer boundary, so the number of face records we have
to create is equal to the number of outer boundaries plus one. From the part of
the doubly-connected edge list we have constructed so far we can easily extract
all boundary cycles. But how do we know whether a cycle is an outer boundary
or the boundary of a hole in a face? This can be decided by looking at the
leftmost vertex v of the cycle, or, in case of ties, at the lowest of the leftmost
ones. Recall that half-edges are directed in such a way that their incident face
locally lies to the left. Consider the two half-edges of the cycle that are incident
to v. Because we know that the incident face lies to the left, we can compute
the angle these two half-edges make inside the incident face. If this angle is
f
smaller than 180◦then the cycle is an outer boundary, and otherwise it is the
boundary of a hole. This property holds for the leftmost vertex of a cycle, but
not necessarily for other vertices of that cycle.
To decide which boundary cycles bound the same face we construct a
graph G. For every boundary cycle—inner and outer—there is a node in G.
There is also one node for the imaginary outer boundary of the unbounded
face. There is an arc between two cycles if and only if one of the cycles is the
boundary of a hole and the other cycle has a half-edge immediately to the left
of the leftmost vertex of that hole cycle. If there is no half-edge to the left of the
leftmost vertex of a cycle, then the node representing the cycle is linked to the
node of the unbounded face. Figure 2.6 gives an example. The dotted segments
in the ﬁgure indicate the linking of the hole cycles to other cycles. The graph
corresponding to the subdivision is also shown in the ﬁgure. The hole cycles
are shown as single circles, and the outer boundary cycles are shown as double
circles. Observe that C3 and C6 are in the same connected component as C2.
This indicates that C3 and C6 are hole cycles in the face whose outer boundary
is C2. If there is only one hole in a face f, then the graph G links the boundary
cycle of the hole to the outer boundary of f. In general this need not be the case:
a hole can also be linked to another hole, as you can see in Figure 2.6. This
hole, which lies in the same face f, may be linked to the outer boundary of f,
or it may be linked to yet another hole. But eventually we must end up linking a
hole to the outer boundary, as the next lemma shows.
Lemma 2.5 Each connected component of the graph G corresponds exactly to
the set of cycles incident to one face.
Proof. Consider a cycle C bounding a hole in a face f. Because f lies locally
to the left of the leftmost vertex of C, C must be linked to another cycle that also
36

Section 2.3
COMPUTING THE OVERLAY OF TWO
SUBDIVISIONS
C1
C∞
C2
C3
C4
C6
C5
C7
C1
C3
C6
C∞
C2
C5
C4
C7
G
Figure 2.6
A subdivision and the corresponding
graph G
bounds f. It follows that cycles in the same connected component of G bound
the same face.
To ﬁnish the proof, we show that every cycle bounding a hole in f is in
the same connected component as the outer boundary of f. Suppose there is a
cycle for which this is not the case. Let C be the leftmost such cycle, that is, the
one whose the leftmost vertex is leftmost. By deﬁnition there is an arc between
the C and another cycle C′ that lies partly to the left of the leftmost vertex of C.
Hence, C′ is in the same connected component as C, which is not the component
of the outer boundary of f. This contradicts the deﬁnition of C.
Lemma 2.5 shows that once we have the graph G, we can create a face record
for every component. Then we can set the IncidentFace() pointers of the half-
edges that bound each face f, and we can construct the list InnerComponents(f)
and the set OuterComponent(f). How can we construct G? Recall that in the
plane sweep algorithm for line segment intersection we always looked for the
segments immediately to the left of an event point. (They had to be tested
for intersection against the leftmost edge through the event point.) Hence, the
information we need to construct G is determined during the plane sweep. So,
to construct G, we ﬁrst make a node for every cycle. To ﬁnd the arcs of G,
new arc
C
C′
f
C′
C
G
we consider the leftmost vertex v of every cycle bounding a hole. If ⃗e is the
half-edge immediately left of v, then we add an arc between the two nodes
in G representing the cycle containing ⃗e and the hole cycle of which v is the
leftmost vertex. To ﬁnd these nodes in G efﬁciently we need pointers from every
half-edge record to the node in G representing the cycle it is in. So the face
information of the doubly-connected edge list can be set in O(n+k) additional
time, after the plane sweep.
37

Chapter 2
LINE SEGMENT INTERSECTION
One thing remains: each face f in the overlay must be labeled with the names of
the faces in the old subdivisions that contained it. To ﬁnd these faces, consider an
arbitrary vertex v of f. If v is the intersection of an edge e1 from S1 and an edge
f
v
e2 from S2 then we can decide which faces of S1 and S2 contain f by looking
at the IncidentFace() pointer of the appropriate half-edges corresponding to e1
and e2. If v is not an intersection but a vertex of, say, S1, then we only know
the face of S1 containing f. To ﬁnd the face of S2 containing f, we have to
do some more work: we have to determine the face of S2 that contains v. In
other words, if we knew for each vertex of S1 in which face of S2 it lay, and
vice versa, then we could label the faces of O(S1,S2) correctly. How can we
compute this information? The solution is to apply the paradigm that has been
introduced in this chapter, plane sweep, once more. However, we won't explain
this ﬁnal step here. It is a good exercise to test your understanding of the plane
sweep approach to design the algorithm yourself. (In fact, it is not necessary to
compute this information in a separate plane sweep. It can also be done in the
sweep that computes the intersections.)
Putting everything together we get the following algorithm.
Algorithm MAPOVERLAY(S1,S2)
Input. Two planar subdivisions S1 and S2 stored in doubly-connected edge lists.
Output. The overlay of S1 and S2 stored in a doubly-connected edge list D.
1.
Copy the doubly-connected edge lists for S1 and S2 to a new doubly-
connected edge list D.
2.
Compute all intersections between edges from S1 and S2 with the plane
sweep algorithm of Section 2.1. In addition to the actions on T and Q
required at the event points, do the following:
Update D as explained above if the event involves edges of both S1
and S2. (This was explained for the case where an edge of S1 passes
through a vertex of S2.)
Store the half-edge immediately to the left of the event point at the
vertex in D representing it.
3.
(∗Now D is the doubly-connected edge list for O(S1,S2), except that the
information about the faces has not been computed yet. ∗)
4.
Determine the boundary cycles in O(S1,S2) by traversing D.
5.
Construct the graph G whose nodes correspond to boundary cycles and
whose arcs connect each hole cycle to the cycle to the left of its leftmost ver-
tex, and compute its connected components. (The information to determine
the arcs of G has been computed in line 2, second item.)
6.
for each connected component in G
7.
do Let C be the unique outer boundary cycle in the component and let
f denote the face bounded by the cycle. Create a face record for f,
set OuterComponent(f) to some half-edge of C, and construct the
list InnerComponents(f) consisting of pointers to one half-edge in
each hole cycle in the component. Let the IncidentFace() pointers
of all half-edges in the cycles point to the face record of f.
38

Section 2.4
BOOLEAN OPERATIONS
8.
Label each face of O(S1,S2) with the names of the faces of S1 and S2
containing it, as explained above.
Theorem 2.6 Let S1 be a planar subdivision of complexity n1, let S2 be a
subdivision of complexity n2, and let n := n1 +n2. The overlay of S1 and S2
can be constructed in O(nlogn+klogn) time, where k is the complexity of the
overlay.
Proof. Copying the doubly-connected edge lists in line 1 takes O(n) time, and
the plane sweep of line 2 takes O(nlogn+klogn) time by Lemma 2.3. Steps 4-
7, where we ﬁll in the face records, takes time linear in the complexity of
O(S1,S2). (The connected components of a graph can be determined in linear
time by a simple depth ﬁrst search.) Finally, labeling each face in the resulting
subdivision with the faces of the original subdivisions that contain it can be
done in O(nlogn+klogn) time.
2.4
Boolean Operations
The map overlay algorithm is a powerful instrument that can be used for various
other applications. One particular useful one is performing the Boolean opera-
tions union, intersection, and difference on two polygons P1 and P2. See Figure
2.7 for an example. Note that the output of the operations might no longer be a
polygon. It can consist of a number of polygonal regions, some with holes.
P1
P2
P1
P2
P1
P2
union
intersection
difference
Figure 2.7
The Boolean operations union,
intersection and difference on two
polygons P1 and P2
To perform the Boolean operation we regard the polygons as planar maps
whose bounded faces are labeled P1 and P2, respectively. We compute the
overlay of these maps, and we extract the faces in the overlay whose labels
correspond to the particular Boolean operation we want to perform. If we want
to compute the intersection P1 ∩P2, we extract the faces in the overlay that are
labeled with P1 and P2. If we want to compute the union P1 ∪P2, we extract the
39

Chapter 2
LINE SEGMENT INTERSECTION
faces in the overlay that are labeled with P1 or P2. And if we want to compute
the difference P1 \P2, we extract the faces in the overlay that are labeled with
P1 and not with P2.
Because every intersection point of an edge of P1 and an edge of P2 is
a vertex of P1 ∩P2, the running time of the algorithm is O(nlogn + klogn),
where n is the total number of vertices in P1 and P2, and k is the complexity of
P1 ∩P2. The same holds for the other Boolean operations: every intersection of
two edges is a vertex of the ﬁnal result, no matter which operation we want to
perform. We immediately get the following result.
Corollary 2.7 Let P1 be a polygon with n1 vertices and P2 a polygon with n2
vertices, and let n := n1 +n2. Then P1 ∩P2, P1 ∪P2, and P1 \P2 can each be
computed in O(nlogn+klogn) time, where k is the complexity of the output.
2.5
Notes and Comments
The line segment intersection problem is one of the most fundamental problems
in computational geometry. The O(nlogn+klogn) solution presented in this
chapter was given by Bentley and Ottmann [47] in 1979. (A few years earlier,
Shamos and Hoey [351] had solved the detection problem, where one is only
interested in deciding whether there is at least one intersection, in O(nlogn)
time.) The method for reducing the working storage from O(n + k) to O(n)
described in this chapter is taken from Pach and Sharir [312], who also show
that the event list can have size Ω(nlogn) before this improvement. Brown [77]
describes an alternative method to achieve the reduction.
The lower bound for the problem of reporting all line segment intersections
is Ω(nlogn + k), so the plane sweep algorithm described in this chapter is
not optimal when k is large. A ﬁrst step towards an optimal algorithm was
taken by Chazelle [88], who gave an algorithm with O(nlog2 n/loglogn+k)
running time. In 1988 Chazelle and Edelsbrunner [99, 100] presented the ﬁrst
O(nlogn+k) time algorithm. Unfortunately, it requires O(n+k) storage. Later
Clarkson and Shor [133] and Mulmuley [288] gave randomized incremental
algorithms whose expected running time is also O(nlogn+k). (See Chapter 4
for an explanation of randomized algorithms.) The working storage of these
algorithms is O(n) and O(n+k), respectively. Unlike the algorithm of Chazelle
and Edelsbrunner, these randomized algorithms also work for computing inter-
sections in a set of curves. Balaban [35] gave the ﬁrst deterministic algorithm
for the segment intersection problem that works in O(nlogn+k) time and O(n)
space. It also works for curves.
There are cases of the line segment intersection problem that are easier than
the general case. One such case is where we have two sets of segments, say
red segments and blue segments, such that no two segments from the same
set intersect each other. (This is, in fact, exactly the network overlay problem.
In the solution described in this chapter, however, the fact that the segments
came from two sets of non-intersecting segments was not used.) This so-called
red-blue line segment intersection problem was solved in O(nlogn+k) time
40

Section 2.6
EXERCISES
and O(n) storage by Mairson and Stolﬁ[262] before the general problem was
solved optimally. Other optimal red-blue intersection algorithms were given
by Chazelle et al. [101] and by Palazzi and Snoeyink [315]. If the two sets of
segments form connected subdivisions then the situation is even better: in this
case the overlay can be computed in O(n+k) time, as has been shown by Finke
and Hinrichs [176]. Their result generalizes and improves previous results on
map overlay by Nievergelt and Preparata [293], Guibas and Seidel [200], and
Mairson and Stolﬁ[262].
The line segment intersection counting problem is to determine the number
of intersection points in a set of n line segments. Since the output is a single
integer, a term with k in the time bound no longer refers to the output size
(which is constant), but only to the number of intersections. Algorithms that do
not depend on the number of intersections take O(n4/3 logc n) time, for some
small constant c [4, 95]; a running time close to O(nlogn) is not known to exist.
Plane sweep is one of the most important paradigms for designing geometric
algorithms. The ﬁrst algorithms in computational geometry based on this
paradigm are by Shamos and Hoey [351], Lee and Preparata [250], and Bentley
and Ottmann [47]. Plane sweep algorithms are especially suited for ﬁnding
intersections in sets of objects, but they can also be used for solving many other
problems. In Chapter 3 plane sweep solves part of the polygon triangulation
problem, and in Chapter 7 we will see a plane sweep algorithm to compute the
so-called Voronoi diagram of a set of points. The algorithm presented in the
current chapter sweeps a horizontal line downwards over the plane. For some
problems it is more convenient to sweep the plane in another way. For instance,
we can sweep the plane with a rotating line—see Chapter 15 for an example—or
with a pseudo-line (a line that need not be straight, but otherwise behaves more
or less as a line) [159]. The plane sweep technique can also be used in higher
dimensions: here we sweep the space with a hyperplane [213, 311, 324]. Such
algorithms are called space sweep algorithms.
In this chapter we described a data structure for storing subdivisions: the doubly-
connected edge list. This structure, or in fact a variant of it, was described by
Muller and Preparata [286]. There are also other data structures for storing
subdivisions, such as the winged edge structure by Baumgart [40] and the quad
edge structure by Guibas and Stolﬁ[202]. The difference between all these
structures is small. They all have more or less the same functionality, but some
save a few bytes of storage per edge.
2.6
Exercises
2.1
Let S be a set of n disjoint line segments whose upper endpoints lie on the
line y = 1 and whose lower endpoints lie on the line y = 0. These segments
partition the horizontal strip [−∞: ∞]×[0 : 1] into n+1 regions. Give an
O(nlogn) time algorithm to build a binary search tree on the segments
41

Chapter 2
LINE SEGMENT INTERSECTION
in S such that the region containing a query point can be determined in
O(logn) time. Also describe the query algorithm in detail.
2.2
The intersection detection problem for a set S of n line segments is to
determine whether there exists a pair of segments in S that intersect. Give
a plane sweep algorithm that solves the intersection detection problem in
O(nlogn) time.
2.3
Change the code of Algorithm FINDINTERSECTIONS (and of the pro-
cedures that it calls) such that the working storage is O(n) instead of
O(n+k).
2.4
Let S be a set of n line segments in the plane that may (partly) overlap
each other. For example, S could contain the segments (0,0)(1,0) and
(−1,0)(2,0). We want to compute all intersections in S. More precisely,
we want to compute each proper intersection of two segments in S (that
is, each intersection of two non-parallel segments) and for each end-
point of a segment all segments containing the point. Adapt algorithm
FINDINTERSECTIONS to this end.
2.5
Which of the following equalities are always true?
Twin(Twin(⃗e))
=
⃗e
Next(Prev(⃗e))
=
⃗e
Twin(Prev(Twin(⃗e)))
=
Next(⃗e)
IncidentFace(⃗e)
=
IncidentFace(Next(⃗e))
2.6
Give an example of a doubly-connected edge list where for an edge e the
faces IncidentFace(⃗e) and IncidentFace(Twin(⃗e)) are the same.
2.7
Given a doubly-connected edge list representation of a subdivision where
Twin(⃗e) = Next(⃗e) holds for every half-edge⃗e, how many faces can the
subdivision have at most?
2.8
Give pseudocode for an algorithm that lists all vertices adjacent to a
given vertex v in a doubly-connected edge list. Also, give pseudocode
for an algorithm that lists all edges that bound a face in a not necessarily
connected subdivision.
2.9
Suppose that a doubly-connected edge list of a connected subdivision is
given. Give pseudocode for an algorithm that lists all faces with vertices
that appear on the outer boundary.
2.10 Let S be a subdivision of complexity n, and let P be a set of m points. Give
a plane sweep algorithm that computes for every point in P in which face
of S it is contained. Show that your algorithm runs in O((n+m)log(n+
m)) time.
2.11 Let S be a set of n circles in the plane. Describe a plane sweep algorithm
to compute all intersection points between the circles. (Because we deal
42

Section 2.6
EXERCISES
with circles, not discs, two circles do not intersect if one lies entirely
inside the other.) Your algorithm should run in O((n + k)logn) time,
where k is the number of intersection points.
2.12 Let S be a set of n triangles in the plane. The boundaries of the triangles
are disjoint, but it is possible that a triangle lies completely inside another
triangle. Let P be a set of n points in the plane. Give an O(nlogn)
algorithm that reports each point in P lying outside all triangles.
2.13* Let S be a set of n disjoint triangles in the plane. We want to ﬁnd a set of
n−1 segments with the following properties:
Each segment connects a point on the boundary of one triangle to a
point on the boundary of another triangle.
The interiors of the segments are pairwise disjoint and they are disjoint
from the triangles.
Together they connect all triangles to each other, that is, by walking
along the segments and the triangle boundaries it must be possible to
walk from a triangle to any other triangle.
Develop a plane sweep algorithm for this problem that runs in O(nlogn)
time. State the events and the data structures that you use explicitly, and
describe the cases that arise and the actions required for each of them.
Also state the sweep invariant.
2.14 Let S be a set of n disjoint line segments in the plane, and let p be a
point not on any of the line segments of S. We wish to determine all
line segments of S that p can see, that is, all line segments of S that
contain some point q so that the open segment pq doesn't intersect any
p
not visible
line segment of S. Give an O(nlogn) time algorithm for this problem that
uses a rotating half-line with its endpoint at p.
43

3
Polygon Triangulation
Guarding an Art Gallery
Works of famous painters are not only popular among art lovers, but also among
criminals. They are very valuable, easy to transport, and apparently not so
difﬁcult to sell. Art galleries therefore have to guard their collections carefully.
Figure 3.1
An art gallery
During the day the attendants can keep a look-out, but at night this has to be
done by video cameras. These cameras are usually hung from the ceiling and
they rotate about a vertical axis. The images from the cameras are sent to TV
screens in the ofﬁce of the night watch. Because it is easier to keep an eye on
few TV screens rather than on many, the number of cameras should be as small
as possible. An additional advantage of a small number of cameras is that the
cost of the security system will be lower. On the other hand we cannot have
too few cameras, because every part of the gallery must be visible to at least
one of them. So we should place the cameras at strategic positions, such that
each of them guards a large part of the gallery. This gives rise to what is usually
referred to as the Art Gallery Problem: how many cameras do we need to guard
a given gallery and how do we decide where to place them?
45

Chapter 3
POLYGON TRIANGULATION
3.1
Guarding and Triangulations
If we want to deﬁne the art gallery problem more precisely, we should ﬁrst
formalize the notion of gallery. A gallery is, of course, a 3-dimensional space,
but a ﬂoor plan gives us enough information to place the cameras. Therefore we
model a gallery as a polygonal region in the plane. We further restrict ourselves
to regions that are simple polygons, that is, regions enclosed by a single closed
polygonal chain that does not intersect itself. Thus we do not allow regions with
holes. A camera position in the gallery corresponds to a point in the polygon. A
camera sees those points in the polygon to which it can be connected with an
open segment that lies in the interior of the polygon.
How many cameras do we need to guard a simple polygon? This clearly
depends on the polygon at hand: the more complex the polygon, the more
cameras are required. We shall therefore express the bound on the number of
cameras needed in terms of n, the number of vertices of the polygon. But even
when two polygons have the same number of vertices, one can be easier to guard
than the other. A convex polygon, for example, can always be guarded with one
camera. To be on the safe side we shall look at the worst-case scenario, that is,
we shall give a bound that is good for any simple polygon with n vertices. (It
would be nice if we could ﬁnd the minimum number of cameras for the speciﬁc
polygon we are given, not just a worst-case bound. Unfortunately, the problem
of ﬁnding the minimum number of cameras for a given polygon is NP-hard.)
Let P be a simple polygon with n vertices. Because P may be a complicated
shape, it seems difﬁcult to say anything about the number of cameras we need
to guard P. Hence, we ﬁrst decompose P into pieces that are easy to guard,
namely triangles. We do this by drawing diagonals between pairs of vertices.
Figure 3.2
A simple polygon and a possible
triangulation of it
A diagonal is an open line segment that connects two vertices of P and lies in
the interior of P. A decomposition of a polygon into triangles by a maximal
set of non-intersecting diagonals is called a triangulation of the polygon—see
Figure 3.2. (We require that the set of non-intersecting diagonals be maximal to
ensure that no triangle has a polygon vertex in the interior of one of its edges.
This could happen if the polygon has three consecutive collinear vertices.)
Triangulations are usually not unique; the polygon in Figure 3.2, for example,
can be triangulated in many different ways. We can guard P by placing a camera
in every triangle of a triangulation TP of P. But does a triangulation always
exist? And how many triangles can there be in a triangulation? The following
theorem answers these questions.
46

Section 3.1
GUARDING AND TRIANGULATIONS
Theorem 3.1 Every simple polygon admits a triangulation, and any triangula-
tion of a simple polygon with n vertices consists of exactly n−2 triangles.
Proof. We prove this theorem by induction on n. When n = 3 the polygon itself
is a triangle and the theorem is trivially true. Let n > 3, and assume that the
theorem is true for all m < n. Let P be a polygon with n vertices. We ﬁrst prove
the existence of a diagonal in P. Let v be the leftmost vertex of P. (In case of
ties, we take the lowest leftmost vertex.) Let u and w be the two neighboring
vertices of v on the boundary of P. If the open segment uw lies in the interior of
v
w
u
P, we have found a diagonal. Otherwise, there are one or more vertices inside
the triangle deﬁned by u, v, and w, or on the diagonal uw. Of those vertices, let
v′ be the one farthest from the line through u and w. The segment connecting v′
to v cannot intersect an edge of P, because such an edge would have an endpoint
v
w
u
v′
inside the triangle that is farther from the line through u and w, contradicting
the deﬁnition of v′. Hence, vv′ is a diagonal.
So a diagonal exists. Any diagonal cuts P into two simple subpolygons P1
and P2. Let m1 be the number of vertices of P1 and m2 the number of vertices
of P2. Both m1 and m2 must be smaller than n, so by induction P1 and P2 can
be triangulated. Hence, P can be triangulated as well.
It remains to prove that any triangulation of P consists of n−2 triangles. To
this end, consider an arbitrary diagonal in some triangulation TP. This diagonal
cuts P into two subpolygons with m1 and m2 vertices, respectively. Every
vertex of P occurs in exactly one of the two subpolygons, except for the vertices
deﬁning the diagonal, which occur in both subpolygons. Hence, m1+m2 = n+2.
By induction, any triangulation of Pi consists of mi −2 triangles, which implies
that TP consists of (m1 −2)+(m2 −2) = n−2 triangles.
Theorem 3.1 implies that any simple polygon with n vertices can be guarded
with n−2 cameras. But placing a camera inside every triangle seems overkill.
A camera placed on a diagonal, for example, will guard two triangles, so by
placing the cameras on well-chosen diagonals we might be able to reduce the
number of cameras to roughly n/2. Placing cameras at vertices seems even
better, because a vertex can be incident to many triangles, and a camera at that
vertex guards all of them. This suggests the following approach.
Let TP be a triangulation of P. Select a subset of the vertices of P, such
that any triangle in TP has at least one selected vertex, and place the cameras at
the selected vertices. To ﬁnd such a subset we assign each vertex of P a color:
white, gray, or black. The coloring will be such that any two vertices connected
by an edge or a diagonal have different colors. This is called a 3-coloring of a
triangulated polygon. In a 3-coloring of a triangulated polygon, every triangle
has a white, a gray, and a black vertex. Hence, if we place cameras at all gray
vertices, say, we have guarded the whole polygon. By choosing the smallest
color class to place the cameras, we can guard P using at most ⌊n/3⌋cameras.
But does a 3-coloring always exist? The answer is yes. To see this, we look
at what is called the dual graph of TP. This graph G(TP) has a node for every
triangle in TP. We denote the triangle corresponding to a node ν by t(ν). There
is an arc between two nodes ν and µ if t(ν) and t(µ) share a diagonal. The arcs
47

Chapter 3
POLYGON TRIANGULATION
in G(TP) correspond to diagonals in TP. Because any diagonal cuts P into two,
the removal of an edge from G(TP) splits the graph into two. Hence, G(TP)
is a tree. (Notice that this is not true for a polygon with holes.) This means
?
µ
ν
that we can ﬁnd a 3-coloring using a simple graph traversal, such as depth ﬁrst
search. Next we describe how to do this. While we do the depth ﬁrst search,
we maintain the following invariant: all vertices of the already encountered
triangles have been colored white, gray, or black, and no two connected vertices
have received the same color. The invariant implies that we have computed a
valid 3-coloring when all triangles have been encountered. The depth ﬁrst search
can be started from any node of G(TP); the three vertices of the corresponding
triangle are colored white, gray, and black. Now suppose that we reach a node
ν in G, coming from node µ. Hence, t(ν) and t(µ) share a diagonal. Since the
vertices of t(µ) have already been colored, only one vertex of t(ν) remains to
be colored. There is one color left for this vertex, namely the color that is not
used for the vertices of the diagonal between t(ν) and t(µ). Because G(TP) is
a tree, the other nodes adjacent to ν have not been visited yet, and we still have
the freedom to give the vertex the remaining color.
We conclude that a triangulated simple polygon can always be 3-colored. As a
⌊n/3⌋prongs
result, any simple polygon can be guarded with ⌊n/3⌋cameras. But perhaps we
can do even better. After all, a camera placed at a vertex may guard more than
just the incident triangles. Unfortunately, for any n there are simple polygons
that require ⌊n/3⌋cameras. An example is a comb-shaped polygon with a long
horizontal base edge and ⌊n/3⌋prongs made of two edges each. The prongs are
connected by horizontal edges. The construction can be made such that there is
no position in the polygon from which a camera can look into two prongs of the
comb simultaneously. So we cannot hope for a strategy that always produces
less than ⌊n/3⌋cameras. In other words, the 3-coloring approach is optimal in
the worst case.
We just proved the Art Gallery Theorem, a classical result from combinato-
rial geometry.
Theorem 3.2 (Art Gallery Theorem) For a simple polygon with n vertices,
⌊n/3⌋cameras are occasionally necessary and always sufﬁcient to have every
point in the polygon visible from at least one of the cameras.
Now we know that ⌊n/3⌋cameras are always sufﬁcient. But we don't have
an efﬁcient algorithm to compute the camera positions yet. What we need is a
fast algorithm for triangulating a simple polygon. The algorithm should deliver
a suitable representation of the triangulation—a doubly-connected edge list, for
instance—so that we can step in constant time from a triangle to its neighbors.
Given such a representation, we can compute a set of at most ⌊n/3⌋camera
positions in linear time with the method described above: use depth ﬁrst search
on the dual graph to compute a 3-coloring and take the smallest color class
to place the cameras. In the coming sections we describe how to compute a
triangulation in O(nlogn) time. Anticipating this, we already state the ﬁnal
result about guarding a polygon.
48

Section 3.2
PARTITIONING A POLYGON INTO
MONOTONE PIECES
Theorem 3.3 Let P be a simple polygon with n vertices. A set of ⌊n/3⌋camera
positions in P such that any point inside P is visible from at least one of the
cameras can be computed in O(nlogn) time.
3.2
Partitioning a Polygon into Monotone Pieces
Let P be a simple polygon with n vertices. We saw in Theorem 3.1 that a
triangulation of P always exists. The proof of that theorem is constructive and
leads to a recursive triangulation algorithm: ﬁnd a diagonal and triangulate
the two resulting subpolygons recursively. To ﬁnd the diagonal we take the
leftmost vertex of P and try to connect its two neighbors u and w; if this fails
we connect v to the vertex farthest from uw inside the triangle deﬁned by u,
v, and w. This way it takes linear time to ﬁnd a diagonal. This diagonal
may split P into a triangle and a polygon with n −1 vertices. Indeed, if we
succeed to connect u and w this will always be the case. As a consequence,
the triangulation algorithm will take quadratic time in the worst case. Can we
do better? For some classes of polygons we surely can. Convex polygons, for
instance, are easy: Pick one vertex of the polygon and draw diagonals from
this vertex to all other vertices except its neighbors. This takes only linear time.
So a possible approach to triangulate a non-convex polygon would be to ﬁrst
decompose P into convex pieces, and then triangulate the pieces. Unfortunately,
it is as difﬁcult to partition a polygon into convex pieces as it is to triangulate it.
Therefore we shall decompose P into so-called monotone pieces, which turns
out to be a lot easier.
y-axis
A simple polygon is called monotone with respect to a line ℓif for any line
ℓ′ perpendicular to ℓthe intersection of the polygon with ℓ′ is connected. In
other words, the intersection should be a line segment, a point, or empty. A
polygon that is monotone with respect to the y-axis is called y-monotone. The
following property is characteristic for y-monotone polygons: if we walk from
a topmost to a bottommost vertex along the left (or the right) boundary chain,
then we always move downwards or horizontally, never upwards.
Our strategy to triangulate the polygon P is to ﬁrst partition P into y-monotone
pieces, and then triangulate the pieces. We can partition a polygon into mono-
tone pieces as follows. Imagine walking from the topmost vertex of P to the
bottommost vertex on the left or right boundary chain. A vertex where the
direction in which we walk switches from downward to upward or from upward
to downward is called a turn vertex. To partition P into y-monotone pieces we
should get rid of these turn vertices. This can be done by adding diagonals. If
v
at a turn vertex v both incident edges go down and the interior of the polygon
locally lies above v, then we must choose a diagonal that goes up from v. The
diagonal splits the polygon into two. The vertex v will appear in both pieces.
Moreover, in both pieces v has an edge going down (namely on original edge
of P) and an edge going up (the diagonal). Hence, v cannot be a turn vertex
anymore in either of them. If both incident edges of a turn vertex go up and
49

Chapter 3
POLYGON TRIANGULATION
the interior locally lies below it, we have to choose a diagonal that goes down.
Apparently there are different types of turn vertices. Let's make this more
precise.
If we want to deﬁne the different types of turn vertices carefully, we should
pay special attention to vertices with equal y-coordinate. We do this by deﬁning
the notions of "below" and "above" as follows: a point p is below another
point q if py < qy or py = qy and px > qx, and p is above q if py > qy or
py = qy and px < qx. (You can imagine rotating the plane slightly in clockwise
direction with respect to the coordinate system, such that no two points have
the same y-coordinate; the above/below relation we just deﬁned is the same as
the above/below relation in this slightly rotated plane.)
Figure 3.3
Five types of vertices
v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
v11
v12
v13
v14
v15
e1
e2
e3
e4
e5
e6
e7
e8
e9
e10
e11
e13
e14
e15
e12
= merge vertex
= regular vertex
= end vertex
= start vertex
= split vertex
We distinguish ﬁve types of vertices in P—see Figure 3.3. Four of these
types are turn vertices: start vertices, split vertices, end vertices, and merge
vertices. They are deﬁned as follows. A vertex v is a start vertex if its two
neighbors lie below it and the interior angle at v is less than π; if the interior
angle is greater than π then v is a split vertex. (If both neighbors lie below
v, then the interior angle cannot be exactly π.) A vertex is an end vertex if
its two neighbors lie above it and the interior angle at v is less than π; if the
interior angle is greater than π then v is a merge vertex. The vertices that
are not turn vertices are regular vertices. Thus a regular vertex has one of its
neighbors above it, and the other neighbor below it. These names have been
chosen because the algorithm will use a downward plane sweep, maintaining the
intersection of the sweep line with the polygon. When the sweep line reaches
a split vertex, a component of the intersection splits, when it reaches a merge
vertex, two components merge, and so on.
The split and merge vertices are sources of local non-monotonicity. The
following, stronger statement is even true.
Lemma 3.4 A polygon is y-monotone if it has no split vertices or merge vertices.
Proof. Suppose P is not y-monotone. We have to prove that P contains a split
or a merge vertex.
50

Section 3.2
PARTITIONING A POLYGON INTO
MONOTONE PIECES
Since P is not monotone, there is a horizontal line ℓthat intersects P in
more than one connected component. We can choose ℓsuch that the leftmost
component is a segment, not a single point. Let p be the left endpoint of this
segment, and let q be the right endpoint. Starting at q, we follow the boundary
of P such that P lies to the left of the boundary. (This means that we go up from
q.) At some point, let's call it r, the boundary will intersect ℓagain. If r ̸= p, as
in Figure 3.4(a), then the highest vertex we encountered while going from q to r
must be a split vertex, and we are done.
ℓ
p
q
r
ℓ
P
merge vertex
p = r
q
r′
split vertex
(a)
(b)
P
P
Figure 3.4
Two cases in the proof of Lemma 3.4
If r = p, as in Figure 3.4(b), we again follow the boundary of P starting
at q, but this time in the other direction. As before, the boundary will intersect ℓ.
Let r′ be the point where this happens. We cannot have r′ = p, because that
would mean that the boundary of P intersects ℓonly twice, contradicting that ℓ
intersects P in more than one component. So we have r′ ̸= p, implying that the
lowest vertex we have encountered while going from q to r′ must be a merge
vertex.
Lemma 3.4 implies that P has been partitioned into y-monotone pieces once
we get rid of its split and merge vertices. We do this by adding a diagonal going
upward from each split vertex and a diagonal going downward from each merge
vertex. These diagonals should not intersect each other, of course. Once we
have done this, P has been partitioned into y-monotone pieces.
Let's ﬁrst see how we can add the diagonals for the split vertices. We use a plane
sweep method for this. Let v1,v2,...,vn be a counterclockwise enumeration
of the vertices of P. Let e1,...,en be the set of edges of P, where ei = vivi+1
for 1 ⩽i < n and en = vnv1. The plane sweep algorithm moves an imaginary
sweep line ℓdownward over the plane. The sweep line halts at certain event
points. In our case these will be the vertices of P; no new event points will be
created during the sweep. The event points are stored in a event queue Q. The
event queue is a priority queue, where the priority of a vertex is its y-coordinate.
If two vertices have the same y-coordinate then the leftmost one has higher
priority. This way the next event to be handled can be found in O(logn) time.
(Because no new events are generated during the sweep, we could also sort the
vertices on y-coordinate before the sweep, and then use the sorted list to ﬁnd
the next event in O(1) time.)
51

Chapter 3
POLYGON TRIANGULATION
The goal of the sweep is to add diagonals from each split vertex to a vertex
lying above it. Suppose that the sweep line reaches a split vertex vi. To which
vertex should we connect vi? A good candidate is a vertex close to vi, because
we can probably connect vi to this vertex without intersecting any edge of P.
Let's make this more precise. Let e j be the edge immediately to the left of vi
ej
ek
vi
ℓ
ei−1
ei
helper(ej)
on the sweep line, and let ek be the edge immediately to the right of vi on the
sweep line. Then we can always connect vi to the lowest vertex in between
ej and ek, and above vi. If there is no such vertex then we can connect vi to
the upper endpoint of ej or to the upper endpoint of ek. We call this vertex the
helper of ej and denote it by helper(ej). Formally, helper(ej) is deﬁned as the
lowest vertex above the sweep line such that the horizontal segment connecting
the vertex to ej lies inside P. Note that helper(ej) can be the upper endpoint of
ej itself.
Now we know how to get rid of split vertices: connect them to the helper of the
edge to their left. What about merge vertices? They seem more difﬁcult to get
rid of, because they need a diagonal to a vertex that is lower than they are. Since
the part of P below the sweep line has not been explored yet, we cannot add
such a diagonal when we encounter a merge vertex. Fortunately, this problem
is easier than it seems at ﬁrst sight. Suppose the sweep line reaches a merge
vertex vi. Let ej and ek be the edges immediately to the right and to the left of
vi on the sweep line, respectively. Observe that vi becomes the new helper of ej
when we reach it. We would like to connect vi to the highest vertex below the
ej
vi
ek
vm
diagonal will be added
when the sweep line
reaches vm
sweep line in between e j and ek. This is exactly the opposite of what we did
for split vertices, which we connected to the lowest vertex above the sweep line
in between ej and ek. This is not surprising: merge vertices are split vertices
upside down. Of course we don't know the highest vertex below the sweep line
when we reach vi. But it is easy to ﬁnd later on: when we reach a vertex vm
that replaces vi as the helper of e j, then this is the vertex we are looking for.
So whenever we replace the helper of some edge, we check whether the old
helper is a merge vertex and, if so, we add the diagonal between the old helper
and the new one. This diagonal is always added when the new helper is a split
vertex, to get rid of the split vertex. If the old helper was a merge vertex, we
thus get rid of a split vertex and a merge vertex with the same diagonal. It can
also happen that the helper of ej is not replaced anymore below vi. In this case
we can connect vi to the lower endpoint of ej.
In the approach above, we need to ﬁnd the edge to the left of each vertex.
Therefore we store the edges of P intersecting the sweep line in the leaves
of a dynamic binary search tree T. The left-to-right order of the leaves of
T corresponds to the left-to-right order of the edges. Because we are only
interested in edges to the left of split and merge vertices we only need to store
edges in T that have the interior of P to their right. With each edge in T we store
its helper. The tree T and the helpers stored with the edges form the status of the
sweep line algorithm. The status changes as the sweep line moves: edges start
or stop intersecting the sweep line, and the helper of an edge may be replaced.
The algorithm partitions P into subpolygons that have to be processed
52

Section 3.2
PARTITIONING A POLYGON INTO
MONOTONE PIECES
further in a later stage. To have easy access to these subpolygons we shall store
the subdivision induced by P and the added diagonals in a doubly-connected
edge list D. We assume that P is initially speciﬁed as a doubly-connected edge
list; if P is given in another form—by a counterclockwise list of its vertices, for
example—we ﬁrst construct a doubly-connected edge list for P. The diagonals
computed for the split and merge vertices are added to the doubly-connected
edge list. To access the doubly-connected edge list we use cross-pointers
between the edges in the status structure and the corresponding edges in the
doubly-connected edge list. Adding a diagonal can then be done in constant
time with some simple pointer manipulations. The global algorithm is now as
follows.
Algorithm MAKEMONOTONE(P)
Input. A simple polygon P stored in a doubly-connected edge list D.
Output. A partitioning of P into monotone subpolygons, stored in D.
1.
Construct a priority queue Q on the vertices of P, using their y-coordinates
as priority. If two points have the same y-coordinate, the one with smaller
x-coordinate has higher priority.
2.
Initialize an empty binary search tree T.
3.
while Q is not empty
4.
do Remove the vertex vi with the highest priority from Q.
5.
Call the appropriate procedure to handle the vertex, depending on
its type.
We next describe more precisely how to handle the event points. You should ﬁrst
read these algorithms without thinking about degenerate cases, and check only
later that they are also correct in degenerate cases. (To this end you should give
an appropriate meaning to "directly left of" in line 1 of HANDLESPLITVERTEX
and line 2 of HANDLEMERGEVERTEX.) There are always two things we must
do when we handle a vertex. First, we must check whether we have to add a
diagonal. This is always the case for a split vertex, and also when we replace
the helper of an edge and the previous helper was a merge vertex. Second, we
must update the information in the status structure T. The precise algorithms
for each type of event are given below. You can use the example ﬁgure on the
next page to see what happens in each of the different cases.
HANDLESTARTVERTEX(vi)
1.
Insert ei in T and set helper(ei) to vi.
At the start vertex v5 in the example ﬁgure, for instance, we insert e5 into the
tree T.
HANDLEENDVERTEX(vi)
1.
if helper(ei−1) is a merge vertex
2.
then Insert the diagonal connecting vi to helper(ei−1) in D.
3.
Delete ei−1 from T.
In the running example, when we reach end vertex v15, the helper of the edge
e14 is v14. v14 is not a merge vertex, so we don't need to insert a diagonal.
53

Chapter 3
POLYGON TRIANGULATION
HANDLESPLITVERTEX(vi)
1.
Search in T to ﬁnd the edge e j directly left of vi.
2.
Insert the diagonal connecting vi to helper(ej) in D.
3.
helper(e j) ←vi
4.
Insert ei in T and set helper(ei) to vi.
For split vertex v14 in our example, e9 is the edge to the left. Its helper is v8, so
we add a diagonal from v14 to v8.
HANDLEMERGEVERTEX(vi)
1.
if helper(ei−1) is a merge vertex
2.
then Insert the diagonal connecting vi to helper(ei−1) in D.
3.
Delete ei−1 from T.
4.
Search in T to ﬁnd the edge e j directly left of vi.
5.
if helper(ej) is a merge vertex
6.
then Insert the diagonal connecting vi to helper(e j) in D.
7.
helper(e j) ←vi
v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
v11
v12
v13
v14
v15
e1
e2
e3
e4
e5
e6
e7
e8
e9
e10e11
e13
e14
e15
e12
For the merge vertex v8 in our example, the helper v2 of edge e7 is a merge
vertex, so we add a diagonal from v8 to v2.
The only routine that remains to be described is the one to handle a regular
vertex. The actions we must take at a regular vertex depend on whether P lies
locally to its left or to its right.
HANDLEREGULARVERTEX(vi)
1.
if the interior of P lies to the right of vi
2.
then if helper(ei−1) is a merge vertex
3.
then Insert the diagonal connecting vi to helper(ei−1) in D.
4.
Delete ei−1 from T.
5.
Insert ei in T and set helper(ei) to vi.
6.
else Search in T to ﬁnd the edge ej directly left of vi.
7.
if helper(ej) is a merge vertex
8.
then Insert the diagonal connecting vi to helper(ej) in D.
9.
helper(e j) ←vi
For instance, at the regular vertex v6 in our example, we add a diagonal from v6
to v4.
It remains to prove that MAKEMONOTONE correctly partitions P into monotone
pieces.
Lemma 3.5 Algorithm MAKEMONOTONE adds a set of non-intersecting diag-
onals that partitions P into monotone subpolygons.
Proof. It is easy to see that the pieces into which P is partitioned contain no
split or merge vertices. Hence, they are monotone by Lemma 3.4. It remains
to prove that the added segments are valid diagonals (that is, that they don't
intersect the edges of P) and that they don't intersect each other. To this
end we will show that when a segment is added, it intersects neither an edge
54

Section 3.3
TRIANGULATING A MONOTONE
POLYGON
of P nor any of the previously added segments. We shall prove this for the
segment added in HANDLESPLITVERTEX; the proof for the segments added
inHANDLEENDVERTEX, HANDLEREGULARVERTEX, and HANDLEMERGE-
VERTEX is similar. We assume that no two vertices have the same y-coordinate;
the extension to the general case is fairly straightforward.
Consider a segment vmvi that is added by HANDLESPLITVERTEX when vi
is reached. Let ej be the edge to the left of vi, and let ek be the edge to the right
ej
vm
vi
Q
ek
of vi. Thus helper(ej) = vm when we reach vi.
We ﬁrst argue that vmvi does not intersect an edge of P. To see this, consider
the quadrilateral Q bounded by the horizontal lines through vm and vi, and by
ej and ek. There are no vertices of P inside Q, otherwise vm would not be the
helper of ej. Now suppose there would be an edge of P intersecting vmvi. Since
the edge cannot have an endpoint inside Q and polygon edges do not intersect
each other, it would have to intersect the horizontal segment connecting vm to
ej or the horizontal segment connecting vi to ej. Both are impossible, since for
both vm and vi, the edge ej lies immediately to the left. Hence, no edge of P can
intersect vmvi.
Now consider a previously added diagonal. Since there are no vertices of P
inside Q, and any previously added diagonal must have both of its endpoints
above vi, it cannot intersect vmvi.
We now analyze the running time of the algorithm. Constructing the priority
queue Q takes linear time and initializing T takes constant time. To handle an
event during the sweep, we perform one operation on Q, at most one query,
one insertion, and one deletion on T, and we insert at most two diagonals into
D. Priority queues and balanced search trees allow for queries and updates in
O(logn) time, and the insertion of a diagonal into D takes O(1) time. Hence,
handling an event takes O(logn) time, and the total algorithm runs in O(nlogn)
time. The amount of storage used by the algorithm is clearly linear: every vertex
is stored at most once in Q, and every edge is stored at most once in T. Together
with Lemma 3.5 this implies the following theorem.
Theorem 3.6 A simple polygon with n vertices can be partitioned into y-
monotone polygons in O(nlogn) time with an algorithm that uses O(n) storage.
3.3
Triangulating a Monotone Polygon
We have just seen how to partition a simple polygon into y-monotone pieces
in O(nlogn) time. In itself this is not very interesting. But in this section we
show that monotone polygons can be triangulated in linear time. Together these
results imply that any simple polygon can be triangulated in O(nlogn) time,
a nice improvement over the quadratic time algorithm that we sketched at the
beginning of the previous section.
Let P be a y-monotone polygon with n vertices. For the moment we assume
that P is strictly y-monotone, that is, we assume that P is y-monotone and does
55

Chapter 3
POLYGON TRIANGULATION
not contain horizontal edges. Thus we always go down when we walk on the
left or right boundary chain of P from the highest vertex of P to the lowest
one. This is the property that makes triangulating a monotone polygon easy:
we can work our way through P from top to bottom on both chains, adding
diagonals whenever this is possible. Next we describe the details of this greedy
triangulation algorithm.
The algorithm handles the vertices in order of decreasing y-coordinate. If two
not yet
triangulated
triangles
split off
vertices have the same y-coordinate, then the leftmost one is handled ﬁrst. The
algorithm requires a stack S as auxiliary data structure. Initially the stack is
empty; later it contains the vertices of P that have been encountered but may
still need more diagonals. When we handle a vertex we add as many diagonals
from this vertex to vertices on the stack as possible. These diagonals split off
triangles from P. The vertices that have been handled but not split off—the
vertices on the stack—are on the boundary of the part of P that still needs to be
triangulated. The lowest of these vertices, which is the one encountered last, is
on top of the stack, the second lowest is second on the stack, and so on. The
part of P that still needs to be triangulated, and lies above the last vertex that
has been encountered so far, has a particular shape: it looks like a funnel turned
upside down. One boundary of the funnel consists of a part of a single edge of
P, and the other boundary is a chain consisting of reﬂex vertices, that is, the
interior angle at these vertices is at least 180◦. Only the highest vertex, which is
at the bottom of the stack, is convex. This property remains true after we have
handled the next vertex. Hence, it is an invariant of the algorithm.
Now, let's see which diagonals we can add when we handle the next vertex.
We distinguish two cases: vj, the next vertex to be handled, lies on the same
chain as the reﬂex vertices on the stack, or it lies on the opposite chain. If vj
vj
popped
pushed
popped and
pushed
e
lies on the opposite chain, it must be the lower endpoint of the single edge e
bounding the funnel. Due to the shape of the funnel, we can add diagonals from
vj to all vertices currently on the stack, except for the last one (that is, the one
at the bottom of the stack); the last vertex on the stack is the upper vertex of e,
so it is already connected to vj. All these vertices are popped from the stack.
The untriangulated part of the polygon above vj is bounded by the diagonal
that connects vj to the vertex previously on top of the stack and the edge of P
extending downward from this vertex, so it looks like a funnel and the invariant
is preserved. This vertex and vj remain part of the not yet triangulated polygon,
so they are pushed onto the stack.
The other case is when vj is on the same chain as the reﬂex vertices on the
stack. This time we may not be able to draw diagonals from vj to all vertices on
the stack. Nevertheless, the ones to which we can connect vj are all consecutive
and they are on top of the stack, so we can proceed as follows. First, pop one
vertex from the stack; this vertex is already connected to vj by an edge of P.
Next, pop vertices from the stack and connect them to vj until we encounter
one where this is not possible. Checking whether a diagonal can be drawn from
vj to a vertex vk on the stack can be done by looking at vj, vk, and the previous
vertex that was popped. When we ﬁnd a vertex to which we cannot connect vj,
we push the last vertex that has been popped back onto the stack. This is either
56

Section 3.3
TRIANGULATING A MONOTONE
POLYGON
the last vertex to which a diagonal was added or, if no diagonals have been
added, it is the neighbor of vj on the boundary of P—see Figure 3.5. After this
pushed
vj
popped
pushed
vj
popped and
pushed
popped and
pushed
Figure 3.5
Two cases when the next vertex is on
the same side as the reﬂex vertices on
the stack
has been done we push vj onto the stack. In both cases the invariant is restored:
one side of the funnel is bounded by a part of a single edge, and the other side
is bounded by a chain of reﬂex vertices. We get the following algorithm. (The
algorithm is actually similar to the convex hull algorithm of Chapter 1.)
Algorithm TRIANGULATEMONOTONEPOLYGON(P)
Input. A strictly y-monotone polygon P stored in a doubly-connected edge
list D.
Output. A triangulation of P stored in the doubly-connected edge list D.
1.
Merge the vertices on the left chain and the vertices on the right chain of P
into one sequence, sorted on decreasing y-coordinate. If two vertices have
the same y-coordinate, then the leftmost one comes ﬁrst. Let u1,...,un
denote the sorted sequence.
2.
Initialize an empty stack S, and push u1 and u2 onto it.
3.
for j ←3 to n−1
4.
do if uj and the vertex on top of S are on different chains
5.
then Pop all vertices from S.
6.
Insert into D a diagonal from u j to each popped vertex,
except the last one.
7.
Push uj−1 and uj onto S.
8.
else Pop one vertex from S.
9.
Pop the other vertices from S as long as the diagonals from
uj to them are inside P. Insert these diagonals into D. Push
the last vertex that has been popped back onto S.
10.
Push uj onto S.
11. Add diagonals from un to all stack vertices except the ﬁrst and the last one.
How much time does the algorithm take? Step 1 takes linear time and Step 2
takes constant time. The for-loop is executed n−3 times, and one execution
may take linear time. But at every execution of the for-loop at most two vertices
are pushed. Hence, the total number of pushes, including the two in Step 2, is
bounded by 2n−4. Because the number of pops cannot exceed the number of
pushes, the total time for all executions of the for-loop is O(n). The last step of
57

Chapter 3
POLYGON TRIANGULATION
the algorithm also takes at most linear time, so the total algorithm runs in O(n)
time.
Theorem 3.7 A strictly y-monotone polygon with n vertices can be triangulated
in linear time.
We wanted a triangulation algorithm for monotone polygons as a subroutine
for triangulating arbitrary simple polygons. The idea was to ﬁrst decompose
a polygon into monotone pieces and then to triangulate these pieces. It seems
that we have all the ingredients we need. There is one problem, however: in
this section we have assumed that the input is a strictly y-monotone polygon,
whereas the algorithm of the previous section may produce monotone pieces
with horizontal edges. Recall that in the previous section we treated vertices
with the same y-coordinates from left to right. This had the same effect as a
slight rotation of the plane in clockwise direction such that no two vertices
are on a horizontal line. It follows that the monotone subpolygons produced
by the algorithm of the previous section are strictly monotone in this slightly
rotated plane. Hence, the triangulation algorithm of the current section operates
correctly if we treat vertices with the same y-coordinate from left to right
(which corresponds to working in the rotated plane). So we can combine the
two algorithms to obtain a triangulation algorithm that works for any simple
polygon.
How much time does the triangulation algorithm take? Decomposing the
polygon into monotone pieces takes O(nlogn) time by Theorem 3.6. In the
second stage we triangulate each of the monotone pieces with the linear-time
algorithm of this section. Since the sum of the number of vertices of the pieces
is O(n), the second stage takes O(n) time in total. We get the following result.
Theorem 3.8 A simple polygon with n vertices can be triangulated in O(nlogn)
time with an algorithm that uses O(n) storage.
We have seen how to triangulate simple polygons. But what about polygons
with holes, can they also be triangulated easily? The answer is yes. In fact,
the algorithm we have seen also works for polygons with holes: nowhere in
the algorithm for splitting a polygon into monotone pieces did we use the fact
that the polygon was simple. It even works in a more general setting: Suppose
we have a planar subdivision S and we want to triangulate that subdivision.
More precisely, if B is a bounding box containing all edges of S in its interior,
we want to ﬁnd a maximal set of non-intersecting diagonals—line segments
connecting vertices of S or B that do not intersect the edges of S—that partitions
B into triangles. Figure 3.6 shows a triangulated subdivision. The edges of
the subdivisions and of the bounding box are shown bold. To compute such a
triangulation we can use the algorithm of this chapter: ﬁrst split the subdivision
into monotone pieces, and then triangulate the pieces. This leads to the following
theorem.
Theorem 3.9 A planar subdivision with n vertices in total can be triangulated
in O(nlogn) time with an algorithm that uses O(n) storage.
58

Section 3.4
NOTES AND COMMENTS
Figure 3.6
A triangulated subdivision
3.4
Notes and Comments
The Art Gallery Problem was posed in 1973 by Victor Klee in a conversation
with Vasek Chv´atal. In 1975 Chv´atal [128] gave the ﬁrst proof that ⌊n/3⌋
cameras are always sufﬁcient and sometimes necessary; a result that became
known as the Art Gallery Theorem or the Watchman Theorem. Chv´atal's proof
is quite complicated. The much simpler proof presented in this chapter was
discovered by Fisk [178]. His proof is based on the Two Ears Theorem by
Meisters [277], from which the 3-colorability of the graph that is a triangulation
of a simple polygon follows easily. The algorithmic problem of ﬁnding the
minimum number of guards for a given simple polygon was shown to be NP-
hard by Aggarwal [10] and Lee and Lin [246]. The book by O'Rourke [298]
and the overview by Shermer [355] contain an extensive treatment of the Art
Gallery Problem and numerous variations.
A decomposition of a polygon, or any other region, into simple pieces is useful in
many problems. Often the simple pieces are triangles, in which case we call the
decomposition a triangulation, but sometimes other shapes such as quadrilaterals
or trapezoids are used—see also Chapters 6, 9, and 14. We only discuss the
results on triangulating polygons here. The linear time algorithm to triangulate a
monotone polygon described in this chapter was given by Garey et al. [188], and
the plane sweep algorithm to partition a polygon into monotone pieces is due to
Lee and Preparata [250]. Avis and Toussaint [32] and Chazelle [85] described
other algorithms for triangulating a simple polygon in O(nlogn) time.
For a long time one of the main open problems in computational geome-
try was whether simple polygons can be triangulated in o(nlogn) time. (For
triangulating subdivisions with holes there is an Ω(nlogn) lower bound.) In
this chapter we have seen that this is indeed the case for monotone polygons.
Linear-time triangulation algorithms were also found for other special classes of
polygons [108, 109, 170, 184, 214] but the problem for general simple polygons
remained open for a number of years. In 1988 Tarjan and Van Wyk [368] broke
the O(nlogn) barrier by presenting an O(nloglogn) algorithm. Their algorithm
was later simpliﬁed by Kirkpatrick et al. [237]. Randomization—an approach
used in Chapters 4, 6, 9, and 11—proved to be a good tool in developing even
59

Chapter 3
POLYGON TRIANGULATION
faster algorithms: Clarkson et al. [134], Devillers [141], and Seidel [345] pre-
sented algorithms with O(nlog∗n) running time, where log∗n is the iterated
logarithm of n (being the number of times you can take the logarithm before the
result is smaller than 1). These algorithms are not only slightly faster than the
O(nloglogn) algorithm, but also simpler. Seidel's algorithm is closely related
to the algorithm for constructing a trapezoidal decomposition of a planar subdi-
vision described in Chapter 6. However, the question whether a simple polygon
can be triangulated in linear time was still open. In 1990 this problem was ﬁnally
settled by Chazelle [92, 94], who gave a (quite complicated) deterministic linear
time algorithm. A randomized linear time algorithm was developed later by
Amato et al. [15].
The 3-dimensional equivalent to the polygon triangulation problem is this: de-
compose a given polytope into non-overlapping tetrahedra, where the vertices of
the tetrahedra must be vertices of the original polytope. Such a decomposition is
called a tetrahedralization of the polytope. This problem is much more difﬁcult
than the two-dimensional version. In fact, it is not always possible to decompose
a polytope into tetrahedra without using additional vertices. Chazelle [86] has
shown that for a simple polytope with n vertices, Θ(n2) additional vertices may
be needed and are always sufﬁcient to obtain a decomposition into tetrahedra.
This bound was reﬁned by Chazelle and Palios [110] to Θ(n+r2), where r is the
number of reﬂex edges of the polytope. The algorithm to compute the decompo-
sition runs in O(nr +r2 logr) time. Deciding whether a given simple polytope
can be tetrahedralized without additional vertices is NP-complete [330].
3.5
Exercises
3.1
Prove that any polygon admits a triangulation, even if it has holes. Can
you say anything about the number of triangles in the triangulation?
3.2
A rectilinear polygon is a simple polygon of which all edges are horizontal
or vertical. Let P be a rectilinear polygon with n vertices. Give an example
to show that ⌊n/4⌋cameras are sometimes necessary to guard it.
3.3
Prove or disprove: The dual graph of the triangulation of a monotone
polygon is always a chain, that is, any node in this graph has degree at
most two.
3.4
Suppose that a simple polygon P with n vertices is given, together with a
set of diagonals that partitions P into convex quadrilaterals. How many
cameras are sufﬁcient to guard P? Why doesn't this contradict the Art
Gallery Theorem?
3.5
Give the pseudo-code of the algorithm to compute a 3-coloring of a
triangulated simple polygon. The algorithm should run in linear time.
60

Section 3.5
EXERCISES
3.6
Give an algorithm that computes in O(nlogn) time a diagonal that splits
a simple polygon with n vertices into two simple polygons each with at
most ⌊2n/3⌋+2 vertices. Hint: Use the dual graph of a triangulation.
3.7
Let P be a simple polygon with n vertices, which has been partitioned
into monotone pieces. Prove that the sum of the number of vertices of the
pieces is O(n).
3.8
The algorithm given in this chapter to partition a simple polygon into
monotone pieces constructs a doubly-connected edge list for the parti-
tioned polygon. During the algorithm, new edges are added to the DCEL
(namely, diagonals to get rid of split and merge vertices). In general,
adding an edge to a DCEL cannot be done in constant time. Discuss
why adding an edge may take more than constant time, and argue that in
the polygon-partitioning algorithm we can add a diagonal in O(1) time
nevertheless.
3.9
Show that if a polygon has O(1) turn vertices, then the algorithm given in
this chapter can be made to run in O(n) time.
3.10 Can the algorithm of this chapter also be used to triangulate a set of n
points? If so, explain how to do this efﬁciently.
3.11 Give an efﬁcient algorithm to determine whether a polygon P with n
vertices is monotone with respect to some line, not necessarily a horizontal
or vertical one.
3.12 The pockets of a simple polygon are the areas outside the polygon, but
inside its convex hull. Let P1 be a simple polygon with m vertices, and
assume that a triangulation of P1 as well as its pockets is given. Let P2
pockets
be a convex polygon with n vertices. Show that the intersection P1 ∩P2
can be computed in O(m+n) time.
3.13 The stabbing number of a triangulated simple polygon P is the maximum
number of diagonals intersected by any line segment interior to P. Give
an algorithm that computes a triangulation of a convex polygon that has
stabbing number O(logn).
3.14 Given a simple polygon P with n vertices and a point p inside it, show
how to compute the region inside P that is visible from p.
61

4
Linear Programming
Manufacturing with Molds
Most objects we see around us today—from car bodies to plastic cups and
cutlery—are made using some form of automated manufacturing. Computers
play an important role in this process, both in the design phase and in the
construction phase; CAD/CAM facilities are a vital part of any modern factory.
The construction process used to manufacture a speciﬁc object depends on
factors such as the material the object should be made of, the shape of the object,
and whether the object will be mass produced. In this chapter we study some
geometric aspects of manufacturing with molds, a commonly used process for
plastic or metal objects. For metal objects this process is often referred to as
casting.
Figure 4.1
The casting process
Figure 4.1 illustrates the casting process: liquid metal is poured into a mold, it
solidiﬁes, and then the object is removed from the mold. The last step is not
always as easy as it seems; the object could be stuck in the mold, so that it
cannot be removed without breaking the mold. Sometimes we can get around
this problem by using a different mold. There are also objects, however, for
which no good mold exists; a sphere is an example. This is the problem we
shall study in this chapter: given an object, is there a mold for it from which it
can be removed?
We shall conﬁne ourselves to the following situation. First of all, we assume
that the object to be constructed is polyhedral. Secondly, we only consider
63

Chapter 4
LINEAR PROGRAMMING
molds of one piece, not molds consisting of two or more pieces. (Using molds
consisting of two pieces, it is possible to manufacture objects such as spheres,
which cannot be manufactured using a mold of a single piece.) Finally, we
only allow the object to be removed from the mold by a single translation. This
means that we will not be able to remove a screw from its mold. Fortunately,
translational motions sufﬁce for many objects.
4.1
The Geometry of Casting
If we want to determine whether an object can be manufactured by casting,
we have to ﬁnd a suitable mold for it. The shape of the cavity in the mold is
determined by the shape of the object, but different orientations of the object
give rise to different molds. Choosing the orientation can be crucial: some
orientations may give rise to molds from which the object cannot be removed,
while other orientations allow removal of the object. One obvious restriction on
the orientation is that the object must have a horizontal top facet. This facet will
be the only one not in contact with the mold. Hence, there are as many potential
orientations—or, equivalently, possible molds—as the object has facets. We call
an object castable if it can be removed from its mold for at least one of these
orientations. In the following we shall concentrate on determining whether an
object is removable by a translation from a speciﬁc given mold. To decide on
the castability of the object we then simply try every potential orientation.
Let P be a 3-dimensional polyhedron—that is, a 3-dimensional solid bounded
top facet
by planar facets—with a designated top facet. (We shall not try to give a precise,
formal deﬁnition of a polyhedron. Giving such a deﬁnition is tricky and not
necessary in this context.) We assume that the mold is a rectangular block
with a cavity that corresponds exactly to P. When the polyhedron is placed in
the mold, its top facet should be coplanar with the topmost facet of the mold,
which we assume to be parallel to the xy-plane. This means that the mold has
no unnecessary parts sticking out on the top that might prevent P from being
removed.
We call a facet of P that is not the top facet an ordinary facet. Every ordinary
facet f has a corresponding facet in the mold, which we denote by ˆf.
We want to decide whether P can be removed from its mold by a single transla-
tion. In other words, we want to decide whether a direction ⃗d exists such that
P can be translated to inﬁnity in direction ⃗d without intersecting the interior
of the mold during the translation. Note that we allow P to slide along the
mold. Because the facet of P not touching the mold is its top facet, the removal
direction has to be upward, that is, it must have a positive z-component. This is
only a necessary condition on the removal direction; we need more constraints
to be sure that a direction is valid.
Let f be an ordinary facet of P. This facet must move away from, or slide
along, its corresponding facet ˆf of the mold. To make this constraint precise,
we need to deﬁne the angle of two vectors in 3-space. We do this as follows.
64

Section 4.1
THE GEOMETRY OF CASTING
Take the plane spanned by the vectors (we assume both vectors are rooted at
the origin); the angle of the vectors is the smaller of the two angles measured in
this plane. Now ˆf blocks any translation in a direction making an angle of less
than 90◦with ⃗η(f), the outward normal of f. So a necessary condition on ⃗d is
that it makes an angle of at least 90◦with the outward normal of every ordinary
facet of P. The next lemma shows that this condition is also sufﬁcient.
Lemma 4.1 The polyhedron P can be removed from its mold by a translation
in direction ⃗d if and only if ⃗d makes an angle of at least 90◦with the outward
normal of all ordinary facets of P.
p
P
⃗η( ˆf)
⃗d
f
Proof. The "only if" part is easy: if ⃗d made an angle less than 90◦with some
outward normal ⃗η(f), then any point q in the interior of f collides with the
mold when translated in direction ⃗d.
To prove the "if" part, suppose that at some moment P collides with the
mold when translated in direction ⃗d. We have to show that there must be an
outward normal making an angle of less than 90◦with ⃗d. Let p be a point of
P that collides with a facet ˆf of the mold. This means that p is about to move
into the interior of the mold, so ⃗η( ˆf), the outward normal of ˆf, must make an
angle greater than 90◦with ⃗d. But then ⃗d makes an angle less than 90◦with the
outward normal of the ordinary facet f of P that corresponds to ˆf.
Lemma 4.1 has an interesting consequence: if P can be removed by a
sequence of small translations, then it can be removed by a single translation.
So allowing for more than one translation does not help in removing the object
from its mold.
We are left with the task of ﬁnding a direction ⃗d that makes an angle of at
z
x
y
z = 1
least 90◦with the outward normal of each ordinary facet of P. A direction in
3-dimensional space can be represented by a vector rooted at the origin. We
already know that we can restrict our attention to directions with a positive
z-component. We can represent all such directions as points in the plane z = 1,
where the point (x,y,1) represents the direction of the vector (x,y,1). This way
every point in the plane z = 1 represents a unique direction, and every direction
with a positive z-value is represented by a unique point in that plane.
Lemma 4.1 gives necessary and sufﬁcient conditions on the removal direc-
tion ⃗d. How do these conditions translate into our plane of directions? Let
⃗η = (⃗ηx,⃗ηy,⃗ηz) be the outward normal of an ordinary facet. The direction
⃗d = (dx,dy,1) makes an angle at least 90◦with ⃗η if and only if the dot product
of ⃗d and ⃗η is non-positive. Hence, an ordinary facet induces a constraint of the
form
⃗ηxdx +⃗ηydy +⃗ηz ⩽0.
This inequality describes a half-plane on the plane z = 1, that is, the area left or
the area right of a line on the plane. (This last statement is not true for horizontal
facets, which have ⃗ηx =⃗ηy = 0. In this case the constraint is either impossible
to satisfy or always satisﬁed, which is easy to test.) Hence, every non-horizontal
facet of P deﬁnes a closed half-plane on the plane z = 1, and any point in the
65

Chapter 4
LINEAR PROGRAMMING
common intersection of these half-planes corresponds to a direction in which P
can be removed. The common intersection of these half-planes may be empty;
in this case P cannot be removed from the given mold.
We have transformed our manufacturing problem to a purely geometric problem
in the plane: given a set of half-planes, ﬁnd a point in their common intersection
or decide that the common intersection is empty. If the polyhedron to be
manufactured has n facets, then the planar problem has at most n−1 half-planes
(the top facet does not induce a half-plane). In the next sections we will see
that the planar problem just stated can be solved in expected linear time—see
Section 4.4, where also the meaning of "expected" is explained.
Recall that the geometric problem corresponds to testing whether P can be
removed from a given mold. If this is impossible, there can still be other molds,
corresponding to different choices of the top facet, from which P is removable.
In order to test whether P is castable, we try all its facets as top facets. This
leads to the following result.
Theorem 4.2 Let P be a polyhedron with n facets. In O(n2) expected time and
using O(n) storage it can be decided whether P is castable. Moreover, if P is
castable, a mold and a valid direction for removing P from it can be computed
in the same amount of time.
4.2
Half-Plane Intersection
Let H = {h1,h2,...,hn} be a set of linear constraints in two variables, that is,
constraints of the form
aix+biy ⩽ci,
where ai, bi, and ci are constants such that at least one of ai and bi is non-zero.
Geometrically, we can interpret such a constraint as a closed half-plane in R2,
bounded by the line aix+biy = ci. The problem we consider in this section is to
ﬁnd the set of all points (x,y) ∈R2 that satisfy all n constraints at the same time.
In other words, we want to ﬁnd all the points lying in the common intersection
of the half-planes in H. (In the previous section we reduced the casting problem
to ﬁnding some point in the intersection of a set of half-planes. The problem we
study now is more general.)
The shape of the intersection of a set of half-planes is easy to determine: a
half-plane is convex, and the intersection of convex sets is again a convex
set, so the intersection of a set of half-planes is a convex region in the plane.
Every point on the intersection boundary must lie on the bounding line of some
half-plane. Hence, the boundary of the region consists of edges contained in
these bounding lines. Since the intersection is convex, every bounding line can
contribute at most one edge. It follows that the intersection of n half-planes
is a convex polygonal region bounded by at most n edges. Figure 4.2 shows
a few examples of intersections of half-planes. To which side of its bounding
66

Section 4.2
HALF-PLANE INTERSECTION
line a half-plane lies is indicated by dark shading in the ﬁgure; the common
intersection is shaded lightly. As you can see in Figures 4.2 (ii) and (iii), the
(i)
(ii)
(iii)
(iv)
(v)
Figure 4.2
Examples of the intersection of
half-planes
intersection does not have to be bounded. The intersection can also degenerate
to a line segment or a point, as in (iv), or it can be empty, as in (v).
We give a rather straightforward divide-and-conquer algorithm to compute the
intersection of a set of n half-planes. It is based on a routine INTERSECTCON-
VEXREGIONS to compute the intersection of two convex polygonal regions. We
ﬁrst give the overall algorithm.
Algorithm INTERSECTHALFPLANES(H)
Input. A set H of n half-planes in the plane.
Output. The convex polygonal region C := !
h∈H h.
1.
if card(H) = 1
2.
then C ←the unique half-plane h ∈H
3.
else Split H into sets H1 and H2 of size ⌈n/2⌉and ⌊n/2⌋.
4.
C1 ←INTERSECTHALFPLANES(H1)
5.
C2 ←INTERSECTHALFPLANES(H2)
6.
C ←INTERSECTCONVEXREGIONS(C1,C2)
What remains is to describe the procedure INTERSECTCONVEXREGIONS. But
wait—didn't we see this problem before, in Chapter 2? Indeed, Corollary 2.7
states that we can compute the intersection of two polygons in O(nlogn +
klogn) time, where n is the total number of vertices in the two polygons. We
must be a bit careful in applying this result to our problem, because the regions
we have can be unbounded, or degenerate to a segment or a point. Hence,
the regions are not necessarily polygons. But it is not difﬁcult to modify the
algorithm from Chapter 2 so that it still works.
Let's analyze this approach. Assume we have already computed the two regions
C1 and C2 by recursion. Since they are both deﬁned by at most n/2 +1 half-
planes, they both have at most n/2+1 edges. The algorithm from Chapter 2
computes their overlay in time O((n + k)logn), where k is the number of
intersection points between edges of C1 and edges of C2. What is k? Look
67

Chapter 4
LINEAR PROGRAMMING
at an intersection point v between an edge e1 of C1 and an edge e2 of C2. No
matter how e1 and e2 intersect, v must be a vertex of C1 ∩C2. But C1 ∩C2 is the
intersection of n half-planes, and therefore has at most n edges and vertices. It
follows that k ⩽n, so the computation of the intersection of C1 and C2 takes
O(nlogn) time.
v
e1
e2
This gives the following recurrence for the total running time:
T(n) =
"
O(1),
if n = 1,
O(nlogn)+2T(n/2),
if n > 1.
This recurrence solves to T(n) = O(nlog2 n).
To obtain this result we used a subroutine for computing the intersection of
two arbitrary polygons. The polygonal regions we deal with in INTERSECT-
HALFPLANES are always convex. Can we use this to develop a more efﬁcient
algorithm? The answer is yes, as we show next. We will assume that the regions
we want to intersect are 2-dimensional; the case where one or both of them is a
segment or a point is easier and left as an exercise.
First, let's specify more precisely how we represent a convex polygonal region
Lleft(C) = h3,h4,h5
Lright(C) = h2,h1
h1
h2
h3
h4
h5
left boundary
right boundary
C. We will store the left and the right boundary of C separately, as sorted lists
of half-planes. The lists are sorted in the order in which the bounding lines of
the half-planes occur when the (left or right) boundary is traversed from top to
bottom. We denote the left boundary list by Lleft(C), and the right boundary
list by Lright(C). Vertices are not stored explicitly; they can be computed by
intersecting consecutive bounding lines.
To simplify the description of the algorithm, we shall assume that there are
no horizontal edges. (To adapt the algorithm to deal with horizontal edges, one
can deﬁne such edges to belong to the left boundary if they bound C from above,
and to the right boundary if they bound C from below. With this convention
only a few adaptations are needed to the algorithm stated below.)
The new algorithm is a plane sweep algorithm, like the one in Chapter 2: we
move a sweep line downward over the plane, and we maintain the edges of C1
and C2 intersecting the sweep line. Since C1 and C2 are convex, there are at most
four such edges. Hence, there is no need to store these edges in a complicated
data structure; instead we simply have pointers left edge C1, right edge C1,
left edge C2, and right edge C2 to them. If the sweep line does not intersect
the right or left boundary of a region, then the corresponding pointer is nil.
Figure 4.3 illustrates the deﬁnitions.
How are these pointers initialized? Let y1 be the y-coordinate of the topmost
vertex of C1; if C1 has an unbounded edge extending upward to inﬁnity then
we deﬁne y1 = ∞. Deﬁne y2 similarly for C2, and let ystart = min(y1,y2). To
compute the intersection of C1 and C2 we can restrict our attention to the part
of the plane with y-coordinate less than or equal to ystart. Hence, we let the
sweep line start at ystart, and we initialize the edges left edge C1, right edge C1,
left edge C2, and right edge C2 as the ones intersecting the line y = ystart.
68

Section 4.2
HALF-PLANE INTERSECTION
C2
C1
left edge C1
right edge C1 = nil
left edge C2
right edge C2
Figure 4.3
The edges maintained by the sweep line
algorithm
In a plane sweep algorithm one normally also needs a queue to store the events.
In our case the events are the points where edges of C1 or of C2 start or stop to
intersect the sweep line. This implies that the next event point, which determines
the next edge to be handled, is the highest of the lower endpoints of the edges
intersecting the sweep line. (Endpoints with the same y-coordinate are handled
from left to right. If two endpoints coincide then the leftmost edge is treated
ﬁrst.) Hence, we don't need an event queue; the next event can be found in
constant time using the pointers left edge C1, right edge C1, left edge C2, and
right edge C2.
At each event point some new edge e appears on the boundary. To handle
the edge e we ﬁrst check whether e belongs to C1 or to C2, and whether it is on
the left or the right boundary, and then call the appropriate procedure. We shall
only describe the procedure that is called when e is on the left boundary of C1.
The other procedures are similar.
Let p be the upper endpoint of e. The procedure that handles e will discover three
possible edges that C might have: the edge with p as upper endpoint, the edge
with e∩left edge C2 as upper endpoint, and the edge with e∩right edge C2
as upper endpoint. It performs the following actions.
First we test whether p lies in between left edge C2 and right edge C2. If
this is the case, then e contributes an edge to C starting at p. We then add
the half-plane whose bounding line contains e to the list Lleft(C).
Next we test whether e intersects right edge C2. If this is the case, then the
intersection point is a vertex of C. Either both edges contribute an edge to
C starting at the intersection point—this happens when p lies to the right
of right edge C2, as in Figure 4.4(i)—or both edges contribute an edge
ending there—this happens when p lies to the left of right edge C2, as in
Figure 4.4(ii). If both edges contribute an edge starting at the intersection
point, then we have to add the half-plane deﬁning e to Lleft(C) and the
half-plane deﬁning right edge C2 to Lright(C). If they contribute an edge
ending at the intersection point we do nothing; these edges have already
been discovered in some other way.
Finally we test whether e intersects left edge C2. If this is the case, then the
69

Chapter 4
LINEAR PROGRAMMING
Figure 4.4
The two possibilities when e intersects
right edge C2
right edge C2
(i)
(ii)
e
e
right edge C2
p
p
intersection point is a vertex of C. The edge of C starting at that vertex is
e
left edge C2
p
either a part of e or it is a part of left edge C2. We can decide between these
possibilities in constant time: if p lies to the left of left edge C2 then it is a
part of e, otherwise it is a part of left edge C2. After we decided whether e
or left edge C2 contributes the edge to C, we add the appropriate half-plane
to Lleft(C).
Notice that we may add two half-planes to Lleft(C): the half-plane bounding e
and the half-plane bounding left edge C2. In which order should we add them?
We add left edge C2 only if it deﬁnes an edge of C starting at the intersection
point of left edge C2 and e. If we also decide to add the half-plane of e, it
must be because e deﬁnes an edge of C starting at its upper endpoint or at
its intersection point with right edge C2. In both cases we should add the
half-plane bounding e ﬁrst, which is guaranteed by the order of the tests given
above.
We conclude that it takes constant time to handle an edge, so the intersection of
two convex polygons can be computed in time O(n). To show that the algorithm
is correct, we have to prove that it adds the half-planes deﬁning the edges of C
in the right order. Consider an edge of C, and let p be its upper endpoint. Then
p is either an upper endpoint of an edge in C1 or C2, or it is the intersection of
two edges e and e′ of C1 and C2, respectively. In the former case we discover the
edge of C when p is reached, and in the latter case when the lower of the upper
endpoints of e and e′ is reached. Hence, all half-planes deﬁning the edges of C
are added. It is not difﬁcult to prove that they are added in the correct order.
We get the following result:
Theorem 4.3 The intersection of two convex polygonal regions in the plane can
be computed in O(n) time.
This theorem shows that we can do the merge step in INTERSECTHALF-
PLANES in linear time. Hence, the recurrence for the running time of the
algorithm becomes
T(n) =
"
O(1),
if n = 1,
O(n)+2T(n/2),
if n > 1,
leading to the following result:
70

Section 4.3
INCREMENTAL LINEAR
PROGRAMMING
Corollary 4.4 The common intersection of a set of n half-planes in the plane
can be computed in O(nlogn) time and linear storage.
The problem of computing the intersection of half-planes is intimately
related to the computation of convex hulls, and an alternative algorithm can be
given that is almost identical to algorithm CONVEXHULL from Chapter 1. The
relationship between convex hulls and intersections of half-planes is discussed
in detail in Sections 8.2 and 11.4. Those sections are independent of the rest of
their chapters, so if you are curious you can already have a look.
4.3
Incremental Linear Programming
In the previous section we showed how to compute the intersection of a set of
n half-planes. In other words, we computed all solutions to a set of n linear
constraints. The running time of our algorithm was O(nlogn). One can prove
that this is optimal: as for the sorting problem, any algorithm that solves the
half-plane intersection problem must take Ω(nlogn) time in the worst case. In
our application to the casting problem, however, we don't need to know all
solutions to the set of linear constraints; just one solution will do ﬁne. It turns
out that this allows for a faster algorithm.
Finding a solution to a set of linear constraints is closely related to a well-
known problem in operations research, called linear optimization or linear
programming. (This term was coined before "programming" came to mean
"giving instructions to a computer".) The only difference is that linear program-
ming involves ﬁnding one speciﬁc solution to the set of constraints, namely the
one that maximizes a given linear function of the variables. More precisely, a
linear optimization problem is described as follows:
Maximize
c1x1 +c2x2 +···+cdxd
Subject to
a1,1x1 +···+a1,dxd ⩽b1
a2,1x1 +···+a2,dxd ⩽b2
...
an,1x1 +···+an,dxd ⩽bn
where the ci, and ai,j, and bi are real numbers, which form the input to the
problem. The function to be maximized is called the objective function, and
the set of constraints together with the objective function is a linear program.
The number of variables, d, is the dimension of the linear program. We already
saw that linear constraints can be viewed as half-spaces in Rd. The intersection
of these half-spaces, which is the set of points satisfying all constraints, is
called the feasible region of the linear program. Points (solutions) in this region
are called feasible, points outside are infeasible. Recall from Figure 4.2 that
the feasible region can be unbounded, and that it can be empty. In the latter
case, the linear program is called infeasible. The objective function can be
viewed as a direction in Rd; maximizing c1x1 +c2x2 +···+cdxd means ﬁnding
71

Chapter 4
LINEAR PROGRAMMING
a point (x1,...,xd) that is extreme in the direction⃗c = (c1,...,cd). Hence, the
solution to the linear program is a point in the feasible region that is extreme
in direction ⃗c. We let f⃗c denote the objective function deﬁned by a direction
vector⃗c.
Many problems in operations research can be described by linear programs,
and a lot of work has been dedicated to linear optimization. This has resulted in
many different linear programming algorithms, several of which—the famous
simplex algorithm for instance—perform well in practice.
feasible region
⃗c
solution
Let's go back to our problem. We have n linear constraints in two variables
and we want to ﬁnd one solution to the set of constraints. We can do this
by taking an arbitrary objective function, and then solving the linear program
deﬁned by the objective function and the linear constraints. For the latter step
we can use the simplex algorithm, or any other linear programming algorithm
developed in operations research. However, this particular linear program is
quite different from the ones usually studied: in operations research both the
number of constraints and the number of variables are large, but in our case the
number of variables is only two. The traditional linear programming methods
are not very efﬁcient in such low-dimensional linear programming problems;
methods developed in computational geometry, like the one described below,
do better.
We denote the set of n linear constraints in our 2-dimensional linear program-
ming problem by H. The vector deﬁning the objective function is⃗c = (cx,cy);
thus the objective function is f⃗c(p) = cxpx +cypy. Our goal is to ﬁnd a point
p ∈R2 such that p ∈!H and f⃗c(p) is maximized. We denote the linear program
by (H,⃗c), and we use C to denote its feasible region. We can distinguish four
cases for the solution of a linear program (H,⃗c). The four cases are illustrated
in Figure 4.5; the vector deﬁning the objective function is vertically downward
in the examples.
Figure 4.5
Different types of solutions to a linear
program.
(i)
(ii)
(iii)
(iv)
v
e
ρ
(i)
The linear program is infeasible, that is, there is no solution to the set of
constraints.
(ii) The feasible region is unbounded in direction⃗c. In this case there is a ray
ρ completely contained in the feasible region C, such that the function f⃗c
takes arbitrarily large values along ρ. The solution we require in this case
is the description of such a ray.
(iii) The feasible region has an edge e whose outward normal points in the
direction⃗c. In this case, there is a solution to the linear program, but it is
not unique: any point on e is a feasible point that maximizes f⃗c(p).
(iv) If none of the preceding three cases applies, then there is a unique solution,
which is the vertex v of C that is extreme in the direction⃗c.
72

Section 4.3
INCREMENTAL LINEAR
PROGRAMMING
Our algorithm for 2-dimensional linear programming is incremental. It adds the
constraints one by one, and maintains the optimal solution to the intermediate
linear programs. It requires, however, that the solution to each intermediate
problem is well-deﬁned and unique. In other words, it assumes that each
intermediate feasible region has a unique optimal vertex as in case (iv) above.
To fulﬁll this requirement, we add to our linear program two additional
constraints that will guarantee that the linear program is bounded. For example,
if cx > 0 and cy > 0 we add the contraints px ⩽M and py ⩽M, for some
large M ∈R. The idea is that M should be chosen so large that the additional
constraints do not inﬂuence the optimal solution, if the original linear program
was bounded.
In many practical applications of linear programming, a bound of this form
is actually a natural restriction. In our application to the casting problem, for
instance, mechanical limitations will not allow us to remove the polyhedron in
a direction that is nearly horizontal. For instance, we may not be able to remove
the polyhedron in a direction whose angle with the xy-plane is less than 1 degree.
This constraint immediately gives a bound on the absolute value of px, py.
We will discuss in Section 4.5 how we can correctly recognize unbounded
linear programs, and how we can solve bounded ones without enforcing artiﬁcial
constraints on the solution.
For preciseness, let's give a name to the two new constraints:
m1 :=
#
px ⩽M
if cx > 0
−px ⩽M
otherwise
and
m2 :=
#
py ⩽M
if cy > 0
−py ⩽M
otherwise
Note that m1,m2 are chosen as a function of⃗c only, they do not depend on the
half-planes H. The feasible region C0 = m1 ∩m2 is an orthogonal wedge.
solution
Another simple convention now allows us to say that case (iii) also has a
unique solution: if there are several optimal points, then we want the lexico-
graphically smallest one. Conceptually, this convention is equivalent to rotating
⃗c a little, such that it is no longer normal to any half-plane.
We have to be careful when doing this, as even a bounded linear program
may not have a lexicographically smallest solution (see Exercise 4.11). Our
choice of the two constraints m1 and m2 is such that this cannot happen.
With these two conventions, any linear program that is feasible has a unique
solution, which is a vertex of the feasible region. We call this vertex the optimal
vertex.
Let (H,⃗c) be a linear program. We number the half-planes h1, h2, ..., hn. Let
Hi be the set of the ﬁrst i constraints, together with the special constraints m1
and m2, and let Ci be the feasible region deﬁned by these constraints:
Hi :=
{m1,m2,h1,h2 ...,hi},
Ci :=
m1 ∩m2 ∩h1 ∩h2 ∩···∩hi.
73

Chapter 4
LINEAR PROGRAMMING
By our choice of C0, each feasible region Ci has a unique optimal vertex, denoted
vi. Clearly, we have
C0 ⊇C1 ⊇C2 ··· ⊇Cn = C.
This implies that if Ci = /0 for some i, then Cj = /0 for all j ⩾i, and the linear
program is infeasible. So our algorithm can stop once the linear program
becomes infeasible.
The next lemma investigates how the optimal vertex changes when we add a
half-plane hi. It is the basis of our algorithm.
Lemma 4.5 Let 1 ⩽i ⩽n, and let Ci and vi be deﬁned as above. Then we have
(i)
If vi−1 ∈hi, then vi = vi−1.
(ii) If vi−1 ̸∈hi, then either Ci = /0 or vi ∈ℓi, where ℓi is the line bounding hi.
Proof. (i) Let vi−1 ∈hi. Because Ci = Ci−1 ∩hi and vi−1 ∈Ci−1 this means that
vi−1 ∈Ci. Furthermore, the optimal point in Ci cannot be better than the
optimal point in Ci−1, since Ci ⊆Ci−1. Hence, vi−1 is the optimal vertex in
Ci as well.
(ii) Let vi−1 ̸∈hi. Suppose for a contradiction that Ci is not empty and that vi
does not lie on ℓi. Consider the line segment vi−1vi. We have vi−1 ∈Ci−1
and, since Ci ⊂Ci−1, also vi ∈Ci−1. Together with the convexity of Ci−1,
this implies that the segment vi−1vi is contained in Ci−1. Since vi−1 is the
optimal point in Ci−1 and the objective function f⃗c is linear, it follows that
f⃗c(p) increases monotonically along vi−1vi as p moves from vi to vi−1. Now
consider the intersection point q of vi−1vi and ℓi. This intersection point
exists, because vi−1 ̸∈hi and vi ∈Ci. Since vi−1vi is contained in Ci−1, the
point q must be in Ci. But the value of the objective function increases along
vi−1vi, so f⃗c(q) > f⃗c(vi). This contradicts the deﬁnition of vi.
vi
vi−1
q
Ci−1
Figure 4.6 illustrates the two cases that arise when adding a half-plane.
In Figure 4.6(i), the optimal vertex v4 that we have after adding the ﬁrst four
half-planes is contained in h5, the next half-plane that we add. Therefore the
optimal vertex remains the same. The optimal vertex is not contained in h6,
however, so when we add h6 we must ﬁnd a new optimal vertex. According
Figure 4.6
Adding a half-plane
(i)
(ii)
⃗c
v4 = v5
h1
h2
h3
h4
h5
v6
v5
h6
h5
h3
h4
h2
h1
to Lemma 4.5, this vertex v6 is contained in the line bounding h6, as is shown
in Figure 4.6(ii). But Lemma 4.5 does not tell us how to ﬁnd the new optimal
vertex. Fortunately, this is not so difﬁcult, as we show next.
74

Section 4.3
INCREMENTAL LINEAR
PROGRAMMING
Assume that the current optimal vertex vi−1 is not contained in the next half-
plane hi. The problem we have to solve can be stated as follows:
Find the point p on ℓi that maximizes f⃗c(p), subject to the con-
straints p ∈h, for h ∈Hi−1.
To simplify the terminology, we assume that ℓi is not vertical, and so we can
parameterize it by x-coordinate. We can then deﬁne a function f⃗c : R /→R
such that f⃗c(p) = f⃗c(px) for points p ∈ℓi. For a half-plane h, let σ(h,ℓi) be the
x-coordinate of the intersection point of ℓi and the bounding line of h. (If there
is no intersection, then either the constraint h is satisﬁed by any point on ℓi, or
by no point on ℓi. In the former case we can ignore the constraint, in the latter
case we can report the linear program infeasible.) Depending on whether ℓi ∩h
is bounded to the left or to the right, we get a constraint on the x-coordinate of
the solution of the form x ⩾σ(h,ℓi) or of the form x ⩽σ(h,ℓi). We can thus
restate our problem as follows:
Maximize
f⃗c(x)
subject to
x ⩾σ(h,ℓi),
h ∈Hi−1 and ℓi ∩h is bounded to the left
x ⩽σ(h,ℓi),
h ∈Hi−1 and ℓi ∩h is bounded to the right
This is a 1-dimensional linear program. Solving it is very easy. Let
xleft = max
h∈Hi−1
{σ(h,ℓi) : ℓi ∩h is bounded to the left}
and
ℓi
h
x ≤σ(h,ℓi)
σ(h,ℓi)
xright = min
h∈Hi−1
{σ(h,ℓi) : ℓi ∩h is bounded to the right}.
The interval [xleft : xright] is the feasible region of the 1-dimensional linear
program. Hence, the linear program is infeasible if xleft > xright, and otherwise
the optimal point is the point on ℓi at either xleft or xright, depending on the
objective function.
Note that the 1-dimensional linear program cannot be unbounded, due to
the constraints m1 and m2.
We get the following lemma:
Lemma 4.6 A 1-dimensional linear program can be solved in linear time. Hence,
if case (ii) of Lemma 4.5 arises, then we can compute the new optimal vertex vi,
or decide that the linear program is infeasible, in O(i) time.
We can now describe the linear programming algorithm in more detail. As
above, we use ℓi to denote the line that bounds the half-plane hi.
Algorithm 2DBOUNDEDLP(H,⃗c,m1,m2)
Input. A linear program (H ∪{m1,m2},⃗c), where H is a set of n half-planes,
⃗c ∈R2, and m1, m2 bound the solution.
Output. If (H ∪{m1,m2},⃗c) is infeasible, then this fact is reported. Otherwise,
the lexicographically smallest point p that maximizes f⃗c(p) is reported.
75

Chapter 4
LINEAR PROGRAMMING
1.
Let v0 be the corner of C0.
2.
Let h1,...,hn be the half-planes of H.
3.
for i ←1 to n
4.
do if vi−1 ∈hi
5.
then vi ←vi−1
6.
else vi ←the point p on ℓi that maximizes f⃗c(p), subject to the
constraints in Hi−1.
7.
if p does not exist
8.
then Report that the linear program is infeasible and quit.
9.
return vn
We now analyze the performance of our algorithm.
Lemma 4.7 Algorithm 2DBOUNDEDLP computes the solution to a bounded
linear program with n constraints and two variables in O(n2) time and linear
storage.
Proof. To prove that the algorithm correctly ﬁnds the solution, we have to show
that after every stage—whenever we have added a new half-plane hi—the point
vi is still the optimum point for Ci. This follows immediately from Lemma 4.5.
If the 1-dimensional linear program on ℓi is infeasible, then Ci is empty, and
consequently C = Cn ⊆Ci is empty, which means that the linear program is
infeasible.
It is easy to see that the algorithm requires only linear storage. We add the
half-planes one by one in n stages. The time spent in stage i is dominated by the
time to solve a 1-dimensional linear program in line 6, which is O(i). Hence,
the total time needed is bounded by
n
∑
i=1
O(i) = O(n2).
Although our linear programming algorithm is nice and simple, its running
time is disappointing—the algorithm is much slower than the previous algorithm,
which computed the whole feasible region. Is our analysis too crude? We
bounded the cost of every stage i by O(i). This is not always a tight bound:
Stage i takes Θ(i) time only when vi−1 ̸∈hi; when vi−1 ∈hi then stage i takes
constant time. So if we could bound the number of times the optimal vertex
changes, we might be able to prove a better running time. Unfortunately the
h1
h2
h3
h4
h5
hn
v2
vn
v5
v4
v3
⃗c
optimum vertex can change n times: there are orders for some conﬁgurations
where every new half-plane makes the previous optimum illegal. The ﬁgure in
the margin shows such an example. This means that the algorithm will really
spend Θ(n2) time. How can we avoid this nasty situation?
4.4
Randomized Linear Programming
If we have a second look at the example where the optimum changes n times,
we see that the problem is not so much that the set of half-planes is bad. If we
76

Section 4.4
RANDOMIZED LINEAR
PROGRAMMING
had added them in the order hn, hn−1, . . . , h3, then the optimal vertex would not
change anymore after the addition of hn. In this case the running time would be
O(n). Is this a general phenomenon? Is it true that, for any set H of half-planes,
there is a good order to treat them? The answer to this question is "yes," but
that doesn't seem to help us much. Even if such a good order exists, there
seems to be no easy way to actually ﬁnd it. Remember that we have to ﬁnd the
order at the beginning of the algorithm, when we don't know anything about
the intersection of the half-planes yet.
We now meet a quite intriguing phenomenon. Although we have no way to
determine an ordering of H that is guaranteed to lead to a good running time,
we have a very simple way out of our problem. We simply pick a random
ordering of H. Of course, we could have bad luck and pick an order that leads
to a quadratic running time. But with some luck, we pick an order that makes it
run much faster. Indeed, we shall prove below that most orders lead to a fast
algorithm. For completeness, we ﬁrst repeat the algorithm.
Algorithm 2DRANDOMIZEDBOUNDEDLP(H,⃗c,m1,m2)
Input. A linear program (H ∪{m1,m2},⃗c), where H is a set of n half-planes,
⃗c ∈R2, and m1, m2 bound the solution.
Output. If (H ∪{m1,m2},⃗c) is infeasible, then this fact is reported. Otherwise,
the lexicographically smallest point p that maximizes f⃗c(p) is reported.
1.
Let v0 be the corner of C0.
2.
Compute a random permutation h1,...,hn of the half-planes by calling
RANDOMPERMUTATION(H[1···n]).
3.
for i ←1 to n
4.
do if vi−1 ∈hi
5.
then vi ←vi−1
6.
else vi ←the point p on ℓi that maximizes f⃗c(p), subject to the
constraints in Hi−1.
7.
if p does not exist
8.
then Report that the linear program is infeasible and quit.
9.
return vn
The only difference from the previous algorithm is in line 2, where we put the
half-planes in random order before we start adding them one by one. To be able
to do this, we assume that we have a random number generator, RANDOM(k),
which has an integer k as input and generates a random integer between 1 and k
in constant time. Computing a random permutation can then be done with the
following linear time algorithm.
Algorithm RANDOMPERMUTATION(A)
Input. An array A[1···n].
Output. The array A[1···n] with the same elements, but rearranged into a
random permutation.
1.
for k ←n downto 2
2.
do rndindex ←RANDOM(k)
3.
Exchange A[k] and A[rndindex].
77

Chapter 4
LINEAR PROGRAMMING
The new linear programming algorithm is called a randomized algorithm; its
running time depends on certain random choices made by the algorithm. (In
the linear programming algorithm, these random choices were made in the
subroutine RANDOMPERMUTATION.)
What is the running time of this randomized version of our incremental linear
programming algorithm? There is no easy answer to that. It all depends on
the order that is computed in line 2. Consider a ﬁxed set H of n half-planes.
2DRANDOMIZEDBOUNDEDLP treats them depending on the permutation cho-
sen in line 2. Since there are n! possible permutations of n objects, there are
n! possible ways in which the algorithm can proceed, each with its own run-
ning time. Because the permutation is random, each of these running times
is equally likely. So what we do is analyze the expected running time of the
algorithm, which is the average running time over all n! possible permutations.
The lemma below states that the expected running time of our randomized linear
programming algorithm is O(n). It is important to realize that we do not make
any assumptions about the input: the expectancy is with respect to the random
order in which the half-planes are treated and holds for any set of half-planes.
Lemma 4.8 The 2-dimensional linear programming problem with n constraints
can be solved in O(n) randomized expected time using worst-case linear storage.
Proof. As we observed before, the storage needed by the algorithm is linear.
The running time RANDOMPERMUTATION is O(n), so what remains is to
analyze the time needed to add the half-planes h1,...,hn. Adding a half-plane
takes constant time when the optimal vertex does not change. When the optimal
vertex does change we need to solve a 1-dimensional linear program. We now
bound the time needed for all these 1-dimensional linear programs.
Let Xi be a random variable, which is 1 if vi−1 ̸∈hi, and 0 otherwise. Recall
that a 1-dimensional linear program on i constraints can be solved in O(i) time.
The total time spent in line 6 over all half-planes h1,...,hn is therefore
n
∑
i=1
O(i)·Xi.
To bound the expected value of this sum we will use linearity of expectation: the
expected value of a sum of random variables is the sum of the expected values
of the random variables. This holds even if the random variables are dependent.
Hence, the expected time for solving all 1-dimensional linear programs is
E[
n
∑
i=1
O(i)·Xi] =
n
∑
i=1
O(i)·E[Xi].
But what is E[Xi]? It is exactly the probability that vi−1 ̸∈hi. Let's analyze this
probability.
We will do this with a technique called backwards analysis: we look at the
algorithm "backwards." Assume that it has already ﬁnished, and that it has
computed the optimum vertex vn. Since vn is a vertex of Cn, it is deﬁned by at
⃗c
vn
half-planes
deﬁning vn
78

Section 4.5
UNBOUNDED LINEAR PROGRAMS
least two of the half-planes. Now we make one step backwards in time, and
look at Cn−1. Note that Cn−1 is obtained from Cn by removing the half-plane hn.
When does the optimum point change? This happens exactly if vn is not a vertex
of Cn−1 that is extreme in the direction⃗c, which is only possible if hn is one of
the half-planes that deﬁne vn. But the half-planes are added in random order, so
hn is a random element of {h1,h2,...,hn}. Hence, the probability that hn is one
of the half-planes deﬁning vn is at most 2/n. Why do we say "at most"? First, it
is possible that the boundaries of more than two half-planes pass through vn. In
that case, removing one of the two half-planes containing the edges incident to
vn may fail to change vn. Furthermore, vn may be deﬁned by m1 or m2, which
are not among the n candidates for the random choice of hn. In both cases the
probability is less than 2/n.
⃗c
vn
The same argument works in general: to bound E[Xi], we ﬁx the subset
of the ﬁrst i half-planes. This determines Ci. To analyze what happened in
the last step, when we added hi, we think backwards. The probability that
we had to compute a new optimal vertex when adding hi is the same as the
probability that the optimal vertex changes when we remove a half-plane from
Ci. The latter event only takes place for at most two half-planes of our ﬁxed set
{h1,...,hi}. Since the half-planes are added in random order, the probability
that hi is one of the special half-planes is at most 2/i. We derived this probability
under the condition that the ﬁrst i half-planes are some ﬁxed subset of H. But
since the derived bound holds for any ﬁxed subset, it holds unconditionally.
Hence, E[Xi] ⩽2/i. We can now bound the expected total time for solving all
1-dimensional linear programs by
n
∑
i=1
O(i)· 2
i = O(n).
We already noted that the time spent in the rest of the algorithm is O(n) as
well.
Note again that the expectancy here is solely with respect to the random
choices made by the algorithm. We do not average over possible choices for
the input. For any input set of n half-planes, the expected running time of the
algorithm is O(n); there are no bad inputs.
4.5
Unbounded Linear Programs
In the preceding sections we avoided handling the case of an unbounded linear
program by adding two additional, artiﬁcial constraints. This is not always a
suitable solution. Even if the linear program is bounded, we may not know a
large enough bound M. Furthermore, unbounded linear programs do occur in
practice, and we have to solve them correctly.
Let's ﬁrst see how we can recognize whether a given linear program (H,⃗c)
is unbounded. As we saw before, that means that there is a ray ρ completely
79

Chapter 4
LINEAR PROGRAMMING
contained in the feasible region C, such that the function f⃗c takes arbitrarily
large values along ρ.
If we denote the ray's starting point as p, and its direction vector as ⃗d, we
can parameterize ρ as follows:
ρ = {p+λ ⃗d : λ > 0}.
The function f⃗c takes arbitrarily large values if and only if ⃗d ·⃗c > 0. On the
other hand, if ⃗η(h) is the normal vector of a half-plane h ∈H oriented towards
the feasible side of h's bounding line, we have ⃗d ·⃗η(h) ⩾0. The next lemma
shows that these two necessary conditions on ⃗d are sufﬁcient to test whether a
linear program is unbounded.
Lemma 4.9 A linear program (H,⃗c) is unbounded if and only if there is a vector
⃗d with ⃗d ·⃗c > 0 such that ⃗d ·⃗η(h) ⩾0 for all h ∈H and the linear program (H′,⃗c)
is feasible, where H′ = {h ∈H :⃗η(h)· ⃗d = 0} .
Proof. The "only if" direction follows from the argument above, so it remains
to show the "if" direction.
We consider a linear program (H,⃗c) and a vector ⃗d with the conditions of
the lemma. Since (H′,⃗c) is feasible, there is a point p0 ∈!
h∈H′ h. Consider now
the ray ρ0 := {p0 + λ ⃗d : λ > 0}. Since ⃗d ·⃗η(h) = 0 for h ∈H′, the ray ρ0 is
completely contained in each h ∈H′. Furthermore, since ⃗d ·⃗c > 0 the objective
function f⃗c takes arbitrarily large values along ρ0.
For a half-plane h ∈H \H′, we have ⃗d ·⃗η(h) > 0. This implies that there is
a parameter λh such that p0 +λ ⃗d ∈h for all λ ⩾λh. Let λ ′ := maxh∈H\H′ λh,
and p := p0 +λ ′⃗d. It follows that the ray
ρ = {p+λ ⃗d : λ > 0}
is completely contained in each half-plane h ∈H, and so (H,⃗c) is unbounded.
We can now test whether a given 2-dimensional linear program (H,⃗c) is
unbounded by proceeding similarly to Section 4.1, and solving a 1-dimensional
linear program.
Let's ﬁrst rotate the coordinate system so that ⃗c is the upward vertical
direction, ⃗c = (0,1). Any direction vector ⃗d = (dx,dy) with ⃗d ·⃗c > 0 can be
normalized to the form ⃗d = (dx,1), and be represented by the point dx on the
line y = 1. Given a normal vector ⃗η(h) = (ηx,ηy), the inequality
⃗d ·⃗η(h) = dxηx +ηy ⩾0
translates to the inequality dxηx ⩾−ηy. We thus obtain a system of n linear
inequalities, or, in other words, a 1-dimensional linear program H. (This
is actually an abuse of the terminology, since a linear program consists of
constraints and an objective function. But since at this point we are only
interested in feasibility, it is convenient to ignore the objective function.)
80

Section 4.5
UNBOUNDED LINEAR PROGRAMS
If H has a feasible solution d∗
x, we identify the set H′ ⊆H of half-planes h
for which the solution is tight, that is, where d∗
xηx +ηy = 0. We still need to
verify that the system H′ is feasible. Are we again left with a 2-dimensional
linear programming problem? Yes, but a very special one: For each h ∈H′
the normal ⃗η(h) is orthogonal to ⃗d = (d∗
x,1), and that means that the bounding
line of h is parallel to ⃗d. In other words, all half-planes in H′ are bounded
by parallel lines, and by intersecting them with the x-axis, we have again a
1-dimensional linear program H′. If H′ is feasible, then the original linear
program is unbounded, and we can construct a feasible ray ρ in time O(n) as in
the lemma above. If H′ is infeasible, then so is H′ and therefore H.
If H does not have a feasible solution, by the lemma above the original
linear program (H,⃗c) is bounded. Can we extract some more information in this
case? Recall the solution for 1-dimensional linear programs: H is infeasible if
and only if the maximum boundary of a half-line h1 bounded to the left is larger
than the minimum boundary of a half-line h2 bounded to the right. These two
half-lines h1 and h2 have an empty intersection. If h1 and h2 are the original
half-planes that correspond to these two constraints, then this is equivalent to
saying that ({h1,h2},⃗c) is bounded. We can call h1 and h2 certiﬁcates: they
'prove' that (H,⃗c) is really bounded.
How useful certiﬁcates are becomes clear with the following observation:
After ﬁnding the two certiﬁcates h1 and h2, we can use them like m1 and m2 in
2DRANDOMIZEDBOUNDEDLP. That means that we no longer need to make an
artiﬁcial restriction on the range in which we allow the solution to lie.
Again, we must be careful. It can happen that the linear program ({h1,h2},⃗c)
is bounded, but has no lexicographically smallest solution. This is the case
when the 1-dimensional linear program is infeasible due to a single constraint
h1, namely when ⃗η(h1) = −⃗c = (0,−1). In that case we scan the remaining
list of half-planes for a half-plane h2 with ηx(h2) > 0. If we are successful,
h1 and h2 are certiﬁcates that guarantee a unique lexicographically smallest
solution. If no such h2 exists, the linear program is either infeasible, or it
has no lexicographically smallest solution. We can solve it by solving the
1-dimensional linear program formed by all half-planes h with ⃗ηx(h) = 0. If it
is feasible, we can return a ray ρ in direction (−1,0), such that all points on ρ
are feasible optimal solutions.
We can now give a general algorithm for the 2-dimensional linear program-
ming problem:
Algorithm 2DRANDOMIZEDLP(H,⃗c)
Input. A linear program (H,⃗c), where H is a set of n half-planes and⃗c ∈R2.
Output. If (H,⃗c) is unbounded, a ray is reported. If it is infeasible, then two or
three certiﬁcate half-planes are reported. Otherwise, the lexicographically
smallest point p that maximizes f⃗c(p) is reported.
1.
Determine whether there is a direction vector ⃗d such that ⃗d ·⃗c > 0 and
⃗d ·⃗η(h) ⩾0 for all h ∈H.
2.
if ⃗d exists
3.
then compute H′ and determine whether H′ is feasible.
4.
if H′ is feasible
81

Chapter 4
LINEAR PROGRAMMING
5.
then Report a ray proving that (H,⃗c) is unbounded and quit.
6.
else Report that (H,⃗c) is infeasible and quit.
7.
Let h1,h2 ∈H be certiﬁcates proving that (H,⃗c) is bounded and has a
unique lexicographically smallest solution.
8.
Let v2 be the intersection of ℓ1 and ℓ2.
9.
Let h3,h4,...,hn be a random permutation of the remaining half-planes in
H.
10. for i ←3 to n
11.
do if vi−1 ∈hi
12.
then vi ←vi−1
13.
else vi ←the point p on ℓi that maximizes f⃗c(p), subject to the
constraints in Hi−1.
14.
if p does not exist
15.
then Let hj,hk (with j,k < i) be the certiﬁcates (possibly
hj = hk) with hj ∩hk ∩ℓi = /0.
16.
Report that the linear program is infeasible, with
hi,h j,hk as certiﬁcates, and quit.
17. return vn
We summarize our results so far in the following theorem.
Theorem 4.10 A 2-dimensional linear programming problem with n constraints
can be solved in O(n) randomized expected time using worst-case linear storage.
4.6*
Linear Programming in Higher Dimensions
The linear programming algorithm presented in the previous sections can be
generalized to higher dimensions. When the dimension is not too high, then the
resulting algorithm compares favorably with traditional algorithms, such as the
simplex algorithm.
Let H be a set of n closed half-spaces in Rd. Given a vector⃗c = (c1,...,cd), we
want to ﬁnd the point p = (p1,..., pd) ∈Rd that maximizes the linear function
f⃗c(p) := c1p1 +···+cd pd, subject to the constraint that p lies in h for all h ∈H.
To make sure that the solution is unique when the linear program is bounded,
we agree to look for the lexicographically smallest point that maximizes f⃗c(p).
As in the planar version, we maintain the optimal solution while incremen-
tally adding the half-space constraints one by one. For this to work, we again
need to make sure that there is a unique optimal solution at each step. We do
this as in the previous section: We ﬁrst determine whether the linear program is
unbounded. If not, we obtain a set of d certiﬁcates h1,h2,...,hd ∈H that guar-
antee that the solution is bounded and that there is a unique lexicographically
smallest solution. We'll look at the details of ﬁnding these certiﬁcates later, and
concentrate on the main algorithm for the moment.
Let h1,h2,...,hd be the d certiﬁcate half-spaces obtained by checking that
the linear program is bounded, and let hd+1, hd+2, . . . , hn be a random permuta-
tion of the remaining half-spaces in H. Furthermore, deﬁne Ci to be the feasible
82

Section 4.6*
LINEAR PROGRAMMING IN HIGHER
DIMENSIONS
region when the ﬁrst i half-spaces have been added, for d ⩽i ⩽n:
Ci := h1 ∩h2 ∩···∩hi.
Let vi denote the optimal vertex of Ci, that is, the vertex that maximizes f⃗c.
Lemma 4.5 gave us an easy way to maintain the optimal vertex in the 2-
dimensional case: either the optimal vertex doesn't change, or the new optimal
vertex is contained in the line that bounds the half-plane hi that we are adding.
The following lemma generalizes this result to higher dimensions; its proof is a
straightforward generalization of the proof of Lemma 4.5.
Lemma 4.11 Let 1 ⩽i ⩽n, and let Ci and vi be deﬁned as above. Then we have
(i)
If vi−1 ∈hi, then vi = vi−1.
(ii) If vi−1 ̸∈hi, then either Ci = /0 or vi ∈gi, where gi is the hyperplane that
bounds hi.
If we denote the hyperplane that bounds the half-space hi by gi, the optimal
vertex vi of Ci can be found by ﬁnding the optimal vertex of the intersection
gi ∩Ci−1.
But how do we ﬁnd the optimal vertex of gi ∩Ci−1? In two dimensions this
was easy to do in linear time, because everything was restricted to a line. Let's
look at the 3-dimensional case. In three dimensions, gi is a plane, and gi ∩Ci−1
is a 2-dimensional convex polygonal region. What do we have to do to ﬁnd the
optimum in gi ∩Ci−1? We have to solve a 2-dimensional linear program! The
linear function f⃗c deﬁned in R3 induces a linear function in gi, and we need to
ﬁnd the point in gi ∩Ci−1 that maximizes this function. In case⃗c is orthogonal to
gi, all points on gi are equally good: following our rule, we then need to ﬁnd the
lexicographically smallest solution. We achieve this by choosing the objective
function correctly—for instance, when gi is not orthogonal to the x1-axis, we
obtain the vector⃗c by projecting the vector (−1,0,0) onto gi.
So in the 3-dimensional case we ﬁnd the optimal vertex of gi ∩Ci−1 as
follows: we compute the intersection of all i−1 half-spaces with gi, and project
the vectors
⃗c,


−1
0
0

,


0
−1
0

,


0
0
−1


on gi until a projection is non-zero. This results in a linear program in two
dimensions, which we solve using algorithm 2DRANDOMIZEDLP.
By now you can probably guess how we will attack the general, d-dimensional
case. There, gi is a hyperplane, a (d −1)-dimensional subspace, and we have
to ﬁnd the point in the intersection Ci−1 ∩gi that maximizes f⃗c. This is a linear
program in d −1 dimensions, and so we will solve it by making a recursive call
to the (d −1)-dimensional version of our algorithm. The recursion bottoms out
when we get to a 1-dimensional linear program, which can be solved directly in
linear time.
We still need to determine whether the linear program is unbounded, and to
ﬁnd suitable certiﬁcates if that is not the case. We ﬁrst verify that Lemma 4.9
83

Chapter 4
LINEAR PROGRAMMING
holds in arbitrary dimensions. The lemma and its proof need no change. The
lemma implies that the d-dimensional linear program (H,⃗c) is bounded if and
only if a certain (d −1)-dimensional linear program is infeasible. We will solve
this (d −1)-dimensional linear program by a recursive call.
If the (d −1)-dimensional linear program is feasible, we obtain a direction
vector ⃗d. The d-dimensional linear program is then either unbounded in di-
rection ⃗d, or infeasible. This can be determined by verifying whether (H′,⃗c)
is feasible, where H′ is as deﬁned in Lemma 4.9. The boundaries of all the
half-spaces in H′ are parallel to ⃗d, and so this can be decided by solving a
second (d −1)-dimensional program, with a second recursive call.
If the (d −1)-dimensional linear program is infeasible, its solution will give
us k certiﬁcate half-spaces h1,h2,...,hk ∈H, with k < d, that 'prove' that (H,⃗c)
is bounded. If k < d, then the set of optimal solutions to ({h1,...,hk},⃗c) is
unbounded. In that case, these optimal solutions form a (d −k)-dimensional
subspace. We determine whether the linear program restricted to this subspace
is bounded with respect to the lexicographical order. If not, we can report
the solution, otherwise we can repeat the process until we obtain a set of
d certiﬁcates with a unique solution.
The global algorithm now looks as follows. Again we use gi to denote the
hyperplane that bounds the half-space hi.
Algorithm RANDOMIZEDLP(H,⃗c)
Input. A linear program (H,⃗c), where H is a set of n half-spaces in Rd and
⃗c ∈Rd.
Output. If (H,⃗c) is unbounded, a ray is reported. If it is infeasible, then at most
d +1 certiﬁcate half-planes are reported. Otherwise, the lexicographically
smallest point p that maximizes f⃗c(p) is reported.
1.
Determine whether a direction vector ⃗d exists such that ⃗d ·⃗c > 0 and
⃗d ·⃗η(h) ⩾0 for all h ∈H.
2.
if ⃗d exists
3.
then compute H′ and determine whether H′ is feasible.
4.
if H′ is feasible
5.
then Report a ray proving that (H,⃗c) is unbounded and quit.
6.
else Report that (H,⃗c) is infeasible, provide certiﬁcates, and
quit.
7.
Let h1,h2,...,hd be certiﬁcates proving that (H,⃗c) is bounded.
8.
Let vd be the intersection of g1,g2,...,gd.
9.
Compute a random permutation hd+1,...,hn of the remaining half-spaces
in H.
10. for i ←d +1 to n
11.
do if vi−1 ∈hi
12.
then vi ←vi−1
13.
else vi ←the point p on gi that maximizes f⃗c(p), subject to the
constraints {h1,...,hi−1}
14.
if p does not exist
15.
then Let H∗be the at most d certiﬁcates for the infeasi-
bility of the (d −1)-dimensional program.
84

Section 4.6*
LINEAR PROGRAMMING IN HIGHER
DIMENSIONS
16.
Report that the linear program is infeasible, with
H∗∪hi as certiﬁcates, and quit.
17. return vn
The following theorem states the performance of RANDOMIZEDLP. Although
we consider d a constant, which means we can state an O(n) bound on the
running time, it is useful to have a close look at the dependency of the running
time on d—see the end of the proof the following theorem.
Theorem 4.12 For each ﬁxed dimension d, a d-dimensional linear programming
problem with n constraints can be solved in O(n) expected time.
Proof. We must prove that there is a constant Cd such that the algorithm takes
at most Cdn expected time. We proceed by induction on the dimension d. For
two dimensions, the result follows from Theorem 4.10, so let's assume d > 2.
The induction step is basically identical to the proof of the 2-dimensional cases.
We start by solving at most d linear programs of dimension d −1. By the
induction assumption, this takes time O(dn)+dCd−1n.
The algorithm spends O(d) time to compute vd. Testing whether vi−1 ∈hi
takes O(d) time. The running time is therefore O(dn) as long as we do not
count the time spent in line 13.
In line 13, we need to project ⃗c on gi, in time O(d), and to intersect i
half-spaces with gi, in time O(di). Furthermore, we make a recursive call with
dimension d −1 and i−1 half-spaces.
Deﬁne a random variable Xi, which is 1 if vi−1 ̸∈hi, and 0 otherwise. The
total expected time spent by the algorithm is bounded by
O(dn)+dCd−1n+
n
∑
i=d+1
(O(di)+Cd−1(i−1))·E[Xi].
To bound E[Xi], we apply backwards analysis. Consider the situation after
adding h1,...,hi. The optimum point is a vertex vi of Ci, so it is deﬁned by d of
the half-spaces. Now we make one step backwards in time. The optimum point
changes only if we remove one of the half-spaces deﬁning vi. Since hd+1,...,hi
is a random permutation, the probability that this happens is at most d/(i−d).
Consequently, we get the following bound for the expected running time of
the algorithm:
O(dn)+dCd−1n+
n
∑
i=d+1
(O(di)+Cd−1(i−1)) d
i−d
This can be bounded by Cdn, with Cd = O(Cd−1d), so Cd = O(cdd!) for a
constant c indpendent on the dimension.
When d is a constant, it is correct to say that the algorithm runs in linear
time. Still, that would be quite misleading. The constant factor Cd grows so fast
as a function of d that this algorithm is useful only for rather small dimensions.
85

Chapter 4
LINEAR PROGRAMMING
4.7*
Smallest Enclosing Discs
The simple randomized technique we used above turns out to be surprisingly
powerful. It can be applied not only to linear programming but to a variety of
other optimization problems as well. In this section we shall look at one such
problem.
Consider a robot arm whose base is ﬁxed to the work ﬂoor. The arm has to pick
up items at various points and place them at other points. What would be a good
position for the base of the arm? This would be somewhere "in the middle" of
the points it must be able to reach. More precisely, a good position is at the
center of the smallest disc that encloses all the points. This point minimizes the
maximum distance between the base of the arm and any point it has to reach.
We arrive at the following problem: given a set P of n points in the plane (the
points on the work ﬂoor that the arm must be able to reach), ﬁnd the smallest
enclosing disc for P, that is, the smallest disc that contains all the points of P.
This smallest enclosing disc is unique—see Lemma 4.14(i) below, which is a
generalization of this statement.
As in the previous sections, we will give a randomized incremental algorithm for
the problem: First we generate a random permutation p1,..., pn of the points in
P. Let Pi := {p1,..., pi}. We add the points one by one while we maintain Di,
the smallest enclosing disc of Pi.
In the case of linear programming, there was a nice fact that helped us to
maintain the optimal vertex: when the current optimal vertex is contained in the
next half-plane then it does not change, and otherwise the new optimal vertex
lies on the boundary of the half-plane. Is a similar statement true for smallest
enclosing discs? The answer is yes:
Lemma 4.13 Let 2 < i < n, and let Pi and Di be deﬁned as above. Then we
have
pi+1
pi
Di−1 = Di
Di+1
(i)
If pi ∈Di−1, then Di = Di−1.
(ii) If pi ̸∈Di−1, then pi lies on the boundary of Di.
We shall prove this lemma later, after we have seen how we can use it to
design a randomized incremental algorithm that is quite similar to the linear
programming algorithm.
Algorithm MINIDISC(P)
Input. A set P of n points in the plane.
Output. The smallest enclosing disc for P.
1.
Compute a random permutation p1,..., pn of P.
2.
Let D2 be the smallest enclosing disc for {p1, p2}.
3.
for i ←3 to n
4.
do if pi ∈Di−1
5.
then Di ←Di−1
6.
else Di ←MINIDISCWITHPOINT({p1,..., pi−1}, pi)
7.
return Dn
86

Section 4.7*
SMALLEST ENCLOSING DISCS
The critical step occurs when pi ̸∈Di−1. We need a subroutine that ﬁnds the
smallest disc enclosing Pi, using the knowledge that pi must lie on the boundary
of that disc. How do we implement this routine? Let q := pi. We use the same
framework once more: we add the points of Pi−1 in random order, and maintain
the smallest enclosing disc of Pi−1 ∪{q} under the extra constraint that it should
have q on its boundary. The addition of a point pj will be facilitated by the
following fact: when pj is contained in the currently smallest enclosing disc
then this disc remains the same, and otherwise it must have pj on its boundary.
So in the latter case, the disc has both q and pj and its boundary. We get the
following subroutine.
MINIDISCWITHPOINT(P,q)
Input. A set P of n points in the plane, and a point q such that there exists an
enclosing disc for P with q on its boundary.
Output. The smallest enclosing disc for P with q on its boundary.
1.
Compute a random permutation p1,..., pn of P.
2.
Let D1 be the smallest disc with q and p1 on its boundary.
3.
for j ←2 to n
4.
do if p j ∈D j−1
5.
then D j ←D j−1
6.
else D j ←MINIDISCWITH2POINTS({p1,..., pj−1}, p j,q)
7.
return Dn
How do we ﬁnd the smallest enclosing disc for a set under the restriction that
two given points q1 and q2 are on its boundary? We simply apply the same
approach one more time. Thus we add the points in random order and maintain
the optimal disc; when the point pk we add is inside the current disc we don't
have to do anything, and when pk is not inside the current disc it must be on
the boundary of the new disc. In the latter case we have three points on the disc
boundary: q1, q2, and pk. This means there is only one disc left: the unique
disc with q1, q2, and pk on its boundary. This following routine describes this
in more detail.
MINIDISCWITH2POINTS(P,q1,q2)
Input. A set P of n points in the plane, and two points q1 and q2 such that there
exists an enclosing disc for P with q1 and q2 on its boundary.
Output. The smallest enclosing disc for P with q1 and q2 on its boundary.
1.
Let D0 be the smallest disc with q1 and q2 on its boundary.
2.
for k ←1 to n
3.
do if pk ∈Dk−1
4.
then Dk ←Dk−1
5.
else Dk ←the disc with q1, q2, and pk on its boundary
6.
return Dn
This ﬁnally completes the algorithm for computing the smallest enclosing disc
of a set of points. Before we analyze it, we must validate its correctness by
proving some facts that we used in the algorithms. For instance, we used the
87

Chapter 4
LINEAR PROGRAMMING
fact that when we added a new point and this point was outside the current
optimal disc, then the new optimal disc must have this point on its boundary.
Lemma 4.14 Let P be a set of points in the plane, let R be a possibly empty set
of points with P∩R = /0, and let p ∈P. Then the following holds:
(i)
If there is a disc that encloses P and has all points of R on its boundary,
then the smallest such disc is unique. We denote it by md(P,R).
(ii) If p ∈md(P\{p},R), then md(P,R) = md(P\{p},R).
(iii) If p ̸∈md(P\{p},R), then md(P,R) = md(P\{p},R∪{p}).
Proof. (i) Assume that there are two distinct enclosing discs D0 and D1 with
centers x0 and x1, respectively, and with the same radius. Clearly, all points
of P must lie in the intersection D0 ∩D1. We deﬁne a continuous family
z
D0
D1
x(λ)
x0
x1
D(λ)
{D(λ) | 0 ⩽λ ⩽1} of discs as follows. Let z be an intersection point of
∂D0 and ∂D1, the boundaries of D0 and D1. The center of D(λ) is the
point x(λ) := (1−λ)x0 +λx1, and the radius of D(λ) is r(λ) := d(x(λ),z).
We have D0 ∩D1 ⊂D(λ) for all λ with 0 ⩽λ ⩽1 and, in particular, for
λ = 1/2. Hence, since both D0 and D1 enclose all points of P, so must
D(1/2). Moreover, ∂D(1/2) passes through the intersection points of ∂D0
and ∂D1. Because R ⊂∂D0 ∩∂D1, this implies that R ⊂∂D(1/2) . In other
words, D(1/2) is an enclosing disc for P with R on its boundary. But the
radius of D(1/2) is strictly less than the radii of D0 and D1. So whenever
there are two distinct enclosing discs of the same radius with R on their
boundary, then there is a smaller enclosing disc with R on its boundary.
Hence, the smallest enclosing disc md(P,R) is unique.
(ii) Let D := md(P \ {p},R). If p ∈D, then D contains P and has R on its
boundary. There cannot be any smaller disc containing P with R on its
boundary, because such a disc would also be a containing disc for P\{p}
with R on its boundary, contradicting the deﬁnition of D. It follows that
D = md(P,R).
(iii) Let D0 := md(P \ {p},R) and let D1 := md(P,R). Consider the family
D(λ) of discs deﬁned above. Note that D(0) = D0 and D(1) = D1, so the
family deﬁnes a continous deformation of D0 to D1. By assumption we
have p ̸∈D0. We also have p ∈D1, so by continuity there must be some
0 < λ ∗⩽1 such that p lies on the boundary of D(λ ∗). As in the proof of
(i), we have P ⊂D(λ ∗) and R ⊂∂D(λ ∗). Since the radius of any D(λ) with
0 < λ < 1 is strictly less than the radius of D1, and D1 is by deﬁnition the
smallest enclosing disc for P, we must have λ ∗= 1. In other words, D1 has
p on its boundary.
Lemma 4.14 implies that MINIDISC correctly computes the smallest enclos-
ing disc of a set of points. The analysis of the running time is given in the proof
of the following theorem.
Theorem 4.15 The smallest enclosing disc for a set of n points in the plane can
be computed in O(n) expected time using worst-case linear storage.
Proof. MINIDISCWITH2POINTS runs in O(n) time because every iteration of
the loop takes constant time, and it uses linear storage. MINIDISCWITHPOINT
88

Section 4.8
NOTES AND COMMENTS
and MINIDISC also need linear storage, so what remains is to analyze their
expected running time.
q
Di
points that together with
q deﬁne Di
The running time of MINIDISCWITHPOINT is O(n) as long as we don't
count the time spent in calls to MINIDISCWITH2POINTS. What is the prob-
ability of having to make such a call? Again we use backwards analysis to
bound this probability: Fix a subset {p1,..., pi}, and let Di be the smallest disc
enclosing {p1,..., pi} and having q on its boundary. Imagine that we remove
one of the points {p1,..., pi}. When does the smallest enclosing circle change?
That happens only when we remove one of the three points on the boundary.
One of the points on the boundary is q, so there are at most two points that cause
the smallest enclosing circle to shrink. The probability that pi is one of those
points is 2/i. (When there are more than three points on the boundary, then the
probability that the smallest enclosing circle changes can only get smaller.) So
we can bound the total expected running time of MINIDISCWITHPOINT by
O(n)+
n
∑
i=2
O(i)2
i = O(n).
Applying the same argument once more, we ﬁnd that the expected running time
of MINIDISC is O(n) as well.
Algorithm MINIDISC can be improved in various ways. First of all, it is not
necessary to use a fresh random permutation in every instance of subroutine
MINIDISCWITHPOINT. Instead, one can compute a permutation once, at
the start of MINIDISC, and pass the permutation to MINIDISCWITHPOINT.
Furthermore, instead of writing three different routines, one could write a single
algorithm MINIDISCWITHPOINTS(P,R) that computes md(P,R) as deﬁned in
Lemma 4.14.
4.8
Notes and Comments
In this chapter we have studied an algorithmic problem that arises when one
wants to manufacture an object using casting. Other manufacturing processes
lead to challenging algorithmic problems as well, and a number of such problems
have been studied within computational geometry over the past years—see for
example the book by Dutta et al. [152] or the surveys by Janardan and Woo [220]
and Bose and Toussaint [72].
The computation of the common intersection of half-planes is an old and
well-studied problem. As we will explain in Chapter 11, the problem is dual to
the computation of the convex hull of points in the plane. Both problems have a
long history in the ﬁeld, and Preparata and Shamos [323] already list a number
of solutions. More information on the computation of 2-dimensional convex
hulls can be found in the notes and comments of Chapter 1.
Computing the common intersection of half-spaces, which can be done
in O(nlogn) time in the plane and in 3-dimensional space, becomes a more
computationally demanding problem when the dimension increases. The reason
89

Chapter 4
LINEAR PROGRAMMING
is that the number of (lower-dimensional) faces of the convex polytope formed
as the common intersection can be as large as Θ(n⌊d/2⌋) [158]. So if the only
goal is to ﬁnd a feasible point, computing the common intersection explicitly
soon becomes an unattractive approach.
Linear programming is one of the basic problems in numerical analysis and
combinatorial optimization. It goes beyond the scope of this chapter to survey
this literature, and we restrict ourselves to mentioning the simplex algorithm
and its variants [139], and the polynomial-time solutions of Khachiyan [234]
and Karmarkar [227]. More information on linear programming can be found
in books by Chv´atal [129] and Schrijver [339].
Linear programming as a problem in computational geometry was ﬁrst
considered by Megiddo [273], who showed that the problem of testing whether
the intersection of half-spaces is empty is strictly simpler than the computa-
tion of the intersection. He gave the ﬁrst deterministic algorithm for linear
programming whose running time is of the form O(Cdn), where Cd is a factor
depending on the dimension only. His algorithm is linear in n for any ﬁxed
dimension. The factor Cd in his algorithm is 22d. This was later improved
to 3d2 [130, 153]. More recently, a number of simpler and more practical ran-
domized algorithms have been given [132, 346, 354]. There are a number of
randomized algorithms whose running time is subexponential, but still not poly-
nomial in the dimension [222, 267]. Finding a strongly polynomial algorithm,
that is of combinatorial polynomial complexity, for linear programming is one
of the major open problems in the area.
The simple randomized incremental algorithm for two and higher dimen-
sions given here is due to Seidel [346]. Unlike in our presentation, he deals
with unbounded linear programs by treating the parameter M symbolically. This
is probably more elegant and efﬁcient than the algorithm we present, which
was chosen to demonstrate the relationship between unbounded d-dimensional
linear programs and feasible (d −1)-dimensional ones. In Seidel's version, the
factor Cd can be shown to be O(d!).
The generalization to the computation of smallest enclosing discs is due to
Welzl [385], who also showed how to ﬁnd the smallest enclosing ball of a set
of points in higher dimensions, and the smallest enclosing ellipse or ellipsoid.
Sharir and Welzl further generalized the technique and introduced the notion
of LP-type problems, which can be solved efﬁciently with an algorithm similar
to the ones described here [189, 354]. Generally speaking, the technique is
applicable to optimization problems where the solution either does not change
when a new constraint is added, or the solution is partially deﬁned by the new
constraint so that the dimension of the problem is reduced. It has also been
shown that the special properties of LP-type problems give rise to so-called
Helly-type theorems [16].
Randomization is a technique that often produces algorithms that are simple and
efﬁcient. We will see more examples in the following chapters. The price we
pay is that the running time is only an expected bound and—as we observed—
there is a certain chance that the algorithm takes much longer. Some people
take this as a reason to say that randomized algorithms cannot be trusted and
90

Section 4.9
EXERCISES
shouldn't be used (think of a computer in an intensive care station in a hospital,
or in a nuclear power plant).
On the other hand, deterministic algorithms are only perfect in theory. In
practice, any non-trivial algorithm may contain bugs, and even if we neglect
this, there is the risk of hardware malfunction or "soft errors": single bits in
core memory ﬂipping under the inﬂuence of ambient α-radiation. Because
randomized algorithms are often much simpler and have shorter code, the
probability of such a mishap is smaller. Therefore the total probability that a
randomized algorithm fails to produce the correct answer in time need not be
larger than the probability that a deterministic algorithm fails. Moreover, we
can always reduce the probability that the actual running time of a randomized
algorithm exceeds its expected running time by allowing a larger constant in the
expected running time.
4.9
Exercises
4.1
In this chapter we studied the casting problem for molds of one piece. A
sphere cannot be manufactured in this manner, but it can be manufactured
if we use a two-piece mold. Give an example of an object that cannot be
manufactured with a two-piece mold, but that can be manufactured with
a three-piece mold.
4.2
Consider the casting problem in the plane: we are given polygon P and a
2-dimensional mold for it. Describe a linear time algorithm that decides
whether P can be removed from the mold by a single translation.
4.3
Suppose that, in the 3-dimensional casting problem, we do not want the
object to slide along a facet of the mold when we remove it. How does
this affect the geometric problem (computing a point in the intersection
of half-planes) that we derived?
4.4
Let P be a castable simple polyhedron with top facet f. Let ⃗d be a removal
direction for P. Show that any line with direction ⃗d intersects P if and
only if it intersects f. Also show that for any line ℓwith direction ⃗d, the
intersection ℓ∩P is connected.
4.5
Let P be a simple polyhedron with n vertices. If P is castable with some
facet f as top facet, then a necessary condition is that the facets adjacent
to f must lie completely to one side of hf , the plane through f. (The
reverse is not necessarily true, of course: if all adjacent facets lie to one
side of h f then P is not necessarily castable with f as top facet.) Give a
linear time algorithm to compute all facets of P for which this condition
holds.
4.6* Consider the restricted version of the casting problem in which we insist
that the object is removed from its mold using a vertical translation
(perpendicular to the top facet).
91

Chapter 4
LINEAR PROGRAMMING
a. Prove that in this case there is always only a constant number of
possible top facets.
b. Give a linear time algorithm that determines whether for a given object
a mold exists under this restricted model.
4.7
Instead of removing the object from the mold by a single translation, we
can also try to remove it by a single rotation. For simplicity, let's consider
the planar variant of this version of the casting problem, and let's only
look at clockwise rotations.
rotation
center
a. Give an example of a simple polygon P with top facet f that is not
castable when we require that P should be removed from the mold by
a single translation, but that is castable using rotation around a point.
Also give an example of a simple polygon P with top facet f that is
not castable when we require that P should be removed from the mold
by a rotation, but that is castable using a single translation.
b. Show that the problem of ﬁnding a center of rotation that allows us
to remove P with a single rotation from its mold can be reduced to
the problem of ﬁnding a point in the common intersection of a set of
half-planes.
4.8
The plane z = 1 can be used to represent all directions of vectors in 3-
dimensional space that have a positive z-value. How can we represent
all directions of vectors in 3-dimensional space that have a non-negative
z-value? And how can we represent the directions of all vectors in 3-
dimensional space?
4.9
Suppose we want to ﬁnd all optimal solutions to a 3-dimensional linear
program with n constraints. Argue that Ω(nlogn) is a lower bound for
the worst-case time complexity of any algorithm solving this problem.
4.10 Let H be a set of at least three half-planes with a non-empty intersection
such that not all bounding lines are parallel. We call a half-plane h ∈H
redundant if it does not contribute an edge to !H. Prove that for any
redundant half-plane h ∈H there are two half-planes h′,h′′ ∈H such that
h′ ∩h′′ ⊂h. Give an O(nlogn) time algorithm to compute all redundant
half-planes.
4.11 Give an example of a 2-dimensional linear program that is bounded, but
where there is no lexicographically smallest solution.
4.12 Prove that RANDOMPERMUTATION(A) is correct, that is, prove that every
possible permutation of A is equally likely to be the output. Also show that
the algorithm is no longer correct (it no longer produces every permutation
with equal probability) if we change the k in line 2 to n.
4.13 In the text we gave a linear time algorithm for computing a random
permutation. The algorithm needed a random number generator that can
produce a random integer between 1 and n in constant time. Now assume
we have a restricted random number generator available that can only
92

Section 4.9
EXERCISES
generate a random bit (0 or 1) in constant time. How can we generate a
random permutation with this restricted random number generator? What
is the running time of your procedure?
4.14 Here is a paranoid algorithm to compute the maximum of a set A of n real
numbers:
Algorithm PARANOIDMAXIMUM(A)
1.
if card(A) = 1
2.
then return the unique element x ∈A
3.
else Pick a random element x from A.
4.
x′ ←PARANOIDMAXIMUM(A\{x})
5.
if x ⩽x′
6.
then return x′
7.
else Now we suspect that x is the maximum, but to be
absolutely sure, we compare x with all card(A) −1
other elements of A.
8.
return x
What is the worst-case running time of this algorithm? What is the
expected running time (with respect to the random choice in line 3)?
4.15 A simple polygon P is called star-shaped if it contains a point q such
that for any point p in P the line segment pq is contained in P. Give
an algorithm whose expected running time is linear to decide whether a
simple polygon is star-shaped.
4.16 On n parallel railway tracks n trains are going with constant speeds v1,
v2, . . . , vn. At time t = 0 the trains are at positions k1, k2, . . . , kn. Give an
O(nlogn) algorithm that detects all trains that at some moment in time
are leading. To this end, use the algorithm for computing the intersection
of half-planes.
4.17* Show how to implement MINIDISC using a single routine MINIDISC-
WITHPOINTS(P,R) that computes md(P,R) as deﬁned in Lemma 4.14.
Your algorithm should compute only a single random permutation during
the whole computation.
93

5
Orthogonal Range Searching
Querying a Database
At ﬁrst sight it seems that databases have little to do with geometry. Nevertheless,
many types of questions—from now on called queries—about data in a database
can be interpreted geometrically. To this end we transform records in a database
into points in a multi-dimensional space, and we transform the queries about
the records into queries on this set of points. Let's demonstrate this with an
example.
date of birth
salary
19,500,000
19,559,999
3,000
4,000
G. Ometer
born: Aug 19, 1954
salary: $3,500
Figure 5.1
Interpreting a database query
geometrically
Consider a database for personnel administration. In such a database the
name, address, date of birth, salary, and so on, of each employee are stored. A
typical query one may want to perform is to report all employees born between
1950 and 1955 who earn between $3,000 and $4,000 a month. To formulate
this as a geometric problem we represent each employee by a point in the
plane. The ﬁrst coordinate of the point is the date of birth, represented by the
integer 10,000 × year + 100 × month + day, and the second coordinate is the
monthly salary. With the point we also store the other information we have
about the employee, such as name and address. The database query asking
for all employees born between 1950 and 1955 who earn between $3,000 and
95

Chapter 5
ORTHOGONAL RANGE SEARCHING
$4,000 transforms into the following geometric query: report all points whose
ﬁrst coordinate lies between 19,500,000 and 19,559,999, and whose second
coordinate lies between 3,000 and 4,000. In other words, we want to report all
the points inside an axis-parallel query rectangle—see Figure 5.1.
What if we also have information about the number of children of each
employee, and we would like to be able to ask queries like "report all employees
born between 1950 and 1955 who earn between $3,000 and $4,000 a month and
have between two and four children"? In this case we represent each employee
by a point in 3-dimensional space: the ﬁrst coordinate represents the date of
birth, the second coordinate the salary, and the third coordinate the number of
children. To answer the query we now have to report all points inside the axis-
parallel box [19,500,000 : 19,559,999]×[3,000 : 4,000]×[2 : 4]. In general, if
19,500,000
19,559,999
3,000
4,000
2
4
we are interested in answering queries on d ﬁelds of the records in our database,
we transform the records to points in d-dimensional space. A query asking to
report all records whose ﬁelds lie between speciﬁed values then transforms to
a query asking for all points inside a d-dimensional axis-parallel box. Such a
query is called a rectangular range query, or an orthogonal range query, in
computational geometry. In this chapter we shall study data structures for such
queries.
5.1
1-Dimensional Range Searching
Before we try to tackle the 2- or higher-dimensional rectangular range searching
problem, let's have a look at the 1-dimensional version. The data we are given
is a set of points in 1-dimensional space—in other words, a set of real numbers.
A query asks for the points inside a 1-dimensional query rectangle—in other
words, an interval [x : x′].
Let P := {p1, p2,..., pn} be the given set of points on the real line. We can
solve the 1-dimensional range searching problem efﬁciently using a well-known
data structure: a balanced binary search tree T. (A solution that uses an array is
also possible. This solution does not generalize to higher dimensions, however,
nor does it allow for efﬁcient updates on P.) The leaves of T store the points
of P and the internal nodes of T store splitting values to guide the search. We
denote the splitting value stored at a node ν by xν. We assume that the left
subtree of a node ν contains all the points smaller than or equal to xν, and that
the right subtree contains all the points strictly greater than xν.
To report the points in a query range [x : x′] we proceed as follows. We
search with x and x′ in T. Let µ and µ′ be the two leaves where the searches
end, respectively. Then the points in the interval [x : x′] are the ones stored in the
leaves in between µ and µ′ plus, possibly, the point stored at µ and the point
stored at µ′. When we search with the interval [18 : 77] in the tree of Figure 5.2,
for instance, we have to report all the points stored in the dark grey leaves, plus
the point stored in the leaf µ. How can we ﬁnd the leaves in between µ and
µ′? As Figure 5.2 already suggests, they are the leaves of certain subtrees in
between the search paths to µ and µ′. (In Figure 5.2, these subtrees are dark
96

Section 5.1
1-DIMENSIONAL RANGE SEARCHING
µ
µ′
3
10 19 23 30 37
49
59 62 70 80
89
3
19
10
30
37
59
70
62
100
89
80
23
49
100 105
Figure 5.2
A 1-dimensional range query in a binary
search tree
grey, whereas the nodes on the search paths are light grey.) More precisely, the
subtrees that we select are rooted at nodes ν in between the two search paths
whose parents are on the search path. To ﬁnd these nodes we ﬁrst search for
the node νsplit where the paths to x and x′ split. This is done with the following
subroutine. Let lc(ν) and rc(ν) denote the left and right child, respectively, of a
node ν.
FINDSPLITNODE(T,x,x′)
Input. A tree T and two values x and x′ with x ⩽x′.
Output. The node ν where the paths to x and x′ split, or the leaf where both
paths end.
1.
ν ←root(T)
2.
while ν is not a leaf and (x′ ⩽xν or x > xν)
3.
do if x′ ⩽xν
4.
then ν ←lc(ν)
5.
else ν ←rc(ν)
6.
return ν
Starting from νsplit we then follow the search path of x. At each node where the
path goes left, we report all the leaves in the right subtree, because this subtree
is in between the two search paths. Similarly, we follow the path of x′ and we
report the leaves in the left subtree of nodes where the path goes right. Finally,
νsplit
µ
µ′
root(T)
the selected subtrees
we have to check the points stored at the leaves where the paths end; they may
or may not lie in the range [x : x′].
Next we describe the query algorithm in more detail. It uses a subroutine
REPORTSUBTREE, which traverses the subtree rooted at a given node and
reports the points stored at its leaves. Since the number of internal nodes of any
binary tree is less than its number of leaves, this subroutine takes an amount of
time that is linear in the number of reported points.
Algorithm 1DRANGEQUERY(T,[x : x′])
Input. A binary search tree T and a range [x : x′].
Output. All points stored in T that lie in the range.
1.
νsplit ←FINDSPLITNODE(T,x,x′)
2.
if νsplit is a leaf
3.
then Check if the point stored at νsplit must be reported.
97

Chapter 5
ORTHOGONAL RANGE SEARCHING
4.
else (∗Follow the path to x and report the points in subtrees right of the
path. ∗)
5.
ν ←lc(νsplit)
6.
while ν is not a leaf
7.
do if x ⩽xν
8.
then REPORTSUBTREE(rc(ν))
9.
ν ←lc(ν)
10.
else ν ←rc(ν)
11.
Check if the point stored at the leaf ν must be reported.
12.
Similarly, follow the path to x′, report the points in subtrees left of
the path, and check if the point stored at the leaf where the path
ends must be reported.
We ﬁrst prove the correctness of the algorithm.
Lemma 5.1 Algorithm 1DRANGEQUERY reports exactly those points that lie
in the query range.
Proof. We ﬁrst show that any reported point p lies in the query range. If p is
stored at the leaf where the path to x or to x′ ends, then p is tested explicitly for
inclusion in the query range. Otherwise, p is reported in a call to REPORTSUB-
TREE. Assume this call was made when we followed the path to x. Let ν be the
node on the path such that p was reported in the call REPORTSUBTREE(rc(ν)).
Since ν and, hence, rc(ν) lie in the left subtree of νsplit, we have p ⩽xνsplit.
Because the search path of x′ goes right at νsplit this means that p < x′. On the
other hand, the search path of x goes left at ν and p is in the right subtree of ν,
so x < p. It follows that p ∈[x : x′]. The proof that p lies in the range when it is
reported while following the path to x′ is symmetrical.
It remains to prove that any point p in the range is reported. Let µ be the
leaf where p is stored, and let ν be the lowest ancestor of µ that is visited by the
query algorithm. We claim that ν = µ, which implies that p is reported. Assume
for a contradiction that ν ̸= µ. Observe that ν cannot be a node visited in a call
to REPORTSUBTREE, because all descendants of such a node are visited. Hence,
ν is either on the search path to x, or on the search path to x′, or both. Because
all three cases are similar, we only consider the third case. Assume ﬁrst that µ
is in the left subtree of ν. Then the search path of x goes right at ν (otherwise ν
would not be the lowest visited ancestor). But this implies that p < x. Similarly,
if µ is in the right subtree of ν, then the path of x′ goes left at ν, and p > x′. In
both cases, the assumption that p lies in the range is contradicted.
We now turn our attention to the performance of the data structure. Because
it is a balanced binary search tree, it uses O(n) storage and it can be built in
O(nlogn) time. What about the query time? In the worst case all the points
could be in the query range. In this case the query time will be Θ(n), which
seems bad. Indeed, we do not need any data structure to achieve Θ(n) query
time; simply checking all the points against the query range leads to the same
result. On the other hand, a query time of Θ(n) cannot be avoided when we
have to report all the points. Therefore we shall give a more reﬁned analysis
98

Section 5.2
KD-TREES
of the query time. The reﬁned analysis takes not only n, the number of points
in the set P, into account, but also k, the number of reported points. In other
words, we will show that the query algorithm is output-sensitive, a concept we
already encountered in Chapter 2.
Recall that the time spent in a call to REPORTSUBTREE is linear in the
number of reported points. Hence, the total time spent in all such calls is O(k).
The remaining nodes that are visited are nodes on the search path of x or x′.
Because T is balanced, these paths have length O(logn). The time we spend
at each node is O(1), so the total time spent in these nodes is O(logn), which
gives a query time of O(logn+k).
The following theorem summarizes the results for 1-dimensional range
searching:
Theorem 5.2 Let P be a set of n points in 1-dimensional space. The set P
can be stored in a balanced binary search tree, which uses O(n) storage and
has O(nlogn) construction time, such that the points in a query range can be
reported in time O(k +logn), where k is the number of reported points.
5.2
Kd-Trees
Now let's go to the 2-dimensional rectangular range searching problem. Let
P be a set of n points in the plane. In the remainder of this section we assume
that no two points in P have the same x-coordinate, and no two points have the
same y-coordinate. This restriction is not very realistic, especially not if the
points represent employees and the coordinates are things like salary or number
of children. Fortunately, the restriction can be overcome with a nice trick that
we describe in Section 5.5.
A 2-dimensional rectangular range query on P asks for the points from P lying
inside a query rectangle [x : x′]×[y : y′]. A point p := (px, py) lies inside this
rectangle if and only if
px ∈[x : x′]
and
py ∈[y : y′].
We could say that a 2-dimensional rectangular range query is composed of two
x
px
x′
y
y′
py
p
1-dimensional sub-queries, one on the x-coordinate of the points and one on the
y-coordinate.
In the previous section we saw a data structure for 1-dimensional range
queries. How can we generalize this structure—which was just a binary search
tree—to 2-dimensional range queries? Let's consider the following recursive
deﬁnition of the binary search tree: the set of (1-dimensional) points is split into
two subsets of roughly equal size; one subset contains the points smaller than or
equal to the splitting value, the other subset contains the points larger than the
splitting value. The splitting value is stored at the root, and the two subsets are
stored recursively in the two subtrees.
In the 2-dimensional case each point has two values that are important:
its x- and its y-coordinate. Therefore we ﬁrst split on x-coordinate, next on
99

Chapter 5
ORTHOGONAL RANGE SEARCHING
y-coordinate, then again on x-coordinate, and so on. More precisely, the process
is as follows. At the root we split the set P with a vertical line ℓinto two subsets
of roughly equal size. The splitting line is stored at the root. Pleft, the subset of
points to the left or on the splitting line, is stored in the left subtree, and Pright,
the subset to the right of it, is stored in the right subtree. At the left child of the
ℓ
Pleft
Pright
root we split Pleft into two subsets with a horizontal line; the points below or on
it are stored in the left subtree of the left child, and the points above it are stored
in the right subtree. The left child itself stores the splitting line. Similarly, the
set Pright is split with a horizontal line into two subsets, which are stored in the
left and right subtree of the right child. At the grandchildren of the root, we
split again with a vertical line. In general, we split with a vertical line at nodes
whose depth is even, and we split with a horizontal line at nodes whose depth is
odd. Figure 5.3 illustrates how the splitting is done and what the corresponding
binary tree looks like. A tree like this is called a kd-tree. Originally, the name
Figure 5.3
A kd-tree: on the left the way the plane
is subdivided and on the right the
corresponding binary tree
p4
p1
p5
p3
p2
p7
p9
p10
p6
p8
ℓ1
ℓ2
ℓ3
ℓ4
ℓ5
ℓ6
ℓ7
ℓ8
ℓ9
p1
p2
ℓ8
ℓ4
ℓ2
ℓ1
ℓ5
p3
p4
p5
p6
p7
p8
p9
p10
ℓ7
ℓ6
ℓ9
ℓ3
stood for k-dimensional tree; the tree we described above would be a 2d-tree.
Nowadays, the original meaning is lost, and what used to be called a 2d-tree is
now called a 2-dimensional kd-tree.
We can construct a kd-tree with the recursive procedure described below.
This procedure has two parameters: a set of points and an integer. The ﬁrst
parameter is the set for which we want to build the kd-tree; initially this is the
set P. The second parameter is depth of recursion or, in other words, the depth
of the root of the subtree that the recursive call constructs. The depth parameter
is zero at the ﬁrst call. The depth is important because, as explained above,
it determines whether we must split with a vertical or a horizontal line. The
procedure returns the root of the kd-tree.
Algorithm BUILDKDTREE(P,depth)
Input. A set of points P and the current depth depth.
Output. The root of a kd-tree storing P.
1.
if P contains only one point
2.
then return a leaf storing this point
3.
else if depth is even
4.
then Split P into two subsets with a vertical line ℓthrough the
median x-coordinate of the points in P. Let P1 be the set of
100

Section 5.2
KD-TREES
points to the left of ℓor on ℓ, and let P2 be the set of points
to the right of ℓ.
5.
else Split P into two subsets with a horizontal line ℓthrough
the median y-coordinate of the points in P. Let P1 be the
set of points below ℓor on ℓ, and let P2 be the set of points
above ℓ.
6.
νleft ←BUILDKDTREE(P1,depth+1)
7.
νright ←BUILDKDTREE(P2,depth+1)
8.
Create a node ν storing ℓ, make νleft the left child of ν, and make
νright the right child of ν.
9.
return ν
The algorithm uses the convention that the point on the splitting line—the one
determining the median x- or y-coordinate—belongs to the subset to the left of,
or below, the splitting line. For this to work correctly, the median of a set of n
numbers should be deﬁned as the ⌈n/2⌉-th smallest number. This means that
the median of two values is the smaller one, which ensures that the algorithm
terminates.
Before we come to the query algorithm, let's analyze the construction time
of a 2-dimensional kd-tree. The most expensive step that is performed at every
recursive call is ﬁnding the splitting line. This requires determining the median
x-coordinate or the median y-coordinate, depending on whether the depth is
even or odd. Median ﬁnding can be done in linear time. Linear time median
ﬁnding algorithms, however, are rather complicated. A better approach is to
presort the set of points both on x- and on y-coordinate. The parameter set P is
now passed to the procedure in the form of two sorted lists, one on x-coordinate
and one on y-coordinate. Given the two sorted lists, it is easy to ﬁnd the median
x-coordinate (when the depth is even) or the median y-coordinate (when the
depth is odd) in linear time. It is also easy to construct the sorted lists for the
two recursive calls in linear time from the given lists. Hence, the building time
T(n) satisﬁes the recurrence
T(n) =
!
O(1),
if n = 1,
O(n)+2T(⌈n/2⌉),
if n > 1,
which solves to O(nlogn). This bound subsumes the time we spend for presort-
ing the points on x- and y-coordinate.
To bound the amount of storage we note that each leaf in the kd-tree stores
a distinct point of P. Hence, there are n leaves. Because a kd-tree is a binary
tree, and every leaf and internal node uses O(1) storage, this implies that the
total amount of storage is O(n). This leads to the following lemma.
Lemma 5.3 A kd-tree for a set of n points uses O(n) storage and can be con-
structed in O(nlogn) time.
We now turn to the query algorithm. The splitting line stored at the root
partitions the plane into two half-planes. The points in the left half-plane are
stored in the left subtree, and the points in the right half-plane are stored in the
101

Chapter 5
ORTHOGONAL RANGE SEARCHING
right subtree. In a sense, the left child of the root corresponds to the left half-
plane and the right child corresponds to the right half-plane. (The convention
used in BUILDKDTREE that the point on the splitting line belongs to the left
subset implies that the left half-plane is closed to the right and the right half-
plane is open to the left.) The other nodes in a kd-tree correspond to a region
of the plane as well. The left child of the left child of the root, for instance,
corresponds to the region bounded to the right by the splitting line stored at
the root and bounded from above by the line stored at the left child of the root.
In general, the region corresponding to a node ν is a rectangle, which can be
unbounded on one or more sides. It is bounded by splitting lines stored at
ancestors of ν—see Figure 5.4. We denote the region corresponding to a node
Figure 5.4
Correspondence between nodes in a
kd-tree and regions in the plane
ℓ1
ℓ2
ℓ3
ν
region(ν)
ℓ3
ℓ2
ℓ1
ν by region(ν). The region of the root of a kd-tree is simply the whole plane.
Observe that a point is stored in the subtree rooted at a node ν if and only if it
lies in region(ν). For instance, the subtree of the node ν in Figure 5.4 stores
the points indicated as black dots. Therefore we have to search the subtree
rooted at ν only if the query rectangle intersects region(ν). This observation
leads to the following query algorithm: we traverse the kd-tree, but visit only
nodes whose region is intersected by the query rectangle. When a region is
fully contained in the query rectangle, we can report all the points stored in
its subtree. When the traversal reaches a leaf, we have to check whether the
point stored at the leaf is contained in the query region and, if so, report it.
Figure 5.5 illustrates the query algorithm. (Note that the kd-tree of Figure 5.5
could not have been constructed by Algorithm BUILDKDTREE; the median
wasn't always chosen as the split value.) The grey nodes are visited when we
query with the grey rectangle. The node marked with a star corresponds to a
region that is completely contained in the query rectangle; in the ﬁgure this
rectangular region is shown darker. Hence, the dark grey subtree rooted at this
node is traversed and all points stored in it are reported. The other leaves that are
visited correspond to regions that are only partially inside the query rectangle.
Hence, the points stored in them must be tested for inclusion in the query range;
this results in points p6 and p11 being reported, and points p3, p12, and p13 not
being reported. The query algorithm is described by the following recursive
102

Section 5.2
KD-TREES
*
p1
p2
p2
p1
p3
p3
p4
p4
p5
p5
p6
p6
p7
p7
p8
p8
p9
p9
p10
p10
p11
p11
p12
p12
p13
p13
Figure 5.5
A query on a kd-tree
procedure, which takes as arguments the root of a kd-tree and the query range R.
It uses a subroutine REPORTSUBTREE(ν), which traverses the subtree rooted
at a node ν and reports all the points stored at its leaves. Recall that lc(ν) and
rc(ν) denote the left and right child of a node ν, respectively.
Algorithm SEARCHKDTREE(ν,R)
Input. The root of (a subtree of) a kd-tree, and a range R.
Output. All points at leaves below ν that lie in the range.
1.
if ν is a leaf
2.
then Report the point stored at ν if it lies in R.
3.
else if region(lc(ν)) is fully contained in R
4.
then REPORTSUBTREE(lc(ν))
5.
else if region(lc(ν)) intersects R
6.
then SEARCHKDTREE(lc(ν),R)
7.
if region(rc(ν)) is fully contained in R
8.
then REPORTSUBTREE(rc(ν))
9.
else if region(rc(ν)) intersects R
10.
then SEARCHKDTREE(rc(ν),R)
The main test the query algorithm performs is whether the query range R
intersects the region corresponding to some node ν. To be able to do this test
we can compute region(ν) for all nodes ν during the preprocessing phase and
store it, but this is not necessary: one can maintain the current region through
the recursive calls using the lines stored in the internal nodes. For instance, the
region corresponding to the left child of a node ν at even depth can be computed
from region(ν) as follows:
region(ν)
ℓ(ν)
region(lc(ν))
ℓ(ν)left
region(lc(ν)) = region(ν)∩ℓ(ν)left,
where ℓ(ν) is the splitting line stored at ν, and ℓ(ν)left is the half-plane to the
left of and including ℓ(ν).
103

Chapter 5
ORTHOGONAL RANGE SEARCHING
Observe that the query algorithm above never assumes that the query range
R is a rectangle. Indeed, it works for any other query range as well.
We now analyze the time a query with a rectangular range takes.
Lemma 5.4 A query with an axis-parallel rectangle in a kd-tree storing n points
can be performed in O(√n+k) time, where k is the number of reported points.
Proof. First of all, note that the time to traverse a subtree and report the points
stored in its leaves is linear in the number of reported points. Hence, the total
time required for traversing subtrees in steps 4 and 8 is O(k), where k is the
total number of reported points. It remains to bound the number of nodes
visited by the query algorithm that are not in one of the traversed subtrees.
(These are the light grey nodes in Figure 5.5.) For each such node ν, the query
range properly intersects region(ν), that is, region(ν) is intersected by, but not
fully contained in the range. In other words, the boundary of the query range
intersects region(ν). To analyze the number of such nodes, we shall bound the
number of regions intersected by any vertical line. This will give us an upper
bound on the number of regions intersected by the left and right edge of the
query rectangle. The number of regions intersected by the bottom and top edges
of the query range can be bounded in the same way.
Let ℓbe a vertical line, and let T be a kd-tree. Let ℓ(root(T)) be the splitting
line stored at the root of the kd-tree. The line ℓintersects either the region to
the left of ℓ(root(T)) or the region to the right of ℓ(root(T)), but not both. This
observation seems to imply that Q(n), the number of intersected regions in a
kd-tree storing a set of n points, satisﬁes the recurrence Q(n) = 1+Q(n/2). But
this is not true, because the splitting lines are horizontal at the children of the
root. This means that if the line ℓintersects for instance region(lc(root(T)), then
it will always intersect the regions corresponding to both children of lc(root(T)).
Hence, the recursive situation we get is not the same as the original situation,
and the recurrence above is incorrect. To overcome this problem we have to
make sure that the recursive situation is exactly the same as the original situation:
the root of the subtree must contain a vertical splitting line. This leads us to
redeﬁne Q(n) as the number of intersected regions in a kd-tree storing n points
whose root contains a vertical splitting line. To write a recurrence for Q(n) we
now have to go down two steps in the tree. Each of the four nodes at depth
two in the tree corresponds to a region containing n/4 points. (To be precise, a
region can contain at most ⌈⌈n/2⌉/2⌉= ⌈n/4⌉points, but asymptotically this
does not inﬂuence the outcome of the recurrence below.) Two of the four nodes
correspond to intersected regions, so we have to count the number of intersected
regions in these subtrees recursively. Moreover, ℓintersects the region of the
root and of one of its children. Hence, Q(n) satisﬁes the recurrence
Q(n) =
!
O(1),
if n = 1,
2+2Q(n/4),
if n > 1.
This recurrence solves to Q(n) = O(√n). In other words, any vertical line
intersects O(√n) regions in a kd-tree. In a similar way one can prove that the
104

Section 5.3
RANGE TREES
total number of regions intersected by a horizontal line is O(√n). The total
number of regions intersected by the boundary of a rectangular query range is
bounded by O(√n) as well.
The analysis of the query time that we gave above is rather pessimistic: we
bounded the number of regions intersecting an edge of the query rectangle by the
number of regions intersecting the line through it. In many practical situations
the range will be small. As a result, the edges are short and will intersect much
fewer regions. For example, when we search with a range [x : x]×[y : y]—this
query effectively asks whether the point (x,y) is in the set—the query time is
bounded by O(logn).
The following theorem summarizes the performance of kd-trees.
Theorem 5.5 A kd-tree for a set P of n points in the plane uses O(n) storage
and can be built in O(nlogn) time. A rectangular range query on the kd-tree
takes O(√n+k) time, where k is the number of reported points.
Kd-trees can also be used for point sets in 3- or higher-dimensional space.
The construction algorithm is very similar to the planar case: At the root, we
split the set of points into two subsets of roughly the same size by a hyperplane
perpendicular to the x1-axis. In other words, at the root the point set is partitioned
based on the ﬁrst coordinate of the points. At the children of the root the partition
is based on the second coordinate, at nodes at depth two on the third coordinate,
and so on, until at depth d −1 we partition on the last coordinate. At depth
d we start all over again, partitioning on ﬁrst coordinate. The recursion stops
when there is only one point left, which is then stored at a leaf. Because a
d-dimensional kd-tree for a set of n points is a binary tree with n leaves, it uses
O(n) storage. The construction time is O(nlogn). (As usual, we assume d to
be a constant.)
Nodes in a d-dimensional kd-tree correspond to regions, as in the plane. The
query algorithm visits those nodes whose regions are properly intersected by
the query range, and traverses subtrees (to report the points stored in the leaves)
that are rooted at nodes whose region is fully contained in the query range. It
can be shown that the query time is bounded by O(n1−1/d +k).
5.3
Range Trees
Kd-trees, which were described in the previous section, have O(√n+k) query
time. So when the number of reported points is small, the query time is relatively
high. In this section we shall describe another data structure for rectangular
range queries, the range tree, which has a better query time, namely O(log2 n+
k). The price we have to pay for this improvement is an increase in storage from
O(n) for kd-trees to O(nlogn) for range trees.
As we observed before, a 2-dimensional range query is essentially composed of
two 1-dimensional sub-queries, one on the x-coordinate of the points and one
105

Chapter 5
ORTHOGONAL RANGE SEARCHING
on the y-coordinate. This gave us the idea to split the given point set alternately
on x- and y-coordinate, leading to the kd-tree. To obtain the range tree, we shall
use this observation in a different way.
Let P be a set of n points in the plane that we want to preprocess for
rectangular range queries. Let [x : x′] × [y : y′] be the query range. We ﬁrst
concentrate on ﬁnding the points whose x-coordinate lies in [x : x′], the x-
interval of the query rectangle, and worry about the y-coordinate later. If we
only care about the x-coordinate then the query is a 1-dimensional range query.
In Section 5.1 we have seen how to answer such a query: with a binary search
tree on the x-coordinate of the points. The query algorithm was roughly as
follows. We search with x and x′ in the tree until we get to a node νsplit where
the search paths split. From the left child of νsplit we continue the search with x,
and at every node ν where the search path of x goes left, we report all points
in the right subtree of ν. Similarly, we continue the search with x′ at the right
child of νsplit, and at every node ν where the search path of x′ goes right we
report all points in the left subtree of ν. Finally, we check the leaves µ and µ′
where the two paths end to see if they contain a point in the range. In effect, we
select a collection of O(logn) subtrees that together contain exactly the points
whose x-coordinate lies in the x-interval of the query rectangle.
νsplit
µ
µ′
Let's call the subset of points stored in the leaves of the subtree rooted at a
node ν the canonical subset of ν. The canonical subset of the root of the tree,
for instance, is the whole set P. The canonical subset of a leaf is simply the
point stored at that leaf. We denote the canonical subset of node ν by P(ν). We
have just seen that the subset of points whose x-coordinate lies in a query range
can be expressed as the disjoint union of O(logn) canonical subsets; these are
the sets P(ν) of the nodes ν that are the roots of the selected subtrees. We are
not interested in all the points in such a canonical subset P(ν), but only want to
report the ones whose y-coordinate lies in the interval [y : y′]. This is another
1-dimensional query, which we can solve, provided we have a binary search tree
on the y-coordinate of the points in P(ν) available. This leads to the following
data structure for rectangular range queries on a set P of n points in the plane.
The main tree is a balanced binary search tree T built on the x-coordinate of
the points in P.
For any internal or leaf node ν in T, the canonical subset P(ν) is stored in
a balanced binary search tree Tassoc(ν) on the y-coordinate of the points.
The node ν stores a pointer to the root of Tassoc(ν), which is called the
associated structure of ν.
This data structure is called a range tree. Figure 5.6 shows the structure of a
range tree. Data structures where nodes have pointers to associated structures
are often called multi-level data structures. The main tree T is then called
the ﬁrst-level tree, and the associated structures are second-level trees. Multi-
level data structures play an important role in computational geometry; more
examples can be found in Chapters 10 and 16.
A range tree can be constructed with the following recursive algorithm, which
receives as input the set P := {p1,..., pn} of points sorted on x-coordinate and
106

Section 5.3
RANGE TREES
T
P(ν)
ν
Tassoc(ν)
P(ν)
binary search tree
on y-coordinates
binary search tree on
x-coordinates
Figure 5.6
A 2-dimensional range tree
returns the root of a 2-dimensional range tree T of P. As in the previous section,
we assume that no two points have the same x- or y-coordinate. We shall get rid
of this assumption in Section 5.5.
Algorithm BUILD2DRANGETREE(P)
Input. A set P of points in the plane.
Output. The root of a 2-dimensional range tree.
1.
Construct the associated structure: Build a binary search tree Tassoc on the
set Py of y-coordinates of the points in P. Store at the leaves of Tassoc not
just the y-coordinate of the points in Py, but the points themselves.
2.
if P contains only one point
3.
then Create a leaf ν storing this point, and make Tassoc the associated
structure of ν.
4.
else Split P into two subsets; one subset Pleft contains the points with
x-coordinate less than or equal to xmid, the median x-coordinate,
and the other subset Pright contains the points with x-coordinate
larger than xmid.
5.
νleft ←BUILD2DRANGETREE(Pleft)
6.
νright ←BUILD2DRANGETREE(Pright)
7.
Create a node ν storing xmid, make νleft the left child of ν, make
νright the right child of ν, and make Tassoc the associated structure
of ν.
8.
return ν
Note that in the leaves of the associated structures we do not just store the
y-coordinate of the points but the points themselves. This is important because,
when searching the associated structures, we need to report the points and not
just the y-coordinates.
Lemma 5.6 A range tree on a set of n points in the plane requires O(nlogn)
storage.
Proof. A point p in P is stored only in the associated structure of nodes on the
path in T towards the leaf containing p. Hence, for all nodes at a given depth of T,
107

Chapter 5
ORTHOGONAL RANGE SEARCHING
the point p is stored in exactly one associated structure. Because 1-dimensional
range trees use linear storage it follows that the associated structures of all nodes
at any depth of T together use O(n) storage. The depth of T is O(logn). Hence,
the total amount of storage required is bounded by O(nlogn).
p
p
p
p
Algorithm BUILD2DRANGETREE as it is described will not result in the
optimal construction time of O(nlogn). To obtain this we have to be a bit
careful. Constructing a binary search tree on an unsorted set of n keys takes
O(nlogn) time. This means that constructing the associated structure in line 1
would take O(nlogn) time. But we can do better if the points in Py are presorted
on y-coordinate; then the binary search tree can be constructed bottom-up in
linear time. During the construction algorithm we therefore maintain the set of
points in two lists, one sorted on x-coordinate and one sorted on y-coordinate.
This way the time we spend at a node in the main tree T is linear in the size of
its canonical subset. This implies that the total construction time is the same as
the amount of storage, namely O(nlogn). Since the presorting takes O(nlogn)
time as well, the total construction time is again O(nlogn).
The query algorithm ﬁrst selects O(logn) canonical subsets that together contain
the points whose x-coordinate lie in the range [x : x′]. This can be done with
the 1-dimensional query algorithm. Of those subsets, we then report the points
whose y-coordinate lie in the range [y : y′]. For this we also use the 1-dimensional
query algorithm; this time it is applied to the associated structures that store the
selected canonical subsets. Thus the query algorithm is virtually the same as
the 1-dimensional query algorithm 1DRANGEQUERY; the only difference is
that calls to REPORTSUBTREE are replaced by calls to 1DRANGEQUERY.
Algorithm 2DRANGEQUERY(T,[x : x′]×[y : y′])
Input. A 2-dimensional range tree T and a range [x : x′]×[y : y′].
Output. All points in T that lie in the range.
1.
νsplit ←FINDSPLITNODE(T,x,x′)
2.
if νsplit is a leaf
3.
then Check if the point stored at νsplit must be reported.
4.
else (∗Follow the path to x and call 1DRANGEQUERY on the subtrees
right of the path. ∗)
5.
ν ←lc(νsplit)
6.
while ν is not a leaf
7.
do if x ⩽xν
8.
then 1DRANGEQUERY(Tassoc(rc(ν)),[y : y′])
9.
ν ←lc(ν)
10.
else ν ←rc(ν)
11.
Check if the point stored at ν must be reported.
12.
Similarly, follow the path from rc(νsplit) to x′, call 1DRANGE-
QUERY with the range [y : y′] on the associated structures of sub-
trees left of the path, and check if the point stored at the leaf where
the path ends must be reported.
108

Section 5.4
HIGHER-DIMENSIONAL RANGE TREES
Lemma 5.7 A query with an axis-parallel rectangle in a range tree storing n
points takes O(log2 n+k) time, where k is the number of reported points.
Proof. At each node ν in the main tree T we spend constant time to decide where
the search path continues, and we possibly call 1DRANGEQUERY. Theorem 5.2
states that the time we spend in this recursive call is O(logn+kν), where kν is
the number of points reported in this call. Hence, the total time we spend is
∑
ν
O(logn+kν),
where the summation is over all nodes in the main tree T that are visited. Notice
that the sum ∑ν kν equals k, the total number of reported points. Furthermore,
the search paths of x and x′ in the main tree T have length O(logn). Hence,
∑ν O(logn) = O(log2 n). The lemma follows.
The following theorem summarizes the performance of 2-dimensional range
trees.
Theorem 5.8 Let P be a set of n points in the plane. A range tree for P uses
O(nlogn) storage and can be constructed in O(nlogn) time. By querying this
range tree one can report the points in P that lie in a rectangular query range in
O(log2 n+k) time, where k is the number of reported points.
The query time stated in Theorem 5.8 can be improved to O(logn+k) by a
technique called fractional cascading. This is described in Section 5.6.
5.4
Higher-Dimensional Range Trees
It is fairly straightforward to generalize 2-dimensional range trees to higher-
dimensional range trees. We only describe the global approach.
Let P be a set of points in d-dimensional space. We construct a balanced
binary search tree on the ﬁrst coordinate of the points. The canonical subset
P(ν) of a node ν in this ﬁrst-level tree, the main tree, consists of the points
stored in the leaves of the subtree rooted at ν. For each node ν we construct
an associated structure Tassoc(ν); the second-level tree Tassoc(ν) is a (d −1)-
dimensional range tree for the points in P(ν), restricted to their last d −1
coordinates. This (d −1)-dimensional range tree is constructed recursively in
the same way: it is a balanced binary search tree on the second coordinate of the
points, in which each node has a pointer to a (d −2)-dimensional range tree of
the points in its subtree, restricted to the last (d −2) coordinates. The recursion
stops when we are left with points restricted to their last coordinate; these are
stored in a 1-dimensional range tree—a balanced binary search tree.
The query algorithm is also very similar to the 2-dimensional case. We use
the ﬁrst-level tree to locate O(logn) nodes whose canonical subsets together
contain all the points whose ﬁrst coordinates are in the correct range. These
canonical subsets are queried further by performing a range query on the cor-
responding second-level structures. In each second-level structure we select
109

Chapter 5
ORTHOGONAL RANGE SEARCHING
O(logn) canonical subsets. This means there are O(log2 n) canonical subsets
in the second-level structures in total. Together, they contain all points whose
ﬁrst and second coordinate lie in the correct ranges. The third-level structures
storing these canonical subsets are then queried with the range for the third
coordinate, and so on, until we reach the 1-dimensional trees. In these trees we
ﬁnd the points whose last coordinate lies in the correct range and report them.
This approach leads to the following result.
Theorem 5.9 Let P be a set of n points in d-dimensional space, where d ⩾2.
A range tree for P uses O(nlogd−1 n) storage and it can be constructed in
O(nlogd−1 n) time. One can report the points in P that lie in a rectangular query
range in O(logd n+k) time, where k is the number of reported points.
Proof. Let Td(n) denote the construction time for a range tree on a set of n points
in d-dimensional space. By Theorem 5.8 we know that T2(n) = O(nlogn). The
construction of a d-dimensional range tree consists of building a balanced binary
search tree, which takes time O(nlogn), and the construction of associated
structures. At the nodes at any depth of the ﬁrst-level tree, each point is stored
in exactly one associated structure. The time required to build all associated
structures of the nodes at some depth is O(Td−1(n)), the time required to build
the associated structure of the root. This follows because the building time is at
least linear. Hence, the total construction time satisﬁes
Td(n) = O(nlogn)+O(logn)·Td−1(n).
Since T2(n) = O(nlogn), this recurrence solves to O(nlogd−1 n). The bound
on the amount of storage follows in the same way.
Let Qd(n) denote the time spent in querying a d-dimensional range tree on
n points, not counting the time to report points. Querying the d-dimensional
range tree involves searching in a ﬁrst-level tree, which takes time O(logn), and
querying a logarithmic number of (d −1)-dimensional range trees. Hence,
Qd(n) = O(logn)+O(logn)·Qd−1(n),
where Q2(n) = O(log2 n). This recurrence easily solves to Qd(n) = O(logd n).
We still have to add the time needed to report points, which is bounded by O(k).
The bound on the query time follows.
As in the 2-dimensional case, the query time can be improved by a logarith-
mic factor—see Section 5.6.
5.5
General Sets of Points
Until now we imposed the restriction that no two points have equal x- or y-
coordinate, which is highly unrealistic. Fortunately, this is easy to remedy. The
crucial observation is that we never assumed the coordinate values to be real
numbers. We only need that they come from a totally ordered universe, so that
110

Section 5.6*
FRACTIONAL CASCADING
we can compare any two coordinates and compute medians. Therefore we can
use the trick described next.
We replace the coordinates, which are real numbers, by elements of the so-called
composite-number space. The elements of this space are pairs of reals. The
composite number of two reals a and b is denoted by (a|b). We deﬁne a total
order on the composite-number space by using a lexicographic order. So, for
two composite numbers (a|b) and (a′|b′), we have
(a|b) < (a′|b′)
⇔
a < a′ or (a = a′ and b < b′).
Now assume we are given a set P of n points in the plane. The points are
distinct, but many points can have the same x- or y-coordinate. We replace each
point p := (px, py) by a new point ˆp := ((px|py),(py|px)) that has composite
numbers as coordinate values. This way we obtain a new set "P of n points.
The ﬁrst coordinate of any two points in "P are distinct; the same holds true for
the second coordinate. Using the order deﬁned above one can now construct
kd-trees and 2-dimensional range trees for "P.
Now suppose we want to report the points of P that lie in a range R :=
[x : x′]×[y : y′]. To this end we must query the tree we have constructed for "P.
This means that we must also transform the query range to our new composite
space. The transformed range "R is deﬁned as follows:
"R := [(x|−∞) : (x′|+∞)]×[(y|−∞) : (y′|+∞)].
It remains to prove that our approach is correct, that is, that the points of "P that
we report when we query with "R correspond exactly to the points of P that lie in
R.
Lemma 5.10 Let p be a point and R a rectangular range. Then
p ∈R ⇔
ˆp ∈"R.
Proof. Let R := [x : x′]×[y : y′] and let p := (px, py). By deﬁnition, p lies in R
if and only if x ⩽px ⩽x′ and y ⩽py ⩽y′. This is easily seen to hold if and only
if (x|−∞) ⩽(px|py) ⩽(x′|+∞) and (y|−∞) ⩽(py|px) ⩽(y′|+∞), that is, if
and only if ˆp lies in "R.
We can conclude that our approach is indeed correct: we will get the correct
answer to a query. Observe that there is no need to actually store the transformed
points: we can just store the original points, provided that we do comparisons
between two x-coordinates or two y-coordinates in the composite space.
The approach of using composite numbers can also be used in higher dimen-
sions.
5.6*
Fractional Cascading
In Section 5.3 we described a data structure for rectangular range queries in the
plane, the range tree, whose query time is O(log2 n + k). (Here n is the total
111

Chapter 5
ORTHOGONAL RANGE SEARCHING
number of points stored in the data structure, and k is the number of reported
points.) In this section we describe a technique, called fractional cascading, to
reduce the query time to O(logn+k).
Let's brieﬂy recall how a range tree works. A range tree for a set P of points
in the plane is a two-level data structure. The main tree is a binary search
tree on the x-coordinate of the points. Each node ν in the main tree has an
associated structure Tassoc(ν), which is a binary search tree on the y-coordinate
of the points in P(ν), the canonical subset of ν. A query with a rectangular
range [x : x′] × [y : y′] is performed as follows: First, a collection of O(logn)
nodes in the main tree is identiﬁed whose canonical subsets together contain the
points with x-coordinate in the range [x : x′]. Second, the associated structures of
these nodes are queried with the range [y : y′]. Querying an associated structure
Tassoc(ν) is a 1-dimensional range query, so it takes O(logn+kν) time, where
kν is the number of reported points. Hence, the total query time is O(log2 n+k).
If we could perform the searches in the associated structures in O(1+kν)
time, then the total query time would reduce to O(logn+k). But how can we
do this? In general, it is not possible to answer a 1-dimensional range query in
O(1+k) time, with k the number of answers. What saves us is that we have to
do many 1-dimensional searches with the same range, and that we can use the
result of one search to speed up other searches.
We ﬁrst illustrate the idea of fractional cascading with a simple example. Let S1
and S2 be two sets of objects, where each object has a key that is a real number.
These sets are stored in sorted order in arrays A1 and A2. Suppose we want to
report all objects in S1 and in S2 whose keys lie in a query interval [y : y′]. We
can do this as follows: we do a binary search with y in A1 to ﬁnd the smallest
key larger than or equal to y. From there we walk through the array to the right,
reporting the objects we pass, until a key larger than y′ is encountered. The
objects from S2 can be reported in a similar fashion. If the total number of
reported objects is k, then the query time will be O(k) plus the time for two
binary searches, one in A1 and one in A2. If, however, the keys of the objects in
S2 are a subset of the keys of the objects in S1, then we can avoid the second
binary search as follows. We add pointers from the entries in A1 to the entries
in A2: if A1[i] stores an object with key yi, then we store a pointer to the entry
in A2 with the smallest key larger than or equal to yi. If there is no such key
then the pointer from A1[i] is nil. Figure 5.7 illustrates this. (Only the keys are
shown in this ﬁgure, not the corresponding objects.)
How can we use this structure to report the objects in S1 and S2 whose keys
are in a query interval [y : y′]? Reporting the objects in S1 is still done as before:
do a binary search with y in A1, and walk through A1 to the right until a key
larger than y′ is encountered. To report the points from S2 we proceed as follows.
Let the search for y in A1 end at A[i]. Hence, the key of A[i] is the smallest one
in S1 that is larger than or equal to y. Since the keys from S2 form a subset of
the keys from S1, this means that the pointer from A[i] must point to the smallest
key from S2 larger than or equal to y. Hence, we can follow this pointer, and
from there start to walk to the right through A2. This way the binary search in
112

Section 5.6*
FRACTIONAL CASCADING
A1
A2
3
10
19
23
30
37
59
62
70
80
100
105
10
19
30
62
70
80
100
Figure 5.7
Speeding up the search by adding
pointers
A2 is avoided, and reporting the objects from S2 takes only O(1+k) time, with
k the number of reported answers.
Figure 5.7 shows an example of a query. We query with the range [20 : 65].
First we use binary search in A1 to ﬁnd 23, the smallest key larger than or equal
to 20. From there we walk to the right until we encounter a key larger than
65. The objects that we pass have their keys in the range, so they are reported.
Then we follow the pointer from 23 into A2. We get to the key 30, which is the
smallest one larger than or equal to 20 in A2. From there we also walk to the
right until we reach a key larger than 65, and report the objects from S2 whose
keys are in the range.
Now let's return to range trees. The crucial observation here is that the canonical
subsets P(lc(ν)) and P(rc(ν)) both are subsets of P(ν). As a result we can
use the same idea to speed up the query time. The details are slightly more
complicated, because we now have two subsets of P(ν) to which we need
fast access rather than only one. Let T be a range tree on a set P of n points
in the plane. Each canonical subset P(ν) is stored in an associated structure.
But instead of using a binary search tree as associated structure, as we did
in Section 5.3, we now store it in an array A(ν). The array is sorted on the
y-coordinate of the points. Furthermore, each entry in an array A(ν) stores two
pointers: a pointer into A(lc(ν)) and a pointer into A(rc(ν)). More precisely,
we add the following pointers. Suppose that A(ν)[i] stores a point p. Then we
store a pointer from A(ν)[i] to the entry of A(lc(ν)) such that the y-coordinate of
the point p′ stored there is the smallest one larger than or equal to py. As noted
above, P(lc(ν)) is a subset of P(ν). Hence, if p has the smallest y-coordinate
larger than or equal to some value y of any point in P(ν), then p′ has the smallest
y-coordinate larger than or equal to y of any point in P(lc(ν)). The pointer
into A(rc(ν)) is deﬁned in the same way: it points to the entry such that the
y-coordinate of the point stored there is the smallest one that is larger than or
equal to py.
This modiﬁed version of the range tree is called a layered range tree;
Figures 5.8 and 5.9 show an example. (The positions in which the arrays are
drawn corresponds to the positions of the nodes in the tree they are associated
with: the topmost array is associated with the root, the left array below it is
associated with the left child of the root, and so on. Not all pointers are shown
in the ﬁgure.)
Let's see how to answer a query with a range [x : x′]×[y : y′] in a layered
113

Chapter 5
ORTHOGONAL RANGE SEARCHING
Figure 5.8
The main tree of a layered range tree:
the leaves show only the x-coordinates;
the points stored are given below
(2,19)
(5,80)
(7,10)
(8,37)
(12,3)
(15,99)
(17,62) (21,49)
(33,30)
(41,95)
(52,23)
(58,59)
(67,89)
(93,70)
2
5
7
8
12 15
17
21 33 41 52
58
67
2
93
67
58
41
21
12
7
5
8
15
17
52
33
Figure 5.9
The arrays associated with the nodes in
the main tree, with the y-coordinate of
the points of the canonical subsets in
sorted order (not all pointers are shown)
3
99
10 19 37 80
30 49
80
3
99
30
49
99
62
62
10 37
10
37
19
80
19
99
62
37
19
10
3
80
30 49
23
59 70 89 95
30 49
23
3
95
59 70 89
23 95
59
70 89
95
23
89
70
3
10 19 23 30 37 49 59 62 70 80 89 95 99
114

Section 5.7
NOTES AND COMMENTS
range tree. As before we search with x and x′ in the main tree T to determine
O(logn) nodes whose canonical subsets together contain the points with x-
coordinate in the range [x : x′]. These nodes are found as follows. Let νsplit be
the node where the two search paths split. The nodes that we are looking for
are the ones below νsplit that are the right child of a node on the search path to x
where the path goes left, or the left child of a node on the search path to x′ where
the path goes right. At νsplit we ﬁnd the entry in A(νsplit) whose y-coordinate is
the smallest one larger than or equal to y. This can be done in O(logn) time by
binary search. While we search further with x and x′ in the main tree, we keep
track of the entry in the associated arrays whose y-coordinate is the smallest one
larger than or equal to y. They can be maintained in constant time by following
the pointers stored in the arrays. Now let ν be one of the O(logn) nodes we
selected. We must report the points stored in A(ν) whose y-coordinate is in
the range [y : y′]. For this it sufﬁces to be able to ﬁnd the point with smallest
y-coordinate larger than or equal to y; from there we can just walk through the
array, reporting points as long as their y-coordinate is less than or equal to y′.
This point can be found in constant time, because parent(ν) is on the search
path, and we kept track of the points with smallest y-coordinate larger than or
equal to y in the arrays on the search path. Hence, we can report the points of
A(ν) whose y-coordinate is in the range [y : y′] in O(1+kν) time, where kν is
the number of reported answers at node ν. The total query time now becomes
O(logn+k).
Fractional cascading also improves the query time of higher-dimensional range
trees by a logarithmic factor. Recall that a d-dimensional range query was
solved by ﬁrst selecting the points whose d-th coordinate is in the correct range
in O(logn) canonical subsets, and then solving a (d −1)-dimensional query
on these subsets. The (d −1)-dimensional query is solved recursively in the
same way. This continues until we arrive at a 2-dimensional query, which can
be solved as described above. This leads to the following theorem.
Theorem 5.11 Let P be a set of n points in d-dimensional space, with d ⩾2. A
layered range tree for P uses O(nlogd−1 n) storage and it can be constructed in
O(nlogd−1 n) time. With this range tree one can report the points in P that lie
in a rectangular query range in O(logd−1 n+k) time, where k is the number of
reported points.
5.7
Notes and Comments
In the 1970s—the early days of computational geometry—orthogonal range
searching was one of the most important problems in the ﬁeld, and many people
worked on it. This resulted in a large number of results, of which we discuss a
few below.
One of the ﬁrst data structures for orthogonal range searching was the
quadtree, which is discussed in Chapter 14 in the context of meshing. Un-
fortunately, the worst-case behavior of quadtrees is quite bad. Kd-trees, de-
115

Chapter 5
ORTHOGONAL RANGE SEARCHING
scribed ﬁrst by Bentley [44] in 1975, are an improvement over quadtrees.
Samet's books [333, 334] discuss quadtrees, kd-trees, and their applications
in great detail. A few years later, the range tree was discovered indepen-
dently by several people [46, 251, 261, 387]. The improvement in query time
to O(logn + k) by fractional cascading was described by Lueker [261] and
Willard [386]. Fractional cascading applies in fact not only to range trees, but
in many situations where one does many searches with the same key. Chazelle
and Guibas [105, 106] discuss this technique in its full generality. Fractional
cascading can also be used in a dynamic setting [275]. The most efﬁcient
data structure for 2-dimensional range queries is a modiﬁed version of the
layered range tree, described by Chazelle [87]; he succeeded in improving
the storage to O(nlogn/loglogn) while keeping the query time O(logn +k).
Chazelle [90, 91] also proved that this is optimal. If the query range is un-
bounded to one side, for instance when it is of the form [x : x′] × [y : +∞],
then one can achieve O(logn) query time with only linear space, using a pri-
ority search tree—see Chapter 10. In higher dimensions the best result for
orthogonal range searching is also due to Chazelle [90]: he gave a structure for
d-dimensional queries with O(n(logn/loglogn)d−1) storage and polylogarith-
mic query time. Again, this result is optimal. Trade-offs between storage and
query time are also possible [338, 391].
The lower-bound result is only valid under certain models of computation.
This allows for improved results in speciﬁc cases. In particular, Overmars [300]
describes more efﬁcient data structures for range searching when the points lie
on a U ×U grid, yielding query time bounds of O(loglogU +k) or O(
√
U +k),
depending on the preprocessing time allowed. The results use data structures
described earlier by Willard [389, 390]. When compared with the general case,
better time bounds can be obtained for many problems in computational ge-
ometry once the coordinates of the objects are restricted to lie on grid points.
Examples are the nearest neighbor searching problem [224, 225], point loca-
tion [287], and line segment intersection [226].
In databases, range queries are considered the most general of three basic types
of multi-dimensional queries. The two other types are exact match queries
and partial match queries. Exact match queries are simply queries of the
type: Is the object (point) with attribute values (coordinates) such and such
present in the database? The obvious data structure for exact match queries
is the balanced binary tree that uses, for instance, a lexicographical order on
the coordinates. With this structure exact match queries can be answered in
O(logn) time. If the dimension—the number of attributes—increases, it can
be useful to express the efﬁciency of queries not only in terms of n, but also
in the dimension d. If a binary tree is used for exact match queries, the query
time is O(d logn) because it takes O(d) time to compare two points. This can
easily be reduced to O(d +logn) time, which is optimal. A partial match query
speciﬁes only a value for a subset of the coordinates and asks for all points with
the speciﬁed coordinate values. In the plane, for instance, a partial match query
speciﬁes only an x-coordinate, or only a y-coordinate. Interpreted geometrically,
a partial match query in the plane asks for the points on a horizontal line, or on
116

Section 5.8
EXERCISES
a vertical line. With a d-dimensional kd-tree, a partial match query specifying s
coordinates (with s < d) can be answered in O(n1−s/d +k) time, where k is the
number of reported points [44].
In many applications the data that we are given are not a set of points, but a
set of certain objects such as polygons. If we want to report the objects that
are completely contained in a query range [x : x′]× [y : y′], then it is possible
to transform the query to a query on point data in higher dimensions—see
Exercise 5.13. Often one also wants to ﬁnd the objects that are only partially
in the range. This speciﬁc problem is called the windowing problem and is
discussed in Chapter 10.
Other variants of the range searching problem are obtained by allowing
other types of query ranges, such as circles or triangles. Many of these variants
can be solved using so-called partition trees, which are discussed in Chapter 16.
5.8
Exercises
5.1
In the proof of the query time of the kd-tree we found the following
recurrence:
Q(n) =
!
O(1),
if n = 1,
2+2Q(n/4),
if n > 1.
Prove that this recurrence solves to Q(n) = O(√n). Also show that
Ω(√n) is a lower bound for querying in a kd-tree by deﬁning a set of n
points and a query rectangle appropriately.
5.2
Describe algorithms to insert and delete points from a kd-tree. In your
algorithm you do not need to take care of rebalancing the structure.
5.3
In Section 5.2 it was indicated that kd-trees can also be used to store
sets of points in higher-dimensional space. Let P be a set of n points
in d-dimensional space. In parts a. and b. you may consider d to be a
constant.
a. Describe an algorithm to construct a d-dimensional kd-tree for the
points in P. Prove that the tree uses linear storage and that your
algorithm takes O(nlogn) time.
b. Describe the query algorithm for performing a d-dimensional range
query. Prove that the query time is bounded by O(n1−1/d +k).
c. Show that the dependence on d in the amount of storage is linear, that
is, show that the amount of storage is O(dn) if we do not consider d to
be a constant. Give the dependence on d of the construction time and
the query time as well.
5.4
Kd-trees can be used for partial match queries. A 2-dimensional partial
match query speciﬁes a value for one of the coordinates and asks for
all points that have that value for the speciﬁed coordinate. In higher
117

Chapter 5
ORTHOGONAL RANGE SEARCHING
dimensions we specify values for a subset of the coordinates. Here we
allow multiple points to have equal values for coordinates.
a. Show that 2-dimensional kd-trees can answer partial match queries in
O(√n+k) time, where k is the number of reported answers.
b. Explain how to use a 2-dimensional range tree to answer partial match
queries. What is the resulting query time?
c. Describe a data structure that uses linear storage and solves 2-dimen-
sional partial match queries in O(logn+k) time.
d. Show that with a d-dimensional kd-tree we can solve a d-dimensional
partial match query in O(n1−s/d +k) time, where s (with s < d) is the
number of speciﬁed coordinates.
e. Describe a data structure that uses linear storage and that can answer
d-dimensional partial match queries in O(logn+k) time. Hint: Use a
structure whose dependence on d in the amount of storage is exponen-
tial (more precisely, a structure that uses O(d2dn) storage).
5.5
Algorithm SEARCHKDTREE can also be used when querying with other
ranges than rectangles. For example, a query is answered correctly if the
range is a triangle.
a. Show that the query time for range queries with triangles is linear in
the worst case, even if no answers are reported at all. Hint: Choose all
points to be stored in the kd-tree on the line y = x.
b. Suppose that a data structure is needed that can answer triangular range
queries, but only for triangles whose edges are horizontal, vertical,
or have slope +1 or −1. Develop a linear size data structure that
answers such range queries in O(n3/4 +k) time, where k is the number
of points reported. Hint: Choose 4 coordinate axes in the plane and
use a 4-dimensional kd-tree.
c. Improve the query time to O(n2/3 +k). Hint: Solve Exercise 5.4 ﬁrst.
5.6
Describe algorithms to insert and delete points from a range tree. You
don't have to take care of rebalancing the structure.
5.7
In the proof of Lemma 5.7 we made a rather rough estimate of the query
time in the associated structures by stating that this was bounded by
O(logn). In reality the query time is dependent on the actual number of
points in the associated structure. Let nν denote the number of points in
the canonical subset P(ν). Then the total time spent is
∑
ν
Θ(lognν +kν),
where the summation is over all nodes in the main tree T that are vis-
ited. Show that this bound is still Θ(log2 n + k). (That is, our more
careful analysis only improves the constant in the bound, not the order of
magnitude.)
5.8
Theorem 5.8 showed that a range tree on a set of n points in the plane
requires O(nlogn) storage. One could bring down the storage require-
118

Section 5.8
EXERCISES
ments by storing associated structures only with a subset of the nodes in
the main tree.
a. Suppose that only the nodes with depth 0, 2, 4, 6, ... have an associated
structure. Show how the query algorithm can be adapted to answer
queries correctly.
b. Analyze the storage requirements and query time of such a data struc-
ture.
c. Suppose that only the nodes with depth 0, ⌊1
j logn⌋, ⌊2
j logn⌋, ... have
an associated structure, where j ⩾2 is a constant. Analyze the storage
requirements and query time of this data structure. Express the bounds
in n and j.
5.9
One can use the data structures described in this chapter to determine
whether a particular point (a,b) is in a given set by performing a range
query with range [a : a]×[b : b].
a. Prove that performing such a range query on a kd-tree takes time
O(logn).
b. What is the time bound for such a query on a range tree? Prove your
answer.
5.10 In some applications one is interested only in the number of points that
lie in a range rather than in reporting all of them. Such queries are often
referred to as range counting queries. In this case one would like to avoid
having an additive term of O(k) in the query time.
a.
Describe how a 1-dimensional range tree can be adapted such that a
range counting query can be performed in O(logn) time. Prove the
query time bound.
b.
Using the solution to the 1-dimensional problem, describe how d-
dimensional range counting queries can be answered in O(logd n)
time. Prove the query time.
c.* Describe how fractional cascading can be used to improve the running
time with a factor of O(logn) for 2- and higher-dimensional range
counting queries.
5.11 Let S1 be a set of n disjoint horizontal line segments and let S2 be a set
of m disjoint vertical line segments. Give a plane-sweep algorithm that
counts in O((n+m)log(n+m)) time how many intersections there are
in S1 ∪S2.
5.12 In Section 5.5 it was shown that one can treat sets of points in the plane
that contain equal coordinate values using composite numbers. Extend
this notion to points in d-dimensional space. To this end you should deﬁne
the composite number of d numbers and deﬁne an appropriate order on
them. Next, show how to transform the point p := (p1,..., pd) and the
range R := [r1 : r′
1] × ··· × [rd : r′
d] using this order; the transformation
should be such that p ∈R if and only if the transformed point lies in the
transformed range.
119

Chapter 5
ORTHOGONAL RANGE SEARCHING
5.13 In many application one wants to do range searching among objects other
than points.
a.
Let S be a set of n axis-parallel rectangles in the plane. We want to
be able to report all rectangles in S that are completely contained in
a query rectangle [x : x′]×[y : y′]. Describe a data structure for this
problem that uses O(nlog3 n) storage and has O(log4 n + k) query
time, where k is the number of reported answers. Hint: Transform the
problem to an orthogonal range searching problem in some higher-
dimensional space.
b.
Let P consist of a set of n polygons in the plane. Again describe
a data structure that uses O(nlog3 n) storage and has O(log4 n + k)
query time to report all polygons completely contained in the query
rectangle, where k is the number of reported answers.
c.* Improve the query time of your solutions (both for a. and b.) to
O(log3 n+k).
5.14* Prove the O(nlogd−1 n) bounds on the storage and construction time in
Theorem 5.11.
120

6
Point Location
Knowing Where You Are
This book has, for the most part, been written in Europe. More precisely, it has
been written very close to a point at longitude 5◦6′ east and latitude 52◦3′ north.
Where that is? You can ﬁnd that out yourself from a map of Europe: using the
scales on the sides of the map, you will ﬁnd that the point with the coordinates
stated above is located in a little country named "the Netherlands".
In this way you would have answered a point location query: Given a map
and a query point q speciﬁed by its coordinates, ﬁnd the region of the map
containing q. A map, of course, is nothing more than a subdivision of the plane
into regions, a planar subdivision, as deﬁned in Chapter 2.
5◦6′
52◦3′
Figure 6.1
Point location in a map
Point location queries arise in various settings. Suppose that you are sailing on
a sea with sand banks and dangerous currents in various parts of it. To be able
to navigate safely, you will have to know the current at your present position.
Fortunately there are maps that indicate the kind of current in the various parts
of the sea. To use such a map, you will have to do the following. First, you must
determine your position. Until not so long ago, you would have to rely for this
121

Chapter 6
POINT LOCATION
on the stars or the sun, and a good chronometer. Nowadays it is much easier
to determine your position: there are little boxes on the market that compute
your position for you, using information from various satellites. Once you have
determined the coordinates of your position, you will have to locate the point on
the map showing the currents, or to ﬁnd the region of the sea you are presently
in.
One step further would be to automate this last step: store the map electroni-
cally, and let the computer do the point location for you. It could then display
the current—or any other information for which you have a thematic map in
electronic form—of the region you are in continuously. In this situation we have
a set of presumably rather detailed thematic maps and we want to answer point
location queries frequently, to update the displayed information while the ship
is moving. This means that we will want to preprocess the maps, and to store
them in a data structure that makes it possible to answer point location queries
fast.
Point location problems arise on a quite different scale as well. Assume that we
want to implement an interactive geographic information system that displays a
map on a screen. By clicking with the mouse on a country, the user can retrieve
information about that country. While the mouse is moved the system should
display the name of the country underneath the mouse pointer somewhere on
the screen. Every time the mouse is moved, the system has to recompute which
name to display. Clearly this is a point location problem in the map displayed
on the screen, with the mouse position as the query point. These queries occur
with high frequency—after all, we want to update the screen information in real
time—and therefore have to be answered fast. Again, we need a data structure
that supports fast point location queries.
6.1
Point Location and Trapezoidal Maps
Let S be a planar subdivision with n edges. The planar point location problem
is to store S in such a way that we can answer queries of the following type:
Given a query point q, report the face f of S that contains q. If q lies on an edge
or coincides with a vertex, the query algorithm should return this information.
To get some insight into the problem, let's ﬁrst give a very simple data structure
to perform point location queries. We draw vertical lines through all vertices of
the subdivision, as in Figure 6.2. This partitions the plane into vertical slabs.
We store the x-coordinates of the vertices in sorted order in an array. This
makes it possible to determine in O(logn) time the slab that contains a query
point q. Within a slab, there are no vertices of S. This means that the part of the
subdivision lying inside the slab has a special form: all edges intersecting a slab
completely cross it—they have no endpoint in the slab—and they don't cross
each other. This means that they can be ordered from top to bottom. Notice
that every region in the slab between two consecutive edges belongs to a unique
face of S. The lowest and highest region of the slab are unbounded, and are part
122

Section 6.1
POINT LOCATION AND TRAPEZOIDAL
MAPS
Figure 6.2
Partition into slabs
of the unbounded face of S. The special structure of the edges intersecting a
slab implies that we can store them in sorted order in an array. We label each
edge with the face of S that is immediately above it inside the slab.
The total query algorithm is now as follows. First, we do a binary search
with the x-coordinate of the query point q in the array storing the x-coordinates
of the vertices of the subdivision. This tells us the slab containing q. Then we
do a binary search with q in the array for that slab. The elementary operation
in this binary search is: Given a segment s and a point q such that the vertical
line through q intersects s, determine whether q lies above s, below s, or on s.
This tells us the segment directly below q, provided there is one. The label
stored with that segment is the face of S containing q. If we ﬁnd that there is no
segment below q then q lies in the unbounded face.
The query time for the data structure is good: we only perform two binary
searches, the ﬁrst in an array of length at most 2n (the n edges of the subdivision
have at most 2n vertices), and the second in an array of length at most n (a slab
is crossed by at most n edges). Hence, the query time is O(logn).
n
4 slabs
n
4
What about the storage requirements? First of all, we have an array on the
x-coordinates of the vertices, which uses O(n) storage. But we also have an
array for every slab. Such an array stores the edges intersecting its slab, so
it uses O(n) storage. Since there are O(n) slabs, the total amount of storage
is O(n2). It's easy to give an example where n/4 slabs are intersected by n/4
edges each, which shows that this worst-case bound can actually occur.
The amount of storage required makes this data structure rather uninteresting—
a quadratic size structure is useless in most practical applications, even for
moderately large values of n. (One may argue that in practice the quadratic
behavior does not occur. But it is rather likely that the amount of storage is
something like O(n√n).) Where does this quadratic behavior come from? Let's
have a second look at Figure 6.2. The segments and the vertical lines through the
endpoints deﬁne a new subdivision S′, whose faces are trapezoids, triangles, and
unbounded trapezoid-like faces. Furthermore, S′ is a reﬁnement of the original
123

Chapter 6
POINT LOCATION
subdivision S: every face of S′ lies completely in one face of S. The query
algorithm described above is in fact an algorithm to do planar point location in
this reﬁned subdivision. This solves the original planar point location as well:
because S′ is a reﬁnement of S, once we know the face f ′ ∈S′ containing q, we
know the face f ∈S containing q. Unfortunately, the reﬁned subdivision can
have quadratic complexity. It is therefore not surprising that the resulting data
structure has quadratic size.
Perhaps we should look for a different reﬁnement of S that—like the decom-
position shown above—makes point location queries easier, and that—unlike
the decomposition shown above—has a complexity that is not much larger than
the complexity of the original subdivision S. Indeed such a reﬁnement exists.
In the rest of this section, we shall describe the trapezoidal map, a reﬁnement
that has the desirable properties just mentioned.
We call two line segments in the plane non-crossing if their intersection is either
empty or a common endpoint. Notice that the edges of any planar subdivision
are non-crossing.
Let S be a set of n non-crossing segments in the plane. Trapezoidal maps
can be deﬁned for such sets in general, but we shall make two simpliﬁcations
that make life easier for us in this and the next sections.
First, it will be convenient to get rid of the unbounded trapezoid-like faces
that occur at the boundary of the scene. This can be done by introducing a large,
axis-parallel rectangle R that contains the whole scene, that is, that contains all
segments of S. For our application—point location in subdivisions—this is not
a problem: a query point outside R always lies in the unbounded face of S, so
we can safely restrict our attention to what happens inside R.
The second simpliﬁcation is more difﬁcult to justify: we will assume that no
two distinct endpoints of segments in the set S have the same x-coordinate. A
consequence of this is that there cannot be any vertical segments. This assump-
tion is not very realistic: vertical edges occur frequently in many applications,
and the situation that two non-intersecting segments have an endpoint with the
same x-coordinate is not so unusual either, because the precision in which the co-
ordinates are given is often limited. We will make this assumption nevertheless,
postponing the treatment of the general case to Section 6.3.
So we have a set S of n non-crossing line segments, enclosed in a bounding
box R, and with the property that no two distinct endpoints lie on a common
vertical line. We call such a set a set of line segments in general position.
The trapezoidal map T(S) of S—also known as the vertical decomposition or
trapezoidal decomposition of S—is obtained by drawing two vertical extensions
from every endpoint p of a segment in S, one extension going upwards and one
going downwards. The extensions stop when they meet another segment of S or
the boundary of R. We call the two vertical extensions starting in an endpoint p
the upper vertical extension and the lower vertical extension. The trapezoidal
map of S is simply the subdivision induced by S, the rectangle R, and the upper
and lower vertical extensions. Figure 6.3 shows an example.
124

Section 6.1
POINT LOCATION AND TRAPEZOIDAL
MAPS
R
Figure 6.3
A trapezoidal map
sides
A face in T(S) is bounded by a number of edges of T(S). Some of these
edges may be adjacent and collinear. We call the union of such edges a side of
the face. In other words, the sides of a face are the segments of maximal length
that are contained in the boundary of a face.
Lemma 6.1 Each face in a trapezoidal map of a set S of line segments in general
position has one or two vertical sides and exactly two non-vertical sides.
Proof. Let f be a face in T(S). We ﬁrst prove that f is convex.
Because the segments in S are non-crossing, any corner of f is either an
endpoint of a segment in S, a point where a vertical extension abuts a segment
of S or an edge of R, or it is a corner of R. Due to the vertical extensions, no
corner that is a segment endpoint can have an interior angle greater than 180◦.
Moreover, any angle at a point where a vertical extension abuts a segment must
be less than or equal to 180◦as well. Finally, the corners of R are 90◦. Hence,
f is convex—the vertical extensions have removed all non-convexities.
Because we are looking at sides of f, rather than at edges of T(S) on the
boundary of f, the convexity of f implies that it can have at most two vertical
sides. Now suppose for a contradiction that f has more than two non-vertical
sides. Then there must be two such sides that are adjacent and either both
bound f from above or both bound f from below. Because any non-vertical
side must be contained in a segment of S or in an edge of R, and the segments
are non-crossing, the two adjacent sides must meet in a segment endpoint. But
then the vertical extensions for that endpoint prevent the two sides from being
adjacent, a contradiction. Hence, f has at most two non-vertical sides.
Finally, we observe that f is bounded (since we have enclosed the whole
scene in a bounding box R), which implies that it cannot have less than two
non-vertical sides and that it must have at least one vertical side.
125

Chapter 6
POINT LOCATION
Lemma 6.1 shows that the trapezoidal map deserves its name: each face
is either a trapezoid or a triangle, which we can view as a trapezoid with one
degenerate edge of length zero.
∆
top(∆)
bottom(∆)
In the proof of Lemma 6.1 we observed that a non-vertical side of a trapezoid
is contained in a segment of S or in a horizontal edge of R. We denote the non-
vertical segment of S, or edge of R, bounding a trapezoid ∆from above by
top(∆), and the one bounding it from below by bottom(∆).
By the general position assumption, a vertical side of a trapezoid either
consists of vertical extensions, or it is the vertical edge of R. More precisely,
we can distinguish ﬁve different cases for the left side and the right side of a
trapezoid ∆. The cases for the left side are as follows:
(a)
It degenerates to a point, which is the common left endpoint of top(∆) and
bottom(∆).
(b)
It is the lower vertical extension of the left endpoint of top(∆) that abuts
on bottom(∆).
(c)
It is the upper vertical extension of the left endpoint of bottom(∆) that
abuts on top(∆).
(d)
It consists of the upper and lower extension of the right endpoint p of a third
segment s. These extensions abut on top(∆) and bottom(∆), respectively.
(e)
It is the left edge of R. This case occurs for a single trapezoid of T(S) only,
namely the unique leftmost trapezoid of T(S).
The ﬁrst four cases are illustrated in Figure 6.4. The ﬁve cases for the right
vertical edge of ∆are symmetrical. You should verify for yourself that the
listing above is indeed exhaustive.
Figure 6.4
Four of the ﬁve cases for the left edge
of trapezoid ∆
(a)
(b)
(c)
top(∆)
bottom(∆)
leftp(∆)
leftp(∆)
leftp(∆)
top(∆)
bottom(∆)
top(∆)
bottom(∆)
(d)
leftp(∆)
top(∆)
bottom(∆)
s
For every trapezoid ∆∈T(S), except the leftmost one, the left vertical edge
of ∆is, in a sense, deﬁned by a segment endpoint p: it is either contained in
the vertical extensions of p, or—when it is degenerate—it is p itself. We will
denote the endpoint deﬁning the left edge of ∆by leftp(∆). As shown above,
leftp(∆) is the left endpoint of top(∆) or bottom(∆), or it is the right endpoint
of a third segment. For the unique trapezoid whose left side is the left edge of
R, we deﬁne leftp(∆) to be the lower left vertex of R. Similarly, we denote the
endpoint that deﬁnes the right vertical edge of ∆by rightp(∆). Notice that ∆is
uniquely determined by top(∆), bottom(∆), leftp(∆), and rightp(∆). Therefore
we will sometimes say that ∆is deﬁned by these segments and endpoints.
The trapezoidal map of the edges of a subdivision is a reﬁnement of that
subdivision. It is not clear, however, why point location in a trapezoidal map
126

Section 6.1
POINT LOCATION AND TRAPEZOIDAL
MAPS
should be any easier than point location in a general subdivision. But before
we come to this in the next section, let's ﬁrst verify that the complexity of the
trapezoidal map is not too much larger than the number of segments in the set
deﬁning it.
Lemma 6.2 The trapezoidal map T(S) of a set S of n line segments in general
position contains at most 6n+4 vertices and at most 3n+1 trapezoids.
Proof. A vertex of T(S) is either a vertex of R, an endpoint of a segment in S,
or else the point where the vertical extension starting in an endpoint abuts on
another segment or on the boundary of R. Since every endpoint of a segment
induces two vertical extensions—one upwards, one downwards—this implies
that the total number of vertices is bounded by 4+2n+2(2n) = 6n+4.
A bound on the number of trapezoids follows from Euler's formula and the
bound on the number of vertices. Here we give a direct proof, using the point
leftp(∆). Recall that each trapezoid has such a point leftp(∆). This point is the
endpoint of one of the n segments, or is the lower left corner of R. By looking
at the ﬁve cases for the left side of a trapezoid, we ﬁnd that the lower left corner
of R plays this role for exactly one trapezoid, a right endpoint of a segment can
play this role for at most one trapezoid, and a left endpoint of a segment can be
the leftp(∆) of at most two different trapezoids. (Since endpoints can coincide,
a point in the plane can be leftp(∆) for many trapezoids. However, if in case (a)
we consider leftp(∆) to be the left endpoint of bottom(∆), then the left endpoint
of a segment s can be leftp(∆) for only two trapezoids, one above s and one
below s.) It follows that the total number of trapezoids is at most 3n+1.
We call two trapezoids ∆and ∆′ adjacent if they meet along a vertical edge.
In Figure 6.5(i), for example, trapezoid ∆is adjacent to ∆1, ∆2, and ∆3, but not
to ∆4 and ∆5. Because the set of line segments is in general position, a trapezoid
has at most four adjacent trapezoids. If the set is not in general position, a
trapezoid can have an arbitrary number of adjacent trapezoids, as illustrated in
Figure 6.5(ii). Let ∆′ be a trapezoid that is adjacent to ∆along the left vertical
(i)
(ii)
∆
∆1
∆2
∆3
∆4
∆5
∆
∆1
∆2
∆3
∆4
∆5
∆6
Figure 6.5
Trapezoids adjacent to ∆are shaded
edge of ∆. Then either top(∆) = top(∆′) or bottom(∆) = bottom(∆′). In the ﬁrst
case we call ∆′ the upper left neighbor of ∆, and in the second case ∆′ is the
lower left neighbor of ∆. So the trapezoid in Figure 6.4(b) has a bottom left
neighbor but no top left neighbor, the trapezoid in Figure 6.4(c) has a top left
neighbor but no bottom left neighbor, and the trapezoid in Figure 6.4(d) has both
a top left neighbor and a bottom left neighbor. The trapezoid in Figure 6.4(a)
127

Chapter 6
POINT LOCATION
and the single trapezoid whose left vertical edge is the left side of R have no left
neighbors. The upper right neighbor and lower right neighbor of a trapezoid
are deﬁned similarly.
To represent a trapezoidal map, we could use the doubly-connected edge
list described in Chapter 2; after all, a trapezoidal map is a planar subdivision.
However, the special shape of the trapezoidal map makes it more convenient
to use a specialized structure. This structure uses the adjacency of trapezoids
to link the subdivision as a whole. There are records for all line segments and
endpoints of S, since they serve as leftp(∆), rightp(∆), top(∆), and bottom(∆).
Furthermore, the structure contains records for the trapezoids of T(S), but not
for edges or vertices of T(S). The record for a trapezoid ∆stores pointers to
top(∆) and bottom(∆), pointers to leftp(∆) and rightp(∆), and ﬁnally, pointers
to its at most four neighbors. Note that the geometry of a trapezoid ∆(that is,
the coordinates of its vertices) is not available explicitly. However, ∆is uniquely
deﬁned by top(∆), bottom(∆), leftp(∆), and rightp(∆). This means that we can
deduce the geometry of ∆in constant time from the information stored for ∆.
6.2
A Randomized Incremental Algorithm
In this section we will develop a randomized incremental algorithm that con-
structs the trapezoidal map T(S) of a set S of n line segments in general position.
During the construction of the trapezoidal map, the algorithm also builds a data
structure D that can be used to perform point location queries in T(S). This is
the reason why a plane sweep algorithm isn't chosen to construct the trapezoidal
map. It would construct it all right, but it wouldn't give us a data structure that
supports point location queries, which is the main objective of this chapter.
Before discussing the algorithm, we ﬁrst describe the point location data struc-
ture D that the algorithm constructs. This structure, which we call the search
structure, is a directed acyclic graph with a single root and exactly one leaf for
every trapezoid of the trapezoidal map of S. Its inner nodes have out-degree 2.
There are two types of inner nodes: x-nodes, which are labeled with an endpoint
of some segment in S, and y-nodes, which are labeled with a segment itself.
A query with a point q starts at the root and proceeds along a directed path
to one of the leaves. This leaf corresponds to the trapezoid ∆∈T(S) containing
q. At each node on the path, q has to be tested to determine in which of the two
child nodes to proceed. At an x-node, the test is of the form: "Does q lie to the
left or to the right of the vertical line through the endpoint stored at this node?"
At a y-node, the test has the form: "Does q lie above or below the segment s
stored here?" We will ensure that whenever we come to a y-node, the vertical
line through q intersects the segment of that node, so that the test makes sense.
The tests at the inner nodes only have two outcomes: left or right of an endpoint
for an x-node, and above or below a segment for a y-node. What should we
do if the query point lies exactly on the vertical line, or on the segment? For
now, we shall simply make the assumption that this does not occur; Section 6.3,
128

Section 6.2
A RANDOMIZED INCREMENTAL
ALGORITHM
which shows how to deal with sets of segments that are not in general position,
will also deal with this type of query point.
The search structure D and the trapezoidal map T(S) computed by the
algorithm are interlinked: a trapezoid ∆∈T(S) has a pointer to the leaf of D
corresponding to it, and a leaf node of D has a pointer to the corresponding
trapezoid in T(S). Figure 6.6 shows the trapezoidal map of a set of two line
s1
s2
A
B
C
D
E
F
G
p1
p2
q1
q2
s1
p1
q1
p2
s2
s2
q2
E
A
B
C
D
F
G
Figure 6.6
The trapezoidal map of two segments
and a search structure
segments s1 and s2, and a search structure for the trapezoidal map. The x-nodes
are white, with the endpoint labeling it inside. The y-nodes are grey and have
the segment labeling it inside. The leaves of the search structure are shown as
squares, and are labeled with the corresponding trapezoid in the trapezoidal
map.
The algorithm we give for the construction of the search structure is incremental:
it adds the segments one at a time, and after each addition it updates the search
structure and the trapezoidal map. The order in which the segments are added
inﬂuences the search structure; some orders lead to a search structure with a
good query time, while for others the query time will be bad. Instead of trying
to be clever in ﬁnding a suitable order, we shall take the same approach we took
in Chapter 4, where we studied linear programming: we use a random ordering.
So the algorithm will be randomized incremental. Later we will prove that the
search structure resulting from a randomized incremental algorithm is expected
to be good. But ﬁrst we describe the algorithm in more detail. We begin with its
global structure; the various substeps will be explained after that.
Algorithm TRAPEZOIDALMAP(S)
Input. A set S of n non-crossing line segments.
Output. The trapezoidal map T(S) and a search structure D for T(S) in a
bounding box.
1.
Determine a bounding box R that contains all segments of S, and initialize
the trapezoidal map structure T and search structure D for it.
2.
Compute a random permutation s1,s2,...,sn of the elements of S.
3.
for i ←1 to n
4.
do Find the set ∆0,∆1,...,∆k of trapezoids in T properly intersected
by si.
5.
Remove ∆0,∆1,...,∆k from T and replace them by the new trape-
zoids that appear because of the insertion of si.
129

Chapter 6
POINT LOCATION
6.
Remove the leaves for ∆0,∆1,...,∆k from D, and create leaves for
the new trapezoids. Link the new leaves to the existing inner nodes
by adding some new inner nodes, as explained below.
We now describe the various steps of the algorithm in more detail. In the follow-
ing, we let Si := {s1,s2,...,si}. The loop invariant of TRAPEZOIDALMAP is
that T is the trapezoidal map for Si, and that D is a valid search structure for T.
The initialization of T as T(S0) = T(/0) and of D in line 1 is easy: the
trapezoidal map for the empty set consists of a single trapezoid—the bounding
rectangle R—and the search structure for T(/0) consists of a single leaf node
for this trapezoid. For the computation of the random permutation in line 2 see
Chapter 4. Now let's see how to insert a segment si in lines 4-6.
To modify the current trapezoidal map, we ﬁrst have to know where it
changes. This is exactly at the trapezoids that are intersected by si. Stated
more precisely, a trapezoid of T(Si−1) is not present in T(Si) if and only if it is
intersected by si. Our ﬁrst task is therefore to ﬁnd the intersected trapezoids.
Let ∆0,∆1,...,∆k denote these trapezoids, ordered from left to right along si.
Observe that ∆j+1 must be one of the right neighbors of ∆j. It is also easy
∆0
∆1
∆2
∆3
si
to test which neighbor it is: if rightp(∆j) lies above si, then ∆j+1 is the lower
right neighbor of ∆j, otherwise it is the upper right neighbor. This means that
once we know ∆0, we can ﬁnd ∆1,...,∆k by traversing the representation of
the trapezoidal map. So, to get started, we need to ﬁnd the trapezoid ∆0 ∈T
containing the left endpoint, p, of si. If p is not yet present in Si−1 as an
endpoint, then, because of our general position assumption, it must lie in the
interior of ∆0. This means we can ﬁnd ∆0 by a point location with p in T(Si−1).
And now comes the exciting part: at this stage in the algorithm D is a search
structure for T = T(Si−1), so all that we need to do is to perform a query on D
with the point p.
If p is already an endpoint of a segment in Si−1—remember that we allow
different segments to share an endpoint—then we must be careful. To ﬁnd ∆0,
we simply start to search in D. If p happens not to be present yet, then the query
algorithm will proceed without trouble, and end up in the leaf corresponding
to ∆0. If, however, p is already present, then the following will happen: at some
point during the search, p will lie on the vertical line through the point in an
x-node. Recall that we decided that such query points are illegal. To remedy
this, we should imagine continuing the query with a point p′ slightly to the
right of p. Replacing p by p′ is only done conceptually. What it actually means
when implementing the search is this: whenever p lies on the vertical line of
an x-node, we decide that it lies to the right. Similarly, whenever p lies on a
segment s of a y-node (this can only happen if si shares its left endpoint, p, with
s) we compare the slopes of s and si; if the slope of si is larger, we decide that p
lies above s, otherwise we decide that it is below s. With this adaptation, the
search will end in the ﬁrst trapezoid ∆0 intersected properly by si. In summary,
we use the following algorithm to ﬁnd ∆0,...,∆k.
Algorithm FOLLOWSEGMENT(T,D,si)
Input. A trapezoidal map T, a search structure D for T, and a new segment si.
Output. The sequence ∆0,...,∆k of trapezoids intersected by si.
130

Section 6.2
A RANDOMIZED INCREMENTAL
ALGORITHM
1.
Let p and q be the left and right endpoint of si.
2.
Search with p in the search structure D to ﬁnd ∆0.
3.
j ←0;
4.
while q lies to the right of rightp(∆j)
5.
do if rightp(∆j) lies above si
6.
then Let ∆j+1 be the lower right neighbor of ∆j.
7.
else Let ∆j+1 be the upper right neighbor of ∆j.
8.
j ←j +1
9.
return ∆0,∆1,...,∆j
We have seen how to ﬁnd the trapezoids intersecting si. The next step is to
update T and D. Let's start with the simple case that si is completely contained
in a trapezoid ∆= ∆0. We are in the situation depicted on the left hand side of
Figure 6.7.
∆
D(Si−1)
si
D(Si−1)
D(Si)
pi
qi
A
C
D
B
∆
T(Si−1)
si
pi
qi
A
B
C
D
T(Si)
Figure 6.7
The new segment si lies completely in
trapezoid ∆
To update T, we delete ∆from T, and replace it by four new trapezoids A, B,
C, and D. Notice that all the information we need to initialize the records for the
new trapezoids correctly (their neighbors, the segments on the top and bottom,
and the points deﬁning their left and right vertical edges) are available: they can
be determined in constant time from the segment si and the information stored
for ∆.
It remains to update D. What we must do is replace the leaf for ∆by a little
tree with four leaves. The tree contains two x-nodes, testing with the left and
right endpoint of si, and one y-node, testing with the segment si itself. This
is sufﬁcient to determine in which of the four new trapezoids A, B, C, or D a
query point lies, if we already know that it lies in ∆. The right hand side of
Figure 6.7 illustrates the modiﬁcations to the search structure. Note that one or
both endpoints of the segment si could be equal to leftp(∆) or rightp(∆). In that
131

Chapter 6
POINT LOCATION
case there would be only two or three new trapezoids, but the modiﬁcation is
done in the same spirit.
The case where si intersects two or more trapezoids is only slightly more
complicated. Let ∆0, ∆1, ..., ∆k be the sequence of intersected trapezoids.
To update T, we ﬁrst erect vertical extensions through the endpoints of si,
Figure 6.8
Segment si intersects four trapezoids
T(Si−1)
∆0
∆1
∆2
∆3
si
pi
qi
D(Si)
D(Si−1)
qi
si
si
si
si
B
A
C
D
E
F
T(Si)
A
B
D
C
E
F
si
D(Si−1)
∆0
∆1
∆2
∆3
partitioning ∆0 and ∆k into three new trapezoids each. This is only necessary for
the endpoints of si that were not already present. Then we shorten the vertical
extensions that now abut on si. This amounts to merging trapezoids along the
segment si, see Figure 6.8. Using the information stored with the trapezoids
∆0, ∆1, ..., ∆k, this step can be done in time that is linear in the number of
intersected trapezoids.
To update D, we have to remove the leaves for ∆0, ∆1, ..., ∆k, we must
create leaves for the new trapezoids, and we must introduce extra inner nodes.
More precisely, we proceed as follows. If ∆0 has the left endpoint of si in its
interior (which means it has been partitioned into three new trapezoids) then we
replace the leaf for ∆0 with an x-node for the left endpoint of si and a y-node
for the segment si. Similarly, if ∆k has the right endpoint of si in its interior, we
replace the leaf for ∆k with an x-node for the right endpoint of si and a y-node
for si. Finally, the leaves of ∆1 to ∆k−1 are replaced with single y-nodes for the
segment si. We make the outgoing edges of the new inner nodes point to the
132

Section 6.2
A RANDOMIZED INCREMENTAL
ALGORITHM
correct new leaves. Notice that, due to the fact that we have merged trapezoids
stemming from different trapezoids of T, there can be several incoming edges
for a new trapezoid. Figure 6.8 illustrates this.
We have ﬁnished the description of the algorithm that constructs T(s) and,
at the same time, builds a search structure D for it. The correctness of the
algorithm follows directly from the loop invariant (stated just after Algorithm
TRAPEZOIDALMAP), so it remains to analyze its performance.
The order in which we treat the segments has considerable inﬂuence on the
resulting search structure D and on the running time of the algorithm itself. For
some cases the resulting search structure has quadratic size and linear search
time, but for other permutations of the same set of segments the resulting search
structure is much better. As in Chapter 4, we haven't tried to determine a
good sequence; we simply took a random insertion sequence. This means that
the analysis will be probabilistic: we shall look at the expected performance
of the algorithm and of the search structure. Perhaps it's not quite clear yet
what the term "expected" means in this context. Consider a ﬁxed set S of n
non-crossing line segments. TRAPEZOIDALMAP computes a search structure
D for T(S). This structure depends on the permutation chosen in line 2. Since
there are n! possible permutations of n objects, there are n! possible ways in
which the algorithm can proceed. The expected running time of the algorithm
is the average of the running time, taken over all n! permutations. For each
permutation, a different search structure will result. The expected size of D is
the average of the sizes of all these n! search structures. Finally, the expected
query time for a point q is the average of the query time for point q, over all n!
search structures. (Notice that this not the same as the average of the maximum
query time for the search structure. Proving a bound on this quantity is a bit
more technical, so we defer that to Section 6.4.)
Theorem 6.3 Algorithm TRAPEZOIDALMAP computes the trapezoidal map
T(S) of a set S of n line segments in general position and a search structure D
for T(S) in O(nlogn) expected time. The expected size of the search structure
is O(n) and for any query point q the expected query time is O(logn).
Proof. As noted earlier, the correctness of the algorithm follows directly from
the loop invariant, so we concentrate on the performance analysis.
We start with the query time of the search structure D. Let q be a ﬁxed
query point. As the query time for q is linear in the length of the path in D that
is traversed when querying with q, it sufﬁces to bound the path length. A simple
case analysis shows that the depth of D (that is, the maximum path length)
increases by at most 3 in every iteration of the algorithm. Hence, 3n is an upper
bound on the query time for q. This bound is the best possible worst-case bound
over all possible insertion orders for S. We are not so much interested in the
worst-case case behavior, however, but in the expected behavior: we want to
bound the average query time for q with respect to the n! possible insertion
orders.
Consider the path traversed by the query for q in D. Every node on this path
was created at some iteration of the algorithm. Let Xi, for 1 ⩽i ⩽n, denote
133

Chapter 6
POINT LOCATION
the number of nodes on the path created in iteration i. Since we consider S and
q to be ﬁxed, Xi is a random variable—it depends on the random order of the
segments only. We can now express the expected path length as
E[
n
∑
i=1
Xi] =
n
∑
i=1
E[Xi].
The equality here is linearity of expectation: the expected value of a sum is
equal to the sum of the expected values.
We already observed that any iteration adds at most three nodes to the search
path for any possible query point, so Xi ⩽3. In other words, if Pi denotes the
probability that there exists a node on the search path of q that is created in
iteration i, we have
E[Xi] ⩽3Pi.
The central observation to derive a bound on Pi is this: iteration i contributes a
node to the search path of q exactly if ∆q(Si−1), the trapezoid containing q in
T(Si−1), is not the same as ∆q(Si), the trapezoid containing q in T(Si). In other
words,
Pi = Pr[∆q(Si) ̸= ∆q(Si−1)].
If ∆q(Si) is not the same as ∆q(Si−1), then ∆q(Si) must be one of the trapezoids
created in iteration i. Note that all trapezoids ∆created in iteration i are adjacent
to si, the segment that is inserted in that iteration: either top(∆) or bottom(∆) is
si, or leftp(∆) or rightp(∆) is an endpoint of si.
Now consider a ﬁxed set Si ⊂S. The trapezoidal map T(Si), and therefore
∆q(Si), are uniquely deﬁned as a function of Si; ∆q(Si) does not depend on the
order in which the segments in Si have been inserted. To bound the probability
that the trapezoid containing q has changed due to the insertion of si, we shall
use a trick we also used in Chapter 4, called backwards analysis: we consider
T(Si) and look at the probability that ∆q(Si) disappears from the trapezoidal map
when we remove the segment si. By what we said above, ∆q(Si) disappears if
and only if one of top(∆q(Si)), bottom(∆q(Si)), leftp(∆q(Si)), or rightp(∆q(Si))
disappears with the removal of si. What is the probability that top(∆q(Si))
disappears? The segments of Si have been inserted in random order, so every
segment in Si is equally likely to be si. This means that the probability that si
happens to be top(∆q(Si)) is 1/i. (If top(∆q(Si)) is the top edge of the rectangle
R surrounding the scene, then the probability is even zero.) Similarly, the
probability that si happens to be bottom(∆q(Si)) is at most 1/i. There can be
many segments sharing the point leftp(∆q(Si)). Hence, the probability that si
is one of these segments can be large. But leftp(∆q(Si)) disappears only if si is
the only segment in Si with leftp(∆q(Si)) as an endpoint. Hence, the probability
that leftp(∆q(Si)) disappears is at most 1/i as well. The same holds true for
rightp(∆q(Si)). Hence, we can conclude that
Pi = Pr[∆q(Si) ̸= ∆q(Si−1)] = Pr[∆q(Si) ̸∈T(Si−1)] ⩽4/i.
(A small technical point: In the argument above we ﬁxed the set Si. This means
that the bound we derived on Pi holds under the condition that Si is this ﬁxed
134

Section 6.2
A RANDOMIZED INCREMENTAL
ALGORITHM
set. But since the bound does not depend on what the ﬁxed set actually is, the
bound holds unconditionally.)
Putting it all together we get the bound on the expected query time:
E[
n
∑
i=1
Xi] ⩽
n
∑
i=1
3Pi ⩽
n
∑
i=1
12
i = 12
n
∑
i=1
1
i = 12Hn.
Here, Hn is the n-th harmonic number, deﬁned as
Hn := 1
1 + 1
2 + 1
3 +···+ 1
n
Harmonic numbers arise quite often in the analysis of algorithms, so it's good
to remember the following bound that holds for all n > 1:
lnn < Hn < lnn+1.
(It can be derived by comparing Hn to the integral
! n
1 1/x dx = lnn.) We
conclude that the expected query time for a query point q is O(logn), as claimed.
Let's now turn to the size of D. To bound the size, it sufﬁces to bound the
number of nodes in D. We ﬁrst note that the leaves in D are in one-to-one
correspondence with the trapezoids in ∆, of which there are O(n) by Lemma 6.2.
This means that the total number of nodes is bounded by
O(n)+
n
∑
i=1
(number of inner nodes created in iteration i).
Let ki be the number of new trapezoids that are created in iteration i, due to the
insertion of segment si. In other words, ki is the number of new leaves in D.
The number of inner nodes created in iteration i is exactly equal to ki −1. A
simple worst case upper bound on ki follows from the fact that the number of
new trapezoids in T(Si) can obviously not be larger than the total number of
trapezoids in T(Si), which is O(i). This leads to a worst-case upper bound on
the size of the structure of
O(n)+
n
∑
i=1
O(i) = O(n2).
Indeed, if we have bad luck and the order in which we insert the segments is
very unfortunate, then the size of D can be quadratic. However, we are more
interested in the expected size of the data structure, over all possible insertion
orders. Using linearity of expectation, we ﬁnd that this expected size is bounded
by
O(n)+E[
n
∑
i=1
(ki −1)] = O(n)+
n
∑
i=1
E[ki].
It remains to bound the expected value of ki. We already prepared the necessary
tools for this when we derived the bound on the query time. Consider a ﬁxed
set Si ⊆S. For a trapezoid ∆∈T(Si) and a segment s ∈Si, let
δ(∆,s) :=
"
1
if ∆disappears from T(Si) when s is removed from Si,
0
otherwise.
135

Chapter 6
POINT LOCATION
In the analysis of the query time we observed that there are at most four segments
that cause a given trapezoid to disappear. Hence,
∑
s∈Si ∑
∆∈T(Si)
δ(∆,s) ⩽4|T(Si)| = O(i).
Now, ki is the number of trapezoids created by the insertion of si, or, equivalently,
the number of trapezoids in T(Si) that disappear when si is removed. Since si
is a random element of Si, we can ﬁnd the expected value of ki by taking the
average over all s ∈Si:
E[ki] = 1
i ∑
s∈Si ∑
∆∈T(Si)
δ(∆,s) ⩽O(i)
i
= O(1).
We conclude that the expected number of newly created trapezoids is O(1) in
every iteration of the algorithm, from which the O(n) bound on the expected
amount of storage follows.
It remains to bound the expected running time of the construction algorithm.
Given the analysis of the query time and storage, this is easy. We only need to
observe that the time to insert segment si is O(ki) plus the time needed to locate
the left endpoint of si in T(Si−1). Using the earlier derived bounds on ki and the
query time, we immediately get that the expected running time of the algorithm
is
O(1)+
n
∑
i=1
#
O(logi)+O(E[ki])
$
= O(nlogn).
This completes the proof.
Note once again that the expectancy in Theorem 6.3 is solely with respect
to the random choices made by the algorithm; we do not average over possible
choices for the input. Hence, there are no bad inputs: for any input set of n line
segments, the expected running time of the algorithm is O(nlogn).
As discussed earlier, Theorem 6.3 does not guarantee anything about the
expected maximum query time over all possible query points. In Section 6.4,
however, it is proved that the expected maximum query time is O(logn) as well.
Hence, we can build a data structure of expected size O(n), whose expected
query time is O(logn). This also proves the existence of a data structure with
O(n) size and O(logn) query time for any query point q—see Theorem 6.8.
Finally we go back to our original problem: point location in a planar subdivi-
sion S. We assume that S is given as a doubly-connected edge list with n edges.
We use algorithm TRAPEZOIDALMAP to compute a search structure D for the
trapezoidal map of the edges of S. To use this search structure for point location
in S, however, we still need to attach to every leaf of D a pointer to the face f
of S that contains the trapezoid of T(S) corresponding to that leaf. But this
is rather easy: recall from Chapter 2 that the doubly-connected edge list of S
stores with every half-edge a pointer to the face that is incident to the left. The
136

Section 6.3
DEALING WITH DEGENERATE CASES
face that is incident to the right can be found in constant time from Twin(⃗e). So
for every trapezoid ∆of T(S) we simply look at the face of S incident to top(∆)
from below. If top(∆) is the top edge of R, then ∆is contained in the unique
unbounded face of S.
In the next section we show that the assumption that the segments be in
general position may be dropped, which leads to a less restricted version of
Theorem 6.3. It implies the following corollary.
Corollary 6.4 Let S be a planar subdivision with n edges. In O(nlogn) expected
time one can construct a data structure that uses O(n) expected storage, such that
for any query point q, the expected time for a point location query is O(logn).
6.3
Dealing with Degenerate Cases
In the previous sections we made two simplifying assumptions. First of all, we
assumed that the set of line segments was in general position, meaning that no
two distinct endpoints have the same x-coordinate. Secondly, we assumed that a
query point never lies on the vertical line of an x-node on its search path, nor on
the segment of a y-node. We now set out to get rid of these assumptions.
We ﬁrst show how to avoid the assumption that no two distinct endpoints lie on
a common vertical line. The crucial observation is that the vertical direction
chosen to deﬁne the trapezoidal map of the set of line segments was immaterial.
Therefore we can rotate the axis-system slightly. If the rotation angle is small
enough, then no two distinct endpoints will lie on the same vertical line anymore.
Rotations by very small angles, however, lead to numerical difﬁculties. Even if
the input coordinates are integer, we need a signiﬁcantly higher precision to do
the calculations properly. A better approach is to do the rotation symbolically. In
Chapter 5 we have seen another form of a symbolic transformation, composite
numbers, which was used to deal with the case where data points have the same
x- or y-coordinate. In this chapter, we will have another look at such a symbolic
perturbation, and will try to interpret it geometrically.
x
y
x
y
ε
1
ϕ*→
Figure 6.9
The shear transformation
It will be convenient not to use rotation, but to use an afﬁne mapping called
shear transformation. In particular, we shall use the shear transformation along
137

Chapter 6
POINT LOCATION
the x-axis by some value ε > 0:
ϕ :
%x
y
&
*→
%x+εy
y
&
.
Figure 6.9 illustrates the effect of this shear transform. It maps a vertical line
to a line with slope 1/ε, so that any two distinct points on the same vertical
line are mapped to points with distinct x-coordinates. Furthermore, if ε > 0 is
small enough, then the transformation does not reverse the order in x-direction
of the given input points. It is not difﬁcult to compute an upper bound for ε to
guarantee that this property holds. In the following, we will assume that we
have a value of ε > 0 available that is small enough that the order of points is
conserved by the shear transform. Surprisingly, we will later realize that we do
not need to compute such an actual value for ε.
Given a set S of n arbitrary non-crossing line segments, we will run algorithm
TRAPEZOIDALMAP on the set ϕS := {ϕs : s ∈S}. As noted earlier, however,
actually performing the transformation may lead to numerical problems, and
therefore we use a trick: a point ϕ p = (x+εy,y) will simply be stored as (x,y).
This is a unique representation. We only need to make sure that the algorithm
treats the segments represented in this way correctly. Here it comes handy
that the algorithm does not compute any geometric objects: we never actually
compute the coordinates of the endpoints of vertical extensions, for instance.
All it does is to apply two types of elementary operations to the input points.
The ﬁrst operation takes two distinct points p and q and decides whether q lies
to the left, to the right, or on the vertical line through p. The second operation
takes one of the input segments, speciﬁed by its two endpoints p1 and p2, and
tests whether a third point q lies above, below, or on this segment. This second
operation is only applied when we already know that a vertical line through q
intersects the segment. All the points p, q, p1, and p2 are endpoints of segments
in the input set S. (You may want to go through the description of the algorithm
again, verifying that it can indeed be realized with these two operations alone.)
Let's look at how to apply the ﬁrst operation to two transformed points
ϕ p and ϕq. These points have coordinates (xp + εyp,yp) and (xq + εyq,yq),
respectively. If xq ̸= xp, then the relation between xq and xp determines the
outcome of the test—after all, we had chosen ε to have this property. If xq = xp,
then the relation between yq and yp decides the horizontal order of the points.
Therefore there is a strict horizontal order for any pair of distinct points. It
follows that ϕq will never lie on the vertical line through ϕ p, except when p
and q coincide. But this is exactly what we need, since no two distinct input
points should have the same x-coordinate.
For the second operation, we are given a segment ϕs with endpoints ϕ p1 =
(x1+εy1,y1) and ϕ p2 = (x2+εy2,y2), and we want to test whether a point ϕq =
(x+εy,y) lies above, below, or on ϕs. The algorithm ensures that whenever we
do this test, the vertical line through ϕq intersects ϕs. In other words,
x1 +εy1 ⩽x+εy ⩽x2 +εy2.
138

Section 6.3
DEALING WITH DEGENERATE CASES
This implies that x1 ⩽x ⩽x2. Moreover, if x = x1 then y ⩾y1, and if x = x2
then y ⩽y2. Let's now distinguish two cases.
If x1 = x2, the untransformed segment s is vertical. Since now x1 = x = x2,
we have y1 ⩽y ⩽y2, which implies that q lies on s. Because the afﬁne mapping
ϕ is incidence preserving—if two points coincide before the transformation,
they do so afterwards—we can conclude that ϕq lies on ϕs.
Now consider the case where x1 < x2. Since we already know that the
vertical line through ϕq intersects ϕs, it is good enough to do the test with ϕs.
Now we observe that the mapping ϕ preserves the relation between points and
lines: if a point is above (or on, or below) a given line, then the transformed
point is above (or on, or below) the transformed line. Hence, we can simply
perform the test with the untransformed point q and segment s.
This shows that to run the algorithm on ϕS instead of on S, the only mod-
iﬁcation we have to make is to compare points lexicographically, when we
want to determine their horizontal order. Of course, the algorithm will com-
pute the trapezoidal map for ϕS, and a search structure for T(ϕS). Note that,
as promised, we never actually needed the value of ε, so there is no need to
compute such a value at the beginning. All we needed was that ε is small
enough.
Using our shear transformation, we got rid of the assumption that any two
distinct endpoints should have distinct x-coordinates. What about the restriction
that a query point never lies on the vertical line of an x-node on the search path,
nor on the segment of a y-node? Our approach solves this problem as well, as
we show next.
Since the constructed search structure is for the transformed map T(ϕS),
we will also have to use the transformed query point ϕq when doing a query.
In other words, all the comparisons we have to do during the search must be
done in the transformed space. We already know how to do the tests in the
transformed space:
At x-nodes we have to do the test lexicographically. As a result, no two
distinct points will ever lie on a vertical line. (Trick question: How can this be
true, if the transformation ϕ is bijective?) This does not mean that the outcome
of the test at an x-node is always "to the right" or "to the left". The outcome
can also be "on the line". This can only happen, however, when the query point
coincides with the endpoint stored at the node—and this answers the query!
At y-nodes we have to test the transformed query point against a transformed
segment. The test described above can have three outcomes: "above", "below",
or "on". In the ﬁrst two cases, there is no problem, and we can descend to the
corresponding child of the y-node. If the outcome of the test is "on", then the
untransformed point lies on the untransformed segment as well, and we can
report that fact as the answer to the query.
We have generalized Theorem 6.3 to arbitrary sets of non-crossing line segments.
Theorem 6.5 Algorithm TRAPEZOIDALMAP computes the trapezoidal map
T(S) of a set S of n non-crossing line segments and a search structure D for
139

Chapter 6
POINT LOCATION
T(S) in O(nlogn) expected time. The expected size of the search structure is
O(n) and for any query point q the expected query time is O(logn).
6.4*
A Tail Estimate
Theorem 6.5 states that for any query point q, the expected query time is
O(logn). This is a rather weak result. In fact, there is no reason to expect that
the maximum query time of the search structure is small: it may be that for any
permutation of the set of segments the resulting search structure has a bad query
time for some query point. In this section we shall prove that there is no need to
worry: the probability that the maximum query time is bad is very small. To
this end we ﬁrst prove the following high-probability bound.
Lemma 6.6 Let S be a set of n non-crossing line segments, let q be a query
point, and let λ be a parameter with λ > 0. Then the probability that the search
path for q in the search structure computed by Algorithm TRAPEZOIDALMAP
has more than 3λ ln(n+1) nodes is at most 1/(n+1)λ ln1.25−1.
Proof. We would like to deﬁne random variables Xi, with 1 ⩽i ⩽n, such that
Xi is 1 if at least one node on the search path for q has been created in iteration i
of the algorithm, and 0 if no such node has been created. Unfortunately, the
random variables deﬁned this way are not independent. (We did not need
independence in the proof of Theorem 6.3, but we shall need it now.) Therefore,
we use a little trick.
We deﬁne a directed acyclic graph G, with one source and one sink. Paths
in G from the source to the sink correspond to the permutations of S. The graph
G is deﬁned as follows. There is a node for every subset of S, including one
for the empty set. With a slight abuse of terminology, we shall often speak of
"the subset S′ " when we mean "the node corresponding to the subset S′ ". It is
convenient to imagine the nodes as being grouped into n+1 layers, such that
layer i contains the subsets of cardinality i. Notice that layers 0 and n both
have exactly one node, which correspond to the empty set and to the set S,
respectively. A node in layer i has outgoing arcs to some of the nodes in layer
i+1. More precisely, a subset S′ of cardinality i has outgoing arcs to the subsets
S′′ of cardinality i+1 if and only if that S′ ⊂S′′. In other words, a subset S′ has
an outgoing arc to a subset S′′ if S′′ can be obtained by adding one segment of
S to S′. We label the arc with this segment. Note that a subset S′ in layer i has
{1,2}
{3,4}
{1,2,3}
{2,3,4}
/0
{1,2,3,4}
{1}
{2}
{3}
{4}
exactly i incoming arcs, each labeled with a segment in S′, and exactly n −i
outgoing arcs, each labeled with a segment in S\S′.
Directed paths from the source to the sink in G now correspond one-to-one
to permutations of S, which correspond to possible executions of the algorithm
TRAPEZOIDALMAP. Consider an arc of G from subset S′ in layer i to subset S′′
in layer i+1. Let the segment s be the label of the arc. The arc represents the
insertion of s into the trapezoidal map of S′. We mark this arc if this insertion
changes the trapezoid containing point q. To be able to say something about
the number of marked arcs, we use the backwards analysis argument from the
140

Section 6.4*
A TAIL ESTIMATE
proof of Theorem 6.3: there are at most four segments that change the trapezoid
containing q when they are removed from a subset S′′. This means that any node
in G has at most four marked incoming arcs. But it is possible that a node has
less than four marked incoming arcs. In that case, we simply mark some other,
arbitrary incoming arcs, so that the number of marked incoming arcs is exactly
four. For nodes in the ﬁrst three layers, which have less than four incoming arcs,
we mark all incoming arcs.
We want to analyze the expected number of steps during which the trapezoid
containing q changes. In other words, we want to analyze the expected number
of marked edges on a source-to-sink path in G. To this end we deﬁne the random
variable Xi as follows:
Xi :=
"
1
if the i-th arc on the sink-to-source path in G is marked,
0
otherwise.
Note the similarity of the deﬁnition of Xi and the deﬁnition of Xi on page 134.
The i-th arc on the path is from a node in layer i−1 to a node in layer i, and
each such arc is equally likely to be the i-th arc. Since each node in layer i
has i incoming arcs, exactly four of which are marked (assuming i ⩾4), this
implies that Pr[Xi = 1] = 4/i for i ⩾4. For i < 4 we have Pr[Xi = 1] = 1 < 4/i.
Moreover, we note that the Xi are independent (unlike the random variables Xi
deﬁned on page 134).
Let Y := ∑n
i=1 Xi. The number of nodes on the search path for q is at most
3Y, and we will bound the probability that Y exceeds λ ln(n+1). We will use
Markov's inequality, which states that for any nonnegative random variable Z
and any α > 0 we have
Pr[Z ⩾α] ⩽E[Z]
α .
So for any t > 0 we have
Pr[Y ⩾λ ln(n+1)] = Pr[etY ⩾etλ ln(n+1)] ⩽e−tλ ln(n+1) E[etY].
Recall that the expected value of the sum of random variables is the sum of
the expected values. In general it is not true that the expected value of a
product is the product of the expected values. But for random variables that are
independent it is true. Our Xi are independent, so we have
E[etY] = E[e∑i tXi] = E[∏
i
etXi] = ∏
i
E[etXi].
If we choose t = ln1.25, we get
E[etXi] ⩽et 4
i +e0(1−4
i ) = (1+1/4)4
i +1−4
i = 1+ 1
i = 1+i
i
,
and we have
n
∏
i=1
E[etXi] ⩽2
1
3
2 ··· n+1
n
= n+1.
141

Chapter 6
POINT LOCATION
Putting everything together, we get the bound we want to prove:
Pr[Y ⩾λ ln(n+1)] ⩽e−λt ln(n+1)(n+1) =
n+1
(n+1)λt = 1/(n+1)λt−1.
We use this lemma to prove a bound on the expected maximum query time.
Lemma 6.7 Let S be a set of n non-crossing line segments, and let λ be a
parameter with λ > 0. Then the probability that the maximum length of a
search path in the structure for S computed by Algorithm TRAPEZOIDALMAP
is more than 3λ ln(n+1) is at most 2/(n+1)λ ln1.25−3.
Proof. We will call two query points q and q′ equivalent if they follow the same
path through the search structure D. Partition the plane into vertical slabs by
passing a vertical line through every endpoint of S. Partition every slab into
trapezoids by intersecting it with all possible segments in S. This deﬁnes a
decomposition of the plane into at most 2(n+1)2 trapezoids. Any two points
lying in the same trapezoid of this decomposition must be equivalent in every
possible search structure for S. After all, the only comparisons made during a
search are the test whether the query point lies to the left or to the right of the
vertical line through a segment endpoint, and the test whether the query point
lies above or below a segment.
This implies that to bound the maximum length of a search path in D, it
sufﬁces to consider the search paths of at most 2(n+1)2 query points, one in
each of these trapezoids. By Lemma 6.6, the probability that the length of the
search path for a ﬁxed point q exceeds 3λ ln(n+1) is at most 1/(n+1)λ ln1.25−1.
In the worst case, the probability that the length of the search path for any one of
the 2(n+1)2 test points exceeds the bound is therefore at most 2(n+1)2/(n+
1)λ ln1.25−1.
This lemma implies that the expected maximum query time is O(logn).
Take for instance λ = 20. Then the probability that the maximum length of a
search path in D is more than 3λ ln(n+1) is at most 2/(n+1)1.4, which is less
than 1/4 for n > 4. In other words, the probability that D has a good query time
is at least 3/4. Similarly one can show that the probability that the size of D is
O(n) is at least 3/4. We already noted that if the maximum length of a search
path in the structure is O(logn), and the size is O(n), then the running time of
the algorithm is O(nlogn). Hence, the probability that the query time, the size,
and the construction time are good is at least 1/2.
Now we can also construct a search structure that has O(logn) worst-case
query time and uses O(n) space in the worst case. What we do is the following.
We run Algorithm TRAPEZOIDALMAP on the set S, keeping track of the size and
the maximum length of a query path of the search structure being created. As
soon as the size exceeds c1n, or the depth exceeds c2 logn, for suitably chosen
constants c1 and c2, we stop the algorithm, and start it again from the beginning,
with a fresh random permutation. Since the probability that a permutation leads
to a data structure with the desired size and depth is at least 1/4, we expect to
be ﬁnished in four trials. (In fact, for large n the probability is almost one, so
142

Section 6.5
NOTES AND COMMENTS
the expected number of trials is only slightly larger than one.) This leads to the
following result.
Theorem 6.8 Let S be a planar subdivision with n edges. There exists a point
location data structure for S that uses O(n) storage and has O(logn) query time
in the worst case.
The constants in this example are not very convincing—a query that visits
60lnn nodes isn't really so attractive. However, much better constants can be
proven with the same technique.
The theorem above does not say anything about preprocessing time. Indeed,
to keep track of the maximum length of a query path, we need to consider
2(n + 1)2 test points—see Lemma 6.7. It is possible to reduce this to only
O(nlogn) test points, which makes the expected preprocessing time O(nlog2 n).
6.5
Notes and Comments
The point location problem has a long history in computational geometry. Early
results are surveyed by Preparata and Shamos [323]; Snoeyink [361] gives
a more recent survey. Of all the methods suggested for the problem, four
basically different approaches lead to optimal O(logn) search time, O(n) storage
solutions. These are the chain method by Edelsbrunner et al. [161], which is
based on segment trees and fractional cascading (see also Chapter 10), the
triangulation reﬁnement method by Kirkpatrick [236], the use of persistency
by Sarnak and Tarjan [336] and Cole [135], and the randomized incremental
method by Mulmuley [289]. Our presentation here follows Seidel's [345]
presentation and analysis of Mulmuley's algorithm.
Quite a lot of recent research has gone into dynamic point location, where
the subdivision can be modiﬁed by adding and deleting edges [41, 115, 120, 19].
A (now somewhat outdated) survey on dynamic point location is given by
Chiang and Tamassia [121].
In more than two dimensions, the point location problem is still not fully
resolved. A general structure for convex subdivisions in three dimensions is
given by Preparata and Tamassia [324]. One can also use dynamic planar
point-location structures together with persistency techniques to obtain a static
three-dimensional point-location structure using O(nlogn) storage and with
O(log2 n) query time [361]. No structure with linear storage and O(logn) query
time is known. In higher dimensions, efﬁcient point-location structures are
only known for special subdivisions, such as arrangements of hyperplanes [95,
104, 131]. Considering the subdivision induced by a set H of n hyperplanes in
d-dimensional space, it is well known that the combinatorial complexity of this
subdivision (the number of vertices, edges, and so on) is Θ(nd) in the worst case
[158]—see also the notes and comments of Chapter 8. Chazelle and Friedman
[104] have shown that such subdivisions can be stored using O(nd) space, such
that point location queries take O(logn) time. Other special subdivisions that
allow for efﬁcient point location are convex polytopes [131, 266], arrangements
of triangles [59], and arrangements of algebraic varieties [102].
143

Chapter 6
POINT LOCATION
Other point location problems in 3- and higher-dimensional space that can
be solved efﬁciently include those where assumptions on the shape of the cells
are made. Two examples are rectangular subdivisions [57, 162], and so-called
fat subdivisions [51, 302, 309].
Usually, a point location query aks for the label of the cell of a subdivision
that contains a given query point. For point location in a convex polytope
in d-dimensional space, that means that there are only two possible answers:
inside the polytope, or on the outside. Therefore, one could hope for point
location structures that require considerably less storage than the combinatorial
complexity of the subdivision. This can be as much as Θ(n⌊d/2⌋) for convex
polytopes deﬁned by the intersection of n half-spaces [158]—see also the notes
and comments of Chapter 11. Indeed, an O(n) size data structure exists in
which queries take O(n1−1/⌊d/2⌋logO(1) n) time [264]. For arrangements of line
segments in the plane, other implicit point location structures have been given
that require less than quadratic space, even if the arrangement of line segments
has quadratic complexity [7, 160].
6.6
Exercises
6.1
Draw the graph of the search structure D for the set of segments depicted
in the margin, for some insertion order of the segments.
6.2
Give an example of a set of n line segments with an order on them that
makes the algorithm create a search structure of size Θ(n2) and worst-case
query time Θ(n).
6.3
In this chapter we have looked at the point location problem with pre-
processing. We have not looked at the single shot problem, where the
subdivision and the query point are given at the same time, and we have
no special preprocessing to speed up the searches. In this exercise and
some of the following ones, we have a look at such problems.
Given a simple polygon P with n vertices and a query point q, here is
an algorithm to determine whether q lies in P. Consider the ray ρ :=
{(qx +λ,qy) : λ > 0} (this is the horizontal ray starting in q and going
rightwards). Determine for every edge e of P whether it intersects ρ. If
the number of intersecting edges is odd, then q ∈P, otherwise q ̸∈P.
Prove that this algorithm is correct, and explain how to deal with degen-
erate cases. (One degenerate case is when ρ intersects an endpoint of
an edge. Are there other special cases?) What is the running time of the
algorithm?
6.4
Show that, given a planar subdivision S with n vertices and edges and a
query point q, the face of S containing q can be computed in time O(n).
Assume that S is given in a doubly-connected edge list.
144

Section 6.6
EXERCISES
6.5
Given a convex polygon P as an array of its n vertices in sorted order
along the boundary. Show that, given a query point q, it can be tested in
time O(logn) whether q lies inside P.
6.6
Given a y-monotone polygon P as an array of its n vertices in sorted
order along the boundary. Can you generalize the solution to the previous
exercise to y-monotone polygons?
P
p
6.7
A polygon P is called star-shaped if a point p in the interior of P exists
such that, for any other point q in P, the line segment pq lies in P. Assume
that such a point p is given with the star-shaped polygon P. As in the
previous two exercises the vertices of P are given in sorted order along
the boundary in an array. Show that, given a query point q, it can be tested
in time O(logn) whether q lies inside P. What if P is star-shaped, but the
point p is not given?
6.8
Design a deterministic algorithm, that is, one that doesn't make random
choices, to compute the trapezoidal map of a set of non-crossing line
segments. Use the plane sweep paradigm from Chapter 2. The worst-case
running time of the algorithm should be O(nlogn).
6.9* Give a randomized algorithm to compute in O(nlogn+A) expected time
all pairs of intersecting segments in a set of n line segments, where A is
the number of intersecting pairs.
6.10 Design an algorithm with running time O(nlogn) for the following prob-
lem: Given a set P of n points, determine a value of ε > 0 such that the
shear transformation ϕ : (x,y) *→(x+εy,y) does not change the order (in
x-direction) of points with unequal x-coordinates.
6.11 Let S be a set of non-crossing segments in the plane, and let s be a new
segment not crossing any of the segments in S. Prove that a trapezoid ∆
of T(S) is also a trapezoid of T(S∪{s}) if and only if s does not intersect
the interior of ∆.
6.12 Prove that the number of inner nodes of the search structure D of algo-
rithm TRAPEZOIDALMAP increases by ki −1 in iteration i, where ki is
the number of new trapezoids in T(Si) (and hence the number of new
leaves of D).
6.13 Use a plane sweep argument to prove that the trapezoidal map of n line
segments in general position has at most 3n + 1 trapezoids. (Imagine
a vertical line sweeping over the plane from left to right, stopping at
all endpoints of segments. Count the number of trapezoids that are
encountered by the sweep line.)
6.14 We have deﬁned the trapezoidal map of a set S of n line segments only for
the case that S is in general position. Give a deﬁnition for the trapezoidal
map T(S) of an arbitrary set of segments. Prove that the upper bound of
3n+1 for the number of trapezoids still holds.
145

Chapter 6
POINT LOCATION
6.15 Although we have started with the point location problem on the surface
of the earth, we have only treated planar point location. But the earth is
a globe. How would you deﬁne a spherical subdivision—a subdivision
of the surface of a sphere? Give a point location structure for such a
subdivision.
6.16 The ray shooting problem occurs in computer graphics (see Chapter 8).
A 2-dimensional version can be given as follows: Store a set S of n
non-crossing line segments such that one can quickly answer queries
of the type: "Given a query ray ρ—a ray is a half-line starting at some
point—ﬁnd the ﬁrst segment in S intersected by ρ." (We leave it to you to
deﬁne the behavior for degenerate cases.)
In this exercise, we look at vertical ray shooting, where the query ray
must be a vertical ray pointing upwards. Only the starting point need be
speciﬁed in such a query.
Give a data structure for the vertical ray shooting problem for a set S of
n non-crossing line segments in general position. Bound the query time
and storage requirement of your data structure. What is the preprocessing
time?
Can you do the same when the segments are allowed to intersect each
other?
6.17* Prove a version of Theorem 6.8 that gives upper bounds for the number of
nodes and the depth of the search structure without using order notation
(O-bounds). The details in the text that lead to Theorem 6.8 should be
modiﬁed so that it gives better constants.
146

7
Voronoi Diagrams
The Post Oﬃce Problem
Suppose you are on the advisory board for the planning of a supermarket chain,
and there are plans to open a new branch at a certain location. To predict whether
the new branch will be proﬁtable, you must estimate the number of customers it
will attract. For this you have to model the behavior of your potential customers:
how do people decide where to do their shopping? A similar question arises in
social geography, when studying the economic activities in a country: what is
the trading area of certain cities? In a more abstract setting we have a set of
Figure 7.1
The trading areas of the capitals of the
twelve provinces in the Netherlands, as
predicted by the Voronoi assignment
model
central places—called sites—that provide certain goods or services, and we want
to know for each site where the people live who obtain their goods or services
from that site. (In computational geometry the sites are traditionally viewed
as post ofﬁces where customers want to post their letters—hence the subtitle
of this chapter.) To study this question we make the following simplifying
assumptions:
the price of a particular good or service is the same at every site;
the cost of acquiring the good or service is equal to the price plus the cost
of transportation to the site;
147

Chapter 7
VORONOI DIAGRAMS
the cost of transportation to a site equals the Euclidean distance to the site
times a ﬁxed price per unit distance;
consumers try to minimize the cost of acquiring the good or service.
Usually these assumptions are not completely satisﬁed: goods may be cheaper
at some sites than at others, and the transportation cost between two points is
probably not linear in the Euclidean distance between them. But the model
above can give a rough approximation of the trading areas of the sites. Areas
where the behavior of the people differs from that predicted by the model can
be subjected to further research, to see what caused the different behavior.
Our interest lies in the geometric interpretation of the model above. The assump-
tions in the model induce a subdivision of the total area under consideration
into regions—the trading areas of the sites—such that the people who live in the
same region all go to the same site. Our assumptions imply that people simply
get their goods at the nearest site—a fairly realistic situation. This means that
the trading area for a given site consists of all those points for which that site is
closer than any other site. Figure 7.1 gives an example. The sites in this ﬁgure
are the capitals of the twelve provinces in the Netherlands.
The model where every point is assigned to the nearest site is called the
Voronoi assignment model. The subdivision induced by this model is called the
Voronoi diagram of the set of sites. From the Voronoi diagram we can derive
all kinds of information about the trading areas of the sites and their relations.
For example, if the regions of two sites have a common boundary then these
two sites are likely to be in direct competition for customers that live in the
boundary region.
The Voronoi diagram is a versatile geometric structure. We have described
an application to social geography, but the Voronoi diagram has applications
in physics, astronomy, robotics, and many more ﬁelds. It is also closely linked
to another important geometric structure, the so-called Delaunay triangulation,
which we shall encounter in Chapter 9. In the current chapter we shall conﬁne
ourselves to the basic properties and the construction of the Voronoi diagram of
a set of point sites in the plane.
7.1
Deﬁnition and Basic Properties
Denote the Euclidean distance between two points p and q by dist(p,q). In the
plane we have
dist(p,q) :=
!
(px −qx)2 +(py −qy)2.
Let P := {p1, p2,..., pn} be a set of n distinct points in the plane; these points
are the sites. We deﬁne the Voronoi diagram of P as the subdivision of the plane
into n cells, one for each site in P, with the property that a point q lies in the
cell corresponding to a site pi if and only if dist(q, pi) < dist(q, pj) for each
pj ∈P with j ̸= i. We denote the Voronoi diagram of P by Vor(P). Abusing the
terminology slightly, we will sometimes use 'Vor(P)' or 'Voronoi diagram' to
indicate only the edges and vertices of the subdivision. For example, when we
148

Section 7.1
DEFINITION AND BASIC PROPERTIES
say that a Voronoi diagram is connected we mean that the union of its edges and
vertices forms a connected set. The cell of Vor(P) that corresponds to a site pi
is denoted V(pi); we call it the Voronoi cell of pi. (In the terminology of the
introduction to this chapter: V(pi) is the trading area of site pi.)
We now take a closer look at the Voronoi diagram. First we study the
structure of a single Voronoi cell. For two points p and q in the plane we deﬁne
the bisector of p and q as the perpendicular bisector of the line segment pq. This
bisector splits the plane into two half-planes. We denote the open half-plane
that contains p by h(p,q) and the open half-plane that contains q by h(q, p).
Notice that r ∈h(p,q) if and only if dist(r, p) < dist(r,q). From this we obtain
the following observation.
Observation 7.1 V(pi) = "
1⩽j⩽n,j̸=i h(pi, p j).
Thus V(pi) is the intersection of n−1 half-planes and, hence, a (possibly
unbounded) open convex polygonal region bounded by at most n−1 vertices
and at most n−1 edges.
What does the complete Voronoi diagram look like? We just saw that each
cell of the diagram is the intersection of a number of half-planes, so the Voronoi
diagram is a planar subdivision whose edges are straight. Some edges are line
segments and others are half-lines. Unless all sites are collinear there will be no
edges that are full lines:
Theorem 7.2 Let P be a set of n point sites in the plane. If all the sites are
collinear then Vor(P) consists of n −1 parallel lines. Otherwise, Vor(P) is
connected and its edges are either segments or half-lines.
Proof. The ﬁrst part of the theorem is easy to prove, so assume that not all sites
in P are collinear.
We ﬁrst show that the edges of Vor(P) are either segments or half-lines. We
already know that the edges of Vor(P) are parts of straight lines, namely parts of
the bisectors between pairs of sites. Now suppose for a contradiction that there
is an edge e of Vor(P) that is a full line. Let e be on the boundary of the Voronoi
cells V(pi) and V(pj). Let pk ∈P be a point that is not collinear with pi and pj.
The bisector of pj and pk is not parallel to e and, hence, it intersects e. But then
pi
p j
pk
e
the part of e that lies in the interior of h(pk, p j) cannot be on the boundary of
V(pj), because it is closer to pk than to pj, a contradiction.
It remains to prove that Vor(P) is connected. If this were not the case
then there would be a Voronoi cell V(pi) splitting the plane into two. Because
Voronoi cells are convex, V(pi) would consist of a strip bounded by two parallel
full lines. But we just proved that the edges of the Voronoi diagram cannot be
full lines, a contradiction.
Now that we understand the structure of the Voronoi diagram we investigate
its complexity, that is, the total number of its vertices and edges. Since there are
n sites and each Voronoi cell has at most n−1 vertices and edges, the complexity
of Vor(P) is at most quadratic. It is not clear, however, whether Vor(P) can
actually have quadratic complexity: it is easy to construct an example where
a single Voronoi cell has linear complexity, but can it happen that many cells
149

Chapter 7
VORONOI DIAGRAMS
have linear complexity? The following theorem shows that this is not the case
and that the average number of vertices of the Voronoi cells is less than six.
Theorem 7.3 For n ⩾3, the number of vertices in the Voronoi diagram of a set
of n point sites in the plane is at most 2n−5 and the number of edges is at most
3n−6.
Proof. If the sites are all collinear then the theorem immediately follows from
Theorem 7.2, so assume this is not the case. We prove the theorem using Euler's
formula, which states that for any connected planar embedded graph with mv
nodes, me arcs, and mf faces the following relation holds:
mv −me +mf = 2.
We cannot apply Euler's formula directly to Vor(P), because Vor(P) has half-
v∞
inﬁnite edges and is therefore not a proper graph. To remedy the situation we
add one extra vertex v∞"at inﬁnity" to the set of vertices and we connect all
half-inﬁnite edges of Vor(P) to this vertex. We now have a connected planar
graph to which we can apply Euler's formula. We obtain the following relation
between nv, the number of vertices of Vor(P), ne, the number of edges of Vor(P),
and n, the number of sites:
(nv +1)−ne +n = 2.
(7.1)
Moreover, every edge in the augmented graph has exactly two vertices, so if
we sum the degrees of all vertices we get twice the number of edges. Because
every vertex, including v∞, has degree at least three we get
2ne ⩾3(nv +1).
(7.2)
Together with equation (7.1) this implies the theorem.
We close this section with a characterization of the edges and vertices of the
Voronoi diagram. We know that the edges are parts of bisectors of pairs of sites
and that the vertices are intersection points between these bisectors. There is
a quadratic number of bisectors, whereas the complexity of the Vor(P) is only
linear. Hence, not all bisectors deﬁne edges of Vor(P) and not all intersections
are vertices of Vor(P). To characterize which bisectors and intersections deﬁne
features of the Voronoi diagram we make the following deﬁnition. For a point
q we deﬁne the largest empty circle of q with respect to P, denoted by CP(q),
as the largest circle with q as its center that does not contain any site of P in
q
CP(q)
its interior. The following theorem characterizes the vertices and edges of the
Voronoi diagram.
Theorem 7.4 For the Voronoi diagram Vor(P) of a set of points P the following
holds:
(i)
A point q is a vertex of Vor(P) if and only if its largest empty circle CP(q)
contains three or more sites on its boundary.
(ii) The bisector between sites pi and p j deﬁnes an edge of Vor(P) if and only
if there is a point q on the bisector such that CP(q) contains both pi and pj
on its boundary but no other site.
150

Section 7.2
COMPUTING THE VORONOI DIAGRAM
Proof. (i) Suppose there is a point q such that CP(q) contains three or more sites
on its boundary. Let pi, pj, and pk be three of those sites. Since the interior
of CP(q) is empty q must be on the boundary of each of V(pi), V(pj), and
V(pk), and q must be a vertex of Vor(P).
On the other hand, every vertex q of Vor(P) is incident to at least three
edges and, hence, to at least three Voronoi cells V(pi), V(pj), and V(pk).
Vertex q must be equidistant to pi, pj, and pk and there cannot be another
site closer to q, since otherwise V(pi), V(pj), and V(pk) would not meet at q.
Hence, the interior of the circle with pi, p j, and pk on its boundary does not
contain any site.
(ii) Suppose there is a point q with the property stated in the theorem. Since
CP(q) does not contain any sites in its interior and pi and pj are on its
boundary, we have dist(q, pi) = dist(q, p j) ⩽dist(q, pk) for all 1 ⩽k ⩽n.
It follows that q lies on an edge or vertex of Vor(P). The ﬁrst part of the
theorem implies that q cannot be a vertex of Vor(P). Hence, q lies on an edge
of Vor(P), which is deﬁned by the bisector of pi and p j.
Conversely, let the bisector of pi and pj deﬁne a Voronoi edge. The
largest empty circle of any point q in the interior of this edge must contain pi
and p j on its boundary and no other sites.
7.2
Computing the Voronoi Diagram
In the previous section we studied the structure of the Voronoi diagram. We
now set out to compute it. Observation 7.1 suggests a simple way to do this:
for each site pi, compute the common intersection of the half-planes h(pi, p j),
with j ̸= i, using the algorithm presented in Chapter 4. This way we spend
O(nlogn) time per Voronoi cell, leading to an O(n2 logn) algorithm to compute
the whole Voronoi diagram. Can't we do better? After all, the total complexity
of the Voronoi diagram is only linear. The answer is yes: the plane sweep
algorithm described below—commonly known as Fortune's algorithm after
its inventor—computes the Voronoi diagram in O(nlogn) time. You may be
tempted to look for an even faster algorithm, for example one that runs in linear
time. This turns out to be too much to ask: the problem of sorting n real numbers
is reducible to the problem of computing Voronoi diagrams, so any algorithm
for computing Voronoi diagrams must take Ω(nlogn) time in the worst case.
Hence, Fortune's algorithm is optimal.
The strategy in a plane sweep algorithm is to sweep a horizontal line—the
sweep line—from top to bottom over the plane. While the sweep is performed
information is maintained regarding the structure that one wants to compute.
More precisely, information is maintained about the intersection of the structure
with the sweep line. While the sweep line moves downwards the information
does not change, except at certain special points—the event points.
Let's try to apply this general strategy to the computation of the Voronoi diagram
of a set P = {p1, p2,..., pn} of point sites in the plane. According to the plane
151

Chapter 7
VORONOI DIAGRAMS
sweep paradigm we move a horizontal sweep line ℓfrom top to bottom over
the plane. The paradigm involves maintaining the intersection of the Voronoi
diagram with the sweep line. Unfortunately this is not so easy, because the part
of Vor(P) above ℓdepends not only on the sites that lie above ℓbut also on sites
below ℓ. Stated differently, when the sweep line reaches the topmost vertex
of the Voronoi cell V(pi) it has not yet encountered the corresponding site pi.
Hence, we do not have all the information needed to compute the vertex. We are
forced to apply the plane sweep paradigm in a slightly different fashion: instead
of maintaining the intersection of the Voronoi diagram with the sweep line, we
maintain information about the part of the Voronoi diagram of the sites above ℓ
that cannot be changed by sites below ℓ.
Denote the closed half-plane above ℓby ℓ+. What is the part of the Voronoi
diagram above ℓthat cannot be changed anymore? In other words, for which
points q ∈ℓ+ do we know for sure what their nearest site is? The distance of
ℓ
a point q ∈ℓ+ to any site below ℓis greater than the distance of q to ℓitself.
Hence, the nearest site of q cannot lie below ℓif q is at least as near to some site
pi ∈ℓ+ as q is to ℓ. The locus of points that are closer to some site pi ∈ℓ+ than
to ℓis bounded by a parabola. Hence, the locus of points that are closer to any
site above ℓthan to ℓitself is bounded by parabolic arcs. We call this sequence
ℓ
of parabolic arcs the beach line. Another way to visualize the beach line is the
following. Every site pi above the sweep line deﬁnes a complete parabola βi.
The beach line is the function that—for each x-coordinate—passes through the
lowest point of all parabolas.
Observation 7.5 The beach line is x-monotone, that is, every vertical line
intersects it in exactly one point.
It is easy to see that one parabola can contribute more than once to the beach
line. We'll worry later about how many pieces there can be. Notice that the
breakpoints between the different parabolic arcs forming the beach line lie on
edges of the Voronoi diagram. This is not a coincidence: the breakpoints exactly
trace out the Voronoi diagram while the sweep line moves from top to bottom.
These properties of the beach line can be proved using elementary geometric
arguments.
So, instead of maintaining the intersection of Vor(P) with ℓwe maintain
the beach line as we move our sweep line ℓ. We do not maintain the beach line
explicitly, since it changes continuously as ℓmoves. For the moment let's ignore
the issue of how to represent the beach line until we understand where and how
its combinatorial structure changes. This happens when a new parabolic arc
appears on it, and when a parabolic arc shrinks to a point and disappears.
First we consider the events where a new arc appears on the beach line. One
occasion where this happens is when the sweep line ℓreaches a new site. The
parabola deﬁned by this site is at ﬁrst a degenerate parabola with zero width: a
vertical line segment connecting the new site to the beach line. As the sweep
line continues to move downward the new parabola gets wider and wider. The
part of the new parabola below the old beach line is now a part of the new beach
152

Section 7.2
COMPUTING THE VORONOI DIAGRAM
line. Figure 7.2 illustrates this process. We call the event where a new site is
encountered a site event.
ℓ
ℓ
ℓ
Figure 7.2
A new arc appears on the beach line
because a site is encountered
What happens to the Voronoi diagram at a site event? Recall that the
breakpoints on the beach line trace out the edges of the Voronoi diagram. At a
site event two new breakpoints appear, which start tracing out edges. In fact,
ℓ
the new breakpoints coincide at ﬁrst, and then move in opposite directions to
trace out the same edge. Initially, this edge is not connected to the rest of the
Voronoi diagram above the sweep line. Later on—we will see shortly exactly
when this will happen—the growing edge will run into another edge, and it
becomes connected to the rest of the diagram.
So now we understand what happens at a site event: a new arc appears on
the beach line, and a new edge of the Voronoi diagram starts to be traced out.
Is it possible that a new arc appears on the beach line in any other way? The
answer is no:
Lemma 7.6 The only way in which a new arc can appear on the beach line is
through a site event.
Proof. Suppose for a contradiction that an already existing parabola β j deﬁned
by a site pj breaks through the beach line. There are two ways in which this
could happen.
The ﬁrst possibility is that β j breaks through in the middle of an arc of a
parabola βi. The moment this is about to happen, βi and β j are tangent, that is,
ℓ
β j
they have exactly one point of intersection. Let ℓy denote the y-coordinate of the
sweep line at the moment of tangency. If pj := (pj,x, p j,y), then the parabola βj
is given by
β j :=
y =
1
2(pj,y −ℓy)
#
x2 −2pj,xx+ p2
j,x + p2
j,y −ℓ2
y
$
.
The formula for βi is similar, of course. Using that both pj,y and pi,y are larger
than ℓy, it is easy to show that it is impossible that βi and β j have only one point
of intersection. Hence, a parabola βj never breaks through in the middle of an
arc of another parabola βi.
ℓ
β j
The second possibility is that β j appears in between two arcs. Let these
arcs be part of parabolas βi and βk. Let q be the intersection point of βi and
βk at which βj is about to appear on the beach line, and assume that βi is on
the beach line left of q and βk is on the beach line right of q, as in Figure 7.3.
Then there is a circle C that passes through pi, pj, and pk, the sites deﬁning
the parabolas. This circle is also tangent to the sweep line ℓ. The cyclic order
153

Chapter 7
VORONOI DIAGRAMS
on C, starting at the point of tangency with ℓand going clockwise, is pi, p j, pk,
because βj is assumed to appear in between the arcs of βi and βk. Consider an
inﬁnitesimal motion of the sweep line downward while keeping the circle C
tangent to ℓ; see Figure 7.3. Then C cannot have empty interior and still pass
Figure 7.3
The situation when β j would appear on
the beach line, and the circle when the
sweep line has proceeded
p j
pi
pk
q
βk
βi
β j
ℓ
C
p j
pi
pk
q
C
ℓ
through pj: either pi or pk will penetrate the interior. Therefore, in a sufﬁciently
small neighborhood of q the parabola β j cannot appear on the beach line when
the sweep line moves downward, because either pi or pk will be closer to ℓ
than pj.
An immediate consequence of the lemma is that the beach line consists of
at most 2n−1 parabolic arcs: each site encountered gives rise to one new arc
and the splitting of at most one existing arc into two, and there is no other way
an arc can appear on the beach line.
Figure 7.4
An arc disappears from the beach line
ℓ
q
pi
p j
pk
ℓ
α′
α
α′′
p j
pk
pi
ℓ
p j
pk
pi
q
The second type of event in the plane sweep algorithm is where an existing arc
of the beach line shrinks to a point and disappears, as in Figure 7.4. Let α′
be the disappearing arc, and let α and α′′ be the two neighboring arcs of α′
before it disappears. The arcs α and α′′ cannot be part of the same parabola;
this possibility can be excluded in the same way as the ﬁrst possibility in the
proof of Lemma 7.6 was excluded. Hence, the three arcs α, α′, and α′′ are
deﬁned by three distinct sites pi, p j, and pk. At the moment α′ disappears, the
parabolas deﬁned by these three sites pass through a common point q. Point
q is equidistant from ℓand each of the three sites. Hence, there is a circle
passing through pi, pj, and pk with q as its center and whose lowest point lies
154

Section 7.2
COMPUTING THE VORONOI DIAGRAM
on ℓ. There cannot be a site in the interior of this circle: such a site would be
closer to q than q is to ℓ, contradicting the fact that q is on the beach line. It
follows that the point q is a vertex of the Voronoi diagram. This is not very
surprising, since we observed earlier that the breakpoints on the beach line trace
out the Voronoi diagram. So when an arc disappears from the beach line and
two breakpoints meet, two edges of the Voronoi diagram meet as well. We call
the event where the sweep line reaches the lowest point of a circle through three
sites deﬁning consecutive arcs on the beach line a circle event. From the above
we can conclude the following lemma.
Lemma 7.7 The only way in which an existing arc can disappear from the beach
line is through a circle event.
Now we know where and how the combinatorial structure of the beach line
changes: at a site event a new arc appears, and at a circle event an existing
arc drops out. We also know how this relates to the Voronoi diagram under
construction: at a site event a new edge starts to grow, and at a circle event
two growing edges meet to form a vertex. It remains to ﬁnd the right data
structures to maintain the necessary information during the sweep. Our goal
is to compute the Voronoi diagram, so we need a data structure that stores
the part of the Voronoi diagram computed thus far. We also need the two
'standard' data structures for any sweep line algorithm: an event queue and a
structure that represents the status of the sweep line. Here the latter structure is
a representation of the beach line. These data structures are implemented in the
following way.
We store the Voronoi diagram under construction in our usual data struc-
ture for subdivisions, the doubly-connected edge list. A Voronoi diagram,
however, is not a true subdivision as deﬁned in Chapter 2: it has edges
that are half-lines or full lines, and these cannot be represented in a doubly-
connected edge list. During the construction this is not a problem, because
the representation of the beach line—described next—will make it possible
to access the relevant parts of the doubly-connected edge list efﬁciently
during its construction. But after the computation is ﬁnished we want to
have a valid doubly-connected edge list. To this end we add a big bounding
box to our scene, which is large enough so that it contains all vertices of
the Voronoi diagram. The ﬁnal subdivision we compute will then be the
bounding box plus the part of the Voronoi diagram inside it.
The beach line is represented by a balanced binary search tree T; it is the
status structure. Its leaves correspond to the arcs of the beach line—which
is x-monotone—in an ordered manner: the leftmost leaf represents the
leftmost arc, the next leaf represents the second leftmost arc, and so on.
Each leaf µ stores the site that deﬁnes the arc it represents. The internal
nodes of T represent the breakpoints on the beach line. A breakpoint is
stored at an internal node by an ordered tuple of sites ⟨pi, p j⟩, where pi
deﬁnes the parabola left of the breakpoint and pj deﬁnes the parabola to the
right. Using this representation of the beach line, we can ﬁnd in O(logn)
155

Chapter 7
VORONOI DIAGRAMS
time the arc of the beach line lying above a new site. At an internal node,
we simply compare the x-coordinate of the new site with the x-coordinate
of the breakpoint, which can be computed from the tuple of sites and the
position of the sweep line in constant time. Note that we do not explicitly
store the parabolas.
In T we also store pointers to the other two data structures used during
the sweep. Each leaf of T, representing an arc α, stores one pointer to a
node in the event queue, namely, the node that represents the circle event in
which α will disappear. This pointer is nil if no circle event exists where α
will disappear, or this circle event hasn't been detected yet. Finally, every
internal node ν has a pointer to a half-edge in the doubly-connected edge
list of the Voronoi diagram. More precisely, ν has a pointer to one of the
half-edges of the edge being traced out by the breakpoint represented by ν.
The event queue Q is implemented as a priority queue, where the priority of
an event is its y-coordinate. It stores the upcoming events that are already
known. For a site event we simply store the site itself. For a circle event the
event point that we store is the lowest point of the circle, with a pointer to
the leaf in T that represents the arc that will disappear in the event.
All the site events are known in advance, but the circle events are not. This
brings us to one ﬁnal issue that we must discuss, namely the detection of circle
events.
During the sweep the beach line changes its topological structure at every
event. This may cause new triples of consecutive arcs to appear on the beach
line and it may cause existing triples to disappear. Our algorithm will make sure
that for every three consecutive arcs on the beach line that deﬁne a potential
circle event, the potential event is stored in the event queue Q. There are two
subtleties involved in this. First of all, there can be consecutive triples whose
two breakpoints do not converge, that is, the directions in which they move are
such that they will not meet in the future; this happens when the breakpoints
move along two bisectors away from the intersection point. In this case the
triple does not deﬁne a potential circle event. Secondly, even if a triple has
converging breakpoints, the corresponding circle event need not take place: it
can happen that the triple disappears (for instance due to the appearance of a
new site on the beach line) before the event has taken place. In this case we call
the event a false alarm.
So what the algorithm does is this. At every event, it checks all the new
triples of consecutive arcs that appear. For instance, at a site event we can
get three new triples: one where the new arc is the left arc of the triple, one
where it is the middle arc, and one where it is the right arc. When such a new
triple has converging breakpoints, the event is inserted into the event queue Q.
Observe that in the case of a site event, the triple with the new arc being the
middle one can never cause a circle event, because the left and right arc of
the triple come from the same parabola and therefore the breakpoints must
diverge. Furthermore, for all disappearing triples it is checked whether they
have a corresponding event in Q. If so, the event is apparently a false alarm, and
156

Section 7.2
COMPUTING THE VORONOI DIAGRAM
it is deleted from Q. This can easily be done using the pointers we have from
the leaves in T to the corresponding circle events in Q.
Lemma 7.8 Every Voronoi vertex is detected by means of a circle event.
Proof. For a Voronoi vertex q, let pi, pj, and pk be the three sites through which
a circle C(pi, p j, pk) passes with no sites in the interior. By Theorem 7.4, such a
circle and three sites indeed exist. For simplicity we only prove the case where
no other sites lie on C(pi, p j, pk), and the lowest point of C(pi, p j, pk) is not
one of the deﬁning sites. Assume without loss of generality that from the lowest
point of C(pi, p j, pk), the clockwise traversal of C(pi, p j, pk) encounters the
sites pi, p j, pk in this order.
We must show that just before the sweep line reaches the lowest point of
C(pi, p j, pk), there are three consecutive arcs α, α′ and α′′ on the beach line
deﬁned by the sites pi, pj, and pk. Only then will the circle event take place.
Consider the sweep line an inﬁnitesimal amount before it reaches the lowest
point of C(pi, p j, pk). Since C(pi, p j, pk) doesn't contain any other sites inside
p j
pi
pk
C(pi, p j, pk)
or on it, there exists a circle through pi and pj that is tangent to the sweep line,
and doesn't contain sites in the interior. So there are adjacent arcs on the beach
line deﬁned by pi and p j. Similarly, there are adjacent arcs on the beach line
deﬁned by pj and pk. It is easy to see that the two arcs deﬁned by pj are actually
the same arc, and it follows that there are three consecutive arcs on the beach
line deﬁned by pi, pj, and pk. Therefore, the corresponding circle event is in Q
just before the event takes place, and the Voronoi vertex is detected.
We can now describe the plane sweep algorithm in detail. Notice that after
all events have been handled and the event queue Q is empty, the beach line
hasn't disappeared yet. The breakpoints that are still present correspond to the
half-inﬁnite edges of the Voronoi diagram. As stated earlier, a doubly-connected
edge list cannot represent half-inﬁnite edges, so we must add a bounding box
to the scene to which these edges can be attached. The overall structure of the
algorithm is as follows.
Algorithm VORONOIDIAGRAM(P)
Input. A set P := {p1,..., pn} of point sites in the plane.
Output. The Voronoi diagram Vor(P) given inside a bounding box in a doubly-
connected edge list D.
1.
Initialize the event queue Q with all site events, initialize an empty status
structure T and an empty doubly-connected edge list D.
2.
while Q is not empty
3.
do Remove the event with largest y-coordinate from Q.
4.
if the event is a site event, occurring at site pi
5.
then HANDLESITEEVENT(pi)
6.
else HANDLECIRCLEEVENT(γ), where γ is the leaf of T repre-
senting the arc that will disappear
7.
The internal nodes still present in T correspond to the half-inﬁnite edges of
the Voronoi diagram. Compute a bounding box that contains all vertices of
the Voronoi diagram in its interior, and attach the half-inﬁnite edges to the
bounding box by updating the doubly-connected edge list appropriately.
157

Chapter 7
VORONOI DIAGRAMS
8.
Traverse the half-edges of the doubly-connected edge list to add the cell
records and the pointers to and from them.
The procedures to handle the events are deﬁned as follows.
HANDLESITEEVENT(pi)
1.
If T is empty, insert pi into it (so that T consists of a single leaf storing pi)
and return. Otherwise, continue with steps 2- 5.
2.
Search in T for the arc α vertically above pi. If the leaf representing α has
a pointer to a circle event in Q, then this circle event is a false alarm and it
must be deleted from Q.
3.
Replace the leaf of T that represents α with a subtree having three leaves.
The middle leaf stores the new site pi and the other two leaves store the site
pj that was originally stored with α. Store the tuples ⟨p j, pi⟩and ⟨pi, p j⟩
representing the new breakpoints at the two new internal nodes. Perform
rebalancing operations on T if necessary.
4.
Create new half-edge records in the Voronoi diagram structure for the
edge separating V(pi) and V(pj), which will be traced out by the two new
breakpoints.
5.
Check the triple of consecutive arcs where the new arc for pi is the left arc
to see if the breakpoints converge. If so, insert the circle event into Q and
add pointers between the node in T and the node in Q. Do the same for the
triple where the new arc is the right arc.
HANDLECIRCLEEVENT(γ)
1.
Delete the leaf γ that represents the disappearing arc α from T. Update
the tuples representing the breakpoints at the internal nodes. Perform
rebalancing operations on T if necessary. Delete all circle events involving
α from Q; these can be found using the pointers from the predecessor and
the successor of γ in T. (The circle event where α is the middle arc is
currently being handled, and has already been deleted from Q.)
2.
Add the center of the circle causing the event as a vertex record to the
doubly-connected edge list D storing the Voronoi diagram under construc-
tion. Create two half-edge records corresponding to the new breakpoint
of the beach line. Set the pointers between them appropriately. Attach the
three new records to the half-edge records that end at the vertex.
3.
Check the new triple of consecutive arcs that has the former left neighbor
of α as its middle arc to see if the two breakpoints of the triple converge.
If so, insert the corresponding circle event into Q. and set pointers between
the new circle event in Q and the corresponding leaf of T. Do the same for
the triple where the former right neighbor is the middle arc.
Lemma 7.9 The algorithm runs in O(nlogn) time and it uses O(n) storage.
Proof. The primitive operations on the tree T and the event queue Q, such
as inserting or deleting an element, take O(logn) time each. The primitive
operations on the doubly-connected edge list take constant time. To handle
an event we do a constant number of such primitive operations, so we spend
158

Section 7.2
COMPUTING THE VORONOI DIAGRAM
O(logn) time to process an event. Obviously, there are n site events. As for
the number of circle events, we observe that every such event that is processed
deﬁnes a vertex of Vor(P). Note that false alarms are deleted from Q before
they are processed. They are created and deleted while processing another, real
event, and the time we spend on them is subsumed under the time we spend to
process this event. Hence, the number of circle events that we process is at most
2n−5. The time and storage bounds follow.
Before we state the ﬁnal result of this section we should say a few words about
degenerate cases.
The algorithm handles the events from top to bottom, so there is a degeneracy
when two or more events lie on a common horizontal line. This happens, for
example, when there are two sites with the same y-coordinate. These events can
be handled in any order when their x-coordinates are distinct, so we can break
ties between events with the same y-coordinate but with different x-coordinates
arbitrarily. However, if this happens right at the start of the algorithm, that is,
if the second site event has the same y-coordinate as the ﬁrst site event, then
special code is needed because there is no arc above the second site yet. Now
zero-length edge
suppose there are event points that coincide. For instance, there will be several
coincident circle events when there are four or more co-circular sites, such that
the interior of the circle through them is empty. The center of this circle is a
vertex of the Voronoi diagram. The degree of this vertex is at least four. We
could write special code to handle such degenerate cases, but there is no need to
do so. What will happen if we let the algorithm handle these events in arbitrary
order? Instead of producing a vertex with degree four, it will just produce two
vertices with degree three at the same location, with a zero length edge between
them. These degenerate edges can be removed in a post-processing step, if
required.
Besides these degeneracies in choosing the order of the events we may also
encounter degeneracies while handling an event. This occurs when a site pi that
we process happens to be located exactly below the breakpoint between two arcs
on the beach line. In this case the algorithm splits either of these two arcs and
inserts the arc for pi in between the two pieces, one of which has zero length.
This piece of zero length now is the middle arc of a triple that deﬁnes a circle
event. The lowest point of this circle coincides with pi. The algorithm inserts
this circle event into the event queue Q, because there are three consecutive arcs
on the beach line that deﬁne it. When this circle event is handled, a vertex of
the Voronoi diagram is correctly created and the zero length arc can be deleted
later. Another degeneracy occurs when three consecutive arcs on the beach line
are deﬁned by three collinear sites. Then these sites don't deﬁne a circle, nor a
circle event.
We conclude that the above algorithm handles degenerate cases correctly.
Theorem 7.10 The Voronoi diagram of a set of n point sites in the plane can be
computed with a sweep line algorithm in O(nlogn) time using O(n) storage.
159

Chapter 7
VORONOI DIAGRAMS
7.3
Voronoi Diagrams of Line Segments
The Voronoi diagram can also be deﬁned for objects other than points. The
distance from a point in the plane to an object is then measured to the closest
point on the object. Whereas the bisector of two points is simply a line, the
bisector of two disjoint line segments has a more complex shape. It consists of
up to seven parts, where each part is either a line segment or a parabolic arc.
Parabolic arcs occur if the closest point of one line segment is an endpoint and
the closest point of the other line segment is in its interior. In all other cases the
bisector part is straight. Although bisectors and therefore the Voronoi diagram
are somewhat more complex, the number of vertices, edges, and faces in the
Voronoi diagram of n disjoint line segments is still only O(n).
Assume for a moment that we allow the line segments to be non-crossing,
that is, we allow them to share endpoints. Then a whole region of the plane can
be equally close to two line segments via their common endpoint, and bisectors
are not even curves anymore. To avoid the complications that arise in deﬁning
and computing Voronoi diagrams of line segments that share endpoints, we
will simply assume here that all line segments are strictly disjoint. In many
applications we can simply shorten the line segments very slightly to obtain
disjoint line segments.
The sweep line algorithm for points can be adapted to the case of line
segment sites. Let S = {s1,...,sn} be a set of n disjoint line segments. We
call the segments of S sites as before, and use the terms site endpoint and site
interior in the following description.
Figure 7.5
The beach line for a set of line segment
sites. The breakpoints trace the dashed
arcs, which include the Voronoi edges
ℓ
s1
s2
s3
s4
s5
Recall that our algorithm for point sites maintained a beach line: a piecewise
parabolic x-monotone curve such that, for points on the curve, the distance to
the closest site above the sweep line is equal to the distance to the sweep line.
What does the beach line look like when the sites are segments? First we note
that a line segment site may be partially above and partially below the sweep
line. When deﬁning the beach line, we consider only those parts of the sites that
are above the sweep line. Hence, for a given position of the sweep line ℓ, the
beach line consists of those points such that the distance to the closest portion
of a site above ℓis equal to the distance to ℓ. This means that the beach line
now consists of parabolic arcs and straight line segments. A parabolic arc arises
when that part of the beach line is closest to a site endpoint, and a straight line
segment arises when that part of the beach line is closest to a site interior. Note
that if a site interior intersects ℓ, then the beach line will have two straight line
segments ending at the intersection—see site s2 in Figure 7.5.
160

Section 7.3
VORONOI DIAGRAMS OF LINE
SEGMENTS
The breakpoints between parabolic arcs and straight segments on the beach
line arise in several different ways. Figure 7.5 illustrates this. Assume any
position ℓof the sweep line during the downward sweep to analyze the types of
breakpoint:
If a point p is closest to two site endpoints while being equidistant from
them and ℓ, then p is a breakpoint that traces a line segment (as in the point
site case).
If a point p is closest to two site interiors while being equidistant from them
and ℓ, then p is a breakpoint that traces a line segment.
If a point p is closest to a site endpoint and a site interior of different sites
while being equidistant from them and ℓ, then p is a breakpoint that traces a
parabolic arc.
If a point p is closest to a site endpoint, the shortest distance is realized by a
segment that is perpendicular to the line segment site, and p has the same
distance from ℓ, then p is a breakpoint that traces a line segment.
If a site interior intersects the sweep line, then the intersection is a breakpoint
that traces a line segment (the site interior).
In the fourth and ﬁfth cases, the breakpoint does not actually trace an arc of the
Voronoi diagram, because only one site is involved. For the proper operation of
the algorithm, dealing with such breakpoints and corresponding events is still
necessary.
As in the sweep line algorithm for point sites, we again have site events and
ℓ
circle events. A site event occurs when the sweep line reaches a site endpoint.
Obviously, site events at upper endpoints should be handled differently from
site events at lower endpoints. At an upper endpoint, an arc of the beach line is
split into two, and in between, four new arcs appear. The breakpoints between
these four arcs are of the last two types. At a lower endpoint, the breakpoint
that is the intersection of the site interior with the sweep line is replaced by two
breakpoints of the fourth type, with a parabolic arc in between (for the newly
discovered site endpoint).
Similarly, there are several types of circle event. They all correspond to the
disappearance of an arc of the beach line, and they occur when the sweep line
reaches the bottom of an empty circle that is deﬁned by two or three sites above
the sweep line. The centers of these empty circles are at locations where two
consecutive breakpoints will meet. Depending on the types of the breakpoints
that meet, several different cases can be distinguished and handled. If the two
breakpoints are of any of the ﬁrst three types, then three sites are involved. If
one of the breakpoints is of the fourth type, then only two sites are involved.
Breakpoints of the ﬁfth type cannot be involved for disjoint line segments.
Notice that the Voronoi diagram that the algorithm computes is a subdivision
with straight edges and parabolic arcs. Can we store this type of subdivision in
a doubly-connected edge list? This is indeed possible, and the structure need
not even be adapted. With each face, we store the corresponding site, so for any
161

Chapter 7
VORONOI DIAGRAMS
half-edge⃗e we can determine the two sites that have e on their bisector (using
IncidentFace(⃗e) and IncidentFace(Twin(⃗e))). Since we can also easily ﬁnd the
two vertices between which the edge lies (Origin(⃗e) and Origin(Twin(⃗e))), we
can determine the shape of any edge in constant time.
The whole sweep line algorithm is now just an extension of the one for point
sites, with more cases to be distinguished and handled. However, the algorithm
still has only O(n) events, and each can be handled in O(logn) time.
Theorem 7.11 The Voronoi diagram of a set of n disjoint line segment sites can
be computed in O(nlogn) time using O(n) storage.
One of the applications of the Voronoi diagram for line segments is in
motion planning (covered more extensively in Chapter 13). Assume that a set
of obstacles is given, consisting of n line segments in total, and that we have
a robot R. We assume that the robot can move freely in all directions, and is
approximated well by an enclosing disc D. Suppose that we wish to ﬁnd a
collision-free motion from one location of the robot to another, or to decide that
none exists.
One motion-planning technique is called retraction. The idea of retraction is
that the arcs of the Voronoi diagram deﬁne the middle between the line segments,
and therefore deﬁne a path with the most clearance. So a path over the arcs
of the Voronoi diagram is the best option for a collision-free path. Figure 7.6
shows a set of line segments inside a rectangle, together with a Voronoi diagram
of the line segments and the sides of the rectangle.
Figure 7.6
Voronoi diagram of line segments, and
start and end positions of a disc
pstart
pend
We can determine a collision-free path between two disc positions amidst a
set of line segments with the following algorithm.
162

Section 7.4
FARTHEST-POINT VORONOI
DIAGRAMS
Algorithm RETRACTION(S,qstart,qend,r)
Input. A set S := {s1,...,sn} of disjoint line segments in the plane, and two
discs Dstart and Dend centered at qstart and qend with radius r. The two disc
positions do not intersect any line segment of S.
Output. A path that connects qstart to qend such that no disc of radius r with its
center on the path intersects any line segment of S. If no such path exists, this
is reported.
1.
Compute the Voronoi diagram Vor(S) of S inside a sufﬁciently large bound-
ing box.
2.
Locate the cells of Vor(P) that contain qstart and qend.
3.
Determine the point pstart on Vor(S) by moving qstart away from the nearest
line segment in S. Similarly, determine the point pend on Vor(S) by moving
qend away from the nearest line segment in S. Add pstart and pend as vertices
to Vor(S), splitting the arcs on which they lie into two.
4.
Let G be the graph corresponding to the vertices and edges of the Voronoi
diagram. Remove all edges from G for which the smallest distance to the
nearest sites is smaller than or equal to r.
5.
Determine with depth-ﬁrst search whether a path exists from pstart to pend
in G. If so, report the line segment from qstart to pstart, the path in G from
pstart to pend, and the line segment from pend to qend as the path. Otherwise,
report that no path exists.
The line segment connecting qstart to pstart cannot give a collision, because the
disc only moves further away from the nearest obstacle. Similarly, the line
segment between pend and qend is collision-free. For any two discs centered on
the Voronoi diagram, a collision-free path between them exists on the Voronoi
diagram if and only if such a path exists at all. Hence, for a disc-shaped robot, a
path is found if one exists.
Theorem 7.12 Given n disjoint line segment obstacles and a disc-shaped robot,
the existence of a collision-free path between two positions of the robot can be
determined in O(nlogn) time using O(n) storage.
7.4
Farthest-Point Voronoi Diagrams
We now continue with a different application where Voronoi diagrams are
needed. When objects are manufactured, slight deviations in the shapes of
the objects will occur. When the objects need to be perfectly round, the man-
ufactured objects are tested for their roundness. This is done by coordinate
measurement machines, which sample points on the surface of the object. As-
sume that we have constructed a disc, and wish to determine its roundness. The
machine gives us a set P of points in the plane that lie nearly on a circle. The
roundness of a set of points is deﬁned as the width of the smallest-width annulus
that contains the points. An annulus is the region between two concentric circles,
and its width is the difference between the radii of those circles.
The smallest-width annulus must of course have some points of the set P on
its bounding circles. Let us call the outer circle Couter and the inner circle Cinner.
163

Chapter 7
VORONOI DIAGRAMS
Clearly, there must be at least one point on Couter, otherwise we can reduce the
size of Couter, and at least one point on Cinner, otherwise we can increase the size
of Cinner. But one point on each bounding circle cannot give us a smallest-width
annulus yet. There appear to be three different cases, each with a total of four
points on the two circles (Figure 7.7):
Couter contains at least three points of P, and Cinner contains at least one
point of P.
Couter contains at least one point of P, and Cinner contains at least three points
of P.
Couter and Cinner both contain two points of P.
Figure 7.7
Three cases of the smallest-width
annulus
If Cinner or Couter contains fewer points than listed in any of these cases, then
we can always ﬁnd an annulus with a smaller width. The problem of ﬁnding the
smallest-width annulus enclosing a given point set looks similar to the problem
of ﬁnding the smallest disc enclosing a point set, studied in Section 4.7. The
technique we used for the smallest-disc problem, however, does not work for
the smallest-width annulus: the property that an added point that does not lie in
the optimal annulus so far must always lie on the boundary of the new optimal
annulus does not hold.
Finding the smallest-width annulus is equivalent to ﬁnding its center point.
Once the center point—let's call it q—is ﬁxed, the annulus is determined by
the points of P that are closest to and farthest from q. If we have the Voronoi
diagram of P, then the closest point is the one in whose cell q lies. It turns out
that a similar structure exists for the farthest point, namely the farthest-point
Voronoi diagram. This divides the plane into cells in which the same point
of P is the farthest point. The farthest-point Voronoi cell of a point pi is the
intersection of n−1 half-planes, just as for a standard Voronoi cell, but we take
the "other sides" of the bisectors, the sides where pi is farther away. Hence, all
pi
cell of pi
p j
cell of pj
cells of the farthest-point Voronoi diagram are convex. Not every point of P has
a cell in the farthest-point Voronoi diagram: the intersections of the half-planes
can be empty. It is not hard to see that for any point in the plane, its farthest
point in the set P must be a point that lies on the convex hull of P. Therefore,
a point that lies inside the convex hull cannot have a cell in the farthest-point
Voronoi diagram.
Observation 7.13 Given a set P of points in the plane, a point of P has a cell in
the farthest-point Voronoi diagram if and only if it is a vertex of the convex hull
of P.
164

Section 7.4
FARTHEST-POINT VORONOI
DIAGRAMS
We can prove more properties of the farthest-point Voronoi diagram. Sup-
pose that a point pi ∈P lies on the convex hull, and let q be some point in
the plane for which pi is the farthest point. Let ℓ(pi,q) be the line through pi
pi
q
and q. Then all points on the half-line starting at q, contained in ℓ(pi,q), and
not containing pi, must also be in the farthest-point Voronoi cell of pi. This
implies that all cells are unbounded. The vertices and edges of the farthest-point
Voronoi diagram form a tree-like structure (in the graph sense), because the
diagram is connected and does not have cycles. A cycle would imply a bounded
cell.
We can show that the farthest-point Voronoi diagram of n points has O(n)
vertices, edges, and cells (see also Exercise 7.14). There is another interesting
property: the center of the smallest enclosing disc (see Section 4.7) is either
a vertex of the farthest-point Voronoi diagram or the midpoint of two sites
deﬁning an edge of the farthest-point Voronoi diagram. In the former case, there
are three equidistant farthest points, and in the latter case, two. Clearly, the
center of the smallest enclosing disc cannot have just one point that is farthest
from it.
Since the farthest-point Voronoi diagram has half-inﬁnite edges, we cannot
store it in a doubly-connected edge list, but we can adapt the structure slightly
to deal with such subdivisions. We use a special vertex-like record as the origin
of each half-edge that has no real vertex as its origin. These new records store
the direction of the half-inﬁnite edge instead of coordinates. Furthermore, half-
edge records corresonding to half-inﬁnite edges have either Next(⃗e) or Prev(⃗e)
undeﬁned. We shall still use the term "doubly-connected edge list" for this
adapted version.
We now present an algorithm to compute the farthest-point Voronoi diagram
of a set P of n points in the plane. First, we compute the convex hull of P, take
its vertices, and put them in random order. Let this random order be p1,..., ph.
We remove the points ph,..., p4 one by one from the cyclic order, and when
removing pi, store its clockwise neighbor cw(pi) and counterclockwise neighbor
ccw(pi) at the time of removal. After a point has been removed, it cannot be
the clockwise or counterclockwise neighbor anymore of points removed later.
pi
cw(pi)
ccw(pi)
pi
cw(pi)
ccw(pi)
ccw(pi)
cw(pi)
cell of pi
cell of
cell of
cw(pi)
cell of
ccw(pi)
cell of
Figure 7.8
Addition of a point pi to the
farthest-point Voronoi diagram of
p1,..., pi−1
We compute the farthest-point Voronoi diagram of p1, p2, p3 to initialize
165

Chapter 7
VORONOI DIAGRAMS
the incremental construction. Then we insert the remaining points p4,..., ph
while constructing the farthest-point Voronoi diagram. To be able to add the
farthest-point Voronoi cell of pi efﬁciently, given the farthest-point Voronoi
diagram of {p1,..., pi−1}, we maintain a pointer for each point p j, 1 ⩽j < i,
to the half-inﬁnite half-edge of the doubly-connected edge list that is most
p j
cell of pj
counterclockwise in a traversal of the boundary of the farthest-point Voronoi
cell of pj.
We now look at the addition of the cell of pi in more detail, see Figure 7.8.
The cell will come "in between" the cells of cw(pi) and ccw(pi). Just before
pi is added, cw(pi) and ccw(pi) are each other's neighbors on the convex hull
of {p1,..., pi−1}, so their cells are separated by a half-inﬁnite edge that is part
of their bisector. The point ccw(pi) has a pointer to this edge. The bisector
of pi and ccw(pi) will give a new half-inﬁnite edge that lies in the farthest-
point Voronoi cell of ccw(pi), and is part of the boundary of the farthest-point
Voronoi cell of pi. We traverse the cell of ccw(pi) in the clockwise direction
to see which edge the bisector intersects. On the other side of this edge is
the farthest-point Voronoi cell of another point pj from {p1,..., pi−1}, and the
bisector of pj and pi will also give an edge of the farthest-point Voronoi cell
of pi. We again traverse the cell of pj in the clockwise direction to determine
where the other insertion of the cell boundary and the bisector is located. By
tracing cell boundaries in clockwise order, we trace the farthest-point Voronoi
cell in counterclockwise order. The last bisector that we will ﬁnd is with cw(pi),
and it will give a new half-inﬁnite edge in the farthest-point Voronoi diagram.
All new edges found are added to the doubly-connected edge list representation,
after which all edges that lie inside the farthest-point Voronoi cell of pi are
removed. They are no longer valid edges of the farthest-point Voronoi diagram
of {p1,..., pi}.
In short, the insertion of the next farthest-point Voronoi cell is done by
tracing the new cell with the help of the existing diagram, adding the new edges,
and removing the edges that have become obsolete.
Theorem 7.14 Given a set of n points in the plane, its farthest-point Voronoi
diagram can be computed in O(nlogn) expected time using O(n) storage.
Proof. It takes O(nlogn) time to compute the h points on the convex hull in
counterclockwise order. The farthest-point Voronoi diagram actually takes only
O(h) expected time to construct after we have the points on the convex hull
in sorted order. To see this, we apply backwards analysis. We consider the
situation after the insertion of the cell of pi. We observe that if the cell of pi has
k edges on its boundary, then the traversal performed to trace this cell visited
k cells in the farthest-point Voronoi diagram of {p1,..., pi−1}, and visited at
most 4k −6 boundary edges of these cells in total.
The farthest-point Voronoi diagram of {p1,..., pi} has at most 2i−3 edges
(see Exercise 7.14), each used by two cells. Since every point of {p1,..., pi}
has the same probability of having been the last one added, the expected size
of the cell of pi is less than four. Hence, the expected time needed for each
insertion is O(1), and the algorithm runs in O(h) expected time.
166

Section 7.5
NOTES AND COMMENTS
Now we return to the problem of computing the smallest-width annulus.
Suppose that the smallest-width annulus is such that Cinner contains at least three
points of P. Then its center is a vertex of the normal Voronoi diagram of P.
Similarly, if the smallest-width annulus is such that Couter contains at least three
points of P, its center is a vertex of the farthest-point Voronoi diagram of P.
Finally, if the smallest-width annulus is such that Cinner and Couter both contain
two points of P, then its center must lie on an edge of the Voronoi diagram and
on an edge of the farthest-point Voronoi diagram simultaneously. This means
that we can obtain a reasonably small set of points that must contain the center
of a smallest-width annulus.
To do this, we generate the vertices of the overlay of the Voronoi diagram
and the farthest-point Voronoi diagram. The vertices of the overlay are exactly
the candidate centers of the smallest-width annulus, covering all three cases. We
don't really need to compute the overlay itself. Once we know a vertex and the
four points that determine Cinner and Couter, we can compute the smallest-width
annulus of those four points directly in O(1) time. This is a candidate for the
smallest-width annulus.
The whole algorithm to compute the smallest-width annulus of a set P of n
points in the plane is as follows. Compute the Voronoi diagram and the farthest-
point Voronoi diagram of P. For each vertex of the farthest-point Voronoi
diagram, determine the point of P that is closest. For each vertex of the normal
Voronoi diagram, determine the point of P that is farthest. This gives us O(n)
sets of four points that deﬁne the candidate annuli in the ﬁrst and second cases.
Next, for every pair of edges, one from each of the diagrams, test if they intersect.
If so, we have another set of four points that forms a candidate annulus. For
all candidates of all three types, choose the one that gives the smallest-width
annulus as the solution.
Theorem 7.15 Given a set P of n points in the plane, the smallest-width annulus
(and the roundness) can be determined in O(n2) time using O(n) storage.
7.5
Notes and Comments
Although it is beyond the scope of this book to give an extensive survey of
the history of Voronoi diagrams it is appropriate to make a few historical
remarks. Voronoi diagrams are often attributed to Dirichlet [148]—hence the
name Dirichlet tessellations that is sometimes used—and Voronoi [379, 380].
They can be found in Descartes's treatment of cosmic fragmentation in Part
III of his Principia Philosophiae, published in 1644. In the twentieth century,
the Voronoi diagram was rediscovered several times. In biology this even
happened twice in a very short period. In 1965 Brown [75] studied the intensity
of trees in a forest. He deﬁned the area potentially available to a tree, which
was in fact the Voronoi cell of that tree. One year later Mead [272] used the
same concept for plants, calling the Voronoi cells plant polygons. Now, there
is an impressive amount of literature concerning Voronoi diagrams and their
applications in all kinds of research areas. The book by Okabe et al. [297]
167

Chapter 7
VORONOI DIAGRAMS
contains an ample treatment of Voronoi diagrams and their applications. We
conﬁne ourselves in this section to a discussion of the various aspects of Voronoi
diagrams encountered in the computational geometry literature.
In this chapter we have proved some properties of the Voronoi diagram, but it
has many more. For example, if one connects all the pairs of sites whose Voronoi
cells are adjacent then the resulting set of segments forms a triangulation of the
point set, called the Delaunay triangulation. This triangulation, which has some
very nice properties, is the topic of Chapter 9.
There is a beautiful connection between Voronoi diagrams and convex
polyhedra. Consider the transformation that maps a point p = (px, py) in E2 to
the non-vertical plane h(p) : z = 2pxx+2pyy−(p2
x + p2
y) in E3. Geometrically,
z = x2 +y2
(px, py,0)
h(p) is the plane that is tangent to the unit paraboloid U : z = x2 +y2 at the point
vertically above (px, py,0). For a set P of point sites in the plane, let H(P) be
the set of planes that are the images of the sites in P. Now consider the convex
polyhedron P that is the intersection of all positive half-spaces deﬁned by the
planes in H(P), that is, P := "
h∈H(P) h+, where h+ denotes the half-space above
h. Surprisingly, if we project the edges and vertices of the polyhedron vertically
downwards onto the xy-plane, we get the Voronoi diagram of P [167]. See
Chapter 11 for a more extensive description of this transformation. A similar
transformation exists for the farthest-point Voronoi diagram.
We have studied Voronoi diagrams in their most basic setting, namely for a set
of point sites in the Euclidean plane. The ﬁrst optimal O(nlogn) time algorithm
for this case was a divide-and-conquer algorithm presented by Shamos and
Hoey [350]; since then, many other optimal algorithms have been developed.
The plane sweep algorithm that we described is due to Fortune [183]. Fortune's
original description of the algorithm is a little different from ours, which follows
the interpretation of the algorithm given by Guibas and Stolﬁ[203].
Voronoi diagrams can be generalized in many ways [28, 297]. One generaliza-
tion is to point sets in higher-dimensional spaces. In Ed, the maximum combina-
torial complexity of the Voronoi diagram of a set of n point sites (the maximum
number of vertices, edges, and so on, of the diagram) is Θ(n⌈d/2⌉) [239] and it
can be computed in O(nlogn+n⌈d/2⌉) optimal time [93, 133, 346]. The fact
that the dual of the Voronoi diagram is a triangulation of the set of sites, and
the connection between Voronoi diagrams and convex polyhedra as discussed
above still hold in higher dimensions.
Another generalization concerns the metric that is used. In the L1-metric, or
Manhattan metric, the distance between two points p and q is deﬁned as
dist1(p,q) := |px −qx|+|py −qy|,
the sum of the absolute differences in the x- and y-coordinates. In a Voronoi
diagram in the L1-metric, all edges are horizontal, vertical, or diagonal (at an
angle of 45◦to the coordinate axes). In the more general Lp-metric, the distance
between two points p and q is deﬁned as
distp(p,q) :=
p!
|px −qx|p +|py −qy|p .
168

Section 7.5
NOTES AND COMMENTS
Note that the L2-metric is simply the Euclidean metric. There are several papers
dealing with Voronoi diagrams in these metrics [118, 248, 252]. One can also
deﬁne a distance function by assigning a weight to each site. Now the distance
from a site to a point is the Euclidean distance to the point, plus its additive
weight. The resulting diagrams are called weighted Voronoi diagrams [183].
The weight can also be used to deﬁne the distance from a site to a point as the
Euclidean distance times the weight. Diagrams based on this multiplicatively
weighted distance are also called weighted Voronoi diagrams [29]. Power
diagrams [25, 26, 27, 30] are another generalization of Voronoi diagrams where
a different distance function is used. It is even possible to drop the distance
function altogether and deﬁne the Voronoi diagram in terms only of bisectors
between pairs of sites. Such diagrams are called abstract Voronoi diagrams [240,
241, 242, 274].
Other generalizations concern the shape of the sites. We have seen the Voronoi
diagram of a set of disjoint line segments in this chapter. We discussed the
application of this diagram to motion planning using the retraction technique;
Chapter 13 discusses motion planning in general.
An important special case of the Voronoi diagram of line segments is the
Voronoi diagram of the edges of a simple polygon, interior to the polygon itself.
Since the edges share endpoints, there can be whole regions inside the polygon
where two edges are equally close. This occurs at every reﬂex vertex of the
polygon. The Voronoi diagram is the subdivision of the interior of the polygon
into faces where one or two edges are the closest. This Voronoi diagram is
also known as the medial axis or skeleton, and it has applications in shape
analysis [366, 377]. The medial axis can be computed in time linear in the
number of edges of the polygon [123].
Instead of partitioning the space into regions according to the closest sites,
one can also partition it according to the k closest sites, for some 1 ⩽k ⩽n−1.
The diagrams obtained in this way are called higher-order Voronoi diagrams,
and, for given k, the diagram is called the order-k Voronoi diagram [6, 31, 70, 98].
Note that the order-1 Voronoi diagram is nothing more than the standard Voronoi
diagram. The order-(n −1) Voronoi diagram is the farthest-point Voronoi
diagram, because the Voronoi cell of a point pi is now the region of points for
which pi is the farthest site. The maximum complexity of the order-k Voronoi
diagram of a set of n point sites in the plane is Θ(k(n −k)) [249]. Currently
the best known algorithms for computing the order-k Voronoi diagram run in
O(nlog3 n+nk) time [6] and in O(nlogn+nk2clog∗k) time [326], where c is a
constant.
The farthest-point Voronoi diagram takes O(nlogn) time to compute, but if
the points are in convex position and are given in the order along the convex
hull, then there exists a simple O(n) expected-time algorithm [116], given in
this chapter, and also an O(n) time deterministic algorithm [11]. Testing the
roundness of an object or set of points is a problem that arises in metrology,
the science of measurement. Several deﬁnitions of roundness exist, the one
used in this chapter being the most widely accepted one. A quadratic-time
169

Chapter 7
VORONOI DIAGRAMS
algorithm for the roundness problem was given by Ebarra et al. [155]. A com-
plex, subquadratic-time algorithm was suggested by Agarwal and Sharir [9]. In
special cases that correspond to point sets that may occur in practice, linear-time
or near-linear-time algorithms exist [52, 142, 187]. A survey of computational
metrology has been given by Yap and Chang [396].
7.6
Exercises
7.1
Prove that for any n > 3 there is a set of n point sites in the plane such
that one of the cells of Vor(P) has n−1 vertices.
7.2
Show that Theorem 7.3 implies that the average number of vertices of a
Voronoi cell is less than six.
7.3
Show that Ω(nlogn) is a lower bound for computing Voronoi diagrams
by reducing the sorting problem to the problem of computing Voronoi
diagrams. You can assume that the Voronoi diagram algorithm should
be able to compute for every vertex of the Voronoi diagram its incident
edges in cyclic order around the vertex.
7.4
Prove that the breakpoints of the beach line, as deﬁned in Section 7.2,
trace out the edges of the Voronoi diagram while the sweep line moves
from top to bottom.
7.5
Give an example where the parabola deﬁned by some site pi contributes
more than one arc to the beach line. Can you give an example where it
contributes a linear number of arcs?
7.6
Give an example of six sites such that the plane sweep algorithm encoun-
ters the six site events before any of the circle events. The sites should lie
in general position: no three sites on a line and no four sites on a circle.
7.7
Do the breakpoints of the beach line always move downwards when the
sweep line moves downwards? Prove this or give a counterexample.
7.8
Write a procedure to compute a big enough bounding box from the
incomplete doubly-connected edge list and the tree T after the sweep is
completed. The box should contain all sites and all Voronoi vertices.
7.9
Write a procedure to add all cell records and the corresponding pointers
to the incomplete doubly-connected edge list after the bounding box has
been added. That is, ﬁll in the details of line 8 of Algorithm VORONOIDI-
AGRAM.
7.10 Let P be a set of n points in the plane. Give an O(nlogn) time algorithm
to ﬁnd two points in P that are closest together. Show that your algorithm
is correct.
170

Section 7.6
EXERCISES
7.11 Let P be a set of n points in the plane. Give an O(nlogn) time algorithm
to ﬁnd for each point p in P another point in P that is closest to it.
7.12 Let the Voronoi diagram of a point set P be stored in a doubly-connected
edge list inside a bounding box. Give an algorithm to compute all points
of P that lie on the boundary of the convex hull of P in time linear in the
output size. Assume that your algorithm receives as its input a pointer to
the record of some half-edge whose origin lies on the bounding box.
7.13 For each of the ten breakpoints shown in Figure 7.5, determine which of
the ﬁve types it corresponds to.
7.14 Show that the farthest point Voronoi diagram on n points in the plane
has at most 2n −3 (bounded or unbounded) edges. Also give an exact
bound on the maximum number of vertices in the farthest point Voronoi
diagram.
7.15 Show that the smallest width annulus cannot be constructed with ran-
domized incremental construction. To this end, show that a point pi
can be added to a set Pi−1 that does not lie in the minimum width annu-
lus, but does not lie on the boundary of the smallest width annulus of
Pi := Pi−1 ∩{pi}.
7.16 Show that for some set P of n points, there can be Ω(n2) intersections
between the edges of the Voronoi diagram and the farthest site Voronoi
diagram.
7.17 Show that if there are only O(n) intersections between the edges of the
Voronoi diagram and the farthest site Voronoi diagram, then the smallest
width annulus can be computed in O(nlogn) expected time.
7.18* In the Voronoi assignment model the goods or services that the consumers
want to acquire have the same market price at every site. Suppose this is
not the case, and that the price of the good at site pi is wi. The trading areas
of the sites now correspond to the cells in the weighted Voronoi diagram
of the sites (see Section 7.5), where site pi has an additive weight wi.
Generalize the sweep line algorithm of Section 7.2 to this case.
7.19* Suppose that we are given a subdivision of the plane into n convex regions.
We suspect that this subdivision is a Voronoi diagram, but we do not know
the sites. Develop an algorithm that ﬁnds a set of n point sites whose
Voronoi diagram is exactly the given subdivision, if such a set exists.
171

8
Arrangements and Duality
Supersampling in Ray Tracing
Computer generated images of 3-dimensional scenes are becoming more and
more realistic. Nowadays, they can hardly be distinguished from photographs.
A technique that has played an important role in this development is ray tracing.
It works as follows.
Figure 8.1
Determining visible objects using ray
tracing
The screen of a computer monitor is composed of many small dots, called
pixels. A nice screen consists of, say, 1280 by 1024 pixels. Suppose that we
are given a 3-dimensional scene consisting of several objects, a light source,
and a view point. Generating an image of this scene—also called rendering the
scene—amounts to determining for every pixel on the screen which object is
visible at that pixel, and determining the intensity of light that the object emits
in the direction of the view point at that particular point. Let's ﬁrst look at the
ﬁrst task, determining the visible object at each pixel. Ray tracing performs this
task by shooting a ray through each pixel, as in Figure 8.1. The ﬁrst object that
is hit is the one that is visible at the pixel. Once the visible object is determined,
one has to compute the intensity of light emitted by the object at the visible
point. Here we have to take into account how much light that point receives
173

Chapter 8
ARRANGEMENTS AND DUALITY
from the light source, either directly or indirectly via reﬂections on other objects.
The strength of ray tracing is that it can perform this second task—which is
crucial to getting a realistic image—quite well. In this chapter, however, we are
mainly interested in the ﬁrst part.
There is one issue in determining the visible object for each pixel that we
have swept under the rug: a pixel is not a point, but a small square area. In
general this is not a problem. Most pixels are covered completely by one of the
objects, and shooting a ray through the pixel center will tell us which object
this is. But problems arise near the edges of the objects. When an edge of an
object crosses a pixel, the object may cover 49% of the pixel area, but be missed
by the ray through the pixel center. If, on the other hand, the object covered
51% of the pixel area, it would be hit by the ray and we would falsely assume
that the whole pixel was covered. This results in the well-known jaggies in the
image. This artifact is diminished if we had not just two categories, "hit" and
"miss", but also categories like "49% hit". Then we could set the intensity of
the pixel to, say, 0.49 times the object intensity. Or if there is more than one
object visible inside the pixel, we could make the pixel intensity a mixture of
the object intensities.
How can we incorporate such different pixel intensities in our ray tracing
scheme? The solution is to shoot more than one ray per pixel. If, for instance, we
shoot 100 rays per pixel and 35 of them hit an object then we expect that object
to be visible in roughly 35% of the pixel area. This is called supersampling:
instead of taking one sample point per pixel, we take many.
How should we distribute the rays over the pixel to make this work? An
obvious choice would be to distribute them regularly; for 100 rays this would
mean having a regular grid of 10 by 10 sample points in the pixel. Indeed, if
35 of these rays hit the object then the area where it is visible cannot be far
from 35%. There is, however, a disadvantage to choosing a regular sample
pattern: although the error at each particular pixel will be small, there will be a
certain regularity across rows (and columns) of pixels. Regularity in the errors
triggers the human visual system, and as a result we see annoying artifacts. So
choosing a regular sample pattern isn't such a good idea. It's better to choose
the sample points in a somewhat random fashion. Of course not every random
pattern works equally well; we still want the sample points to be distributed in
such a way that the number of hits is close to the percentage of covered area.
Suppose that we have generated a random set of sample points. We'd like to
have a way of deciding whether this set is good. What we want is that the
difference between the percentage of hits for an object and the percentage of
the pixel area where that object is visible is small. This difference is called
the discrepancy of the sample set with respect to the object. Of course we do
not know in advance which objects will be visible in the pixel, so we have to
prepare for the worst-case scenario: we want the maximum discrepancy over all
possible ways that an object can be visible inside the pixel to be small. This is
called the discrepancy of the sample set, and it depends on the type of objects
that are present in the scene. So formally the discrepancy of a sample set is
deﬁned with respect to a given class of objects. Based on the discrepancy of the
174

Section 8.1
COMPUTING THE DISCREPANCY
given set of sample points we can decide if it is good enough: if the discrepancy
is low enough we decide to keep it, and otherwise we generate a new random
set. For this we need an algorithm that computes the discrepancy of a given
point set.
8.1
Computing the Discrepancy
We mentioned above that the discrepancy of a point set is deﬁned with respect
to a class of objects. The objects that we must consider are the projections of
the 3-dimensional objects that make up our scene. As is common in computer
graphics, we assume that curved objects are approximated using polygonal
meshes. So the 2-dimensional objects that we must consider are the projections
of the facets of polyhedra. In other words, we are interested in the discrepancy
with respect to the class of polygons. In general scenes, most pixels will be
crossed by at most one edge of a given polygon, unless the scene consists
of many polygons that are extremely thin or tiny. If a pixel is intersected by
one polygon edge, the polygon behaves inside the pixel like a half-plane. The
situation that a pixel is intersected by more polygon edges is much less common.
Also, it doesn't cause regularity in the error, which was the source of the
annoying artifacts. Therefore we restrict our attention to half-plane discrepancy.
Let U := [0 : 1]×[0 : 1] be the unit square—U models a pixel—and let S be a
set of n sample points in U. Let H denote the (inﬁnite) set of all possible closed
half-planes. The continuous measure of a half-plane h ∈H is deﬁned as the
area of h∩U. We denote it by µ(h). For a half-plane h that completely covers
U we have µ(h) = 1, for instance. The discrete measure of h, which we denote
as µS(h), is deﬁned as the fraction of the sample points that is contained in h,
so µS(h) := card(S∩h)/card(S), where card(·) denotes the cardinality of a set.
The discrepancy of h with respect to the sample set S, denoted as ∆S(h), is the
absolute difference between the continuous and the discrete measure:
∆S(h) := |µ(h)−µS(h)|.
The discrepancy of the half-plane in the margin ﬁgure, for example, is |0.25−
h
U
0.3| = 0.05. Finally, the half-plane discrepancy of S is the supremum of the
discrepancies over all possible half-planes:
∆H(S) := sup
h∈H
∆S(h).
We have deﬁned what it is that we want to compute. Now let's see how to
compute it.
The supremum of the discrepancy over all closed half-planes equals the max-
imum discrepancy over all open or closed half-planes. To get the search for
the half-plane with maximum discrepancy started, we ﬁrst identify a ﬁnite set
of candidate half-planes. It's always a good idea to replace an inﬁnite set of
175

Chapter 8
ARRANGEMENTS AND DUALITY
candidates by a ﬁnite set, provided the latter contains the ones we're interested
in. So the ﬁnite set we identify must contain the half-plane that has maximum
discrepancy. We'll select each half-plane that locally has maximal discrep-
ancy. This means that if we translated or rotated such a half-plane slightly, its
discrepancy would decrease. One of these half-planes with locally maximal
discrepancy will be the one that gives the real maximum.
Any half-plane that does not contain a point of S on its boundary can be
translated slightly such that its continuous measure increases while its discrete
measure remains the same. A slight translation in the opposite direction will
decrease the continuous measure while the discrete measure remains the same.
So one of the two translations results in an increase in the discrepancy. Hence,
the half-plane we are looking for has a point of S on its boundary. Now consider
a half-plane h having only one point p ∈S on its boundary. Can we always
rotate h around p such that the discrepancy increases? In other words, does
the half-plane with the maximum discrepancy always have two points on its
boundary? The answer is no: when we rotate h around p we can reach a local
extremum in the continuous measure function. Suppose this local extremum is a
local maximum. Then any slight rotation will decrease the continuous measure.
If the discrete measure is smaller than the continuous measure at the local
maximum, rotating will decrease the discrepancy. Similarly, any slight rotation
at a local minimum of the continuous measure where the discrete measure is
greater than the continuous measure will decrease the discrepancy. Hence, the
maximum discrepancy could very well be achieved at such an extremum.
Let's investigate the extrema more closely. Let p := (px, py) be a point in
S. For 0 ⩽φ < 2π, let ℓp(φ) be the line through p that makes an angle φ
with the positive x-axis. Consider the continuous measure function of the half-
plane initially lying above ℓp(φ); we denote this half-plane by hp(φ). We are
interested in the local extrema of the function φ %→µ(hp(φ)). When φ increases
from 0 to 2π, the line ℓp(φ) rotates around p. First of all, an extremum may
occur when ℓp(φ) sweeps over one of the vertices of U. This happens at most
eight times. In between two such events ℓp(φ) intersects two ﬁxed edges of
U. A little calculation yields the continuous measure functions for the various
cases that occur. For example, when ℓp(φ) intersects the top and left boundary
of U we have
p
ℓp(φ)
φ
hp(φ)
µ(hp(φ)) = 1
2(1−py + px tanφ)(px + 1−py
tanφ ).
In this case there are at most two local extrema. The continuous measure
function is similar when ℓp(φ) intersects two other edges of U, so we may
conclude that there is a constant number of local extrema per point p ∈S. Thus
the total number of candidate half-planes with one point on their boundary is
O(n). Moreover, we can ﬁnd the extrema and the corresponding half-planes in
O(1) time per point. We have proved the following lemma.
Lemma 8.1 Let S be a set of n points in the unit square U. A half-plane h that
achieves the maximum discrepancy with respect to S is of one of the following
types:
176

Section 8.2
DUALITY
(i)
h contains one point p ∈S on its boundary,
(ii) h contains two or more points of S on its boundary.
The number of type (i) candidates is O(n), and they can be found in O(n) time.
The number of type (ii) candidates is clearly quadratic. Because the number
of type (i) candidates is much smaller than this, we treat them in a brute-force
way: for each of the O(n) half-planes we compute their continuous measure in
constant time, and their discrete measure in O(n) time. This way the maximum
of the discrepancies of these half-planes can be computed in O(n2) time. For
the type (ii) candidates we must be more careful when computing the discrete
measures. For this we need some new techniques. In the remainder of this
chapter we introduce these techniques and we show how to use them to compute
all discrete measures in O(n2) time. We can then compute the discrepancy
of these half-planes in constant time per half-plane, and take the maximum.
Finally, by comparing this maximum to the maximum discrepancy of the type
(i) candidates we ﬁnd the discrepancy of S. This leads to the following theorem.
Theorem 8.2 The half-plane discrepancy of a set S of n points in the unit square
can be computed in O(n2) time.
8.2
Duality
A point in the plane has two parameters: its x-coordinate and its y-coordinate.
A (non-vertical) line in the plane also has two parameters: its slope and its
intersection with the y-axis. Therefore we can map a set of points to a set of
lines, and vice versa, in a one-to-one manner. We can even do this in such a way
that certain properties of the set of points translate to certain other properties
for the set of lines. For instance, three points on a line become three lines
through a point. Several different mappings that achieve this are possible; they
x
y
x
y
primal plane
dual plane
ℓ
ℓ∗
p1
p2
p3
p4
p2∗
p4∗
p1∗
p3∗
Figure 8.2
An example of duality
are called duality transforms. The image of an object under a duality transform
is called the dual of the object. A simple duality transform is the following.
Let p := (px, py) be a point in the plane. The dual of p, denoted p∗, is the line
deﬁned as
p∗:=
(y = pxx−py).
177

Chapter 8
ARRANGEMENTS AND DUALITY
The dual of a line ℓ: y = mx+b is the point p such that p∗= ℓ. In other words,
ℓ∗:= (m,−b).
The duality transform is not deﬁned for vertical lines. In most cases vertical
lines can be handled separately, so this is not a problem. Another solution is to
rotate the scene so that there are no vertical lines.
We say that the duality transform maps objects from the primal plane to the
dual plane. Certain properties that hold in the primal plane also hold in the dual
plane:
Observation 8.3 Let p be a point in the plane and let ℓbe a non-vertical line in
the plane. The duality transform o %→o∗has the following properties.
It is incidence preserving: p ∈ℓif and only if ℓ∗∈p∗.
It is order preserving: p lies above ℓif and only if ℓ∗lies above p∗.
Figure 8.2 illustrates these properties. The three points p1, p2, and p3 lie on
the line ℓin the primal plane; the three lines p1∗, p2∗, and p3∗go through the
point ℓ∗in the dual plane. The point p4 lies above the line ℓin the primal plane;
the point ℓ∗lies above the line p4∗in the dual plane.
The duality transform can also be applied to other objects than points and lines.
What would be the dual of a line segment s := pq, for example? A logical choice
for s∗is the union of the duals of all points on s. What we get is an inﬁnite set
of lines. All the points on s are collinear, so all the dual lines pass through one
point. Their union forms a double wedge, which is bounded by the duals of the
endpoints of s. The lines dual to the endpoints of s deﬁne two double wedges, a
left-right wedge and a top-bottom wedge; s∗is the left-right wedge. Figure 8.3
shows the dual of a segment s. It also shows a line ℓintersecting s, whose dual
Figure 8.3
The dual transform applied to a line
segment
primal plane
dual plane
s
p
q
ℓ
s∗
p∗
q∗
ℓ∗
ℓ∗lies in s∗. This is not a coincidence: any line that intersects s must have either
p or q above it and the other point below it, so the dual of such a line lies in s∗
by the order preserving property of the dual transform.
The dual transform deﬁned above has a nice geometric interpretation. Let U
denote the parabola U : y = x2/2. Let's ﬁrst look at the dual of a point p that
lies on U. The derivative of U at p is px, so p∗has the same slope as the tangent
line of U at p. As a matter of fact, the dual of a point p ∈U is the tangent line
at p, because the intersection of the tangent with the y-axis is (0,−p2
x/2). Now
178

Section 8.3
ARRANGEMENTS OF LINES
suppose that a point q does not lie on U. What is the slope of q∗? Well, any two
points on the same vertical line have duals with equal slope. In particular, q∗is
parallel to p∗, where p is the point that lies on U and has the same x-coordinate
as q. Let q′ be the point with the same x-coordinate as q (and as p) such that
q′
y −py = py −qy. The vertical distance between the duals of points with the
p
q
q′
p∗
q∗
U
same x-coordinate is equal to the difference in y-coordinates of these points.
Hence, q∗is the line through q′ that is parallel to the tangent of U at p.
When you think about duality for a few minutes you may wonder how duality
can be useful. If you can solve a problem in the dual plane, you could have
solved it in the primal plane as well by mimicking the solution to the dual
problem in the primal plane. After all, the primal and dual problems are
essentially the same. Still, transforming a problem to the dual plane has one
important advantage: it provides a new perspective. Looking at a problem from
a different angle can give the insight needed to solve it.
Let's see what happens when we consider the discrepancy problem in the
dual plane. In the previous section we were left with the following problem:
p
q
p∗
q∗
ℓ(p,q)∗
ℓ(p,q)
Given a set S of n points, compute the discrete measure of every half-plane
bounded by a line through two of the points. When we dualize the set S of points
we get a set S∗:= {p∗: p ∈S} of lines. Let ℓ(p,q) denote the line through two
points p,q ∈S. The dual of this line is the intersection point of the two lines
p∗,q∗∈S∗. Consider the open half-plane bounded by and below ℓ(p,q). The
discrete measure of this half-plane is the number of points strictly below ℓ(p,q).
This means that in the dual plane we are interested in the number of lines strictly
above ℓ(p,q)∗. For the closed half-plane below ℓ(p,q) we must also take the
lines through ℓ(p,q)∗into account. Similarly, for the half-plane bounded by and
above ℓ(p,q) we are interested in the number of lines below ℓ(p,q)∗. In the next
section we study sets of lines, and we give an efﬁcient algorithm to compute the
number of lines above every intersection point, through every intersection point,
and below every intersection point. When we apply this algorithm to S∗we get
all the information we need to compute the discrete measure of all half-planes
bounded by lines through two points in S.
There is one thing that we should be careful about: two points in S with the
same x-coordinate dualize to lines with the same slope. So the line through these
points does not show up as an intersection in the dual plane. This makes sense,
because the dual transform is undeﬁned for vertical lines. In our application this
calls for an additional step. For every vertical line through at least two points, we
must determine the discrete measures of the corresponding half-planes. Since
there is only a linear number of vertical lines through two (or more) points in S,
the discrete measures for these lines can be computed in a brute-force manner
in O(n2) time in total.
8.3
Arrangements of Lines
Let L be a set of n lines in the plane. The set L induces a subdivision of the
plane that consists of vertices, edges, and faces. Some of the edges and faces
179

Chapter 8
ARRANGEMENTS AND DUALITY
are unbounded. This subdivision is usually referred to as the arrangement
induced by L, and it is denoted by A(L). An arrangement is called simple if
no three lines pass through the same point and no two lines are parallel. The
(combinatorial) complexity of an arrangement is the total number of vertices,
edges, and faces of the arrangement. Arrangements of lines and their higher-
dimensional counterparts occur frequently in computational geometry. Often a
problem that is deﬁned on a set of points is dualized and turned into a problem
on arrangements. This is done because the structure of a line arrangement is
more apparent than the structure of a point set. A line through a pair of points
edge
vertex
face
in the primal plane, for instance, becomes a vertex in the dual arrangement—a
much more explicit feature. The extra structure in an arrangement does not
come for free: constructing a full arrangement is a time- and storage-consuming
task, because the combinatorial complexity of an arrangement is high.
Theorem 8.4 Let L be a set of n lines in the plane, and let A(L) be the arrange-
ment induced by L.
(i)
The number of vertices of A(L) is at most n(n−1)/2.
(ii) The number of edges of A(L) is at most n2.
(iii) The number of faces of A(L) is at most n2/2+n/2+1.
Equality holds in these three statements if and only if A(L) is simple.
Proof. The vertices of A(L) are the intersection points of pairs of lines in L.
Hence, there are at most n(n−1)/2 of them. This number of vertices is achieved
if and only if every pair of lines gives rise to a unique intersection point, which
happens exactly if A(L) is simple.
The number of edges on a line is one more than the number of vertices on
that line. The latter number is at most n−1, so the number of edges on a line
is bounded by n. This gives at most n2 edges in total, which is achieved if and
only if A(L) is simple.
To bound the number of faces of A(L) we add the lines one by one and
bound the increase in the number of faces at each step. Let L := {ℓ1,...,ℓn}.
For 1 ⩽i ⩽n, deﬁne Li := {ℓ1,...,ℓi}. What is the increase in the number of
faces when we add ℓi and go from A(Li−1) to A(Li)? Every edge on ℓi splits a
face of A(Li−1) into two. Hence, the number of faces increase by the number
of edges of A(Li−1) on ℓi. The latter number is at most i, so the total number of
faces is at most
1+
n
∑
i=1
i = n2/2+n/2+1.
Again, this is achieved if and only if A(L) is simple.
So the arrangement A(L) induced by a set L of lines is a planar subdivision
of at most quadratic complexity. The doubly-connected edge list seems a
suitable way to store an arrangement; with this representation we can efﬁciently
list the edges of a given face, step from one face to a neighboring one, and so
on. A doubly-connected edge list, however, can only store bounded edges, and
an arrangement also has a number of unbounded edges. Therefore we place a
large bounding box that encloses the interesting part of the arrangement, that is,
180

Section 8.3
ARRANGEMENTS OF LINES
a bounding box that contains all vertices of the arrangement in its interior. The
subdivision deﬁned by the bounding box plus the part of the arrangement inside
it has bounded edges only and can be stored in a doubly-connected edge list.
How can we construct this doubly-connected edge list? The approach that
immediately comes to mind is plane sweep. In Chapter 2 all intersection points
of a set of line segments were computed by plane sweep, and this algorithm was
subsequently used to compute the doubly-connected edge list for the overlay of
two planar subdivisions. Indeed, it's not so difﬁcult to adapt the algorithms of
Chapter 2 to compute the arrangement A(L). Since the number of intersection
points is quadratic, the algorithm would run in O(n2 logn) time. Not bad, but not
optimal. So let's try another approach that may come to mind: an incremental
construction algorithm.
A bounding box B(L) that contains all vertices of A(L) in its interior can
easily be computed in quadratic time: compute all intersection points of pairs
of lines, and choose the leftmost one, the rightmost one, the bottom one, and
the top one. An axis-parallel rectangle that contains these four points contains
all vertices of the arrangement.
The incremental algorithm adds the lines ℓ1,ℓ2,...,ℓn one after the other and
updates the doubly-connected edge list after each addition. Let Ai denote the
subdivision of the plane induced by the bounding box B(L) and the part of
A({ℓ1,...,ℓi}) inside B(L). To add the line ℓi, we must split the faces in Ai−1
that are intersected by ℓi. We can ﬁnd these faces by walking along ℓi from left
to right, as follows. Suppose we enter a face f through an edge e. We walk
ℓi
Figure 8.4
Traversing the arrangement
along the boundary of f following Next()-pointers in the doubly-connected
edge list until we ﬁnd the half-edge of the edge e′ where ℓi leaves f. We then
step to the next face using the Twin()-pointer of that half-edge to reach the other
half-edge for e′ in the doubly-connected edge list. This way we ﬁnd the next
face in time proportional to the complexity of f. It can also happen that we leave
f through a vertex v. In this case we walk around v, visiting its incident edges,
until we ﬁnd the next face intersected by ℓi. The doubly-connected edge list
181

Chapter 8
ARRANGEMENTS AND DUALITY
allows us to do this in time proportional to the degree of v. Figure 8.4 illustrates
how we traverse the arrangement.
Two things are left: how do we ﬁnd the leftmost edge intersected by ℓi—this
is the edge where we start the walk through Ai−1—and how do we actually split
the faces we encounter?
The ﬁrst issue is easy. The leftmost intersection point of ℓi and Ai−1 is an
edge on B(L). We simply test all of them to locate the one where the traversal
can be started. The face incident to this edge and inside B(L) is the ﬁrst face
that is split by ℓi. In case ℓi intersects Ai−1 ﬁrst in a corner of B(L), the ﬁrst
face split by ℓi is the unique face incident to this corner and inside B(L). If ℓi is
a vertical line we can locate the bottom intersection point of ℓi and Ai−1 to start
off the traversal. Since Ai−1 contains at most 2i+2 edges on B(L), the time
needed for this step is linear for each line.
Suppose we have to split a face f, and assume that the face intersected by
ℓi to the left of f has already been split. In particular, we assume that the edge
e where we enter f has already been split. Splitting f is done as follows—see
Figure 8.5. First of all we create two new face records, one for the part of f
Figure 8.5
Splitting a face
f
ℓi
=⇒
ℓi
above ℓi and one for the part of f below ℓi. Next we split e′, the edge where
ℓi leaves f, and create a new vertex for ℓi ∩e′. Thus we create one new vertex
record, and two new half-edge records for both new edges. (If ℓi leaves f
through a vertex, then this step is omitted.) Furthermore, we create half-edge
records for the edge ℓi ∩f. It remains to correctly initialize the various pointers
in the new face, vertex, and half-edge records, set some existing pointers to the
new vertex record, half-edge records, and face records, and destroy the face
record for f and the half-edge records for e′. This is done in the same way as in
Section 2.3, where the overlay of two subdivisions was constructed. The total
time for the split is linear in the complexity of f.
The algorithm for constructing an arrangement can be summarized as follows:
Algorithm CONSTRUCTARRANGEMENT(L)
Input. A set L of n lines in the plane.
Output. The doubly-connected edge list for the subdivision induced by B(L)
and the part of A(L) inside B(L), where B(L) is a bounding box containing
all vertices of A(L) in its interior.
182

Section 8.3
ARRANGEMENTS OF LINES
1.
Compute a bounding box B(L) that contains all vertices of A(L) in its
interior.
2.
Construct the doubly-connected edge list for the subdivision induced by
B(L).
3.
for i ←1 to n
4.
do Find the edge e on B(L) that contains the leftmost intersection point
of ℓi and Ai.
5.
f ←the bounded face incident to e
6.
while f is not the unbounded face, that is, the face outside B(L)
7.
do Split f, and set f to be the next intersected face.
We have given a simple incremental algorithm for computing an arrangement.
Next we analyze its running time. Step 1 of the algorithm, computing B(L),
can be done in O(n2) time. Step 2 takes only constant time. Finding the ﬁrst
face split by ℓi takes O(n) time, as we noted before. We now bound the time it
takes to split the faces intersected by ℓi.
First, assume that A(L) is simple. In this case the time we spend to split
a face f and to ﬁnd the next intersected face is linear in the complexity of f.
Hence, the total time we need to insert line ℓi is linear in the sum of the
complexities of the faces of Ai−1 intersected by ℓi. When A(L) is not simple,
we may leave f through a vertex v. In that case we have to walk around v to
ﬁnd the next face to split, and we encounter edges that are not on the boundary
of an intersected face. But notice that the edges we encounter in this case are on
the boundary of faces whose closure is intersected by ℓi. This leads us to the
concept of zones.
The zone of a line ℓin the arrangement A(L) induced by a set L of lines in the
ℓ
Figure 8.6
The zone of a line in an arrangement of
lines
plane is the set of faces of A(L) whose closure intersects ℓ. Figure 8.6 gives an
example of a zone consisting of nine faces. The complexity of a zone is deﬁned
as the total complexity of all faces it consists of, that is, the sum of the number
of edges and vertices of these faces. In Figure 8.6 you can see that some vertices
are counted once in the zone complexity, others are counted twice, three times,
or even four times. The time we need to insert line ℓi is linear in the complexity
of the zone of ℓi in A({ℓ1,...,ℓi}). The Zone Theorem tells us that this quantity
is linear:
183

Chapter 8
ARRANGEMENTS AND DUALITY
Theorem 8.5 (Zone Theorem) The complexity of the zone of a line in an
arrangement of m lines in the plane is O(m).
Proof. Let L be a set of m lines in the plane, and let ℓbe another line. Without
loss of generality we assume that ℓcoincides with the x-axis; we can change
the coordinate system for this. We assume that no line of L is horizontal. This
assumption is removed at the end of the proof.
Each edge in A(L) bounds two faces. We say that an edge is a left bounding
edge for the face lying to the right of it and a right bounding edge for the face
lying to the left of it. We shall prove that the number of left bounding edges
of the faces in the zone of ℓis at most 5m. By symmetry, the number of right
bounding edges is then bounded by 5m as well, and the theorem follows.
The proof is by induction on m. The base case, m = 1, is trivially true. Now
let m > 1. Of the lines in L, let ℓ1 be the one that has the rightmost intersection
with ℓ. We ﬁrst assume that this line is uniquely deﬁned. By induction, the zone
of ℓin A(L\{ℓ1}) has at most 5(m−1) left bounding edges. When we add the
line ℓ1, the number of left bounding edges increases in two ways: there are new
left bounding edges on ℓ1 and there are old left bounding edges that are split by
ℓ1. Let v be the ﬁrst intersection point of ℓ1 with another line in L above ℓ, and
let w be the ﬁrst intersection point of ℓ1 below ℓ. The edge connecting v and w
ℓ
ℓ1
v
w
ℓ2
is a new left bounding edge on ℓ1. Furthermore, ℓ1 splits a left bounding edge
at the points v and w. This adds up to an increase of three in the number of left
bounding edges. If v or w doesn't exist, the increase is even less. We claim that
this is the only increase.
Consider the part of ℓ1 above v. Let ℓ2 be a line that intersects ℓ1 at v. The
region above v enclosed by ℓ1 and ℓ2 is not in the zone of ℓ. Because ℓ2 crosses
ℓ1 from left to right at v, the region lies to the right of ℓ1. Hence, the part of ℓ1
above v cannot contribute any left bounding edges to the zone. Moreover, if a
left bounding edge e that was in the zone is intersected by ℓ1 somewhere above
v, then the part of e to the right of ℓ1 is no longer in the zone. Hence, there is no
increase in the number of left bounding edges due to such an intersection.
In the same way it can be shown that the part of ℓ1 below w does not increase
the number of left bounding edges in the zone of ℓ. Therefore the total increase
is at most three, as claimed. The total number of left bounding edges in this
case is therefore at most 5(m−1)+3 < 5m.
Up to now we assumed that the line ℓ1 through the rightmost intersection
point on ℓis unique. If there is more than one line passing through the rightmost
intersection point, then we take an arbitrary one to be ℓ1. Following the same
arguments as above, the increase in the number of left bounding edges can be
shown to be at most ﬁve. (If more than two lines pass through the intersection
point, the increase is at most four. If exactly two lines pass through it, the
increase is at most ﬁve.) Hence, the total number of left bounding edges is at
most 5(m−1)+5 = 5m.
Finally, we remove the assumption that no line of L is horizontal. For a
horizontal line that doesn't coincide with ℓ, a slight rotation only increases the
complexity of the zone of ℓin A(L). Since we are proving an upper bound on
the zone complexity, we can safely assume such lines don't exist. If L contains
184

Section 8.4
LEVELS AND DISCREPANCY
a line ℓi that coincides with ℓ, then the proof above shows that the zone of ℓin
A(L \ {ℓi}) has at most 10m −10 edges, and the addition of ℓi increases this
quantity by at most 4m−2: at most m edges on ℓi for the faces above ℓi, at most
m edges on ℓi for the faces below ℓi, and at most m−1 edges are split into two,
each of which is counted as a left bounding edge and a right bounding edge.
This concludes the proof of the Zone Theorem.
We can now bound the running time of the incremental algorithm for con-
structing an arrangement. We have seen that the time needed to insert ℓi is linear
in the complexity of the zone of ℓi in A({ℓ1,...,ℓi−1}). By the Zone Theorem
this is O(i), so the time required to insert all lines is
n
∑
i=1
O(i) = O(n2).
Steps 1-2 of the algorithm together take O(n2) time, so the total running time
of the algorithm is O(n2). Because the complexity of A(L) is Θ(n2) when A(L)
is simple, our algorithm is optimal.
Theorem 8.6 A doubly-connected edge list for the arrangement induced by a
set of n lines in the plane can be constructed in O(n2) time.
8.4
Levels and Discrepancy
It's time to go back to the discrepancy problem. We had dualized the set S of n
sample points into a set S∗of n lines, and we needed to compute for every vertex
of A(S∗) how many lines lie above it, pass through it, and lie below it. For each
vertex, these three numbers add up to exactly n, so it is sufﬁcient to compute
two of the numbers. After we have constructed a doubly-connected edge list for
A(S∗) we know how many lines pass through each vertex. We deﬁne the level
3
1
0
2
2
2
1
3
3
levels of vertices
in an arrangement
3
4
of a point in an arrangement of lines to be the number of lines strictly above it.
We next show how to compute the level of each vertex in A(S∗).
To compute the levels of the vertices of A(S∗) we do the following for each
line ℓ∈S∗. First, we compute the level of the leftmost vertex on ℓin O(n) time,
by checking for each of the remaining lines whether it lies strictly above that
vertex. Next we walk along ℓfrom left to right to visit the other vertices on ℓ,
using the doubly-connected edge list. It is easy to maintain the level while we
level = 1
ℓ
1
1
0
0
0
1
3
Figure 8.7
Maintaining the level while walking
along a line
walk: the level only changes at a vertex, and the change can be computed by
185

Chapter 8
ARRANGEMENTS AND DUALITY
inspecting the edges incident to the vertex that is encountered. In Figure 8.7, for
instance, the leftmost vertex on ℓhas level one. The points on the edge incident
to that vertex and going to the right also have level one. At the second vertex
a line crosses ℓcoming from above; the level decreases by one and becomes
zero. Since the level is deﬁned as the number of lines strictly above a point, the
level of the second vertex itself is also zero. At the third vertex a line crosses ℓ
coming from below. Hence, the level increases to one after the vertex is passed;
the vertex itself still has level zero. And so on. Note that we needn't worry
about vertical lines; our set is obtained by dualizing a set of points. This way
the time to compute the levels of the vertices on ℓis O(n). Hence, the levels of
all vertices of A(S∗) can be computed in O(n2) time.
The number of lines above, through, and below each vertex of A(S∗) gives
us all the information we need to compute the discrete measure of the half-planes
bounded by lines containing two points in S. Hence, these discrete measures
can be computed in O(n2) time. This ﬁnally ﬁnishes the proof of Theorem 8.2.
8.5
Notes and Comments
In this chapter some important non-algorithmic concepts were introduced: geo-
metric duality and arrangements. Duality is a transform that can shed a different
light on a geometric problem, and is a standard tool for the computational
geometer. The duality transform of Section 8.2 is not deﬁned for vertical lines.
Usually, vertical lines can be treated as a special case or by a perturbation of the
setting. There exist different duality transforms that can handle vertical lines,
but these have other drawbacks—see Edelsbrunner's book [158]. Duality ap-
plies to higher-dimensional point sets as well. For a point p = (p1, p2,..., pd),
q
q∗
U
its dual p∗is the hyperplane xd = p1x1 + p2x2 + ··· + pd−1xd−1 −pd. For
a hyperplane xd = a1x1 + a2x2 + ··· + ad−1xd−1 + ad, its dual is the point
(a1,a2,...,ad−1,−ad). The transform is incidence and order preserving.
Recall that with the geometric interpretation of the duality transform using
the parabola y = x2/2, the dual of any point can be constructed. Interestingly, the
dual of a point q can also be constructed without measuring distances. Assume
that q lies below U. Draw the two lines that pass through q and are tangent to U.
The line q∗is the line through the two points where these tangents touch U. By
construction, point q is the intersection of the two tangent lines. Hence, the dual
of q must go through the duals of these two tangents, which are the points where
the tangents touch U. The dual of a point above U can be constructed without
measuring distances as well. We won't show how to do this here. (One hint:
you will need to be able to draw the line through a given point that is parallel to
a given line.)
Another geometric transform that has been applied successfully in computa-
tional geometry is inversion. It can change the point-inside-circle relation in the
plane to a point-below-plane relation in 3-dimensional space. More speciﬁcally,
a point p := (px, py) is lifted to the unit paraboloid z = x2 +y2 in 3-dimensional
186

Section 8.5
NOTES AND COMMENTS
space, so
p⋄:= (px, py, p2
x + p2
y).
A circle C := (x−a)2 +(y−b)2 = r2 in the plane is transformed to a plane in
3-dimensional space by lifting the circle to the unit paraboloid and taking the
plane through the lifted circle. In particular,
C⋄:= (z = a(x−a)+b(y−b)+r2).
Now p lies inside C if and only if p⋄is below C⋄. This transform can be
extended to higher dimensions, where a hypersphere in d-dimensional space
becomes a hyperplane in (d +1)-dimensional space.
Arrangements have been studied extensively in computational and combinato-
rial geometry. Arrangements are not restricted to the plane. A 3-dimensional
arrangement is induced by a set of planes, and a higher-dimensional arrange-
ment is induced by a set of hyperplanes. The book of Edelsbrunner [158] is an
excellent exposition of the research on arrangements up to 1987. It also contains
the references to earlier textbooks on combinatorial—but not computational—
geometry. For a more recent survey see the handbook chapter by Halperin [206].
We list a selection of results on arrangements in the plane and in higher dimen-
sions.
The complexity of an arrangement of n hyperplanes in d-dimensional space
is Θ(nd) in the worst case. Any simple arrangement—one where any d hyper-
planes but no d + 1 hyperplanes intersect in one point—achieves this bound.
Edelsbrunner et al. [165] presented the ﬁrst optimal algorithm for constructing
arrangements. The optimality of this incremental construction algorithm de-
pends on a higher-dimensional version of the Zone Theorem, which states that
the zone of a hyperplane in an arrangement of n hyperplanes in d-dimensional
space has complexity O(nd−1). A proof of this theorem is given by Edelsbrunner
et al. [168].
The concept of levels in arrangements extends to higher dimensions as
well—see Edelsbrunner's book [158]. The k-level in an arrangement A(H)
induced by a set H of n hyperplanes is deﬁned as the set of points with at most
k −1 hyperplanes strictly above it, and at most n−k hyperplanes strictly below.
Tight bounds on the maximum complexity of k-levels are still unknown, even
in the planar case. In the dual setting, the problem is closely related to the
following question: given an set of n points, how many subsets of k points
can be separated from the other n −k points by a hyperplane? Such subsets
are called k-sets, and the maximum number of k-sets in a set of n points is
again unknown. For the planar case—both for k-sets and k-levels—Erd˝os et
al. [174] in 1973 proved a lower bound of Ω(nlog(k +1)) and an upper bound
of O(n
√
k). This was were the problem stood for a long time (except for a slight
improvement of the upper bound by Pach et al. [313] to O(n
√
k/log∗(k +1)))
until in 1997 Dey [143, 144] managed to prove an O(nk1/3) upper bound, which
is now the best known bound.
Given a set of n points in the plane, how many subsets of at most k points can
be separated from the other n−k points by a line? Such subsets are called (⩽k)-
sets. Unlike for k-sets, tight bounds on the maximum number of (⩽k)-sets
187

Chapter 8
ARRANGEMENTS AND DUALITY
are known. In the plane the maximum number is Θ(nk), and in d-dimensional
space it is Θ(n⌊d/2⌋k⌈d/2⌉), which was shown by Clarkson and Shor [133]. The
same bounds hold for (⩽k)-levels in arrangements.
In the notes and comments of Chapter 7, a connection between Voronoi
diagrams and convex polyhedra in one dimension higher was explained: the
Voronoi diagram of a set of points in the plane is the same as the projection of the
boundary of the common intersection of a set of half-spaces in 3-dimensional
space. This boundary is in fact the 0-level of the arrangement of planes bounding
these half-spaces. This connection extends to order-k Voronoi diagrams and
k-levels in arrangements: the k-level in the same arrangement of planes projects
down to the order-k Voronoi diagram of the points.
Arrangements can be deﬁned for objects other than lines and hyperplanes.
A set of line segments in the plane, for instance, also forms an arrangement.
For such arrangements, even bounds on the maximum complexity of a single
face aren't simple to prove. Since faces can be non-convex, line segments can
appear several times on the boundary. Indeed, the maximum complexity of a
single face can be superlinear: it is Θ(nα(n)) in the worst case, where α(n) is
the extremely slowly growing functional inverse of Ackermann's function. The
upper bound can be proved using Davenport-Schinzel sequences; the interested
reader is referred to the book by Sharir and Agarwal [353].
The main motivation for studying combinatorial structures like arrange-
ments, single cells in arrangements, and envelopes, lies in motion planning.
Several motion planning problems can be formulated as problems on arrange-
ments and their substructures [201, 207, 208, 231, 342, 343].
Our original motivation for studying arrangements arose from computer graphics
and the quality of random samples. The use of discrepancy was introduced to
computer graphics by Shirley [358], and developed algorithmically by Dobkin
and Mitchell [150], Dobkin and Eppstein [149], Chazelle [96], and de Berg [50].
8.6
Exercises
8.1
Prove that the duality transform introduced in this chapter is indeed
incidence and order preserving, as claimed in Observation 8.3.
8.2
The dual of a line segment is a left-right double wedge, as was shown in
Section 8.2.
a. What is the dual of the collection of points inside a given triangle with
vertices p, q, and r?
b. What type of object in the primal plane would dualize to a top-bottom
double wedge?
8.3
Use Euler's formula to show that the maximum number of faces is n2/2+
n/2+1 for an arrangement with n(n−1)/2 vertices and n2 edges.
188

Section 8.6
EXERCISES
8.4
Let L be a set of n lines in the plane. Give an O(nlogn) time algorithm to
compute an axis-parallel rectangle that contains all the vertices of A(L)
in its interior.
8.5
Let S be a set of n points in the plane. In this chapter an algorithm was
given to determine for every line ℓthrough two points of S how many
points of S lie strictly above ℓ. This was done by dualizing the problem
ﬁrst. Transform the algorithm for the dual problem back to the primal
plane, and give the corresponding O(n2) time algorithm for the given
problem. (This exercise should help you to appreciate duality.)
8.6
Let S be a set of n points in the plane and let L be a set of m lines in the
plane. Suppose we wish to determine whether there is a point in S that
lies on a line in L. What is the dual of this problem?
8.7
Let R be a set of n red points in the plane, and let B be a set of n blue
points in the plane. We call a line ℓa separator for R and B if ℓhas
all points of R to one side and all points of B to the other side. Give a
randomized algorithm that can decide in O(n) expected time whether R
and B have a separator.
8.8
The dual transform of Section 8.2 has minus signs. Suppose we change
them to plus signs, so the dual of a point (px, py) is the line y = pxx+ py,
and the dual of the line y = mx + b is the point (m,b). Is this dual
transform incidence and order preserving?
8.9
Let P be a set of n points in the plane. Let p ∈P be one of these points.
Give a randomized algorithm that can decide in O(n) expected time
whether p is a vertex of the convex hull of P.
8.10 Let L be a set of n non-vertical lines in the plane. Suppose the arrange-
ment A(L) only has vertices with level 0. What can you say about this
arrangement? Next suppose that lines of L can be vertical. What can you
say now about the arrangement?
8.11 Let L be a set of lines in the plane, and let f be the face of A(L) containing
the origin. Describe the set of lines that are the duals of the points in f.
Also describe the dual of a vertex of f; distinguish between a vertex that
is the intersection of two lines that both pass above the origin, a vertex
that is the intersection of two lines that both pass below the origin, and a
vertex that is the intersection of one line passing above the origin and one
line passing below it.
8.12 While constructing the arrangement of a set L of lines, we traversed
every line of L from left to right when we added it. When computing
the discrepancy, we needed the level of each vertex in the arrangement.
To determine these levels, we traversed every line of L from left to right
again. Is it possible to combine these two traversals, that is, can you add
the lines to the arrangement and compute the levels of the intersection
points immediately?
189

Chapter 8
ARRANGEMENTS AND DUALITY
8.13 Given a set L of n lines in the plane, give an O(nlogn) time algorithm to
compute the maximum level of any vertex in the arrangement A(L).
8.14 Let S be a set of n points in the plane. Give an O(n2) time algorithm to
ﬁnd the line containing the maximum number of points in S.
8.15 Let S be a set of n segments in the plane. We want to preprocess S into a
data structure that can answer the following query: Given a query line ℓ,
how many segments in S does it intersect?
a. Formulate the problem in the dual plane.
b. Describe a data structure for this problem that uses O(n2) expected
storage and has O(logn) expected query time.
c. Describe how the data structure can be built in O(n2 logn) expected
time.
8.16 Let S be a set of n segments in the plane. A line ℓthat intersects all
segments of S is called a transversal or stabber for S.
a. Give an O(n2) algorithm to decide if a stabber exists for S.
b. Now assume that all segments are vertical. Give a randomized algo-
rithm with O(n) expected running time that decides if a stabber exists
for S.
ℓ
190

9
Delaunay Triangulations
Height Interpolation
When we talked about maps of a piece of the earth's surface in previous chapters,
we implicitly assumed there is no relief. This may be reasonable for a country
like the Netherlands, but it is a bad assumption for Switzerland. In this chapter
we set out to remedy this situation.
We can model a piece of the earth's surface as a terrain. A terrain is a
2-dimensional surface in 3-dimensional space with a special property: every
vertical line intersects it in a point, if it intersects it at all. In other words, it
is the graph of a function f : A ⊂R2 →R that assigns a height f(p) to every
point p in the domain, A, of the terrain. (The earth is round, so on a global
scale terrains deﬁned in this manner are not a good model of the earth. But
on a more local scale terrains provide a fairly good model.) A terrain can be
visualized with a perspective drawing like the one in Figure 9.1, or with contour
lines—lines of equal height—like on a topographic map.
Figure 9.1
A perspective view of a terrain
Of course, we don't know the height of every point on earth; we only know it
where we've measured it. This means that when we talk about some terrain, we
only know the value of the function f at a ﬁnite set P ⊂A of sample points. From
the height of the sample points we somehow have to approximate the height
at the other points in the domain. A naive approach assigns to every p ∈A the
height of the nearest sample point. However, this gives a discrete terrain, which
191

Chapter 9
DELAUNAY TRIANGULATIONS
doesn't look very natural. Therefore our approach for approximating a terrain
is as follows. We ﬁrst determine a triangulation of P: a planar subdivision
whose bounded faces are triangles and whose vertices are the points of P. (We
assume that the sample points are such that we can make the triangles cover
the domain of the terrain.) We then lift each sample point to its correct height,
thereby mapping every triangle in the triangulation to a triangle in 3-space.
Figure 9.2 illustrates this. What we get is a polyhedral terrain, the graph of a
continuous function that is piecewise linear. We can use the polyhedral terrain
as an approximation of the original terrain.
Figure 9.2
Obtaining a polyhedral terrain from a
set of sample points
The question remains: how do we triangulate the set of sample points? In
general, this can be done in many different ways. But which triangulation is the
most appropriate one for our purpose, namely to approximate a terrain? There
is no deﬁnitive answer to this question. We do not know the original terrain, we
only know its height at the sample points. Since we have no other information,
and the height at the sample points is the correct height for any triangulation, all
triangulations of P seem equally good. Nevertheless, some triangulations look
more natural than others. For example, have a look at Figure 9.3, which shows
two triangulations of the same point set. From the heights of the sample points
we get the impression that the sample points were taken from a mountain ridge.
Triangulation (a) reﬂects this intuition. Triangulation (b), however, where one
single edge has been "ﬂipped," has introduced a narrow valley cutting through
the mountain ridge. Intuitively, this looks wrong. Can we turn this intuition into
a criterion that tells us that triangulation (a) is better than triangulation (b)?
Figure 9.3
Flipping one edge can make a big
difference
(a)
(b)
height = 985
height = 23
0
10
6
20
36
28
1240
1000
980
990
1008
890
q
0
4
23
19
0
10
6
20
36
28
1240
1000
980
990
1008
890
q
0
4
23
19
The problem with triangulation (b) is that the height of the point q is deter-
192

Section 9.1
TRIANGULATIONS OF PLANAR POINT
SETS
mined by two points that are relatively far away. This happens because q lies in
the middle of an edge of two long and sharp triangles. The skinniness of these
triangles causes the trouble. So it seems that a triangulation that contains small
angles is bad. Therefore we will rank triangulations by comparing their smallest
angle. If the minimum angles of two triangulations are identical, then we can
look at the second smallest angle, and so on. Since there is only a ﬁnite number
of different triangulations of a given point set P, this implies that there must be
an optimal triangulation, one that maximizes the minimum angle. This will be
the triangulation we are looking for.
9.1
Triangulations of Planar Point Sets
Let P := {p1, p2,..., pn} be a set of points in the plane. To be able to formally
deﬁne a triangulation of P, we ﬁrst deﬁne a maximal planar subdivision as
a subdivision S such that no edge connecting two vertices can be added to
S without destroying its planarity. In other words, any edge that is not in S
intersects one of the existing edges. A triangulation of P is now deﬁned as a
maximal planar subdivision whose vertex set is P.
With this deﬁnition it is obvious that a triangulation exists. But does it
consist of triangles? Yes, every face except the unbounded one must be a
triangle: a bounded face is a polygon, and we have seen in Chapter 3 that any
polygon can be triangulated. What about the unbounded face? It is not difﬁcult
to see that any segment connecting two consecutive points on the boundary of
the convex hull of P is an edge in any triangulation T. This implies that the
convex hull boundary
union of the bounded faces of T is always the convex hull of P, and that the
unbounded face is always the complement of the convex hull. (In our application
this means that if the domain is a rectangular area, say, we have to make sure
that the corners of the domain are included in the set of sample points, so that
the triangles in the triangulation cover the domain of the terrain.) The number
of triangles is the same in any triangulation of P. This also holds for the number
of edges. The exact numbers depend on the number of points in P that are on
the boundary of the convex hull of P. (Here we also count points in the interior
of convex hull edges. Hence, the number of points on the convex hull boundary
is not necessarily the same as the number of convex hull vertices.) This is made
precise in the following theorem.
Theorem 9.1 Let P be a set of n points in the plane, not all collinear, and let k
denote the number of points in P that lie on the boundary of the convex hull
of P. Then any triangulation of P has 2n−2−k triangles and 3n−3−k edges.
Proof. Let T be a triangulation of P, and let m denote the number of triangles
of T. Note that the number of faces of the triangulation, which we denote by
nf , is m + 1. Every triangle has three edges, and the unbounded face has k
edges. Furthermore, every edge is incident to exactly two faces. Hence, the
total number of edges of T is ne := (3m+k)/2. Euler's formula tells us that
n−ne +nf = 2.
193

Chapter 9
DELAUNAY TRIANGULATIONS
Plugging the values for ne and nf into the formula, we get m = 2n−2−k, which
in turn implies ne = 3n−3−k.
Let T be a triangulation of P, and suppose it has m triangles. Consider the
3m angles of the triangles of T, sorted by increasing value. Let α1,α2,...,α3m
be the resulting sequence of angles; hence, αi ⩽α j, for i < j. We call A(T) :=
(α1,α2,...,α3m) the angle-vector of T. Let T′ be another triangulation of the
same point set P, and let A(T′) := (α′
1,α′
2,...,α′
3m) be its angle-vector. We
say that the angle-vector of T is larger than the angle-vector of T′ if A(T) is
lexicographically larger than A(T′), or, in other words, if there exists an index i
with 1 ⩽i ⩽3m such that
α j = α′
j for all j < i,
and
αi > α′
i.
We denote this as A(T) > A(T′). A triangulation T is called angle-optimal if
A(T) ⩾A(T′) for all triangulations T′ of P. Angle-optimal triangulations are
interesting because, as we have seen in the introduction to this chapter, they are
good triangulations if we want to construct a polyhedral terrain from a set of
sample points.
Below we will study when a triangulation is angle-optimal. To do this it is
useful to know the following theorem, often called Thales's Theorem. Denote
the smaller angle deﬁned by three points p, q, r by ∡pqr.
ℓ
C
p
q
r
s
a
b
Theorem 9.2 Let C be a circle, ℓa line intersecting C in points a and b, and p,
q, r, and s points lying on the same side of ℓ. Suppose that p and q lie on C, that
r lies inside C, and that s lies outside C. Then
∡arb > ∡apb = ∡aqb > ∡asb.
Now consider an edge e = pipj of a triangulation T of P. If e is not an edge
of the unbounded face, it is incident to two triangles pipjpk and pip jpl. If these
two triangles form a convex quadrilateral, we can obtain a new triangulation
T′ by removing pip j from T and inserting pkpl instead. We call this operation
an edge ﬂip. The only difference in the angle-vector of T and T′ are the six
Figure 9.4
Flipping an edge
α′
1
α′
4
α′
3
α′
5
α′
2
α′
6
pi
p j
pk
pl
pi
α4
α1
α3
α5
α2
α6
p j
pk
pl
edge ﬂip
angles α1,...,α6 in A(T), which are replaced by α′
1,...,α′
6 in A(T′). Figure 9.4
illustrates this. We call the edge e = pipj an illegal edge if
min
1⩽i⩽6αi <
min
1⩽i⩽6α′
i.
194

Section 9.1
TRIANGULATIONS OF PLANAR POINT
SETS
In other words, an edge is illegal if we can locally increase the smallest angle
by ﬂipping that edge. The following observation immediately follows from the
deﬁnition of an illegal edge.
Observation 9.3 Let T be a triangulation with an illegal edge e. Let T′ be the
triangulation obtained from T by ﬂipping e. Then A(T′) > A(T).
It turns out that it is not necessary to compute the angles α1,...,α6,α′
1,...,α′
6
to check whether a given edge is legal. Instead, we can use the simple criterion
stated in the next lemma. The correctness of this criterion follows from Thales's
Theorem.
pi
p j
pk
pl
illegal
Lemma 9.4 Let edge pip j be incident to triangles pipjpk and pipjpl, and let C
be the circle through pi, pj, and pk. The edge pip j is illegal if and only if the
point pl lies in the interior of C. Furthermore, if the points pi, pj, pk, pl form
a convex quadrilateral and do not lie on a common circle, then exactly one of
pipj and pkpl is an illegal edge.
Observe that the criterion is symmetric in pk and pl: pl lies inside the circle
through pi, pj, pk if and only if pk lies inside the circle through pi, pj, pl. When
all four points lie on a circle, both pipj and pkpl are legal. Note that the two
triangles incident to an illegal edge must form a convex quadrilateral, so that it
is always possible to ﬂip an illegal edge.
We deﬁne a legal triangulation to be a triangulation that does not contain
any illegal edge. From the observation above it follows that any angle-optimal
triangulation is legal. Computing a legal triangulation is quite simple, once we
are given an initial triangulation. We simply ﬂip illegal edges until all edges are
legal.
Algorithm LEGALTRIANGULATION(T)
Input. Some triangulation T of a point set P.
Output. A legal triangulation of P.
1.
while T contains an illegal edge pip j
2.
do (∗Flip pipj ∗)
3.
Let pipjpk and pipjpl be the two triangles adjacent to pipj.
4.
Remove pipj from T, and add pkpl instead.
5.
return T
Why does this algorithm terminate? It follows from Observation 9.3 that the
angle-vector of T increases in every iteration of the loop. Since there is only
a ﬁnite number of different triangulations of P, this proves termination of the
algorithm. Once it terminates, the result is a legal triangulation. Although the
algorithm is guaranteed to terminate, it is too slow to be interesting. We have
given the algorithm anyway, because later we shall need a similar procedure.
But ﬁrst we will look at something completely different—or so it seems.
195

Chapter 9
DELAUNAY TRIANGULATIONS
9.2
The Delaunay Triangulation
Let P be a set of n points—or sites, as we shall sometimes call them—in the
plane. Recall from Chapter 7 that the Voronoi diagram of P is the subdivision
of the plane into n regions, one for each site in P, such that the region of a
site p ∈P contains all points in the plane for which p is the closest site. The
Voronoi diagram of P is denoted by Vor(P). The region of a site p is called
Figure 9.5
The dual graph of Vor(P)
Vor(P)
G
the Voronoi cell of p; it is denoted by V(p). In this section we will study the
dual graph of the Voronoi diagram. This graph G has a node for every Voronoi
cell—equivalently, for every site—and it has an arc between two nodes if the
corresponding cells share an edge. Note that this means that G has an arc for
every edge of Vor(P). As you can see in Figure 9.5, there is a one-to-one
correspondence between the bounded faces of G and the vertices of Vor(P).
Figure 9.6
The Delaunay graph DG(P)
Consider the straight-line embedding of G, where the node corresponding
to the Voronoi cell V(p) is the point p, and the arc connecting the nodes of
V(p) and V(q) is the segment pq—see Figure 9.6. We call this embedding the
Delaunay graph of P, and we denote it by DG(P). (Although the name sounds
French, Delaunay graphs have nothing to do with the French painter. They
196

Section 9.2
THE DELAUNAY TRIANGULATION
are named after the Russian mathematician Boris Nikolaevich Delone, who
wrote his own name as "Boris Nikolaeviq Delone," which would be
transliterated into English as "Delone." However, since his work was published
in French—at his time, the languages of science were French and German—his
name is better known in the French transliteration.) The Delaunay graph of a
point set turns out to have a number of surprising properties. The ﬁrst is that it
is always a plane graph: no two edges in the embedding cross.
Theorem 9.5 The Delaunay graph of a planar point set is a plane graph.
Proof. To prove this, we need a property of the edges in the Voronoi diagram
stated in Theorem 7.4(ii). For completeness we repeat the property, phrased
here in terms of Delaunay graphs.
The edge pipj is in the Delaunay graph DG(P) if and only if there
is a closed disc Ci j with pi and p j on its boundary and no other site
of P contained in it. (The center of such a disc lies on the common
edge of V(pi) and V(pj).)
Cij
pi
p j
contained in V(pi)
contained in V(pj)
Deﬁne tij to be the triangle whose vertices are pi, pj, and the center of Cij.
Note that the edge of ti j connecting pi to the center of Cij is contained in V(pi);
a similar observation holds for pj. Now let pkpl be another edge of DG(P),
and deﬁne the circle Ckl and the triangle tkl similar to the way Cij and tij were
deﬁned.
Suppose for a contradiction that pipj and pkpl intersect. Both pk and pl
must lie outside Cij and so they also lie outside ti j. This implies that pkpl must
intersect one of the edges of ti j incident to the center of Cij. Similarly, pipj
must intersect one of the edges of tkl incident to the center of Ckl. It follows
that one of the edges of ti j incident to the center of Cij must intersect one of the
edges of tkl incident to the center of Ckl. But this contradicts that these edges
are contained in disjoint Voronoi cells.
The Delaunay graph of P is an embedding of the dual graph of the Voronoi
diagram. As observed earlier, it has a face for every vertex of Vor(P). The edges
v
f
around a face correspond to the Voronoi edges incident to the corresponding
Voronoi vertex. In particular, if a vertex v of Vor(P) is a vertex of the Voronoi
cells for the sites p1, p2, p3,..., pk, then the corresponding face f in DG(P) has
p1, p2, p3,..., pk as its vertices. Theorem 7.4(i) tells us that in this situation the
points p1, p2, p3,..., pk lie on a circle around v, so we not only know that f is a
k-gon, but even that it is convex.
If the points of P are distributed at random, the chance that four points
happen to lie on a circle is very small. We will—in this chapter—say that a set
of points is in general position if it contains no four points on a circle. If P is
in general position, then all vertices of the Voronoi diagram have degree three,
and consequently all bounded faces of DG(P) are triangles. This explains why
DG(P) is often called the Delaunay triangulation of P. We shall be a bit more
careful, and will call DG(P) the Delaunay graph of P. We deﬁne a Delaunay
triangulation to be any triangulation obtained by adding edges to the Delaunay
graph. Since all faces of DG(P) are convex, obtaining such a triangulation
197

Chapter 9
DELAUNAY TRIANGULATIONS
is easy. Observe that the Delaunay triangulation of P is unique if and only if
DG(P) is a triangulation, which is the case if P is in general position.
We now rephrase Theorem 7.4 about Voronoi diagrams in terms of Delaunay
graphs.
Theorem 9.6 Let P be a set of points in the plane.
(i)
Three points pi, p j, pk ∈P are vertices of the same face of the Delaunay
graph of P if and only if the circle through pi, p j, pk contains no point of
P in its interior.
(ii) Two points pi, p j ∈P form an edge of the Delaunay graph of P if and only
if there is a closed disc C that contains pi and pj on its boundary and does
not contain any other point of P.
Theorem 9.6 readily implies the following characterization of Delaunay
triangulations.
Theorem 9.7 Let P be a set of points in the plane, and let T be a triangulation
of P. Then T is a Delaunay triangulation of P if and only if the circumcircle of
any triangle of T does not contain a point of P in its interior.
Since we argued before that a triangulation is good for the purpose of height
interpolation if its angle-vector is as large as possible, our next step should be
to look at the angle-vector of Delaunay triangulations. We do this by a slight
detour through legal triangulations.
Theorem 9.8 Let P be a set of points in the plane. A triangulation T of P is
legal if and only if T is a Delaunay triangulation of P.
Proof. It follows immediately from the deﬁnitions that any Delaunay triangula-
tion is legal.
pi
p j
pk
pl
C(pipj pk)
pm
C(pipj pm)
e
We shall prove that any legal triangulation is a Delaunay triangulation by
contradiction. So assume T is a legal triangulation of P that is not a Delaunay
triangulation. By Theorem 9.6, this means that there is a triangle pipjpk such
that the circumcircle C(pipjpk) contains a point pl ∈P in its interior. Let
e := pip j be the edge of pip jpl such that the triangle pipjpl does not intersect
pipjpk. Of all such pairs (pipjpk, pl) in T, choose the one that maximizes the
angle ∡pipl p j. Now look at the triangle pipjpm adjacent to pipjpk along e.
Since T is legal, e is legal. By Lemma 9.4 this implies that pm does not lie in the
interior of C(pipjpk). The circumcircle C(pip jpm) of pipjpm contains the part
of C(pipjpk) that is separated from pipjpk by e. Consequently, pl ∈C(pip jpm).
Assume that pjpm is the edge of pipjpm such that pjpmpl does not intersect
pipjpm. But now ∡p jpl pm > ∡pipl pj by Thales's Theorem, contradicting the
deﬁnition of the pair (pipjpk, pl).
Since any angle-optimal triangulation must be legal, Theorem 9.8 implies
that any angle-optimal triangulation of P is a Delaunay triangulation of P. When
P is in general position, there is only one legal triangulation, which is then the
only angle-optimal triangulation, namely the unique Delaunay triangulation
198

Section 9.3
COMPUTING THE DELAUNAY
TRIANGULATION
that coincides with the Delaunay graph. When P is not in general position,
then any triangulation of the Delaunay graph is legal. Not all these Delaunay
triangulations need to be angle-optimal. However, their angle-vectors do not
differ too much. Moreover, using Thales's Theorem one can show that the
minimum angle in any triangulation of a set of co-circular points is the same,
that is, the minimum angle is independent of the triangulation. This implies that
any triangulation turning the Delaunay graph into a Delaunay triangulation has
the same minimum angle. The following theorem summarizes this.
Theorem 9.9 Let P be a set of points in the plane. Any angle-optimal trian-
gulation of P is a Delaunay triangulation of P. Furthermore, any Delaunay
triangulation of P maximizes the minimum angle over all triangulations of P.
9.3
Computing the Delaunay Triangulation
We have seen that for our purpose—approximating a terrain by constructing a
polyhedral terrain from a set P of sample points—a Delaunay triangulation of P
is a suitable triangulation. This is because the Delaunay triangulation maximizes
the minimum angle. So how do we compute such a Delaunay triangulation?
We already know from Chapter 7 how to compute the Voronoi diagram
of P. From Vor(P) we can easily obtain the Delaunay graph DG(P), and by
triangulating the faces with more than three vertices we can obtain a Delaunay
triangulation. In this section we describe a different approach: we will compute
a Delaunay triangulation directly, using the randomized incremental approach
we have so successfully applied to the linear programming problem in Chapter 4
and to the point location problem in Chapter 6.
In Chapter 6 we found it convenient to start with a large rectangle containing
p−1
p0
p−2
the scene, to avoid problems caused by unbounded trapezoids. In the same
spirit we now start with a large triangle that contains the set P. We will add
two extra points p−1 and p−2 that, together with the highest point p0 of P,
form a triangle containing all the points. This means we are now computing a
Delaunay triangulation of P∪{p−1, p−2} instead of the Delaunay triangulation
of P. Later we want to obtain the Delaunay triangulation of P by discarding p−1
and p−2, together with all incident edges. For this to work we have to choose
p−1 and p−2 far enough away, so that they don't destroy any triangles in the
Delaunay triangulation of P. In particular, we must ensure they do not lie in
any circle deﬁned by three points in P. We postpone the details of this to a later
stage; ﬁrst we have a look at the algorithm.
The algorithm is randomized incremental, so it adds the points in random
order and it maintains a Delaunay triangulation of the current point set. Consider
the addition of a point pr. We ﬁrst ﬁnd the triangle of the current triangulation
that contains pr—how this is done will be explained later—and we add edges
from pr to the vertices of this triangle. If pr happens to fall on an edge e of
the triangulation, we have to add edges from pr to the opposite vertices in
the triangles sharing e. Figure 9.7 illustrates these two cases. We now have
199

Chapter 9
DELAUNAY TRIANGULATIONS
a triangulation again, but not necessarily a Delaunay triangulation. This is
because the addition of pr can make some of the existing edges illegal. To
Figure 9.7
The two cases when adding a point pr
pr
pi
pk
p j
pr
p j
pi
pk
pl
pr lies in the interior of a triangle
pr falls on an edge
remedy this, we call a procedure LEGALIZEEDGE with each potentially illegal
edge. This procedure replaces illegal edges by legal ones through edge ﬂips.
Before we come to the details of this, we give a precise description of the main
algorithm. It will be convenient for the analysis to let P be a set of n+1 points.
Algorithm DELAUNAYTRIANGULATION(P)
Input. A set P of n+1 points in the plane.
Output. A Delaunay triangulation of P.
1.
Let p0 be the lexicographically highest point of P, that is, the rightmost
among the points with largest y-coordinate.
2.
Let p−1 and p−2 be two points in R2 sufﬁciently far away and such that P
is contained in the triangle p0p−1p−2.
3.
Initialize T as the triangulation consisting of the single triangle p0p−1p−2.
4.
Compute a random permutation p1, p2,..., pn of P\{p0}.
5.
for r ←1 to n
6.
do (∗Insert pr into T: ∗)
7.
Find a triangle pipjpk ∈T containing pr.
8.
if pr lies in the interior of the triangle pipjpk
9.
then Add edges from pr to the three vertices of pipjpk, thereby
splitting pipjpk into three triangles.
10.
LEGALIZEEDGE(pr, pipj,T)
11.
LEGALIZEEDGE(pr, pjpk,T)
12.
LEGALIZEEDGE(pr, pkpi,T)
13.
else (∗pr lies on an edge of pipjpk, say the edge pipj ∗)
14.
Add edges from pr to pk and to the third vertex pl of the
other triangle that is incident to pip j, thereby splitting the
two triangles incident to pipj into four triangles.
15.
LEGALIZEEDGE(pr, pipl,T)
16.
LEGALIZEEDGE(pr, pl pj,T)
17.
LEGALIZEEDGE(pr, pjpk,T)
18.
LEGALIZEEDGE(pr, pkpi,T)
19. Discard p−1 and p−2 with all their incident edges from T.
20. return T
200

Section 9.3
COMPUTING THE DELAUNAY
TRIANGULATION
Next we discuss the details of turning the triangulation we get after line 9 (or
line 14) into a Delaunay triangulation. We know from Theorem 9.8 that a
triangulation is a Delaunay triangulation if all its edges are legal. In the spirit
of algorithm LEGALTRIANGULATION, we therefore ﬂip illegal edges until the
triangulation is legal again. The question that remains is which edges may
become illegal due to the insertion of pr. Observe that an edge pipj that was
legal before can only become illegal if one of the triangles incident to it has
changed. So only the edges of the new triangles need to be checked. This
is done using the subroutine LEGALIZEEDGE, which tests and possibly ﬂips
an edge. If LEGALIZEEDGE ﬂips an edge, other edges may become illegal.
Therefore LEGALIZEEDGE calls itself recursively with such potentially illegal
edges.
pr
pi
p j
pk
LEGALIZEEDGE(pr, pipj,T)
1.
(∗The point being inserted is pr, and pipj is the edge of T that may need
to be ﬂipped. ∗)
2.
if pipj is illegal
3.
then Let pipjpk be the triangle adjacent to prpipj along pipj.
4.
(∗Flip pipj: ∗) Replace pip j with prpk.
5.
LEGALIZEEDGE(pr, pipk,T)
6.
LEGALIZEEDGE(pr, pkp j,T)
The test in line 2 whether an edge is illegal can normally be done by applying
Lemma 9.4. There are some complications because of the presence of the
special points p−1 and p−2. We shall come back to this later; ﬁrst we prove that
the algorithm is correct.
=⇒
pr
Figure 9.8
All edges created are incident to pr
To ensure the correctness of the algorithm, we need to prove that no illegal
edges remain after all calls to LEGALIZEEDGE have been processed. From
the code of LEGALIZEEDGE it is clear that every new edge created due to the
insertion of pr is incident to pr. Figure 9.8 illustrates this; the triangles that
are destroyed and the new triangles are shown in grey. The crucial observation
(proved below) is that every new edge must be legal, so there is no need to test
them. Together with the earlier observation that an edge can only become illegal
if one of its incident triangles changes, this proves that the algorithm tests any
edge that may become illegal. Hence, the algorithm is correct. Note that, as in
Algorithm LEGALTRIANGULATION, the algorithm cannot get into an inﬁnite
loop, because every ﬂip makes the angle-vector of the triangulation larger.
201

Chapter 9
DELAUNAY TRIANGULATIONS
Lemma 9.10 Every new edge created in DELAUNAYTRIANGULATION or in
LEGALIZEEDGE during the insertion of pr is an edge of the Delaunay graph of
{p−2, p−1, p0,..., pr}.
Proof. Consider ﬁrst the edges prpi, prp j, prpk (and perhaps prpl) created by
splitting pipjpk (and maybe pip jpl). Since pip jpk is a triangle in the Delaunay
triangulation before the addition of pr, the circumcircle C of pipjpk contains
no point pt with t < r in its interior. By shrinking C we can ﬁnd a circle C′
through pi and pr contained in C. Because C′ ⊂C we know that C′ is empty.
This implies that prpi is an edge of the Delaunay graph after the addition of pr.
The same holds for prpj and prpk (and for prpl, if it exists).
Now consider an edge ﬂipped by LEGALIZEEDGE. Such an edge ﬂip
always replaces an edge pipj of a triangle pipjpl by an edge prpl incident to
pr. Since pip jpl was a Delaunay triangle before the addition of pr and because
its circumcircle C contains pr—otherwise pip j would not be illegal—we can
shrink the circumcircle to obtain an empty circle C′ with only pr and pl on its
boundary. Hence, prpl is an edge of the Delaunay graph after the addition.
pi
p j
pl
pr
C
C′
We have proved the correctness of the algorithm. What remains is to describe
how to implement two important steps: how to ﬁnd the triangle containing the
point pr in line 7 of DELAUNAYTRIANGULATION, and how to deal correctly
with the points p−1 and p−2 in the test in line 2 in LEGALIZEEDGE. We start
with the former issue.
To ﬁnd the triangle containing pr we use an approach quite similar to what we
did in Chapter 6: while we build the Delaunay triangulation, we also build a
point location structure D, which is a directed acyclic graph. The leaves of
D correspond to the triangles of the current triangulation T, and we maintain
cross-pointers between those leaves and the triangulation. The internal nodes of
D correspond to triangles that were in the triangulation at some earlier stage,
but have already been destroyed. The point location structure is built as follows.
In line 3 we initialize D as a DAG with a single leaf node, which corresponds
to the triangle p0p−1p−2.
Now suppose that at some point we split a triangle pipjpk of the current
triangulation into three (or two) new triangles. The corresponding change in
D is to add three (or two) new leaves to D, and to make the leaf for pip jpk
into an internal node with outgoing pointers to those three (or two) leaves.
Similarly, when we replace two triangles pkpipj and pipjpl by triangles pkpipl
and pkpl pj by an edge ﬂip, we create leaves for the two new triangles, and the
nodes of pkpipj and pipjpl get pointers to the two new leaves. Figure 9.9 shows
an example of the changes in D caused by the addition of a point. Observe
that when we make a leaf into an internal node, it gets at most three outgoing
pointers.
Using D we can locate the next point pr to be added in the current triangu-
lation. This is done as follows. We start at the root of D, which corresponds
to the initial triangle p0p−1p−2. We check the three children of the root to see
in which triangle pr lies, and we descend to the corresponding child. We then
202

Section 9.3
COMPUTING THE DELAUNAY
TRIANGULATION
∆1
∆2
∆3
∆4
∆3
∆5
∆4
∆5
∆1
∆1
∆2
∆2
∆3
∆3
∆1
∆2
∆3
∆4
∆4
∆5
∆6
∆7
∆7
∆6
split ∆1
ﬂip pip j
ﬂip pipk
pr
∆1
∆2
∆3
∆2
∆3
pi
p j
pi
pk
Figure 9.9
The effect of inserting point pr into
triangle ∆1 on the data structure D (the
part of D that does not change is
omitted in the ﬁgure)
203

Chapter 9
DELAUNAY TRIANGULATIONS
check the children of this node, descend to a child whose triangle contains pr,
and so on, until we reach a leaf of D. This leaf corresponds to a triangle in the
current triangulation that contains pr. Since the out-degree of any node is at
most three, this takes linear time in the number of nodes on the search path, or,
in other words, in the number of triangles stored in D that contain pr.
There is only one detail left, namely how to choose p−1 and p−2, and how to
implement the test of whether an edge is legal. On the one hand, we have to
choose p−1 and p−2 to be far away, because we don't want their presence to
inﬂuence the Delaunay triangulation of P. One the other hand, we don't want to
introduce the huge coordinates needed for that. So what we do is to treat these
points symbolically: we do not actually assign coordinates to them, but instead
modify the tests for point location and for illegal edges such that they work as if
we had chosen the points to be very far away.
In the following, we will say that p = (xp,yp) is higher than q = (xq,yq)
if yp > yq or yp = yq and xq > xp, and use the (lexicographic) ordering on P
induced by this relation.
Let ℓ−1 be a horizontal line lying below the entire set P, and let ℓ−2 be a
horizontal line lying above P. Conceptually, we choose p−1 to lie on the line
ℓ−1 sufﬁciently far to the right that p−1 lies outside every circle deﬁned by three
non-collinear points of P, and such that the clockwise ordering of the points of
P around p−1 is identical to their (lexicographic) ordering. Next, we choose
p−2 to lie on the line ℓ−2 sufﬁciently far to the left that p−2 lies outside every
circle deﬁned by three non-collinear points of P ∪{p−1}, and such that the
counterclockwise ordering of the points of P∪{p−1} around p−2 is identical to
their (lexicographic) ordering.
The Delaunay triangulation of P ∪{p−1, p−2} consists of the Delaunay
triangulation of P, edges connecting p−1 to every point on the right convex hull
of P, edges connecting p−2 to every point on the left convex hull of P, and
the one edge p−1p−2. The lowest point of P and the highest point p0 of P are
connected to both p−1 and p−2.
During the point location step, we need to determine the position of a
point pj with respect to the oriented line from pi to pk. By our choice of p−1
and p−2, the following conditions are equivalent:
pj lies to the left of the line from pi to p−1;
pj lies to the left of the line from p−2 to pi;
pj is lexicographically larger than pi.
It remains to explain how to treat p−1 and p−2 when we check whether an
edge is illegal. Let pip j be the edge to be tested, and let pk and pl be the other
vertices of the triangles incident to pipj (if they exist).
pipj is an edge of the triangle p0p−1p−2. These edges are always legal.
The indices i, j,k,l are all non-negative. This is the normal case; none of
the points involved in the test is treated symbolically. Hence, pipj is illegal
if and only if pl lies inside the circle deﬁned by pi, pj, and pk.
All other cases. In this case, pipj is legal if and only if min(k,l) < min(i, j).
Only the last case requires further justiﬁcation. Since the situation where
pipj is p−1p−2 is handled in the ﬁrst case, at most one of the indices i and j
204

Section 9.4
THE ANALYSIS
is negative. On the other hand, either pk or pl is the point pr that we have just
inserted, and so at most one of the indices k and l is negative.
If only one of the four indices is negative, then this point lies outside the
circle deﬁned by the other three points, and the method is correct.
Otherwise, both min(i, j) and min(k,l) are negative, and the fact that p−2
lies outside any circle deﬁned by three points in P ∪{p−1} implies that the
method is correct.
9.4
The Analysis
We ﬁrst look at the structural change generated by the algorithm. This is the
number of triangles created and deleted during the course of the algorithm.
Before we start the analysis, we introduce some notation: Pr := {p1,..., pr}
and DGr := DG({p−2, p−1, p0}∪Pr).
Lemma 9.11 The expected number of triangles created by algorithm DELAU-
NAYTRIANGULATION is at most 9n+1.
Proof. In the beginning, we create the single triangle p0p−1p−2. In iteration r
of the algorithm, when we insert pr, we ﬁrst split one or two triangles, creating
three or four new triangles. This splitting creates the same number of edges in
DGr, namely prpi, prp j, prpk (and maybe prpl). Furthermore, for every edge
that we ﬂip in procedure LEGALIZEEDGE, we create two new triangles. Again,
the ﬂipping creates an edge of DGr incident to pr. To summarize: if after the
insertion of pr there are k edges of DGr incident to pr, then we have created
at most 2(k −3)+3 = 2k −3 new triangles. The number k is the degree of pr
in DGr; we denote this degree by deg(pr,DGr). degree of pr, over all possible
permutations of the set P? As in Chapter 4 and 6 we use backwards analysis to
bound this value. So, for the moment, we ﬁx the set Pr. We want to bound the
expected degree of the point pr, which is a random element of the set Pr. By
Theorem 7.3, the Delaunay graph DGr has at most 3(r +3)−6 edges. Three of
these are the edges of p0p−1p−2, and therefore the total degree of the vertices
in Pr is less than 2[3(r +3)−9] = 6r. This means that the expected degree of
a random point of Pr is at most 6. Summarizing the above, we can bound the
number of triangles created in step r as follows.
E
!
number of triangles created in step r
"
⩽
E
!
2deg(pr,DGr)−3
"
=
2E
!
deg(pr,DGr)
"
−3
⩽
2·6−3 = 9
The total number of created triangles is one for the triangle p0p−1p−2 that we
start with, plus the number of triangles created in each of the insertion steps.
Using linearity of expectation, we get that the expected total number of created
triangles is bounded by 1+9n.
We now state the main result.
205

Chapter 9
DELAUNAY TRIANGULATIONS
Theorem 9.12 The Delaunay triangulation of a set P of n points in the plane
can be computed in O(nlogn) expected time, using O(n) expected storage.
Proof. The correctness of the algorithm follows from the discussion above. As
for the storage requirement, we note that only the search structure D could use
more than linear storage. However, every node of D corresponds to a triangle
created by the algorithm, and by the previous lemma the expected number of
these is O(n).
To bound the expected running time we ﬁrst ignore the time spent in the
point location step (line 7). Now the time spent by the algorithm is proportional
to the number of created triangles. From the previous lemma we can therefore
conclude that the expected running time, not counting the time for point location,
is O(n).
It remains to account for the point location steps. The time to locate the
point pr in the current triangulation is linear in the number of nodes of D that we
visit. Any visited node corresponds to a triangle that was created at some earlier
stage and that contains pr. If we count the triangle of the current triangulation
separately, then the time for locating pr is O(1) plus linear time in the number
of triangles that were present at some earlier stage, but have been destroyed,
and contain pr.
A triangle pipjpk can be destroyed from the triangulation for one of two
reasons:
A new point pl has been inserted inside (or on the boundary of) pipjpk, and
pipjpk was split into three (or two) subtriangles.
An edge ﬂip has replaced pipjpk and an adjacent triangle pipjpl by the pair
pkpipl and pkpjpl.
In the ﬁrst case, the triangle pipjpk was a Delaunay triangle before pl was
inserted. In the second case, either pipjpk was a Delaunay triangle and pl was
inserted, or pip jpl was a Delaunay triangle and pk was inserted. If pipjpl was
the Delaunay triangle, then the fact that the edge pipj was ﬂipped means that
both pk and pr lie inside the circumcircle of pipjpl.
In all cases we can charge the fact that triangle pip jpk was visited to a
Delaunay triangle ∆that has been destroyed in the same stage as pip jpk, and
such that the circumcircle of ∆contains pr. Denote the subset of points in P
that lie in the circumcircle of a given triangle ∆by K(∆). In the argument above
the visit to a triangle during the location of pr is charged to a triangle ∆with
pr ∈K(∆). It is easy to see that a triangle ∆can be charged at most once for
every one of the points in K(∆). Therefore the total time for the point location
steps is
O(n+∑
∆
card(K(∆))),
(9.1)
where the summation is over all Delaunay triangles ∆created by the algorithm.
We shall prove later that the expected value of this sum is O(nlogn). This
proves the theorem.
It remains to bound the expected size of the sets K(∆). If ∆is a triangle of
the Delaunay triangulation DGr, then what would we expect card(K(∆)) to be?
206

Section 9.4
THE ANALYSIS
For r = 1 we would expect it to be roughly n, and for r = n we know that it is
zero. What happens in between? The nice thing about randomization is that it
"interpolates" between those two extremes. The right intuition would be that,
since Pr is a random sample, the number of points lying inside the circumcircle
of a triangle ∆∈DGr is about O(n/r). But be warned: this is not really true for
all triangles in DGr. Nevertheless, the sum in expression (9.1) behaves as if it
were true.
In the remainder of this section we will give a quick proof of this fact for
the case of a point set in general position. The result is true for the general case
as well, but to see that we have to work a little bit harder, so we postpone that to
the next section, where we treat the problem in more generality.
Lemma 9.13 If P is a point set in general position, then
∑
∆
card(K(∆)) = O(nlogn),
where the summation is over all Delaunay triangles ∆created by the algorithm.
Proof. Since P is in general position, every subset Pr is in general position. This
implies that the triangulation after adding the point pr is the unique triangulation
DGr. We denote the set of triangles of DGr by Tr. Now the set of Delaunay
triangles created in stage r equals Tr \Tr−1 by deﬁnition. Hence, we can rewrite
the sum we want to bound as
n
∑
r=1
#
∑
∆∈Tr\Tr−1
card(K(∆))
$
.
For a point q, let k(Pr,q) denote the number of triangles ∆∈Tr such that
q ∈K(∆), and let k(Pr,q, pr) be the number of triangles ∆∈Tr such that not
only q ∈K(∆) but for which we also have that pr is incident to ∆. Recall that
any Delaunay triangle created in stage r is incident to pr, so we have
∑
∆∈Tr\Tr−1
card(K(∆)) = ∑
q∈P\Pr
k(Pr,q, pr).
(9.2)
For the moment, we ﬁx Pr. In other words, we consider all expectations to be
over the set of permutations of the set P where Pr is equal to a ﬁxed set P∗
r . The
value of k(Pr,q, pr) then depends only on the choice of pr. Since a triangle
∆∈Tr is incident to a random point p ∈P∗
r with probability at most 3/r, we get
E
!
k(Pr,q, pr)
"
⩽3k(Pr,q)
r
.
If we sum this over all q ∈P\Pr and use (9.2), we get
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽3
r ∑
q∈P\Pr
k(Pr,q).
(9.3)
Every q ∈P\Pr is equally likely to appear as pr+1, and so we have
E
!
k(Pr, pr+1)
"
=
1
n−r ∑
q∈P\Pr
k(Pr,q).
207

Chapter 9
DELAUNAY TRIANGULATIONS
We can substitute this into (9.3), and get
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽3
%n−r
r
&
E
!
k(Pr, pr+1)
"
.
What is k(Pr, pr+1)? It is the number of triangles ∆of Tr that have pr+1 ∈K(∆).
By the criterion from Theorem 9.6 (i), these triangles are exactly the triangles
of Tr that will be destroyed by the insertion of pr+1. Hence, we can rewrite the
previous expression as
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽3
%n−r
r
&
E
!
card(Tr \Tr+1)
"
.
Theorem 9.1 shows that the number of triangles in Tm is precisely 2(m+3)−
2−3 = 2m+1. Therefore, the number of triangles destroyed by the insertion
of point pr+1 is exactly two less than the number of triangles created by the
insertion of pr+1, and we can rewrite the sum as
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽3
%n−r
r
&%
E
!
card(Tr+1 \Tr)
"
−2
&
.
Until now we considered Pr to be ﬁxed. At this point, we can simply take the
average over all choices of Pr ⊂P on both sides of the inequality above, and
ﬁnd that it also holds if we consider the expectation to be over all possible
permutations of the set P.
We already know that the number of triangles created by the insertion of
pr+1 is identical to the number of edges incident to pr+1 in Tr+1, and that the
expected number of these edges is at most 6. We conclude that
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽12
%n−r
r
&
.
Summing over r proves the lemma.
9.5*
A Framework for Randomized Algorithms
Up to now we have seen three randomized incremental algorithms in this book:
one for linear programming in Chapter 4, one for computing a trapezoidal map
in Chapter 6, and one for computing a Delaunay triangulation in this chapter.
(We will see one more in Chapter 11.) These algorithms, and most other
randomized incremental algorithms in the computational geometry literature,
all work according to the following principle.
Suppose the problem is to compute some geometric structure T(X), deﬁned
by a set X of geometric objects. (For instance, a Delaunay triangulation de-
ﬁned by a set of points in the plane.) A randomized incremental algorithm
does this by adding the objects in X in random order, meanwhile maintaining
the structure T. To add the next object, the algorithm ﬁrst ﬁnds out where
208

Section 9.5*
A FRAMEWORK FOR RANDOMIZED
ALGORITHMS
the current structure has to be changed because there is a conﬂict with the
object—the location step—and then it updates the structure locally—the update
step. Because all randomized incremental algorithms are so much alike, their
analyses are quite similar as well. To avoid having to prove the same bounds
over and over again for different problems, an axiomatic framework has been
developed that captures the essence of randomized incremental algorithms. This
framework—called a conﬁguration space—can be used to prove ready-to-use
bounds for the expected running time of many randomized incremental algo-
rithms. (Unfortunately, the term "conﬁguration space" is also used in motion
planning, where it means something completely different—see Chapter 13.) In
this section we describe this framework, and we give a theorem that can be used
to analyze any randomized incremental algorithm that ﬁts into the framework.
For instance, the theorem can immediately be applied to prove Lemma 9.13,
this time without assuming that P has to be in general position.
A conﬁguration space is deﬁned to be a four-tuple (X,Π,D,K). Here X is the
input to the problem, which is a ﬁnite set of (geometric) objects; we denote the
cardinality of X by n. The set Π is a set whose elements are called conﬁgurations.
Finally, D and K both assign to every conﬁguration ∆∈Π a subset of X, denoted
D(∆) and K(∆) repectively. Elements of the set D(∆) are said to deﬁne the
conﬁguration ∆, and the elements of the set K(∆) are said to be in conﬂict with,
or to kill, ∆. The number of elements of K(∆) is called the conﬂict size of the
conﬁguration ∆. We require that (X,Π,D,K) satisﬁes the following conditions.
The number d := max{card(D(∆)) | ∆∈Π} is a constant. We call this
number the maximum degree of the conﬁguration space. Moreover, the
number of conﬁgurations sharing the same deﬁning set should be bounded
by a constant.
We have D(∆)∩K(∆) = /0 for all conﬁgurations ∆∈Π.
A conﬁguration ∆is called active over a subset S ⊆X if D(∆) is contained in S
and K(∆) is disjoint from S. We denote the set of conﬁgurations active over S
by T(S), so we have
T(S) := {∆∈Π : D(∆) ⊆S and K(∆)∩S = /0}.
The active conﬁgurations form the structure we want to compute. More precisely,
the goal is to compute T(X). Before we continue our discussion of this abstract
framework, let's see how the geometric structures we have met so far ﬁt in.
Half-plane intersection.
In this case the input set X is a set of half-planes
in the plane. We want to deﬁne Π, D, and K in such a way that T(X) is what
we want to compute, namely the intersection of the half-planes in X. We
can achieve this as follows. The set Π of conﬁgurations consists of all the
intersection points of the lines bounding the half-planes in X. The deﬁning set
D(∆) of a conﬁguration ∆∈Π consists of the two lines deﬁning the intersection,
and the killing set K(∆) consists of all half-planes that do not contain the
intersection point. Hence, for any subset S ⊂X, and in particular for X itself,
T(S) is the set of vertices of the common intersection of the half-planes in S.
209

Chapter 9
DELAUNAY TRIANGULATIONS
Trapezoidal maps.
Here the input set X is a set of segments in the plane. The
set Π of conﬁgurations contains all trapezoids appearing in the trapezoidal map
of any S ⊆X. The deﬁning set D(∆) of a conﬁguration ∆is the set of segments
that are necessary to deﬁne ∆. The killing set K(∆) of a trapezoid ∆is the set
of segments that intersect ∆. With these deﬁnitions, T(S) is exactly the set of
trapezoids of the trapezoidal map of S.
Delaunay Triangulation.
The input set X is a set of points in general position
in the plane. The set Π of conﬁgurations consists of triangles formed by three
(non-collinear) points in X. The deﬁning set D(∆) consists of the points that
form the vertices of ∆, and the killing set K(∆) is the set of points lying inside
the circumcircle of ∆. By Theorem 9.6, T(S) is exactly the set of triangles of
the unique Delaunay triangulation of S.
As stated earlier, the goal is to compute the structure T(X). Randomized incre-
mental algorithms do this by computing a random permutation x1,x2,...,xn of
the objects in X and then adding the objects in this order, meanwhile maintaining
T(Xr), where Xr := {x1,x2,...,xr}. The fundamental property of conﬁguration
spaces that makes this possible is that we can decide whether or not a conﬁg-
uration ∆appears in T(Xr) by looking at it locally—we only need to look for
the deﬁning and killing objects of ∆. In particular, T(Xr) does not depend on
the order in which the objects in Xr were added. For instance, a triangle ∆is in
the Delaunay triangulation of S if and only if the vertices of ∆are in S, and no
point of S lies in the circumcircle of ∆.
The ﬁrst thing we usually did when we analyzed a randomized incremental
algorithm was to prove a bound on the expected structural change—see for
instance Lemma 9.11. The next theorem does the same, but now in the abstract
conﬁguration-space framework.
Theorem 9.14 Let (X,Π,D,K) be a conﬁguration space, and let T and Xr be
deﬁned as above. Then the expected number of conﬁgurations in T(Xr) \
T(Xr−1) is at most
d
r E[card(T(Xr))],
where d is the maximum degree of the conﬁguration space.
Proof. As in previous occasions where we wanted to bound the structural
change, we use backwards analysis: instead of trying to argue about the number
of conﬁgurations that appear due to the addition of xr into Xr−1, we shall argue
about the number of conﬁgurations that disappear when we remove xr from Xr.
To this end we temporarily let Xr be some ﬁxed subset X∗
r ⊂X of cardinality r.
We now want to bound the expected number of conﬁgurations ∆∈T(Xr) that
disappear when we remove a random object xr from Xr. By deﬁnition of T, such
a conﬁguration ∆must have xr ∈D(∆). Since there are at most d ·card(T(Xr))
pairs (x,∆) with ∆∈T(Xr) and x ∈D(∆), we have
∑
x∈Xr
card({∆∈T(Xr) | x ∈D(∆)}) ⩽d ·card(T(Xr)).
210

Section 9.5*
A FRAMEWORK FOR RANDOMIZED
ALGORITHMS
Hence, the expected number of conﬁgurations disappearing due to the removal
of a random object from Xr is at most d
r card(T(Xr)). In this argument, the set
Xr was a ﬁxed subset X∗
r ⊂X of cardinality r. To obtain the general bound,
we have to average over all possible subsets of size r, which gives a bound of
d
r E
!
card(T(Xr))
"
.
This theorem gives a generic bound for the expected size of the structural
change during a randomized incremental algorithm. But what about the cost of
the location steps? In many cases we will need a bound of the same form as in
this chapter, namely we need to bound
∑
∆
card(K(∆)),
where the summation is over all conﬁgurations ∆that are created by the algo-
rithm, that is, all conﬁgurations that appear in one of the T(Xr). This bound is
given in the following theorem.
Theorem 9.15 Let (X,Π,D,K) be a conﬁguration space, and let T and Xr be
deﬁned as above. Then the expected value of
∑
∆
card(K(∆)),
where the summation is over all conﬁgurations ∆appearing in at least one T(Xr)
with 1 ⩽r ⩽n, is at most
n
∑
r=1
d2%n−r
r
&%E
!
card(T(Xr))
"
r
&
,
where d is the maximum degree of the conﬁguration space.
Proof. We can follow the proof of Lemma 9.13 quite closely. We ﬁrst rewrite
the sum as
n
∑
r=1
#
∑
∆∈Tr\Tr−1
card(K(∆))
$
.
Next, let k(Xr,y) denote the number of conﬁgurations ∆∈T(Xr) such that
y ∈K(∆), and let k(Xr,y,xr) be the number of conﬁgurations ∆∈T(Xr) such that
not only y ∈K(∆) but for which we also have xr ∈D(∆). Any new conﬁguration
appearing due to the addition of xr must have xr ∈D(∆). This implies that
∑
∆∈Tr\Tr−1
card(K(∆)) = ∑
y∈X\Xr
k(Xr,y,xr).
(9.4)
We now ﬁx the set Xr. The expected value of k(Xr,y,xr) then depends only on
the choice of xr ∈Xr. Since the probability that y ∈D(∆) for a conﬁguration
∆∈T(Xr) is at most d/r, we have
E
!
k(Xr,y,xr)
"
⩽dk(Xr,y)
r
.
211

Chapter 9
DELAUNAY TRIANGULATIONS
If we sum this over all y ∈X \Xr and use (9.4), we get
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽d
r ∑
y∈X\Xr
k(Xr,y).
(9.5)
On the other hand, every y ∈X \Xr is equally likely to appear as xr+1, so
E
!
k(Xr,xr+1)
"
=
1
n−r ∑
y∈X\Xr
k(Xr,y).
Substituting this into (9.5) gives
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽d
%n−r
r
&
E
!
k(Xr,xr+1)
"
.
Now observe that k(Xr,xr+1) is the number of conﬁgurations ∆of T(Xr) that
will be destroyed in the next stage, when xr+1 is inserted. This means we can
rewrite the last expression as
E
!
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽d
%n−r
r
&
E
!
card(T(Xr)\T(Xr+1))
"
.
(9.6)
Unlike in the proof of Lemma 9.13, however, we cannot simply bound the num-
ber of conﬁgurations destroyed in stage r +1 by the number of conﬁgurations
created at that stage, because that need not be true in a general conﬁguration
space. Hence, we proceed somewhat differently.
First we observe that we can take the average over all choices of Xr on both
sides of (9.6) and ﬁnd that it also holds if the expectation is over all permutations
of X. Next, we sum over all r, and rewrite the sum as follows:
n
∑
r=1
d
%n−r
r
&
card(T(Xr)\T(Xr+1)) = ∑
∆
d
%n−[j(∆)−1]
j(∆)−1
&
,
(9.7)
where the summation on the right hand side is over all conﬁgurations ∆that
are created and later destroyed by the algorithm, and where j(∆) denotes the
stage when conﬁguration ∆is destroyed. Let i(∆) denote the stage when the
conﬁguration ∆is created. Since i(∆) ⩽j(∆)−1, we have
n−[j(∆)−1]
j(∆)−1
=
n
j(∆)−1 −1 ⩽
n
i(∆) −1 = n−i(∆)
i(∆)
.
If we substitute this into (9.7), we see that
n
∑
r=1
d
%n−r
r
&
card(T(Xr)\T(Xr+1)) ⩽∑
∆
d
%n−i(∆)
i(∆)
&
.
The right hand side of this expression is at most
n
∑
r=1
d
%n−r
r
&
card(T(Xr)\T(Xr−1))
212

Section 9.5*
A FRAMEWORK FOR RANDOMIZED
ALGORITHMS
(the difference being only those conﬁgurations that are created but never de-
stroyed) and so we have
E
! n
∑
r=1
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽
n
∑
r=1
d
%n−r
r
&
E
!
card(T(Xr)\T(Xr−1))
"
.
By Theorem 9.14, we get the bound we wanted to prove:
E
! n
∑
r=1
∑
∆∈Tr\Tr−1
card(K(∆))
"
⩽
n
∑
r=1
d
%n−r
r
&d
r E
!
card(T(Xr))
"
.
This ﬁnishes the analysis in the abstract setting. As an example, we will
show how to apply the results to our randomized incremental algorithm for
computing the Delaunay triangulation. In particular, we will prove that
∑
∆
card(K(∆)) = O(nlogn),
where the summation is over all triangles ∆created by the algorithm, and where
K(∆) is the set of points in the circumcircle of the triangle.
Unfortunately, it seems impossible to properly deﬁne a conﬁguration space
whose conﬁgurations are triangles when the points are not in general position.
Therefore we shall choose the conﬁgurations slightly differently.
Let P be a set of points in the plane, not necessarily in general position. Let Ω:=
{p0, p−1, p−2} denote the set of three points we used to start the construction.
Recall that p0 is the lexicographically largest point from P, while points p−1
and p−2 were chosen such that they do not destroy any Delaunay edges between
points in P. We set X := P\{p0}. Every triple ∆= (pi, p j, pk) of points in X ∪Ω
that do not lie on a line deﬁnes a conﬁguration with D(∆) := {pi, p j, pk}∩X
and K(∆) is the set of points of X that lie either in the interior of the circumcircle
of the triangle pipjpk or on the circular arc on the circumcircle from pi to pk
containing pj. We call such a conﬁguration ∆a Delaunay corner of X, because
∆is active over S ⊆X if and only if pi, pj, and pk are consecutive points on the
boundary of one face of the Delaunay graph DG(Ω∪S). Note that any set of
three non-collinear points deﬁnes three different conﬁgurations.
The important observation is that whenever DELAUNAYTRIANGULATION
creates a new triangle, this triangle is of the form piprpj, where pr is the
point inserted in this stage, and prpi and prpj are edges of the Delaunay graph
DG(Ω∪Pr)—see Lemma 9.10. It follows that when the triangle piprpj is
created, the triple (p j, pr, pi) is a Delaunay corner of DG(Ω∪Pr) and, hence,
it is an active conﬁguration over the set Pr. The set K(∆) deﬁned for this
conﬁguration contains all points contained in the circumcircle of the triangle
piprpj. We can therefore bound the original sum by
pi
p j
pk
points in K(∆)
points not in K(∆)
∑
∆
card(K(∆)),
where the sum is over all Delaunay corners ∆that appear in some intermediate
Delaunay graph DG(Ω∪Pr).
213

Chapter 9
DELAUNAY TRIANGULATIONS
Now Theorem 9.15 applies. How many Delaunay corners are there in the
Delaunay graph of S ∪Ω? The worst case is when the Delaunay graph is a
triangulation. If S contains r points, then the triangulation has 2(r + 3) −5
triangles, and therefore 6(r + 3) −15 = 6r + 3 Delaunay corners. It follows
from Theorem 9.15 that
∑
∆
card(K(∆)) ⩽
n
∑
r=1
9
%n−r
r
&%6r −3
r
&
⩽54n
n
∑
r=1
1
r ⩽54n(lnn+1).
This ﬁnally completes the proof of Theorem 9.12.
9.6
Notes and Comments
The problem of triangulating a set of points is a topic in computational geometry
that is well known outside this ﬁeld. Triangulations of point sets in two and more
dimensions are of paramount importance in numerical analysis, for instance
for ﬁnite element methods, but also in computer graphics. In this chapter we
looked at the case of triangulations that only use the given points as vertices. If
additional points—so-called Steiner points—are allowed, the problem is also
known as meshing and is treated in more detail in Chapter 14.
Lawson [244] proved that any two triangulations of a planar point set can be
transformed into each other by ﬂipping edges. He later suggested ﬁnding a good
triangulation by iteratively ﬂipping edges, where such an edge-ﬂip improves
some cost function of the triangulation [245].
It had been observed for some time that triangulations that lead to good
interpolations avoid long and skinny triangles [38]. The result that there is—if
we ignore degenerate cases—only one locally optimal triangulation with respect
to the angle-vector, namely the Delaunay triangulation, is due to Sibson [360].
Looking only at the angle-vector completely ignores the height of the data
points, and is therefore also called the data-independent approach. A good
motivation for this approach is given by Rippa [328], who proves that the De-
launay triangulation is the triangulation that minimizes the roughness of the
resulting terrain, no matter what the actual height data is. Here, roughness is
deﬁned as the integral of the square of the L2-norm of the gradient of the terrain.
More recent research tries to ﬁnd improved triangulations by taking the height
information into account. This data-dependent approach was ﬁrst proposed by
Dyn et al. [154], who suggest different cost criteria for triangulations, which
depend on the height of the data points. Interestingly, they compute their im-
proved triangulations by starting with the Delaunay triangulation and iteratively
ﬂipping edges. The same approach is taken by Quak and Schumaker [325], who
consider piecewise cubic interpolation, and Brown [76]. Quak and Schumaker
observe that their triangulations are small improvements compared to the Delau-
nay triangulation when they try to approximate smooth surfaces, but that they
can be drastically different for non-smooth surfaces.
More references relevant to Delaunay triangulations as the dual of Voronoi
diagrams can be found in Chapter 7.
214

Section 9.7
EXERCISES
The randomized incremental algorithm we have given here is due to Guibas
et al. [196], but our analysis of ∑∆card(K(∆)) is from Mulmuley's book [290].
The argument that extends the analysis to the case of points in degenerate
position is new. Alternative randomized algorithms were given by Boissonnat
et al. [69, 71], and by Clarkson and Shor [133].
Various geometric graphs deﬁned on a set of points P have been found to be
subgraphs of the Delaunay triangulation of P. The most important one is proba-
bly the Euclidean minimum spanning tree (EMST) of the set of points [349];
others are the Gabriel graph [186] and the relative neighborhood graph [374].
We treat these geometric graphs in the exercises.
Another important triangulation is the minimum weight triangulation, that
is, a triangulation whose weight is minimal (where the weight of a triangulation
is the sum of the lengths of all edges of the triangulation) [12, 42, 146, 147].
Determining a minimum weight triangulation among all triangulations of a
given point set was recently shown to be NP-complete [291].
9.7
Exercises
9.1
In this exercise we look at the number of different triangulations that a
set of n points in the plane may allow.
a. Prove that no set of n points can be triangulated in more than 2(n
2)
ways.
b. Prove that there are sets of n points that can be triangulated in at least
2n−2√n different ways.
9.2
The degree of a point in a triangulation is the number of edges incident to
it. Give an example of a set of n points in the plane such that, no matter
how the set is triangulated, there is always a point whose degree is n−1.
9.3
Prove that any two triangulations of a planar point set can be transformed
into each other by edge ﬂips. Hint: Show ﬁrst that any two triangulations
of a convex polygon can be transformed into each other by edge ﬂips.
9.4
Prove that the smallest angle of any triangulation of a convex polygon
whose vertices lie on a circle is the same. This implies that any completion
of the Delaunay triangulation of a set of points maximizes the minimum
angle.
9.5
a. Given four points p, q, r, s in the plane, prove that point s lies in the
interior of the circle through p, q, and r if and only if the following
condition holds. Assume that p, q, r form the vertices of a triangle in
clockwise order.
det




px
py
p2
x + p2
y
1
qx
qy
q2
x +q2
y
1
rx
ry
r2
x +r2
y
1
sx
sy
s2
x +s2
y
1



> 0.
215

Chapter 9
DELAUNAY TRIANGULATIONS
b. The determinant test of part a. can be used to test if an edge in a
triangulation is legal. Can you come up with an alternative way to
implement this test? Discuss the advantages and/or disadvantages of
your method compared to the determinant test.
9.6
We have described algorithm DELAUNAYTRIANGULATION by calling
a recursive procedure LEGALIZEEDGE. Give an iterative version of
this procedure, and discuss the advantages and/or disadvantages of your
procedure over the recursive one.
9.7
Prove that all edges of DG(Pr) that are not in DG(Pr−1) are incident to
pr. In other words, the new edges of DG(Pr) form a star as in Figure 9.8.
Give a direct proof, without referring to algorithm DELAUNAYTRIANGU-
LATION.
9.8
Let P be a set of n points in general position, and let q ̸∈P be a point
inside the convex hull of P. Let pi, p j, pk be the vertices of a triangle
in the Delaunay triangulation of P that contains q. (Since q can lie on
an edge of the Delaunay triangulation, there can be two such triangles.)
Prove that qpi, qp j, and qpk are edges of the Delaunay triangulation of
P∪{q}.
9.9
The algorithm given in this chapter is randomized, and it computes the
Delaunay triangulation of a set of n points in O(nlogn) expected time.
Show that the worst-case running time of the algorithm is Ω(n2).
9.10 The algorithm given in this chapter uses two extra points p−1 and p−2 to
start the construction of the Delaunay triangulation. These points should
not lie in any circle deﬁned by three input points, and so far away that
they see the points of P in their lexicographic order. These conditions
were enforced by implementing operations involving these points in a
special way—see page 204. Compute explicit coordinates for the extra
points such that this special implementation is not needed. Is this a better
approach?
9.11 A Euclidean minimum spanning tree (EMST) of a set P of points in the
plane is a tree of minimum total edge length connecting all the points.
EMST's are interesting in applications where we want to connect sites
in a planar environment by communication lines (local area networks),
roads, railroads, or the like.
a. Prove that the set of edges of a Delaunay triangulation of P contains
an EMST for P.
b. Use this result to give an O(nlogn) algorithm to compute an EMST
for P.
9.12 The traveling salesman problem (TSP) is to compute a shortest tour
visiting all points in a given point set. The traveling salesman problem is
NP-hard. Show how to ﬁnd a tour whose length is at most two times the
optimal length, using the EMST deﬁned in the previous exercise.
216

Section 9.7
EXERCISES
9.13 The Gabriel graph of a set P of points in the plane is deﬁned as follows:
p
q
Two points p and q are connected by an edge of the Gabriel graph if and
only if the disc with diameter pq does not contain any other point of P.
a. Prove that DG(P) contains the Gabriel graph of P.
b. Prove that p and q are adjacent in the Gabriel graph of P if and only if
the Delaunay edge between p and q intersects its dual Voronoi edge.
c. Give an O(nlogn) time algorithm to compute the Gabriel graph of a
set of n points.
9.14 The relative neighborhood graph of a set P of points in the plane is
deﬁned as follows: Two points p and q are connected by an edge of the
relative neighborhood graph if and only if
d(p,q) ⩽
min
r∈P,r̸=p,qmax(d(p,r),d(q,r)).
a. Given two points p and q, let lune(p,q) be the moon-shaped region
p
q
lune(p,q)
formed as the intersection of the two circles around p and q whose
radius is d(p,q). Prove that p and q are connected in the relative
neighborhood graph if and only if lune(p,q) does not contain any
point of P in its interior.
b. Prove that DG(P) contains the relative neighborhood graph of P.
c. Design an algorithm to compute the relative neighborhood graph of a
given point set.
9.15 Prove the following relationship between the edge sets of an EMST, of
the relative neighborhood graph (RNG), the Gabriel graph (GG), and the
Delaunay graph (DG) of a point set P.
EMST ⊆RNG ⊆GG ⊆DG.
(See the previous exercises for the deﬁnition of these graphs.)
9.16 A k-clustering of a set P of n points in the plane is a partitioning of P
into k non-empty subsets P1,...,Pk. Deﬁne the distance between any pair
Pi,Pj of clusters to be the minimum distance between one point from Pi
and one point from Pj, that is,
dist(Pi,Pj) :=
min
p∈Pi,q∈Pj dist(p,q).
We want to ﬁnd a k-clustering (for given k and P) that maximizes the
minimum distance between clusters.
a. Suppose the mimimum distance between clusters is achieved by points
p ∈Pi and q ∈Pj. Prove that pq is an edge of the Delaunay triangulation
of P.
b. Give an O(nlogn) time algorithm to compute a k-clustering maximiz-
ing the minimum distance between clusters. Hint: Use a Union-Find
data structure.
217

Chapter 9
DELAUNAY TRIANGULATIONS
9.17 The weight of a triangulation is the sum of the lengths of all edges of
the triangulation. A minimum weight triangulation is a triangulation
whose weight is minimal. Disprove the conjecture that the Delaunay
triangulation is a minimum weight triangulation.
9.18* Give an example of a geometric conﬁguration space (X,Π,D,K) where
T(Xr)\T(Xr+1) can be arbitrarily large compared to T(Xr+1)\T(Xr).
9.19* Apply conﬁguration spaces to analyze the randomized incremental algo-
rithm of Chapter 6.
218

10
More Geometric Data Structures
Windowing
In the future most cars will be equipped with a vehicle navigation system to
help you determine your position and to guide you to your destination. Such
a system stores a roadmap of, say, the whole of the U.S. It also keeps track of
where you are, so that it can show the appropriate part of the roadmap at any
time on a little computer screen; this will usually be a rectangular region around
your present position. Sometimes the system will do even more for you. For
example, it might warn you when a turn is coming up that you should take to
get to your destination.
Figure 10.1
A windowing query in a map of the U.S.
To be of any use, the map should contain sufﬁcient detail. A detailed map
of the whole of Europe contains an enormous amount of data. Fortunately, only
a small part of the map has to be displayed. Nevertheless, the system still has to
ﬁnd that part of the map: given a rectangular region, or a window, the system
must determine the part of the map (roads, cities, and so on) that lie in the
window, and display them. This is called a windowing query.
Checking every single feature of the map to see if it lies inside the window
is not a workable method with the amount of data that we are dealing with.
What we should do is to store the map in some kind of data structure that allows
us to retrieve the part inside a window quickly.
Windowing queries are not only useful operations on geographic maps. They
also play an important role in several applications in computer graphics and
219

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
CAD/CAM. One example is ﬂight simulation. A model of a landscape can
consist of a huge number of triangles, but only a small part of the landscape
will be within sight of the pilot. So we have to select the part of the landscape
that lies within a given region. Here the region is 3-dimensional, and it is called
the viewing volume. Another example comes from the design of printed circuit
boards. Such a design typically consists of (a number of layers of) a planar
drawing showing the location of traces and components. (See also Chapter 14.)
In the design process one often wants to zoom in onto a certain portion of the
board to inspect it more closely. Again, what we need is to determine the traces
and components on the board that lie inside the window. In fact, windowing
is required whenever one wants to inspect a small portion of a large, complex
object.
Windowing queries are similar to the range queries studied in Chapter 5. The
difference is the type of data that is dealt with: the data for range queries are
points, whereas the data for windowing queries are typically line segments,
polygons, curves, and so on. Also, for range queries we often deal with higher-
dimensional search spaces, while for windowing queries the search space usually
is 2- or 3-dimensional.
10.1
Interval Trees
Let's start with the easiest of the examples that we gave above, namely window-
ing for a printed circuit board. The reason that this example is easier than the
others is that the type of data is restricted: the objects on a printed circuit board
normally have boundaries that consist of line segments with a limited number
of possible orientations. Often they are parallel to one of the sides of the board
or make 45 degree angles with the sides. Here we only consider the case were
the segments are parallel to the sides. In other words, if we consider the x-axis
to be aligned with the bottom side of the board, and the y-axis to be aligned
with the left side of the board, then any segment is parallel to either the x-axis
or the y-axis: the segments are axis-parallel, or orthogonal. We assume that the
query window is an axis-parallel rectangle, that is, a rectangle whose edges are
axis-parallel.
Let S be a set of n axis-parallel line segments. To solve windowing queries we
need a data structure that stores S in such a way that the segments intersecting a
query window W := [x : x′]×[y : y′] can be reported efﬁciently. Let's ﬁrst see in
what ways a segment can intersect the window. There are a number of different
cases: the segment can lie entirely inside W, it can intersect the boundary of
W once, it can intersect the boundary twice, or it can (partially) overlap the
boundary of W. In most cases the segment has at least one endpoint inside W.
We can ﬁnd such segments by performing a range query with W in the set of 2n
endpoints of the segments in S. In Chapter 5 we have seen a data structure for
this: the range tree. A 2-dimensional range tree uses O(nlogn) storage, and a
range query can be answered in O(log2 n+k) time, where k is the number of
220

Section 10.1
INTERVAL TREES
reported points. We have also shown that we can apply fractional cascading to
reduce the query time to O(logn+k). There is one little problem. If we do a
range query with W in the set of segment endpoints, we report the segments that
have both endpoints inside W twice. This can be avoided by marking a segment
when we report it for the ﬁrst time, and only reporting segments that are not yet
marked. Alternatively, when we ﬁnd an endpoint of a segment to lie inside W,
we can check whether the other endpoint lies inside W as well. If not we report
the segment. If the other endpoint does lie inside W, we only report the segment
when the current endpoint is the leftmost or bottom endpoint. This leads to the
following lemma.
Lemma 10.1 Let S be a set of n axis-parallel line segments in the plane. The
segments that have at least one endpoint inside an axis-parallel query window
W can be reported in O(logn+k) time with a data structure that uses O(nlogn)
storage and preprocessing time, where k is the number of reported segments.
It remains to ﬁnd the segments that do not have an endpoint inside the query
window. Such segments either cross the boundary of W twice or contain one
edge of the boundary. When the segment is vertical it will cross both horizontal
edges of the boundary. When it is horizontal it will cross both vertical edges.
Hence, we can ﬁnd such segments by reporting all segments that intersect the
left edge of the boundary and all segments that intersect the bottom edge of the
boundary. (Note that there is no need to query with the other two edges of the
boundary.) To be precise, we should only report those segments that do not have
an endpoint inside W, because the others were already reported before. Let's
consider the problem of ﬁnding the horizontal segments intersected by the left
edge of W; to deal with the top edge we just have to reverse the roles of the x-
and y-coordinates.
We have arrived at the following problem: preprocess a set S of horizontal
line segments in the plane such that the segments intersecting a vertical query
segment can be reported efﬁciently. To gain some insight into the problem we
ﬁrst look at a simpler version, namely where the query segment is a full line.
Let ℓ:= (x = qx) denote the query line. A horizontal segment s := (x,y)(x′,y)
is intersected by ℓif and only if x ⩽qx ⩽x′. So only the x-coordinates of the
segments play a role here. In other words, the problem becomes 1-dimensional:
given a set of intervals on the real line, report the ones that contain the query
point qx.
ℓ
(x,y)
(x′,y)
x
x′
qx
Let I := {[x1 : x′
1],[x2 : x′
2],...,[xn : x′
n]} be a set of closed intervals on the
real line. To keep the connection with the 2-dimensional problem alive we
image the real line to be horizontal, and we say "to the left (right) of" instead of
"less (greater) than". Let xmid be the median of the 2n interval endpoints, So at
most half of the interval endpoints lies to the left of xmid and at most half of the
endpoints lies to the right of xmid. If the query value qx lies to the left of xmid
then the intervals that lie completely to the right of xmid obviously do not contain
qx. We construct a binary tree based on this idea. The right subtree of the tree
stores the set Iright of the intervals lying completely to the right of xmid, and the
left subtree stores the set Ileft of intervals completely to the left of xmid. These
subtrees are constructed recursively in the same way. There is one problem that
221

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
we still have to deal with: what to do with the intervals that contain xmid? One
Figure 10.2
Classiﬁcation of the segments with
respect to xmid
xmid
Ileft
Iright
Imid
Ileft
Iright
Imid
possibility would be to store such intervals in both subtrees. For the children of
the node, however, the same thing could happen again. Eventually one interval
could be stored many times, and the amount of storage our data structure uses
could become very large. To avoid the proliferation of intervals we deal with
the problem differently: we store the set Imid of intervals containing xmid in a
separate structure and associate that structure with the root of our tree. See
Figure 10.2 for the situation. Note that in this ﬁgure (and others), although
the intervals lie on a real line, we draw them on slightly different heights to
distinguish them.
The associated structure should enable us to report the intervals in Imid that
contain qx. So we ended up with the same problem that we started with: given
a set Imid of intervals, ﬁnd those that contain qx. But if we have bad luck Imid
could be the same as I. It seems we are back to exactly the same problem, but
there is a difference. We know that all the intervals in Imid contain xmid, and this
helps a great deal. Suppose, for example, that qx lies to the left of xmid. In that
case we already know that the right endpoint of all intervals in Imid lies right
of qx. So only the left endpoints of the intervals are important: qx is contained
in an interval [x j : x′
j] ∈Imid if and only if xj ⩽qx. If we store the intervals in
a list ordered on increasing left endpoint, then qx can only be contained in an
interval if qx is also contained in all its predecessors in the sorted list. In other
xmid
qx
words, we can simply walk along the sorted list reporting intervals, until we
come to an interval that does not contain qx. At that point we can stop: none
of the remaining intervals can contain qx. Similarly, when qx lies right of xmid
we can walk along a list that stores the intervals sorted on right endpoint. This
list must be sorted on decreasing right endpoint, because it is traversed if the
query point qx lies to the right of xmid. Finally, when qx = xmid we can report all
intervals in Imid. (We do not need to treat this as a separate case. We can simply
walk along one of the sorted lists.)
We now give a succinct description of the whole data structure that stores the
intervals in I. The data structure is called an interval tree. Figure 10.3 shows an
interval tree; the dotted vertical segments indicate the values xmid for each node.
If I = /0 then the interval tree is a leaf.
222

Section 10.1
INTERVAL TREES
Lleft = s1,s2
Lright = s1,s2
s1
s2
s3
s4
s5
s6
s7
Lleft = s3,s4,s5
Lright = s5,s3,s4
Lleft = s6,s7
Lright = s7,s6
Figure 10.3
An interval tree
Otherwise, let xmid be the median of the endpoints of the intervals. Let
Ileft
:=
{[xj : x′
j] ∈I : x′
j < xmid},
Imid
:=
{[xj : x′
j] ∈I : x j ⩽xmid ⩽x′
j},
Iright
:=
{[xj : x′
j] ∈I : xmid < x j}.
The interval tree consists of a root node ν storing xmid. Furthermore,
the set Imid is stored twice; once in a list Lleft(ν) that is sorted on the
left endpoints of the intervals, and once in a list Lright(ν) that is sorted
on the right endpoints of the intervals,
the left subtree of ν is an interval tree for the set Ileft,
the right subtree of ν is an interval tree for the set Iright.
Lemma 10.2 An interval tree on a set of n intervals uses O(n) storage and has
depth O(logn).
Proof. The bound on the depth is trivial, so we prove the storage bound. Note
that Ileft, Imid, and Iright are disjoint subsets. As a result, each interval is only
stored in a set Imid once and, hence, only appears once in the two sorted lists.
This shows that the total amount of storage required for all associated lists is
bounded by O(n). The tree itself uses O(n) storage as well.
The following recursive algorithm for building an interval tree follows directly
from its deﬁnition. (Recall that lc(ν) and rc(ν) denote the left and right child,
respectively, of a node ν.)
Algorithm CONSTRUCTINTERVALTREE(I)
Input. A set I of intervals on the real line.
Output. The root of an interval tree for I.
1.
if I = /0
2.
then return an empty leaf
3.
else Create a node ν. Compute xmid, the median of the set of interval
endpoints, and store xmid with ν.
223

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
4.
Compute Imid and construct two sorted lists for Imid: a list Lleft(ν)
sorted on left endpoint and a list Lright(ν) sorted on right endpoint.
Store these two lists at ν.
5.
lc(ν) ←CONSTRUCTINTERVALTREE(Ileft)
6.
rc(ν) ←CONSTRUCTINTERVALTREE(Iright)
7.
return ν
Finding the median of a set of points takes linear time. Actually, it is better to
compute the median by presorting the set of points, as in Chapter 5. It is easy to
maintain these presorted sets through the recursive calls. Let nmid := card(Imid).
Creating the lists Lleft(ν) and Lright(ν) takes O(nmid lognmid) time. Hence,
the time we spend (not counting the time needed for the recursive calls) is
O(n+nmid lognmid). Using similar arguments as in the proof of Lemma 10.2
we can conclude that the algorithm runs in O(nlogn) time.
Lemma 10.3 An interval tree on a set of n intervals can be built in O(nlogn)
time.
It remains to show how to use the interval tree to ﬁnd the intervals containing
a query point qx. We already sketched how to do this, but now we can give the
exact algorithm.
Algorithm QUERYINTERVALTREE(ν,qx)
Input. The root ν of an interval tree and a query point qx.
Output. All intervals that contain qx.
1.
if ν is not a leaf
2.
then if qx < xmid(ν)
3.
then Walk along the list Lleft(ν), starting at the interval with the
leftmost endpoint, reporting all the intervals that contain qx.
Stop as soon as an interval does not contain qx.
4.
QUERYINTERVALTREE(lc(ν),qx)
5.
else Walk along the list Lright(ν), starting at the interval with the
rightmost endpoint, reporting all the intervals that contain
qx. Stop as soon as an interval does not contain qx.
6.
QUERYINTERVALTREE(rc(ν),qx)
Analyzing the query time is not very difﬁcult. At any node ν that we visit we
spend O(1+kν) time, where kν is the number of intervals that we report at ν.
The sum of the kν's over the visited nodes is, of course, k. Furthermore, we
visit at most one node at any depth of the tree. As noted above, the depth of the
interval tree is O(logn). So the total query time is O(logn+k).
The following theorem summarizes the results about interval trees.
Theorem 10.4 An interval tree for a set I of n intervals uses O(n) storage and
can be built in O(nlogn) time. Using the interval tree we can report all intervals
that contain a query point in O(logn+k) time, where k is the number of reported
intervals.
224

Section 10.1
INTERVAL TREES
It's time to pause a moment and see where all this has brought us. The
problem we originally wanted to solve is this: store a set S of axis-parallel
segments in a data structure that allows us to ﬁnd the segments intersecting a
query window W = [x : x′]×[y : y′]. Finding the segments that have an endpoint
inside W could be done using a data structure from Chapter 5, the range tree.
The other segments intersecting W had to intersect the boundary of W twice.
We planned to ﬁnd these segments by querying with the left and top edges of W.
So we needed a data structure that stores a set of horizontal segments such that
the ones intersecting a vertical query segment can be reported efﬁciently, and a
similar data structure storing the vertical segments that allows for intersection
queries with horizontal segments. We started by developing a data structure that
solves a slightly simpler problem, namely where the query object is a full line.
This led to the interval tree. Now let's see how to extend the interval tree to the
case where the query object is a vertical line segment.
Let SH ⊆S be the subset of horizontal segments in S, and let q be the
vertical query segment qx ×[qy : q′
y]. For a segment s := [sx : s′
x]×sy in SH, we
call [sx : s′
x] the x-interval of the segment. Suppose we have stored the segments
in SH in an interval tree T according to their x-intervals. Let's go through the
query algorithm QUERYINTERVALTREE to see what happens when we query T
with the vertical query segment q. Suppose qx lies to the left of the xmid-value
stored at the root of the interval tree T. It is still correct that we only search
recursively in the left subtree: segments completely to the right of xmid cannot be
intersected by q so we can skip the right subtree. The way the set Imid is treated,
(sx,sy)
(s′x,sy)
s
(qx,q′y)
(qx,qy)
q
however, is not correct anymore. For a segment s ∈Imid to be intersected by q,
it is not sufﬁcient that its left endpoint lies to the left of q; it is also required
that its y-coordinate lies in the range [qy : q′
y]. Figure 10.4 illustrates this. So
q
xmid
[−∞: qx]×[qy : q′y]
Figure 10.4
Segments intersected by q must have
their left endpoint in the shaded region
storing the endpoints in an ordered list is not enough. We need a more elaborate
associated structure: given a query range (−∞: qx]×[qy : q′
y], we must be able
to report all the segments whose left endpoint lies in that range. If q lies to
the right of xmid we want to report all segments whose right endpoint lies in
the range [qx : +∞) × [qy : q′
y], so for this case we need a second associated
225

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
structure. How should we implement the associated structure? Well, the query
that we wish to perform is nothing more than a rectangular range query on a set
of points. A 2-dimensional range tree, described in Chapter 5, will therefore do
the trick. This way the associated structure uses O(nmid lognmid) storage, where
nmid := card(Imid), and its query time is O(lognmid +k).
The data structure that stores the set SH of horizontal segments is now as
follows. The main structure is an interval tree T on the x-intervals of the
segments. Instead of the sorted lists Lleft(ν) and Lright(ν) we have two range
trees: a range tree Tleft(ν) on the left endpoints of the segments in Imid(ν),
and a range tree Tright(ν) on the right endpoints of the segments in Imid(ν).
Because the storage required for a range tree is a factor logn larger than for
sorted lists, the total amount of storage for the data structure becomes O(nlogn).
The preprocessing time remains O(nlogn).
The query algorithm is the same as QUERYINTERVALTREE, except that,
instead of walking along the sorted list Lleft(ν), say, we perform a query in the
range tree Tleft(ν). So at each of the O(logn) nodes ν on the search path we
spend O(logn+kν) time, where kν is the number of reported segments. The
total query time therefore becomes O(log2 n+k).
We have proved the following theorem.
Theorem 10.5 Let S be a set of n horizontal segments in the plane. The segments
intersecting a vertical query segment can be reported in O(log2 n+k) time with
a data structure that uses O(nlogn) storage, where k is the number of reported
segments. The structure can be built in O(nlogn) time.
If we combine this with the result of Lemma 10.1 we get a solution to the
windowing problem for axis-parallel segments.
Corollary 10.6 Let S be a set of n axis-parallel segments in the plane. The
segments intersecting an axis-parallel rectangular query window can be reported
in O(log2 n+k) time with a data structure that uses O(nlogn) storage, where
k is the number of reported segments. The structure can be built in O(nlogn)
time.
10.2
Priority Search Trees
In the structure for windowing described in Section 10.1 we used a range tree for
the associated structures. The range queries we perform on them have a special
property: they are unbounded on one side. In this section we will describe
a different data structure, the priority search tree, that uses this property to
improve the bound on storage to O(n). This data structure is also a lot simpler
because it does not require fractional cascading. Using priority search trees
instead of range trees in the data structure for windowing reduces the storage
bound in Theorem 10.5 to O(n). It does not improve the storage bound in
226

Section 10.2
PRIORITY SEARCH TREES
Corollary 10.6 because there we also need a range tree to report the endpoints
that lie in the window.
Let P := {p1, p2,..., pn} be a set of points in the plane. We want to design
a structure for rectangular range queries of the form (−∞: qx] ×[qy : q′
y]. To
get some idea of how this special property can be used, let's look at the 1-
dimensional case. A normal 1-dimensional range query would ask for the points
lying in a range [q′
x : qx]. To ﬁnd these points efﬁciently we can store the set of
points in a 1-dimensional range tree, as described in Chapter 5. If the range is
unbounded to the left, we are asking for the points lying in (−∞: qx]. This can
be solved by simply walking along an ordered list starting at the leftmost point,
until we encounter a point that is not in the range. The query time is O(1+k),
instead of O(logn+k) which we needed in the general case.
How can we extend this strategy to 2-dimensional range queries that are
unbounded to the left? Somehow we must integrate information about the
y-coordinate in the structure without using associated structures, so that, among
the points whose x-coordinate is in (−∞: qx], we can easily select the ones
whose y-coordinate is in [qy : q′
y]. A simple linear list doesn't lend itself well
for this. Therefore we take a different structure to work with: the heap.
1
3
4
11
8
21
15
22
36
Figure 10.5
A heap for the set
{1,3,4,8,11,15,21,22,36}
A heap is normally used for priority queries that ask for the smallest (or largest)
value in a set. But heaps can also be used to answer 1-dimensional range queries
of the form (−∞: qx]. A heap has the same query time as a sorted list, namely
O(1+k). Normally the advantage of a heap over a sorted list is that points can
be inserted and the maximum deleted more efﬁciently. For us the tree structure
of a heap has another advantage: it makes it easier to integrate information
about the y-coordinate, as we shall see shortly. A heap is a binary tree deﬁned
as follows. The root of the tree stores the point with minimum x-value. The
remainder of the set is partitioned into two subsets of almost equal size, and
these subsets are stored recursively in the same way. Figure 10.5 gives an
example of a heap. We can do a query with (−∞: qx] by walking down the tree.
When we visit a node we check if the x-coordinate of the point stored at the
node lies in (−∞: qx]. If it does, we report the point and continue the search
in both subtrees; otherwise, we abort the search in this part of the tree. For
example, when we search with (−∞: 5] in the tree of Figure 10.5, we report the
points 1, 3, and 4. We also visit the nodes with 8 and 11 but abort the search
there.
227

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
Heaps give us some freedom in how to partition the set into two subsets.
If we also want to search on y-coordinate then the trick is to perform the
partitioning not in an arbitrary way, as is the case for normal heaps, but according
to y-coordinate. More precisely, we split the remainder of the set into two subsets
of almost equal size such that the y-coordinate of any point in one subset is
smaller than the y-coordinate of any point in the other subset. This is illustrated
in Figure 10.6. The tree is drawn sideways to indicate that the partitioning
Figure 10.6
A set of points and the corresponding
priority search tree
p6
p5
p3
p4
p2
p1
p5
p6
p3
p4
p2
p1
is on y-coordinate. In the example of Figure 10.6 the point p5 has smallest
x-coordinate and, hence, is stored in the root. The other points are partitioned
in y-coordinate. The points p3, p4, and p6 have smaller y-coordinate and are
stored in the left subtree. Of these p3 has smallest x-coordinate, so this point is
placed in the root of the subtree, and so on.
A formal deﬁnition of a priority search tree for a set P of points is as follows.
We assume that all the points have distinct coordinates. In Chapter 5 (more
precisely, in Section 5.5) we saw that this involves no loss of generality; by
using composite numbers we can simulate that all coordinates are distinct.
If P = /0 then the priority search tree is an empty leaf.
Otherwise, let pmin be the point in the set P with the smallest x-coordinate.
Let ymid be the median of the y-coordinates of the remaining points. Let
Pbelow
:=
{p ∈P\{pmin} : py < ymid},
Pabove
:=
{p ∈P\{pmin} : py > ymid}.
The priority search tree consists of a root node ν where the point p(ν) :=
pmin and the value y(ν) := ymid are stored. Furthermore,
the left subtree of ν is a priority search tree for the set Pbelow,
the right subtree of ν is a priority search tree for the set Pabove.
It's straightforward to derive a recursive O(nlogn) algorithm for building a
priority search tree. Interestingly, priority search trees can even be built in linear
time, if the points are already sorted on y-coordinate. The idea is to construct
the tree bottom-up instead of top-down, in the same way heaps are normally
constructed.
A query with a range (−∞: qx]×[qy : q′
y] in a priority search tree is performed
roughly as follows. We search with qy and q′
y, as indicated in Figure 10.7. All
228

Section 10.2
PRIORITY SEARCH TREES
the shaded subtrees in the ﬁgure store only points whose y-coordinate lies in the
correct range. So we can search those subtrees based on x-coordinate only. This
is done with the following subroutine, which is basically the query algorithm
for a heap.
qy
q′y
νsplit
Figure 10.7
Querying a priority search tree
REPORTINSUBTREE(ν,qx)
Input. The root ν of a subtree of a priority search tree and a value qx.
Output. All points in the subtree with x-coordinate at most qx.
1.
if ν is not a leaf and (p(ν))x ⩽qx
2.
then Report p(ν).
3.
REPORTINSUBTREE(lc(ν),qx)
4.
REPORTINSUBTREE(rc(ν),qx)
Lemma 10.7 REPORTINSUBTREE(ν,qx) reports in O(1+kν) time all points in
the subtree rooted at ν whose x-coordinate is at most qx, where kν is the number
of reported points.
Proof. Let p(µ) be a point with (p(µ))x ⩽qx that is stored at a node µ in the
subtree rooted at ν. By deﬁnition of the data structure, the x-coordinates of the
points stored at the path from µ to ν form a decreasing sequence, so all these
points must have x-coordinates at most qx. Hence, the search is not aborted at
any of these nodes, which implies that µ is reached and p(µ) is reported. We
conclude that all points with x-coordinate at most qx are reported. Obviously,
those are the only reported points.
At any node µ that we visit we spend O(1) time. When we visit a node µ
with µ ̸= ν we must have reported a point at the parent of µ. We charge the
time we spend at µ to this point. This way any reported point gets charged
twice, which means that the time we spend at nodes µ with µ ̸= ν is O(kν).
Adding the time we spend at ν we get a total of O(1+kν) time.
If we call REPORTINSUBTREE at each of the subtrees that we select (the shaded
subtrees of Figure 10.7), do we ﬁnd all the points that lie in the query range? The
answer is no. The root of the tree, for example, stores the point with smallest
x-coordinate. This may very well be a point in the query range. In fact, any
229

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
point stored at a node on the search path to qy or q′
y may lie in the query range,
so we should test them as well. This leads to the following query algorithm.
Algorithm QUERYPRIOSEARCHTREE(T,(−∞: qx]×[qy : q′
y])
Input. A priority search tree and a range, unbounded to the left.
Output. All points lying in the range.
1.
Search with qy and q′
y in T. Let νsplit be the node where the two search
paths split.
2.
for each node ν on the search path of qy or q′
y
3.
do if p(ν) ∈(−∞: qx]×[qy : q′
y] then report p(ν).
4.
for each node ν on the path of qy in the left subtree of νsplit
5.
do if the search path goes left at ν
6.
then REPORTINSUBTREE(rc(ν),qx)
7.
for each node ν on the path of q′
y in the right subtree of νsplit
8.
do if the search path goes right at ν
9.
then REPORTINSUBTREE(lc(ν),qx)
Lemma 10.8 Algorithm QUERYPRIOSEARCHTREE reports the points in a
query range (−∞: qx]×[qy : q′
y] in O(logn+k) time, where k is the number of
reported points.
Proof. First we prove that any point that is reported by the algorithm lies in
the query range. For the points on the search paths to qy and q′
y this is obvious:
these points are tested explicitly for containment in the range. Consider a call
REPORTINSUBTREE(rc(ν),qx) in line 6. Let p be a point that is reported in this
call. By Lemma 10.7 we have px ⩽qx. Furthermore, py ⩽q′
y, because all the
nodes visited in this call lie to the left of νsplit and q′
y > y(νsplit). Finally, py ⩾qy,
because all the nodes visited in this call lie to the right of ν and the search path
to qy went left at ν. A similar argument applies for the points reported in line 9.
We have proved that all the reported points lie in the query range. Conversely,
let p(µ) be a point in the range. Any point stored to the left of a node where the
search path to qy goes right must have a y-coordinate smaller than qy. Similarly,
any point stored to the right of a node where the search path to q′
y goes left
must have a y-coordinate greater than q′
y. Hence, µ must either be on one of the
search paths, or in one of the subtrees for which REPORTINSUBTREE is called.
In both cases p(µ) will be reported.
It remains to analyze the time taken by the algorithm. This is linear in the
number of nodes on the search paths to qy and q′
y plus the time taken by all the
executions of the procedure REPORTINSUBTREE. The depth of the tree and,
hence, the number of nodes on the search paths, is O(logn). The time taken by
all executions of REPORTINSUBTREE is O(logn+k) by Lemma 10.7.
The performance of priority search trees is summarized in the following theorem.
Theorem 10.9 A priority search tree for a set P of n points in the plane uses
O(n) storage and can be built in O(nlogn) time. Using the priority search tree
we can report all points in a query range of the form (−∞: qx] × [qy : q′
y] in
O(logn+k) time, where k is the number of reported points.
230

Section 10.3
SEGMENT TREES
10.3
Segment Trees
So far we have studied the windowing problem for a set of axis-parallel line
segments. We developed a nice data structure for this problem using interval
trees with priority search trees as associated structure. The restriction to axis-
parallel segments was inspired by the application to printed circuit board design.
When we are doing windowing queries in roadmaps, however, we must drop
this restriction: roadmaps contain line segments at arbitrary orientation.
There is a trick that we can use to reduce the general problem to a problem
on axis-parallel segments. We can replace each segment by its bounding box.
window
Using the data structure for axis-parallel segments that we developed earlier,
we can now ﬁnd all the bounding boxes that intersect the query window W.
We then check every segment whose bounding box intersects W to see if the
segment itself also intersects W. In practice this technique usually works quite
well: the majority of the segments whose bounding box intersects W will also
intersect W themselves. In the worst case, however, the solution is quite bad:
all bounding boxes may intersect W whereas none of the segments do. So if we
want to guarantee a fast query time, we must look for another method.
As before we make a distinction between segments that have an endpoint in
the window and segments that intersect the window boundary. The ﬁrst type of
segments can be reported using a range tree. To ﬁnd the answers of the second
type we perform an intersection query with each of the four boundary edges
of the window. (Of course care has to be taken that answers are reported only
once.) We will only show how to perform queries with vertical boundary edges.
For the horizontal edges a similar approach can be used. So we are given a set S
of line segments with arbitrary orientations in the plane, and we want to ﬁnd
those segments in S that intersect a vertical query segment q := qx ×[qy : q′
y]. We
will assume that the segments in S don't intersect each other, but we allow them
to touch. (For intersecting segments the problem is a lot harder to solve and the
time bounds are worse. Techniques like the ones described in Chapter 16 are
required in this case.)
Let's ﬁrst see if we can adapt the solution of the previous sections to the
case of arbitrarily oriented segments. By searching with qx in the interval tree
we select a number of subsets Imid(ν). For a selected node ν with xmid(ν) > qx,
the right endpoint of any segment in Imid(ν) lies to the right of q. If the segment
is horizontal, then it is intersected by the query segment if and only if its left
endpoint lies in the range (−∞: qx]×[qy : q′
y]. If the segments have arbitrary
orientation, however, things are not so simple: knowing that the right endpoint of
a segment is to the right of q doesn't help us much. The interval tree is therefore
not very useful in this case. Let's try to design a different data structure for
q
xmid
[−∞: qx]×[qy : q′
y]
the 1-dimensional problem, one that is more suited for dealing with arbitrarily
oriented segments.
One of the paradigms for developing data structures is the locus approach. A
query is described by a number of parameters; for the windowing problem,
for example, there are four parameters, namely qx, q′
x, qy, and q′
y. For each
231

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
choice of the parameters we get a certain answer. Often nearby choices give
the same answer; if we move the window slightly it will often still intersect
the same collection of segments. Let the parameter space be the space of all
possible choices for the parameters. For the windowing problem this space is
4-dimensional. The locus approach suggests partitioning the parameter space
into regions such that queries in the same region have the same answer. Hence,
if we locate the region that contains the query then we know the answer to it.
Such an approach only works well when the number of regions is small. For the
windowing problem this is not true. There can be Θ(n4) different regions. But
we can use the locus approach to create an alternative for the interval tree.
Let I := {[x1 : x′
1],[x2 : x′
2],...,[xn : x′
n]} be a set of n intervals on the real
line. The data structure that we are looking for should be able to report the
intervals containing a query point qx. Our query has only one parameter, qx,
so the parameter space is the real line. Let p1, p2,..., pm be the list of distinct
interval endpoints, sorted from left to right. The partitioning of the parameter
space is simply the partitioning of the real line induced by the points pi. We
call the regions in this partitioning elementary intervals. Thus the elementary
intervals are, from left to right,
p1
p2
pm−1 pm
(−∞: p1),[p1 : p1],(p1 : p2),[p2 : p2],...,
(pm−1 : pm),[pm : pm],(pm : +∞).
The list of elementary intervals consists of open intervals between two consecu-
tive endpoints pi and pi+1, alternated with closed intervals consisting of a single
endpoint. The reason that we treat the points pi themselves as intervals is, of
course, that the answer to a query is not necessarily the same at the interior of
an elementary interval and at its endpoints.
To ﬁnd the intervals that contain a query point qx, we must determine the
elementary interval that contains qx. To this end we build a binary search tree T
whose leaves correspond to the elementary intervals. We denote the elementary
interval corresponding to a leaf µ by Int(µ).
If all the intervals in I containing Int(µ) are stored at the leaf µ, then we can
report the k intervals containing qx in O(logn+k) time: ﬁrst search in O(logn)
time with qx in T, and then report all the intervals stored at µ in O(1+k) time.
So queries can be answered efﬁciently. But what about the storage requirement
of the data structure? Intervals that span a lot of elementary intervals are stored
at many leaves in the data structure. Hence, the amount of storage will be high
if there are many pairs of overlapping intervals. If we have bad luck the amount
of storage can even become quadratic. Let's see if we can do something to
reduce the amount of storage. In Figure 10.8 you see an interval that spans ﬁve
elementary intervals. Consider the elementary intervals corresponding to the
leaves µ1, µ2, µ3, and µ4. When the search path to qx ends in one of those leaves
we must report the interval. The crucial observation is that a search path ends in
µ1, µ2, µ3, or µ4 if and only if the path passes through the internal node ν. So
why not store the interval at node ν (and at µ5) instead of at the leaves µ1, µ2,
µ3, and µ4 (and at µ5)? In general, we store an interval at a number of nodes
that together cover the interval, and we choose these nodes as high as possible.
232

Section 10.3
SEGMENT TREES
µ1
µ2
µ5
ν
s
µ3
µ4
Figure 10.8
The segment s is stored at ν instead of
at µ1, µ2, µ3, and µ4
The data structure based on this principle is called a segment tree. We now
describe the segment tree for a set I of intervals more precisely. Figure 10.9
shows a segment tree for a set of ﬁve intervals.
s1
s1
s1
s2,s5
s2,s5
s3
s3
s4
s3,s4
s1
s2
s3
s4
s5
s5
Figure 10.9
A segment tree: the arrows from the
nodes point to their canonical subsets
The skeleton of the segment tree is a balanced binary tree T. The leaves of
T correspond to the elementary intervals induced by the endpoints of the
intervals in I in an ordered way: the leftmost leaf corresponds to the leftmost
elementary interval, and so on. The elementary interval corresponding to
leaf µ is denoted Int(µ).
The internal nodes of T correspond to intervals that are the union of ele-
mentary intervals: the interval Int(ν) corresponding to node ν is the union
of the elementary intervals Int(µ) of the leaves in the subtree rooted at ν.
(This implies that Int(ν) is the union of the intervals of its two children.)
Each node or leaf ν in T stores the interval Int(ν) and a set I(ν) ⊆I of inter-
vals (for example, in a linked list). This canonical subset of node ν contains
the intervals [x : x′] ∈I such that Int(ν) ⊆[x : x′] and Int(parent(ν)) ̸⊆[x : x′].
Let's see if our strategy of storing intervals as high as possible has helped to
reduce the amount of storage.
233

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
Lemma 10.10 A segment tree on a set of n intervals uses O(nlogn) storage.
Proof. Because T is a balanced binary search tree with at most 4n+1 leaves, its
height is O(logn). We claim that any interval [x : x′] ∈I is stored in the set I(ν)
for at most two nodes at the same depth of the tree. To see why this is true, let
ν1,ν2,ν3 be three nodes at the same depth, numbered from left to right. Suppose
[x : x′] is stored at ν1 and ν3. This means that [x : x′] spans the whole interval
from the left endpoint of Int(ν1) to the right endpoint of Int(ν3). Because ν2
lies between ν1 and ν3, Int(parent(ν2)) must be contained in [x : x′]. Hence,
[x : x′] will not be stored at ν2. It follows that any interval is stored at most twice
ν1
ν2
ν3
parent(ν2)
at a given depth of the tree, so the total amount of storage is O(nlogn).
So the strategy has helped: we have reduced the worst-case amount of
storage from quadratic to O(nlogn). But what about queries: can they still be
answered easily? The answer is yes. The following simple algorithm describes
how this is done. It is ﬁrst called with ν = root(T).
Algorithm QUERYSEGMENTTREE(ν,qx)
Input. The root of a (subtree of a) segment tree and a query point qx.
Output. All intervals in the tree containing qx.
1.
Report all the intervals in I(ν).
2.
if ν is not a leaf
3.
then if qx ∈Int(lc(ν))
4.
then QUERYSEGMENTTREE(lc(ν),qx)
5.
else QUERYSEGMENTTREE(rc(ν),qx)
The query algorithm visits one node per level of the tree, so O(logn) nodes in
total. At a node ν we spend O(1+kν) time, where kν is the number of reported
intervals. This leads to the following lemma.
Lemma 10.11 Using a segment tree, the intervals containing a query point qx
can be reported in O(logn+k) time, where k is the number of reported intervals.
To construct a segment tree we proceed as follows. First we sort the end-
points of the intervals in I in O(nlogn) time. This gives us the elementary
intervals. We then construct a balanced binary tree on the elementary intervals,
and we determine for each node ν of the tree the interval Int(ν) it represents.
This can be done bottom-up in linear time. It remains to compute the canonical
subsets for the nodes. To this end we insert the intervals one by one into the
segment tree. An interval is inserted into T by calling the following procedure
with ν = root(T).
Algorithm INSERTSEGMENTTREE(ν,[x : x′])
Input. The root of a (subtree of a) segment tree and an interval.
Output. The interval will be stored in the subtree.
1.
if Int(ν) ⊆[x : x′]
2.
then store [x : x′] at ν
3.
else if Int(lc(ν))∩[x : x′] ̸= /0
4.
then INSERTSEGMENTTREE(lc(ν),[x : x′])
234

Section 10.3
SEGMENT TREES
5.
if Int(rc(ν))∩[x : x′] ̸= /0
6.
then INSERTSEGMENTTREE(rc(ν),[x : x′])
How much time does it take to insert an interval [x : x′] into the segment tree?
At every node that we visit we spend constant time (assuming we store I(ν) in
a simple structure like a linked list). When we visit a node ν, we either store
[x : x′] at ν, or Int(ν) contains an endpoint of [x : x′]. We have already seen
that an interval is stored at most twice at each level of T. There is also at most
one node at every level whose corresponding interval contains x and one node
whose interval contains x′. So we visit at most 4 nodes per level. Hence, the
time to insert a single interval is O(logn), and the total time to construct the
segment tree is O(nlogn).
The performance of segment trees is summarized in the following theorem.
Theorem 10.12 A segment tree for a set I of n intervals uses O(nlogn) storage
and can be built in O(nlogn) time. Using the segment tree we can report all
intervals that contain a query point in O(logn+k) time, where k is the number
of reported intervals.
Recall that an interval tree uses only linear storage, and that it also allows
us to report the intervals containing a query point in O(logn+k) time. So for
this task an interval tree is to be preferred to a segment tree. When we want to
answer more complicated queries, such as windowing in a set of line segments,
then a segment tree is a more powerful structure to work with. The reason is that
the set of intervals containing qx is exactly the union of the canonical subsets
that we select when we search in the segment tree. In an interval tree, on the
other hand, we also select O(logn) nodes during a query, but not all intervals
stored at those nodes contain the query point. We still have to walk along the
sorted list to ﬁnd the intersected intervals. So for segment trees, we have the
possibility of storing the canonical subsets in an associated structure that allows
for further querying.
Let's go back to the windowing problem. Let S be a set of arbitrarily oriented,
disjoint segments in the plane. We want to report the segments intersecting a
vertical query segment q := qx ×[qy : q′
y]. Let's see what happens when we build
a segment tree T on the x-intervals of the segments in S. A node ν in T can now
be considered to correspond to the vertical slab Int(ν)×(−∞: +∞). A segment
is in the canonical subset of ν if it completely crosses the slab corresponding
to ν—we say that the segment spans the slab—but not the slab corresponding
to the parent of ν. We denote these subsets with S(ν). See Figure 10.10 for an
illustration.
When we search with qx in T we ﬁnd O(logn) canonical subsets—those of
the nodes on the search path—that collectively contain all the segments whose
x-interval contains qx. A segment s in such a canonical subset is intersected
by q if and only if the lower endpoint of q is below s and the upper endpoint
of q is above s. How do we ﬁnd the segments between the endpoints of q?
Here we use the fact that the segments in the canonical subset S(ν) span the
235

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
Figure 10.10
Canonical subsets contain segments that
span the slab of a node, but not the slab
of its parent
ν1
ν2
s1
s2
s3
s4
s5
s6
s7
S(ν2) = {s1,s2}
S(ν3) = {s4,s6}
ν3
S(ν1) = {s3}
slab corresponding to ν and that they do not intersect each other. This implies
that the segments can be ordered vertically. Hence, we can store S(ν) in a
search tree T(ν) according to the vertical order. By searching in T(ν) we can
s1
s2
s4
s5
s1
s2
s3
s3
s5
s4
ﬁnd the intersected segments in O(logn+kν) time, where kν is the number of
intersected segments. The total data structure for the set S is thus as follows.
The set S is stored in a segment tree T based on the x-intervals of the
segments.
The canonical subset of a node ν in T, which contains the segments spanning
the slab corresponding to ν but not the slab corresponding to the parent of
ν, is stored in a binary search tree T(ν) based on the vertical order of the
segments within the slab.
Because the associated structure of any node ν uses storage linear in the size of
S(ν), the total amount of storage remains O(nlogn). The associated structures
can be built in O(nlogn) time, leading to a preprocessing time of O(nlog2 n).
With a bit of extra work this can be improved to O(nlogn). The idea is to
maintain a (partial) vertical order on the segments while building the segment
tree. With this order available the associated structures can be computed in
linear time.
The query algorithm is quite simple: we search with qx in the segment tree in
the usual way, and at every node ν on the search path we search with the upper
and lower endpoint of q in T(ν) to report the segments in S(ν) intersected by q.
This basically is a 1-dimensional range query—see Section 5.1. The search
in T(ν) takes O(logn+kν) time, where kν is the number of reported segments
at ν. Hence, the total query time is O(log2 n+k), and we obtain the following
theorem.
236

Section 10.4
NOTES AND COMMENTS
Theorem 10.13 Let S be a set of n disjoint segments in the plane. The segments
intersecting a vertical query segment can be reported in O(log2 n+k) time with
a data structure that uses O(nlogn) storage, where k is the number of reported
segments. The structure can be built in O(nlogn) time.
Actually, it is only required that the segments have disjoint interiors. It
is easily veriﬁed that the same approach can be used when the endpoints of
segments are allowed to coincide with other endpoints or segments. This leads
to the following result.
Corollary 10.14 Let S be a set of n segments in the plane with disjoint interiors.
The segments intersecting an axis-parallel rectangular query window can be
reported in O(log2 n+k) time with a data structure that uses O(nlogn) storage,
where k is the number of reported segments. The structure can be built in
O(nlogn) time.
10.4
Notes and Comments
The query that asks for all intervals that contain a given point is often referred
to as a stabbing query. The interval tree structure for stabbing queries is due to
Edelsbrunner [157] and McCreight [270]. The priority search tree was designed
by McCreight [271]. He observed that the priority search tree can be used for
stabbing queries as well. The transformation is simple: map each interval [a : b]
to the point (a,b) in the plane. Performing a stabbing query with a value qx can
be done by doing a query with the range (−∞: qx]×[qx : +∞). Ranges of this
type are a special case of the ones supported by priority search trees.
(qx,qx)
(a,b)
The segment tree was discovered by Bentley [45]. Used as a 1-dimensional
data structure for stabbing queries it is less efﬁcient than the interval tree
since it requires O(nlogn) storage. The importance of the segment tree is
mainly that the sets of intervals stored with the nodes can be structured in
any manner convenient for the problem at hand. Therefore, there are many
extensions of segment trees that deal with 2- and higher-dimensional objects
[103, 157, 163, 301, 375]. A second plus of the segment tree over the interval
tree is that the segment tree can easily be adapted to stabbing counting queries:
report the number of intervals containing the query point. Instead of storing the
intervals in a list with the nodes, we store an integer representing their number.
A query with a point is answered by adding the integers on one search path.
Such a segment tree for stabbing counting queries uses only linear storage and
queries require O(logn) time, so it is optimal.
There has been a lot of research in the past on the dynamization of interval
trees and segment trees, that is, making it possible to insert and/or delete
intervals. Priority search trees were initially described as a fully dynamic
data structure, by replacing the binary tree with a red-black tree [199, 137] or
other balanced binary search tree that requires only O(1) rotations per update.
Dynamization is important in situations where the input changes. Dynamic data
structures are also important in many plane sweep algorithms where the status
237

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
normally needs to be stored in a dynamic structure. In a number of problems a
dynamic version of the segment tree is required here.
The notion of decomposable searching problems [46, 48, 166, 254, 269, 276,
304, 306, 307, 308, 337] gave a great push toward the dynamization of a large
class of data structures. Let S be a set of objects involved in a searching problem,
and let A∪B be any partition of S. The searching problem is decomposable if the
solution to the searching problem on S can be obtained in constant time from the
solutions to the searching problems on A and on B. The stabbing query problem
is decomposable because if we have reported the stabbed intervals in the two
subsets of a partition, we have automatically reported all intervals in the whole
set. Similarly, the stabbing counting query problem is decomposable because
we can add the integer answers yielded by the subsets to give the answer for the
whole set. Many other searching problems, like the range searching problem,
are decomposable as well. Static data structures for decomposable searching
problems can be turned into dynamic structures using general techniques. For
an overview see the book by Overmars [299].
The stabbing query problem can be generalized to higher dimensions. Here
we are given a collection of axis-parallel (hyper-)rectangles and ask for those
rectangles that contain a given query point. To solve these higher-dimensional
stabbing queries we can use multi-level segment trees. The structure uses
O(nlogd−1 n) storage and stabbing queries can be answered in O(logd n) time.
The use of fractional cascading—see Chapter 5—lowers the query time bound
by a logarithmic factor. The use of the interval tree on the deepest level of
associated structures lowers the storage bound with a logarithmic factor. A
higher-dimensional version of the interval tree and priority search tree doesn't
exist, that is, there is no clear extension of the structure that solves the analogous
query problem in higher dimensions. But the structures can be used as associated
structure of segment trees and range trees. This is useful, for instance, to solve
the stabbing query problem in sets of axis-parallel rectangles, and for range
searching with ranges of the form [x : x′]×[y : y′]×[z : +∞).
In geographic information systems, the most widely used geometric data
structure is the R-tree [204]. It is a 2-dimensional extension of the well-known
B-tree, suitable for storage on disk. On disk, memory is partitioned in blocks of
some size B, and the idea is to have nodes of high degree that ﬁt exactly into
one block. An internal node gives rise to a multi-way split instead of just a
binary split, allowing the tree to be much less deep. Queries that follow only one
path down the tree require fewer blocks from disk, and are therefore answered
more efﬁciently than with a binary tree. An R-tree can store any set of objects
(points, line segments, polygons), and answer intersection queries with any
query object. An R-tree uses linear storage, but the worst-case query time is
also linear, making its performance nothing better than a simple linked list, in
theory. In practice, however, R-trees perform well. A few theoretical results on
R-trees are known. It was shown in [5] that any type of R-tree built on n points
in d-dimensional space has to visit Ω((n/B)1−1/d +k/B) blocks (where k is the
number of answers). A variation of the R-tree, called the PR-tree [18], attains
this bound if both the objects stored and the query objects are axis-parallel
hyperrectangles. For an extensive overview of R-trees and related structures,
see [335].
238

Section 10.5
EXERCISES
10.5
Exercises
10.1
In Section 10.1 we solved the problem of ﬁnding all horizontal line
segments in a set that intersect a vertical segment. For this we used an
interval tree with priority search trees as associated structures. There
is also an alternative approach. We can use a 1-dimensional range tree
on the y-coordinate of the segments to determine those segments whose
y-coordinate lies in the y-range of the query segment. The resulting
segments cannot lie above or below the query segment, but they may
lie completely to the left or to the right of it. We get those segments
in O(logn) canonical subsets. For each of these subsets we use as an
associated structure an interval tree on the x-coordinates to ﬁnd the
segments that actually intersect the query segment.
q
a. Give the algorithm in pseudocode.
b. Prove that the data structure correctly solves the queries.
c. What are the bounds for preprocessing time, storage, and query time
of this structure? Prove your answers.
10.2
Let P be a set of n points in the plane, sorted on y-coordinate. Show
that, because P is sorted, a priority search tree of the points in P can be
constructed in O(n) time.
10.3
In the exercisepart of the algorithms for priority search trees we assumed
that all points have distinct coordinates. It was indicated that this re-
striction can be removed by using composite numbers as described in
Section 5.5. Show that all basic operations required in building and
querying priority search trees can indeed be performed with composite
numbers.
10.4
Windowing queries in sets of non-intersecting segments are performed
using a range query on the set of endpoints and intersection queries with
the four boundaries of the window. Explain how to avoid that segments
are reported more than once. To this end, make a list of all possible ways
in which an arbitrarily oriented segment can intersect a query window.
10.5
In this exercise you must show how the data structure of Theorem 10.13
can be built in O(nlogn) time. The associated structures that were
used are binary search trees on the vertical order of the segments. If
the vertical order is given, then an associated structure can be built in
linear time. So it remains to compute a sorted list of segments for each
canonical subset in O(nlogn) time in total.
Let S be a set of n disjoint segments in the plane. For two segments
s,s′ ∈S, we say that s lies below s′, denoted s ≺s′, if and only if there
are points p ∈s, p′ ∈s′ with px = p′
x and py < p′
y.
s′
s
p
p′
px
a. Prove that the relation ≺deﬁnes an acyclic relation on S. In other
words, you must prove that an order s1,s2,...,sn for the segments in
S exists such that i > j implies si ̸≺s j.
239

Chapter 10
MORE GEOMETRIC DATA STRUCTURES
b. Describe an O(nlogn) algorithm to compute such an order. Hint: Use
plane sweep to ﬁnd all segments that are vertically adjacent, construct
a directed graph on these adjacencies, and apply topological sorting
on the graph.
c. Explain how to obtain the sorted lists for the canonical subsets in the
segment tree using this acyclic order.
10.6
Let I be a set of intervals on the real line. We want to be able to count the
number of intervals containing a query point in O(logn) time. Thus, the
query time must be independent of the number of segments containing
the query point.
a. Describe a data structure for this problem based on segment trees,
which uses only O(n) storage. Analyze the preprocessing time, and
the query time of the data structure.
b. Describe a data structure for this problem based on interval trees. You
should replace the lists associated with the nodes of the interval tree
with other structures. Analyze the amount of storage, preprocessing
time, and the query time of the data structure.
c. Describe a data structure for this problem based on a simple binary
search tree. Your structure should have O(n) storage and O(logn)
query time. (Hence, segment trees are actually not needed to solve
this problem efﬁciently.)
10.7
a. We want to solve the following query problem: Given a set S of n
disjoint line segments in the plane, determine those segments that
intersect a vertical ray running from a point (qx,qy) vertically up-
wards to inﬁnity. Describe a data structure for this problem that uses
O(nlogn) storage and has a query time of O(logn + k), where k is
the number of reported answers.
b. Now suppose we only want to report the ﬁrst segment hit by the query
ray. Describe a data structure for this problem with O(n) expected
storage and O(logn) expected query time. Hint: Apply the locus
approach.
(qx,qy)
10.8
Segment trees can be used for multi-level data structures.
a. Let R be a set of n axis-parallel rectangles in the plane. Design a data
structure for R such that the rectangles containing a query point q
can be reported efﬁciently. Analyze the amount of storage and the
query time of your data structure. Hint: Use a segment tree on the
x-intervals of the rectangles, and store canonical subsets of the nodes
in this segment tree in an appropriate associated structure.
b. Generalize this data structure to d-dimensional space. Here we are
given a set of axis-parallel hyperrectangles—that is, polytopes of the
form [x1 : x′
1]×[x2 : x′
2]×···×[xd : x′
d]—and we want to report the
hyperrectangles containing a query point. Analyze the amount of
storage and the query time of your data structure.
240

Section 10.5
EXERCISES
10.9
Let I be a set of intervals on the real line. We want to store these intervals
such that we can efﬁciently determine those intervals that are completely
contained in a given interval [x : x′]. Describe a data structure that uses
O(nlogn) storage and solves such queries in O(logn+k) time, where k
is the number of answers. Hint: Use a range tree.
10.10 Again we have a collection I of intervals on the real line, but this time we
want to efﬁciently determine those intervals that contain a given interval
[x : x′]. Describe a data structure that uses O(n) storage and solves such
queries in O(logn + k) time, where k is the number of answers. Hint:
Use a priority search tree.
10.11 Consider the following alternative approach to solve the 2-dimensional
range searching problem: We construct a balanced binary search tree on
the x-coordinate of the points. For a node ν in the tree, let P(ν) be the
set of points stored in the subtree rooted at ν. For each node ν we store
two associated priority search trees of P(ν), a tree Tleft(ν) allowing
for range queries that are unbounded to the left, and a tree Tright(ν) for
range queries that are unbounded to the right.
A query with a range [x : x′]×[y : y′] is performed as follows. We search
for the node νsplit where the search paths toward x and x′ split in the tree.
Now we perform a query with range [x : +∞)×[y : y′] on Tright(lc(νsplit))
and a query with range (−∞: x′]×[y : y′] on Tleft(rc(νsplit)). This gives
all the answers (there is no need to search further down in the tree!).
a. Prove that the data structure correctly solves range queries.
b. What are the bounds for preprocessing time, storage, and query time
of this structure? Prove your answers.
10.12 a. In Section 10.3 an algorithm was given to insert an interval into
a segment tree (assuming its endpoints are already present in the
skeleton of the tree). Show that it is also possible to delete intervals
in O(logn) time. (You may need to maintain some extra information
for this.)
b. Let P = {p1,..., pn} be a set of n points in the plane, and let R =
{r1,...,rn} be a set of n possibly intersecting rectangles. Give an
algorithm to report all pairs pi,r j such that pi ∈r j. Your algorithm
should run in O(nlogn+k) time, where k is the number of reported
pairs.
241

11
Convex Hulls
Mixing Things
The output of oil wells is a mixture of several different components, and the
proportions of these components vary between different sources. This can
sometimes be exploited: by mixing together the output of different wells, one
can produce a mixture with proportions that are particularly favorable for the
reﬁning process.
Let's look at an example. For simplicity we assume that we are only
interested in two of the components—call them A and B—of our product.
Assume that we are given a mixture ξ1 with 10% of component A and 35% of
component B, and another mixture ξ2 with 16% of A and 20% of B. Assume
further that what we really need is a mixture that contains 12% of A and 30%
of B. Can we produce this mixture from the given ones? Yes, mixing ξ1 and ξ2
in the ratio 2 : 1 gives the desired product. However, it is impossible to make
a mixture of ξ1 and ξ2 that contains 13% of A and 22% of B. But if we have a
third mixture ξ3 containing 7% of A and 15% of B, then mixing ξ1, ξ2, and ξ3
in the ratio of 1 : 3 : 1 will give the desired result.
(0.1,0.35)
(0.16,0.2)
(0.07,0.15)
(0.12,0.3)
(0.13,0.22)
What has all this to do with geometry? This becomes clear when we
represent the mixtures ξ1, ξ2, and ξ3 by points in the plane, namely by p1 :=
(0.1,0.35), p2 := (0.16,0.2), and p3 := (0.07,0.15). Mixing ξ1 and ξ2 in the
ratio 2 : 1 gives the mixture represented by the point q := (2/3)p1 +(1/3)p2.
This is the point on the segment p1p2 such that dist(p2,q) : dist(q, p1) = 2 : 1,
where dist(.,.) denotes the distance between two points. More generally, by
mixing ξ1 and ξ2 in varying ratios, we can produce the mixtures represented
by any point on the line segment p1p2. If we start with the three base mixtures
ξ1, ξ2, and ξ3, we can produce any point in the triangle p1p2p3. For instance,
mixing ξ1, ξ2, and ξ3 in the ratio 1 : 3 : 1 gives the mixture represented by the
point (1/5)p1 +(3/5)p2 +(1/5)p3 = (0.13,0.22).
What happens if we don't have three but n base mixtures, for some n > 3,
represented by points p1, p2,..., pn? Suppose that we mix them in the ratio
l1 : l2 : ··· : ln. Let L := ∑n
j=1 lj and let λi := li/L. Note that
λi ⩾0 for all i
and
n
∑
i=1
λi = 1.
The mixture we get by mixing the base mixtures in the given ratio is the one
243

Chapter 11
CONVEX HULLS
represented by
n
∑
i=1
λipi.
Such a linear combination of the points pi where the λi satisfy the conditions
stated above—each λi is non-negative, and the sum of the λi is one—is called a
convex combination. In Chapter 1 we deﬁned the convex hull of a set of points
as the smallest convex set containing the points or, more precisely, as the inter-
section of all convex sets containing the points. One can show that the convex
hull of a set of points is exactly the set of all possible convex combinations of
the points. We can therefore test whether a mixture can be obtained from the
base mixtures by computing the convex hull of their representative points, and
checking whether the point representing the mixture lies inside it.
What if there are more than two interesting components in the mixtures?
Well, what we have said above remains true; we just have to move to a space of
higher dimension. More precisely, if we want to take d components into account
we have to represent a mixture by a point in d-dimensional space. The convex
hull of the points representing the base mixtures, which is a convex polytope,
represents the set of all possible mixtures.
Convex hulls—in particular convex hulls in 3-dimensional space—are used in
various applications. For instance, they are used to speed up collision detection
in computer animation. Suppose that we want to check whether two objects P1
and P2 intersect. If the answer to this question is negative most of the time, then
the following strategy pays off. Approximate the objects by simpler objects
!
P1 and !
P2 that contain the originals. If we want to check whether P1 and P2
intersect, we ﬁrst check whether !
P1 and !
P2 intersect; only if this is the case do
we need to perform the—supposedly more costly—test on the original objects.
There is a trade-off in the choice of the approximating objects. On the one
hand, we want them to be simple so that intersection tests are cheap. On the other
hand, simple approximations most likely do not approximate the original objects
very well, so there is a bigger chance we have to test the originals. Bounding
spheres are on one side of the spectrum: intersection tests for spheres are quite
simple, but for many objects spheres do not provide a good approximation.
Convex hulls are more on the other side of the spectrum: intersection tests for
convex hulls are more complicated than for spheres—but still simpler than for
non-convex objects—but convex hulls can approximate most objects a lot better.
11.1
The Complexity of Convex Hulls in 3-Space
In Chapter 1 we have seen that the convex hull of a set P of n points in the plane
is a convex polygon whose vertices are points in P. Hence, the convex hull
has at most n vertices. In 3-dimensional space a similar statement is true: the
convex hull of a set P of n points is a convex polytope whose vertices are points
in P and, hence, it has at most n vertices. In the planar case the bound on the
number of vertices immediately implies that the complexity of the convex hull
244

Section 11.1
THE COMPLEXITY OF CONVEX HULLS
IN 3-SPACE
is linear, since the number of edges of a planar polygon is equal to the number
of vertices. In 3-space this is no longer true; the number of edges of a polytope
can be higher than the number of vertices. But fortunately the difference cannot
be too large, as follows from the following theorem on the number of edges and
facets of convex polytopes. (Formally, a facet of a convex polytope is deﬁned
to be a maximal subset of coplanar points on its boundary. A facet of a convex
polytope is necessarily a convex polygon. An edge of a convex polytope is an
edge of one of its facets.)
facet
edge
vertex
Theorem 11.1 Let P be a convex polytope with n vertices. The number of
edges of P is at most 3n−6, and the number of facets of P is at most 2n−4.
Proof. Recall that Euler's formula states for a connected planar graph with n
nodes, ne arcs, and n f faces the following relation holds:
n−ne +nf = 2.
Since we can interpret the boundary of a convex polytope as a planar graph—see
Figure 11.1—the same relation holds for the numbers of vertices, edges, and
facets in a convex polytope. (In fact, Euler's formula was originally stated in
Figure 11.1
A cube interpreted as a planar graph:
note that one facet maps to the
unbounded face of the graph
terms of polytopes, not in terms of planar graphs.) Every face of the graph
corresponding to P has at least three arcs, and every arc is incident to two faces,
so we have 2ne ⩾3nf . Plugging this into Euler's formula we get
n+n f −2 ⩾3n f /2,
so n f ⩽2n−4. Applying Euler's formula once more, we see that ne ⩽3n−6.
For the special case that every facet is a triangle—the case of a simplicial
polytope—the bounds on the number of edges and facets of an n-vertex polytope
are exact, because then 2ne = 3n f .
Theorem 11.1 also holds for non-convex polytopes whose so-called genus
is zero, that is, polytopes without holes or tunnels; for polytopes of larger genus
similar bounds hold. Since this chapter deals with convex hulls, however, we
refrain from deﬁning what a (non-convex) polytope exactly is, which we would
need to do to prove the theorem in the non-convex case.
245

Chapter 11
CONVEX HULLS
If we combine Theorem 11.1 with the earlier observation that the convex
hull of a set of points in 3-space is a convex polytope whose vertices are points
in P, we get the following result.
Corollary 11.2 The complexity of the convex hull of a set of n points in three-
dimensional space is O(n).
11.2
Computing Convex Hulls in 3-Space
Let P be a set of n points in 3-space. We will compute CH(P), the convex hull
of P, using a randomized incremental algorithm, following the paradigm we
have met before in Chapters 4, 6, and 9.
The incremental construction starts by choosing four points in P that do not lie
in a common plane, so that their convex hull is a tetrahedron. This can be done
as follows. Let p1 and p2 be two points in P. We walk through the set P until
we ﬁnd a point p3 that does not lie on the line through p1 and p2. We continue
searching P until we ﬁnd a point p4 that does not lie in the plane through p1, p2,
and p3. (If we cannot ﬁnd four such points, then all points in P lie in a plane. In
this case we can use the planar convex hull algorithm of Chapter 1 to compute
the convex hull.)
Next we compute a random permutation p5,..., pn of the remaining points.
We will consider the points one by one in this random order, maintaining the
convex hull as we go. For an integer r ⩾1, let Pr := {p1,..., pr}. In a generic
step of the algorithm, we have to add the point pr to the convex hull of Pr−1,
that is, we have to transform CH(Pr−1) into CH(Pr). There are two cases.
If pr lies inside CH(Pr−1), or on its boundary, then CH(Pr) = CH(Pr−1),
and there is nothing to be done.
Figure 11.2
The horizon of a polytope
pr
horizon
Now suppose that pr lies outside CH(Pr−1). Imagine that you are standing
at pr, and that you are looking at CH(Pr−1). You will be able to see some
246

Section 11.2
COMPUTING CONVEX HULLS IN
3-SPACE
facets of CH(Pr−1)—the ones on the front side—but others will be invisible
because they are on the back side. The visible facets form a connected region
on the surface of CH(Pr−1), called the visible region of pr on CH(Pr−1),
which is enclosed by a closed curve consisting of edges of CH(Pr−1). We
call this curve the horizon of pr on CH(Pr−1). As you can see in Figure 11.2,
the projection of the horizon is the boundary of the convex polygon obtained
f is visible from p,
but not from q
f
p
q
hf
by projecting CH(Pr−1) onto a plane, with pr as the center of projection.
What exactly does "visible" mean geometrically? Consider the plane hf
containing a facet f of CH(Pr−1). By convexity, CH(Pr−1) is completely
contained in one of the closed half-spaces deﬁned by hf . The face f is
visible from a point if that point lies in the open half-space on the other side
of h f .
The horizon of pr plays a crucial role when we want to transform CH(Pr−1)
to CH(Pr): it forms the border between the part of the boundary that can
be kept—the invisible facets—and the part of the boundary that must be
replaced—the visible facets. The visible facets must be replaced by facets
connecting pr to its horizon.
Before we go into more details, we should decide how we are going to represent
the convex hull of points in space. As we observed before, the boundary of a
3-dimensional convex polytope can be interpreted as a planar graph. Therefore
we store the convex hull in the form of a doubly-connected edge list, a data
structure developed in Chapter 2 for storing planar subdivisions. The only
difference is that vertices will now be 3-dimensional points. We will keep the
convention that the half-edges are directed such that the ones bounding any face
form a counterclockwise cycle when seen from the outside of the polytope.
pr
CH(Pr−1)
CH(Pr)
pr
Figure 11.3
Adding a point to the convex hull
Back to the addition of pr to the convex hull. We have a doubly-connected
edge list representing CH(Pr−1), which we have to transform into a doubly-
connected edge list for CH(Pr). Suppose that we knew all facets of CH(Pr−1)
visible from pr. Then it would be easy to remove all the information stored
for these facets from the doubly-connected edge list, compute the new facets
connecting pr to the horizon, and store the information for the new facets in the
doubly-connected edge list. All this will take linear time in the total complexity
of the facets that disappear.
There is one subtlety we should take care of after the addition of the new
247

Chapter 11
CONVEX HULLS
facets: we have to check whether we have created any coplanar facets. This
happens if pr lies in the plane of a face of CH(Pr−1). Such a face f is not visible
from pr by our deﬁnition of visibility above. Hence, f will remain unchanged,
pr
f
and we will add triangles connecting pr to the edges of f that are part of the
horizon. Those triangles are coplanar with f, and so they have to be merged
with f into one facet.
In the discussion so far we have ignored the problem of ﬁnding the facets of
CH(Pr−1) that are visible from pr. Of course this could be done by testing
every facet. Since such a test takes constant time—we have to check to which
side of a given plane the point pr lies—we can ﬁnd all visible facets in O(r)
time. This would lead to an O(n2) algorithm. Next we show how to do better.
The trick is that we are going to work ahead: besides the convex hull of
the current point set we shall maintain some additional information, which will
make it easy to ﬁnd the visible facets. In particular, we maintain for each facet
f of the current convex hull CH(Pr) a set Pconﬂict(f) ⊆{pr+1, pr+2,..., pn}
containing the points that can see f. Conversely, we store for every point pt,
with t > r, the set Fconﬂict(pt) of facets of CH(Pr) visible from pt. We will say
that a point p ∈Pconﬂict(f) is in conﬂict with the facet f, because p and f cannot
peacefully live together in the convex hull—once we add a point p ∈Pconﬂict(f)
to the convex hull, the facet f must go. We call Pconﬂict(f) and Fconﬂict(pt)
conﬂict lists.
points
facets
conﬂicts
Fconﬂict(pt)
Pconﬂict(f)
pt
f
We maintain the conﬂicts in a so-called conﬂict graph, which we denote
by G. The conﬂict graph is a bipartite graph. It has one node set with a node for
every point of P that has not been inserted yet, and one node set with a node
for every facet of the current convex hull. There is an arc for every conﬂict
between a point and a facet. In other words, there is an arc between a point
pt ∈P and facet f of CH(Pr) if r < t and f is visible from pt. Using the conﬂict
graph G, we can report the set Fconﬂict(pt) for a given point pt (or Pconﬂict(f) for
a given facet f) in time linear in its size. This means that when we insert pr
into CH(Pr−1), all we have to do is to look up Fconﬂict(pr) in G to get the visible
facets, which we can then replace by the new convex hull facets connecting pr
to the horizon.
Initializing the conﬂict graph G for CH(P4) can be done in linear time: we
simply walk through the list of points P and determine which of the four faces
of CH(P4) they can see.
To update G after adding a point pr, we ﬁrst discard the nodes and incident
arcs for all the facets of CH(Pr−1) that disappear from the convex hull. These
are the facets visible from pr, which are exactly the neighbors of pr in G, so
this is easy. We also discard the node for pr. We then add nodes to G for the
new facets we created, which connect pr to the horizon. The essential step is to
ﬁnd the conﬂict lists of these new facets. No other conﬂicts have to be updated:
the conﬂict set Pconﬂict(f) of a facet f that is unaffected by the insertion of pr
remains unchanged.
The facets created by the insertion of pr are all triangles, except for those
that have been merged with existing coplanar facets. The conﬂict list of a facet
248

Section 11.2
COMPUTING CONVEX HULLS IN
3-SPACE
of the latter type is trivial to ﬁnd: it is the same as the conﬂict list of the existing
facet, since the merging does not change the plane containing the facet. So let's
look at one of the new triangles f incident to pr in CH(Pr). Suppose that a point
pt can see f. Then pt can certainly see the edge e of f that is opposite pr. This
edge e is a horizon edge of pr, and it was already present in CH(Pr−1). Since
CH(Pr−1) ⊂CH(Pr), the edge e must have been visible from pt in CH(Pr−1)
as well. That can only be the case if one of the two facets incident to e in
CH(Pr−1) is visible from pt. This implies that the conﬂict list of f can be found
by testing the points in the conﬂict lists of the two facets f1 and f2 that were
incident to the horizon edge e in CH(Pr−1).
e
pr
f
f1
f2
We stated earlier that we store the convex hull as a doubly-connected edge list,
so changing the convex hull means changing the information in the doubly-
connected edge list. To keep the code short, however, we have omitted all
explicit references to the doubly-connected edge list in the pseudocode below,
which summarizes the convex hull algorithm.
Algorithm CONVEXHULL(P)
Input. A set P of n points in three-space.
Output. The convex hull CH(P) of P.
1.
Find four points p1, p2, p3, p4 in P that form a tetrahedron.
2.
C ←CH({p1, p2, p3, p4})
3.
Compute a random permutation p5, p6,..., pn of the remaining points.
4.
Initialize the conﬂict graph G with all visible pairs (pt, f), where f is a
facet of C and t > 4.
5.
for r ←5 to n
6.
do (∗Insert pr into C: ∗)
7.
if Fconﬂict(pr) is not empty (∗that is, pr lies outside C ∗)
8.
then Delete all facets in Fconﬂict(pr) from C.
9.
Walk along the boundary of the visible region of pr (which
consists exactly of the facets in Fconﬂict(pr)) and create a list
L of horizon edges in order.
10.
for all e ∈L
11.
do Connect e to pr by creating a triangular facet f.
12.
if f is coplanar with its neighbor facet f ′ along e
13.
then Merge f and f ′ into one facet, whose conﬂict
list is the same as that of f ′.
14.
else (∗Determine conﬂicts for f: ∗)
15.
Create a node for f in G.
16.
Let f1 and f2 be the facets incident to e in the
old convex hull.
17.
P(e) ←Pconﬂict(f1)∪Pconﬂict(f2)
18.
for all points p ∈P(e)
19.
do If f is visible from p, add (p, f) to G.
20.
Delete the node corresponding to pr and the nodes corre-
sponding to the facets in Fconﬂict(pr) from G, together with
their incident arcs.
21. return C
249

Chapter 11
CONVEX HULLS
11.3*
The Analysis
As usual when we analyse a randomized incremental algorithm, we ﬁrst try to
bound the expected structural change. For the convex hull algorithm this means
we want to bound the total number of facets created by the algorithm.
Lemma 11.3 The expected number of facets created by CONVEXHULL is at
most 6n−20.
Proof. The algorithm starts with a tetrahedron, which has four facets. In every
stage r of the algorithm where pr lies outside CH(Pr−1), new triangular facets
connecting pr to its horizon on CH(Pr−1) are created. What is the expected
number of new facets? As in previous occasions where we analyzed randomized
algorithms, we use backwards analysis. We look at CH(Pr) and imagine remov-
ing vertex pr; the number of facets that disappear due to the removal of pr from
CH(Pr) is the same as the number of facets that were created due to the insertion
of pr into CH(Pr−1). The disappearing facets are exactly the ones incident to pr,
and their number equals the number of edges incident to pr in CH(Pr). We call
this number the degree of pr in CH(Pr), and we denote it by deg(pr,CH(Pr)).
We now want to bound the expected value of deg(pr,CH(Pr)).
By Theorem 11.1 a convex polytope with r vertices has at most 3r−6 edges.
This means that the sum of the degrees of the vertices of CH(Pr), which is a
convex polytope with r or less vertices, is at most 6r −12. Hence, the average
degree is bounded by 6 −12/r. Since we treat the vertices in random order,
it seems that the expected degree of pr is bounded by 6 −12/r. We have to
be a little bit careful, though: the ﬁrst four points are already ﬁxed when we
generate the random permutation, so pr is a random element of {p5,..., pr},
not of Pr. Because p1,..., p4 have total degree at least 12, the expected value of
deg(pr,CH(Pr)) is bounded as follows:
E[deg(pr,CH(Pr))]
=
1
r −4
r
∑
i=5
deg(pi,CH(Pr))
⩽
1
r −4
" # r
∑
i=1
deg(pi,CH(Pr))
$
−12
%
⩽
6r −12−12
r −4
= 6.
The expected number of facets created by CONVEXHULL is the number of
facets we start with (four) plus the expected total number of facets created
during the additions of p5,..., pn to the hull. Hence, the expected number of
created facets is
4+
n
∑
r=5
E[deg(pr,CH(Pr))] ⩽4+6(n−4) = 6n−20.
Now that we have bounded the total amount of structural change we can
bound the expected running time of the algorithm.
250

Section 11.3*
THE ANALYSIS
Lemma 11.4 Algorithm CONVEXHULL computes the convex hull of a set P of
n points in R3 in O(nlogn) expected time, where the expectation is with respect
to the random permutation used by the algorithm.
Proof. The steps before the main loop can certainly be done in O(nlogn) time.
Stage r of the algorithm takes constant time if Fconﬂict(pr) is empty, which is
when pr lies inside, or on the boundary of, the current convex hull.
If that is not the case, most of stage r takes O(card(Fconﬂict(pr))) time,
where card() denotes the cardinality of a set. The exceptions to this are the
lines 17-19 and line 20. We shall bound the time spent in these lines later;
ﬁrst, we bound card(Fconﬂict(pr)). Note that card(Fconﬂict(pr)) is the number of
facets deleted due to the addition of the point pr. Clearly, a facet can only be
deleted if it has been created before, and it is deleted at most once. Since the
expected number of facets created by the algorithm is O(n) by Lemma 11.3,
this implies that the total number of deletions is O(n) as well, so
E[
n
∑
r=5
card(Fconﬂict(pr))] = O(n).
Now for lines 17-19 and line 20. Line 20 takes time linear in the number
of nodes and arcs that are deleted from G. Again, a node or arc is deleted at
most once, and we can charge the cost of this deletion to the stage where we
created it. It remains to look at lines 17-19. In stage r, these lines are executed
for all horizon edges, that is, all edges in L. For one edge e ∈L, they take
O(card(P(e))) time. Hence, the total time spent in these lines in stage r is
O(∑e∈L card(P(e))). To bound the total expected running time, we therefore
have to bound the expected value of
∑
e
card(P(e)),
where the summation is over all horizon edges that appear at any stage of the
algorithm. We will prove below that this is O(nlogn), which implies that the
total running time is O(nlogn).
We use the framework of conﬁguration spaces from Chapter 9 to supply
the missing bound. The universe X is the set of P, and the conﬁgurations ∆
correspond to convex hull edges. However, for technical reasons—in particular,
to be able to deal correctly with degenerate cases—we attach a half-edge to
both sides of the edge. To be more precise, a ﬂap ∆is deﬁned as an ordered
four-tuple of points (p,q,s,t) that do not all lie in a plane. The deﬁning set D(∆)
is simply the set {p,q,s,t}. The killing set K(∆) is more difﬁcult to visualize.
Denote the line through p and q by ℓ. Given a point x, let h(ℓ,x) denote the
half-plane bounded by ℓthat contains x. Given two points x, y, let ρ(x,y) be the
half-line starting in x and passing through y. A point x ∈X is in K(∆) if and
only if it lies in one of the following regions:
p
q
s
t
h(ℓ,s)
ρ(p,s)
ℓ
ρ(q,t)
h(ℓ,t)
outside the closed convex 3-dimensional wedge deﬁned by h(ℓ,s) and h(ℓ,t),
inside h(ℓ,s) but outside the closed 2-dimensional wedge deﬁned by ρ(p,q)
and ρ(p,s),
251

Chapter 11
CONVEX HULLS
inside h(ℓ,t) but outside the closed 2-dimensional wedge deﬁned by ρ(q,t)
and ρ(q, p),
inside the line ℓbut outside the segment pq,
inside the half-line ρ(p,s) but outside the segment ps,
inside the half-line ρ(q,t) but outside the segment qt.
For every subset S ⊆P, we deﬁne the set T(S) of active conﬁgurations—this is
what we want to compute—as prescribed in Chapter 9: ∆∈T(S) if and only if
D(∆) ⊆S and K(∆)∩S = /0.
Lemma 11.5 A ﬂap ∆= (p,q,s,t) is in T(S) if and only if pq, ps, and qt are
edges of the convex hull CH(S), there is a facet f1 incident to pq and ps, and a
different facet f2 incident to pq and qt. Furthermore, if one of the facets f1 or
f2 is visible from a point x ∈P then x ∈K(∆).
We leave the proof—which involves looking precisely at the cases when
points are collinear or coplanar, but which is otherwise not difﬁcult—to the
reader.
As you may have guessed, the ﬂaps take over the role of the horizon edges.
Lemma 11.6 The expected value of ∑e card(P(e)), where the summation is
over all horizon edges that appear at some stage of the algorithm, is O(nlogn).
Proof. Consider an edge e of the horizon of pr on CH(Pr−1). Let ∆= (p,q,s,t)
be one of the two ﬂaps with pq = e. By Lemma 11.5, ∆∈T(Pr−1), and the
points in P\Pr that can see one of the facets incident to e are all in K(∆), so
P(e) ⊆K(∆). By Theorem 9.15, it follows that the expected value of
∑
∆
card(K(∆)),
where the summation is over all ﬂaps ∆appearing in at least one T(Pr), is
bounded by
n
∑
r=1
16
"n−r
r
%"E
&
card(T(Pr))
'
r
%
.
The cardinality of T(Pr) is twice the number of edges of CH(Pr). Therefore it
is at most 6r −12, so we get the bound
∑
e
card(P(e)) ⩽∑
∆
card(K(∆)) ⩽
n
∑
r=1
16
"n−r
r
%"6r −12
r
%
⩽96nlnn.
This ﬁnishes the last piece of the analysis of the convex hull algorithm. We
get the following result:
Theorem 11.7 The convex hull of a set of n points in R3 can be computed in
O(nlogn) randomized expected time.
252

Section 11.4*
CONVEX HULLS AND HALF-SPACE
INTERSECTION
11.4*
Convex Hulls and Half-Space Intersection
In Chapter 8 we have met the concept of duality. The strenth of duality lies in
that it allows us to look at a problem from a new perspective, which can lead to
more insight in what is really going on. Recall that we denote the line that is
the dual of a point p by p∗, and the point that is the dual of a line ℓby ℓ∗. The
duality transform is incidence and order preserving: p ∈ℓif and only if ℓ∗∈p∗,
and p lies above ℓif and only if ℓ∗lies above p∗.
Let's have a closer look at what convex hulls correspond to in dual space.
We will do this for the planar case. Let P be a set of points in the plane. For
technical reasons we focus on its upper convex hull, denoted UH(P), which
consists of the convex hull edges that have P below their supporting line—see
the left side of Figure 11.4. The upper convex hull is a polygonal chain that
connects the leftmost point in P to the rightmost one. (We assume for simplicity
that no two points have the same x-coordinate.)
primal plane
dual plane
UH(P)
LE(P∗)
Figure 11.4
Upper hulls correspond to lower
envelopes
When does a point p ∈P appear as a vertex of the upper convex hull? That
is the case if and only if there is a non-vertical line ℓthrough p such that all
other points of P lie below ℓ. In the dual plane this statement translates to the
following condition: there is a point ℓ∗on the line p∗∈P∗such that ℓ∗lies
below all other lines of P∗. If we look at the arrangement A(P∗), this means that
p∗contributes an edge to the unique bottom cell of the arrangement. This cell is
the intersection of the half-planes bounded by a line in P∗and lying below that
line. The boundary of the bottom cell is an x-monotone chain. We can deﬁne
this chain as the minimum of the linear functions whose graphs are the lines in
P∗. For this reason, the boundary of the bottom cell in an arrangement is often
called the lower envelope of the set of lines. We denote the lower envelope of
P∗by LE(P∗)—see the right hand side of Figure 11.4.
The points in P that appear on UH(P) do so in order of increasing x-
coordinate. The lines of P∗appear on the boundary of the bottom cell in order
of decreasing slope. Since the slope of the line p∗is equal to the x-coordinate of
p, it follows that the left-to-right list of points on UH(P) corresponds exactly
to the right-to-left list of edges of LE(P∗). So the upper convex hull of a set of
points is essentially the same as the lower envelope of a set of lines.
Let's do one ﬁnal check. Two points p and q in P form an upper convex
253

Chapter 11
CONVEX HULLS
hull edge if and only if all other points in P lie below the line ℓthrough p and
q. In the dual plane, this means that all lines r∗, with r ∈P\{p,q}, lie above
the intersection point ℓ∗of p∗and q∗. This is exactly the condition under which
p∗∩q∗is a vertex of LE(P∗).
What about the lower convex hull of P and the upper envelope of P∗? (We
leave the precise deﬁnitions to the reader.) By symmetry, these concepts are
dual to each other as well.
We now know that the intersection of lower half-planes—half-planes bounded
from above by a non-vertical line—can be computed by computing an upper
convex hull, and that the intersection of upper half-planes can be computed by
computing a lower convex hull. But what if we want to compute the intersection
of an arbitrary set H of half-planes? Of course, we can split the set H into a
set H+ of upper half-planes and a set H−of lower half-planes, compute (H+
by computing the lower convex hull of H+∗and (H−by computing the upper
convex hull of H−∗, and then compute )H by intersecting (H+ and (H−.
But is this really necessary? If lower envelopes correspond to upper convex
hulls, and upper envelopes correspond to lower convex hulls, shouldn't then
the intersection of arbitrary half-planes correspond to full convex hulls? In a
sense, this is true. The problem is that our duality transformation cannot handle
vertical lines, and lines that are close to vertical but have opposite slope are
mapped to very different points. This explains why the dual of the convex hull
consists of two parts that lie rather far apart.
It is possible to deﬁne a different duality transformation that allows vertical
lines. However, to apply this duality to a given set of half-planes, we need a
point in the intersection of the half-planes. But that was to be expected. As long
as we do not want to leave the Euclidean plane, there cannot be any general
duality that turns the intersection of a set of half-planes into a convex hull,
because the intersection of half-planes can have one special property: it can be
empty. What could that possibly correspond to in the dual? The convex hull
of a set of points in Euclidean space is always well deﬁned: there is no such
thing as "emptiness." (This problem is nicely solved if one works in oriented
projective space, but this concept is beyond the scope of this book.) Only once
you know that the intersection is not empty, and a point in the interior is known,
can you deﬁne a duality that relates the intersection with a convex hull.
We leave it at this for now. The important thing is that—although there are
technical complications—convex hulls and intersections of half-planes (or half-
spaces in three dimensions) are essentially dual concepts. Hence, an algorithm
to compute the intersection of half-planes in the plane (or half-spaces in three
dimensions) can be given by dualizing a convex-hull algorithm.
11.5*
Voronoi Diagrams Revisited
In Chapter 7 we introduced the Voronoi diagram of a set of points in the plane. It
may come as a surprise that there is a close relationship between planar Voronoi
254

Section 11.5*
VORONOI DIAGRAMS REVISITED
diagrams and the intersection of upper half-spaces in 3-dimensional space. By
the result on duality of the previous section, this implies a close relation between
planar Voronoi diagrams and lower convex hulls in 3-space.
This has to do with an amazing property of the unit paraboloid in 3-space.
Let U := (z = x2 +y2) denote the unit paraboloid, and let p := (px, py,0) be
a point in the plane z = 0. Consider the vertical line through p. It intersects
U in the point p′ := (px, py, p2
x + p2
y). Let h(p) be the non-vertical plane z =
2pxx+2pyy−(p2
x + p2
y). Notice that h(p) contains the point p′. Now consider
any other point q := (qx,qy,0) in the plane z = 0. The vertical line through q
intersects U in the point q′ := (qx,qy,q2
x +q2
y), and it intersects h(p) in
p
p′
h(p)
U
q
q(p)
q′











dist(p,q)2
q(p) := (qx,qy,2pxqx +2pyqy −(p2
x + p2
y)).
The vertical distance between q′ and q(p) is
q2
x +q2
y −2pxqx −2pyqy + p2
x + p2
y = (qx −px)2 +(qy −py)2 = dist(p,q)2.
Hence, the plane h(p) encodes—together with the unit paraboloid—the distance
between p and any other point in the plane z = 0. (Since dist(p,q)2 ⩾0 for
any point q, and p′ ∈h(p), this also implies that h(p) is the tangent plane to U
at p′.)
The fact that the plane h(p) encodes the distance of other points to p leads to
a correspondence between Voronoi diagrams and upper envelopes, as explained
next. Let P be a planar point set, which we imagine to lie in the plane z = 0 of
3-dimensional space. Consider the set H := {h(p) | p ∈P} of planes, and let
UE(H) be the upper envelope of the planes in H. We claim that the projection
of UE(H) on the plane z = 0 is the Voronoi diagram of P. Figure 11.5 illustrates
this one dimension lower: the Voronoi diagram of the points pi on the line y = 0
is the projection of the upper envelope of the lines h(pi).
Theorem 11.8 Let P be a set of points in 3-dimensional space, all lying in the
plane z = 0. Let H be the set of planes h(p), for p ∈P, deﬁned as above. Then
the projection of UE(H) on the plane z = 0 is the Voronoi diagram of P.
Proof. To prove the theorem, we will show that the Voronoi cell of a point
p ∈P is exactly the projection of the facet of UE(H) that lies on the plane h(p).
Let q be a point in the plane z = 0 lying in the Voronoi cell of p. Hence, we
have dist(q, p) < dist(q,r) for all r ∈P with r ̸= p. We have to prove that the
vertical line through q intersects UE(H) at a point lying on h(p). Recall that for
a point r ∈P, the plane h(r) is intersected by the vertical line through q at the
point q(r) := (qx,qy,q2
x + q2
y −dist(q,r)2). Of all points in P, the point p has
the smallest distance to q, so q(p) is the highest intersection point. Hence, the
vertical line through q intersects UE(H) at a point lying on h(p), as claimed.
This theorem implies that we can compute a Voronoi diagram in the plane by
computing the upper envelope of a set of planes in 3-space. By Exercise 11.10
(see also the previous section), the upper envelope of a set of planes in 3-space
is in one-to-one correspondence to the lower convex hull of the points H∗, so
we can immediately use our algorithm CONVEXHULL.
255

Chapter 11
CONVEX HULLS
Figure 11.5
The correspondence between Voronoi
diagrams and upper envelopes
h(p2)
U
p3
p1
h(p3)
h(p1)
y = 0
Voronoi cell of p3
Voronoi cell of p1
Voronoi cell of p2
p2
Not surprisingly, the lower convex hull of H∗has a geometric meaning as
well: its projection on the plane z = 0 is the Delaunay graph of P.
11.6
Notes and Comments
The early convex hull algorithms worked only for points in the plane—see
the notes and comments of Chapter 1 for a discussion of these algorithms.
Computing convex hulls in 3-dimensional space turns out to be considerably
more difﬁcult. One of the ﬁrst algorithms was the "gift wrapping" algorithm
due to Chand and Kapur [84]. It ﬁnds facet after facet by "rotating" a plane
over known edges of the hull until the ﬁrst point is found. The running time is
O(n f) for a convex hull with f facets, which is O(n2) in the worst case. The
ﬁrst algorithm to achieve O(nlogn) running time was a divide-and-conquer
algorithm by Preparata and Hong [322, 323]. Early incremental algorithms run
in time O(n2) [223, 344]. The randomized version presented here is due to
Clarkson and Shor [133]. The version we presented needs O(nlogn) space; the
original paper gives a simple improvement to linear space. The idea of a conﬂict
graph, used here for the ﬁrst time in this book, also comes from the paper of
Clarkson and Shor. Our analysis, however, is due to Mulmuley [290].
In this chapter we have concentrated on 3-dimensional space, where convex
hulls have linear complexity. The so-called Upper Bound Theorem states that
the worst-case combinatorial complexity of the convex hull of n points in d-
dimensional space—phrased in dual space: the intersection of n half-spaces—is
Θ(n⌊d/2⌋). (We proved this result for the case d = 3, using Euler's relation.)
The algorithm described in this chapter generalizes to higher dimensions, and is
optimal in the worst case: its expected running time is Θ(n⌊d/2⌋). Interestingly,
the best known deterministic convex hull algorithm for odd-dimensional spaces
is based on a (quite complicated) derandomization of this algorithm [97]. Since
the convex hull in dimensions greater than three can have non-linear complexity,
256

Section 11.7
EXERCISES
output-sensitive algorithms may be useful. The best known output-sensitive
algorithm for computing convex hulls in Rd is due to Chan [82]. Its running
time is O(nlogk +(nk)1−1/(⌊d/2⌋+1) logO(1) n), where k denotes the complexity
of the convex hull. A good overview of the many results on convex-hull
computations is given in the survey by Seidel [347]. Readers who want to know
more about the mathematical aspects of polytopes in higher dimensions can
consult Gr¨unbaum's book [194], which is a classical reference for polytope
theory, or Ziegler's book [399], which treats the combinatorial aspects.
In Section 11.5 we have seen that the Voronoi diagram of a planar point set is
the projection of the upper envelope of a certain set of planes in 3-dimensional
space. A similar statement is true in higher dimensions: the Voronoi diagram
of a set of points in Rd is the projection of the upper envelope of a certain set
of hyperplanes in Rd+1. Not all sets of (hyper)planes deﬁne an upper envelope
whose projection is the Voronoi diagram of some point set. Interestingly, any
upper envelope does project onto a so-called power diagram, a generalization of
the Voronoi diagram where the sites are spheres rather than points [25].
11.7
Exercises
11.1
In Chapter 1 we deﬁned the convex hull of a set P of points as the
intersection of all convex sets containing the points. In the current
chapter we saw another deﬁnition: the convex hull of P is the set of all
convex combinations of the points in P. Prove that these two deﬁnitions
are equivalent, that is, prove that a point q is a convex combination of
points in P if and only if q lies in every convex set containing P.
11.2
Prove that the worst case running time of algorithm CONVEXHULL is
O(n3), and that there are sets of points where a bad choice of the random
permutation makes the algorithm actually need Θ(n3) time.
11.3
Describe a randomized incremental algorithm to compute the convex
hull of n points in the plane. Describe how to deal with degeneracies.
Analyze the expected running time of your algorithm.
11.4
In many applications, only a small percentage of the points in a given set
P of n points are extreme. In such a case, the convex hull of P has less
than n vertices. This can actually make our algorithm CONVEXHULL
run faster than Θ(nlogn).
Assume, for instance, that the expected number of extreme points in a
random sample of P of size r is O(rα), for some constant α < 1. (This
is true when the set P has been created by picking points uniformly at
random in a ball.) Prove that under this condition, the running time of
the algorithm is O(n).
11.5
The convex hull of a set P of n points in 3-dimensional space can also
be computed by "rotating" a plane over known edges of the convex
257

Chapter 11
CONVEX HULLS
hull, thereby discovering new facets. Give a detailed description of an
algorithm using this approach, and analyze its running time.
11.6
Describe a data structure that allows you to test whether a query point
q lies inside a convex polytope with n vertices in R3. (Hint: Use the
results from Chapter 6.)
11.7
Deﬁne a simple polytope to be a region in 3-space that is topologically
equivalent to a ball (but not necessarily convex) and whose boundary
consists of planar polygons. Describe how to test in O(n) time whether
a point lies inside a simple polytope with n vertices in 3-dimensional
space.
11.8
Describe a randomized incremental algorithm to compute the intersec-
tion of half-planes, and analyze its expected running time. Your algo-
rithm should maintain the intersection of the current set of half-planes.
To ﬁgure out where to insert a new half-plane, maintain a conﬂict graph
between the vertices of the current intersection and the half-planes that
are still to be inserted.
11.9
Describe a randomized incremental algorithm to compute the intersec-
tion of half-spaces in 3-dimensional space, and analyze its expected
running time. Maintain a conﬂict graph analogous to the previous exer-
cise.
11.10 In this exercise you have to work out the details of a 3-dimensional
duality transformation. Given a point p := (px, py, pz) in R3, let p∗be
the plane z = pxx+ pyy−pz. For a non-vertical plane h, deﬁne h∗such
that (h∗)∗= h. Give a deﬁnition of the upper convex hull UH(P) of
a set of points P and the lower envelope LE(H) of a set H of planes
in 3-space, similar to the way they were deﬁned for the planar case in
Section 11.4.
Show the following properties.
A point p lies on a plane h if and only if h∗lies on p∗.
A point p lies above h if and only if h∗lies above p∗.
A point p ∈P is a vertex of UH(P) if and only if p∗appears on
LE(P∗).
A segment pq is an edge of UH(P) if and only if p∗and q∗share an
edge on LE(P∗).
Points p1, p2,..., pk are the vertices of a facet f of UH(P) if and
only if p1∗, p2∗,..., pk∗support facets of LE(P∗) that share a com-
mon vertex.
258

12
Binary Space Partitions
The Painter's Algorithm
These days pilots no longer have their ﬁrst ﬂying experience in the air, but on
the ground in a ﬂight simulator. This is cheaper for the air company, safer for
the pilot, and better for the environment. Only after spending many hours in the
simulator are pilots allowed to operate the control stick of a real airplane. Flight
simulators must perform many different tasks to make the pilot forget that she
is sitting in a simulator. An important task is visualization: pilots must be able
to see the landscape above which they are ﬂying, or the runway on which they
are landing. This involves both modeling landscapes and rendering the models.
To render a scene we must determine for each pixel on the screen the object
that is visible at that pixel; this is called hidden surface removal. We must
also perform shading calculations, that is, we must compute the intensity of the
light that the visible object emits in the direction of the view point. The latter
task is very time-consuming if highly realistic images are desired: we must
compute how much light reaches the object—either directly from light sources
or indirectly via reﬂections on other objects—and consider the interaction of
the light with the surface of the object to see how much of it is reﬂected in the
direction of the view point. In ﬂight simulators rendering must be performed in
real-time, so there is no time for accurate shading calculations. Therefore a fast
and simple shading technique is employed and hidden surface removal becomes
an important factor in the rendering time.
The z-buffer algorithm is a very simple method for hidden surface removal.
This method works as follows. First, the scene is transformed such that the
viewing direction is the positive z-direction. Then the objects in the scene
are scan-converted in arbitrary order. Scan-converting an object amounts to
determining which pixels it covers in the projection; these are the pixels where
the object is potentially visible. The algorithm maintains information about the
already processed objects in two buffers: a frame buffer and a z-buffer. The
frame buffer stores for each pixel the intensity of the currently visible object,
that is, the object that is visible among those already processed. The z-buffer
stores for each pixel the z-coordinate of the currently visible object. (More
precisely, it stores the z-coordinate of the point on the object that is visible at
the pixel.) Now suppose that we select a pixel when scan-converting an object.
259

Chapter 12
BINARY SPACE PARTITIONS
If the z-coordinate of the object at that pixel is smaller than the z-coordinate
stored in the z-buffer, then the new object lies in front of the currently visible
object. So we write the intensity of the new object to the frame buffer, and its
z-coordinate to the z-buffer. If the z-coordinate of the object at that pixel is larger
than the z-coordinate stored in the z-buffer, then the new object is not visible,
and the frame buffer and z-buffer remain unchanged. The z-buffer algorithm is
easily implemented in hardware and quite fast in practice. Hence, this is the
most popular hidden surface removal method. Nevertheless, the algorithm has
Figure 12.1
The painter's algorithm in action
1
2
3
some disadvantages: a large amount of extra storage is needed for the z-buffer,
and an extra test on the z-coordinate is required for every pixel covered by an
object. The painter's algorithm avoids these extra costs by ﬁrst sorting the
objects according to their distance to the view point. Then the objects are scan-
converted in this so-called depth order, starting with the object farthest from
the view point. When an object is scan-converted we do not need to perform
any test on its z-coordinate, we always write its intensity to the frame buffer.
Entries in the frame buffer that have been ﬁlled before are simply overwritten.
Figure 12.1 illustrates the algorithm on a scene consisting of three triangles.
On the left, the triangles are shown with numbers corresponding to the order
in which they are scan-converted. The images after the ﬁrst, second, and third
triangle have been scan-converted are shown as well. This approach is correct
because we scan-convert the objects in back-to-front order: for each pixel the
last object written to the corresponding entry in the frame buffer will be the one
closest to the viewpoint, resulting in a correct view of the scene. The process
resembles the way painters work when they put layers of paint on top of each
other, hence the name of the algorithm.
To apply this method successfully we must be able to sort the objects quickly.
Unfortunately this is not so easy. Even worse, a depth order may not always
exist: the in-front-of relation among the objects can contain cycles. When such
a cyclic overlap occurs, no ordering will produce a correct view of this scene. In
this case we must break the cycles by splitting one or more of the objects, and
hope that a depth order exists for the pieces that result from the splitting. When
there is a cycle of three triangles, for instance, we can always split one of them
into a triangular piece and a quadrilateral piece, such that a correct displaying
order exists for the resulting set of four objects. Computing which objects to
split, where to split them, and then sorting the object fragments is an expensive
process. Because the order depends on the position of the view point, we must
recompute the order every time the view point moves. If we want to use the
painter's algorithm in a real-time environment such as ﬂight simulation, we
should preprocess the scene such that a correct displaying order can be found
260

Section 12.1
THE DEFINITION OF BSP TREES
quickly for any view point. An elegant data structure that makes this possible is
the binary space partition tree, or BSP tree for short.
12.1
The Deﬁnition of BSP Trees
To get a feeling for what a BSP tree is, take a look at Figure 12.2. This ﬁgure
shows a binary space partition (BSP) for a set of objects in the plane, together
with the tree that corresponds to the BSP. As you can see, the binary space
partition is obtained by recursively splitting the plane with a line: ﬁrst we split
the entire plane with ℓ1, then we split the half-plane above ℓ1 with ℓ2 and the
half-plane below ℓ1 with ℓ3, and so on. The splitting lines not only partition the
plane, they may also cut objects into fragments. The splitting continues until
there is only one fragment left in the interior of each region. This process is
ℓ1
o4
o5
o4
o2
o1 o1
o3
ℓ4
ℓ5
ℓ2
ℓ3
ℓ6
ℓ1
ℓ2
ℓ3
o1
o2
o3
o4
o5
ℓ4
ℓ5
ℓ6
Figure 12.2
A binary space partition and the
corresponding tree
naturally modeled as a binary tree. Each leaf of this tree corresponds to a face
of the ﬁnal subdivision; the object fragment that lies in the face is stored at the
leaf. Each internal node corresponds to a splitting line; this line is stored at the
node. When there are 1-dimensional objects (line segments) in the scene then
objects could be contained in a splitting line; in that case the corresponding
internal node stores these objects in a list.
For a hyperplane h : a1x1 +a2x2 +···+adxd +ad+1 = 0, we let h+ be the open
positive half-space bounded by h and we let h−be the open negative half-space:
h+ := {(x1,x2,...,xd) : a1x1 +a2x2 +···+adxd +ad+1 > 0}
and
h−:= {(x1,x2,...,xd) : a1x1 +a2x2 +···+adxd +ad+1 < 0}.
A binary space partition tree, or BSP tree, for a set S of objects in d-dimensional
space is now deﬁned as a binary tree T with the following properties:
If card(S) ⩽1 then T is a leaf; the object fragment in S (if it exists) is stored
explicitly at this leaf. If the leaf is denoted by ν, then the (possibly empty)
set stored at the leaf is denoted by S(ν).
261

Chapter 12
BINARY SPACE PARTITIONS
If card(S) > 1 then the root ν of T stores a hyperplane hν, together with the
set S(ν) of objects that are fully contained in hν. The left child of ν is the
root of a BSP tree T−for the set S−:= {h−
ν ∩s : s ∈S}, and the right child
of ν is the root of a BSP tree T+ for the set S+ := {h+
ν ∩s : s ∈S}.
The size of a BSP tree is the total size of the sets S(ν) over all nodes ν of the
BSP tree. In other words, the size of a BSP tree is the total number of object
fragments that are generated. If the BSP does not contain useless splitting
lines—lines that split off an empty subspace—then the number of nodes of the
tree is at most linear in the size of the BSP tree. Strictly speaking, the size
of the BSP tree does not say anything about the amount of storage needed to
store it, because it says nothing about the amount of storage needed for a single
fragment. Nevertheless, the size of a BSP tree as we deﬁned it is a good measure
to compare the quality of different BSP trees for a given set of objects.
Figure 12.3
The correspondence between nodes and
regions
ℓ1
ℓ2
ℓ3
ℓ1
ℓ2
ℓ3
The leaves in a BSP tree represent the faces in the subdivision that the BSP
induces. More generally, we can identify a convex region with each node ν in a
BSP tree T: this region is the intersection of the half-spaces h⋄
µ, where µ is an
ancestor of ν and ⋄= −when ν is in the left subtree of µ, and ⋄= + when it
is in the right subtree. The region corresponding to the root of T is the whole
space. Figure 12.3 illustrates this: the grey node corresponds to the grey region
ℓ+
1 ∩ℓ+
2 ∩ℓ−
3 .
The splitting hyperplanes used in a BSP can be arbitrary. For computational
purposes, however, it can be convenient to restrict the set of allowable splitting
hyperplanes. A usual restriction is the following. Suppose we want to construct
a BSP for a set of line segments in the plane. An obvious set of candidates for
the splitting lines is the set of extensions of the input segments. A BSP that only
uses such splitting lines is called an auto-partition. For a set of planar polygons
in 3-space, an auto-partition is a BSP that only uses planes through the input
polygons as splitting planes. It seems that the restriction to auto-partitions is a
severe one. But, although auto-partitions cannot always produce minimum-size
BSP trees, we shall see that they can produce reasonably small ones.
262

Section 12.2
BSP TREES AND THE PAINTER'S
ALGORITHM
12.2
BSP Trees and the Painter's Algorithm
Suppose we have built a BSP tree T on a set S of objects in 3-dimensional
space. How can we use T to get the depth order we need to display the set S
with the painter's algorithm? Let pview be the view point and suppose that pview
lies above the splitting plane stored at the root of T. Then clearly none of the
objects below the splitting plane can obscure any of the objects above it. Hence,
h−ν
h+ν
we can safely display all the objects (more precisely, object fragments) in the
subtree T−before displaying those in T+. The order for the object fragments
in the two subtrees T+ and T−is obtained recursively in the same way. This is
summarized in the following algorithm.
Algorithm PAINTERSALGORITHM(T, pview)
1.
Let ν be the root of T.
2.
if ν is a leaf
3.
then Scan-convert the object fragments in S(ν).
4.
else if pview ∈h+
ν
5.
then PAINTERSALGORITHM(T−, pview)
6.
Scan-convert the object fragments in S(ν).
7.
PAINTERSALGORITHM(T+, pview)
8.
else if pview ∈h−
ν
9.
then PAINTERSALGORITHM(T+, pview)
10.
Scan-convert the object fragments in S(ν).
11.
PAINTERSALGORITHM(T−, pview)
12.
else (∗pview ∈hν ∗)
13.
PAINTERSALGORITHM(T+, pview)
14.
PAINTERSALGORITHM(T−, pview)
Note that we do not draw the polygons in S(ν) when pview lies on the splitting
hν
plane hν, because polygons are ﬂat 2-dimensional objects and therefore not
visible from points that lie in the plane containing them.
The efﬁciency of this algorithm—indeed, of any algorithm that uses BSP trees—
depends largely on the size of the BSP tree. So we must choose the splitting
planes in such a way that fragmentation of the objects is kept to a minimum.
Before we can develop splitting strategies that produce small BSP trees, we
must decide on which types of objects we allow. We became interested in BSP
trees because we needed a fast way of doing hidden surface removal for ﬂight
simulators. Because speed is our main concern, we should keep the type of
objects in the scene simple: we should not use curved surfaces, but represent
everything in a polyhedral model. We assume that the facets of the polyhedra
have been triangulated. So we want to construct a BSP tree of small size for a
given set of triangles in 3-dimensional space.
263

Chapter 12
BINARY SPACE PARTITIONS
12.3
Constructing a BSP Tree
When you want to solve a 3-dimensional problem, it is usually not a bad idea to
gain some insight by ﬁrst studying the planar version of the problem. This is
also what we do in this section.
Let S be a set of n non-intersecting line segments in the plane. We will restrict
our attention to auto-partitions, that is, we only consider lines containing one of
the segments in S as candidate splitting lines. The following recursive algorithm
for constructing a BSP immediately suggests itself. Let ℓ(s) denote the line that
contains a segment s.
s
ℓ(s)
Algorithm 2DBSP(S)
Input. A set S = {s1,s2,...,sn} of segments.
Output. A BSP tree for S.
1.
if card(S) ⩽1
2.
then Create a tree T consisting of a single leaf node, where the set S is
stored explicitly.
3.
return T
4.
else (∗Use ℓ(s1) as the splitting line. ∗)
5.
S+ ←{s∩ℓ(s1)+ : s ∈S};
T+ ←2DBSP(S+)
6.
S−←{s∩ℓ(s1)−: s ∈S};
T−←2DBSP(S−)
7.
Create a BSP tree T with root node ν, left subtree T−, right sub-
tree T+, and with S(ν) = {s ∈S : s ⊂ℓ(s1)}.
8.
return T
The algorithm clearly constructs a BSP tree for the set S. But is it a small one?
Perhaps we should spend a little more effort in choosing the right segment to do
the splitting, instead of blindly taking the ﬁrst segment, s1. One approach that
comes to mind is to take the segment s ∈S such that ℓ(s) cuts as few segments
as possible. But this is too greedy: there are conﬁgurations of segments where
this approach doesn't work well. Furthermore, ﬁnding this segment would be
time consuming. What else can we do? Perhaps you already guessed: as in
previous chapters where we had to make a difﬁcult choice, we simply make a
random choice. That is to say, we use a random segment to do the splitting. As
we shall see later, the resulting BSP is expected to be fairly small.
To implement this, we put the segments in random order before we start the
construction:
Algorithm 2DRANDOMBSP(S)
1.
Generate a random permutation S′ = s1,...,sn of the set S.
2.
T ←2DBSP(S′)
3.
return T
Before we analyze this randomized algorithm, we note that one simple opti-
mization is possible. Suppose that we have chosen the ﬁrst few partition lines.
These lines induce a subdivision of the plane whose faces correspond to nodes
in the BSP tree that we are constructing. Consider one such face f. There can
264

Section 12.3
CONSTRUCTING A BSP TREE
be segments that cross f completely. Selecting one of these crossing segments
to split f will not cause any fragmentation of other segments inside f, while the
segment itself can be excluded from further consideration. It would be foolish
not to take advantage of such free splits. So our improved strategy is to make
free split
f
free splits whenever possible, and to use random splits otherwise. To implement
this optimization, we must be able to tell whether a segment is a free split. To
this end we maintain two boolean variables with each segment, which indicate
whether the left and right endpoint lie on one of the already added splitting lines.
When both variables become true, then the segment is a free split.
We now analyze the performance of algorithm 2DRANDOMBSP. To keep it
simple, we will analyze the version without free splits. (In fact, free splits do
not make a difference asympotically.)
We start by analyzing the size of the BSP tree or, in other words, the number
of fragments that are generated. Of course, this number depends heavily on
the particular permutation generated in line 1: some permutations may give
small BSP trees, while others give very large ones. As an example, consider the
(b)
s1
s2
s3
s1
s2
s3
(a)
Figure 12.4
Different orders give different BSPs
collection of three segments depicted in Figure 12.4. If the segments are treated
as illustrated in part (a) of the ﬁgure, then ﬁve fragments result. A different
order, however, gives only three fragments, as shown in part (b). Because the
size of the BSP varies with the permutation that is used, we will analyze the
expected size of the BSP tree, that is, the average size over all n! permutations.
Lemma 12.1 The expected number of fragments generated by the algorithm
2DRANDOMBSP is O(nlogn).
Proof. Let si be a ﬁxed segment in S. We shall analyze the expected number
of other segments that are cut when ℓ(si) is added by the algorithm as the next
splitting line.
In Figure 12.4 we can see that whether or not a segment sj is cut when ℓ(si)
is added—assuming it can be cut at all by ℓ(si)—depends on segments that are
also cut by ℓ(si) and are 'in between' si and sj. In particular, when the line
through such a segment is used before ℓ(si), then it shields s j from si. This is
si
dist= 0
dist= 1
dist= 2
what happened in Figure 12.4(b): the segment s1 shielded s3 from s2. These
considerations lead us to deﬁne the distance of a segment with respect to the
ﬁxed segment si:
distsi(sj) =



the number of segments intersecting
ℓ(si) in between si and s j
if ℓ(si) intersects sj
+∞
otherwise
265

Chapter 12
BINARY SPACE PARTITIONS
For any ﬁnite distance, there are at most two segments at that distance, one on
either side of si.
Let k := distsi(s j), and let sj1,s j2,...,sjk be the segments in between si
and sj. What is the probability that ℓ(si) cuts sj when added as a splitting line?
For this to happen, si must come before sj in the random ordering and, moreover,
it must come before any of the segments in between si and sj, which shield
sj from si. In other words, of the set {i, j, j1,..., jk} of indices, i must be the
smallest one. Because the order of the segments is random, this implies
Pr[ℓ(si) cuts s j] ⩽
1
distsi(sj)+2.
Notice that there can be segments that are not cut by ℓ(si) but whose extension
shields sj. This explains why the expression above is not an equality.
We can now bound the expected total number of cuts generated by si:
E[number of cuts generated by si]
⩽∑
j̸=i
1
distsi(s j)+2
⩽
2
n−2
∑
k=0
1
k +2
⩽
2lnn.
By linearity of expectation, we can conclude that the expected total number of
cuts generated by all segments is at most 2nlnn. Since we start with n segments,
the expected total number of fragments is bounded by n+2nlnn.
We have shown that the expected size of the BSP that is generated by
2DRANDOMBSP is n+2nlnn. As a consequence, we have proven that a BSP
of size n+2nlnn exists for any set of n segments. Furthermore, at least half of
all permutations lead to a BSP of size n+4nlnn. We can use this to ﬁnd a BSP
of that size: After running 2DRANDOMBSP we test the size of the tree, and if it
exceeds that bound, we simply start the algorithm again with a fresh random
permutation. The expected number of trials is two.
We have analyzed the size of the BSP that is produced by 2DRANDOMBSP.
What about the running time? Again, this depends on the random permutation
that is used, so we look at the expected running time. Computing the random
permutation takes linear time. If we ignore the time for the recursive calls,
then the time taken by algorithm 2DBSP is linear in the number of fragments
in S. This number is never larger than n—in fact, it gets smaller with each
recursive call. Finally, the number of recursive calls is obviously bounded by
the total number of generated fragments, which is O(nlogn). Hence, the total
construction time is O(n2 logn), and we get the following result.
Theorem 12.2 A BSP of size O(nlogn) can be computed in expected time
O(n2 logn).
266

Section 12.3
CONSTRUCTING A BSP TREE
Although the expected size of the BSP that is constructed by 2DRAN-
DOMBSP is fairly good, the running time of the algorithm is somewhat disap-
pointing. In many applications this is not so important, because the construction
is done off-line. Moreover, the construction time is only quadratic when the BSP
is very unbalanced, which is rather unlikely to occur in practice. Nevertheless,
from a theoretical point of view the construction time is disappointing. Using an
approach based on segment trees—see Chapter 10—this can be improved: one
can construct a BSP of size O(nlogn) in O(nlogn) time with a deterministic
algorithm. This approach does not give an auto-partition, however, and in
practice it produces BSPs that are slightly larger.
A natural question is whether the size of the BSP generated by 2DRAN-
DOMBSP can be improved. Would it for example be possible te devise an
algorithm that produces a BSP of size O(n) for any set of n disjoint segments
in the plane? The answer is no: there are sets of segments for which any BSP
must have size Ω(nlogn/loglogn). Note that the algorithm we presented does
not achieve this bound, so there may still be room for a slight improvement.
The algorithm we described for the planar case generalizes to 3-dimensional
space. Let S be a set of n non-intersecting triangles in R3. Again we restrict
ourselves to auto-partitions, that is, we only use partition planes containing a
triangle of S. For a triangle t we denote the plane containing it by h(t).
t
h(t)
Algorithm 3DBSP(S)
Input. A set S = {t1,t2,...,tn} of triangles in R3.
Output. A BSP tree for S.
1.
if card(S) ⩽1
2.
then Create a tree T consisting of a single leaf node, where the set S is
stored explicitly.
3.
return T
4.
else (∗Use h(t1) as the splitting plane. ∗)
5.
S+ ←{t ∩h(t1)+ : t ∈S};
T+ ←3DBSP(S+)
6.
S−←{t ∩h(t1)−: t ∈S};
T−←3DBSP(S−)
7.
Create a BSP tree T with root node ν, left subtree T−, right
subtree T+, and with S(ν) = {t ∈S : t ⊂h(t1)}.
8.
return T
The size of the resulting BSP again depends on the order of the triangles; some
orders give more fragments than others. As in the planar case, we can try to
get a good expected size by ﬁrst putting the triangles in a random order. This
usually gives a good result in practice. However, it is not known how to analyze
the expected behavior of this algorithm theoretically. Therefore we will analyze
a variant of the algorithm in the next section, although the algorithm described
above is probably superior in practice.
267

Chapter 12
BINARY SPACE PARTITIONS
12.4*
The Size of BSP Trees in 3-Space
The randomized algorithm for constructing a BSP tree in 3-space that we analyze
in this section is almost the same as the improved algorithm described above: it
treats the triangles in random order, and it makes free splits whenever possible.
A free split now occurs when a triangle of S splits a cell into two disconnected
free split
subcells. The only difference is that when we use some plane h(t) as a splitting
plane, we use it in all cells intersected by that plane, not just in the cells that
are intersected by t. (And therefore a simple recursive implementation is no
longer possible.) There is one exception to the rule that we split all cells with
h(t): when the split is completely useless for a cell, because all the triangles in
that cell lie completely to one side of it, then we do not split it.
Figure 12.5 illustrates this on a 2-dimensional example. In part (a) of
the ﬁgure, the subdivision is shown that is generated by the algorithm of the
previous section after treating segments s1, s2, and s3 (in that order). In part (b)
the subdivision is shown as generated by the modiﬁed algorithm. Note that the
modiﬁed algorithm uses ℓ(s2) as a splitting line in the subspace below ℓ(s1),
and that ℓ(s3) is used as a splitting line in the subspace to the right of ℓ(s2). The
line ℓ(s3) is not used in the subspace between ℓ(s1) and ℓ(s2), however, because
it is useless there.
Figure 12.5
The original and the modiﬁed algorithm
(a)
s1
s2
s3
s1
s2
s3
(b)
The modiﬁed algorithm can be summarized as follows. Working out the
details is left as an exercise.
Algorithm 3DRANDOMBSP2(S)
Input. A set S = {t1,t2,...,tn} of triangles in R3.
Output. A BSP tree for S.
1.
Generate a random permutation t1,...,tn of the set S.
2.
for i ←1 to n
3.
do Use h(ti) to split every cell where the split is useful.
4.
Make all possible free splits.
The next lemma analyzes the expected number of fragments generated by the
algorithm.
Lemma 12.3 The expected number of object fragments generated by algorithm
3DRANDOMBSP2 over all n! possible permutations is O(n2).
Proof. We shall prove a bound on the expected number of fragments into
which a ﬁxed triangle tk ∈S is cut. For a triangle ti with i < k we deﬁne
268

Section 12.4*
THE SIZE OF BSP TREES IN 3-SPACE
ℓi := h(ti) ∩h(tk). The set L := {ℓ1,...,ℓk−1} is a set of at most k −1 lines
lying in the plane h(tk). Some of these lines intersect tk, others miss tk. For
ℓ1
ℓ2
ℓ3
ℓ4
tk
a line ℓi that intersects tk we deﬁne si := ℓi ∩tk. Let I be the set of all such
intersections si. Due to free splits the number of fragments into which tk is cut
is in general not simply the number of faces in the arrangement that I induces
on tk. To understand this, consider the moment that tk−1 is treated. Assume that
ℓk−1 intersects tk; otherwise tk−1 does not cause any fragmentation on tk. The
segment sk−1 can intersect several of the faces of the arrangement on tk induced
by I \{sk}. If, however, such a face f is not incident to the one of the edges of
tk—we call f an interior face—then a free split already has been made through
this part of tk. In other words, h(tk−1) only causes cuts in exterior faces, that
is, faces that are incident to one of the three edges of tk. Hence, the number of
splits on tk caused by h(tk−1) equals the number of edges that sk−1 contributes
to exterior faces of the arrangement on tk induced by I. (In the analysis that
follows, it is important that the collection of exterior faces is independent of
the order in which t1,...,tk−1 have been treated. This is not the case for the
algorithm in the previous section, which is the reason for the modiﬁcation.)
What is the expected number of such edges? To answer this question we ﬁrst
bound the total number of edges of the exterior faces.
tk
sk−1
In Chapter 8 we deﬁned the zone of a line ℓin an arrangement of lines in
the plane as the set of faces of the arrangement intersected by ℓ. You may recall
that for an arrangement of m lines the complexity of the zone is O(m). Now let
e1, e2, and e3 be the edges of tk and let ℓ(ei) be the line through ei, for i = 1,2,3.
The edges that we are interested in must be in the zone of either ℓ(e1), ℓ(e2),
or ℓ(e3) in the arrangement induced by the set L on the plane h(tk). Hence, the
total number of edges of exterior faces is O(k).
If the total number of edges of exterior faces is O(k), then the average
number of edges lying on a segment si is O(1). Because t1,...,tn is a random
permutation, so is t1,...,tk−1. Hence, the expected number of edges on segment
sk−1 is constant, and therefore the expected number of extra fragments on tk
caused by h(tk−1) is O(1). The same argument shows that the expected number
of fragmentations on tk generated by each of the splitting planes h(t1) through
h(tk−2) is constant. This implies that the expected number of fragments into
which tk is cut is O(k). The total number of fragments is therefore
O(
n
∑
k=1
k) = O(n2).
The quadratic bound on the expected size of the partitioning generated by
3DRANDOMBSP immediately proves that a BSP tree of quadratic size exists.
You may be a bit disappointed by the bound that we have achieved. A quadratic
size BSP tree is not what you are hoping for when you have a set of 10,000
triangles. The following theorem tells us that we cannot hope to prove anything
better if we restrict ourselves to auto-partitions.
Lemma 12.4 There are sets of n non-intersecting triangles in 3-space for which
any auto-partition has size Ω(n2).
269

Chapter 12
BINARY SPACE PARTITIONS
Proof. Consider a collection of rectangles consisting of a set R1 of rectangles
parallel to the xy-plane and a set R2 of rectangles parallel to the yz-plane, as
illustrated in the margin. (The example also works with a set of triangles,
R1
R2
but with rectangles it is easier to visualize.) Let n1 := card(R1), let n2 :=
card(R2), and let G(n1,n2) be the minimum size of an auto-partition for such
a conﬁguration. We claim that G(n1,n2) = (n1 +1)(n2 +1)−1. The proof is
by induction on n1 +n2. The claim is obviously true for G(1,0) and G(0,1), so
now consider the case where n1 +n2 > 1. Without loss of generality, assume
that the auto-partition chooses a rectangle r from the set R1. The plane through
r will split all the rectangles in R2. Moreover, the conﬁgurations in the two
subscenes that must be treated recursively have exactly the same form as the
initial conﬁguration. If m denotes the number of rectangles of R1 lying above r,
then we have
G(n1,n2) = 1+G(m,n2)+G(n1 −m−1,n2)
= 1+((m+1)(n2 +1)−1)+((n1 −m)(n2 +1)−1)
= (n1 +1)(n2 +1)−1.
So perhaps we should not restrict ourselves to auto-partitions. In the lower
bound in the proof of Lemma 12.4 the restriction to auto-partitions is deﬁnitely
a bad idea: we have shown that such a partition necessarily has quadratic size,
Figure 12.6
The general lower bound construction
whereas we can easily get a linear BSP if we ﬁrst separate the set R1 from the
set R2 with a plane parallel to the xz-plane. But even unrestricted partitions fail
to give a small BSP for the conﬁguration of Figure 12.6. This conﬁguration is
obtained as follows. We start by taking a grid in the plane made up of n/2 lines
parallel to the x-axis and n/2 lines parallel to the y-axis. (Instead of the lines we
could also take very long and skinny triangles.) We skew these lines a little to
get the conﬁguration of Figure 12.6; the lines now lie on a so-called hyperbolic
paraboloid. Finally we move the lines parallel to the y-axis slightly upward so
that the lines no longer intersect. What we get is the set of lines
{y = i, z = ix : 1 ⩽i ⩽n/2}∪{x = i, z = iy+ε : 1 ⩽i ⩽n/2},
where ε is a small positive constant. If ε is sufﬁciently small then any BSP
must cut at least one of the four lines that bound a grid cell in the immediate
270

Section 12.5
BSP TREES FOR LOW-DENSITY
SCENES
neighborhood of that cell. The formal proof of this fact is elementary, but
tedious and not very instructive. The idea is to show that the lines are skewed in
such a way that no plane ﬁts simultaneously through the four "openings" at its
corners. Since there is a quadratic number of grid cells, this will result in Θ(n2)
fragments.
Theorem 12.5 For any set of n non-intersecting triangles in R3 a BSP tree of
size O(n2) exists. Moreover, there are conﬁgurations for which the size of any
BSP is Ω(n2).
12.5
BSP Trees for Low-Density Scenes
In the previous section, we described an algorithm that constructs a BSP tree for
a set of n disjoint triangles in R3. It always produces a BSP tree of size O(n2).
We also gave an example of a set of n triangles for which any BSP tree has
size Ω(n2). Hence, the O(n2) upper bound is tight in the worst case, and from a
theoretical point of view the problem seems to be solved. The tight quadratic
bound might give you the idea that BSP trees are useless in practice. Fortunately
this is not the case: in many practical situations, BSP trees perform just ﬁne.
Evidently, the theoretical analysis fails to predict the practical performance of
BSP trees.
This is disturbing: based on our theoretical analysis we might have discarded
a structure that is quite useful in practice. The problem is that certain inputs—
the grid-like lower-bound construction, for example—force a BSP to cut may
objects, while other inputs—the ones that usually occur in practice—admit
a BSP that cuts only few objects. We would like our analysis to reﬂect this:
it should give different bounds for different types of input. This means that
we can no longer do the analysis in terms of the input size, n, only. We must
introduce another parameter, which distinguishes easy inputs from difﬁcult ones.
What are easy inputs? Intuitively, easy inputs are inputs where the objects are
relatively well separated, whereas difﬁcult inputs have many objects packed
closely together. Note that whether or not objects are close to each other is not
a matter of the absolute distance between them, but of the distance relative to
their size; otherwise, scaling the whole scene would lead to different results,
which is undesirable. Therefore we deﬁne our parameter, which we will call the
density of a scene, as follows.
Let diam(o) denote the diameter of an object o. The density of a set S of
objects in Rd is deﬁned as the smallest number λ such that the following holds:
any ball B intersects at most λ objects o ∈S such that diam(o) ⩾diam(B).
Figure 12.7 illustrates this deﬁnition. Note that the deﬁnition speaks of any
ball: B is not an object in S, but a ball of arbitrary radius whose center can be
anywhere in space.
It is easy to come up with a set of n objects whose density is n: any set of
lines will do. When the objects are bounded, the density can still be high. The
grid-like construction of Figure 12.6, for example, has density Θ(n) even if the
objects in the construction are line segments rather than full lines. On the other
271

Chapter 12
BINARY SPACE PARTITIONS
Figure 12.7
A set of eight segments with density 3.
The disc B intersects ﬁve segments, but
two of them have diameter less than
diam(B) and so they are not counted
B
hand, the density can also be quite low: a set of n unit balls such that any two
balls are more than unit distance apart has density 1. In fact, one can prove that
any set of n disjoint balls has density Θ(1), even if they have vastly different
sizes—see Exercise 12.13.
Let's see where we stand. We have deﬁned a parameter, density, that captures
the difﬁculty of a scene in the following sense: if the density is low then the
objects are reasonably well separated, and if the density is high then there are
regions with many objects close together. Next we want to show that if the
density is low—a constant independent of n, for instance—then we can ﬁnd a
small BSP. One possibility could be to analyze the randomized algorithm of the
previous section more carefully, and show that it produces a small BSP if the
density of the input set is low. Unfortunately, this is not the case: even for inputs
of low density, it may produce a BSP whose expected size is quadratic. In other
words, the algorithm fails to always take advantage of the situation when the
input scene is easy. We need a new algorithm.
Let S be a set of objects in R2—S can contain segments, discs, triangles, etc.—
and let λ be the density of S. (The algorithm presented below also works in
R3 and, in fact, even in higher dimensions. For simplicity we shall conﬁne
ourselves to R2 from now on.) The idea behind the algorithm is to deﬁne, for
each object o ∈S, a small set of points—we call them guards—such that the
distribution of the guards is representative of the distribution of the objects, and
then to let the construction of the BSP be guided by the guards. We shall now
make this idea precise.
Let bb(o) denote the bounding box of o, that is, bb(o) is the smallest axis-
aligned rectangle that contains o. The guards that we deﬁne for o are simply
the four vertices of bb(o). Let G(S) be the multiset of 4n guards deﬁned for the
objects in S. (G(S) is a multiset because bounding-box vertices can coincide.
When this happens, we want those guards to be put multiple times into G(S),
once for each object of which they are a bounding-box vertex.) When S has
low density, the guards in G(S) are representative of the distribution of S in
the following sense: for any square σ, the number of objects intersecting σ is
not much more than the number of guards inside σ. The next lemma makes
this precise. Note that this lemma gives only an upper bound on the number of
objects intersecting a square, not a lower bound: it is very well possible that
a square contains many guards without intersecting a single object. Note also
that even though the deﬁnition of density (in 2D) uses discs, the property of the
guards given in the following lemma is with respect to squares.
272

Section 12.5
BSP TREES FOR LOW-DENSITY
SCENES
Lemma 12.6 Any axis-parallel square that contains k guards from G(S) in its
interior intersects at most k +4λ objects from S.
Proof. Let σ be an axis-parallel square with k guards in its interior. Obviously
there are at most k objects that have a guard (that is, a bounding-box vertex)
inside σ. Deﬁne S′ to be the set of the remaining objects from S, that is, the
ones without a guard inside σ. Clearly, the density of S′ is at most λ. We have
to show that at most 4λ objects from S′ can intersect σ.
σ
x
y
Figure 12.8
The square σ intersects a segment but
does not contain a guard. Hence, the
diameter of the segment is at least the
edge length of σ
If an object o ∈S′ intersects σ then, obviously, bb(o) intersects σ as well.
By the deﬁnition of S′, the square σ does not contain a vertex of bb(o) in its
interior. But then the projection of bb(o) onto the x-axis contains the projection
of σ onto the x-axis, or the projection of bb(o) onto the y-axis contains the
projection of σ onto the y-axis (or both)—see Figure 12.8. This implies that the
diameter of o is at least the side length of σ, and so diam(o) ⩾diam(σ)/
√
2.
Now cover σ with four discs D1,...,D4 of diameter diam(σ)/2. The object o
intersects at least one of these discs, Di. We charge o to Di. We have
diam(o) ⩾diam(σ)/
√
2 > diam(σ)/2 = diam(Di).
Since S′ has density at most λ, each Di is charged at most λ times. Hence, σ
intersects at most 4λ objects from S′. Adding the at most k objects not in S′, we
ﬁnd that σ intersects at most k +4λ objects from S.
Lemma 12.6 suggests the following two-phase algorithm to construct a BSP.
Let U be a square that contains all objects from S in its interior.
In the ﬁrst phase, we recursively subdivide U into squares until each square
contains at most one guard in its interior. In other words, we construct a quadtree
on G(s)—see Chapter 14. Partitioning a square into its four quadrants can be
done by ﬁrst splitting the square into two equal halves with a vertical line and
then splitting each half with a horizontal line. Hence, the quadtree subdivision
gives us a BSP tree on the set G(S). Figure 12.9 illustrates this. Notice that
some splitting lines, ℓ2 and ℓ3 for instance, are in fact the same line; what differs
is which portion of the line is relevant, but this information is not stored with
the nodes in the BSP tree. By Lemma 12.6, each leaf region in the resulting
subdivision intersects only a few objects—at most 1+4λ, to be precise. The
second phase of the algorithm then partitions each leaf region further, until all
objects are separated. How the second phase is done exactly depends on the
273

Chapter 12
BINARY SPACE PARTITIONS
Figure 12.9
A quadtree subdivision and the
corresponding BSP tree.
ℓ2
ℓ3
ℓ1
ℓ4
ℓ5
ℓ6
ℓ7
ℓ9
ℓ8
ℓ2
ℓ1
ℓ3
ℓ9
ℓ7
ℓ8
ℓ4
ℓ5
ℓ6
type of objects in S. If the objects are line segments, for example, we can apply
Algorithm 2DRANDOMBSP given in Section 12.3 to the segment fragments in
each leaf region.
The crucial property of the algorithm we just sketched is that for low-density
scenes the ﬁrst phase produces leaf regions that intersect few objects. Unfortu-
nately, there is one problem: the number of leaf regions can be very large. This
happens for example when two guards lie very close together near a corner of
the initial square U. Therefore we modify the ﬁrst phase of the algorithm to
guarantee it produces a linear number of leaf regions. This is done as follows.
The ﬁrst modiﬁcation is that we do not continue subdividing until each
region has only one guard in it, but instead we stop when the region contains k
or fewer guards, for some suitable parameter k ⩾1. The reason for this and the
choice of k will be discussed later.
The second modiﬁcation is the following. Suppose that in the recursive
subdivision procedure we have to subdivide a square σ. Consider the four
quadrants of σ. If at least two of them contain more than k guards in their
interior, then we proceed as before by applying a quadtree split: we partition
σ into its four quadrants by ﬁrst splitting it with a vertical line ℓv(σ) and
then splitting each half with a horizontal line ℓh(v)—see Figure 12.10. After
applying the quadtree split, we recurse on the quadrants. If none of the quadrants
Figure 12.10
Examples of a quadtree split and a
shrinking step with k = 4
quadtree split
shrinking step
ℓv(σ)
ℓh(σ)
ℓv(σ)
ℓh(σ)
σ′
contains more than k guards, we also perform a quadtree split; in this case the
four quadrants all become leaf regions. If there is exactly one quadrant, say σ′,
with more than k guards in it we have to be careful: all guards could be very
close to a corner, and then it may take many quadtree splits before we ﬁnally
separate them—see also Lemma 14.1. Therefore we perform a shrinking step.
Intuitively, we shrink σ′ until at least k guards are not in the interior of σ′. More
274

Section 12.5
BSP TREES FOR LOW-DENSITY
SCENES
precisely, a shrinking step proceeds as follows. Assume that σ′ is the north-west
quadrant of σ; the other three cases are handled in a symmetrical fashion. We
shrink σ′ by moving its bottom-right corner diagonally to the north-west—σ′
thus remains a square during the shrinking process—until at least k guards are
outside the interior of σ′. With a slight abuse of notation, we use σ′ to denote
the shrunk quadrant. Notice that σ′ has at least one guard on its boundary. We
partition σ by ﬁrst splitting it with a vertical line ℓv(σ) through the right edge of
σ′, and then splitting the two resulting parts with a horizontal line ℓh(σ) through
the bottom edge of σ′—see Figure 12.10. This partitions σ into four regions,
two of which are squares. In particular σ′, the only region that can contain more
than k guards and therefore may have to be split further, is a square.
Algorithm PHASE1 summarizes the recursive splitting procedure.
Algorithm PHASE1(σ,G,k)
Input. A region σ, a set G of guards in the interior of σ, and an integer k ⩾1.
Output. A BSP tree T such that each leaf region contains at most k guards.
1.
if card(G) ⩽k
2.
then Create a BSP tree T consisting of a single leaf node.
3.
else if exactly one quadrant of σ contains more than k guards in its
interior
4.
then Determine the splitting lines ℓv(σ) and ℓh(σ) for a shrink-
ing step, as explained above.
5.
else Determine the splitting lines ℓv(σ) and ℓh(σ) for a quadtree
split, as explained above.
6.
Create a BSP tree T with three internal nodes; the root of T stores
ℓv(σ) as its splitting line, and both children of the root store ℓh(σ)
as their splitting line.
7.
Replace each leaf µ of T by a BSP tree Tµ computed recursively
on the region corresponding to µ and the guards inside that region.
8.
return T
Lemma 12.7 PHASE1(U,G(S),k) produces a BSP tree with O(n/k) leaves,
where each leaf region intersects at most k +4λ objects.
Proof. We ﬁrst prove the bound on the number of leaves. This number is one
more than the number of internal nodes, and so it sufﬁces to bound the latter
number.
Let N(m) denote the maximum number of internal nodes in a BSP tree cre-
ated by PHASE1(σ,G,k) when card(G) = m. If m ⩽k, no splits are performed,
and so N(m) = 0 in this case. Otherwise, a quadtree split or a shrinking step
is applied to σ. This results in three internal nodes, and four regions in which
we recurse. Let m1,...,m4 denote the numbers of guards in the four regions,
and let I := {i : 1 ⩽i ⩽4 and mi > k}. Since a region that contains k or fewer
guards is a leaf region, we know that N(mi) = 0 for i ̸∈I. Hence,
N(m) ⩽
$
0
if m ⩽k
3+∑i∈I N(mi)
otherwise
We will prove by induction that N(m) ⩽max(0,(6m/k)−3). This is obviously
true for m ⩽k, and so we now assume that m > k. A guard can be in the interior
275

Chapter 12
BINARY SPACE PARTITIONS
of at most one region, which means that ∑i∈I mi ⩽m. If at least two quadrants
of σ contain more than k guards, then card(I) ⩾2 and we have
N(m) ⩽3+∑
i∈I
N(mi) ⩽3+
%
∑
i∈I
(6mi/k)
&
−card(I)·3 ⩽6m/k −3,
as claimed. If none of the quadrants contains more than k guards, then the
four regions are all leaf regions and N(m) = 3. Together with the assumption
m > k, this implies N(m) ⩽(6m/k)−3. The remaining case is where exactly
one quadrant contains more than k guards. In this case we do a shrinking step.
Because of the way a shrinking step is performed, a shrunk quadrant contains
fewer than m−k guards and the other resulting regions contain at most k guards.
Hence, in this case we have
N(m) ⩽3+N(m−k) ⩽3+(6(m−k)/k −3) ⩽6m/k −3.
So in all cases N(m) ⩽(6m/k)−3, as claimed. This proves the bound on the
number of internal nodes.
σ′
σ′′
It remains to prove that each leaf region intersects at most k +4λ objects. By
construction, a leaf region contains at most k guards in its interior. Hence, if
a leaf region is a square, Lemma 12.6 implies that it intersects k +4λ objects.
There can also be leaf regions, however, that are not square, and then we
cannot apply Lemma 12.6 directly. A non-square leaf region σ′′ must have
been produced in a shrinking step, as shown in Figure 12.10. Recall that the
shrinking process stops as soon as k or more guards are not in the interior of the
shrunk quadrant σ′. At least one of these guards must be on the boundary of
σ′, and in fact the number of guards exterior to σ′ (not counting the guards on
the boundary of σ′) must be less than k. This implies that σ′′ can be covered
by a square with at most k guards in its interior; this square is shown in gray in
the ﬁgure in the margin. By Lemma 12.6, this square intersects at most k +4λ
objects, and so σ′′ intersects at most that many objects as well.
Lemma 12.7 explains why it can be advantageous to use a value for k that is
larger than 1: the larger k is, the fewer leaf regions we will have. On the other
hand, a larger k will also mean more objects per leaf region. A good choice of
k is therefore one that reduces the number of leaf regions as much as possible,
without signiﬁcantly increasing the number of objects per leaf region. Setting
k := λ will do this: the number of leaf regions will decrease by a factor λ (as
compared with k = 1), while the maximum number of objects per leaf region
does not increase asymptotically—it only goes from 1+4λ to 5λ.
There is one problem, however: we do not know λ, the density of the input
scene, and so we cannot use it as a parameter in the algorithm. Therefore we
use the following trick. We guess a small value for λ, say λ = 2. Then we
run PHASE1 with our guess as the value of k, and we check whether each leaf
region in the resulting BSP tree intersects at most 5k objects. If so, we proceed
with the second phase of the algorithm; otherwise, we double our guess and try
again. This leads to the following algorithm.
276

Section 12.5
BSP TREES FOR LOW-DENSITY
SCENES
Algorithm LOWDENSITYBSP2D(S)
Input. A set S of n objects in the plane.
Output. A BSP tree T for S.
1.
Let G(S) be the set of 4n bounding-box vertices of the objects in S.
2.
k ←1; done ←false; U ←a bounding square of S
3.
while not done
4.
do k ←2k; T ←PHASE1(U,G(S),k); done ←true
5.
for each leaf µ of T
6.
do Compute the set S(µ) of object fragments in the region of µ.
7.
if card(S(µ)) > 5k then done ←false
8.
for each leaf µ of T
9.
do Compute a BSP tree Tµ for S(µ) and replace µ by Tµ.
10.
return T
When the input S consists of non-intersecting segments in the plane, we can use
2DRANDOMBSP to compute the BSPs in line 9, leading to the following result.
Theorem 12.8 For any set S of n disjoint line segments in the plane, there is a
BSP of size O(nlogλ), where λ is the density of S.
Proof. By Lemma 12.7, PHASE1(U,G(S),k) results in a BSP tree where each
leaf region intersects at most k + 4λ objects. Hence, the test in line 7 of
LOWDENSITYBSP2D is guaranteed to be false if k ⩾λ. (If k < λ, the test
may or may not be false.) The while-loop therefore ends, at the latest, when k
becomes larger than λ for the ﬁrst time. Since k doubles every time, this implies
k ⩽2λ when we get to the second phase in line 8.
Let k∗denote the value of k when we get to line 8. We have just argued that
k∗⩽2λ. The test in line 7 guarantees that each leaf region intersects at most 5k∗
segments. Hence, according to Lemma 12.1, each tree Tµ has (expected) size
O(k∗logk∗) when 2DRANDOMBSP is used in line 9. Because there are O(n/k∗)
leaf regions, the total size of the BSP tree is O(nlogk∗). Since k∗⩽2λ, this
proves the theorem.
The bound in Theorem 12.8 is never worse than O(nlogn). In other words,
the algorithm described above is as good as the algorithm given in Section 12.3
in the worst case, but it provably proﬁts when the density of the input is low.
Recall that the reason for introducing the concept of density was the quadratic
worst-case bound for BSPs for triangles in R3. The algorithm that we have just
described works very well for segments in the plane: it produces a BSP whose
size is O(nlogn) in the worst case and O(n) when the density of the input is
a constant. What happens when we apply this approach to a set of triangles
in R3? As it turns out, it also leads to good results in this case, as stated in the
next theorem. (Exercise 12.18 asks you to prove this theorem.)
Theorem 12.9 For any set S of n disjoint triangles in R3, there is a BSP of size
O(nλ), where λ is the density of S.
277

Chapter 12
BINARY SPACE PARTITIONS
The bound in Theorem 12.9 interpolates nicely between O(n) and O(n2)
as λ varies from 1 to n. Therefore the algorithm produces a BSP whose size
is optimal in the worst case. But the result is even stronger: the O(nλ) bound
is optimal for all values of λ: for any n and any λ with 1 ⩽λ ⩽n there is a
collection of n triangles in R3 whose density is λ and for which any BSP must
have size Ω(nλ).
12.6
Notes and Comments
BSP trees are popular in many application areas, in particular in computer
graphics. The application mentioned in this chapter is to performing hidden-
surface removal with the painter's algorithm [185]. Other applications include
shadow generation [124], set operations on polyhedra [292, 370], and visibility
preprocessing for interactive walkthroughs [369]. BSP trees have also been used
in cell decomposition methods in motion planning [36], for range searching [60],
and as general indexing structure in GIS [294]. Two other well-known structures,
kd-trees and quadtrees, are in fact special cases of BSP trees, where only
orthogonal splitting planes are used. Kd-trees were discussed extensively in
Chapter 5 and quadtrees will be discussed in Chapter 14.
The study of BSP trees from a theoretical point of view was initiated by Pa-
terson and Yao [317]; the results in Sections 12.3 and 12.4 come from their
paper. They also proved bounds on BSPs in higher dimensions: any set of
(d −1)-dimensional simplices in Rd, with d ⩾3, admits a BSP of size O(nd−1).
Paterson and Yao also obtained results for orthogonal objects in higher dimen-
sions [318]. For instance, they proved that any set of orthogonal rectangles in
R3 admits a BSP of size O(n√n), and that this bound is tight in the worst case.
Below we discuss several of the results that have been obtained since. A more
extensive overview has been given by T´oth [373].
For a long time it was unknown whether any set of n disjoint line segments
in the plane would admit a BSP of size O(n), but T´oth [372] proved that this
is not the case, by constructing a set of segments for which any BSP must
have size Ω(nlogn/loglogn). Note that there is still a small gap between this
lower bound and the currently known upper bound, which is O(nlogn). There
are several special cases, however, where an O(n) size BSP is possible. For
example, Paterson and Yao [317] have shown that any set of n disjoint segments
in the plane that are all either horizontal or vertical admit a BSP of size O(n).
The same result was achieved by d'Amore and Franciosa [138]. T´oth [371]
generalized this result to segments with a limited number of orientations. Other
special cases where a linear-size BSP is always possible are for line segments
with more or less the same length [54] and, as we have already seen in this
chapter, for sets of objects of constant density.
In Section 12.5, we studied BSPs for low-density scenes. This was inspired by
the observation that the worst-case size of BSPs for 3-dimensional scenes has
little to do with their practical performance. A similar situation arises frequently
278

Section 12.7
EXERCISES
in the study of geometric algorithms: often one can come up with input sets
for which the algorithm at hand is not very efﬁcient, but in many cases such
input sets are not very realistic. This can have two disadvantages. First, a worst-
case analysis of the algorithm may not be very informative as to whether the
algorithm is useful in practice. Second, since algorithms are typically designed
to have the best worst-case performance, they may be geared towards handling
situations that will not arise in practice and therefore they may be needlessly
complicated. The underlying reason for this is that, usually, the running time
of a geometric algorithm not only is determined by the size of the input but
also is strongly inﬂuenced by the shape of the input objects and their spatial
distribution. To overcome this problem, one can try to deﬁne a parameter that
captures the geometry of the input—just as we did in Section 12.5.
The parameter that has been used most often in this context is fatness. A
triangle is called β-fat if all its angles are at least β. It has been shown that
the complexity of the union of n intersecting β-fat triangles in the plane is
near-linear in n if β is a constant [268]; currently, the best known bound is
O((1/β)log(1/β) · nloglogn) [314]. The concept of fatness has been gener-
alized to arbitrary convex objects, and even to non-convex objects. One of
the most general deﬁnitions was given by van der Stappen [362], who deﬁned
an object o in Rd to be β-fat if the following holds: for any ball B whose
center lies in o and that does not fully contain o in its interior, we have that
vol(o ∩B) ⩾β · vol(B), where vol(·) denotes the volume. There are many
problems that can be solved more efﬁciently for fat objects than for general
objects. Examples are range searching and point location [51, 60], motion
planning [363], hidden-surface removal [229], ray shooting [21, 49, 53, 228],
and computing depth orders [53, 228].
The parameter used in Section 12.5, density, has also been studied a lot. It
can be shown that any set of disjoint β-fat objects has density O(1/β) [55, 362],
and so any result obtained for low-density scenes immediately gives a result
for disjoint fat objects. The algorithm for constructing a BSP for low-density
scenes described in Section 12.5 is a modiﬁed and slightly improved version of
the construction by de Berg [51]. Some of the results mentioned above for fat
objects are in fact based on this construction [60, 363] and hence also apply to
low-density scenes.
12.7
Exercises
12.1
Prove that PAINTERSALGORITHM is correct. That is, prove that if (some
part of) an object A is scan-converted before (some part of) object B is
scan-converted, then A cannot lie in front of B.
12.2
Let S be a set of m polygons in the plane with n vertices in total. Let
T be a BSP tree for S of size k. Prove that the total complexity of the
fragments generated by the BSP is O(n+k).
279

Chapter 12
BINARY SPACE PARTITIONS
12.3
Give an example of a set of line segments in the plane where the greedy
method of constructing an auto-partition (where the splitting line ℓ(s) is
taken that induces the least number of cuts) results in a BSP of quadratic
size.
12.4
Give an example of a set S of n non-intersecting line segments in the
plane for which a BSP tree of size n exists, whereas any auto-partition
of S has size at least ⌊4n/3⌋.
12.5
Give an example of a set S of n disjoint line segments in the plane such
that any auto-partition for S has depth Ω(n).
12.6
We have shown that the expected size of the partitioning produced by
2DRANDOMBSP is O(nlogn). What is the worst-case size?
12.7
Suppose we apply 2DRANDOMBSP to a set of intersecting line segments
in the plane. Can you say anything about the expected size of the
resulting BSP tree?
12.8
In 3DRANDOMBSP2, it is not described how to ﬁnd the cells that
must be split when a splitting plane is added, nor is it described how
to perform the split efﬁciently. Work out the details for this step, and
analyze the running time of your algorithm.
12.9
Give a deterministic divide-and-conquer algorithm that constructs a BSP
tree of size O(nlogn) for a set of n line segments in the plane. Hint: Use
as many free splits as possible and use vertical splitting lines otherwise.
12.10 Let C be a set of n disjoint unit discs—discs of radius 1—in the plane.
Show that there is a BSP of size O(n) for C. Hint: Start by using a
suitable collection of vertical lines of the form x = 2i for some integer i.
12.11 BSP trees can be used for a variety of tasks. Suppose we have a BSP on
the edges of a planar subdivision.
a. Give an algorithm that uses the BSP tree to perform point location on
the subdivision. What is the worst-case query time?
b. Give an algorithm that uses the BSP tree to report all the faces of the
subdivision intersected by a query segment. What is the worst-case
query time?
c. Give an algorithm that uses the BSP tree to report all the faces of the
subdivision intersected by an axis-parallel query rectangle. What is
the worst-case query time?
12.12 In Chapter 5 kd-trees were introduced. Kd-trees can also store segments
instead of points, in which case they are in fact a special type of BSP
tree, where the splitting lines for nodes at even depth in the tree are
horizontal and the splitting lines at odd levels are vertical.
a. Discuss the advantages and/or disadvantages of BSP trees over kd-
trees.
280

Section 12.7
EXERCISES
b. For any set of two non-intersecting line segments in the plane there
exists a BSP tree of size 2. Prove that there is no constant c such that
for any set of two non-intersecting line segments there exists a kd-tree
of size at most c.
12.13 Prove that the density of any set of disjoint discs in the plane is at most 9.
(Thus the dependency is independent of the number of discs.) Use this
to show that any set of n disjoint discs in the plane has a BSP of size
O(n). Generalize the result to higher dimensions.
12.14 A triangle is called α-fat if all its angles are at least α. Prove that any
set of α-fat disjoint triangles in the plane has density O(1/α). Use
this to show that any set of n disjoint α-fat triangles admits a BSP of
size O(nlog(1/α)), and argue that this implies that any set of n disjoint
squares admits a BSP of size O(n).
12.15 Give an example of a set of n triangles whose density is a constant
such that any auto-partition—that is, any BSP that only uses splitting
planes containing input triangles—has size Ω(n2). (This example shows
that the randomized algorithm of Section 12.4 can produce a BSP of
expected size Ω(n2) even when the density of the input set of triangles is
a constant.) Hint:Use a set of triangles that are all parallel to the z-axis
and whose projections onto the xy-plane form a grid.
12.16 Let T be a set of disjoint triangles in the plane. Instead of taking the
bounding-box vertices of the triangles as guards, we could also take the
vertices of the triangles themselves. Show that this is not a good set
of guards by giving a counterexample to Lemma 12.6 if the guards are
deﬁned in this way. Can you give a counterexample for the case where
all triangles are equilateral triangles?
12.17 Show that Algorithm LOWDENSITYBSP2D can be implemented so that
it runs in O(n2) time.
12.18 Generalize Algorithm LOWDENSITYBSP2D to R3, and analyze the size
of the resulting BSP.
281

13
Robot Motion Planning
Getting Where You Want to Be
One of the ultimate goals in robotics is to design autonomous robots: robots
that you can tell what to do without having to say how to do it. Among other
things, this means a robot has to be able to plan its own motion.
To be able to plan a motion, a robot must have some knowledge about the
environment in which it is moving. For example, a mobile robot moving around
in a factory must know where obstacles are located. Some of this information—
where walls and machines are located—can be provided by a ﬂoor plan. For
other information the robot will have to rely on its sensors. It should be able to
detect obstacles that are not on the ﬂoor plan—people, for instance. Using the
information about the environment, the robot has to move to its goal position
without colliding with any of the obstacles.
This motion planning problem has to be solved whenever any kind of robot
wants to move in physical space. The description above assumed that we have
an autonomous robot moving around in some factory environment. That kind of
robot is still quite rare compared to the robot arms that are now widely employed
in industry.
A robot arm, or articulated robot, consists of a number of links, connected
by joints. Normally, one end of the arm—its base—is ﬁrmly connected to the
ground, while the other end carries a hand or some kind of tool. The number
of links varies between three to six or even more. The joints are usually of two
types, the revolute joint type that allows the links to rotate around the joint,
much like an elbow, or the prismatic joint type that allows one of the links to
slide in and out. Robot arms are mostly used to assemble or manipulate parts
of an object, or to perform tasks like welding or spraying. To do this, they
must be able to move from one position to another, without colliding with the
environment, the object they are operating on, or—an interesting complication—
with themselves.
In this chapter we introduce some of the basic notions and techniques used in
motion planning. The general motion planning problem is quite difﬁcult, and
we shall make some simplifying assumptions.
The most drastic simpliﬁcation is that we will look at a 2-dimensional motion
planning problem. The environment will be a planar region with polygonal
283

Chapter 13
ROBOT MOTION PLANNING
obstacles, and the robot itself will also be polygonal. We also assume that the
environment is static—there are no people walking in the way of our robot—and
known to the robot. The restriction to planar robots is not as severe as it appears
at ﬁrst sight: for a robot moving around on a work ﬂoor, a ﬂoor plan showing
the location of walls, tables, machines, and so on, is often sufﬁcient to plan a
motion.
The types of motions a robot can execute depend on its mechanics. Some
robots can move in any direction, while others are constrained in their motions.
Car-like robots, for instance, cannot move sideways—otherwise parallel parking
would be less challenging. In addition, they often have a certain minimum turn-
ing radius. The geometry of the motions of car-like robots is quite complicated,
so we will restrict ourselves to robots that can move in arbitrary directions.
In fact, we will mainly look at robots that can translate only; at the end of
the chapter we'll brieﬂy consider the case of robots that can also change their
orientation by rotation.
13.1
Work Space and Conﬁguration Space
R(0,0)
R(6,4)
reference point
5
10
10
5
Let R be a robot moving around in a 2-dimensional environment, or work
space, consisting of a set S = {P1,...,Pt} of obstacles. We assume that R
is a simple polygon. A placement, or conﬁguration, of the robot can now be
speciﬁed by a translation vector. We denote the robot translated over a vector
(x,y) by R(x,y). For instance, if the robot is the polygon with vertices (1,−1),
(1,1), (0,3), (−1,1), and (−1,−1), then the vertices of R(6,4) are (7,3), (7,5),
(6,7), (5,5), and (5,3). With this notation, a robot can be speciﬁed by listing
the vertices of R(0,0).
An alternative way to view this is in terms of a reference point. This is most
intuitive if the origin (0,0) lies in the interior of R(0,0). By deﬁnition, this
point is then called the reference point of the robot. We can specify a placement
of R by simply stating the coordinates of the reference point if the robot is
in the given placement. Thus R(x,y) speciﬁes that the robot is placed with
its reference point at (x,y). In general, the reference point does not have to
be inside the robot; it can also be a point outside the robot, which we might
imagine to be attached to the robot by an invisible stick. By deﬁnition, this point
is at the origin for R(0,0).
Now suppose the robot can change its orientation by rotation, say around its
R(6,4,45)
5
10
10
5
45◦
R(0,0,0)
reference point. We then need an extra parameter, φ, to specify the orientation
of the robot. We let R(x,y,φ) denote the robot with its reference point at (x,y)
and rotated counterclockwise through an angle φ. So what is speciﬁed initially
is R(0,0,0).
In general, a placement of a robot is speciﬁed by a number of parameters that
corresponds to the number of degrees of freedom (DOF) of the robot. This
number is two for planar robots that can only translate, and it is three for planar
robots that can rotate as well as translate. The number of parameters we need
284

Section 13.1
WORK SPACE AND CONFIGURATION
SPACE
for a robot in 3-dimensional space is higher, of course: a translating robot in R3
has three degrees of freedom, and a robot that is free to translate and rotate in
R3 has six degrees of freedom.
The parameter space of a robot R is usually called its conﬁguration space. It is
denoted by C(R). A point p in this conﬁguration space corresponds to a certain
placement R(p) of the robot in the work space.
In the example of a translating and rotating robot in the plane the conﬁg-
uration space is 3-dimensional. A point (x,y,φ) in this space corresponds to
the placement R(x,y,φ) in the work space. The conﬁguration space is not the
Euclidean 3-dimensional space; it is the space R2 ×[0 : 360). Because rotations
over zero and 360 degrees are equivalent, the conﬁguration space of a rotating
robot has a special topology, which is like a cylinder.
The conﬁguration space of a translating robot in the plane is the 2-dimen-
sional Euclidean plane, and therefore identical to the work space. Still, it is
useful to distinguish the two notions: the work space is the space where the
robot actually moves around—the real world, so to speak—and the conﬁguration
space is the parameter space of the robot. A polygonal robot in the work space
is represented by a point in conﬁguration space, and any point in conﬁguration
space corresponds to some placement of an actual robot in work space.
We now have a way to specify a placement of the robot, namely by specifying
values for the parameters determining the placement or, in other words, by
specifying a point in conﬁguration space. But clearly not all points in con-
ﬁguration space are possible; points corresponding to placements where the
robot intersects one of the obstacles in S are forbidden. We call the part of
the conﬁguration space consisting of these points the forbidden conﬁguration
space, or forbidden space for short. It is denoted by Cforb(R,S). The rest of the
conﬁguration space, which consists of the points corresponding to free place-
ments—placements where the robot does not intersect any obstacle—is called
the free conﬁguration space, or free space, and it is denoted by Cfree(R,S).
A path for the robot maps to a curve in the conﬁguration space, and vice
versa: every placement along the path simply maps to the corresponding point
in conﬁguration space. A collision-free path maps to a curve in the free space.
Figure 13.1 illustrates this for a translating planar robot. On the left the work
space is shown, with a collision-free path from the initial position to the goal
position of the robot. On the right the conﬁguration space is shown, with the
grey area indicating the forbidden part of it. The unshaded area in between
the grey area is the free space. For clarity, the obstacles are still shown in
the conﬁguration space, although they have no meaning there. The curve
corresponding to the collision-free path is also shown.
We have seen how to map placements of the robot to points in the conﬁgu-
ration space, and paths of the robot to curves in that space. Can we also map
obstacles to conﬁguration space? The answer is yes: an obstacle P is mapped
to the set of points p in conﬁguration space such that R(p) intersects P. The
resulting set is called the conﬁguration-space obstacle, or C-obstacle for short,
of P.
285

Chapter 13
ROBOT MOTION PLANNING
Figure 13.1
A path in the work space and the
corresponding curve in the
conﬁguration space
work space
conﬁguration space
reference point
C-obstacles may overlap even when the obstacles in the work space are
disjoint. This happens when there are placements of the robot where it intersects
more than one obstacle at the same time.
There is one subtle issue that we have ignored so far: does the robot collide
with an obstacle when it touches that obstacle? In other words, do we deﬁne
the obstacles to be topologically open or closed sets? In the remainder we will
choose the ﬁrst option: obstacles are open sets, so that the robot is allowed to
touch them. This is of little importance in this chapter, but it will become useful
in Chapter 15. In practice a movement where the robot passes very close to an
obstacle cannot be considered safe because of possible errors in robot control.
Such movements can be avoided by slightly enlarging all the obstacles before
the computation of a path.
13.2
A Point Robot
Before we try to plan the motion of a polygonal robot in the plane, let's have a
look at point robots. Given the mapping from work space to conﬁguration space
that we saw in the previous section this is not such a strange idea. Furthermore,
it's always good to start with a simple case. As before, we denote the robot
by R and we denote the obstacles by P1,...,Pt. The obstacles are polygons
with disjoint interiors, whose total number of vertices is denoted by n. For a
point robot, the work space and the conﬁguration space are identical. (That is to
say, if we make the natural assumption that its reference point is the point robot
itself. Otherwise the conﬁguration space is a translated copy of the work space.)
Rather than ﬁnding a path from a particular start position to a particular goal
position we will construct a data structure storing a representation of the free
space. This data structure can then be used to compute a path between any two
given start and goal positions. Such an approach is useful if the work space of
the robot does not change and many paths have to be computed.
To simplify the description we restrict the motion of the robot to a large bounding
box B that contains the set of polygons. In other words, we add one extra
286

Section 13.2
A POINT ROBOT
inﬁnitely large obstacle, which is the area outside B. The free conﬁguration
space Cfree now consists of the part of B not covered by any obstacle:
Cfree = B \
t!
i=1
Pi.
The free space is a possibly disconnected region, which may have holes. Our
Cfree
B
goal is to compute a representation of the free space that allows us to ﬁnd a
path for any start and goal position. We will use the trapezoidal map for this.
Recall from Chapter 6 that the trapezoidal map of a set of non-intersecting line
segments inside a bounding box is obtained by drawing two vertical extensions
from every segment endpoint, one going upward until a segment (or the bound-
ing box) is hit, and one going downward until a segment (or the bounding box)
is hit. In Chapter 6 we developed a randomized algorithm, TRAPEZOIDALMAP,
vertical
extensions
that computes the trapezoidal map of a set of n segments in O(nlogn) expected
time. The following algorithm, which computes a representation of the free
space, uses this algorithm as a subroutine.
Algorithm COMPUTEFREESPACE(S)
Input. A set S of disjoint polygons.
Output. A trapezoidal map of Cfree(R,S) for a point robot R.
1.
Let E be the set of edges of the polygons in S.
2.
Compute the trapezoidal map T(E) with algorithm TRAPEZOIDALMAP
described in Chapter 6.
3.
Remove the trapezoids that lie inside one of the polygons from T(E) and
return the resulting subdivision.
The algorithm is illustrated in Figure 13.2. Part (a) of the ﬁgure shows the
trapezoidal map of the obstacle edges inside the bounding box; this is what is
computed in line 2 of the algorithm. Part (b) shows the map after the trapezoids
inside the obstacles have been removed in line 3.
(a)
(b)
Figure 13.2
Computing a trapezoidal map of the
free space
There is one detail left: how do we ﬁnd the trapezoids inside the obstacles,
which have to be removed? This is not so difﬁcult, because after running
TRAPEZOIDALMAP we know for each trapezoid the edge that bounds it from
the top, and we known to which obstacle that edge belongs. To decide whether
or not to remove the trapezoid, it sufﬁces to check whether the edge bounds
the obstacle from above or from below. The latter test takes only constant time,
287

Chapter 13
ROBOT MOTION PLANNING
because the edges of the obstacles are listed in order along the boundary so that
the obstacle lies to a speciﬁc, known side of the edges.
The expected time taken by TRAPEZOIDALMAP is O(nlogn), so we get the
following result.
Lemma 13.1 A trapezoidal map of the free conﬁguration space for a point robot
moving among a set of disjoint polygonal obstacles with n edges in total can be
computed by a randomized algorithm in O(nlogn) expected time.
In what follows, we will denote the trapezoidal map of the free space by
T(Cfree).
How do we use T(Cfree) to ﬁnd a path from a start position pstart to a goal
position pgoal?
If pstart and pgoal are in the same trapezoid of the map, this is easy: the robot
can simply move to its goal in a straight line.
pstart
pgoal
If the start and goal position are in different trapezoids, however, then things
are not so easy. In this case the path will cross a number of trapezoids and it
may have to make turns in some of them. To guide the motion across trapezoids
we construct a road map through the free space. The road map is a graph Groad,
which is embedded in the plane. More precisely, it is embedded in the free
space. Except for an initial and ﬁnal portion, paths will always follow the road
map. Notice that any two neighboring trapezoids share a vertical edge that is a
vertical extension of a segment endpoint. This leads us to deﬁne the road map
as follows. We place one node in the center of each trapezoid, and we place
one node in the middle of each vertical extension. There is an arc between two
Figure 13.3
A road map
nodes if and only if one node is in the center of a trapezoid and the other node
is on the boundary of that same trapezoid. The arcs are embedded in the plane
as straight line segments, so following an arc in the road map corresponds to
a straight-line motion of the robot. Figure 13.3 illustrates this. The road map
Groad can be constructed in O(n) time by traversing the doubly-connected edge
list of T(Cfree). Using the arcs in the road map we can go from the node in the
center of one trapezoid to the node in the center of a neighboring trapezoid via
the node on their common boundary.
288

Section 13.2
A POINT ROBOT
We can use the road map, together with the trapezoidal map, to plan a motion
from a start to a goal position. To this end we ﬁrst determine the trapezoids
∆start and ∆goal containing these points. If they are the same trapezoid, then we
move from pstart to pgoal in a straight line. Otherwise, let νstart and νgoal be the
nodes of Groad that have been placed in the center of these trapezoids. The path
from pstart to pgoal that we will construct now consists of three parts: the ﬁrst
part is a straight-line motion from pstart to νstart, the second part is a path from
νstart to νgoal along the arcs of the road map, and the ﬁnal part is a straight-line
motion from νgoal to pgoal. Figure 13.4 illustrates this.
pgoal
pstart
νgoal
∆start
∆goal
νstart
Figure 13.4
A path computed from the road map of
Figure 13.3
The following algorithm summarizes how a path is found.
Algorithm COMPUTEPATH(T(Cfree),Groad, pstart, pgoal)
Input. The trapezoidal map T(Cfree) of the free space, the road map Groad, a start
position pstart, and goal position pgoal.
Output. A path from pstart to pgoal if it exists. If a path does not exist, this fact is
reported.
1.
Find the trapezoid ∆start containing pstart and the trapezoid ∆goal containing
pgoal.
2.
if ∆start or ∆goal does not exist
3.
then Report that the start or goal position is in the forbidden space.
4.
else Let νstart be the node of Groad in the center of ∆start.
5.
Let νgoal be the node of Groad in the center of ∆goal.
6.
Compute a path in Groad from νstart to νgoal using breadth-ﬁrst
search.
7.
if there is no such path
8.
then Report that there is no path from pstart to pgoal.
9.
else Report the path consisting of a straight-line motion from
pstart to νstart, the path found in Groad, and a straight-line
motion from νgoal to pgoal.
Before we analyze the time complexity of algorithm, let's think about its cor-
rectness. Are the paths we report always collision-free, and do we always ﬁnd a
collision-free path if one exists?
289

Chapter 13
ROBOT MOTION PLANNING
The ﬁrst question is easy to answer: any path we report must be collision-
free, since it consists of segments inside trapezoids and all trapezoids are in the
free space.
To answer the second question, suppose that there is a collision-free path
from pstart to pgoal. Obviously pstart and pgoal must lie in one of the trapezoids
covering the free space, so it remains to show that there is a path in Groad from
νstart to νgoal. The path from pstart to pgoal must cross a sequence of trapezoids.
Denote the sequence of trapezoids by ∆1,∆2,...,∆k. By deﬁnition, ∆1 = ∆start
pgoal
pstart
∆1
∆2
∆3
∆5
∆4
and ∆k = ∆goal. Let νi be the node of Groad that is in the center of ∆i. If the path
goes from ∆i to ∆i+1, then ∆i and ∆i+1 must be neighbors, so they share a vertical
extension. But Groad is constructed such that the nodes of such trapezoids are
connected through the node on their common boundary. Hence, there is a path
(consisting of two arcs) in Groad from νi to νi+1. This means that there is a path
from ν1 to νk as well. It follows that the breadth-ﬁrst search in Groad will ﬁnd
some (possibly different) path from νstart to νgoal.
We now analyze the time the algorithm takes.
Finding the trapezoids containing the start and goal can be done in O(logn)
using the point location structure of Chapter 6. Alternatively, we can simply
check all trapezoids in linear time; we shall see that the rest of the algorithm
takes linear time anyway, so this does not increase the time bound asymptoti-
cally.
The breadth-ﬁrst search takes linear time in the size of the graph Groad. This
graph has one node per trapezoid plus one node per vertical extension. Both
the number of vertical extensions and the number of trapezoids are linear in the
total number of vertices of the obstacles. The number of arcs in the graph is
linear as well, because it is planar. Hence, the breadth-ﬁrst search takes O(n)
time.
The time to report the path is bounded by the maximum number of arcs on
a path in Groad, which is O(n).
We get the following theorem.
Theorem 13.2 Let R be a point robot moving among a set S of polygonal
obstacles with n edges in total. We can preprocess S in O(nlogn) expected time,
such that between any start and goal position a collision-free path for R can be
computed in O(n) time, if it exists.
The path computed by the algorithm of this section is collision-free, but we
can give no guarantee that the path does not make large detours. In Chapter 15
we will develop an algorithm that actually computes the shortest possible path.
That algorithm, however, will be slower by an order of magnitude.
13.3
Minkowski Sums
In the previous section we solved the motion planning problem for a point
robot; we computed a trapezoidal map of its free space and used that map to
290

Section 13.3
MINKOWSKI SUMS
plan its motions. The same approach can be used if the robot is a polygon.
There is one difference that makes dealing with a polygonal robot more difﬁcult:
the conﬁguration-space obstacles are no longer the same as the obstacles in
work space. Therefore we start by studying the free conﬁguration space of a
translating polygonal robot. In the next section we will then describe how to
compute it, so that we can use it to plan the motion of the robot.
We assume that the robot R is convex, and for the moment we also assume that
the obstacles are convex. Recall that we use R(x,y) to denote the placement
of R with its reference point at (x,y). The conﬁguration-space obstacle, or
C-obstacle, of an obstacle P and the robot R is deﬁned as the set of points in
conﬁguration space such that the corresponding placement of R intersects P.
So if we denote the C-obstacle of P by CP, then we have
R
P
CP
CP := {(x,y) : R(x,y)∩P ̸= /0}.
You can visualize the shape of CP by sliding R along the boundary of P; the
curve traced by the reference point of R is the boundary of CP.
We can describe this in a different way using the notion of Minkowski sums.
The Minkowski sum of two sets S1 ⊂R2 and S2 ⊂R2, denoted by S1 ⊕S2, is
deﬁned as
S1 ⊕S2 := {p+q : p ∈S1, q ∈S2},
where p+q denotes the vector sum of the vectors p and q, that is, if p = (px, py)
and q = (qx,qy) then we have
p+q := (px +qx, py +qy).
Because a polygon is a planar set the deﬁnition of Minkowski sums also applies
S1
S2
S1 ⊕S2
to them.
To be able to express the C-obstacles as Minkowski sums, we need one
more piece of notation. For a point p = (px, py) we deﬁne −p := (−px,−py),
and for a set S we deﬁne −S := {−p : p ∈S}. In other words, we get −S by
reﬂecting S about the origin. We now have the following theorem.
Theorem 13.3 Let R be a planar, translating robot and let P be an obstacle.
Then the C-obstacle of P is P⊕(−R(0,0)).
Proof. We have to prove that R(x,y) intersects P if and only if we have that
(x,y) ∈P⊕(−R(0,0)).
First, suppose that R(x,y) intersects P, and let q = (qx,qy) be a point in the
intersection. It follows from q ∈R(x,y) that we have (qx −x,qy −y) ∈R(0,0)
or, equivalently, that (−qx + x,−qy + y) ∈−R(0,0). Because we also have
q ∈P, this implies that (x,y) ∈P⊕(−R(0,0)).
Conversely, let (x,y) ∈P ⊕(−R(0,0)). Then there are points (rx,ry) ∈
R(0,0) and (px, py) ∈P such that (x,y) = (px −rx, py −ry) or, in other words,
such that px = rx+x and py = ry+y, which implies that R(x,y) intersects P.
291

Chapter 13
ROBOT MOTION PLANNING
So for a planar translating robot R the C-obstacles are the Minkowski sums
of the obstacles and −R(0,0). (Sometimes P ⊕(−R(0,0)) is referred to as
the Minkowski difference of P and R(0,0). Since Minkowski differences are
deﬁned differently in the mathematics literature we shall avoid this term.)
In the remainder of this section we will derive some useful properties of
Minkowski sums and develop an algorithm to compute them.
We start with a simple observation about extreme points on Minkowski sums.
Observation 13.4 Let P and R be two objects in the plane, and let CP := P⊕R.
An extreme point in direction ⃗d on CP is the sum of extreme points in direction ⃗d
on P and R.
Figure 13.5 illustrates the observation. Using this observation we now prove
that the Minkowski sum of two convex polygons has linear complexity.
Figure 13.5
An extreme point on a Minkowski sum
is the sum of extreme points
R
P
P ⊕R
r
p
p+r
⃗d
Theorem 13.5 Let P and R be two convex polygons with n and m edges, re-
spectively. Then the Minkowski sum P⊕R is a convex polygon with at most
n+m edges.
Proof. The convexity of the Minkowski sum of two convex sets follows directly
from the deﬁnition.
p
e = e′ ⊕p
e′
To see that the complexity of the Minkowski sum is linear, consider an edge
e of P⊕R. This edge is extreme in the direction of its outer normal. Hence, it
must be generated by points on P and R that are extreme in the same direction.
Moreover, at least one of P and R must have an edge that is extreme in that
direction. We charge e to this edge. This way each edge is charged at most once,
so the total number of edges is at most n+m. (If P and R don't have parallel
edges, then the number of edges of the Minkowski sum is exactly n+m.)
So the Minkowski sum of two convex polygons is convex and has linear
complexity. But there is more: the boundaries of two Minkowski sums can only
intersect in a very special manner. To make this precise, we need one more
piece of terminology.
Consider two planar objects o1 and o2, each bounded by a simple closed
curve. Intuitively, the pair o1,o2 is called a pair of pseudodiscs if their bound-
aries ∂o1 and ∂o2 intersect in at most two points. Figure 13.6 illustrates this.
In degenerate situations—when the boundaries have a 1-dimensional overlap
292

Section 13.3
MINKOWSKI SUMS
for instance—this deﬁnition is not quite general enough. Therefore we for-
mally deﬁne o1,o2 to be a pair of pseudodiscs if the following condition holds:
∂o1 ∩int(o2) is connected and ∂o2 ∩int(o1) is connected. (Here int(o) denotes
the interior of an object o.) A collection of objects, each bounded by a simple
closed curve, is called a collection of pseudodiscs if every pair of objects in the
collection is a pair of pseudodiscs. Examples of collections of pseudodiscs are
collections of discs, and collections of axis-parallel squares. Note that the pseu-
dodisc property is about the way in which (the boundaries of) two objects can
interact—it does not make sense to say of a single object that it is a pseudodisc.
pseudodiscs
not pseudodiscs
Figure 13.6
The pseudodisc property
Now consider two polygons P and P′. We say that an intersection point
p ∈∂P ∩∂P′ is a boundary crossing if ∂P crosses from the interior of P′ to
the exterior of P′ at p. Polygonal pseudodiscs satisfy the following important
property:
Observation 13.6 A pair of polygonal pseudodiscs P,P′ deﬁnes at most two
boundary crossings.
Below we will prove that a collection of Minkowski sums forms a collection
of pseudodiscs. But ﬁrst we need one more observation about directions and
extreme points on pairs of convex polygons with disjoint interiors. We say
that one polygon is more extreme in a direction ⃗d than another polygon if its
extreme points lie further in that direction than the extreme points of the other
polygon. For instance, a polygon is more extreme in the positive x-direction if
its rightmost points lie to the right of the rightmost points of the other polygon.
We will look at extreme points for various directions. To this end we model
the set of all directions by the unit circle centered at the origin: a point p on
the unit circle represents the direction given by the vector from the origin to p.
The range from a direction ⃗d1 to a direction ⃗d2 is deﬁned as the directions
corresponding to points in the counterclockwise circle segment from the point
representing ⃗d1 to the point representing ⃗d2. Note that the range from ⃗d1 to ⃗d2 is
⃗d2
⃗d1
directions
between
⃗d1 and ⃗d2
not the same as the range from ⃗d2 to ⃗d1. The following observation is illustrated
in Figure 13.7.
Observation 13.7 Let P1 and P2 be convex polygons with disjoint interiors.
Let ⃗d1 and ⃗d2 be directions in which P1 is more extreme than P2. Then either
P1 is more extreme than P2 in all directions in the range from ⃗d1 to ⃗d2, or it is
more extreme in all directions in the range from ⃗d2 to ⃗d1.
We are now ready to prove that Minkowski sums are pseudodiscs.
293

Chapter 13
ROBOT MOTION PLANNING
Figure 13.7
One convex polygon is more extreme
than another for a connected range of
directions
P1 is more
extreme
P2 is more
extreme
⃗d3
⃗d1
⃗d2
P1 and P2 are
equally extreme
⃗d1
⃗d2
⃗d3
P1
P2
Theorem 13.8 Let P1 and P2 be two convex polygons with disjoint interiors,
and let R be another convex polygon. Then the two Minkowski sums P1 ⊕R
and P2 ⊕R are pseudodiscs.
Proof. Deﬁne CP1 := P1 ⊕R and CP2 := P2 ⊕R. By symmetry, it sufﬁces to
show that ∂CP1 ∩int(CP2) is connected.
⃗dp
p
q
r
s
⃗dq
⃗dr
⃗ds
Suppose ∂CP1 ∩int(CP2) is not connected. Then there are four alternat-
ing directions ⃗dp, ⃗dq, ⃗dr, ⃗ds—these directions are outward normals of points
p,q,r,s ∈∂CP1 that occur in the given order along ∂CP1 with p,r ∈int(CP2)
and q,s ̸∈int(CP2)—-such that CP2 is more extreme than CP1 in directions ⃗dp
and ⃗dr while this is not the case for ⃗dq and ⃗ds. From Observation 13.4 it follows
that P1 is more extreme than P2 in directions ⃗dp and ⃗dr and not more extreme
in directions ⃗dq and ⃗ds. But this contradicts Observation 13.7.
This result is useful in combination with the following theorem.
Theorem 13.9 Let S be a collection of convex polygonal pseudodiscs with n
edges in total. Then the complexity of their union is at most 2n.
Proof. We prove the bound by charging every vertex of the union to a pseu-
dodisc vertex in such a way that any pseudodisc vertex is charged at most two
times. This leads to a bound of 2n on the complexity of the union.
The charging is done as follows. There are two types of vertices in the union
boundary: pseudodisc vertices and intersection points of the interiors of two
pseudodisc edges.
Vertices of the former type are simply charged to themselves.
Now consider a union vertex v that is the intersection of an edge e of a
pseudodisc P ∈S and an edge e′ of a pseudodisc P′ ∈S. Then e ∩e′ is a
boundary crossing. Hence, by Observation 13.6 either e does not have another
boundary crossing with ∂P′, or e′ does not have another boundary crossing with
∂P (or both). Assume without loss of generality that e does not have another
crossing with ∂P′. Starting at e∩e′, follow e into the interior of P′. Because e
does not cross ∂P′ a second time, we must reach an endpoint of e before we
reach the exterior of P′. We charge v to this endpoint of e.
e
e′
v
P
P′
We claim that if we do the charging in this way, then every pseudodisc
vertex v of any pseudodisc P is charged at most twice.
First consider the case that v does not lie in the interior or on the boundary
of any other pseudodisc P′. Then obviously v is a union vertex, and it is charged
only by itself. Next consider the case where v lies in the interior of some
294

Section 13.3
MINKOWSKI SUMS
pseudodisc P′. This means that v lies in the interior of the union. Now follow
the two edges of P incident to v until the union boundary is reached at a crossing
with some other edge; these two crossings, if they exist, are the only ones that
get charged to v. Finally, if v lies on the boundary of some other pseudodisc
P′, then v may be charged by itself and (similar to the case where v is in a
pseudodisc interior) by the ﬁrst two crossings along its two incident edges. It
can only be charged from both its incident edges, however, when the incident
edges go from v into the interior of the union of the pseudodiscs. In that case v
is not a union vertex and it will not be charged by itself. Hence, in all cases v is
charged at most twice.
The proof of this theorem depends heavily on the pseudodiscs being polygo-
nal, but the theorem itself generalizes to arbitrary pseudodiscs: the complexity
of the union of any set of pseudodiscs is linear in the total complexity of the
pseudodiscs. This implies for instance that the union of n discs in the plane has
O(n) complexity. In fact, the following stronger result holds: the number of
boundary intersections that show up as union vertices is linear in the number of
pseudodiscs. This more general theorem is a lot more difﬁcult to prove.
Before we return to our motion planning application, we give an algorithm to
compute the Minkowski sum of two convex polygons P and R. A very simple
algorithm is the following. For each pair v,w of vertices, with v ∈P and w ∈R,
compute v+w. Next, compute the convex hull of all these sums. Unfortunately
this algorithm inefﬁcient when the polygons have many vertices, because it
looks at every pair of vertices. Below we give an alternative algorithm, which is
as easy to implement. It only looks at pairs of vertices that are extreme in the
same direction—this is allowed because of Observation 13.4—which makes it
run in linear time. In the algorithm we use the notation angle(pq) to denote the
angle that the vector ⃗pq makes with the positive x-axis.
p
q
angle(pq)
Algorithm MINKOWSKISUM(P,R)
Input. A convex polygon P with vertices v1,...,vn, and a convex polygon R
with vertices w1,...,wm. The lists of vertices are assumed to be in counter-
clockwise order, with v1 and w1 being the vertices with smallest y-coordinate
(and smallest x-coordinate in case of ties).
Output. The Minkowski sum P⊕R.
1.
i ←1; j ←1
2.
vn+1 ←v1; vn+2 ←v2; wm+1 ←w1; wm+2 ←w2
3.
repeat
4.
Add vi +wj as a vertex to P⊕R.
5.
if angle(vivi+1) < angle(w jwj+1)
6.
then i ←(i+1)
7.
else if angle(vivi+1) > angle(w jw j+1)
8.
then j ←(j +1)
9.
else i ←(i+1); j ←(j +1)
10. until i = n+1 and j = m+1
MINKOWSKISUM runs in linear time, because at each execution of the repeat
loop either i or j is incremented and—as is not difﬁcult to prove—they will
295

Chapter 13
ROBOT MOTION PLANNING
not be incremented after reaching the values n+1 and m+1. The fact that the
correct pairs of vertices are taken is similar to the proof of Theorem 13.5; one
just has to observe that any vertex of the Minkowski sum is the sum of two
original vertices that are extreme in a common direction, and argue that the
angle test ensures that all extreme pairs are found.
We conclude with the following theorem:
Theorem 13.10 The Minkowski sum of two convex polygons with n and m
vertices, respectively, can be computed in O(n+m) time.
What happens if one or both of the polygons are not convex? This question
is not so hard to answer if we realize that the following equality holds for any
sets S1, S2, and S3:
S1 ⊕(S2 ∪S3) = (S1 ⊕S2)∪(S1 ⊕S3).
Now consider the Minkowski sum of a non-convex polygon P and a convex
polygon R with n and m vertices respectively. What is the complexity of P⊕R?
We know from Chapter 3 that the polygon P can be triangulated into n −2
triangles t1,...,tn−2, where n is its number of vertices. From the equality above
we can conclude that
P⊕R =
n−2
!
i=1
ti ⊕R.
Since ti is a triangle and R a convex polygon with m vertices, we know that
ti ⊕R is a convex polygon with at most m+3 vertices. Moreover, the triangles
have disjoint interiors, so the collection of Minkowski sums is a collection of
pseudodiscs. Hence, the complexity of their union is linear in the sum of their
complexities. This implies that the complexity of P⊕R is O(nm).
Figure 13.8
The Minkowski sum of a non-convex
and a convex polygon
R
P
P ⊕R
This upper bound on the complexity of a non-convex and a convex polygon
is tight in the worst case. To see this, consider a polygon P with ⌊n/2⌋spikes
pointing upward, and a much smaller polygon R that is the top half of a regular
(2m −2)-gon. The Minkowski sum of these polygons will also have ⌊n/2⌋
spikes, each of which has m vertices at its top. Figure 13.8 illustrates the
construction.
296

Section 13.4
TRANSLATIONAL MOTION PLANNING
To bound the complexity of the Minkowski sum of two non-convex polygons P
and R, we triangulate both polygons. We get a collection of n−2 triangles ti,
and a collection of m−2 triangles uj. The Minkowski sum of P and R is now the
union of the Minkowski sums of the pairs ti,u j. Each sum ti ⊕uj has constant
complexity. Hence, P⊕R is the union of (n−2)(m−2) constant-complexity
polygons. This implies that the total complexity of P⊕R is O(n2m2). Again,
this bound is tight in the worst case: there are non-convex polygons whose
Minkowski sum really has Θ(n2m2) complexity. Figure 13.9 illustrates this.
P
R
P ⊕R
Figure 13.9
The Minkowski sum of two non-convex
polygons
The following theorem summarizes the results on the complexity of Minkowski
sums. For completeness the complexity in the case of two convex polygons is
given as well.
Theorem 13.11 Let P and R be polygons with n and m vertices, respectively.
The complexity of the Minkowski sum P⊕R is bounded as follows:
(i)
it is O(n+m) if both polygons are convex;
(ii) it is O(nm) if one of the polygons is convex and one is non-convex;
(iii) it is O(n2m2) if both polygons are non-convex.
These bounds are tight in the worst case.
Computing Minkowski sums of non-convex polygons is not very difﬁcult:
triangulate both polygons, compute the Minkowski sum of each pair of triangles,
and take their union. This approach is basically the same as the approach
described in the next section for computing the forbidden space of a translating
robot, so we omit the details here.
13.4
Translational Motion Planning
It is time to return to the planar motion planning problem. Recall that our robot
R can only translate and that the obstacles are disjoint polygons. Early in the
297

Chapter 13
ROBOT MOTION PLANNING
previous section we have shown that the C-obstacle corresponding to an obstacle
Pi is the Minkowski sum Pi ⊕(−R). Moreover, we have seen that Minkowski
sums of convex polygons are pseudodiscs. We use this to prove our ﬁrst major
result on the motion planning problem, which states that the complexity of the
free space of a translating planar robot is linear.
Theorem 13.12 Let R be a convex robot of constant complexity, translating
among a set S of non-intersecting polygonal obstacles with a total of n edges.
Then the complexity of the free conﬁguration space Cfree(R,S) is O(n).
Proof. First we triangulate each obstacle polygon. We get a set of O(n) tri-
angular, and hence convex, obstacles with disjoint interiors. The free con-
ﬁguration space is the complement of the union of the C-obstacles of these
triangles. Because the robot has constant complexity, the C-obstacles have con-
stant complexity, and according to Theorem 13.8 they form a set of pseudodiscs.
Theorem 13.9 now implies that the union has linear complexity.
It remains to ﬁnd an algorithm to construct the free space. Rather than
computing the free space Cfree, we shall compute the forbidden space Cforb; the
free space is simply its complement.
Let P1,...,Pn denote the triangles that we get when we triangulate the
obstacles. We want to compute
Cforb =
n!
i=1
CPi =
n!
i=1
Pi ⊕(−R(0,0)).
In Section 13.3 we saw how to compute the individual Minkowski sums CPi.
To compute their union we use a simple divide-and-conquer approach.
Algorithm FORBIDDENSPACE(CP1,...,CPn)
Input. A collection of C-obstacles.
Output. The forbidden space Cforb = "n
i=1 CPi.
1.
if n = 1
2.
then return CP1
3.
else C1
forb ←FORBIDDENSPACE(P1,...,P⌈n/2⌉)
4.
C2
forb ←FORBIDDENSPACE(P⌈n/2⌉+1,...,Pn)
5.
Compute Cforb = C1
forb ∪C2
forb.
6.
return Cforb
The heart of this algorithm is the subroutine to compute the union of two planar
regions, which we need to perform the merge step (line 5). If we represent
these regions by doubly-connected edge lists, this can be done by the overlay
algorithm described in Chapter 2.
The following lemma summarizes the result.
Lemma 13.13 The free conﬁguration space Cfree of a convex robot of constant
complexity translating among a set of polygons with n edges in total can be
computed in O(nlog2 n) time.
298

Section 13.5*
MOTION PLANNING WITH ROTATIONS
Proof. In Chapter 3 we saw that a polygon with m vertices can be triangulated
in O(mlogm) time. (In fact, it can even be done in O(m) time with a very
complicated algorithm, as stated in the notes and comments of Chapter 3.)
Hence, if mi denotes the complexity of obstacle Pi, then triangulating all the
obstacles takes time proportional to
t
∑
i=1
mi logmi ⩽
t
∑
i=1
mi logn = nlogn.
Computing the C-obstacles of each of the resulting triangles takes linear time in
total. It remains to bound the time that FORBIDDENSPACE needs to compute
the union of the C-obstacles.
Using the results from Chapter 2, the merge step (line 5) can be done in
O((n1 + n2 + k)log(n1 + n2)) where n1, n2, and k denote the complexity of
C1
forb, C2
forb, and C1
forb ∪C2
forb. Theorem 13.12 states that the complexity of the
free space—and, hence, of the forbidden space—is linear in the sum of the
complexities of the obstacles. In our case this means that n1, n2, and k are
all O(n), so the time for the merge step is O(nlogn). We get the following
recurrence for T(n), the time the algorithm needs when applied to a set of n
constant-complexity C-obstacles:
T(n) = T(⌈n/2⌉)+T(⌊n/2⌋)+O(nlogn).
The solution of this recurrence is O(nlog2 n).
The result of this theorem is not the best possible—see the notes and com-
ments of this chapter.
Now that we have computed the free space, we can continue in exactly the
same way as in Section 13.2: we compute a trapezoidal map of the free space,
together with a roadmap. Given a start and a goal placement of the robot R,
we ﬁnd a path as follows. First, we map the start and goal placement to points
in the conﬁguration space. Then we compute a path between these two points
through the free space using the trapezoidal map and the road map, as described
in Section 13.2. Finally, we map the path back to a path for R in the work space.
The next theorem summarizes the result of our efforts.
Theorem 13.14 Let R be a convex robot of constant complexity translating
among a set S of disjoint polygonal obstacles with n edges in total. We can
preprocess S in O(nlog2 n) expected time, such that between any start and goal
position a collision-free path for R, if it exists, can be computed in O(n) time.
13.5*
Motion Planning with Rotations
In the previous sections the robot was only allowed to translate. When the robot
is circular this does not limit its possible motion. On the other hand, when it is
long and skinny, translational motion is often not enough: it may have to change
299

Chapter 13
ROBOT MOTION PLANNING
its orientation to be able to pass through a narrow passage or to go around a
corner. In this section we sketch a method to plan motion for robots that can
rotate as well as translate.
Let R be a convex polygonal robot that can translate and rotate in a planar work
space that contains a set P1,...,Pt of disjoint polygonal obstacles. The robot
R has three degrees of freedom: two translational and one rotational degree of
freedom. Hence, we can specify a placement for R by three parameters: the
x- and y-coordinate of a reference point of R, and an angle φ that speciﬁes its
orientation. As in Section 13.1, we use R(x,y,φ) to denote the robot placed
with its reference point at (x,y) and rotated over an angle φ.
The conﬁguration space that we get is the 3-dimensional space R2×[0 : 360),
with a topology where points (x,y,0) and (x,y,360) are identiﬁed. Recall that
CPi, the C-obstacle of an obstacle Pi, is deﬁned as follows:
CPi := {(x,y,φ) ∈R2 ×[0 : 360) : R(x,y,φ)∩Pi ̸= /0}.
What do these C-obstacles look like? This question is difﬁcult to answer directly,
but we can get an idea by looking at cross-sections with planes of constant φ.
In such a plane the rotation angle is ﬁxed, so we are dealing with a purely
translational problem. Hence, we know the shape of the cross-section: it is
a Minkowski sum. More precisely, the cross-section of CPi with the plane
h : φ = φ0 is equal to Pi ⊕R(0,0,φ0). (More precisely, it is a copy of the
Figure 13.10
The C-obstacle of a rotating and
translating robot
Pi
R
x
y
work space
conﬁguration space
x
y
φ
CPi
Minkowski sum placed at height φ0.) Now imagine sweeping a horizontal
plane upwards through conﬁguration space, starting at φ = 0 and ending at
φ = 360. At any time during the sweep the cross-section of the plane with CPi
is a Minkowski sum. The shape of the Minkowski sum changes continuously: at
φ = φ0 the cross-section is Pi ⊕R(0,0,φ0), and at φ = φ0 +ε the cross-section
is Pi ⊕R(0,0,φ0 + ε). This means that CPi looks like a twisted pillar, as in
Figure 13.10. The edges and facets of this twisted pillar, except for the top facet
and bottom facet, are curved.
300

Section 13.5*
MOTION PLANNING WITH ROTATIONS
So we know more or less what C-obstacles look like. The free space is the
complement of the union of these C-obstacles. Due to the nasty shape of the
C-obstacles, the free space is rather complicated: its boundary is no longer
polygonal, but curved. Moreover, the combinatorial complexity of the free
space can be quadratic for a convex robot, and even cubic for a non-convex
robot. Nevertheless, we can solve the motion planning problem using the same
approach we took before: compute a decomposition of the free space into simple
cells, and construct a road map to guide the motions between neighboring cells.
Given a start and goal placement of the robot we then ﬁnd a path as follows.
We map these placements to points in conﬁguration space, ﬁnd the cells that
contain the points, and construct a path consisting of three parts: a path from the
start position to the node of the road map in the center of the start cell, a path
along the road map to the node in the center of the goal cell, and a path inside
the goal cell to the ﬁnal destination. It remains to map the path in conﬁguration
space back to a motion in the work space.
Because of the complex shape of the C-obstacles, it is difﬁcult to compute a
suitable cell decomposition, especially when it comes to an actual implemen-
tation. Therefore we shall describe a different, simpler approach. As we shall
see, however, this approach has its drawbacks as well. Our approach is based
on the same observation we used to study the shape of the C-obstacles, namely
that the motion planning problem reduces to a purely translational problem if
we restrict the attention to a horizontal cross-section of the conﬁguration space.
We will call such a cross-section a slice. The idea is to compute a ﬁnite number
of slices. A path for the robot now consists of two types of motion: motions
within a slice—these are purely translational—and motions from one slice to
the next or previous one—these will be purely rotational.
Let's formalize this. Let z denote the number of slices we take. For every integer
i with 0 ⩽i ⩽z−1, let φi = i×(360/z). We compute a slice of the free space
for each φi. Since within the slice we are dealing with a purely translational
problem for the robot R(0,0,φi), we can compute the slice using the methods of
the previous section. This will give us the trapezoidal map Ti of the free space
within the slice. For each Ti we compute a road map Gi. These road maps are
used to plan the motions within a slice, as in Section 13.2.
It remains to connect consecutive slices. More precisely, we connect every
pair of roadmaps Gi, Gi+1 to obtain a roadmap Groad of the entire conﬁguration
space. This is done as follows. We take the trapezoidal maps of each pair of
consecutive slices, and compute their overlay with the algorithm of Chapter 2.
(Strictly speaking, we should say that we compute the overlay of the projections
of Ti and Ti+1 onto the plane h : φ = 0.) This tells us all the pairs ∆1,∆2
with ∆1 ∈Ti and ∆2 ∈Ti+1 such that ∆1 intersects ∆2. Let (x,y,0) be a point
in ∆1 ∩∆2. We then add an extra node to Groad at (x,y,φi) and at (x,y,φi+1),
which we connect by an arc. Moving from one slice to the other along this arc
corresponds to a rotation from φi to φi+1, or back. Furthermore, the node at
(x,y,φi) is connected to the node at the center of ∆1, and the node at (x,y,φi+1)
is connected to the node at the center of ∆2. These connections stay within a
slice, so they correspond to purely translational motions. We connect Gz−1 and
301

Chapter 13
ROBOT MOTION PLANNING
G0 in the same way. Note that paths in the graph Groad correspond to paths of the
robot that are composed of purely translational motion (when we move along
an arc connecting nodes in the same slice) and purely rotational motion (when
we move along an arc connecting nodes in different slices).
Once we have constructed this road map, we can use it to plan a mo-
tion for R from any start placement R(xstart,ystart,φstart) to any goal placement
R(xgoal,ygoal,φgoal). To do this, we ﬁrst determine the slices closest to the start
and goal placement by rounding the orientations φstart and φgoal to the nearest
orientation φi for which we constructed a slice. Within those slices we deter-
mine the trapezoids ∆start and ∆goal containing the start and goal position. If
one of these trapezoids does not exist because the start or goal position lies in
the forbidden space within the slice, then we report that we cannot compute
a path. Otherwise, let νstart and νgoal be the nodes of the road map that have
been placed in their center. We try to ﬁnd a path in Groad from νstart to νgoal
using breadth-ﬁrst search. If there is no path in the graph, we report that we
cannot compute a motion. Otherwise we report a motion consisting of ﬁve
parts: a purely rotational motion from the start position to the nearest slice, a
purely translational motion within that slice to the node νstart, a motion that
corresponds to the path from νstart to νgoal in Groad, a purely translational motion
from νgoal to the ﬁnal position within that slice (which is the slice nearest to the
goal position), and ﬁnally a purely rotational motion to the real goal position.
This method is a generalization of the method we used for translating motions,
but is has a major problem: it is not always correct. Sometimes it may erro-
neously report that a path does not exist. For instance, the start position can be
in the free space, whereas the start position within the nearest slice is not. In
this case we report that there is no path, which need not be true. Even worse is
that the paths we report need not be collision-free. The translational motions
within a slice are okay, because we solved the problem within a slice exactly,
but the rotational motions from one slice to the next may cause problems: the
placements within the two slices are collision-free, but halfway the robot could
collide with an obstacle. Both problems are less likely to occur when we in-
crease the number of slices, but we can never be certain of the correctness of
the result. This is especially bothersome for the second problem: we deﬁnitely
don't want our possibly very expensive robot to have a collision.
Figure 13.11
Enlarging the robot
R
sweep area
R′
Therefore we use the following trick. We make the robot slightly larger,
and use the method described above on the enlarged robot R′. This is done in
such a way that although R′ can collide during rotations, the original robot R
302

Section 13.6
NOTES AND COMMENTS
cannot. To achieve this, the robot is enlarged as follows. Rotate R clockwise
and counterclockwise over an angle of (180/z)◦. During this rotation R sweeps
a part of the plane. We use for the enlarged robot R′ a convex polygon that
contains the sweep area—see Figure 13.11. We now compute the trapezoidal
maps and the road map for R′ instead of R. It is not difﬁcult to prove that R
cannot collide with an obstacle during a purely rotational motion between two
adjacent slices, even though R′ can. By enlarging the robot we have introduced
another way to incorrectly decide that there is no path. Again, this becomes less
likely when the number of slices increases. So with a large number of slices,
the method probably performs reasonably well in practical situations.
13.6
Notes and Comments
The motion-planning problem has received a lot of attention over the years,
both from people working in computational geometry and from people work-
ing in robotics, and this chapter only scratches the surface of all the research.
A much more extensive treatment of the problem is given by Latombe [243].
Sharir [352] gives a survey of the theoretical results that have been obtained.
Nevertheless, the concepts we have introduced—conﬁguration space, decompo-
sition of the free space, road maps that transform the geometric problem into a
graph searching problem—underly the majority of approaches that have been
proposed.
These concepts date back to the work of Lozano-P´erez [258, 259, 260].
An important difference between his method and the method of this chapter is
that he used an approximate decomposition of the free space. The approach of
Section 13.2, which uses an exact decomposition of the free space of a planar
translating robot into trapezoids, is based on more recent work by Kedem et
al. [231, 232]. An improved algorithm, which runs in O(nlogn) time, was given
by Bhattacharya and Zorbas [68].
A very general method that is based on ﬁnding an exact cell decomposition
of the free space was given by Schwartz and Sharir [341]. It is based on a
decomposition method of Collins [136]. Unfortunately, this method takes time
doubly exponential in the dimension of the conﬁguration space. This can be
improved using a decomposition method of Chazelle et al. [102].
In this chapter we have seen that the cell decomposition approach leads
to an O(nlog2 n) algorithm when applied to a convex robot translating in the
plane. The bottleneck in the algorithm was the computation of the union of
a collection of Minkowski sums. Using a randomized incremental algorithm,
instead of a divide-and-conquer algorithm, this step can be done in O(nlogn)
time [58, 280]. Our approach was based on the fact that the Minkowski sums of
convex disjoint polygons with some ﬁxed other convex polygon form a set of
pseudodiscs, and that the union complexity of these pseudodiscs is O(n). The
latter statement is even true for non-polygonal pseudodiscs: there are at most
6n−12 breakpoints (that is, boundary intersections of two pseudodiscs) on the
boundary of the union of n pseudodiscs [231]. Here a set of objects is deﬁned
303

Chapter 13
ROBOT MOTION PLANNING
to be a set of pseudodiscs if the boundaries of any pair of them intersect in at
most two points. Extensions of this result, and many other results on the union
complexity of certain families of objects are discussed in a recent survey by
Agarwal et al. [3].
The translational motion planning problem in 3-dimensional space can be
solved in O(n2 log3 n) time [22].
The approach we sketched for robots that can translate and rotate is ap-
proximate: it isn't guaranteed to ﬁnd a path if it exists. It is possible to ﬁnd
an exact solution by computing an exact cell decomposition of the free space
in O(n3) time [33]. For a convex robot, the running time can be reduced to
O(n2 log2 n) [233].
The free space of a robot may consist of a number of disconnected com-
ponents. Of course, the motions of the robot are conﬁned to the component
where it starts; to go to another component it would have to pass through the
forbidden space. Hence, it is sufﬁcient to compute only a single cell in the free
space, instead of the entire free space. Usually the worst-case complexity of a
single cell is one order of magnitude lower than the complexity of the entire free
space. This can be used to speed up the asymptotic running time of the motion
planning algorithms. The book by Agarwal and Sharir [353] and the thesis by
Halperin [205] discuss single cells and their connection to motion planning at
length.
The theoretical complexity of the motion planning is exponential in the
number of degrees of freedom of the robot, which makes the problem appear in-
tractable for high DOF robots. Under some mild restrictions on the shape of the
robot and the obstacles—which are likely to be satisﬁed in practical situations—
one can show that the complexity of the free space is only linear [364, 365].
Cell decomposition methods are not the only exact methods for motion planning.
Another approach is the so-called retraction method. Here a road map is
constructed directly, without decomposing the free space. Furthermore, a
retraction function is deﬁned, which maps any point in the free space to a point
on the road map. Once this has been done, paths can be found by retracting
both start and goal to the road map and next following a path along the road
map. Different types of road maps and retraction functions have been proposed.
A nice road map is the Voronoi diagram, because it stays as far away from the
obstacles as possible. If the robot is a disc, then we can use the normal Voronoi
diagram—this was already discussed in Section 7.3. Otherwise one has to use
a different distance function in the deﬁnition of the Voronoi diagram, which
depends on the shape of the robot. Still, such a diagram can often be computed
in O(nlogn) time [255, 296], leading to another O(nlogn) time algorithm for
translational motion planning. A very general road map method has been
proposed by Canny [80]. It can solve almost any motion planning problem in
time O(nd logn), where d is the dimension of the conﬁguration space, that is,
the number of degrees of freedom of the robot. Unfortunately, the method is
very complicated, and it has the disadvantage that most of the time the robot
moves in contact with an obstacle. This is often not the preferred type of motion.
304

Section 13.7
EXERCISES
In this chapter we have concentrated mainly on exact motion planning. There
are also a number of heuristic approaches.
For instance, one can use approximate cell decompositions [74, 258, 259,
398] instead of exact ones. These are often based on quadtrees.
Another heuristic is the potential ﬁeld method [39, 235, 378]. Here one
deﬁnes a potential ﬁeld on the conﬁguration space by making the goal position
attract the robot and making the obstacles repel it. The robot then moves in the
direction dictated by the potential ﬁeld. The problem with this approach is that
the robot can get stuck in local minima of the potential ﬁeld. Various techniques
have been proposed to escape the minima.
Another heuristic which has recently become popular is the probabilistic
road map method [230, 310]. It computes a number of random placements for
the robot, which are connected in some way to form a road map of the free
space. The road map can then be used to plan paths between any given start and
goal placement.
Minkowski sums play an important role not only in motion planning, but also
in other problems. An example is the problem of placing one polygon inside
another [119]; this can be useful if one wants to cut out some shape from a piece
of fabric. For some basic results on properties of Minkowski sums and their
computation we refer to [43, 197].
In this chapter we concentrated on ﬁnding some path for the robot, but we didn't
try to ﬁnd a short path. This is the topic of Chapter 15.
Finally, we note that we allowed paths where the robot touches an obstacle.
Such paths are sometimes called "semi-free" [243, 340]. Paths that do not touch
any obstacle are then called "free". It is useful to be aware of these terms when
studying the motion planning literature.
13.7
Exercises
13.1
Let R be a robotic arm with a ﬁxed base and seven links. The last joint
of R is a prismatic joint, the other ones are revolute joints. Give a set of
parameters that determines a placement of R. What is the dimension of
the conﬁguration space resulting from your choice of parameters?
13.2
In the road map Groad that was constructed on the trapezoidal decomposi-
tion of the free space we added a node in the center of each trapezoid and
on each vertical wall. It is possible to avoid the nodes in the center of
each trapezoid. Show how the graph can be changed such that only nodes
on the vertical walls are required. (Avoid an increase in the number of
edges in the graph.) Explain how to adapt the query algorithm.
13.3
Prove that the shape of CPi is independent of the choice of the reference
point in the robot R.
305

Chapter 13
ROBOT MOTION PLANNING
13.4
Draw the Minkowski sum P1 ⊕P2 for the case where
a. both P1 and P2 are unit discs;
b. both P1 and P2 are unit squares;
c. P1 is a unit disc and P2 is a unit square;
d. P1 is a unit square and P2 is a triangle with vertices (0,0),(1,0),(0,1).
13.5
Let P1 and P2 be two convex polygons. Let S1 be the collection of
vertices of P1 and S2 be the collection of vertices of P2. Prove that
P1 ⊕P2 = ConvexHull(S1 ⊕S2).
13.6
Prove Observation 13.4.
13.7
In Theorem 13.9 we gave an O(n) bound on the complexity of the
union of a set of polygonal pseudodiscs with n vertices in total. We are
interested in the precise bound.
a. Assume that the union boundary contains m original vertices of the
polygons. Show that the complexity of the union boundary is at most
2n−m. Use this to prove an upper bound of 2n−3 on the complexity
of the union boundary.
b. Prove a lower bound of 2n−6 by constructing an example that has
this complexity.
306

14
Quadtrees
Non-Uniform Mesh Generation
Almost all electrical devices, from shavers and telephones to televisions and
computers, contain some electronic circuitry to control their functioning. This
circuitry—VLSI circuits, resistors, capacitors, and other electric components—
is placed on a printed circuit board. To design printed circuit boards one has to
decide where to place the components, and how to connect them. This raises a
number of interesting geometric problems, of which this chapter tackles one:
mesh generation.
Many components on a printed circuit board emit heat during operation. In order
for the board to function properly, the emission of heat should be below a certain
threshold. It is difﬁcult to predict in advance whether the heat emission will
cause problems, since this depends on the relative positions of the components
and the connections. In former days one therefore made a prototype of the board
to determine the heat emission experimentally. If it turned out to be too high,
an alternative layout had to be designed. Today the experiments can often be
simulated. Because the design is largely automated, a computer model of the
board is readily available and simulation is a lot faster than building a prototype.
Simulation also allows testing during the initial stages of the design phase, so
that faulty designs can be rejected as early as possible.
The heat transfer between different materials on the printed circuit board is
a quite complicated process. To simulate the heat processes on the board one
therefore has to resort to approximation using ﬁnite element methods. Such
methods ﬁrst divide the board into many small regions, or elements. The
elements are usually triangles or quadrilaterals. The heat that each element
emits by itself is assumed to be known. It is also assumed to be known how
neighboring elements inﬂuence each other. This leads to a big system of
equations, which is then solved numerically.
The accuracy of ﬁnite element methods depends heavily on the mesh: the
ﬁner the mesh, the better the solution. The other side of the coin is that the
computation time for the numerical process increases drastically when the
number of elements increases. So we would like to use a ﬁne mesh only where
necessary. Often this is at the border between regions of different material. It
is also important that the mesh elements respect the borders, that is, that any
307

Chapter 14
QUADTREES
mesh element is contained in only one region. Finally, the shape of the mesh
elements plays an important role: irregularly shaped elements such as very thin
triangles often lead to a slower convergence of the numerical process.
Figure 14.1
Triangular mesh of a part of a printed
circuit board
14.1
Uniform and Non-Uniform Meshes
0
U
U
0
We'll study the following variant of the mesh generation problem. The input
is a square—the printed circuit board—with a number of disjoint polygonal
components inside it. The square together with the components is sometimes
called the domain of the mesh. The vertices of the square are at (0,0), (0,U),
(U,0), (U,U), where U = 2 j for a positive integer j. The coordinates of the
vertices of the components are assumed to be integers between 0 and U. We
make one more assumption, which is satisﬁed in a number of applications: the
edges of the components have only four different orientations. In particular, the
angle that an edge makes with the x-axis is either 0◦, 45◦, 90◦, or 135◦.
Our goal is to compute a triangular mesh of the square, that is, a subdivi-
sion of the square into triangles. We require the mesh to have the following
properties:
The mesh must be conforming: a triangle is not allowed to have a vertex of
another triangle in the interior of one of its edges.
The mesh must respect the input: the edges of the components must be
contained in the union of the edges of the mesh triangles.
The mesh triangles must be well-shaped: the angles of any mesh triangle
should not be too large nor too small. In particular, we require them to be in
the range from 45◦to 90◦.
doesn't respect input
component
not conforming
not well-shaped
Finally, we would like the mesh to be ﬁne only where necessary. Where this is
depends on the application. The property we shall require is as follows.
The mesh must be non-uniform: it should be ﬁne near the edges of the
components and coarse far away from the edges.
308

Section 14.2
QUADTREES FOR POINT SETS
We have already seen triangulations earlier in this book. Chapter 3 presented
an algorithm for triangulating a simple polygon, and Chapter 9 presented an
algorithm for triangulating a point set. The latter algorithm computes the
Delaunay triangulation, a triangulation that maximizes the minimum angle over
all possible triangulations. Given our restriction on the angles of the mesh
triangles this seems quite useful, but there are two problems.
First of all, a triangulation of the vertices of the components need not respect
the edges of the components. Even if it did, there is a second problem: there
can still be angles that are too small. Suppose that the input is a square with
side length 16 and one component, which is a small square with side length 1
placed in the top left corner at distance 1 from the left and the top side of the
square. Then the Delaunay triangulation contains triangles that have an angle of
less than 5◦. Since the Delaunay triangulation maximizes the minimum angle, it
seems impossible to generate a mesh with only well-shaped triangles. But there
is a catch: unlike in a triangulation, the triangles in a mesh are not required to
have their vertices at the input points. We are allowed to add extra points, called
Steiner points, to help us obtain well-shaped triangles. A triangulation that
uses Steiner points is sometimes called a Steiner triangulation. In our example,
if we add a Steiner point at every grid point inside the square, then we can
easily obtain a mesh consisting only of triangles with two 45◦angles and one
90◦angle—see the mesh shown on the left in Figure 14.2. Unfortunately, this
Figure 14.2
A uniform and a non-uniform mesh
mesh suffers from another problem: it uses small triangles everywhere, not only
near the edges of the input, so it is a uniform mesh. As a result, it has many
triangles. We cannot simply replace all the triangles in, say, the bottom right
quarter of the square by two big triangles, because then the mesh would no
longer be conforming. Nevertheless, if we gradually increase the size of the
triangles when we get farther away from top left corner, then it is possible to
get a conforming mesh with only well-shaped triangles, as shown on the right
in Figure 14.2. This leads to a signiﬁcantly smaller number of triangles: the
uniform mesh has 512 triangles, whereas the non-uniform one only has 52.
14.2
Quadtrees for Point Sets
The non-uniform mesh generation method we shall describe in the next section
is based on quadtrees. A quadtree is a rooted tree in which every internal node
has four children. Every node in the quadtree corresponds to a square. If a
309

Chapter 14
QUADTREES
node ν has children, then their corresponding squares are the four quadrants
of the square of ν—hence the name of the tree. This implies that the squares
of the leaves together form a subdivision of the square of the root. We call
this subdivision the quadtree subdivision. Figure 14.3 gives an example of
a quadtree and the corresponding subdivision. The children of the root are
Figure 14.3
A quadtree and the corresponding
subdivision
NW
SW
NE
SE
labelled NE, NW, SW, and SE to indicate to which quadrant they correspond;
NE stands for the north-east quadrant, NW for the north-west quadrant, and so
on.
Before we continue, we introduce some terminology related to quadtree
subdivisions. The faces in a quadtree subdivision have the shape of a square.
Although they can have more than four vertices, we shall call them squares
anyway. The four vertices at the corners of the square are called corner vertices,
or corners for short. The line segments connecting consecutive corners are the
sides of the square. The edges of the quadtree subdivision that are contained
in the boundary of a square are called the edges of the square. Hence, a side
contains at least one, but possibly many more, edges. We say that two squares
are neighbors if they share an edge.
corner
edge
side
Quadtrees can be used to store different types of data. We will describe the
variant that stores a set of points in the plane. In this case the recursive splitting
of squares continues as long as there is more than one point in a square. So the
deﬁnition of a quadtree for a set P of points inside a square σ is as follows. Let
σ := [xσ : x′
σ]×[yσ : y′
σ].
If card(P) ⩽1 then the quadtree consists of a single leaf where the set P
and the square σ are stored.
Otherwise, let σNE, σNW, σSW, and σSE denote the four quadrants of σ. Let
xmid := (xσ +x′
σ)/2 and ymid := (yσ +y′
σ)/2, and deﬁne
PNE
:=
{p ∈P : px > xmid and py > ymid},
PNW
:=
{p ∈P : px ⩽xmid and py > ymid},
PSW
:=
{p ∈P : px ⩽xmid and py ⩽ymid},
PSE
:=
{p ∈P : px > xmid and py ⩽ymid}.
The quadtree now consists of a root node ν where the square σ is stored.
Below we shall denote the square stored at ν by σ(ν). Furthermore, ν has
four children:
310

Section 14.2
QUADTREES FOR POINT SETS
the NE-child is the root of a quadtree for the set PNE inside the square σNE,
the NW-child is the root of a quadtree for the set PNW inside the
square σNW,
the SW-child is the root of a quadtree for the set PSW inside the square σSW,
the SE-child is the root of a quadtree for the set PSE inside the square σSE.
σNW
σSW
σSE
σNE
xmid
ymid
The choice of using less-than-or-equal-to and greater-than in the deﬁnition of
the sets PNE, PNW, PSW, and PSE means that we deﬁne the vertical splitting line
to belong to the left two quadrants, and the horizontal splitting line to the lower
two quadrants.
Every node ν of the quadtree stores its corresponding square σ(ν). This is
not necessary; we could store only the square of the root of the tree. When we
walk down the tree we would then have to maintain the square of the current
node. This alternative uses less storage at the expense of extra computations
that have to be done when the quadtree is queried.
The recursive deﬁnition of a quadtree immediately translates into a recursive
algorithm: split the current square into four quadrants, partition the point set
accordingly, and recursively construct quadtrees for each quadrant with its
associated point set. The recursion stops when the point set contains less than
two points. The only detail that does not follow from the recursive deﬁnition
is how to ﬁnd the square with which to start the construction. Sometimes this
square will be given as a part of the input. If this is not the case, then we
compute a smallest enclosing square for the set of points. This can be done in
linear time by computing the extreme points in the x- and y-directions.
At every step of the quadtree construction the square containing the points
is split into four smaller squares. This does not mean that the point set is split
as well: it can happen that all the points lie in the same quadrant. Therefore a
quadtree can be quite unbalanced, and it is not possible to express the size and
depth of a quadtree as a function of the number of points it stores. However, the
depth of a quadtree is related to the distance between the points and the size of
the initial square. This is made precise in the following lemma.
Lemma 14.1 The depth of a quadtree for a set P of points in the plane is at most
log(s/c)+ 3
2, where c is the smallest distance between any two points in P and
s is the side length of the initial square that contains P.
Proof. When we descend from a node to one of its children, the size of the
corresponding square halves. Hence, the side length of the square of a node at
depth i is s/2i. The maximum distance between two points inside a square is
given by the length of its diagonal, which is s
√
2/2i for the square of a node
at depth i. Since an internal node of a quadtree has at least two points in its
associated square, and the minimum distance between two points is c, the depth
i of an internal node must satisfy
s
√
2/2i ⩾c,
which implies
i ⩽log s
√
2
c
= log(s/c)+ 1
2.
311

Chapter 14
QUADTREES
The lemma now follows from the fact that the depth of a quadtree is exactly one
more than the maximum depth of any internal node.
Figure 14.4
A node at depth i corresponds to a
square of side length s/2i
depth = 3
s
side length = s/8
The size of a quadtree and the construction time are a function of the depth
of the quadtree and the number of points in P.
Theorem 14.2 A quadtree of depth d storing a set of n points has O((d +1)n)
nodes and can be constructed in O((d +1)n) time.
Proof. Every internal node in a quadtree has four children, so the total number
of leaves is one plus three times the number of internal nodes. Hence, it sufﬁces
to bound the number of internal nodes.
Any internal node has one or more points inside its associated square.
Moreover, the squares of the nodes at the same depth in the quadtree are disjoint
and exactly cover the initial square. This means that the total number of internal
nodes at any given depth is at most n. The bound on the size of the quadtree
follows.
The most time-consuming task in one step of the recursive construction
algorithm is the distribution of points over the quadrants of the current square.
Hence, the amount of time spent at an internal node is linear in the number of
points that lie in the associated square. We argued above that the total number
of points associated with nodes at the same depth in the tree is at most n, from
which the time bound follows.
An operation on quadtrees that is often needed is neighbor ﬁnding: given a
node ν and a direction—north, east, south, or west—ﬁnd a node ν′ such that
σ(ν′) is adjacent to σ(ν) in the given direction. Usually the given node is a
leaf, and one wants the reported node to be a leaf as well. This corresponds to
ﬁnding an adjacent square of a given square in the quadtree subdivision. The
algorithm we shall describe is a little bit different: the given node ν can also be
internal, and the algorithm tries to ﬁnd the node ν′ such that σ(ν′) is adjacent
to σ(ν) in the given direction and ν′ and ν are at the same depth. If there is no
such node, then it ﬁnds the deepest node whose square is adjacent. If there is
no adjacent square in the given direction—this can happen if σ(ν) has an edge
contained in an edge of the initial square—then the algorithm reports nil.
The neighbor-ﬁnding algorithm works as follows. Suppose that we want
to ﬁnd the north-neighbor of ν. If ν happens to be the SE- or SW-child of its
312

Section 14.2
QUADTREES FOR POINT SETS
parent, then its north-neighbor is easy to ﬁnd: it is the NE- or NW-child of the
parent, respectively. If ν itself is the NE- or NW-child of its parent, then we
proceed as follows. We recursively ﬁnd the north-neighbor, µ, of the parent of
ν. If µ is an internal node, then the north-neighbor of ν is a child of µ; if µ
σ(ν)
north-neighbor of parent(ν)
is a leaf, then the north-neighbor we seek is µ itself. The pseudocode for this
algorithm is as follows.
Algorithm NORTHNEIGHBOR(ν,T)
Input. A node ν in a quadtree T.
Output. The deepest node ν′ whose depth is at most the depth of ν such that
σ(ν′) is a north-neighbor of σ(ν), and nil if there is no such node.
1.
if ν = root(T) then return nil
2.
if ν = SW-child of parent(ν) then return NW-child of parent(ν)
3.
if ν = SE-child of parent(ν) then return NE-child of parent(ν)
4.
µ ←NORTHNEIGHBOR(parent(ν),T)
5.
if µ = nil or µ is a leaf
6.
then return µ
7.
else if ν = NW-child of parent(ν)
8.
then return SW-child of µ
9.
else return SE-child of µ
This algorithm does not necessarily report a leaf node. If we insist on ﬁnding a
leaf node, then we have to walk down the quadtree from the node found by our
algorithm, always proceeding to a south-child.
The algorithm spends O(1) time at every recursive call. Moreover, at every
call the depth of the argument node ν decreases by one. Hence, the running
time is linear in the depth of the quadtree. We get the following theorem:
Theorem 14.3 Let T be a quadtree of depth d. The neighbor of a given node ν
in T in a given direction, as deﬁned above, can be found in O(d +1) time.
We already observed that a quadtree can be quite unbalanced. As a result,
large squares can be adjacent to many small squares. In some applications—in
particular in the application to meshing—this is unwanted. Therefore we now
discuss a variant of quadtrees, the balanced quadtree, that does not have this
problem.
A quadtree subdivision is called balanced if any two neighboring squares
differ at most a factor two in size. A quadtree is called balanced if its subdivision
is balanced. So in a balanced quadtree any two leaves whose squares are
neighbors can differ at most one in depth. Figure 14.5 shows an example of a
quadtree subdivision that has been made balanced. The original subdivision is
shown solid, and its reﬁnement is dotted.
A quadtree can be made balanced with the following algorithm:
Algorithm BALANCEQUADTREE(T)
Input. A quadtree T.
Output. A balanced version of T.
1.
Insert all the leaves of T into a linear list L.
2.
while L is not empty
313

Chapter 14
QUADTREES
Figure 14.5
A quadtree and its balanced version
balancing
3.
do Remove a leaf µ from L.
4.
if σ(µ) has to be split
5.
then Make µ into an internal node with four children, which are
leaves that correspond to the four quadrants of σ(µ). If µ
stores a point, then store the point in the correct new leaf
instead.
6.
Insert the four new leaves into L.
7.
Check if σ(µ) had neighbors that now need to be split and,
if so, insert them into L.
8.
return T
There are two steps in the algorithm that require some explanation.
First, we have to check whether a given square σ(µ) needs to be split. This
means that we have to check whether σ(µ) is adjacent to a square of less than
half its size. This can be done using the neighbor-ﬁnding algorithm described
earlier, as follows. Suppose that we are looking for a north-neighbor of σ(µ)
that is less than half the size of σ(µ). There is such a square if and only if
NORTHNEIGHBOR(µ,T) reports a node that has a SW-child or a SE-child that
is not a leaf.
Second, we have to check if σ(µ) had neighbors that now need to be split.
Again, this can be done using the neighbor-ﬁnding algorithm: for example,
σ(µ) has such a neighbor to the north if and only if NORTHNEIGHBOR(µ,T)
reports a node whose square is larger than σ(µ).
So now we have an algorithm to make a quadtree balanced. But before we
can analyze the running time of the balancing algorithm we must answer the
following question: what happens to the size of the quadtree when we make
it balanced? From Figure 14.5 we may get the idea that the complexity of a
balanced quadtree subdivision can be quite a lot higher than that of its unbal-
anced version. First of all, large squares adjacent to very small ones get split up
many times. Secondly, the splitting may propagate: sometimes it seems that a
square σ need not be split because its neighbors initially have the right size, but
these neighbors may have to be split so that σ must be split after all. The next
314

Section 14.3
FROM QUADTREES TO MESHES
theorem shows that things are not as bad as appears at ﬁrst sight, and that the
balancing can be done efﬁciently.
Theorem 14.4 Let T be a quadtree with m nodes. Then the balanced version of
T has O(m) nodes and it can be constructed in O((d +1)m) time.
Proof. We ﬁrst prove the bound on the number of nodes. Denote the balanced
version of T by TB. The tree TB is obtained from T by a number of splitting
operations, which replace a leaf by one internal node with four leaves. We shall
prove that only 8m splitting operations are performed. Since a single splitting
operation increases the total number of (internal and leaf) nodes by four, this
proves that the number of nodes in TB is O(m), as claimed.
Call the squares of nodes in T old squares, and the squares of nodes that
are in TB but not in T new squares. Suppose that we have to split an—old or
new—square σ in the balancing process. (The quadrants of σ may have to be
split further, but this will be accounted for separately. Here we only need to
account for the increase in the number of nodes by four due to the splitting
of σ.) Below we shall prove that at least one of the eight equal-sized squares
surrounding σ must be an old square. We charge the splitting of σ to one
of these old square. Every old square—equivalently, every node of T—gets
charged at most eight splittings in this manner, so the total number of splits is at
most 8m, as claimed.
We now prove that for any square that is split in the balancing process, at
σ
σ′
least one of the eight equal-sized squares surrounding it must be an old square.
Suppose there would be squares for which this claim does not hold. Let σ be
the smallest such square. Since σ is split, it must have been adjacent to a small
square, one of less than half its size. Let σ′ be the square that has exactly half
the size of σ and that contains this small square. Since σ′ is contained in a
new square, it is also new itself. Hence, it must have been split in the balancing
process. Now we observe that all eight equal-sized squares surrounding σ′ must
be new, because they are either contained in one the squares surrounding σ
(which are new) or in σ (which is assumed to be split in the balancing process).
Hence, σ′, which is smaller than σ, is a square that is split and none of whose
eight surrounding squares is old. This contradicts the deﬁnition of σ, thus
ﬁnishing the proof of the ﬁrst part of the theorem.
What remains is to prove that BALANCEQUADTREE takes O((d +1)m) time.
The time needed to handle a node µ is O(d +1), because a constant number of
neighbor-ﬁnding operations is performed. Since any node is handled at most
once and the total number of nodes is O(m), the total time is O((d +1)m).
14.3
From Quadtrees to Meshes
We now return to the mesh generation problem. Recall that the input is a square
[0 : U] × [0 : U], where U = 2 j for some positive integer j, with a number of
disjoint polygonal components inside it. The polygon vertices have integer
315

Chapter 14
QUADTREES
coordinates, and the polygon edges have one of four possible orientations: the
angle that any edge makes with the positive x-axis is 0◦, 45◦, 90◦, or 135◦. The
0
U
U
0
goal is to compute a triangular mesh of the square (both outside and inside the
components) that is conforming, respects the input, has well-shaped triangles,
and is non-uniform.
The idea is to use a quadtree subdivision as the ﬁrst step towards a mesh.
When we construct a quadtree on a point set, then we stop the recursive con-
struction when a square contains less than two points. Since we are now dealing
with polygonal input, we have to reformulate the stopping criterion. Because
we want the mesh triangles to be ﬁne near the edges of the components, we
keep on splitting as long as a square is intersected by an edge. More precisely,
the new stopping criterion is this: we stop splitting when the square is no longer
intersected by any edge of any component, or when it has unit size. We consider
the square and the edges to be closed, so that for instance a square that has an
edge contained in one of its sides is intersected by that edge. This stopping
criterion guarantees that the quadtree subdivision will be non-uniform: the edges
of the components will be surrounded by unit size squares, and the squares will
be bigger farther away from the edges.
We claim that the interior of any square in the resulting quadtree subdivision
can be intersected by an edge of a component in only one way: the intersection
must be a diagonal of the square. Indeed, squares whose closure is intersected
have unit size, and the vertices of the components have integers coordinates.
Hence, the interior of a square cannot be intersected by a horizontal or vertical
edge, and the intersection with an edge of orientation 45◦or 135◦must be a
diagonal. It seems that we only have to add diagonals to the squares whose
interior is not intersected to obtain a good mesh. The resulting triangles will
respect the input, they will be well-shaped, and the mesh will be non-uniform.
Unfortunately, the mesh is not conforming. We can remedy this by taking the
subdivision vertices on the sides of the square into account when we triangulate
it, but this leads to another problem: if a square has many vertices on a side,
then not all triangles we get are well-shaped.
To avoid these problems we make the quadtree subdivision balanced before
we triangulate it. Once we have a balanced quadtree subdivision, we can easily
generate a mesh with well-shaped triangles as follows. Squares that do not have
a vertex in the interior of one of their sides (and that are not already triangulated
by an edge of one of the components) are triangulated by adding a diagonal.
Because the subdivision is balanced, the remaining squares have at most one
vertex in the interior of each side. Moreover, this vertex must be in the middle
of the side. Hence, if we add one Steiner point in the center of such a square
and connect it to all vertices on the boundary, then we get only triangles with
two 45◦angles and one 90◦angle.
Summarizing, the mesh generation algorithm is as follows. GENERATEMESH
constructs the mesh in the form of a doubly-connected edge list. We omit the
details related to handling the doubly-connected edge list—in particular, how to
construct the doubly-connected edge list corresponding to a given quadtree.
316

Section 14.3
FROM QUADTREES TO MESHES
Algorithm GENERATEMESH(S)
Input. A set S of components inside the square [0 :U]×[0 :U] with the properties
stated at the beginning of this section.
Output. A triangular mesh M that is conforming, respects the input, consists of
well-shaped triangles, and is non-uniform.
1.
Construct a quadtree T on the set S inside the square [0 : U]×[0 : U] with
the following stopping criterion: a square is split as long as it is larger than
unit size and its closure intersects the boundary of some component.
2.
T ←BALANCEQUADTREE(T)
3.
Construct the doubly-connected edge list for the quadtree subdivision M
corresponding to T.
4.
for each face σ of M
5.
do if the interior of σ is intersected by an edge of a component
6.
then Add the intersection (which is a diagonal) as an edge to M.
7.
else if σ has only vertices at its corners
8.
then Add a diagonal of σ as an edge to M.
9.
else Add a Steiner point in the center of σ, connect it
to all vertices on the boundary of σ, and change M
accordingly.
10.
return M
The following theorem summarizes the properties of the mesh that is constructed
by our algorithm.
Theorem 14.5 Let S be a set of disjoint polygonal components inside the square
[0 : U]×[0 : U] with the properties stated in the beginning of this section. Then
there exists an non-uniform triangular mesh for this input that is conforming,
respects the input, and has only well-shaped triangles. The number of triangles
is O(p(S)logU), where p(S) is the sum of the perimeters of the components in
S, and the mesh can be constructed in O(p(S)log2U) time.
Proof. The properties of the mesh—that it is non-uniform, conforming, that
it respects the input, and consists of well-shaped triangles—follow from the
discussion above. What remains is to bound the size and preprocessing time of
the mesh.
The mesh construction has three phases: ﬁrst we construct a quadtree
subdivision, then we make it balanced, and ﬁnally we triangulate it.
To bound the size of the quadtree subdivision resulting from the ﬁrst phase
we use the following observation. Consider the grid whose cells have unit size.
Then the number of cells whose closure is intersected by a line segment of length
l is at most 4 + 3l/
√
2. It follows that the number of cells whose closure is
intersected by an edge of any of the components is O(p(S)). Obviously the same
is true for any grid with larger cells. Hence, the number of internal nodes of
the quadtree at a ﬁxed depth is O(p(S)). The depth of the quadtree is O(logU),
since we stop splitting once a cell has unit size. The total number of nodes of
the quadtree and, hence, the complexity of the corresponding subdivision, is
therefore O(p(S)logU).
317

Chapter 14
QUADTREES
We know from Theorem 14.4 that making a quadtree subdivision balanced
does not increase its complexity asymptotically. The same is true for the third
phase of the mesh generation, the triangulation of the squares of the balanced
subdivision. (This follows because each cell is subdivided into a constant
number of triangles.) We conclude that the number of triangles of the ﬁnal mesh
is linear in the complexity of the quadtree subdivision resulting from the ﬁrst
phase, which we just proved to be O(p(S)logU).
What remains is to prove the bound on the construction time. The ﬁrst
phase is dominated by the time needed to do the splitting steps in the recursive
quadtree construction algorithm. For a given node this is linear in the number of
component edges intersecting its closure. We argued above that the sum of the
number of intersecting edges over all nodes at a ﬁxed depth is O(p(S)). Hence,
the total time needed in the ﬁrst phase is O(p(S)logU). By Theorem 14.4, the
time needed to do the balancing introduces an extra O(logU) factor. Construct-
ing a doubly-connected edge list from a given quadtree, as well as triangulating
the balanced subdivision, can be done within the same amount of time. The
bound on the preprocessing time follows.
14.4
Notes and Comments
Quadtrees were one of the ﬁrst data structures for higher-dimensional data. They
were developed by Finkel and Bentley in 1974 [177]. Since then, there have
been hundreds of papers dealing with quadtrees. The surveys and books by
Samet [332, 333, 334, 335] and the handbook chapter by Aluru [14] give an
extensive overview of the various types of quadtrees and their applications.
As we have seen in this chapter, the size and depth of a quadtree for a set
of n points cannot be bounded in terms of the number of the number of points
only. The reason is that there can be many nodes with only one non-empty
subtree. If one removes such nodes from the tree, then its size becomes linear.
The resulting structure is called a compressed quadtree—see for example the
survey paper by Aluru [14]. Combining the compressed quadtree with ideas
from skip lists, Eppstein et al. [173] developed a quadtree variant that has not
only linear size, but also allows insertions, deletions, and search operations in
O(logn) time.
Mesh generation is but one application of quadtrees. They are also used in com-
puter graphics, image analysis, geographic information systems, and many other
areas. Quadtrees are often used to answer range queries. From a theoretical point
of view quadtrees are not the best solution to range searching problems, because
usually no sublinear bounds on the query time can be proven. Other solutions
to various range searching problems are described in Chapters 5, 10, and 16.
In practice, quadtrees often seem to perform well. Quadtrees have also been
applied to solve tasks like hidden surface removal, ray tracing, medial axis
transforms, map overlay of raster maps, and nearest neighbor ﬁnding.
Quadtrees can easily be generalized to higher dimensions, where they are
usually called octrees.
318

Section 14.4
NOTES AND COMMENTS
The mesh generation problem is important in many areas and it has been studied
extensively, both in the plane and in 3-dimensional space. The surveys by Bern
and Eppstein [62], Bern and Plasman [65], Bern [61], and Ho-Le [215] are good
starting points to look for results on a speciﬁc setting of the mesh generation
problem. We shall brieﬂy describe a few of the results.
One can distinguish structured and unstructured meshes. Structured meshes
are usually (deformed) grids; unstructured meshes are often triangulations or
quadrilateral meshes. We shall restrict our discussion to unstructured triangu-
lations. Furthermore, we mainly concentrate on the case where the domain
to be meshed is 2-dimensional and polygonal. In most applications there are
requirements on the mesh that are similar to the ones imposed in this chapter.
In particular, one usually requires the mesh to be conforming—sometimes the
term "consistent" is used—and to respect the input. Furthermore, some sort
of well-shapedness is important. This usually means that the triangles have to
satisfy one or both of the following criteria: (i) There should be no small angles,
that is, every angle should be at least some ﬁxed (not too small) constant θ.
This constant should not be larger than the smallest angle of the input domain,
because we cannot avoid the input angles in the mesh. (ii) There should be
no obtuse angles, that is, no angles larger than 90◦. In the example studied in
this chapter we wanted the triangles to satisfy both criteria, and we then took
θ = 45◦. Often the goal is to minimize the number of mesh elements under the
given conditions. This implies some sort of non-uniformity: small triangles
should be used only where needed.
We ﬁrst consider minimizing the number of triangles in the mesh under the
condition that there be no small angles. The number of triangles that we obtain
in a mesh of a polygonal domain under this condition not only depends on the
number of vertices of the domain, it also depends on the shape of the domain.
To see this, we introduce a parameter that is closely related to the minimum
angle of a triangle, namely the aspect ratio of the triangle. This is the ratio of the
length of the longest side of the triangle to the height of the triangle, where the
height of a triangle is the Euclidean distance of the longest edge to its opposite
vertex. If the smallest angle of a triangle is θ then the aspect ratio is between
1/sinθ and 2/sinθ. Now consider a rectangular domain whose shorter sides
have length 1 and whose longer sides have length A. Suppose we require that the
minimum angle be, say, 30◦. This implies that the aspect ratio of any triangle in
the mesh must be less than or equal to 2/sin30◦= 4. Furthermore, the height
of any triangle in the domain is at most one. Hence, the area of any triangle
is O(1). Because the total area of the rectangular domain is A, this implies
that we need at least Ω(A) triangles in the mesh. Bern et al. [64] describe a
method based on quadtrees that produces an asymptotically optimal number
of triangles. The method described in this chapter is based on their technique.
Other mesh-generation algorithms are based on the Delaunay triangulation
described in Chapter 9. See for example the work by Shewchuck [356, 357] for
a discussion of Delaunay-based algorithms in the plane.
If the only requirement is that the triangles in the mesh are non-obtuse then it
turns out to be possible to construct a mesh for a given polygonal domain whose
number of triangles only depends on the number of vertices of the domain.
319

Chapter 14
QUADTREES
More precisely, Bern and Eppstein [63] have shown that for any polygonal
domain with n vertices there is a mesh consisting of O(n2) non-obtuse triangles.
Bern et al. [67] improved this bound to O(n).
Melissaratos and Souvaine [278] extended the approach of Bern et al. [64]
for computing a mesh without small angles so that it also avoids obtuse triangles.
The number of triangles in the mesh is still at most a constant factor from
optimal.
Minimizing the number of triangles is not always the goal of meshing
algorithms. It can also be important to be able to control the mesh density,
so that one can have a dense mesh in interesting areas and a coarse mesh in
uninteresting areas. This is the setting studied by Chew [117]. He describes
a meshing algorithm that allows the user to deﬁne a function that determines
whether a triangle of the mesh is ﬁne enough. The angles of the triangles
produced by his algorithm are between 30◦and 120◦. Another nice aspect of
his work is that the algorithm not only deals with planar regions, but also with
regions on surface patches.
A technique called edge insertion can also give optimal triangulations for
various criteria [66]. The idea is to improve a triangulation in steps by adding
an edge, removing the intersected edges, and retriangulating the polygonal
regions that appear optimally. It works for certain 'minmax' type of criteria,
like minimizing the maximum angle in the triangulation, and minimizing the
maximum slope when the triangulation represents a piecewise linear surface.
14.5
Exercises
14.1
In Figure 14.2 a uniform and a non-uniform mesh are shown for a
unit square in the top left corner of square domain of size 16. Consider
similar meshes in squares of larger sizes U = 2 j for an integer j. Express
the number of triangles in the mesh for both meshes in terms of j.
14.2
Suppose a triangular mesh is needed inside a rectangle whose sides have
length 1 and length k > 1. Steiner points may not be used on the sides,
but they may be used inside the rectangle. Also assume that all triangles
must have angles between 30◦and 90◦. Is it always possible to create a
triangular mesh with these properties? Suppose it is possible to create
a mesh for a particular input, what is the minimum number of Steiner
points needed?
14.3
All triangles produced by the meshing algorithm of this chapter are
non-obtuse, that is, they do not have angles larger than 90◦. Prove that if
a triangulation of a set P of points in the plane contains only non-obtuse
triangles, then it must be a Delaunay triangulation of P.
14.4
Let P be a set of point in 3-dimensional space. Describe an algorithm to
construct an octree on P. (An octree is the 3-dimensional variant of the
quadtree.)
320

Section 14.5
EXERCISES
14.5
It is possible to reduce the size of a quadtree of depth d for a set of
points (with real coordinates) inside a square from O((d +1)n) to O(n).
The idea is to discard any node ν that has only one child under which
points are stored. The node is discarded by replacing the pointer from
the parent of ν to ν by the pointer from the parent to the only interesting
child of ν. Prove that the resulting tree has linear size. Can you also
improve upon the O((d +1)n) construction time?
14.6
In this chapter we called a quadtree balanced if two adjacent squares of
the quadtree subdivision differ by no more than a factor two in size. To
save a constant factor in the number of extra nodes needed to balance a
quadtree, we could weaken the balance condition by allowing adjacent
squares to differ by a factor of four in size. Can you still complete such
a weakly balanced quadtree subdivision to a conforming mesh such that
all angles are between 45◦and 90◦by using only O(1) triangles per
square?
14.7
Suppose we make the balancing condition for quadtrees more severe:
we no longer allow adjacent squares to differ by a factor two in size, but
we require them to have exactly the same size. Is the number of nodes
in the new balanced version still linear in the number of nodes of the
original quadtree? If not, can you say anything about this number?
14.8
The algorithm to construct a balanced quadtree had two phases: a normal
quadtree was constructed, which was then balanced in a postprocessing
step. It is also possible to construct a balanced quadtree without ﬁrst
constructing the unbalanced version. To this end we maintain the current
quadtree subdivision in a doubly-connected edge list during the quadtree
construction, and whenever we split a square we check whether any
neighbors have to be split. Describe the algorithm in detail and analyze
its running time.
14.9
One of the steps in Algorithm GENERATEMESH is to construct a doubly-
connected edge list for the quadtree subdivision of a given quadtree.
Describe an algorithm for this step, and analyze its running time.
14.10 A quadtree can also be used to store a subdivision for efﬁcient point lo-
cation. The idea is to keep splitting a bounding square of the subdivision
until all leaf nodes correspond to squares that contain at most one vertex
and only edges incident to that vertex, or no vertex and at most one edge.
a. Since a vertex can be incident to many edges, we need an additional
data structure at the quadtree leaves storing vertices. Which data
structure would you use?
b. Describe the algorithm for constructing the point location data struc-
ture in detail, and analyze its running time.
c. Describe the query algorithm in detail, and analyze its running time.
14.11 Quadtrees are often used to store pixel images. In this case the initial
square is exactly the size of the image (which is assumed to be a 2k ×2k
321

Chapter 14
QUADTREES
grid for some integer k). A square is split into subsquares if not all pixels
inside have the same intensity.
Prove a bound on the complexity of the quadtree subdivision. Hint: This
is similar to the bound we proved on the size of the quadtree mesh.
14.12 Suppose we have quadtrees on pixel images I1 and I2 (see the previous
exercise). Both images have size 2k ×2k, and contain only two intensi-
ties, 0 and 1. Give algorithms for Boolean operations on these images,
that is, give algorithms to compute a quadtree for I1 ∨I2 and I1 ∧I2.
(Here I1 ∨I2 is the 2k ×2k image where pixel (i, j) has intensity 1 if and
only if (i, j) has intensity 1 in image I1 or in image I2. The image I1 ∧I2
is deﬁned similarly.)
14.13 Quadtrees can be used to perform range queries. Describe an algorithm
for querying a quadtree on a set P of points with a query region R.
Analyze the worst-case query time for the case where R is a rectangle,
and for the case where R is a half-plane bounded by a vertical line.
14.14 In this chapter we studied quadtrees that store a set of point in the
plane. In Chapter 5 we studied two other data structures for storing
sets of points in the plane, the kd-tree and the range tree. Discuss the
advantages and disadvantages of each of the three structures.
322

15
Visibility Graphs
Finding the Shortest Route
In Chapter 13 we saw how to plan a path for a robot from a given start position
to a given goal position. The algorithm we gave always ﬁnds a path if it exists,
but we made no claims about the quality of the path: it could make a large
detour, or make lots of unnecessary turns. In practical situations we would
prefer to ﬁnd not just any path, but a good path.
Figure 15.1
A shortest path
What constitutes a good path depends on the robot. In general, the longer a
path, the more time it will take the robot to reach its goal position. For a mobile
robot on a factory ﬂoor this means it can transport less goods per time unit,
resulting in a loss of productivity. Therefore we would prefer a short path. Often
there are other issues that play a role as well. For example, some robots can only
move in a straight line; they have to slow down, stop, and rotate, before they
can start moving into a different direction, so any turn along the path causes
some delay. For this type of robot not only the path length but also the number
of turns on the path has to be taken into account. In this chapter we ignore
this aspect; we only show how to compute the Euclidean shortest path for a
translating planar robot.
323

Chapter 15
VISIBILITY GRAPHS
15.1
Shortest Paths for a Point Robot
As in Chapter 13 we ﬁrst consider the case of a point robot moving among a
set S of disjoint simple polygons in the plane. The polygons in S are called
obstacles, and their total number of edges is denoted by n. Obstacles are open
sets, so the robot is allowed to touch them. We are given a start position pstart
and a goal position pgoal, which we assume are in the free space. Our goal is to
compute a shortest collision-free path from pstart to pgoal, that is, a shortest path
that does not intersect the interior of any of the obstacles. Notice that we cannot
say the shortest path, because it need not be unique. For a shortest path to exist,
it is important that obstacles are open sets; if they were closed, then (unless the
robot can move to its goal in a straight line) a shortest path would not exist, as it
would always be possible to shorten a path by moving closer to an obstacle.
Let's quickly review the method from Chapter 13. We computed a trapezoidal
map T(Cfree) of the free conﬁguration space Cfree. For a point robot, Cfree was
simply the empty space between the obstacles, so this was rather easy. The key
idea was then to replace the continuous work space, where there are inﬁnitely
many paths, by a discrete road map Groad, where there are only ﬁnitely many
paths. The road map we used was a plane graph with nodes in the centers of the
trapezoids of T(Cfree) and in the middle of the vertical extensions that separate
adjacent trapezoids. The nodes in the center of each trapezoid were connected
to the nodes on its boundary. After ﬁnding the trapezoids containing the start
and goal position of the robot, we found a path in the road map between the
nodes in the centers of these trapezoids by breadth-ﬁrst search.
Figure 15.2
The shortest path does not follow the
road map
pstart
pgoal
shortest path in road map
real shortest path
Because we used breadth-ﬁrst search, the path that is found uses a minimum
number of arcs in Groad. This is not necessarily a short path, because some arcs
are between nodes that are far apart, whereas others are between nodes that
are close to each other. An obvious improvement would be to give each arc
a weight corresponding to the Euclidean length of the segment connecting its
incident nodes, and to use a graph search algorithm that ﬁnds a shortest path in
a weighted graph, such as Dijkstra's algorithm. Although this may improve the
path length, we still do not get the shortest path. This is illustrated in Figure 15.2:
324

Section 15.1
SHORTEST PATHS FOR A POINT ROBOT
the shortest path from pstart to pgoal following the road map passes below the
triangle, but the real shortest path passes above it. What we need is a different
road map, one which guarantees that the shortest path following the road map is
the real shortest path.
Let's see what we can say about the shape of a shortest path. Consider some
pstart
pgoal
path from pstart to pgoal. Think of this path as an elastic rubber band, whose
endpoints we ﬁx at the start and goal position and which we force to take the
shape of the path. At the moment we release the rubber band, it will try to
contract and become as short as possible, but it will be stopped by the obstacles.
The new path will follow parts of the obstacle boundaries and straight line
segments through open space. The next lemma formulates this observation
more precisely. It uses the notion of an inner vertex of a polygonal path, which
is a vertex that is not the begin- or endpoint of the path.
Lemma 15.1 Any shortest path between pstart and pgoal among a set S of disjoint
polygonal obstacles is a polygonal path whose inner vertices are vertices of S.
Proof. Suppose for a contradiction that a shortest path τ is not polygonal. Since
the obstacles are polygonal, this means there is a point p on τ that lies in the
interior of the free space with the property that no line segment containing p
is contained in τ. Since p is in the interior of the free space, there is a disc of
positive radius centered at p that is completely contained in the free space. But
p
short cut
then the part of τ inside the disc, which is not a straight line segment, can be
shortened by replacing it with the segment connecting the point where it enters
the disc to the point where it leaves the disc. This contradicts the optimality of τ,
since any shortest path must be locally shortest, that is, any subpath connecting
points q and r on the path must be the shortest path from q to r.
Now consider a vertex v of τ. It cannot lie in the interior of the free space:
then there would be a disc centered at p that is completely in the free space,
and we could replace the subpath of τ inside the disc— which turns at v—by
a straight line segment which is shorter. Similarly, v cannot lie in the relative
interior of an obstacle edge: then there would be a disc centered at v such that
half of the disc is contained in the free space, which again implies that we
can replace the subpath inside the disc with a straight line segment. The only
possibility left is that v is an obstacle vertex.
With this characterization of the shortest path, we can construct a road map
that allows us to ﬁnd the shortest path. This road map is the visibility graph
of S, which we denote by Gvis(S). Its nodes are the vertices of S, and there is an
arc between vertices v and w if they can see each other, that is, if the segment
vw does not intersect the interior of any obstacle in S. Two vertices that can see
each other are called (mutually) visible, and the segment connecting them is
called a visibility edge. Note that endpoints of the same obstacle edge always
see each other. Hence, the obstacle edges form a subset of the arcs of Gvis(S).
By Lemma 15.1 the segments on a shortest path are visibility edges, except
for the ﬁrst and last segment. To make them visibility edges as well, we add
the start and goal position as vertices to S, that is, we consider the visibility
325

Chapter 15
VISIBILITY GRAPHS
graph of the set S∗:= S∪{pstart, pgoal}. By deﬁnition, the arcs of Gvis(S∗) are
between vertices—which now include pstart and pgoal—that can see each other.
We get the following corollary.
pstart
pgoal
shortest path
Corollary 15.2 The shortest path between pstart and pgoal among a set S of
disjoint polygonal obstacles consists of arcs of the visibility graph Gvis(S∗),
where S∗:= S∪{pstart, pgoal}.
We get the following algorithm to compute a shortest path from pstart to pgoal.
Algorithm SHORTESTPATH(S, pstart, pgoal)
Input. A set S of disjoint polygonal obstacles, and two points pstart and pgoal in
the free space.
Output. The shortest collision-free path connecting pstart and pgoal.
1.
Gvis ←VISIBILITYGRAPH(S∪{pstart, pgoal})
2.
Assign each arc (v,w) in Gvis a weight, which is the Euclidean length of
the segment vw.
3.
Use Dijkstra's algorithm to compute a shortest path between pstart and
pgoal in Gvis.
In the next section we show how to compute the visibility graph in O(n2 logn)
time, where n is the total number of obstacle edges. The number of arcs of
Gvis is of course bounded by
!n+2
2
"
. Hence, line 2 of the algorithm takes O(n2)
time. Dijkstra's algorithm computes the shortest path between two nodes in
graph with k arcs, each having a non-negative weight, in O(nlogn + k) time.
Since k = O(n2), we conclude that the total running time of SHORTESTPATH is
O(n2 logn), leading to the following theorem.
Theorem 15.3 A shortest path between two points among a set of polygonal
obstacles with n edges in total can be computed in O(n2 logn) time.
15.2
Computing the Visibility Graph
Let S be a set of disjoint polygonal obstacles in the plane with n edges in
total. (Algorithm SHORTESTPATH of the previous section needs to compute the
visibility graph of the set S∗, which includes the start and goal position. The
presence of these 'isolated vertices' does not cause any problems and therefore
we do not explicitly deal with them in this section.) To compute the visibility
graph of S, we have to ﬁnd the pairs of vertices that can see each other. This
means that for every pair we have to test whether the line segment connecting
them intersects any obstacle. Such a test would cost O(n) time when done
naively, leading to an O(n3) running time. We will see shortly that the test can
be done more efﬁciently if we don't consider the pairs in arbitrary order, but
concentrate on one vertex at a time and identify all vertices visible from it, as in
the following algorithm.
326

Section 15.2
COMPUTING THE VISIBILITY GRAPH
Algorithm VISIBILITYGRAPH(S)
Input. A set S of disjoint polygonal obstacles.
Output. The visibility graph Gvis(S).
1.
Initialize a graph G = (V,E) where V is the set of all vertices of the
polygons in S and E = /0.
2.
for all vertices v ∈V
3.
do W ←VISIBLEVERTICES(v,S)
4.
For every vertex w ∈W, add the arc (v,w) to E.
5.
return G
The procedure VISIBLEVERTICES has as input a set S of polygonal obstacles
and a point p in the plane; in our case p is a vertex of S, but that is not required.
It should return all obstacle vertices visible from p.
If we just want to test whether one speciﬁc vertex w is visible from p, there is
not much we can do: we have to check the segment pw against all obstacles.
But there is hope if we want to test all vertices of S: we might be able to use
the information we get when we test one vertex to speed up the test for other
vertices. Now consider the set of all segments pw. What would be a good order
to treat them, so that we can use the information from one vertex when we treat
the next one? The logical choice is the cyclic order around p. So what we will
p
do is treat the vertices in cyclic order, meanwhile maintaining information that
will help us to decide on the visibility of the next vertex to be treated.
A vertex w is visible from p if the segment pw does not intersect the interior
of any obstacle. Consider the half-line ρ starting at p and passing through w.
If w is not visible, then ρ must hit an obstacle edge before it reaches w. To
w
p
ρ
check this we perform a binary search with the vertex w on the obstacle edges
intersected by ρ. This way we can ﬁnd out whether w lies behind any of these
edges, as seen from p. (If p itself is also an obstacle vertex, then there is another
case where w is not visible, namely when p and w are vertices of the same
obstacle and pw is contained in the interior of that obstacle. This case can be
checked by looking at the edges incident to w, to see whether ρ is in the interior
of the obstacle before it reaches w. For the moment we ignore degenerate cases,
where one of the incident edges of w is contained in pw.)
e1
e2
e3
e4
e5
e6
e2
e3
e4
e5
e6
e4
e2
e3
e1
e5
e1
Figure 15.3
The search tree on the intersected edges
While treating the vertices in cyclic order around p we therefore maintain
the obstacle edges intersected by ρ in a balanced search tree T. (As we will see
later, edges that are contained in ρ need not be stored in T.) The leaves of T
store the intersected edges in order: the leftmost leaf stores the ﬁrst segment
327

Chapter 15
VISIBILITY GRAPHS
intersected by ρ, the next leaf stores the segment that is intersected next, and
so on. The interior nodes, which guide the search in T, also store edges. More
precisely, an interior node ν stores the rightmost edge in its left subtree, so that
all edges in its right subtree are greater (with respect to the order along ρ) than
this segment eν, and all segments in its left subtree are smaller than or equal
to eν (with respect to the order along ρ). Figure 15.3 shows an example.
Treating the vertices in cyclic order effectively means that we rotate the
half-line ρ around p. So our approach is similar to the plane sweep paradigm we
used at various other places; the difference is that instead of using a horizontal
line moving downward to sweep the plane, we use a rotating half-line.
The status of our rotational plane sweep is the ordered sequence of obstacle
edges intersected by ρ. It is maintained in T. The events in the sweep are the
vertices of S. To deal with a vertex w we have to decide whether w is visible
from p by searching in the status structure T, and we have to update T by
inserting and/or deleting the obstacle edges incident to w.
Algorithm VISIBLEVERTICES summarizes our rotational plane sweep. The
sweep is started with the half-line ρ pointing into the positive x-direction and
proceeds in clockwise direction. So the algorithm ﬁrst sorts the vertices by the
ρ
clockwise angle that the segment from p to each vertex makes with the positive
x-axis. What do we do if this angle is equal for two or more vertices? To be able
to decide on the visibility of a vertex w, we need to know whether pw intersects
the interior of any obstacle. Hence, the obvious choice is to treat any vertices
that may lie in the interior of pw before we treat w. In other words, vertices for
which the angle is the same are treated in order of increasing distance to p. The
algorithm now becomes as follows:
Algorithm VISIBLEVERTICES(p,S)
Input. A set S of polygonal obstacles and a point p that does not lie in the
interior of any obstacle.
Output. The set of all obstacle vertices visible from p.
1.
Sort the obstacle vertices according to the clockwise angle that the half-
line from p to each vertex makes with the positive x-axis. In case of
ties, vertices closer to p should come before vertices farther from p. Let
w1,...,wn be the sorted list.
2.
Let ρ be the half-line parallel to the positive x-axis starting at p. Find
the obstacle edges that are properly intersected by ρ, and store them in a
balanced search tree T in the order in which they are intersected by ρ.
3.
W ←/0
4.
for i ←1 to n
5.
do if VISIBLE(wi) then Add wi to W.
6.
Insert into T the obstacle edges incident to wi that lie on the clock-
wise side of the half-line from p to wi.
7.
Delete from T the obstacle edges incident to wi that lie on the
counterclockwise side of the half-line from p to wi.
8.
return W
328

Section 15.2
COMPUTING THE VISIBILITY GRAPH
The subroutine VISIBLE must decide whether a vertex wi is visible. Normally,
this only involves searching in T to see if the edge closest to p, which is stored
in the leftmost leaf, intersects pwi. But we have to be careful when pwi contains
other vertices. Is wi visible or not in such a case? That depends. See Figure 15.4
for some of the cases that can occur. pwi may or may not intersect the interior
of the obstacles incident to these vertices. It seems that we have to inspect
all edges with a vertex on pwi to decide if wi is visible. Fortunately we have
already inspected them while treating the preceding vertices that lie on pwi. We
can therefore decide on the visibility of wi as follows. If wi−1 is not visible then
wi is not visible either. If wi−1 is visible then there are two ways in which wi
can be invisible. Either the whole segment wi−1wi lies in an obstacle of which
both wi−1 and wi are vertices, or the segment wi−1wi is intersected by an edge
in T. (Because in the latter case this edge lies between wi−1 and wi, it must
properly intersect wi−1wi.) This test is correct because pwi = pwi−1 ∪wi−1wi.
(If i = 1, then there is no vertex in between p and wi, so we only have to look at
the segment pwi.) We get the following subroutine:
wi−1
wi
wi−1
wi
p
p
wi−1
wi
p
wi−1
wi
p
Figure 15.4
Some examples where ρ contains
multiple vertices. In all these cases
wi−1 is visible. In the left two cases wi
is also visible and in the right two cases
wi is not visible.
VISIBLE(wi)
1.
if pwi intersects the interior of the obstacle of which wi is a vertex, locally
at wi
2.
then return false
3.
else if i = 1 or wi−1 is not on the segment pwi
4.
then Search in T for the edge e in the leftmost leaf.
5.
if e exists and pwi intersects e
6.
then return false
7.
else return true
8.
else if wi−1 is not visible
9.
then return false
10.
else Search in T for an edge e that intersects wi−1wi.
11.
if e exists
12.
then return false
13.
else return true
329

Chapter 15
VISIBILITY GRAPHS
This ﬁnishes the description of the algorithm VISIBLEVERTICES to compute
the vertices visible from a given point p.
What is the running time of VISIBLEVERTICES? The time we spent before
line 4 is dominated by the time to sort the vertices in cyclic order around p,
which is O(nlogn). Each execution of the loop involves a constant number
of operations on the balanced search tree T, which take O(logn) time, plus a
constant number of geometric tests that take constant time. Hence, one execution
takes O(logn) time, leading to an overall running time of O(nlogn).
Recall that we have to apply VISIBLEVERTICES to each of the n vertices of S
in order to compute the entire visibility graph. We get the following theorem:
Theorem 15.4 The visibility graph of a set S of disjoint polygonal obstacles
with n edges in total can be computed in O(n2 logn) time.
15.3
Shortest Paths for a Translating Polygonal Robot
In Chapter 13 we have seen that we can reduce the motion planning problem
for a translating, convex, polygonal robot R to the case of a point robot by
computing the free conﬁguration space Cfree. The reduction involves computing
the Minkowski sum of −R, a reﬂected copy of R, with each of the obstacles, and
taking the union of the resulting conﬁguration-space obstacles. This gives us a
Figure 15.5
Computing a shortest path for a
polygonal robot
work space
conﬁguration space
visibility graph
set of disjoint polygons, whose union is the forbidden conﬁguration space. We
can then compute a shortest path with the method we used for a point robot: we
extend the set of polygons with the points in conﬁguration space that correspond
to the start and goal placement, compute the visibility graph of the polygons,
assign each arc a weight which is the Euclidean length of the corresponding
visibility edge, and ﬁnd a shortest path in the visibility graph using Dijkstra's
algorithm.
To what running time does this approach lead? Lemma 13.13 states that
the forbidden space can be computed in O(nlog2 n) time. Furthermore, the
complexity of the forbidden space is O(n) by Theorem 13.12, so from the
330

Section 15.4
NOTES AND COMMENTS
previous section we know that the visibility graph of the forbidden space can be
computed in O(n2 logn) time.
This leads to the following result:
Theorem 15.5 Let R be a convex, constant-complexity robot that can translate
among a set of polygonal obstacles with n edges in total. A shortest collision-
free path for R from a given start placement to a given goal placement can be
computed in O(n2 logn) time.
15.4
Notes and Comments
The problem of computing the shortest path in a weighted graph has been studied
extensively. Dijkstra's algorithm and other solutions are described in most books
on graph algorithms and in many books on algorithms and data structures. In
Section 15.1 we stated that Dijkstra's algorithm runs in O(nlogn+k) time. To
achieve this time bound, one has to use Fibonacci heaps in the implementation.
In our application an O((n+k)logn) algorithm would also do ﬁne, since the
rest of the algorithm needs that much time anyway.
The geometric version of the shortest path problem has also received consid-
erable attention. The algorithm given here is due to Lee [247]. More efﬁcient
algorithms based on arrangements have been proposed; they run in O(n2)
time [23, 158, 383].
Any algorithm that computes a shortest path by ﬁrst constructing the entire
visibility graph is doomed to have at least quadratic running time in the worst
case, because the visibility graph can have a quadratic number of edges. For a
long time no approaches were known with a subquadratic worst-case running
time. Mitchell [281] was the ﬁrst to break the quadratic barrier: he showed that
the shortest path for a point robot can be computed in O(n5/3+ε) time. Later he
improved the running time of his algorithm to O(n3/2+ε) [282]. In the mean
time, however, Hershberger and Suri [210, 212] succeeded in developing an
optimal O(nlogn) time algorithm.
In the special case where the free space of the robot is a polygon without
holes, a shortest path can be computed in linear time by combining the linear-
time triangulation algorithm of Chazelle [94] with the shortest path method of
Guibas et al. [195].
The 3-dimensional version of the Euclidean shortest path problem is much
harder. This is due to the fact that there is no easy way to discretize the problem:
the inﬂection points of the shortest path are not restricted to a ﬁnite set of points,
but they can lie anywhere on the obstacle edges. Canny [80] proved that the
problem of computing a shortest path connecting two points among polyhedral
obstacles in 3-dimensional space is NP-hard. Reif and Storer [327] gave a single-
exponential algorithm for the problem, by reducing it to a decision problem
in the theory of real numbers. There are also several papers that approximate
the shortest path in polynomial time, for example, by adding points on obstacle
edges and searching a graph with these points as nodes [13, 125, 126, 260, 316].
331

Chapter 15
VISIBILITY GRAPHS
In this chapter we concentrated on the Euclidean metric. Various papers deal
with shortest paths under a different metric. Because the number of settings is
quite large, we mention only a few, and we give only a few references for each
setting. An interesting metric that has been studied is the link metric, where
the length of a polygonal path is the number of links it consists of [20, 122,
284, 367]. Another case that has been studied extensively is that of rectilinear
paths. Such paths play an important role in VLSI design, for instance. Lee et
al. [253] give a survey of rectilinear path problems. An interesting metric that
has been studied for rectilinear paths is the combined metric, which is a linear
combination of the Euclidean metric and the link metric [56]. Finally, there
are papers that consider paths in a subdivision where each region has a weight
associated with it. The cost of a path through a region is then its Euclidean
length times the weight of the region. Obstacles can be modeled by regions
with inﬁnite weight [113, 283].
While there are many obvious metrics for translating robots—in particular,
the Euclidean metric immediately comes to mind—it is not so easy to give a
good deﬁnition of a shortest path for a robot that can rotate as well as trans-
late. Some results have been obtained for the case where the robot is a line
segment [24, 114, 218].
The visibility graph was introduced for planning motions by Nilson [295]. The
O(n2 logn) time algorithm that we described to compute it is due to Lee [247]. A
number of faster algorithms are known [23, 383], including an optimal algorithm
by Ghosh and Mount [190], which runs in O(nlogn+k) time, where k is the
number of arcs of the visibility graph.
To compute a shortest path for a point robot among a set of convex polygonal
obstacles, one does not need all the visibility edges. One only needs the visibility
edges that deﬁne a common tangent. Rohnert [329] gave an algorithm that
computes this reduced visibility graph in time O(n+c2 logn), where c is the
number of obstacles, and n is their total number of edges.
The visibility complex, introduced by Vegter and Pocchiola [319, 320, 376]
is a structure that has the same complexity as the visibility graph, but contains
more information. It is deﬁned on a set of convex (not necessarily polygonal)
objects in the plane, and can be used for shortest path problems and ray shooting.
It can be computed in O(nlogn + k) time. Wein et al. [382] introduced an
interesting variant of this, the visibility-Voronoi complex, which combines the
visibility complex with the Voronoi diagram of the obstacles. This allows one
to ﬁnd short paths that do not come too close to the obstacles.
15.5
Exercises
15.1
Let S be a set of disjoint simple polygons in the plane with n edges in
total. Prove that for any start and goal position the number of segments
on the shortest path is bounded by O(n). Give an example where it
is Θ(n).
332

Section 15.5
EXERCISES
15.2
Algorithm VISIBILITYGRAPH calls algorithm VISIBLEVERTICES with
each obstacle vertex. VISIBLEVERTICES sorts all vertices around its
input point. This means that n cyclic sortings are done, one around each
obstacle vertex. In this chapter we simply did every sort in O(nlogn)
time, leading to O(n2 logn) time for all sortings. Show that this can be
improved to O(n2) time using dualization (see Chapter 8). Does this
improve the running time of VISIBILITYGRAPH?
15.3
The algorithm for ﬁnding the shortest path can be extended to objects
other than polygons. Let S be a set of n disjoint disc-shaped obstacles,
not necessarily of equal radius.
a. Prove that in this case the shortest path between two points not seeing
each other consists of parts of boundaries of the discs, and/or common
tangents of discs, and/or tangents from the start or goal point to the
discs.
b. Adapt the notion of a visibility graph for this situation.
c. Adapt the shortest path algorithm to ﬁnd the shortest path between
two points among the discs in S.
15.4
What is the maximal number of shortest paths connecting two ﬁxed
points among a set of n triangles in the plane?
15.5
Let S be a set of disjoint polygons and let a starting point pstart be given.
We want to preprocess the set S (and pstart) such that for different goal
points we can efﬁciently ﬁnd the shortest path from pstart to the goal.
Describe how the preprocessing can be done in time O(n2 logn) such
that for any given goal point pgoal we can compute the shortest path from
pstart to pgoal in time O(nlogn).
15.6
Design an algorithm to ﬁnd a shortest paths between two points inside a
simple polygon. Your algorithm should run in subquadratic time.
15.7
When all obstacles are convex polygons we can improve the shortest
path algorithm by only considering common tangents rather than all
visibility edges.
a. Prove that the only visibility edges that are required in the shortest
path algorithm are the common tangents of the polygons.
b. Give a fast algorithm to ﬁnd the common tangents of two disjoint
convex polygons.
c. Give an algorithm to compute those common tangents that are also
visibility edges among a set of convex polygons.
15.8* If you are familiar with homogeneous coordinates, it is interesting to see
that the rotational sweep that we used in this chapter can be transformed
into a normal plane sweep using a horizontal line that translates over the
plane. Show that this is the case using a projective transformation that
moves the center of the sweep to a point at inﬁnity.
333

16
Simplex Range Searching
Windowing Revisited
In Chapter 2 we saw that geographic information systems often store a map in a
number of separate layers. Each layer represents a theme from the map, that is,
a speciﬁc type of feature such as roads or cities. Distinguishing layers makes it
easy for the user to concentrate her attention on a speciﬁc feature. Sometimes
one is not interested in all the features of a given type, but only in the ones lying
inside a certain region. Chapter 10 contains an example of this: from a road
map of the whole of the U.S.A. we wanted to select the part lying inside a much
smaller region. There the query region, or window, was rectangular, but it is
easy to imagine situations where the region has a different shape. Suppose that
affected area
Figure 16.1
Population density of the Netherlands
we have a map layer whose theme is population density. The density is shown
on the map by plotting a point for every 5,000 people, say. An example of such
a map is given in Figure 16.1. If we want to estimate the impact of building, say,
a new airport at a given location, it is useful to know how many people live in
the affected area. In geometric terms we have a set of points in the plane and
we want to count the points lying inside a query region (for instance, the region
within which the noise of planes exceeds a certain level).
In Chapter 5, where we studied database queries, we developed a data
335

Chapter 16
SIMPLEX RANGE SEARCHING
structure to report the points inside an axis-parallel query rectangle. The area
affected by the airport, however, is determined by the dominating direction of
the wind, and unlikely to be rectangular, so the data structures from Chapter 5
are of little help here. We need to develop a data structure that can handle more
general query ranges.
16.1
Partition Trees
Given a set of points in the plane, we want to count the points lying inside a
query region. (Here and in the following we follow the convention of using the
expression "to count the points" in the sense of "to report the number of points",
not in the sense of enumerating the points.) Let's assume that the query region
is a simple polygon; if it is not, we can always approximate it. To simplify
the query answering algorithm we ﬁrst triangulate the query region, that is, we
decompose it into triangles. Chapter 3 describes how to do this. After we have
triangulated the region, we query with each of the resulting triangles. The set
of points lying inside the region is just the union of the sets of points inside
the triangles. When counting the points we have to be a bit careful with points
lying on the common boundary of two triangles, but that is not difﬁcult to take
care of.
h
We have arrived at the triangular range searching problem: given a set S of n
points in the plane, count the points from S lying in a query triangle t. Let's
ﬁrst study a slightly simpler version of this problem, where the query triangle
degenerates into a half-plane. What should a data structure for half-plane range
queries look like? As a warm-up, let's look at the one-dimensional problem
ﬁrst: in the one-dimensional version of the problem we are given a set of n
points on the real line, and we want to count the points in a query half-line
(that is, the points lying on a speciﬁed side of a query point). Using a balanced
binary search tree where every node also stores the number of points in its
subtree, we can answer such queries in O(logn) time. How can we generalize
this to a 2-dimensional setting? To answer this question we must ﬁrst interpret
a balanced search tree geometrically. Each node of the tree contains a key—the
coordinate of a point—that is used to split the point set into the sets that are
stored in the left and right subtrees. Similarly, we can consider this key value to
split the real line into two pieces. In this way, every node of the tree corresponds
to a region on the line—the root to the whole line, the two children of the
root to two half-lines, and so on. For any query half-line and any node, the
region of one child of the node is either completely contained in the half-line or
completely disjoint from it. All points in that region are in the half-line, or none
of them is. Hence, we only have to search recursively in the other subtree of
the node. This is shown in Figure 16.2. The points are drawn below the tree as
black dots; the two regions of the real line corresponding to the two subtrees are
also indicated. The query half-line is the grey region. The region corresponding
to the subtree rooted at the black node is completely inside the query half-line.
Hence, we only have to recurse on the right subtree.
336

Section 16.1
PARTITION TREES
recursively visited subtree
Figure 16.2
Answering a half-line query with a
binary tree
To generalize this to two dimensions we could try to partition the plane
into two regions, such that for any query half-plane there is one region that
is either completely contained in the half-plane or completely disjoint from
it. Unfortunately, such a partitioning does not exist, so we need a further
generalization: instead of partitioning into two regions, we must partition into
more regions. The partitioning should be such that for any query half-plane we
have to search recursively in only few of the regions.
We now give a formal deﬁnition of the type of partitioning we need. A sim-
plicial partition for a set S of n points in the plane is a collection Ψ(S) :=
{(S1,t1),...,(Sr,tr)}, where the Si's are disjoint subsets of S whose union is S,
and ti is a triangle containing Si. The subsets Si are called classes. We do not
require the triangles to be disjoint, so a point of S may lie in more than one
triangle. Still, such a point is a member of only one class. We call r, the number
of triangles, the size of Ψ(S). Figure 16.3 gives an example of a simplicial
partition of size ﬁve; different shades of grey are used to indicate the different
classes. We say that a line ℓcrosses a triangle ti if ℓintersects the interior
ℓ
Figure 16.3
A ﬁne simplicial partition
of ti. When the point set S is not in general position we sometimes need to
use (relatively open) segments as well as triangles in the simplicial partition.
A line is deﬁned to cross a segment when it intersects its relative interior but
does not contain it. The crossing number of a line ℓwith respect to Ψ(S) is
the number of triangles of Ψ(S) crossed by ℓ. Thus the crossing number of the
line ℓin Figure 16.3 is two. The crossing number of Ψ(S) is the maximum
crossing number over all possible lines ℓ. In Figure 16.3 you can ﬁnd lines
337

Chapter 16
SIMPLEX RANGE SEARCHING
that intersect four triangles, but no line intersects all ﬁve. Finally, we say that
a simplicial partition is ﬁne if |Si| ⩽2n/r for every 1 ⩽i ⩽r. In other words,
in ﬁne simplicial partitions none of the classes contains more than twice the
average number of points of the classes.
Now that we have formalized the notion of partition, let's see how we can
use such a partition to answer half-plane range queries. Let h be the query
half-plane. If a triangle ti of the partition is not crossed by the bounding line of
h, then its class Si either lies completely in h, or it is completely disjoint from h.
This means we only have to recurse on the classes Si for which ti is crossed by
the bounding line of h. For example, if in Figure 16.3 we queried with ℓ+, the
half-plane lying above ℓ, we would have to recurse on two of the ﬁve classes.
The efﬁciency of the query answering process therefore depends on the crossing
number of the simplicial partition: the lower the crossing number, the better
the query time. The following theorem states that it is always possible to ﬁnd a
simplicial partition with crossing number O(√r); later we shall see what this
implies for the query time.
Theorem 16.1 For any set S of n points in the plane, and any parameter r with
1 ⩽r ⩽n, a ﬁne simplicial partition of size r and crossing number O(√r)
exists. Moreover, for any ε > 0 such a simplicial partition can be constructed in
time O(n1+ε).
It seems a bit strange to claim a construction time of O(n1.1) or O(n1.01) or
with the exponent even closer to 1. Still, no matter how small ε is, as long as
it is a positive constant, the bound in the theorem can be attained. But a better
upper bound like O(n) or O(nlogn) isn't claimed in the theorem.
Section 16.4 gives pointers to the literature where a proof of this theorem
can be found. We shall take the theorem for granted, and concentrate on how to
use it in the design of an efﬁcient data structure for half-plane range queries. The
data structure we'll obtain is called a partition tree. Probably you can already
Figure 16.4
A simplicial partition and the
corresponding tree
guess what such a partition tree looks like: it's a tree whose root has r children,
each being the root of a recursively deﬁned partition tree for one of the classes
in a simplicial partition. There is no speciﬁc order on the children; it happens to
be irrelevant. Figure 16.4 shows a simplicial partition and the corresponding
tree. The dotted triangles form the partition computed recursively for the class
corresponding to the middle child of the root; the ﬁve "subclasses" are stored
in ﬁve subtrees below the middle child. Depending on the application, we also
338

Section 16.1
PARTITION TREES
store some extra information about the classes. The basic structure of a partition
tree is thus as follows:
If S contains only one point, p, the partition tree consists of a single leaf
where p is stored explicitly. The set S is the canonical subset of the leaf.
Otherwise, the structure is a tree T of branching degree r, where r is a
sufﬁciently large constant. (Below we shall see how r should be chosen.)
The children of the root of the tree are in one-to-one correspondence with the
triangles of a ﬁne simplicial partition of size r for the set S. The triangle of
the partition corresponding to child ν is denoted by t(ν). The corresponding
class in S is called the canonical subset of ν; it is denoted S(ν). The child
ν is the root of a recursively deﬁned partition tree Tν on the set S(ν).
With each child ν we store the triangle t(ν). We also store information
about the subset S(ν); for half-plane range counting this information is the
cardinality of S(ν), but for other applications we may want to store other
information.
We can now describe the query algorithm for counting the number of points
from S in a query half-plane h. The algorithm returns a set ϒ of nodes from the
partition tree T, called the selected nodes, such that the subset of points from
S lying in h is the disjoint union of the canonical subsets of the nodes in ϒ. In
other words, ϒ is a set of nodes whose canonical subsets are disjoint, and such
that
S∩h =
!
ν∈ϒ
S(ν).
The selected nodes are exactly the nodes ν with the property: t(ν) ⊂h (or, in
case ν is a leaf, the point stored at ν lies in h) and there is no ancestor µ of ν
such that t(µ) ⊂h. The number of points in h can be computed by summing
the cardinalities of the selected canonical subsets.
Algorithm SELECTINHALFPLANE(h,T)
Input. A query half-plane h and a partition tree or subtree of it.
Output. A set of canonical nodes for all points in the tree that lie in h.
1.
ϒ ←/0
2.
if T consists of a single leaf µ
3.
then if the point stored at µ lies in h then ϒ ←{µ}
4.
else for each child ν of the root of T
5.
do if t(ν) ⊂h
6.
then ϒ ←ϒ ∪{ν}
7.
else if t(ν)∩h ̸= /0
8.
then ϒ ←ϒ ∪SELECTINHALFPLANE(h,Tν)
9.
return ϒ
Figure 16.5 illustrates the operation of the query algorithm. The selected chil-
dren of the root are shown in black. The children that are visited recursively (as
well as the root itself, since it has also been visited) are grey. As said before, a
339

Chapter 16
SIMPLEX RANGE SEARCHING
Figure 16.5
Answering a half-plane range query
using a partition tree
recursively visited subtrees
= visited node
ν1
ν2
ν3
ν4
ν7
ν6
ν5
h
t(ν1)
t(ν2)
t(ν3)
t(ν4)
t(ν6)
t(ν7)
t(ν5)
= selected node
half-plane range counting query can be answered by calling SELECTINHALF-
PLANE and summing the cardinalities of the selected nodes, which are stored
at the nodes. In practice, one would probably not keep track of the set ϒ, but
one would maintain a counter; when a node is selected, the cardinality of its
canonical subset is added the counter.
We have described the partition tree, a data structure for half-plane range
counting, and its query algorithm. Now it's time to analyze our structure. We
start with the amount of storage.
Lemma 16.2 Let S be a set of n points in the plane. A partition tree on S uses
O(n) storage.
Proof. Let M(n) be the maximum number of nodes that a partition tree on a set
of n points can have, and let nν denote the cardinality of the canonical subset
S(ν). Then M(n) satisﬁes the following recurrence:
M(n) ⩽
"
1
if n = 1,
1+∑ν M(nν)
if n > 1,
where we sum over all children ν of the root of the tree. Because the classes in
a simplicial partition are disjoint, we have ∑ν nν = n. Furthermore, nν ⩽2n/r
for all ν. Hence, for any constant r > 2 the recurrence solves to M(n) = O(n).
The storage needed for a single node of the tree is O(r). Since r is a constant,
the lemma follows.
Linear storage is the best one could hope for, but what about the query time?
Here it becomes important what the exact value of r is. When we perform a
query we have to recurse in at most c√r subtrees, for some constant c that does
340

Section 16.1
PARTITION TREES
not depend on r or n. It turns out that this constant has an inﬂuence on the
exponent of n in the query time. To decrease this inﬂuence, we need to make r
large enough, and then, as we will see, we can get a query time close to O(√n).
Lemma 16.3 Let S be a set of n points in the plane. For any ε > 0, there is a
partition tree for S such that for a query half-plane h we can select O(n1/2+ε)
nodes from the tree with the property that the subset of points from S in h is the
disjoint union of the canonical subsets of the selected nodes. The selection of
these nodes takes O(n1/2+ε) time. As a consequence, half-plane range counting
queries can be answered in O(n1/2+ε) time.
Proof. Let ε > 0 be given. According to Theorem 16.1 there is a constant c
such that for any parameter r we can construct a simplicial partition of size r
with crossing number at most c√r. We base the partition tree on simplicial
partitions of size r := ⌈2(c
√
2)1/ε⌉. Let Q(n) denote the maximum query time
for any query in a tree for a set of n points. Let h be a query half-plane, and let
nν denote the cardinality of the canonical subset S(ν). Then Q(n) satisﬁes the
following recurrence:
Q(n) ⩽
" 1
if n = 1,
r +∑ν∈C(h) Q(nν)
if n > 1,
where we sum over the set C(h) of all children ν of the root such that the
boundary of h crosses t(ν). Because the simplicial partition underlying the data
structure has crossing number c√r, we know that the number of nodes in the
set C(h) is at most c√r. We also know that nν ⩽2n/r for each ν, because the
simplicial partition is ﬁne. Using these two facts one can show that with our
choice of r the recurrence for Q(n) solves to O(n1/2+ε).
You may be a bit disappointed by the query time: the query time of most
geometric data structures we have seen up to now is O(logn) or a polynomial in
logn, whereas the query time for the partition tree is around O(√n). Apparently
this is the price we have to pay if we want to solve truly 2-dimensional query
problems, such as half-plane range counting. Is it impossible to answer such
queries in logarithmic time? No: later in this chapter we shall design a data
structure for half-plane range queries with logarithmic query time. But the
improvement in query time will not come for free, as that data structure will
need quadratic storage.
It is helpful to compare the approach we have taken here with the range trees
from Chapter 5 and the segment trees from Chapter 10. In these data structures,
we would like to return information about a subset of a given set of geometric
objects (points in range trees and partition trees, intervals in segment trees), or
to report the subset itself. If we could precompute the requested information
for every possible subset that can appear in a query, queries could be answered
very fast. However, the number of possible different answers often prohibits
such an approach. Instead, we identiﬁed what we have called canonical subsets,
and we precomputed the required information for these subsets only. A query is
341

Chapter 16
SIMPLEX RANGE SEARCHING
then solved by expressing the answer to the query as the disjoint union of some
of these canonical subsets. The query time is roughly linear in the number of
canonical subsets that are required to express any possible query subset. The
storage is proportional to the total number of precomputed canonical subsets
for range trees and partition trees, and proportional to the sum of the sizes
of the precomputed canonical subsets for segment trees. There is a trade-off
between query time and storage: to make sure that every possible query can
be expressed as the union of only a few canonical subsets, we need to provide
a large repertoire of such subsets, and need a lot of storage. To decrease the
storage, we need to decrease the number of precomputed canonical subsets—but
that may mean that the number of canonical subsets needed to express a given
query will be larger, and the query time increases.
This phenomenon can be observed clearly for 2-dimensional range search-
ing: the partition tree we constructed in this section provides a repertoire of
only O(n) canonical subsets, and needs only linear storage, but in general one
needs Ω(√n) canonical subsets to express the set of points lying in a half-plane.
Only by providing roughly a quadratic number of canonical subsets can one
achieve logarithmic query time.
Now let's go back to the problem that we wanted to solve, namely triangular
range queries. Which modiﬁcations do we need if we want to use partition trees
for triangles instead of half-planes as query regions? The answer is simple:
none. We can use exactly the same data structure and query algorithm, with the
query half-plane replaced by a query triangle. In fact, the solution works for
any query range γ. The only question is what happens to the query time.
When the query algorithm visits a node, there are three types of children: the
children ν for which t(ν) lies completely inside the query range, the children for
which t(ν) lies outside the range, and the children for which t(ν) lies partially
inside the query range. Only the children of the third type have to be visited
recursively. The query time therefore depends on the number of triangles in the
partition that are crossed by the boundary of the query range γ. In other words,
we have to see what the crossing number of γ is with respect to the simplicial
partition. For a triangular query region, this is easy: a triangle in the partition
is crossed by the boundary of γ only if it is crossed by one of the three lines
through the edges of γ. Since each one of these lines intersects at most c√r
triangles, the crossing number of γ is at most 3c√r.
The recursion for the query time therefore remains nearly the same, only
the constant c changes to 3c. As a result, we will need to choose r larger, but in
the end the query time remains asymptotically the same. We get the following
theorem:
Theorem 16.4 Let S be a set of n points in the plane. For any ε > 0, there is a
data structure for S, called a partition tree, that uses O(n) storage, such that the
points from S lying inside a query triangle can be counted in O(n1/2+ε) time.
The points can be reported in O(k) additional time, where k is the number of
reported points. The structure can be constructed in O(n1+ε) time.
342

Section 16.2
MULTI-LEVEL PARTITION TREES
Proof. The only two issues that have not been discussed yet are the construction
time and the reporting of points.
Constructing a partition tree is easy: the recursive deﬁnition given before
immediately implies a recursive construction algorithm. We denote the time this
algorithm needs to construct a partition tree for a set of n points by T(n). Let
ε > 0 be given. According to Theorem 16.1 we can construct a ﬁne simplicial
partition for S of size r with crossing number O(√r) in time O(n1+ε′), for any
ε′ > 0. We let ε′ = ε/2. Hence, T(n) satisﬁes the recurrence
T(n) =
" O(1)
if n = 1,
O(n1+ε/2)+∑ν T(nν)
if n > 1,
where we sum over all children ν of the root of the tree. Because the classes in
a simplicial partition are disjoint, we have ∑ν nν = n, and the recurrence solves
to T(n) = O(n1+ε).
It remains to show that the k points in a query triangle can be reported in
O(k) additional time. These points are stored in the leaves below the selected
nodes. Hence, they can be reported by traversing the subtrees rooted at the
selected nodes. Because the number of internal nodes of a tree is linear in the
number of leaves of that tree when each internal node has degree at least two,
this takes time linear in the number of reported points.
16.2
Multi-Level Partition Trees
Partition trees are powerful data structures. Their strength is that the points
lying in a query half-plane can be selected in a small number of groups, namely,
the canonical subsets of the nodes selected by the query algorithm. In the
example above we used a partition tree for half-plane range counting, so the
only information that we needed about the selected canonical subsets was their
cardinality. In other query applications we will need other information about
the canonical subsets, so we have to precompute and store that data. The
information that we store about a canonical subset does not have to be a single
number, like its cardinality. We can also store the elements of the canonical
subset in a list, or a tree, or any kind of data structure we like. This way we get
a multi-level data structure. The concept of multi-level data structures is not
new: we already used it in Chapter 5 to answer multi-dimensional rectangular
range queries and in Chapter 10 for windowing queries.
We now give an example of a multi-level data structure based on partition trees.
pleft(s)
pright(s)
pright(s)
pleft(s)
pleft(s)
pright(s)
Let S be a set of n line segments in the plane. We want to count the number of
segments intersected by a query line ℓ. Let pright(s) and pleft(s) denote the right
and left endpoint of a segment s, respectively. A line ℓintersects s if and only if
either the endpoints of s lie to distinct sides of ℓ, or s has an endpoint on ℓ. We
show how to count the number of segments s ∈S with pright(s) lying above ℓ
and pleft(s) lying below ℓ. The segments with an endpoint on ℓ, and the ones
with pright(s) lying below ℓand pleft(s) above ℓ, can be counted with a similar
343

Chapter 16
SIMPLEX RANGE SEARCHING
data structure. Here we choose—for a vertical line ℓ—the left side to be below
ℓand the right side above ℓ.
The idea of the data structure is simple. We ﬁrst ﬁnd all the segments s ∈S
such that pright(s) lies above ℓ. In the previous section we saw how to use a
partition tree to select these segments in a number of canonical subsets. For each
of the selected canonical subsets we are interested in the number of segments s
with pleft(s) below ℓ. This is a half-plane range counting query, which can be
answered if we store each canonical subset in a partition tree. Let's describe
this solution in a little more detail. The data structure is deﬁned as follows.
For a set S′ of segments, let Pright(S′) := {pright(s) : s ∈S′} be the set of right
endpoints of the segments in S′, and let Pleft(S′) := {pleft(s) : s ∈S′} be the set
of left endpoints of the segments in S′.
The set Pright(S) is stored in a partition tree T. The canonical subset of a
node ν of T is denoted Pright(ν). The set of segments corresponding to the
left endpoints in Pright(ν) is denoted S(ν), that is, S(ν) = {s : pright(s) ∈
Pright(ν)}. (Abusing the terminology slightly, we sometimes call S(ν) the
canonical subset of ν.)
With each node ν of the ﬁrst-level tree T, we store the set Pleft(S(ν)) in
a second-level partition tree Tassoc
ν
for half-plane range counting. This
partition tree is the associated structure of ν.
With this data structure we can select the segments s ∈S with pright(s) above ℓ
and pleft(s) below ℓin a number of canonical subsets. The query algorithm for
this is described below. To count the number of such segments, all we have to
do is sum the cardinalities of the selected subsets. Let Tν denote the subtree of
T rooted at ν.
Algorithm SELECTINTSEGMENTS(ℓ,T)
Input. A query line ℓand a partition tree or subtree of it.
Output. A set of canonical nodes for all segments in the tree that are intersected
by ℓ.
1.
ϒ ←/0
2.
if T consists of a single leaf µ
3.
then if the segment stored at µ intersects ℓthen ϒ ←{µ}
4.
else for each child ν of the root of T
5.
do if t(ν) ⊂ℓ+
6.
then ϒ ←ϒ ∪SELECTINHALFPLANE(ℓ−,Tassoc
ν
)
7.
else if t(ν)∩ℓ̸= /0
8.
then ϒ ←ϒ ∪SELECTINTSEGMENTS(ℓ,Tν)
9.
return ϒ
The query algorithm just given can ﬁnd the segments with the right endpoint
above the query line and the left endpoint below it. Interestingly, the same
partition tree can be used to ﬁnd the segments with the left endpoint above the
query line and the right endpoint below it. Only the query algorithm has to be
changed: exchange the "ℓ+" and the "ℓ−" and we are done.
344

Section 16.2
MULTI-LEVEL PARTITION TREES
Let's analyze our multi-level partition tree for segment intersection selection.
We start with the amount of storage.
Lemma 16.5 Let S be a set of n segments in the plane. A two-level partition
tree for segment intersection selection queries in S uses O(nlogn) storage.
Proof. Let nν denote the cardinality of the canonical subset S(ν) in the ﬁrst-
level partition tree. The storage for this node consists of a partition tree for Sν,
and as we know from the previous section, it needs linear storage. Hence, the
storage M(n) for a two-level partition tree on n segments satisﬁes the recurrence
M(n) =
"
O(1)
if n = 1,
∑ν[O(nν)+M(nν)]
if n > 1,
where we sum over all children ν of the root of the tree. We know that ∑ν nν = n
and nν ⩽2n/r. Since r > 2 is a constant the recurrence for M(n) solves to
M(n) = O(nlogn).
Adding a second level to the partition tree has increased the amount of
storage by a logarithmic factor. What about the query time? Surprisingly, the
asymptotic query time does not change at all.
Lemma 16.6 Let S be a set of n segments in the plane. For any ε > 0, there
is a two-level partition tree for S such that for a query line ℓwe can select
O(n1/2+ε) nodes from the tree with the property that the subset of segments
from S intersected by ℓis the disjoint union of the canonical subsets of the
selected nodes. The selection of these nodes takes O(n1/2+ε) time. As a
consequence, the number of intersected segments can be counted in O(n1/2+ε)
time.
Proof. Again we use a recurrence to analyze the query time. Let ε > 0 be given.
Let nν denote the cardinality of the canonical subset S(ν). Lemma 16.3 tells us
that we can construct the associated structure T assoc
ν
of node ν in such a way that
the query time in T assoc
ν
is O(n1/2+ε
ν
). Now consider the full two-level tree T on
S. We base this tree on a ﬁne simplicial partition of size r with crossing number
at most c√r, for r := ⌈2(c
√
2)1/ε⌉; such a partition exists by Theorem 16.1. Let
Q(n) denote the query time in the two-level tree for a set of n segments. Then
Q(n) satisﬁes the recurrence:
Q(n) =
#
O(1)
if n = 1,
O(rn1/2+ε)+∑
c√r
i=1 Q(2n/r)
if n > 1.
With our choice of r the recurrence for Q(n) solves to O(n1/2+ε). This bound
on the query time immediately implies the bound on the number of selected
canonical subsets.
345

Chapter 16
SIMPLEX RANGE SEARCHING
16.3
Cutting Trees
In the previous sections we have solved planar range searching problems with
partition trees. The storage requirements of partition trees are good: they use
roughly linear storage. The query time, however, is O(n1/2+ε), and this is rather
high. Can we achieve a better query time, for example O(logn), if we are
willing to spend more than linear storage? To have any hope of success, we
must abandon the approach of using simplicial partitions: it is not possible to
construct simplicial partitions with less than O(√r) crossing number, which
would be needed to achieve a query time faster than O(√n).
Figure 16.6
Half-plane range counting in the dual
plane: how may lines are below a
query point?
q
To come up with a new approach to the problem, we need to look at it in
a different light. We apply the duality transform from Chapter 8. The ﬁrst
problem we solved in Section 16.1 was the half-plane range counting problem:
given a set of points, count the number of points lying in a query half-plane.
Let's see what we get when we look at this problem in the dual plane. Assume
that the query half-plane is positive, that is, we want to count the points above
the query line. In the dual plane we then have the following setting: given a
set L of n lines in the plane, count the number of lines below a query point q.
With the tools we constructed in the previous chapters it is easy to design a data
structure with logarithmic query time for this problem: the key observation is
that the number of lines below the query point q is uniquely determined by the
face of the arrangement A(L) that contains q. Hence, we can construct A(L)
and preprocess it for point location queries, as described in Chapter 6, and store
with each face the number of lines below it. Counting the number of lines below
a query point now boils down to doing point location. This solution uses O(n2)
storage and it has O(logn) query time.
Note that this was a situation where we could afford to precompute the
answer for every possible query—in other words, the collection of canonical
subsets consists of all possible subsets that can appear. But if we go to triangular
range counting, this approach is not so good: there are just too many possible
triangles to precompute all possible answers. Instead, we'll try to express the
set of lines below a query point by a small number of canonical subsets in a
recursive way. We can then use the multi-level approach from the previous
section to solve the triangular range searching problem.
346

Section 16.3
CUTTING TREES
We construct the whole collection of canonical subsets using a data structure
called a cutting tree. The idea behind cutting trees is the same as for partition
trees: the plane is partitioned into triangular regions, as depicted in Figure 16.7.
This time, however, we require that the triangles be disjoint. How can such
Figure 16.7
A (1/2)-cutting of size ten for a set of
six lines
a partitioning help to count the number of lines below a query point? Let
L := {ℓ1,ℓ2,...,ℓn} be the set of lines that we obtained after dualizing the
points to be preprocessed for triangular range queries. Consider a triangle t of
the partitioning, and a line ℓi that does not intersect t. If ℓi lies below t, then ℓi
lies below any query point inside t. Similarly, if ℓi lies above t, it lies above
any query point inside t. This means that if our query point q lies in t, then
the only lines of which we don't know yet whether they lie above or below q
are the ones that intersect t. Our data structure will store each triangle of the
partitioning, with a counter indicating the number of lines below it; for each
triangle we also have a recursively deﬁned structure on the lines intersecting
it. To query in this structure, we ﬁrst determine in which triangle t the query
point q falls. We then compute how many lines from the ones that intersect t are
below q, by recursively visiting the subtree corresponding to t. Finally, we add
the number we computed in the recursive call to the number of lines below t.
The efﬁciency of this approach depends on the number of lines intersecting a
triangle: the smaller this number, the fewer lines on which we have to recurse.
We now formally deﬁne the kind of partitioning we need.
Let L be a set of n lines in the plane, and let r be a parameter with 1 ⩽r ⩽n. A
line is said to cross a triangle if it intersects the interior of the triangle. A (1/r)-
cutting for L is a set Ξ(L) := {t1,t2,...,tm} of possibly unbounded triangles
with disjoint interiors that together cover the plane, with the property that no
triangle of the partitioning is crossed by more than n/r lines from L. The size
of the cutting Ξ(L) is the number of triangles it consists of. Figure 16.7 gives
an example of a cutting.
347

Chapter 16
SIMPLEX RANGE SEARCHING
Theorem 16.7 For any set L of n lines in the plane, and any parameter r with
1 ⩽r ⩽n, a (1/r)-cutting of size O(r2) exists. Moreover, such a cutting (with
for each triangle in the cutting the subset of lines from L that cross it) can be
constructed in O(nr) time.
In Section 16.4 references are given to the papers where this theorem is
proved. We shall only concern ourselves with how cuttings can be used to
design data structures. The data structure based on cuttings is called a cutting
tree. The basic structure of a cutting tree for a set L of n lines is as follows.
If the cardinality of L is one then the cutting tree consists of a single leaf
where L is stored explicitly. The set L is the canonical subset of the leaf.
Otherwise, the structure is a tree T. There is a one-to-one correspondence
between the children of the root of the tree and the triangles of a (1/r)-
cutting Ξ(L) for the set L, where r is a sufﬁciently large constant. (Below
we shall see how r should be chosen.) The triangle of the cutting that
corresponds to a child ν is denoted by t(ν). The subset of lines in L that lie
below t(ν) is called the lower canonical subset of ν; it is denoted L−(ν).
The subset of lines in L that lie above t(ν) is called the upper canonical
subset of ν; it is denoted L+(ν). The subset of lines that cross t(ν) is called
the crossing subset of t(ν). The child ν is the root of a recursively deﬁned
partition tree on its crossing subset; this subtree is denoted by Tν.
With each child ν we store the triangle t(ν). We also store information about
the lower and upper canonical subsets L−(ν) and L+(ν); for counting the
number of lines below the query point we only need to store the cardinality
of the set L−(ν), but for other applications we may store other information.
Figure 16.8 illustrates the notions of lower canonical subset, upper canonical
subset, and crossing subset. We describe an algorithm for selecting the lines
Figure 16.8
The canonical subsets and the crossing
subset for a triangle
= crossing subset
= lower canonical subset
= upper canonical subset
from L below a query point in a number of canonical subsets. To count the
number of such lines we have to sum the cardinalities of the selected canonical
subsets. Let q be the query point. The set of selected nodes will be denoted
by ϒ.
348

Section 16.3
CUTTING TREES
Algorithm SELECTBELOWPOINT(q,T)
Input. A query point q and a cutting tree or subtree of it.
Output. A set of canonical nodes for all lines in the tree that lie below q.
1.
ϒ ←/0
2.
if T consists of a single leaf µ
3.
then if the line stored at µ lies below q then ϒ ←{µ}
4.
else for each child ν of the root of T
5.
do Check if q lies in t(ν).
6.
Let νq be the child such that q ∈t(νq).
7.
ϒ ←{νq} ∪SELECTBELOWPOINT(q,Tνq)
8.
return ϒ
Lemma 16.8 Let L be a set of n lines in the plane. Using a cutting tree, the
lines from L below a query point can be selected in O(logn) time in O(logn)
canonical subsets. As a consequence, the number of such lines can be counted
in O(logn) time. For any ε > 0, a cutting tree on L can be constructed that uses
O(n2+ε) storage.
Proof. Let Q(n) denote the query time in a cutting tree for a set of n lines. Then
Q(n) satisﬁes the recurrence:
Q(n) =
" O(1)
if n = 1,
O(r2)+Q(n/r)
if n > 1.
This recurrence solves to Q(n) = O(logn) for any constant r > 1.
Let ε > 0 be given. According to Theorem 16.7 we can construct a (1/r)-
cutting for L of size cr2, where c is a constant. We construct a cutting tree based
on (1/r)-cuttings for r = ⌈(2c)1/ε⌉. The amount of storage used by the cutting
tree, M(n), satisﬁes
M(n) =
" O(1)
if n = 1,
O(r2)+∑ν M(nν)
if n > 1,
where we sum over all children ν of the root of the tree. The number of children
of the root is cr2, and nν ⩽n/r for each child ν. Hence, with our choice of r
the recurrence solves to M(n) = O(n2+ε).
We conclude that we can count the number of lines below a query point in
O(logn) time with a structure that uses O(n2+ε) storage. By duality, we can do
half-plane range counting within the same bounds. Now let's look at triangular
range counting again: given a set S of points in the plane, count the number of
points inside a query triangle. Following the approach for half-plane queries,
we go to the dual plane. What problem do we get in the dual plane? The set of
points dualizes to a set of lines, of course, but it is less clear what happens to the
query triangle. A triangle is the intersection of three half-planes, so a point p lies
in a triangle if and only if it lies in each of the half-planes. In Figure 16.9, for
instance, point p lies in the triangle because p ∈ℓ+
1 and p ∈∩ℓ−
2 and p ∈∩ℓ−
3 .
The line dual to p therefore has ℓ1∗above it, and ℓ2∗and ℓ3∗below it. In general,
the dual statement of the triangular range searching problem is: given a set L
349

Chapter 16
SIMPLEX RANGE SEARCHING
Figure 16.9
Triangular range searching
primal plane
dual plane
ℓ1
ℓ2
ℓ3
p
ℓ1∗
ℓ2∗
ℓ3∗
p∗
of lines in the plane, and a triple q1,q2,q3 of query points labeled "above" or
"below", count the number of lines from L that lie on the speciﬁed sides of the
three query points. This problem can be solved with a three-level cutting tree.
We now describe a data structure for the following, slightly simpler, problem:
given a set L of lines, and a pair q1,q2 of query points, select the lines that lie
below both query points. After having seen the two-level cutting tree that solves
this problem, designing a three-level cutting tree for the dual of the triangular
range searching problem should be easy.
A two-level cutting tree on a set L of n lines for selecting the lines below a
pair q1,q2 of query points is deﬁned as follows.
The set L is stored in a cutting tree T.
With each node ν of the ﬁrst-level tree T, we store its lower canonical subset
in a second-level cutting tree Tassoc
ν
.
The idea is that the ﬁrst level of the tree is used to select the lines below q1 in a
number of canonical subsets. The associated structures (or, trees at the second
level) storing the selected canonical subsets are then used to select the lines
that lie below q2. Because the associated structures are one-level cuttings trees,
we can use algorithm SELECTBELOWPOINT to query them. The total query
algorithm is thus as follows.
Algorithm SELECTBELOWPAIR(q1,q2,T)
Input. Two query points q1 and q2 and a cutting tree or subtree of it.
Output. A set of canonical nodes for all lines in the tree that lie below q1 and q2.
1.
ϒ ←/0
2.
if T consists of a single leaf µ
3.
then if the line stored at µ lies below q1 and q2 then ϒ ←{µ}
4.
else for each child ν of the root of T
5.
do Check if q1 lies in t(ν).
6.
Let νq1 be the child such that q1 ∈t(νq1).
7.
ϒ1 ←SELECTBELOWPOINT(q2,Tassoc
νq1 )
8.
ϒ2 ←SELECTBELOWPAIR(q1,q2,Tνq1 )
9.
ϒ ←ϒ1 ∪ϒ2
10. return ϒ
Recall that adding an extra level to a partition tree did not increase its query
time, whereas the amount of storage it used increased by a logarithmic factor.
For cutting trees this is exactly the other way around: adding an extra level
350

Section 16.3
CUTTING TREES
increases the query time by a logarithmic factor, whereas the amount of storage
stays the same. This is proved in the next lemma.
Lemma 16.9 Let L be a set of n lines in the plane. Using a two-level cutting
tree, the lines from L below a pair of query points can be selected in O(log2 n)
time in O(log2 n) canonical subsets. As a consequence, the number of such
lines can be counted in O(log2 n) time. For any ε > 0, such a two-level cutting
tree on L can be constructed that uses O(n2+ε) storage.
Proof. Let Q(n) denote the query time in a two-level cutting tree for a set of n
lines. The associated structures are one-level cutting trees, so the query time for
the associated structures is O(logn) by Lemma 16.8. Hence, Q(n) satisﬁes the
recurrence:
Q(n) =
" O(1)
if n = 1,
O(r2)+O(logn)+Q(n/r)
if n > 1.
This recurrence solves to Q(n) = O(log2 n) for any constant r > 1.
Let ε > 0 be given. According to Lemma 16.8 we can construct the associ-
ated structures of the children of the root such that each of them uses O(n2+ε)
storage. Hence, the amount of storage used by the cutting tree, M(n), satisﬁes
M(n) =
" O(1)
if n = 1,
∑ν[O(n2+ε +M(nν)]
if n > 1,
where we sum over all children ν of the root of the tree. The number of children
of the root is O(r2), and nν ⩽n/r for each child ν. It follows that, if r is a large
enough constant, the recurrence solves to M(n) = O(n2+ε). (If you are a bit
bored by now, you are on the right track: cutting trees, partition trees, and their
multi-level variants are all analyzed in the same way.)
We designed and analyzed a two-level cutting tree for selecting (or counting)
the number of lines below a pair of query points. For the triangular range
searching we need a three-level cutting tree. The design and analysis of three-
level cutting trees follows exactly the same pattern as for two-level cutting trees.
Therefore you should hopefully have no difﬁculties in proving the following
result.
Theorem 16.10 Let S be a set of n points in the plane. For any ε > 0, there is a
data structure for S, called a cutting tree, that uses O(n2+ε) storage such that
the points from S lying inside a query triangle can be counted in O(log3 n) time.
The points can be reported in O(k) additional time, where k is the number of
reported points. The structure can be constructed in O(n2+ε) time.
One can even do a little bit better than in this theorem. This is discussed in
Section 16.4 and in the exercises.
351

Chapter 16
SIMPLEX RANGE SEARCHING
16.4
Notes and Comments
Range searching is one of the best studied problems in computational geometry.
For extensive overviews of results on range searching, see the surveys by Agar-
wal [1] and Agarwal and Erickson [2]. We can distinguish between orthogonal
range searching and simplex range searching. Orthogonal range searching was
the topic of Chapter 5. In this chapter we discussed the planar variant of simplex
range searching, namely triangular range searching. We conclude with a brief
overview of the history of simplex range searching, and a discussion of the
higher-dimensional variants of the theory we presented.
We begin our discussion with data structures for simplex range searching in
the plane that use roughly linear storage. Willard [388] was the ﬁrst to present
such a data structure. His structure is based on the same idea as the partition
trees described in this chapter, namely a partition of the plane into regions. His
partition, however, did not have such a good crossing number, so the query time
of his structure was O(n0.774). As better simplicial partitions were developed,
more efﬁcient partition trees were possible [111, 169, 209, 394]. Improvements
were also obtained using a somewhat different structure than a partition tree,
namely a spanning tree with low stabbing number [112, 384]. The best solution
for triangular range searching so far has been given by Matouˇsek [263]. Theo-
rem 16.1 is proved in that paper. Matouˇsek also describes a more complicated
data structure with O(√n2O(log∗n)) query time. This structure, however, cannot
be used so easily as a basis for multi-level trees.
The simplex range searching problem in Rd is stated as follows: preprocess
a set S of points in Rd into a data structure, such that the points from S lying
in a query simplex can be counted (or reported) efﬁciently. Matouˇsek also
proved results for simplicial partitions in higher dimensions. The deﬁnition
of simplicial partitions in Rd is similar to the deﬁnition in the plane; the only
difference is that the triangles of the partition are replaced by d-simplices, and
that the crossing number is deﬁned with respect to hyperplanes instead of lines.
Matouˇsek proved that any set of points in Rd admits a simplicial partition of
size r with crossing number O(r1−1/d). Using such a simplicial partition one
can construct, for any ε > 0, a partition tree for simplex range searching in Rd
that uses linear space and has O(n1−1/d+ε) query time. The query time can
be improved to O(n1−1/d(logn)O(1)). The query time of Matouˇsek's structure
comes close to the lower bounds proved by Chazelle [89], which state that a
data structure for triangular range searching that uses O(m) storage must have
Ω(n/(m1/d logn)) query time. A structure that uses linear space must thus have
Ω(n1−1/d/logn) query time. (In the plane, a slightly sharper lower bound is
known, namely Ω(n/√m).)
Data structures for simplex range searching with logarithmic query time
have also received a lot of attention. Clarkson [131] was the ﬁrst to realize
that cuttings can be used as the basis for a data structure for range searching.
Using a probabilistic argument, he proved the existence of O(logr/r)-cuttings
of size O(rd) for sets of hyperplanes in Rd, and he used this to develop a data
structure for half-space range queries. After this, several people worked on
352

Section 16.5
EXERCISES
improving the results and on developing efﬁcient algorithms for computing
cuttings. Currently the best known algorithm is by Chazelle [95]. He has shown
that for any parameter r, it is possible to compute a (1/r)-cutting of size O(rd)
with a deterministic algorithm that takes O(nrd−1) time. These cuttings can
be used to design a (multi-level) cutting tree for simplex range searching, as
shown in this chapter for the planar case. The resulting data structure has
O(logd n) query time and uses O(nd+ε) storage. The query time can be reduced
to O(logn). Due to a special property of Chazelle's cuttings, it is also possible
to get rid of the O(nε) factor in the storage [265], but for the new structure it
is no longer possible to reduce the query time to O(logn). These bounds are
again close to Chazelle's lower bounds.
By combining partition trees and cutting trees the right way, one can get
data structures that have storage in between that of partition trees and cutting
trees. In particular, for any n ⩽m ⩽nd, a data structure of size O(m1+ε) has
O(n1+ε/m1/d) query time, close to the lower bound: Exercise 16.16 shows how
to do this.
Partition trees use linear space, but their query time is rather high. Cutting
trees, on the other hand, have logarithmic query time but they need a lot of
storage, Ideally one would like to have a structure that uses linear space and
has logarithmic query time. While Chazelle's lower bounds [89] show that
this is not possible for exact range searching, one can achieve such bounds for
approximate range searching. The idea here is that points that are "almost" in
the query range (that is, that are very close to it) may be reported as well—see
the survey by Duncan and Goodrich [151] for details.
In the discussion above we have concentrated on simplex range searching. Half-
space range searching is, of course, a special case of this. It turns out that for
this special case better results can be achieved. For example, for half-plane
range reporting (not for counting) in the plane, there is a data structure with
O(logn+k) query time that uses O(n) storage [107]. Here k is the number of
reported points. Improved results are possible in higher dimensions as well:
the points in a query half-space can be reported in O(n1−1/⌊d/2⌋(logn)O(1) +k)
time with a data structure that uses O(nloglogn) storage [264].
Finally, Agarwal and Matouˇsek [8] generalized the results on range search-
ing to query ranges that are semi-algebraic sets.
16.5
Exercises
16.1
Let S be a set of n points in the plane.
a. Suppose that the points in S lie on a √n×√n grid. (Assume for sim-
plicity that n is a square.) Let r be a parameter with 1 ⩽r ⩽n. Draw
a ﬁne simplicial partition for S of size r and crossing number O(√r).
b. Now suppose all points from S are collinear. Draw a ﬁne simplicial
partition for S of size r. What is the crossing number of your partition?
353

Chapter 16
SIMPLEX RANGE SEARCHING
16.2
Prove that the selected nodes in a partition tree are exactly the nodes ν
with the following property: t(ν) ⊂h (or, in case ν is a leaf, the point
stored at ν lies in h) and there is no ancestor µ of ν such that t(µ) ⊂h.
Use this to prove that S∩h is the disjoint union of the canonical subsets
of the selected nodes.
16.3
Prove that the recurrence for M(n) given in the proof of Lemma 16.2
solves to M(n) = O(n).
16.4
Prove that the recurrence for Q(n) given in the proof of Lemma 16.3
solves to Q(n) = O(n1/2+ε).
16.5
Suppose we have a partition tree as deﬁned on page 339, except that
the simplicial partitions used in the construction are not necessarily ﬁne.
What does this mean for the amount of storage used by the partition
tree? And for its query time?
16.6
Lemma 16.3 shows that for any ε > 0 we can build a partition tree with
O(n1/2+ε) query time, by choosing the parameter r that determines the
branching degree of the tree to be a large enough constant. We can do
even better if we choose r depending on n. Show that the query time
reduces to O(√nlogn) if we choose r = √n. (Note that the value of r is
not the same any more at different nodes in the tree. However, this is
not a problem.)
16.7
Prove that the recurrence for M(n) given in the proof of Lemma 16.5
solves to M(n) = O(nlogn).
16.8
Prove that the recurrence for Q(n) given in the proof of Lemma 16.6
solves to Q(n) = O(n1/2+ε).
16.9
Let T be a set of n triangles in the plane. An inverse range counting
query asks to count the number of triangles from T containing a query
point q.
a. Design a data structure for inverse range counting queries that uses
roughly linear storage (for example, O(nlogc n) for some constant c).
Analyze the amount of storage and the query time of your data struc-
ture.
b. Can you do better if you know that all triangles are disjoint?
16.10 Let L be a set of n lines in the plane.
a. Suppose that L consists of ⌊n/2⌋vertical lines and ⌈n/2⌉horizontal
lines. Let r be a parameter with 1 ⩽r ⩽n. Draw a (1/r)-cutting for L
of size O(r2).
b. Now suppose all lines from L are vertical. Draw a (1/r)-cutting for L.
What is the size of your cutting?
16.11 Prove that the recurrences for Q(n) and M(n) given in the proof of
Lemma 16.8 solve to Q(n) = O(logn) and M(n) = O(n2+ε).
354

Section 16.5
EXERCISES
16.12 Prove that the recurrences for Q(n) and M(n) given in the proof of
Lemma 16.9 solve to Q(n) = O(log2 n) and M(n) = O(n2+ε).
16.13 A query in a two-level cutting tree visits the associated structures of the
nodes on one path in the tree. The query time in an associated structure
storing m lines is O(logm) by Lemma 16.8. Because the depth of the
main tree is O(logn), the total query time is O(log2 n). If we could
choose the value of the parameter r of the main cutting tree larger than
constant, for example nδ for some small δ > 0, then the depth of the
main tree would become smaller, resulting in a reduction of the query
time. Unfortunately, there is an O(r2) term in the recurrence for Q(n) in
the proof of Lemma 16.9.
a. Describe a way to get around this problem, so that you can choose
r := nδ.
b. Prove that the query time of your two-level data structure is O(logn).
c. Prove that the amount of storage of the data structure is still O(n2+ε).
16.14 Design a data structure for triangular range searching that has O(log3 n)
query time. Describe the data structure as well as the query algorithm
precisely, and analyze both storage and query time.
16.15 Let S be a set of n points in the plane, each having a positive real weight
associated with them. Describe two data structures for the following
query problem: ﬁnd the point in a query half-plane with the largest
weight. One data structure should use linear storage, and the other data
structure should have logarithmic query time. Analyze the amount of
storage and the query time of both data structures.
16.16 In this chapter we have seen a data structure for half-plane range search-
ing with linear storage but a rather high query time (the partition tree)
and a data structure with logarithmic query time but a rather high use of
storage (the cutting tree). Sometimes one would like to have something
in between: a data structure that has a better query time than partition
trees, but uses less storage than a cutting tree. In this exercise we show
how to design such a structure.
Suppose that we have O(m1+ε) storage available, for some m between n
and n2. We want to build a structure for selecting points in half-planes
that uses O(m1+ε) storage and has as fast a query time as possible.
The idea is to start with the fastest structure we have (the cutting tree)
and switch to the slow structure (the partition tree) when we run out
of storage. That is, we continue the construction of the cutting tree
recursively until the number of lines we have to store drops below some
threshold ˆn.
a. Describe the data structure and the query algorithm in detail.
b. Compute the value of the threshold ˆn such that the amount of storage
is O(m1+ε).
c. Analyze the query time of the resulting data structure.
355

Bibliography
[1] P. Agarwal. Range searching. In J. E. Goodman and J. O'Rourke, editors, Hand-
book of Discrete and Computational Geometry, 2nd edn., chapter 36. Chapman &
Hall/CRC, 2004.
[2] P. Agarwal and J. Erickson. Geometric range searching and its relatives. In
B. Chazelle, J. Goodman, and R. Pollack, editors, Advances in Discrete and
Computational Geometry, pages 1-56. American Mathematical Society, 1998.
[3] P. Agarwal, J. Pach, and M. Sharir. State of the union (of geometric objects): A
review. In J. Goodman, J. Pach, and R. Pollack, editors, Computational Geometry:
Twenty Years Later. American Mathematical Society, 2007.
[4] P. K. Agarwal. Partitioning arrangements of lines II: Applications. Discrete
Comput. Geom., 5:533-573, 1990.
[5] P. K. Agarwal, M. de Berg, J. Gudmundsson, M. Hammar, and H. J. Haverkort.
Box-trees and R-trees with near-optimal query time. Discrete Comput. Geom.,
28:291-312, 2002.
[6] P. K. Agarwal, M. de Berg, J. Matouˇsek, and O. Schwarzkopf. Constructing
levels in arrangements and higher order Voronoi diagrams. SIAM J. Comput.,
27:654-667, 1998.
[7] P. K. Agarwal and M. van Kreveld. Implicit point location in arrangements of line
segments, with an application to motion planning. Internat. J. Comput. Geom.
Appl., 4:369-383, 1994.
[8] P. K. Agarwal and J. Matouˇsek. On range searching with semialgebraic sets.
Discrete Comput. Geom., 11:393-418, 1994.
[9] P. K. Agarwal and M. Sharir. Efﬁcient randomized algorithms for some geometric
optimization problems. Discrete Comput. Geom., 16:317-337, 1996.
[10] A. Aggarwal. The Art Gallery Problem: Its Variations, Applications, and Algo-
rithmic Aspects. Ph.D. thesis, Johns Hopkins Univ., Baltimore, MD, 1984.
[11] A. Aggarwal, L. J. Guibas, J. B. Saxe, and P. W. Shor. A linear-time algorithm for
computing the Voronoi diagram of a convex polygon. Discrete Comput. Geom.,
4:591-604, 1989.
[12] O. Aichholzer, F. Aurenhammer, S.-W. Cheng, N. Katoh, M. Taschwer, G. Rote,
and Y.-F. Xu. Triangulations intersect nicely. Discrete Comput. Geom., 16:339-
359, 1996.
[13] V. Akman. Unobstructed Shortest Paths in Polyhedral Environments. Lecture
Notes in Computer Science, vol. 251. Springer-Verlag, 1987.
[14] S. Aluru. Quadtrees and octrees. In D. Metha and S. Sahni, editors, Handbook of
Data Structures and Applications, chapter 19. Chapman & Hall/CRC, 2005.
[15] N. M. Amato, M. T. Goodrich, and E. A. Ramos. A randomized algorithm for
triangulating a simple polygon in linear time. Discrete Comput. Geom., 26:245-
265, 2001.
357

BIBLIOGRAPHY
[16] N. Amenta. Helly-type theorems and generalized linear programming. Discrete
Comput. Geom., 12:241-261, 1994.
[17] A. M. Andrew. Another efﬁcient algorithm for convex hulls in two dimensions.
Inform. Process. Lett., 9:216-219, 1979.
[18] L. Arge, M. de Berg, H. J. Haverkort, and K. Yi. The priority R-tree: A practically
efﬁcient and worst-case optimal R-tree. In SIGMOD Conf., pages 347-358, 2004.
[19] L. Arge, G. Brodal, and L. Georgiadis. Improved dynamic planar point location.
In Proc. 47th Annu. IEEE Sympos. Found. Comput. Sci., pages 305-314, 2006.
[20] E. M. Arkin, J. S. B. Mitchell, and S. Suri. Logarithmic-time link path queries in
a simple polygon. Internat. J. Comput. Geom. Appl., 5:369-395, 1995.
[21] B. Aronov, M. de Berg, and C. Gray. Ray shooting and intersection searching
amidst fat convex polyhedra in 3-space. In Proc. 22nd Annu. ACM Sympos.
Comput. Geom., pages 88-94, 2006.
[22] B. Aronov and M. Sharir. On translational motion planning of a convex polyhedron
in 3-space. SIAM J. Comput., 26:1785-1803, 1997.
[23] T. Asano, T. Asano, L. J. Guibas, J. Hershberger, and H. Imai. Visibility of disjoint
polygons. Algorithmica, 1:49-63, 1986.
[24] T. Asano, D. Kirkpatrick, and C. K. Yap. d1-optimal motion for a rod. In Proc.
12th Annu. ACM Sympos. Comput. Geom., pages 252-263, 1996.
[25] F. Aurenhammer. A criterion for the afﬁne equality of cell complexes in Rd and
convex polyhedra in Rd+1. Discrete Comput. Geom., 2:49-64, 1987.
[26] F. Aurenhammer. Power diagrams: Properties, algorithms and applications. SIAM
J. Comput., 16:78-96, 1987.
[27] F. Aurenhammer. Linear combinations from power domains. Geom. Dedicata,
28:45-52, 1988.
[28] F. Aurenhammer. Voronoi diagrams: A survey of a fundamental geometric data
structure. ACM Comput. Surv., 23:345-405, 1991.
[29] F. Aurenhammer and H. Edelsbrunner. An optimal algorithm for constructing the
weighted Voronoi diagram in the plane. Pattern Recogn., 17:251-257, 1984.
[30] F. Aurenhammer, F. Hoffmann, and B. Aronov. Minkowski-type theorems and
least-squares clustering. Algorithmica, 20:61-76, 1998.
[31] F. Aurenhammer and O. Schwarzkopf. A simple on-line randomized incremental
algorithm for computing higher order Voronoi diagrams. Internat. J. Comput.
Geom. Appl., 2:363-381, 1992.
[32] D. Avis and G. T. Toussaint. An efﬁcient algorithm for decomposing a polygon
into star-shaped polygons. Pattern Recogn., 13:395-398, 1981.
[33] F. Avnaim, J.-D. Boissonnat, and B. Faverjon. A practical exact motion planning
algorithm for polygonal objects amidst polygonal obstacles. In Proc. 5th IEEE
Internat. Conf. Robot. Autom., pages 1656-1661, 1988.
[34] C. Bajaj and T. K. Dey. Convex decomposition of polyhedra and robustness.
SIAM J. Comput., 21:339-364, 1992.
[35] I. J. Balaban. An optimal algorithm for ﬁnding segment intersections. In Proc.
11th Annu. ACM Sympos. Comput. Geom., pages 211-219, 1995.
[36] C. Ballieux. Motion planning using binary space partitions. Technical Report
Inf/src/93-25, Utrecht Univ., 1993.
[37] B. Barber and M. Hirsch. A robust algorithm for point in polyhedron. In Proc.
5th Canad. Conf. Comput. Geom., pages 479-484, Waterloo, Canada, 1993.
358

BIBLIOGRAPHY
[38] R. E. Barnhill. Representation and approximation of surfaces. In J. R. Rice, editor,
Math. Software III, pages 69-120. Academic Press, New York, 1977.
[39] J. Barraquand and J.-C. Latombe. Robot motion planning: A distributed represen-
tation approach. Internat. J. Robot. Res., 10:628-649, 1991.
[40] B. G. Baumgart. A polyhedron representation for computer vision. In Proc.
AFIPS Natl. Comput. Conf., vol. 44, pages 589-596, 1975.
[41] H. Baumgarten, H. Jung, and K. Mehlhorn. Dynamic point location in general
subdivisions. J. Algorithms, 17:342-380, 1994.
[42] P. Belleville, M. Keil, M. McAllister, and J. Snoeyink. On computing edges that
are in all minimum-weight triangulations. In Proc. 12th Annu. ACM Sympos.
Comput. Geom., pages V7-V8, 1996.
[43] R. V. Benson. Euclidean Geometry and Convexity. McGraw-Hill, New York,
1966.
[44] J. L. Bentley. Multidimensional binary search trees used for associative searching.
Commun. ACM, 18:509-517, 1975.
[45] J. L. Bentley. Solutions to Klee's rectangle problems. Technical report, Carnegie-
Mellon Univ., Pittsburgh, PA, 1977.
[46] J. L. Bentley. Decomposable searching problems. Inform. Process. Lett., 8:244-
251, 1979.
[47] J. L. Bentley and T. A. Ottmann. Algorithms for reporting and counting geometric
intersections. IEEE Trans. Comput., C-28:643-647, 1979.
[48] J. L. Bentley and J. B. Saxe. Decomposable searching problems I: Static-to-
dynamic transformation. J. Algorithms, 1:301-358, 1980.
[49] M. Berg. Vertical ray shooting for fat objects. In Proc. 21st Annu. ACM Sympos.
Comput. Geom., pages 288-295, 2005.
[50] M. de Berg. Computing half-plane and strip discrepancy of planar point sets.
Comput. Geom. Theory Appl., 6:69-83, 1996.
[51] M. de Berg. Linear size binary space partitions for uncluttered scenes. Algorith-
mica, 28:353-366, 2000.
[52] M. de Berg, P. Bose, D. Bremner, S. Ramaswami, and G. T. Wilfong. Computing
constrained minimum-width annuli of point sets. Comput.-Aided Design, 30:267-
275, 1998.
[53] M. de Berg and C. Gray. Vertical ray shooting and computing depth orders for fat
objects. In Proc. 17th ACM-SIAM Sympos. Discrete Algorithms, pages 494-503,
2006.
[54] M. de Berg, M. de Groot, and M. Overmars. New results on binary space partitions
in the plane. Comput. Geom. Theory Appl., 8:317-333, 1997.
[55] M. de Berg, M. Katz, A. F. van der Stappen, and J. Vleugels. Realistic input
models for geometric algorithms. Algorithmica, 34:81-97, 2002.
[56] M. de Berg, M. van Kreveld, B. J. Nilsson, and M. H. Overmars. Shortest path
queries in rectilinear worlds. Internat. J. Comput. Geom. Appl., 2:287-309, 1992.
[57] M. de Berg, M. van Kreveld, and J. Snoeyink. Two- and three-dimensional point
location in rectangular subdivisions. J. Algorithms, 18:256-277, 1995.
[58] M. de Berg, J. Matouˇsek, and O. Schwarzkopf. Piecewise linear paths among
convex obstacles. Discrete Comput. Geom., 14:9-29, 1995.
[59] M. de Berg and O. Schwarzkopf. Cuttings and applications. Internat. J. Comput.
Geom. Appl., 5:343-355, 1995.
359

BIBLIOGRAPHY
[60] M. de Berg and M. Streppel. Approximate range searching using binary space
partitions. Comput. Geom. Theory Appl., 33:139-151, 2006.
[61] M. Bern. Triangulation and mesh generation. In J. E. Goodman and J. O'Rourke,
editors, Handbook of Discrete and Computational Geometry, 2nd edn., chapter 25.
Chapman & Hall/CRC, 2004.
[62] M. Bern and D. Eppstein. Mesh generation and optimal triangulation. In D.-Z.
Du and F. K. Hwang, editors, Computing in Euclidean Geometry. Lecture Notes
Series on Computing, vol. 1, pages 23-90. World Scientiﬁc, Singapore, 1992.
[63] M. Bern and D. Eppstein. Polynomial-size nonobtuse triangulation of polygons.
Internat. J. Comput. Geom. Appl., 2:241-255, 1992. Corrigendum in 2:449-450,
1992.
[64] M. Bern, D. Eppstein, and J. Gilbert. Provably good mesh generation. J. Comput.
Syst. Sci., 48:384-409, 1994.
[65] M. Bern and P. Plasman. Mesh generation. In J.-R. Sack and J. Urrutia, editors,
Handbook of Computational Geometry, 2nd edn., chapter 6. Elsevier, 1999.
[66] M. W. Bern, H. Edelsbrunner, D. Eppstein, S. L. Mitchell, and T. S. Tan. Edge
insertion for optimal triangulations. Discrete Comput. Geom., 10:47-65, 1993.
[67] M. W. Bern, S. A. Mitchell, and J. Ruppert. Linear-size nonobtuse triangulation
of polygons. Discrete Comput. Geom., 14:411-428, 1995.
[68] B. K. Bhattacharya and J. Zorbas. Solving the two-dimensional ﬁndpath problem
using a line-triangle representation of the robot. J. Algorithms, 9:449-469, 1988.
[69] J.-D. Boissonnat, O. Devillers, R. Schott, M. Teillaud, and M. Yvinec. Appli-
cations of random sampling to on-line algorithms in computational geometry.
Discrete Comput. Geom., 8:51-71, 1992.
[70] J.-D. Boissonnat, O. Devillers, and M. Teillaud. A semidynamic construction
of higher-order Voronoi diagrams and its randomized analysis. Algorithmica,
9:329-356, 1993.
[71] J.-D. Boissonnat and M. Teillaud. On the randomized construction of the Delaunay
tree. Theoret. Comput. Sci., 112:339-354, 1993.
[72] P. Bose and G. Toussaint. Geometric and computational aspects of manufacturing
processes. Comput. & Graphics, 18:487-497, 1994.
[73] G. S. Brodal and R. Jacob. Dynamic planar convex hull. In Proc. 43rd Annu.
IEEE Sympos. Found. Comput. Sci., pages 617-626, 2002.
[74] R. A. Brooks and T. Lozano-P´erez. A subdivision algorithm in conﬁguration
space for ﬁndpath with rotation. IEEE Trans. Syst. Man Cybern., 15:224-233,
1985.
[75] G. Brown. Point density in stems per acre. New Zealand Forestry Service Research
Notes, 38:1-11, 1965.
[76] J. L. Brown. Vertex based data dependent triangulations. Comput. Aided Geom.
Design, 8:239-251, 1991.
[77] K. Q. Brown. Comments on "Algorithms for reporting and counting geometric
intersections". IEEE Trans. Comput., C-30:147-148, 1981.
[78] C. Burnikel, K. Mehlhorn, and S. Schirra. On degeneracy in geometric com-
putations. In Proc. 5th ACM-SIAM Sympos. Discrete Algorithms, pages 16-23,
1994.
[79] A. Bykat. Convex hull of a ﬁnite set of points in two dimensions. Inform. Process.
Lett., 7:296-298, 1978.
360

BIBLIOGRAPHY
[80] J. Canny. The Complexity of Robot Motion Planning. MIT Press, Cambridge,
MA, 1987.
[81] J. Canny, B. R. Donald, and E. K. Ressler. A rational rotation method for robust
geometric algorithms. In Proc. 8th Annu. ACM Sympos. Comput. Geom., pages
251-260, 1992.
[82] T. M. Chan. Output-sensitive results on convex hulls, extreme points, and related
problems. Discrete Comput. Geom., 16:369-387, 1996.
[83] T. M. Chan. Dynamic planar convex hull operations in near-logarithmaic amor-
tized time. J. ACM, 48:1-12, 2001.
[84] D. R. Chand and S. S. Kapur. An algorithm for convex polytopes. J. ACM,
17:78-86, 1970.
[85] B. Chazelle. A theorem on polygon cutting with applications. In Proc. 23rd Annu.
IEEE Sympos. Found. Comput. Sci., pages 339-349, 1982.
[86] B. Chazelle. Convex partitions of polyhedra: A lower bound and worst-case
optimal algorithm. SIAM J. Comput., 13:488-507, 1984.
[87] B. Chazelle. Filtering search: A new approach to query-answering. SIAM J.
Comput., 15:703-724, 1986.
[88] B. Chazelle. Reporting and counting segment intersections. J. Comput. Syst. Sci.,
32:156-182, 1986.
[89] B. Chazelle. Lower bounds on the complexity of polytope range searching. J.
Amer. Math. Soc., 2:637-666, 1989.
[90] B. Chazelle. Lower bounds for orthogonal range searching, I: The reporting case.
J. ACM, 37:200-212, 1990.
[91] B. Chazelle. Lower bounds for orthogonal range searching, II: The arithmetic
model. J. ACM, 37:439-463, 1990.
[92] B. Chazelle. Triangulating a simple polygon in linear time. In Proc. 31st Annu.
IEEE Sympos. Found. Comput. Sci., pages 220-230, 1990.
[93] B. Chazelle. An optimal convex hull algorithm and new results on cuttings. In
Proc. 32nd Annu. IEEE Sympos. Found. Comput. Sci., pages 29-38, 1991.
[94] B. Chazelle. Triangulating a simple polygon in linear time. Discrete Comput.
Geom., 6:485-524, 1991.
[95] B. Chazelle. Cutting hyperplanes for divide-and-conquer. Discrete Comput.
Geom., 9:145-158, 1993.
[96] B. Chazelle. Geometric discrepancy revisited. In Proc. 34th Annu. IEEE Sympos.
Found. Comput. Sci., pages 392-399, 1993.
[97] B. Chazelle. An optimal convex hull algorithm in any ﬁxed dimension. Discrete
Comput. Geom., 10:377-409, 1993.
[98] B. Chazelle and H. Edelsbrunner. An improved algorithm for constructing kth-
order Voronoi diagrams. IEEE Trans. Comput., C-36:1349-1354, 1987.
[99] B. Chazelle and H. Edelsbrunner. An optimal algorithm for intersecting line
segments in the plane. In Proc. 29th Annu. IEEE Sympos. Found. Comput. Sci.,
pages 590-600, 1988.
[100] B. Chazelle and H. Edelsbrunner. An optimal algorithm for intersecting line
segments in the plane. J. ACM, 39:1-54, 1992.
[101] B. Chazelle, H. Edelsbrunner, L. Guibas, and M. Sharir. Algorithms for bichro-
matic line segment problems and polyhedral terrains. Report UIUCDCS-R-90-
1578, Dept. Comput. Sci., Univ. Illinois, Urbana, IL, 1989.
361

BIBLIOGRAPHY
[102] B. Chazelle, H. Edelsbrunner, L. Guibas, and M. Sharir. A singly-exponential
stratiﬁcation scheme for real semi-algebraic varieties and its applications. Theoret.
Comput. Sci., 84:77-105, 1991.
[103] B. Chazelle, H. Edelsbrunner, L. Guibas, and M. Sharir. Algorithms for bichro-
matic line segment problems and polyhedral terrains. Algorithmica, 11:116-132,
1994.
[104] B. Chazelle and J. Friedman. Point location among hyperplanes and unidirectional
ray-shooting. Comput. Geom. Theory Appl., 4:53-62, 1994.
[105] B. Chazelle and L. J. Guibas. Fractional cascading: I. A data structuring technique.
Algorithmica, 1:133-162, 1986.
[106] B. Chazelle and L. J. Guibas. Fractional cascading: II. Applications. Algorithmica,
1:163-191, 1986.
[107] B. Chazelle, L. J. Guibas, and D. T. Lee. The power of geometric duality. BIT,
25:76-90, 1985.
[108] B. Chazelle and J. Incerpi. Triangulating a polygon by divide and conquer. In
Proc. 21st Allerton Conf. Commun. Control Comput., pages 447-456, 1983.
[109] B. Chazelle and J. Incerpi. Triangulation and shape-complexity. ACM Trans.
Graph., 3:135-152, 1984.
[110] B. Chazelle and L. Palios. Triangulating a non-convex polytope. Discrete Comput.
Geom., 5:505-526, 1990.
[111] B. Chazelle, M. Sharir, and E. Welzl. Quasi-optimal upper bounds for simplex
range searching and new zone theorems. Algorithmica, 8:407-429, 1992.
[112] B. Chazelle and E. Welzl. Quasi-optimal range searching in spaces of ﬁnite
VC-dimension. Discrete Comput. Geom., 4:467-489, 1989.
[113] D. Z. Chen, K. S. Klenk, and H.-Y. T. Tu. Shortest path queries among weighted
obstacles in the rectilinear plane. In Proc. 11th Annu. ACM Sympos. Comput.
Geom., pages 370-379, 1995.
[114] Y.-B. Chen and D. Ierardi. Time-optimal trajectories of a rod in the plane subject
to velocity constraints. Algorithmica, 18:165-197, June 1997.
[115] S. W. Cheng and R. Janardan. New results on dynamic planar point location.
SIAM J. Comput., 21:972-999, 1992.
[116] L. P. Chew. Building Voronoi diagrams for convex polygons in linear expected
time. Technical Report PCS-TR90-147, Dept. Math. Comput. Sci., Dartmouth
College, Hanover, NH, 1986.
[117] L. P. Chew. Guaranteed-quality mesh generation for curved surfaces. In Proc. 9th
Annu. ACM Sympos. Comput. Geom., pages 274-280, 1993.
[118] L. P. Chew and R. L. Drysdale, III. Voronoi diagrams based on convex distance
functions. In Proc. 1st Annu. ACM Sympos. Comput. Geom., pages 235-244,
1985.
[119] L. P. Chew and K. Kedem.
A convex polygon among polygonal obstacles:
Placement and high-clearance motion. Comput. Geom. Theory Appl., 3:59-89,
1993.
[120] Y.-J. Chiang, F. P. Preparata, and R. Tamassia. A uniﬁed approach to dynamic
point location, ray shooting, and shortest paths in planar maps. SIAM J. Comput.,
25:207-233, 1996.
[121] Y.-J. Chiang and R. Tamassia. Dynamic algorithms in computational geometry.
Proc. IEEE, 80:1412-1434, September 1992.
362

BIBLIOGRAPHY
[122] Y.-J. Chiang and R. Tamassia. Optimal shortest path and minimum-link path
queries between two convex polygons inside a simple polygonal obstacle. Internat.
J. Comput. Geom. Appl., 7:85-121, 1997.
[123] F. Chin, J. Snoeyink, and C.-A. Wang. Finding the medial axis of a simple
polygon in linear time. In Proc. 6th Annu. Internat. Sympos. Algorithms Comput.
(ISAAC 95). Lecture Notes in Computer Science, vol. 1004, pages 382-391.
Springer-Verlag, 1995.
[124] N. Chin and S. Feiner. Near real time shadow generation using bsp trees. In Proc.
SIGGRAPH '89, pages 99-106, 1989.
[125] J. Choi, J. Sellen, and C.-K. Yap. Precision-sensitive Euclidean shortest path
in 3-space. In Proc. 11th Annu. ACM Sympos. Comput. Geom., pages 350-359,
1995.
[126] J. Choi, J. Sellen, and C. K. Yap. Approximate Euclidean shortest paths in 3-space.
Internat. J. Comput. Geom. Appl., 7:271-295, August 1997.
[127] H. Choset, K. M. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L. E. Kavraki, and
S. Thrun. Principles of Robot Motion: Theory, Algorithms, and Implementations.
MIT Press, Cambridge, MA, 2005.
[128] V. Chv´atal. A combinatorial theorem in plane geometry. J. Combin. Theory Ser.
B, 18:39-41, 1975.
[129] V. Chv´atal. Linear Programming. W. H. Freeman, New York, 1983.
[130] K. L. Clarkson. Linear programming in O(n3d2) time. Inform. Process. Lett.,
22:21-24, 1986.
[131] K. L. Clarkson. New applications of random sampling in computational geometry.
Discrete Comput. Geom., 2:195-222, 1987.
[132] K. L. Clarkson. Las Vegas algorithms for linear and integer programming when
the dimension is small. J. ACM, 42:488-499, 1995.
[133] K. L. Clarkson and P. W. Shor. Applications of random sampling in computational
geometry, II. Discrete Comput. Geom., 4:387-421, 1989.
[134] K. L. Clarkson, R. E. Tarjan, and C. J. Van Wyk. A fast Las Vegas algorithm for
triangulating a simple polygon. Discrete Comput. Geom., 4:423-432, 1989.
[135] R. Cole. Searching and storing similar lists. J. Algorithms, 7:202-220, 1986.
[136] G. E. Collins. Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic
decomposition. In Proc. 2nd GI Conf. on Automata Theory and Formal Languages.
Lecture Notes in Computer Science, vol. 33, pages 134-183. Springer-Verlag,
1975.
[137] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to
Algorithms, 2nd edn. MIT Press, Cambridge, MA, 2001.
[138] F. d'Amore and P. G. Franciosa. On the optimal binary plane partition for sets of
isothetic rectangles. Inform. Process. Lett., 44:255-259, 1992.
[139] G. B. Dantzig. Linear Programming and Extensions. Princeton University Press,
Princeton, NJ, 1963.
[140] M. N. Demers. Fundamentals of Geographical Information Systems, 4th edn.
Wiley, 2008.
[141] O. Devillers. Randomization yields simple O(nlog∗n) algorithms for difﬁcult
Ω(n) problems. Internat. J. Comput. Geom. Appl., 2:97-111, 1992.
[142] O. Devillers and P. A. Ramos. Computing roundness is easy if the set is almost
round. Internat. J. Comput. Geom. Appl., 12:229-248, 2002.
363

BIBLIOGRAPHY
[143] T. K. Dey. Improved bounds for k-Sets and k-th levels. In Proc. 38th Annu. IEEE
Sympos. Found. Comput. Sci., pages 156-161, 1997.
[144] T. K. Dey. Improved bounds on planar k-sets and related problems. Discrete
Comput. Geom., 19:373-383, 1998.
[145] T. K. Dey, K. Sugihara, and C. L. Bajaj. Delaunay triangulations in three dimen-
sions with ﬁnite precision arithmetic. Comput. Aided Geom. Design, 9:457-470,
1992.
[146] M. T. Dickerson, S. A. McElfresh, and M. H. Montague. New algorithms and
empirical ﬁndings on minimum weight triangulation heuristics. In Proc. 11th
Annu. ACM Sympos. Comput. Geom., pages 238-247, 1995.
[147] M. T. Dickerson and M. H. Montague. A (usually?) connected subgraph of
the minimum weight triangulation. In Proc. 12th Annu. ACM Sympos. Comput.
Geom., pages 204-213, 1996.
[148] G. L. Dirichlet. ¨Uber die Reduktion der positiven quadratischen Formen mit drei
unbestimmten ganzen Zahlen. J. Reine Angew. Math., 40:209-227, 1850.
[149] D. Dobkin and D. Eppstein. Computing the discrepancy. In Proc. 9th Annu. ACM
Sympos. Comput. Geom., pages 47-52, 1993.
[150] D. Dobkin and D. Mitchell. Random-edge discrepancy of supersampling patterns.
In Graphics Interface '93, 1993.
[151] C. Duncan and M. Goodrich.
Approximate geometric query structures.
In
D. Metha and S. Sahni, editors, Handbook of Data Structures and Applications,
chapter 26. Chapman & Hall/CRC, 2005.
[152] D. Dutta, R. Janardan, and M. Smid. Geometric and Algorithmic Aspects of
Computer-Aided Design and Manufacturing. DIMACS Series in Discrete Math-
ematics and Theoretical Computer Science, vol. 67. American Mathematical
Society, 2005.
[153] M. E. Dyer. On a multidimensional search technique and its application to the
Euclidean one-centre problem. SIAM J. Comput., 15:725-738, 1986.
[154] N. Dyn, D. Levin, and S. Rippa. Data dependent triangulations for piecewise
linear interpolation. IMA J. Numer. Anal., 10:137-154, 1990.
[155] H. Ebarra, N. Fukuyama, H. Nakano, and Y. Nakanishi. Roundness algorithms us-
ing Voronoi diagrams. In Proc. First Canadian Conf. on Computational Geometry,
page 41, 1989.
[156] W. F. Eddy. A new convex hull algorithm for planar sets. ACM Trans. Math.
Softw., 3:398-403 and 411-412, 1977.
[157] H. Edelsbrunner. Dynamic data structures for orthogonal intersection queries.
Report F59, Inst. Informationsverarb., Tech. Univ. Graz, Graz, Austria, 1980.
[158] H. Edelsbrunner. Algorithms in Combinatorial Geometry. EATCS Monographs
on Theoretical Computer Science, vol. 10. Springer-Verlag, 1987.
[159] H. Edelsbrunner and L. J. Guibas. Topologically sweeping an arrangement. J.
Comput. Syst. Sci., 38:165-194, 1989. Corrigendum in 42:249-251, 1991.
[160] H. Edelsbrunner, L. J. Guibas, J. Hershberger, R. Seidel, M. Sharir, J. Snoeyink,
and E. Welzl. Implicitly representing arrangements of lines or segments. Discrete
Comput. Geom., 4:433-466, 1989.
[161] H. Edelsbrunner, L. J. Guibas, and J. Stolﬁ. Optimal point location in a monotone
subdivision. SIAM J. Comput., 15:317-340, 1986.
[162] H. Edelsbrunner, G. Haring, and D. Hilbert. Rectangular point location in d
dimensions with applications. Comput. J., 29:76-82, 1986.
364

BIBLIOGRAPHY
[163] H. Edelsbrunner and H. A. Maurer. On the intersection of orthogonal objects.
Inform. Process. Lett., 13:177-181, 1981.
[164] H. Edelsbrunner and E. P. M¨ucke. Simulation of simplicity: A technique to cope
with degenerate cases in geometric algorithms. ACM Trans. Graph., 9:66-104,
1990.
[165] H. Edelsbrunner, J. O'Rourke, and R. Seidel. Constructing arrangements of lines
and hyperplanes with applications. SIAM J. Comput., 15:341-363, 1986.
[166] H. Edelsbrunner and M. H. Overmars. Batched dynamic solutions to decompos-
able searching problems. J. Algorithms, 6:515-542, 1985.
[167] H. Edelsbrunner and R. Seidel. Voronoi diagrams and arrangements. Discrete
Comput. Geom., 1:25-44, 1986.
[168] H. Edelsbrunner, R. Seidel, and M. Sharir. On the zone theorem for hyperplane
arrangements. SIAM J. Comput., 22:418-429, 1993.
[169] H. Edelsbrunner and E. Welzl. Halfplanar range search in linear space and
O(n0.695) query time. Inform. Process. Lett., 23:289-293, 1986.
[170] H. ElGindy and G. T. Toussaint. On triangulating palm polygons in linear time.
In N. Magnenat-Thalmann and D. Thalmann, editors, New Trends in Computer
Graphics, pages 308-317. Springer-Verlag, 1988.
[171] I. Emiris and J. Canny. An efﬁcient approach to removing geometric degeneracies.
In Proc. 8th Annu. ACM Sympos. Comput. Geom., pages 74-82, 1992.
[172] I. Emiris and J. Canny. A general approach to removing degeneracies. SIAM J.
Comput., 24:650-664, 1995.
[173] D. Eppstein, M. Goodrich, and J. Sun. The skip quadtree: A simple dynamic data
structure for multidimensional data. In Proc. 21st ACM Sympos. Comput. Geom.,
pages 296-205, 2005.
[174] P. Erd˝os, L. Lov´asz, A. Simmons, and E. Straus. Dissection graphs of planar
point sets. In J. N. Srivastava, editor, A Survey of Combinatorial Theory, pages
139-154. North-Holland, Amsterdam, 1973.
[175] I. D. Faux and M. J. Pratt. Computational Geometry for Design and Manufacture.
Ellis Horwood, Chichester, U.K., 1979.
[176] U. Finke and K. Hinrichs. Overlaying simply connected planar subdivisions in
linear time. In Proc. 11th Annu. ACM Sympos. Comput. Geom., pages 119-126,
1995.
[177] R. A. Finkel and J. L. Bentley. Quad trees: a data structure for retrieval on
composite keys. Acta Inform., 4:1-9, 1974.
[178] S. Fisk. A short proof of Chv`atal's watchman theorem. J. Combin. Theory Ser. B,
24:374, 1978.
[179] J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes. Computer Graphics:
Principles and Practice, 2nd edn. Addison-Wesley, Reading, MA, 1995.
[180] S. Fortune. Numerical stability of algorithms for 2-d Delaunay triangulations
and Voronoi diagrams. In Proc. 8th Annu. ACM Sympos. Comput. Geom., pages
83-92, 1992.
[181] S. Fortune and V. Milenkovic. Numerical stability of algorithms for line ar-
rangements. In Proc. 7th Annu. ACM Sympos. Comput. Geom., pages 334-341,
1991.
[182] S. Fortune and C. J. Van Wyk. Efﬁcient exact arithmetic for computational
geometry. In Proc. 9th Annu. ACM Sympos. Comput. Geom., pages 163-172,
1993.
365

BIBLIOGRAPHY
[183] S. J. Fortune. A sweepline algorithm for Voronoi diagrams. Algorithmica, 2:153-
174, 1987.
[184] A. Fournier and D. Y. Montuno. Triangulating simple polygons and equivalent
problems. ACM Trans. Graph., 3:153-174, 1984.
[185] H. Fuchs, Z. M. Kedem, and B. Naylor. On visible surface generation by a priori
tree structures. Comput. Graph., 14:124-133, 1980. Proc. SIGGRAPH '80.
[186] K. R. Gabriel and R. R. Sokal. A new statistical approach to geographic variation
analysis. Systematic Zoology, 18:259-278, 1969.
[187] J. Garcia-Lopez, P. A. Ramos, and J. Snoeyink. Fitting a set of points by a circle.
Discrete Comput. Geom., 20:389-402, 1998.
[188] M. R. Garey, D. S. Johnson, F. P. Preparata, and R. E. Tarjan. Triangulating a
simple polygon. Inform. Process. Lett., 7:175-179, 1978.
[189] B. G¨artner. A subexponential algorithm for abstract optimization problems. SIAM
J. Comput., 24:1018-1035, 1995.
[190] S. K. Ghosh and D. M. Mount. An output-sensitive algorithm for computing
visibility graphs. SIAM J. Comput., 20:888-910, 1991.
[191] J. E. Goodman and J. O'Rourke, editors. Handbook of Discrete and Computa-
tional Geometry, 2nd edn. Chapman & Hall/CRC, 2004.
[192] R. L. Graham. An efﬁcient algorithm for determining the convex hull of a ﬁnite
planar set. Inform. Process. Lett., 1:132-133, 1972.
[193] P. J. Green and B. W. Silverman. Constructing the convex hull of a set of points
in the plane. Comput. J., 22:262-266, 1979.
[194] B. Gr¨unbaum. Convex Polytopes. Wiley, 1967.
[195] L. J. Guibas, J. Hershberger, D. Leven, M. Sharir, and R. E. Tarjan. Linear-time
algorithms for visibility and shortest path problems inside triangulated simple
polygons. Algorithmica, 2:209-233, 1987.
[196] L. J. Guibas, D. E. Knuth, and M. Sharir. Randomized incremental construction
of Delaunay and Voronoi diagrams. Algorithmica, 7:381-413, 1992.
[197] L. J. Guibas, L. Ramshaw, and J. Stolﬁ. A kinetic framework for computational
geometry. In Proc. 24th Annu. IEEE Sympos. Found. Comput. Sci., pages 100-111,
1983.
[198] L. J. Guibas, D. Salesin, and J. Stolﬁ. Epsilon geometry: Building robust algo-
rithms from imprecise computations. In Proc. 5th Annu. ACM Sympos. Comput.
Geom., pages 208-217, 1989.
[199] L. J. Guibas and R. Sedgewick. A dichromatic framework for balanced trees. In
Proc. 19th Annu. IEEE Sympos. Found. Comput. Sci., pages 8-21, 1978.
[200] L. J. Guibas and R. Seidel. Computing convolutions by reciprocal search. Discrete
Comput. Geom., 2:175-193, 1987.
[201] L. J. Guibas, M. Sharir, and S. Sifrony. On the general motion planning problem
with two degrees of freedom. Discrete Comput. Geom., 4:491-521, 1989.
[202] L. J. Guibas and J. Stolﬁ. Primitives for the manipulation of general subdivisions
and the computation of Voronoi diagrams. ACM Trans. Graph., 4:74-123, 1985.
[203] L. J. Guibas and J. Stolﬁ. Ruler, compass and computer: The design and analysis
of geometric algorithms. In R. A. Earnshaw, editor, Theoretical Foundations
of Computer Graphics and CAD. NATO ASI Series F, vol. 40, pages 111-165.
Springer-Verlag, 1988.
[204] A. Guttman. R-trees: A dynamic index structure for spatial searching. In SIGMOD
Conf., pages 47-57, 1984.
366

BIBLIOGRAPHY
[205] D. Halperin. Algorithmic Motion Planning via Arrangements of Curves and of
Surfaces. Ph.D. thesis, Comput. Sci. Dept., Tel-Aviv Univ., Tel Aviv, 1992.
[206] D. Halperin. Arrangements. In J. E. Goodman and J. O'Rourke, editors, Handbook
of Discrete and Computational Geometry, 2nd edn., chapter 24. Chapman &
Hall/CRC, 2004.
[207] D. Halperin and M. Sharir. Almost tight upper bounds for the single cell and zone
problems in three dimensions. Discrete Comput. Geom., 14:385-410, 1995.
[208] D. Halperin and M. Sharir. Arrangements and their applications in robotics:
Recent developments. In K. Goldbergs, D. Halperin, J.-C. Latombe, and R. Wilson,
editors, Proc. Workshop on Algorithmic Foundations of Robotics. A. K. Peters,
Boston, MA, 1995.
[209] D. Haussler and E. Welzl. Epsilon-nets and simplex range queries. Discrete
Comput. Geom., 2:127-151, 1987.
[210] J. Hershberger and S. Suri. Efﬁcient computation of Euclidean shortest paths
in the plane. In Proc. 34th Annu. IEEE Sympos. Found. Comput. Sci., pages
508-517, 1993.
[211] J. Hershberger and S. Suri. Off-line maintenance of planar conﬁgurations. J.
Algorithms, 21:453-475, 1996.
[212] J. Hershberger and S. Suri. An optimal algorithm for Euclidean shortest paths in
the plane. SIAM J. Comput., 28:2215-2256, 1999.
[213] S. Hertel, M. M¨antyl¨a, K. Mehlhorn, and J. Nievergelt. Space sweep solves
intersection of convex polyhedra. Acta Inform., 21:501-519, 1984.
[214] S. Hertel and K. Mehlhorn. Fast triangulation of simple polygons. In Proc. 4th
Internat. Conf. Found. Comput. Theory. Lecture Notes in Computer Science,
vol. 158, pages 207-218. Springer-Verlag, 1983.
[215] K. Ho-Le. Finite element mesh generation methods: A review and classiﬁcation.
Comput. Aided Design, 20:27-38, 1988.
[216] C. Hoffmann. Geometric and Solid Modeling. Morgan Kaufmann, San Mateo,
CA, 1989.
[217] J. E. Hopcroft, J. T. Schwartz, and M. Sharir. Planning, Geometry, and Complexity
of Robot Motion. Ablex Publishing, Norwood, NJ, 1987.
[218] C. Icking, G. Rote, E. Welzl, and C. Yap. Shortest paths for line segments.
Algorithmica, 10:182-200, 1993.
[219] H. Inagaki and K. Sugihara. Numerically robust algorithm for constructing
constrained Delaunay triangulation. In Proc. 6th Canad. Conf. Comput. Geom.,
pages 171-176, 1994.
[220] R. Janardan and T. C. Woo. Design and manufacturing. In J. E. Goodman and
J. O'Rourke, editors, Handbook of Discrete and Computational Geometry, 2nd
edn., chapter 55. Chapman & Hall/CRC, 2004.
[221] R. A. Jarvis. On the identiﬁcation of the convex hull of a ﬁnite set of points in the
plane. Inform. Process. Lett., 2:18-21, 1973.
[222] G. Kalai. A subexponential randomized simplex algorithm. In Proc. 24th Annu.
ACM Sympos. Theory Comput., pages 475-482, 1992.
[223] M. Kallay. The complexity of incremental convex hull algorithms in Rd. Inform.
Process. Lett., 19:197, 1984.
[224] R. G. Karlsson. Algorithms in a restricted universe. Report CS-84-50, Univ.
Waterloo, Waterloo, ON, 1984.
367

BIBLIOGRAPHY
[225] R. G. Karlsson and J. I. Munro. Proximity on a grid. In Proc. 2nd Sympos. on
Theoretical Aspects of Computer Science. Lecture Notes in Computer Science,
vol. 182, pages 187-196. Springer-Verlag, 1985.
[226] R. G. Karlsson and M. H. Overmars. Scanline algorithms on a grid. BIT, 28:227-
241, 1988.
[227] N. Karmarkar. A new polynomial-time algorithm for linear programming. Com-
binatorica, 4:373-395, 1984.
[228] M. Katz. 3-d vertical ray shooting and 2-d point enclosure, range searching, and
arc shooting amidst convex fat objects. Comput. Geom. Theory Appl., 8:299-316,
1997.
[229] M. Katz, M. Overmars, and M. Sharir. Efﬁcient hidden surface removal for objects
with small union size. Comput. Geom. Theory Appl., 2:223-234, 1992.
[230] L. Kavraki, P. ˇSvestka, J.-C. Latombe, and M. H. Overmars.
Probabilistic
roadmaps for path planning in high dimensional conﬁguration spaces. IEEE
Trans. Robot. Autom., 12:566-580, 1996.
[231] K. Kedem, R. Livne, J. Pach, and M. Sharir. On the union of Jordan regions and
collision-free translational motion amidst polygonal obstacles. Discrete Comput.
Geom., 1:59-71, 1986.
[232] K. Kedem and M. Sharir. An efﬁcient algorithm for planning collision-free
translational motion of a convex polygonal object in 2-dimensional space amidst
polygonal obstacles. In Proc. 1st Annu. ACM Sympos. Comput. Geom., pages
75-80, 1985.
[233] K. Kedem and M. Sharir. An efﬁcient motion planning algorithm for a convex
rigid polygonal object in 2-dimensional polygonal space. Discrete Comput. Geom.,
5:43-75, 1990.
[234] L. G. Khachiyan. Polynomial algorithm in linear programming. U.S.S.R. Comput.
Math. Math. Phys., 20:53-72, 1980.
[235] O. Khatib. Real-time obstacle avoidance for manipulators and mobile robots.
Internat. J. Robot. Res., 5:90-98, 1985.
[236] D. G. Kirkpatrick. Optimal search in planar subdivisions. SIAM J. Comput.,
12:28-35, 1983.
[237] D. G. Kirkpatrick, M. M. Klawe, and R. E. Tarjan. Polygon triangulation in
o(nloglogn) time with simple data structures. Discrete Comput. Geom., 7:329-
346, 1992.
[238] D. G. Kirkpatrick and R. Seidel. The ultimate planar convex hull algorithm?
SIAM J. Comput., 15:287-299, 1986.
[239] V. Klee. On the complexity of d-dimensional Voronoi diagrams. Archiv der
Mathematik, 34:75-80, 1980.
[240] R. Klein. Abstract Voronoi diagrams and their applications. In Computational
Geometry and its Applications. Lecture Notes in Computer Science, vol. 333,
pages 148-157. Springer-Verlag, 1988.
[241] R. Klein. Concrete and Abstract Voronoi Diagrams. Lecture Notes in Computer
Science, vol. 400. Springer-Verlag, 1989.
[242] R. Klein, K. Mehlhorn, and S. Meiser. Randomized incremental construction of
abstract Voronoi diagrams. Comput. Geom. Theory Appl., 3:157-184, 1993.
[243] J.-C. Latombe. Robot Motion Planning. Kluwer Academic, Boston, 1991.
[244] C. L. Lawson. Transforming triangulations. Discrete Math., 3:365-372, 1972.
368

BIBLIOGRAPHY
[245] C. L. Lawson. Software for C1 surface interpolation. In J. R. Rice, editor, Math.
Software III, pages 161-194. Academic Press, New York, 1977.
[246] D. Lee and A. Lin. Computational complexity of art gallery problems. IEEE
Trans. Inform. Theory, 32:276-282, 1986.
[247] D. T. Lee. Proximity and reachability in the plane. Report R-831, Dept. Elect.
Engrg., Univ. Illinois, Urbana, IL, 1978.
[248] D. T. Lee. Two-dimensional Voronoi diagrams in the Lp-metric. J. ACM, 27:604-
618, 1980.
[249] D. T. Lee. On k-nearest neighbor Voronoi diagrams in the plane. IEEE Trans.
Comput., C-31:478-487, 1982.
[250] D. T. Lee and F. P. Preparata. Location of a point in a planar subdivision and its
applications. SIAM J. Comput., 6:594-606, 1977.
[251] D. T. Lee and C. K. Wong. Quintary trees: A ﬁle structure for multidimensional
database systems. ACM Trans. Database Syst., 5:339-353, 1980.
[252] D. T. Lee and C. K. Wong.
Voronoi diagrams in L1 (L∞) metrics with 2-
dimensional storage applications. SIAM J. Comput., 9:200-211, 1980.
[253] D. T. Lee, C. D. Yang, and C. K. Wong. Rectilinear paths among rectilinear
obstacles. Discrete Appl. Math., 70:185-215, 1996.
[254] J. van Leeuwen and D. Wood. Dynamization of decomposable searching problems.
Inform. Process. Lett., 10:51-56, 1980.
[255] D. Leven and M. Sharir. Planning a purely translational motion for a convex
object in two-dimensional space using generalized Voronoi diagrams. Discrete
Comput. Geom., 2:9-31, 1987.
[256] C. Li, S. Pion, and C. K. Yap. Recent progress in exact geometric computation. J.
Log. Algebr. Program., 64:85-111, 2005.
[257] P. A. Longley, M. F. Goodchild, D. J. Maguire, and D. W. Rhind. Geographic
Information Systems and Science, 2nd edn. Wiley, 2005.
[258] T. Lozano-P´erez. Automatic planning of manipulator transfer movements. IEEE
Trans. Syst. Man Cybern., SMC-11:681-698, 1981.
[259] T. Lozano-P´erez. Spatial planning: A conﬁguration space approach. IEEE Trans.
Comput., C-32:108-120, 1983.
[260] T. Lozano-P´erez and M. A. Wesley. An algorithm for planning collision-free
paths among polyhedral obstacles. Commun. ACM, 22:560-570, 1979.
[261] G. S. Lueker. A data structure for orthogonal range queries. In Proc. 19th Annu.
IEEE Sympos. Found. Comput. Sci., pages 28-34, 1978.
[262] H. G. Mairson and J. Stolﬁ. Reporting and counting intersections between two sets
of line segments. In R. A. Earnshaw, editor, Theoretical Foundations of Computer
Graphics and CAD. NATO ASI Series F, vol. 40, pages 307-325. Springer-Verlag,
1988.
[263] J. Matouˇsek. Efﬁcient partition trees. Discrete Comput. Geom., 8:315-334, 1992.
[264] J. Matouˇsek. Reporting points in halfspaces. Comput. Geom. Theory Appl.,
2:169-186, 1992.
[265] J. Matouˇsek. Range searching with efﬁcient hierarchical cuttings. Discrete
Comput. Geom., 10:157-182, 1993.
[266] J. Matouˇsek and O. Schwarzkopf. On ray shooting in convex polytopes. Discrete
Comput. Geom., 10:215-232, 1993.
[267] J. Matouˇsek, M. Sharir, and E. Welzl. A subexponential bound for linear program-
ming. Algorithmica, 16:498-516, 1996.
369

BIBLIOGRAPHY
[268] J. Matouˇsek, J. Pach, M. Sharir, S. Sifrony, and E. Welzl. Fat triangles determine
linearly many holes. SIAM J. Comput., 23:154-169, 1994.
[269] H. A. Maurer and T. A. Ottmann. Dynamic solutions of decomposable searching
problems. In U. Pape, editor, Discrete Structures and Algorithms, pages 17-24.
Carl Hanser Verlag, Munich, 1979.
[270] E. M. McCreight. Efﬁcient algorithms for enumerating intersecting intervals and
rectangles. Report CSL-80-9, Xerox Palo Alto Res. Center, Palo Alto, CA, 1980.
[271] E. M. McCreight. Priority search trees. SIAM J. Comput., 14:257-276, 1985.
[272] R. Mead. A relation between the individual plant-spacing and yield. Ann. of Bot.,
N.S., 30:301-309, 1966.
[273] N. Megiddo. Linear programming in linear time when the dimension is ﬁxed. J.
ACM, 31:114-127, 1984.
[274] K. Mehlhorn, S. Meiser, and C. ´O'D´unlaing. On the construction of abstract
Voronoi diagrams. Discrete Comput. Geom., 6:211-224, 1991.
[275] K. Mehlhorn and S. N¨aher. Dynamic fractional cascading. Algorithmica, 5:215-
241, 1990.
[276] K. Mehlhorn and M. H. Overmars. Optimal dynamization of decomposable
searching problems. Inform. Process. Lett., 12:93-98, 1981.
[277] G. Meisters. Polygons have ears. Amer. Math. Monthly, 82:648-651, 1975.
[278] E. A. Melissaratos and D. L. Souvaine. Coping with inconsistencies: A new
approach to produce quality triangulations of polygonal domains with holes. In
Proc. 8th Annu. ACM Sympos. Comput. Geom., pages 202-211, 1992.
[279] V. Milenkovic. Robust construction of the Voronoi diagram of a polyhedron. In
Proc. 5th Canad. Conf. Comput. Geom., pages 473-478, Waterloo, Canada, 1993.
[280] N. Miller and M. Sharir. Efﬁcient randomized algorithms for constructing the
union of fat triangles and pseudodiscs. Unpublished manuscript.
[281] J. S. B. Mitchell. Shortest paths among obstacles in the plane. In Proc. 9th Annu.
ACM Sympos. Comput. Geom., pages 308-317, 1993.
[282] J. S. B. Mitchell. Shortest paths among obstacles in the plane. Internat. J. Comput.
Geom. Appl., 6:309-332, 1996.
[283] J. S. B. Mitchell and C. H. Papadimitriou. The weighted region problem: ﬁnding
shortest paths through a weighted planar subdivision. J. ACM, 38:18-73, 1991.
[284] J. S. B. Mitchell, G. Rote, and G. Woeginger. Minimum-link paths among
obstacles in the plane. Algorithmica, 8:431-459, 1992.
[285] M. E. Mortenson. Geometric Modeling, 3rd edn. Industrial Press, New York,
2006.
[286] D. E. Muller and F. P. Preparata. Finding the intersection of two convex polyhedra.
Theoret. Comput. Sci., 7:217-236, 1978.
[287] H. M¨uller. Rasterized point location. In Proc. Workshop on Graph-Theoretic
Concepts in Computer Science, pages 281-293. Trauner Verlag, Linz, Austria,
1985.
[288] K. Mulmuley. A fast planar partition algorithm, I. In Proc. 29th Annu. IEEE
Sympos. Found. Comput. Sci., pages 580-589, 1988.
[289] K. Mulmuley. A fast planar partition algorithm, I. Journal of Symbolic Computa-
tion, 10:253-280, 1990.
[290] K. Mulmuley. Computational Geometry: An Introduction Through Randomized
Algorithms. Prentice Hall, Englewood Cliffs, NJ, 1994.
370

BIBLIOGRAPHY
[291] W. Mulzer and G. Rote. Minimum weight triangulation is NP-hard. In Proc. 22nd
Annu. ACM Sympos. Comput. Geom., pages 1-10, 2006.
[292] B. Naylor, J. A. Amanatides, and W. Thibault. Merging BSP trees yields polyhe-
dral set operations. Comput. Graph., 24:115-124, August 1990. Proc. SIGGRAPH
'90.
[293] J. Nievergelt and F. P. Preparata. Plane-sweep algorithms for intersecting geomet-
ric ﬁgures. Commun. ACM, 25:739-747, 1982.
[294] J. Nievergelt and P. Widmayer. Spatial data structures: Concepts and design
choices. In M. van Kreveld, J. Nievergelt, T. Roos, and P. Widmayer, editors,
Algorithmic Foundations of Geographic Information Systems. Lecture Notes in
Computer Science, vol. 1340. Springer-Verlag, 1997.
[295] N. Nilsson. A mobile automaton: An application of artiﬁcial intelligence tech-
niques. In Proc. IJCAI, pages 509-520, 1969.
[296] C. ´O'D´unlaing and C. K. Yap. A "retraction" method for planning the motion of
a disk. J. Algorithms, 6:104-111, 1985.
[297] A. Okabe, B. Boots, and K. Sugihara. Spatial Tessellations: Concepts and
Applications of Voronoi Diagrams. Wiley, 1992.
[298] J. O'Rourke. Art Gallery Theorems and Algorithms. Oxford University Press,
New York, 1987.
[299] M. H. Overmars. The Design of Dynamic Data Structures. Lecture Notes in
Computer Science, vol. 156. Springer-Verlag, 1983.
[300] M. H. Overmars. Efﬁcient data structures for range searching on a grid. J.
Algorithms, 9:254-275, 1988.
[301] M. H. Overmars. Geometric data structures for computer graphics: An overview.
In R. A. Earnshaw, editor, Theoretical Foundations of Computer Graphics and
CAD. NATO ASI Series F, vol. 40, pages 21-49. Springer-Verlag, 1988.
[302] M. H. Overmars. Point location in fat subdivisions. Inform. Process. Lett.,
44:261-265, 1992.
[303] M. H. Overmars and J. van Leeuwen. Further comments on Bykat's convex hull
algorithm. Inform. Process. Lett., 10:209-212, 1980.
[304] M. H. Overmars and J. van Leeuwen. Dynamization of decomposable searching
problems yielding good worst-case bounds. In Proc. 5th GI Conf. Theoret. Comput.
Sci. Lecture Notes in Computer Science, vol. 104, pages 224-233. Springer-
Verlag, 1981.
[305] M. H. Overmars and J. van Leeuwen. Maintenance of conﬁgurations in the plane.
J. Comput. Syst. Sci., 23:166-204, 1981.
[306] M. H. Overmars and J. van Leeuwen. Some principles for dynamizing decompos-
able searching problems. Inform. Process. Lett., 12:49-54, 1981.
[307] M. H. Overmars and J. van Leeuwen. Two general methods for dynamizing
decomposable searching problems. Computing, 26:155-166, 1981.
[308] M. H. Overmars and J. van Leeuwen. Worst-case optimal insertion and deletion
methods for decomposable searching problems. Inform. Process. Lett., 12:168-
173, 1981.
[309] M. H. Overmars and A. F. van der Stappen. Range searching and point location
among fat objects. In J. van Leeuwen, editor, Algorithms - ESA'94. Lecture Notes
in Computer Science, vol. 855, pages 240-253. Springer-Verlag, 1994.
[310] M. H. Overmars and P. ˇSvestka. A probabilistic learning approach to motion
planning. In Algorithmic Foundations of Robotics, pages 19-38. A. K. Peters,
Boston, MA, 1995.
371

BIBLIOGRAPHY
[311] M. H. Overmars and C.-K. Yap. New upper bounds in Klee's measure problem.
SIAM J. Comput., 20:1034-1045, 1991.
[312] J. Pach and M. Sharir. On vertical visibility in arrangements of segments and the
queue size in the Bentley-Ottman line sweeping algorithm. SIAM J. Comput.,
20:460-470, 1991.
[313] J. Pach, W. Steiger, and E. Szemer´edi. An upper bound on the number of planar
k-sets. Discrete Comput. Geom., 7:109-123, 1992.
[314] J. Pach and G. Tardos. On the boundary complexity of the union of fat triangles.
SIAM J. Comput., 31:1745-1760, 2002.
[315] L. Palazzi and J. Snoeyink. Counting and reporting red/blue segment intersections.
CVGIP: Graph. Models Image Process., 56:304-311, 1994.
[316] C. H. Papadimitriou. An algorithm for shortest-path motion in three dimensions.
Inform. Process. Lett., 20:259-263, 1985.
[317] M. S. Paterson and F. F. Yao. Efﬁcient binary space partitions for hidden-surface
removal and solid modeling. Discrete Comput. Geom., 5:485-503, 1990.
[318] M. S. Paterson and F. F. Yao. Optimal binary space partitions for orthogonal
objects. J. Algorithms, 13:99-113, 1992.
[319] M. Pocchiola and G. Vegter. Topologically sweeping visibility complexes via
pseudotriangulations. Discrete Comput. Geom., 16:419-453, 1996.
[320] M. Pocchiola and G. Vegter. The visibility complex. Internat. J. Comput. Geom.
Appl., 6:279-308, 1996.
[321] F. P. Preparata. An optimal real-time algorithm for planar convex hulls. Commun.
ACM, 22:402-405, 1979.
[322] F. P. Preparata and S. J. Hong. Convex hulls of ﬁnite sets of points in two and
three dimensions. Commun. ACM, 20:87-93, 1977.
[323] F. P. Preparata and M. I. Shamos. Computational Geometry: An Introduction.
Springer-Verlag, 1985.
[324] F. P. Preparata and R. Tamassia. Efﬁcient point location in a convex spatial
cell-complex. SIAM J. Comput., 21:267-280, 1992.
[325] E. Quak and L. Schumaker. Cubic spline ﬁtting using data dependent triangula-
tions. Comput. Aided Geom. Design, 7:293-302, 1990.
[326] E. A. Ramos. On range reporting, ray shooting and k-level construction. In Proc.
15th Annu. ACM Sympos. on Comput. Geom., pages 390-399, 1999.
[327] J. H. Reif and J. A. Storer. A single-exponential upper bound for ﬁnding shortest
paths in three dimensions. J. ACM, 41:1013-1019, 1994.
[328] S. Rippa. Minimal roughness property of the Delaunay triangulation. Comput.
Aided Geom. Design, 7:489-497, 1990.
[329] H. Rohnert. Shortest paths in the plane with convex polygonal obstacles. Inform.
Process. Lett., 23:71-76, 1986.
[330] J. Ruppert and R. Seidel. On the difﬁculty of triangulating three-dimensional
non-convex polyhedra. Discrete Comput. Geom., 7:227-253, 1992.
[331] J.-R. Sack and J. Urrutia, editors. Handbook of Computational Geometry. Elsevier,
1997.
[332] H. Samet. An overview of quadtrees, octrees, and related hierarchical data
structures. In R. A. Earnshaw, editor, Theoretical Foundations of Computer
Graphics and CAD. NATO ASI Series F, vol. 40, pages 51-68. Springer-Verlag,
1988.
372

BIBLIOGRAPHY
[333] H. Samet. Applications of Spatial Data Structures: Computer Graphics, Image
Processing, and GIS. Addison-Wesley, Reading, MA, 1990.
[334] H. Samet. The Design and Analysis of Spatial Data Structures. Addison-Wesley,
Reading, MA, 1990.
[335] H. Samet. Foundations of Multidimensional and Metric Data Structures. Morgan
Kaufmann, San Mateo, CA, 2006.
[336] N. Sarnak and R. E. Tarjan. Planar point location using persistent search trees.
Commun. ACM, 29:669-679, 1986.
[337] J. B. Saxe and J. L. Bentley. Transforming static data structures to dynamic
structures. In Proc. 20th Annu. IEEE Sympos. Found. Comput. Sci., pages 148-
168, 1979.
[338] H. W. Scholten and M. H. Overmars. General methods for adding range re-
strictions to decomposable searching problems. J. Symbolic Comput., 7:1-10,
1989.
[339] A. Schrijver. Theory of Linear and Integer Programming. Wiley, 1986.
[340] J. T. Schwartz and M. Sharir. On the "piano movers" problem I: The case of a two-
dimensional rigid polygonal body moving amidst polygonal barriers. Commun.
Pure Appl. Math., 36:345-398, 1983.
[341] J. T. Schwartz and M. Sharir. On the "piano movers" problem II: General tech-
niques for computing topological properties of real algebraic manifolds. Adv.
Appl. Math., 4:298-351, 1983.
[342] J. T. Schwartz and M. Sharir. A survey of motion planning and related geometric
algorithms. In D. Kapur and J. Mundy, editors, Geometric Reasoning, pages
157-169. MIT Press, Cambridge, MA, 1989.
[343] J. T. Schwartz and M. Sharir. Algorithmic motion planning in robotics. In J. van
Leeuwen, editor, Algorithms and Complexity. Handbook of Theoretical Computer
Science, vol. A, pages 391-430. Elsevier, 1990.
[344] R. Seidel. Output-Size Sensitive Algorithms for Constructive Problems in Compu-
tational Geometry. Ph.D. thesis, Dept. Comput. Sci., Cornell Univ., Ithaca, NY,
1986. Technical Report TR 86-784.
[345] R. Seidel. A simple and fast incremental randomized algorithm for comput-
ing trapezoidal decompositions and for triangulating polygons. Comput. Geom.
Theory Appl., 1:51-64, 1991.
[346] R. Seidel. Small-dimensional linear programming and convex hulls made easy.
Discrete Comput. Geom., 6:423-434, 1991.
[347] R. Seidel.
Convex hull computations.
In J. E. Goodman and J. O'Rourke,
editors, Handbook of Discrete and Computational Geometry, 2nd edn., chapter 22.
Chapman & Hall/CRC, 2004.
[348] J. Selig. Geometric Fundamentals of Robotics, 2nd edn. Monographs in Computer
Science. Springer-Verlag, 2004.
[349] M. I. Shamos. Computational Geometry. Ph.D. thesis, Dept. Comput. Sci., Yale
Univ., New Haven, CT, 1978.
[350] M. I. Shamos and D. Hoey. Closest-point problems. In Proc. 16th Annu. IEEE
Sympos. Found. Comput. Sci., pages 151-162, 1975.
[351] M. I. Shamos and D. Hoey. Geometric intersection problems. In Proc. 17th Annu.
IEEE Sympos. Found. Comput. Sci., pages 208-215, 1976.
[352] M. Sharir. Algorithmic motion planning. In J. E. Goodman and J. O'Rourke,
editors, Handbook of Discrete and Computational Geometry, 2nd edn., chapter 47.
Chapman & Hall/CRC, 2004.
373

BIBLIOGRAPHY
[353] M. Sharir and P. K. Agarwal. Davenport-Schinzel Sequences and Their Geometric
Applications. Cambridge University Press, 1995.
[354] M. Sharir and E. Welzl. A combinatorial bound for linear programming and
related problems. In Proc. 9th Sympos. Theoret. Aspects Comput. Sci. Lecture
Notes in Computer Science, vol. 577, pages 569-579. Springer-Verlag, 1992.
[355] T. C. Shermer. Recent results in art galleries. Proc. IEEE, 80:1384-1399, Septem-
ber 1992.
[356] J. Shewchuck. Delaunay Reﬁnement Mesh Generation. Ph.D. thesis, Carnegie-
Mellon Univ., Pittsburgh, PA, 1997.
[357] J. Shewchuck. Delaunay reﬁnement algorithms for triangular mesh generation.
Comput. Geom. Theory Appl., 22:21-74, 2002.
[358] P. Shirley. Discrepancy as a quality measure for sample distributions. In F. H.
Post and W. Barth, editors, Proc. Eurographics'91, pages 183-194. Elsevier,
September 1991.
[359] P. Shirley, M. Ashikhmin, M. Gleicher, S. Marschner, E. Reinhard, K. Sung,
W. Thompson, and P. Willemsen. Fundamentals of Computer Graphics, 2nd edn.
A.K. Peters, 2005.
[360] R. Sibson. Locally equiangular triangulations. Comput. J., 21:243-245, 1978.
[361] J. Snoeyink. Point location. In J. E. Goodman and J. O'Rourke, editors, Handbook
of Discrete and Computational Geometry, 2nd edn., chapter 34. Chapman &
Hall/CRC, 2004.
[362] A. van der Stappen. Motion Planning Amidst Fat Obstacles. Ph.D. thesis, Utrecht
Univ., Utrecht, Netherlands, 1994.
[363] A. van der Stappen, M. Overmars, M. de Berg, and J. Vleugels. Motion planning
in environments with low obstacle density. Discrete Comput. Geom., 20:561-587,
1998.
[364] A. F. van der Stappen, D. Halperin, and M. H. Overmars. The complexity of the
free space for a robot moving amidst fat obstacles. Comput. Geom. Theory Appl.,
3:353-373, 1993.
[365] A. F. van der Stappen and M. H. Overmars. Motion planning amidst fat obstacles.
In Proc. 10th Annu. ACM Sympos. Comput. Geom., pages 31-40, 1994.
[366] H. Sundar, D. Silver, N. Gagvani, and S. J. Dickinson. Skeleton based shape
matching and retrieval. In Shape Modeling International, pages 130-142, 2003.
[367] S. Suri. Minimum Link Paths in Polygons and Related Problems. Ph.D. thesis,
Dept. Comput. Sci., Johns Hopkins Univ., Baltimore, MD, 1987.
[368] R. E. Tarjan and C. J. Van Wyk. An O(nloglogn)-time algorithm for triangulating
a simple polygon. SIAM J. Comput., 17:143-178, 1988. Erratum in 17:1061,
1988.
[369] S. J. Teller and C. H. S´equin. Visibility preprocessing for interactive walkthroughs.
Comput. Graph., 25:61-69, July 1991. Proc. SIGGRAPH '91.
[370] W. C. Thibault and B. F. Naylor. Set operations on polyhedra using binary space
partitioning trees. Comput. Graph., 21:153-162, 1987. Proc. SIGGRAPH '87.
[371] C. T´oth. Binary space partition for line segments with a limited number of
directions. SIAM J. Comput., 32:307-325, 2003.
[372] C. T´oth. A note on binary plane partitions. Discrete Comput. Geom., 30:3-16,
2003.
[373] C. T´oth. Binary space partitions: Recent developments. In J. E. Goodman,
J. Pach, and E. Welzl, editors, Combinatorial and Computational Geometry.
MSRI Publications, vol. 52, pages 529-556. Cambridge University Press, 2005.
374

BIBLIOGRAPHY
[374] G. T. Toussaint. The relative neighbourhood graph of a ﬁnite planar set. Pattern
Recogn., 12:261-268, 1980.
[375] V. K. Vaishnavi and D. Wood. Rectilinear line segment intersection, layered
segment trees and dynamization. J. Algorithms, 3:160-176, 1982.
[376] G. Vegter. The visibility diagram: A data structure for visibility problems and
motion planning. In Proc. 2nd Scand. Workshop Algorithm Theory. Lecture Notes
in Computer Science, vol. 447, pages 97-110. Springer-Verlag, 1990.
[377] R. C. Veltkamp. Shape matching: Similarity measures and algorithms. In Shape
Modeling International, pages 188-197, 2001.
[378] R. Volpe and P. Khosla. Artiﬁcial potential with elliptical isopotential contours
for obstacle avoidance. In Proc. 26th IEEE Conf. on Decision and Control, pages
180-185, 1987.
[379] G. M. Voronoi. Nouvelles applications des param`etres continus `a la th´eorie
des formes quadratiques. Premier M´emoire: Sur quelques propri´et´es des formes
quadratiques positives parfaites. J. Reine Angew. Math., 133:97-178, 1907.
[380] G. M. Voronoi. Nouvelles applications des param`etres continus `a la th´eorie des
formes quadratiques. Deuxi`eme M´emoire: Recherches sur les parall´ello`edres
primitifs. J. Reine Angew. Math., 134:198-287, 1908.
[381] A. Watt. 3D Computer Graphics, 3rd edn. Addison-Wesley, Reading, MA, 1999.
[382] R. Wein, J. van den Berg, and D. Halperin. The visibility-Voronoi complex and
its applications. Comput. Geom. Theory Appl., 36:66-87, 2007.
[383] E. Welzl. Constructing the visibility graph for n line segments in O(n2) time.
Inform. Process. Lett., 20:167-171, 1985.
[384] E. Welzl. Partition trees for triangle counting and other range searching problems.
In Proc. 4th Annu. ACM Sympos. Comput. Geom., pages 23-33, 1988.
[385] E. Welzl. Smallest enclosing disks (balls and ellipsoids). In H. Maurer, editor,
New Results and New Trends in Computer Science. Lecture Notes in Computer
Science, vol. 555, pages 359-370. Springer-Verlag, 1991.
[386] D. E. Willard. Predicate-Oriented Database Search Algorithms. Ph.D. thesis,
Aiken Comput. Lab., Harvard Univ., Cambridge, MA, 1978. Report TR-20-78.
[387] D. E. Willard. The super-b-tree algorithm. Report TR-03-79, Aiken Comput.
Lab., Harvard Univ., Cambridge, MA, 1979.
[388] D. E. Willard. Polygon retrieval. SIAM J. Comput., 11:149-165, 1982.
[389] D. E. Willard. Log-logarithmic worst case range queries are possible in space
O(n). Inform. Process. Lett., 17:81-89, 1983.
[390] D. E. Willard. New trie data structures which support very fast search operations.
J. Comput. Syst. Sci., 28:379-394, 1984.
[391] D. E. Willard and G. S. Lueker. Adding range restriction capability to dynamic
data structures. J. ACM, 32:597-617, 1985.
[392] M. F. Worboys and M. Duckham.
GIS, a Computing Perspective, 2nd edn.
Chapman & Hall/CRC, 2004.
[393] A. C. Yao. A lower bound to ﬁnding convex hulls. J. ACM, 28:780-787, 1981.
[394] A. C. Yao and F. F. Yao. A general approach to D-dimensional geometric queries.
In Proc. 17th Annu. ACM Sympos. Theory Comput., pages 163-168, 1985.
[395] C. Yap. Towards exact geometric computation. Comput. Geom. Theory Appl.,
7:3-23, 1997.
375

BIBLIOGRAPHY
[396] C. Yap and E. Chang. Issues in the metrology of geometric tolerancing. In
J. Laumond and M. Overmars, editors, Robotics Motion and Manipulation, pages
393-400. A.K. Peters, 1997.
[397] C. K. Yap. A geometric consistency theorem for a symbolic perturbation scheme.
J. Comput. Syst. Sci., 40:2-18, 1990.
[398] D. Zhu and J.-C. Latombe. New heuristic algorithms for efﬁcient hierarchical
path planning. IEEE Trans. Robot. Autom., 7:9-20, 1991.
[399] G. M. Ziegler. Lectures on Polytopes. Graduate Texts in Mathematics, vol. 152.
Springer-Verlag, 1994.
376

Index
1DRANGEQUERY, 97
2DRANGEQUERY, 108
2d-tree, 100
2DBOUNDEDLP, 75
2DBSP, 264
2DRANDOMBSP, 264
2DRANDOMIZEDBOUNDEDLP, 77
2DRANDOMIZEDLP, 81
3-coloring
of triangulated polygon, 47, 59
3DBSP, 267
3DRANDOMBSP2, 268
abstract Voronoi diagram, 169
adjacency
of trapezoids, 127
algorithm
randomized, 78
angle
of vectors in 3-space, 64
angle-optimal, 194
angle-vector, 194
annulus, 163
smallest-width, 163
approximate cell decomposition, 303,
305
approximate range searching, 353
approximation, 244
arm
robot, 86
arrangement, 180, 187
complexity of, 180
in higher dimensions, 187
point location in, 143
simple, 180
Art Gallery Problem, 45, 59
Art Gallery Theorem, 48
aspect ratio, 319
associated structure, 106, 222, 225, 235,
344, 350
auto-partition, 262
lower bound, 269
automated manufacturing, 63, 89
autonomous robot, 283
average running time, 78
axis-parallel line segment, 220
backwards analysis, 78, 85, 89, 134,
140, 205, 250
balanced quadtree, 313
BALANCEQUADTREE, 313
ball
smallest enclosing, 90
beach line, 152, 160
binary search tree, 96
binary space partition, 259, 261
for low-density scene, 271
lower bound, 270
binary space partition tree, 261
Boolean operation
on polygons, 39
bounded linear program, 73
bounding box, 58, 124, 155, 231
breakpoint, 152, 161
BSP, 261
for low-density scene, 271
lower bound, 270
BSP tree, 261
BUILD2DRANGETREE, 107
BUILDKDTREE, 100
C-obstacle, 285
for translating robot, 291
CAD/CAM, 12, 15, 63, 307
canonical subset, 106, 109, 233, 339,
341
car-like robot, 284
castable, 64
casting, 63
cell decomposition
approximate, 303, 305
exact, 303
377

INDEX
chain method
point location by, 143
circle event, 155, 161
false alarm, 156
closest pair, 170
collision, 286
collision detection, 244
coloring
of triangulated polygon, 47, 59
combinatorial optimization, 90
combined metric, 332
common intersection
of half-planes, 66, 89
composite number, 111
composite-number space, 111
compressed quadtree, 318
COMPUTEFREESPACE, 287
COMPUTEPATH, 289
computer aided design, 12, 15, 63, 307
computer aided manufacturing, 12, 15,
63, 89
computer animation, 244
computer graphics, 10, 14, 175, 188
conﬁguration of robot, 284
conﬁguration space, 285, 330
forbidden, 285
free, 285, 324, 330
of randomized algorithm, 209
of translating polygon, 291
conﬁguration-space obstacle, 285, 330
for translating polygon, 291
for translating robot, 291
conﬂict, 248
conﬂict graph, 248
conﬂict list, 248
conforming mesh, 308, 319
connected subdivision, 30
consistent mesh, 319
constraint
linear, 65, 66, 71
point on circle, 87
CONSTRUCTARRANGEMENT, 182
CONSTRUCTINTERVALTREE, 223
continuous measure, 175
contour line, 191
convex, 2
convex combination, 244
convex hull, 2, 89, 193, 243
3-dimensional, 244
computation, 3, 246
d-dimensional, 256
dynamic, 13
Graham's scan, 13
Jarvis's march, 13
lower bound, 13
pseudo code, 3, 6, 249
convex polytope, 244
point location in, 144
convex set, 2
CONVEXHULL, 6, 249
corner vertex, 310
cutting, 347, 352
cutting tree, 346
cyclic overlap, 260
data structure, 219
1-dimensional range tree, 99
binary search tree, 96
BSP tree, 261
cutting tree, 346
for point location, 128
heap, 227
interval tree, 220, 222, 237
kd-tree, 100
multi-level, 343
octree, 318
partition tree, 336, 338
priority search tree, 226, 237
quadtree, 115, 307, 309
range tree, 105, 109
segment tree, 231, 233, 237
database, 95, 116, 335
database query, 95
Davenport-Schinzel sequence, 188
decomposable searching problem, 238
decomposition
trapezoidal, 124
vertical, 124
degeneracy, 5, 8, 137
degenerate case, 5, 8, 137
degree of freedom, 284
Delaunay corner, 213
Delaunay graph, 196
Delaunay triangulation, 168, 197
computation, 199
pseudo code, 200
DELAUNAYTRIANGULATION, 200
density, 271
depth order, 260
design for assembly, 12
destination
of half-edge, 31
378

INDEX
diagonal
of polygon, 46
difference
of polygons, 39
Dijkstra's algorithm, 324, 326, 331
direction
representation of, 65
Dirichlet tessellation, 167
disc
smallest enclosing, 86
discrepancy, 174, 175, 188
computation, 175
half-plane, 175
discrete measure, 175
distance
Euclidean, 148, 169, 332
L1, 168
L2, 148, 169, 332
Lp, 168
link, 332
Manhattan, 168
distance function, 169
domain
of mesh, 308
of terrain, 191
double wedge, 178
doubly-connected edge list, 29, 30, 48,
155, 180, 247
dual
of line, 178
of object, 177
of point, 177
of segment, 178
dual graph
of triangulated polygon, 47
dual plane, 178
duality
in higher dimensions, 186
in the plane, 177
dynamic convex hull, 13
dynamic point location, 143
dynamization, 237
edge, 30
illegal, 194
of polytope, 245
edge ﬂip, 194
edge list
doubly-connected, 29, 30, 48, 155,
180, 247
element
of mesh, 307
elementary interval, 232
ellipse
smallest enclosing, 90
embedding
of graph, 30
end vertex, 50
envelope
lower, 253
upper, 254
Euclidean distance, 148, 169, 332
Euclidean minimum spanning tree, 215,
216
Euler's formula, 28, 150, 245
event, 22, 51, 151
circle, 155, 161
site, 153, 161
event point, 22, 51, 151
event queue, 24, 51, 155
exact arithmetic, 9
exact cell decomposition, 303
exact match query, 116
expectation
linearity of, 78, 134, 205
expected performance, 78, 133
expected running time, 78
face, 30
record of, 31
facet, 64, 245
ordinary, 64
top, 64
false alarm, 156
farthest-point Voronoi diagram, 164, 169
fat subdivision
point location in, 144
fatness, 279
feasible point, 71
feasible region, 71
feasible solution, 71
Fibonacci heap, 331
FINDINTERSECTIONS, 25
FINDNEWEVENT, 27
FINDSPLITNODE, 97
ﬁnite element method, 307
ﬁrst-level tree, 106, 344
ﬂap, 251
ﬂip
of edge, 194
ﬂoating point arithmetic, 5
FOLLOWSEGMENT, 130
379

INDEX
forbidden conﬁguration space, 285, 330
FORBIDDENSPACE, 298
Fortune's algorithm, 151
fractional cascading, 109, 112, 143, 221
free conﬁguration space, 285, 324, 330
free path, 305
free space, 285, 324, 330
representation of, 287
trapezoidal map of, 287, 324
free split, 265
Gabriel graph, 215, 217
general position, 9, 124
GENERATEMESH, 316
genus, 245
geographic information systems, 1, 11,
15
geometric graph, 215
geometric modeling, 15
GIS, 11
Graham's scan, 13
graph
Gabriel, 215, 217
geometric, 215
relative neighborhood, 215, 217
visibility, 323
grid, 116
guard
for low-density scene, 272
half-edge, 31
destination of, 31
origin of, 31
record of, 32
half-plane discrepancy, 175
half-planes
common intersection of, 66, 89
HANDLECIRCLEEVENT, 158
HANDLEENDVERTEX, 53
HANDLEEVENTPOINT, 26
HANDLEMERGEVERTEX, 54
HANDLEREGULARVERTEX, 54
HANDLESITEEVENT, 158
HANDLESPLITVERTEX, 53
HANDLESTARTVERTEX, 53
harmonic number, 135
heap, 227
Helly-type theorem, 90
hidden surface removal, 259
higher-dimensional linear programming,
82
higher-order Voronoi diagram, 169
horizon, 247
hull
convex, 2, 89, 193, 243
lower, 6, 254
upper, 6, 253
illegal edge, 194
implicit point location, 144
incidence preserving, 178
incident, 30
infeasible linear program, 71
infeasible point, 71
inner vertex, 325
INSERTSEGMENTTREE, 234
interpolation
data-independent, 214
linear, 191
INTERSECTHALFPLANES, 67
intersection
of half-planes, 66, 89
of line segments, 19
of polygons, 39
intersection-sensitive algorithm, 21
interval
elementary, 232
interval tree, 220, 222, 237
inversion, 186
isolated vertex, 31
iterated logarithm, 60
jaggies, 174
Jarvis's march, 13
joint
of robot, 283
prismatic, 283
revolute, 283
k-level
in arrangement, 187
k-set, 187
kd-tree, 100, 116
L1-metric, 168
L2-metric, 148, 169, 332
Lp-metric, 168
layer, 19, 335
layered range tree, 113
legal triangulation, 195
LEGALIZEEDGE, 201
LEGALTRIANGULATION, 195
380

INDEX
level
in arrangement, 185
in data structure, 106, 343
line segment
axis-parallel, 220
line segment intersection, 19, 40
computation, 20
counting, 41
lower bound, 40
pseudo code, 25
red-blue, 40
linear constraint, 65, 66, 71
linear interpolation, 191
linear optimization, 71
linear program, 71
bounded, 73
dimension of, 71
infeasible, 71
unbounded, 73, 79
linear programming, 71
1-dimensional, 75
higher-dimensional, 82
low-dimensional, 72
pseudo code, 75, 77, 84
linearity of expectation, 78, 134, 205
link metric, 332
locally shortest path, 325
locus approach, 231
logarithm
iterated, 60
LOWDENSITYBSP2D, 276
lower bound
convex hull, 13
line segment intersection, 40
of auto-partition, 269
of binary space partition, 270
range tree, 116
simplex range searching, 352
triangulation, 59
Voronoi diagram, 170
lower envelope, 253
lower hull, 6, 254
lower vertical extension, 124
LP-type problem, 90
MAKEMONOTONE, 53
Manhattan metric, 168
manufacturing, 12, 15, 63, 89
map
point location on, 122
trapezoidal, 122, 124, 287
map layer, 19, 335
map overlay, 1, 20
computation, 33
pseudo code, 38
MAPOVERLAY, 38
Markov's inequality, 141
maximal planar subdivision, 193
measure
continuous, 175
discrete, 175
medial axis, 169
merge vertex, 50
mesh, 307
conforming, 308, 319
consistent, 319
domain of, 308
non-uniform, 308
requirements, 308
respecting the input, 308
structured, 319
triangular, 308
uniform, 309
unstructured, 319
mesh element, 307
mesh generation, 307
pseudo code, 316
metric
combined, 332
Euclidean, 148, 169, 332
L1, 168
L2, 148, 169, 332
Lp, 168
link, 332
Manhattan, 168
MINIDISC, 86
MINIDISCWITH2POINTS, 87
MINIDISCWITHPOINT, 87
minimum spanning tree
Euclidean, 215, 216
minimum weight triangulation, 215, 218
Minkowski difference, 292
Minkowski sum, 291, 305, 330
complexity, 297
computation, 295
of non-convex polygons, 296
pseudo code, 295
MINKOWSKISUM, 295
mixture, 243
mobile robot, 283, 323
molecular modeling, 12
monotone polygon, 49
381

INDEX
strictly, 55
triangulation of, 55
motion planning, 1, 14, 162, 283, 323
exact cell decomposition, 303
retraction method, 304
road map for, 288
mouse
clicking with, 5, 122
multi-level data structure, 106, 238, 343
neighbor ﬁnding
in quadtree, 312
non-crossing, 124
non-linear optimization, 86, 90
non-uniform mesh, 308
NORTHNEIGHBOR, 313
objective function, 71
obstacle, 283, 324
conﬁguration-space, 285
obtuse angle, 319
octree, 318
operations research, 72
optimal vertex, 73
optimization
combinatorial, 90
linear, 71
non-linear, 86, 90
order preserving, 178
order-k Voronoi diagram, 169, 188
ordinary facet, 64
origin
of half-edge, 31
orthogonal line segment, 220
orthogonal range query, 96
output-sensitive algorithm, 21, 99
overlap
cyclic, 260
overlay, 1, 20, 33, 167
computation, 33
pseudo code, 38
painter's algorithm, 260
PAINTERSALGORITHM, 263
paraboloid, 178
parameter space, 232
PARANOIDMAXIMUM, 93
partial match query, 116
partition
auto, 262
binary space, 259
simplicial, 337
partition tree, 336, 338
path
free, 305
locally shortest, 325
semi-free, 305
persistency, 143
perturbation
symbolic, 9, 14
PHASE1, 275
pixel, 173, 259
placement of robot, 284
planar point location, 122
planar subdivision, 30, 121
maximal, 193
plane sweep, 22, 34, 51, 68, 151, 328
rotational, 328
pocket, 61
point
infeasible, 71
Steiner, 309
point location, 121
dynamic, 143
higher dimensional, 143
implicit, 144
planar, 121
query, 128
point location query, 121
point robot, 286, 324
polygon
monotone, 49
rectilinear, 60
simple, 46
star-shaped, 93, 145
strictly monotone, 55
triangulation of, 45
y-monotone, 49
polyhedral terrain, 192
polyhedron, 64
polytope, 244
simplicial, 245
tetrahedralization of, 60
potential ﬁeld method, 305
power diagram, 169
preprocessing, 122
primal plane, 178
primitive operation, 4
printed circuit board, 307
priority search tree, 226, 237
prismatic joint, 283
pseudodisc, 292
382

INDEX
pseudodisc property, 293
pseudodiscs
complexity of union, 294
quadtree, 115, 273, 307, 309
balanced, 313
compressed, 318
higher-dimensional, 318
neighbor ﬁnding, 312
quadtree subdivision, 310
query
database, 95
exact match, 116
partial match, 116
point location, 121, 128
range, 96
simplex range, 352
stabbing, 237
stabbing counting, 237
triangular range, 336
windowing, 219, 335
QUERYINTERVALTREE, 224
QUERYPRIOSEARCHTREE, 230
QUERYSEGMENTTREE, 234
random number generator, 77
random permutation
computation, 77
randomized algorithm, 75, 77, 78, 84,
128, 199, 208, 246, 264
analysis, 76, 78, 88, 133, 205,
210, 250, 265
RANDOMIZEDLP, 84
RANDOMPERMUTATION, 77
range query, 220, 318
1-dimensional, 96
2-dimensional, 99
counting, 119
orthogonal, 96
rectangular, 96
simplex, 352
triangular, 336
range searching
approximate, 353
range tree, 99, 105, 109, 220
1-dimensional, 99
2-dimensional, 106
d-dimensional, 109
layered, 113
lower bound, 116
ray shooting, 146, 332
ray tracing, 173
rectangular range query, 96
rectangular subdivision
point location in, 144
rectilinear polygon, 60
red-blue line segment intersection, 40
redundant half-plane, 92
reference point, 284
region
feasible, 71
regular vertex, 50
relative neighborhood graph, 215, 217
rendering, 173, 259
REPORTINSUBTREE, 229
RETRACTION, 162
retraction, 162, 163
retraction method, 304
revolute joint, 283
road map, 288, 324
robot
articulated, 283
autonomous, 283
car-like, 284
conﬁguration of, 284
conﬁguration space of, 285
degree of freedom, 284
mobile, 283
motion planning for, 283
open or closed, 286
parameter space of, 285
placement of, 284
point, 286, 324
reference point of, 284
road map for, 288
work space of, 284
robot arm, 86, 283
robotics, 1, 10, 14, 283
robustness, 5, 8, 9
rotational plane sweep, 328
rounding error, 5
roundness, 163
running time
average, 78
expected, 78
sample points, 174
scan-conversion, 259
search structure
for point location, 128
search tree
binary, 96
383

INDEX
SEARCHKDTREE, 103
second-level tree, 106, 344
see, 325
segment
axis-parallel, 220
segment tree, 231, 233, 237
SELECTBELOWPAIR, 350
SELECTBELOWPOINT, 348
SELECTINHALFPLANE, 339
SELECTINTSEGMENTS, 344
semi-free path, 305
shear transformation, 137
shortest path, 323
for polygonal robot, 330
in graph, 331
SHORTESTPATH, 326
side
in trapezoidal map, 125
simple arrangement, 180
simple polygon, 46
simplex algorithm, 72, 90
simplex range query, 352
lower bound, 352
simplicial partition, 337
simplicial polytope, 245
single cell, 304
site, 147
line segment, 160
site event, 153, 161
skeleton
of polygon, 169
slab, 122, 235
SLOWCONVEXHULL, 3
smallest enclosing ball, 90
smallest enclosing disc, 86
smallest enclosing ellipse, 90
smallest-width annulus, 163
solution
feasible, 71
split
free, 265
split vertex, 50
stabber, 189, 190
stabbing counting query, 237
stabbing number
of polygon, 61
stabbing query, 237
star-shaped polygon, 93, 145
start vertex, 50
status
of sweep line, 22, 25, 52, 155,
328
status structure, 25, 52, 155, 328
Steiner point, 214, 309
Steiner triangulation, 309
strictly monotone polygon, 55
structural change, 205, 210, 250
subdivision, 30, 121
complexity of, 30
connected, 30
maximal planar, 193
quadtree, 310
representation of, 29
subdivision overlay, 33
sum
Minkowski, 291
of two points, 291
vector, 291
supersampling, 174
sweep algorithm, 22, 34, 51, 68, 151,
328
rotational, 328
sweep line, 22, 51, 151, 160
symbolic perturbation, 9, 14
tail estimate, 140
terrain, 191
domain of, 191
polyhedral, 192
tetrahedralization
of polytope, 60
Thales' Theorem, 194
thematic map layer, 19, 335
thematic map overlay, 20
top facet, 64
trading area, 147
transform
duality, 177
inversion, 186
shear, 137
translating polygon
conﬁguration space of, 291
conﬁguration-space obstacle for,
291
transversal, 189, 190
trapezoidal decomposition, 124
trapezoidal map, 122, 124, 287, 324
complexity of, 127
computation, 128
of the free space, 287, 324
pseudo code, 129
384

INDEX
TRAPEZOIDALMAP, 129
traveling salesman problem, 216
tree
1-dimensional range, 99
binary search, 96
binary space partition, 261
BSP, 261
cutting, 346
heap, 227
interval, 220, 222, 237
kd-tree, 100
octree, 318
partition, 336, 338
priority search, 226, 237
quadtree, 115, 307, 309
range, 105, 109
segment, 231, 233, 237
triangle
well-shaped, 308
triangular mesh, 308
triangular range query, 336
triangulated polygon
3-coloring, 47, 59
dual graph, 47
TRIANGULATEMONOTONEPOLYGON,
57
triangulation
computation, 49, 55
Delaunay, 197
legal, 195
lower bound, 59
minimum weight, 215, 218
of monotone polygon, 55
of point set, 192, 193
of polygon, 45, 46
pseudo code, 53, 57
Steiner, 309
triangulation reﬁnement
point location by, 143
turn vertex, 49
twin
of half-edge, 31
unbounded linear program, 73, 79
uniform mesh, 309
union
complexity of, 304
of polygons, 39
Upper Bound Theorem, 256
upper envelope, 254
upper hull, 6, 253
upper vertical extension, 124
vector sum, 291
vertex, 30
inner, 325
isolated, 31
optimal, 73
record of, 31
vertical decomposition, 124
vertical extension, 124
lower, 124
upper, 124
viewing volume, 220
visibility complex, 332
visibility edge, 325
visibility graph, 323
computation, 326
pseudo code, 326
visibility-Voronoi complex, 332
VISIBILITYGRAPH, 326
VISIBLE, 329
visible, 48, 259, 325
visible region, 61, 247
VISIBLEVERTICES, 328
Voronoi assignment model, 148
Voronoi cell, 149
Voronoi diagram, 1, 147, 148, 254
abstract, 169
complexity of, 149, 168
computation, 151
degenerate cases, 159
farthest-point, 164, 169
higher-order, 169
lower bound, 170
medial axis, 169
of line segments, 160
order-k, 169, 188
power diagram, 169
pseudo code, 157
skeleton, 169
weighted, 169
VORONOIDIAGRAM, 157
weighted Voronoi diagram, 169
well-shaped triangle, 308
window, 219, 335
window system, 122
windowing query, 219, 335
work space, 284, 324
y-monotone polygon, 49
385

INDEX
z-buffer algorithm, 259
zone
of hyperplane, 187
of line, 183, 269
Zone Theorem, 184, 187
386

