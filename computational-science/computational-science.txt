### BKOS

This section discusses two crucial aspects of developing geometric algorithms: degeneracies and robustness.

1. Degeneracies: These are special cases or situations within a problem that can complicate algorithm design and implementation. They often arise due to the limitations of numerical precision, such as floating-point arithmetic causing rounding errors. In computational geometry, degenerate cases can lead to issues like points lying exactly on a line segment, multiple points having the same coordinates, or near-collinear points.

   In the convex hull example provided, we initially ignored degeneracies by assuming no three points are collinear and no two points have equal x-coordinates. However, this assumption is not realistic in practical applications where input data might contain such cases. To handle these situations, we can integrate special cases into our algorithm design rather than handling them separately with case distinctions. For instance, in the convex hull algorithm, using lexicographical order instead of only x-coordinate order accommodates equal x-coordinates without significantly increasing complexity.

2. Robustness: This refers to an algorithm's ability to maintain correct and consistent behavior even when faced with degenerate cases or numerical imprecision. A robust geometric algorithm should be able to handle these issues gracefully, avoiding crashes or producing incorrect results.

   In the convex hull example, we initially assumed exact arithmetic with real numbers for simplicity. However, in practice, floating-point computations are used due to efficiency concerns. This introduces rounding errors that can cause problems when determining whether a point lies on one side of a line segment. To address this issue, we can use techniques such as symbolic perturbation schemes or carefully implement the algorithm to detect and handle inconsistencies. These techniques may involve using exact arithmetic libraries, adapting the algorithm to deal with potential errors, or incorporating additional checks to ensure correctness.

   In summary, when designing geometric algorithms, it's essential to be aware of degeneracies (special cases) that can complicate the problem and to develop robust solutions capable of handling these situations gracefully. This often involves integrating special cases into the general algorithm design rather than treating them separately, as well as understanding and addressing numerical precision issues in implementation.


The problem addressed in this section is finding all intersections among a set of line segments (closed) in the plane. A brute-force algorithm would take O(n^2) time by testing every pair of segments for intersection, which is not efficient when most segments intersect only a few others.

To overcome this, the authors propose an output-sensitive algorithm called FINDINTERSECTIONS based on the plane sweep paradigm. The key idea is to use a horizontally moving "sweep line" (event point) that processes the segments one at a time while maintaining an ordered list of intersecting segments.

1. **Event Queue and Status Structure**: The algorithm uses two data structures:
   - Event queue (Q): A balanced binary search tree storing events sorted by their y-coordinate, with ties broken by x-coordinate. It also stores the corresponding segment at each event point.
   - Status structure (T): Another balanced binary search tree that maintains an ordered list of segments intersecting the sweep line.

2. **Algorithm Workflow**:
   - Initialize Q and T as empty data structures, then insert all segment endpoints into Q with their associated segments.
   - While Q is not empty:
     - Remove the next event (highest priority) from Q.
     - Call HANDLEEVENTPOINT to process this event, which may involve adding/removing segments from T or reporting an intersection.

3. **Handling Events**:
   - For endpoint events (upper and lower), handle by inserting/deleting associated segments in T based on the segment's direction (left/right) relative to the sweep line.
   - For intersection events, find intersecting segments above and below the sweep line, then report intersections and adjust T accordingly.

4. **Correctness**:
   - The algorithm correctly identifies all intersection points using Lemma 2.2, proving that it computes intersection points and associated segments accurately.
   - Lemma 2.3 establishes its time complexity as O((n + k)logn), where k is the number of intersections, which can be further refined to O((n + I)logn), where I is the total number of intersections.

5. **Space Complexity**:
   - Taking storage into account, Theorem 2.4 shows that all intersection points with associated segments can be reported in O(nlogn+I logn) time and O(n) space by only storing intersection points among currently adjacent segments on the sweep line.

This plane sweep algorithm efficiently solves the line segment intersection problem while being output-sensitive, meaning its running time depends on the number of intersections rather than just the input size. This approach is valuable in applications like thematic map overlay in geographic information systems where not all pairs of line segments intersect.


The Art Gallery Problem is concerned with guarding an art gallery or polygonal region using the minimum number of cameras or vertices. A polygon can be triangulated, which means it can be divided into triangles by drawing non-intersecting diagonals between its vertices. This triangulation allows for efficient coverage of the polygon with cameras placed at specific vertices.

**Key Concepts:**

1. **Triangulation**: A division of a simple polygon into triangles using non-intersecting diagonals.
2. **Y-Monotone Polygon**: A polygon that is monotone with respect to the y-axis, meaning its intersection with any vertical line perpendicular to the y-axis results in connected pieces (line segments, points, or empty).
3. **Turn Vertex**: A vertex in a polygon where the direction of movement changes from downward to upward or vice versa when traversing the boundary chain.
4. **Dual Graph**: The graph derived from a triangulation, with vertices representing triangles and edges connecting two triangles that share a diagonal.
5. **3-Coloring**: A coloring scheme for the vertices of a triangulated polygon such that no two adjacent vertices (connected by an edge or diagonal) have the same color, ensuring every triangle has one vertex of each color (white, gray, and black).

**Art Gallery Theorem (Theorem 3.2):** For any simple polygon with n vertices, ⌊n/3⌋ cameras are both necessary and sufficient to ensure that every point in the polygon is visible from at least one camera. This result comes from triangulating the polygon and using a 3-coloring scheme for placing the cameras at selected vertices (preferably the gray ones).

**Partitioning into Monotone Pieces (Section 3.2)**: To improve upon the quadratic time complexity of naive triangulation algorithms, we first decompose the polygon P into y-monotone pieces. A turn vertex in P is a point where the direction of movement changes when walking along the boundary chain. By adding diagonals at these vertices, we can eliminate them and create y-monotone segments that are easier to triangulate.

**Triangulation Algorithm (Anticipated Result - Theorem 3.3):** Using the monotone partitioning approach, a triangulation of a simple polygon P with n vertices can be computed in O(n log n) time. Once we have this triangulation represented as a doubly-connected edge list, it takes linear time to find ⌊n/3⌋ suitable camera positions by performing a depth-first search on the dual graph and selecting the smallest color class (preferably gray vertices). This results in an efficient algorithm for guarding polygons with a worst-case time complexity of O(n log n) for triangulation and O(n) for placing cameras, leading to an overall time complexity of O(n log n) for solving the Art Gallery Problem.


In this section, we discuss the problem of finding the intersection of a set of half-planes, which is a more general version of the casting problem introduced earlier. The goal is to determine all points (x, y) that satisfy n linear constraints simultaneously, where each constraint is of the form ax + by ≤ c.

The intersection of half-planes results in a convex polygonal region with at most n edges and vertices. Figures 4.2(ii) and (iii) demonstrate that this region can be unbounded or even degenerate into a line segment, point, or empty set.

A divide-and-conquer algorithm for computing the intersection of half-planes is presented:

1. If there's only one half-plane (n = 1), return that single half-plane as the result.
2. Otherwise, split the set H into two smaller sets, H1 and H2, each containing approximately n/2 constraints.
3. Recursively compute the intersection of H1 and H2 using INTERSECTHALFPLANES(H1) and INTERSECTHALFPLANES(H2).
4. Use a subroutine called INTERSECTCONVEXREGIONS to combine the results from steps 3a and 3b, yielding the final convex polygonal region C.

The subroutine INTERSECTCONVEXREGIONS computes the intersection of two convex polygons. In this case, it can be adapted to handle unbounded or degenerate cases (segments/points) by following a modified approach from Chapter 2.

To analyze the algorithm's running time, we use a recurrence relation: T(n) = O(1) for n = 1 and T(n) = O(n log n) + 2T(n/2) when n > 1. Solving this recurrence gives T(n) = O(n log² n).

The new, more efficient plane sweep algorithm maintains the left and right boundary edges of C1 and C2 separately as sorted lists of half-planes. The pointers left edge C1, right edge C1, left edge C2, and right edge C2 track which edges intersect the sweep line at any given moment.

The sweep line's y-coordinate is initialized to ystart = min(y1, y2), where y1 and y2 are the y-coordinates of the topmost vertices in C1 and C2, respectively. The pointers are updated according to which edges intersect the current y value of the sweep line.

No explicit event queue is required because the next edge can be determined in constant time using the pointers. When a new edge e appears on the boundary, its associated procedure is called based on whether it belongs to C1 or C2 and if it's on the left or right boundary. The algorithm focuses on handling edges appearing on the left boundary of C1 as an example; similar procedures handle other cases.


The text discusses an algorithm for solving 2-dimensional linear programming problems, which is an extension of the method used to compute the intersection of convex polygons. This algorithm, called Randomized Incremental Linear Programming (RANDOMIZEDLP), aims to find a solution that maximizes a given linear function under a set of linear constraints in two variables.

The key idea behind RANDOMIZEDLP is incremental: it adds the half-planes defining the linear constraints one by one and maintains the optimal solution at each step. The algorithm requires the solution to each intermediate problem (feasible region) to be well-defined and unique, which is ensured by adding two artificial bounding constraints m1 and m2. These constraints ensure that the feasible region remains bounded, preventing unbounded linear programs.

The core of RANDOMIZEDLP lies in Lemma 4.5, which describes how the optimal vertex (the point maximizing the linear function within the feasible region) changes when a new half-plane is added. In simple terms:

1. If the previous optimal vertex is contained in the new half-plane, then it remains the same.
2. If not, then either the current feasible region is empty (infeasible linear program), or the new optimal vertex lies on the boundary line of the half-plane being added.

To find the new optimal vertex when it doesn't lie on the previous one, RANDOMIZEDLP solves a 1-dimensional linear program along the line that bounds the half-plane (Lemma 4.6). This 1D LP can be solved in linear time using standard techniques from operations research.

The main advantage of RANDOMIZEDLP is its randomized nature: it computes a random permutation of the input half-planes and processes them one by one, leading to an expected running time of O(n) for a problem with n constraints. The "expected" here refers to the average case over all possible permutations of the input, which guarantees a good performance regardless of the specific order of input half-planes.

It's worth noting that while this approach is simple and elegant, its practical efficiency might be limited by the worst-case scenario, where the chosen random permutation results in a quadratic running time (O(n^2)). However, extensive testing has shown that in most cases, the algorithm performs much faster than this upper bound.

The text also briefly touches on extensions of this method to higher dimensions and other optimization problems like finding the smallest enclosing disc for a set of points, demonstrating the versatility of this randomized incremental approach.


The text discusses two data structures for efficiently answering rectangular range queries on a set of points in a plane, namely kd-trees and range trees.

1. **kd-trees**: 
   - A kd-tree is a binary tree where each node stores a splitting line (either horizontal or vertical) that divides the space into two subregions. The left child contains points with smaller x-coordinates (or y-coordinates, depending on the depth), and the right child contains points with larger x-coordinates (or y-coordinates).
   - Construction: 
     - Points are sorted on both x and y coordinates.
     - Starting from the root, a point is chosen as the splitting value (median of the current set), and the points are partitioned accordingly. This process continues recursively until every node has a single point or no points at all.
   - Query time: O(√n + k), where n is the number of points and k is the number of reported points. The query algorithm traverses the tree, visiting only nodes whose regions intersect the query rectangle, and reports points stored in leaf nodes that lie within the range.

2. **Range Trees**:
   - Range trees improve upon kd-trees by providing better query times at the cost of increased storage. They are designed to handle 1D range queries more efficiently by organizing points along a line (either x or y) using an auxiliary data structure called a segment tree or a binary search tree.
   - Construction:
     - For each dimension, build a balanced binary search tree (BBST) storing the sorted points. These trees are interconnected to create a multi-dimensional structure.
     - The root of the range tree corresponds to the entire space containing all n points. Internal nodes represent subspaces divided by splitting lines, and leaf nodes contain individual points.
   - Query time: O(log²n + k), where n is the number of points and k is the number of reported points. Range trees achieve this query time by traversing multiple BBSTs simultaneously to identify subspaces that intersect the query rectangle.

In summary, both kd-trees and range trees are effective data structures for answering rectangular range queries on sets of points in a plane. Kd-trees offer better query times when dealing with small numbers of reported points (O(√n + k)), while range trees provide improved performance for larger values of k at the cost of increased storage requirements (O(n log n)). The choice between these data structures depends on the specific application and desired trade-offs between time complexity and space usage.


The text describes a solution to the planar point location problem using a data structure called a trapezoidal map (or vertical decomposition). Here's a summary of the key points:

1. **Planar Point Location Problem**: Given a planar subdivision S with n edges, find the face containing a query point q. If q lies on an edge or vertex, return this information.

2. **Simple Data Structure**: A naive solution involves drawing vertical lines through all vertices of S and storing their x-coordinates in sorted order. This creates slabs, and within each slab, edges are ordered from top to bottom. The face containing q can be found by binary searching the appropriate array for the slab containing q and checking the label associated with the segment just below q.

3. **Quadratic Storage**: While this simple structure has O(log n) query time, its storage requirements are quadratic (O(n^2)) due to storing an array for each slab, resulting from a reﬁnement of S that increases complexity.

4. **Trapezoidal Map**: A better reﬁnement is the trapezoidal map T(S), which is created by extending every endpoint of segments in S vertically upwards and downwards until they meet another segment or the boundary of a bounding rectangle R. This results in a subdivision with trapezoids, triangles, and possibly unbounded faces.

5. **Trapezoidal Map Properties**: Each face in T(S) has one or two vertical sides and exactly two non-vertical sides (Lemma 6.1). Non-vertical sides are contained within segments of S or horizontal edges of R, denoted as top(∆) and bottom(∆).

The trapezoidal map offers a more efficient solution to the point location problem by creating a reﬁnement that maintains manageable complexity while still allowing for fast queries. The next sections will delve into the construction of this map and query algorithms using it.


The text discusses a randomized incremental algorithm for constructing a trapezoidal map (T(S)) and a corresponding search structure D, which allows for efficient point location queries within the trapezoidal map. Here's a detailed summary and explanation of the key points:

1. **Trapezoidal Map**: The trapezoidal map T(S) is constructed from a set S of n non-crossing line segments in general position. It consists of trapezoids formed by these segments, with each trapezoid uniquely defined by its top, bottom, left, and right vertical edges.

2. **Search Structure (D)**: This is a directed acyclic graph with a single root and one leaf for every trapezoid in T(S). Inner nodes have out-degree 2 and can be either x-nodes (labeled by an endpoint of a segment in S) or y-nodes (labeled by the segment itself). The search structure is designed to guide point location queries by testing whether a query point lies left, right, above, or below certain geometric objects.

3. **Algorithm TRAPEZOIDALMAP**: This randomized incremental algorithm constructs T(S) and D simultaneously as follows:

   - **Initialization**: Determine the bounding box R for S, initialize T(S), and D accordingly.
   - **Random Permutation**: Generate a random permutation s1, s2, ..., sn of the elements in S.
   - **Iteration**: For each segment si (i = 1 to n):
     - Find all trapezoids ∆0, ∆1, ..., ∆k intersected by si using FOLLOWSEGMENT algorithm.
     - Remove these trapezoids from T(S) and add new trapezoids resulting from the insertion of si.
     - Update D by removing leaves for ∆0, ∆1, ..., ∆k and adding new leaves for the new trapezoids, connecting them with additional inner nodes as necessary.

4. **FOLLOWSEGMENT Algorithm**: This auxiliary algorithm finds the sequence of intersected trapezoids (∆0, ∆1, ..., ∆k) when a segment si is inserted:

   - Initialize left endpoint p and right endpoint q of si.
   - Perform a query in D starting from p to find ∆0.
   - While q lies to the right of rightp(∆j):
     - Determine if ∆j+1 is lower or upper right neighbor based on si's position relative to ∆j.
   - Return the sequence (∆0, ∆1, ..., ∆j).

5. **Handling Trapezoid Updates**: When updating T(S) and D due to inserting si:

   - If si is completely contained within a trapezoid ∆, four new trapezoids are formed by partitioning ∆ along si's endpoints.
   - If si intersects multiple trapezoids, vertical extensions through si's endpoints partition the intersected trapezoids into three new ones each. Shorten these vertical extensions to merge trapezoids along si.

6. **Query Structure and Time Complexity**: The search structure D facilitates point location queries within T(S). Its expected size is O(n), and query time is O(log n) on average, according to Theorem 6.3. This result holds even when relaxing the assumptions of general position and avoidance of vertical lines/segment intersections through symbolic transformations (Theorem 6.5).

7. **Tail Estimate (Lemma 6.6 & 6.7)**: These lemmas provide probabilistic bounds on the query path length in D, indicating that the maximum search path length is O(log n) with high probability. This allows constructing a data structure with O(n) storage and worst-case O(log n) query time (Theorem 6.8).

In summary, this text presents an efficient algorithm for constructing a trapezoidal map and associated search structure for point location in planar subdivisions of line segments, even when relaxing assumptions about general position or avoidance of vertical lines/segment intersections. The resulting data structure provides good average-case performance with high probability, enabling optimal worst-case query times through additional preprocessing steps if needed.


The text discusses Voronoi diagrams, specifically focusing on their computation for sets of point sites and line segments. Here's a detailed summary:

1. **Voronoi Diagrams for Point Sites:**
   - Definition: A Voronoi diagram subdivides the plane into n regions, each associated with one site from a set P = {p1, p2, ..., pn}. For any point q in region V(pi), dist(q, pi) < dist(q, pj) for all j ≠ i.
   - Structure: Each Voronoi cell (region) is an open convex polygon defined by n-1 half-planes h(pi, p j). The entire diagram consists of edges (line segments and half-lines), forming a connected planar subdivision with at most 2n-5 vertices and 3n-6 edges.
   - Computation: A simple method is to compute each cell separately using the intersection of n-1 half-planes, resulting in an O(n^2 log n) algorithm. Fortune's sweep line algorithm provides a more efficient solution with O(n log n) time complexity and O(n) storage.

2. **Voronoi Diagrams for Line Segments:**
   - Definition: Similar to point sites, but the distance is measured from a point to the closest point on a segment. Bisectors can be curves (parabolic arcs) if the closest points are endpoints or interiors of segments.
   - Structure: A subdivision with straight edges and parabolic arcs, having O(n) vertices, edges, and faces.
   - Computation: The sweep line algorithm for point sites can be adapted to handle segments, resulting in an O(n log n) time complexity using O(n) storage.

3. **Applications:**
   - Post Office Problem (Social Geography): Voronoi diagrams model trading areas of facilities like supermarkets or post offices based on customer proximity and transportation costs.
   - Motion Planning: Voronoi diagrams for line segments can guide collision-free paths for robots, with the arcs between segments offering the most clearance from obstacles.

4. **Farthest-Point Voronoi Diagrams:**
   - Definition: A variant where each site represents a circle of radius r. The goal is to find two circles (Couter and Cinner) that form the smallest annulus enclosing points in P. There are three cases based on point distribution on Couter and Cinner, with four points total on these circles.
   - Application: Used for evaluating roundness of objects by coordinate measurement machines, determining the smallest-width annulus containing a set of nearly circular points.

The text concludes with the observation that Voronoi diagrams' computation can be time-consuming but offers efficient solutions for various spatial problems in fields like geography and robotics.


The text discusses various concepts related to computational geometry, focusing on Voronoi diagrams, duality, and arrangements of lines. Here's a summary and explanation of these topics:

1. **Voronoi Diagrams**:
   - A Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane (called sites or generators). Each region contains all points closer to its corresponding site than to any other.
   - Properties and algorithms for Voronoi diagrams are explored, including the relationship between Voronoi cells and convex hull vertices.

2. **Farthest-Point Voronoi Diagram**:
   - This is a variation of the standard Voronoi diagram, where each cell corresponds to the set of points farthest from a given site (farthest point).
   - Properties such as the tree-like structure of cells and connections to the smallest enclosing annulus are discussed.

3. **Duality**:
   - Duality is a transformation between geometric objects in a plane, mapping points to lines and vice versa while preserving certain properties like incidence and order.
   - The duality transform is used to simplify problems by changing the perspective, often making it easier to solve complex issues related to point sets.

4. **Arrangements of Lines**:
   - An arrangement of lines is a subdivision of the plane created by intersecting a set of lines, consisting of vertices, edges, and faces (some of which may be unbounded).
   - The complexity of such an arrangement depends on the number and configuration of the input lines. Simple arrangements (no three lines meet at a point and no two are parallel) have maximum possible complexities.

5. **Constructing Arrangements**:
   - An incremental algorithm for constructing doubly-connected edge lists representing planar subdivisions, including line arrangements, is presented. This algorithm has a time complexity of O(n^2).

6. **Levels and Discrepancy**:
   - In the context of Voronoi diagrams or line arrangements, "levels" refer to the number of lines lying strictly above a vertex (or point) in the dual structure.
   - The discrepancy problem involves computing how many lines pass through, lie above, or are below vertices in an arrangement, which is crucial for evaluating sampling techniques like supersampling in ray tracing to reduce visual artifacts.

These concepts form the basis for understanding and solving problems related to spatial partitioning, duality transformations, and efficient geometric data structures, with applications ranging from computer graphics (ray tracing) to computational geometry algorithms.


The text discusses an algorithm for computing a Delaunay triangulation (DT) of a given set P of points in the plane, using a randomized incremental approach. Here's a detailed summary and explanation:

1. **Algorithm Initialization**: The algorithm starts by selecting p0 as the lexicographically highest point in P, and introducing two additional points p−1 and p−2 that are sufficiently far away from P so as not to affect the Delaunay triangulation of P.

2. **Triangulation Construction**: The main algorithm, DELAUNAYTRIANGULATION(P), initializes a triangulation T consisting of the triangle p0p−1p−2 and then proceeds iteratively through each point pr (r from 1 to n):

   - It finds the triangle in T that contains pr.
   - If pr lies inside this triangle, it splits the triangle into three triangles by adding edges pr-pi, pr-pj, and pr-pk (and possibly pr-pl if necessary).
   - The procedure LEGALIZEEDGE is then called to ensure the triangulation remains legal (i.e., all edges are non-illegal) after the insertion of pr.

3. **Legalize Edge Procedure**: The subroutine LEGALIZEEDGE flips illegal edges until they become legal:

   - It identifies potentially illegal edges by comparing circles formed by three points in P (including potentially p−1 and p−2).
   - If an edge is found to be illegal, it is replaced with a new set of edges that form a legal configuration (typically through an edge flip operation).

4. **Handling p−1 and p−2**: The algorithm symbolically treats points p−1 and p−2 as infinitely far away in both the point location and illegal edge tests, ensuring they don't influence the Delaunay triangulation of P. They are connected to all points on the convex hull of P (right for p−1 and left for p−2).

5. **Point Location Data Structure**: To efficiently locate pr in T during its insertion, a point location data structure D is maintained alongside T:

   - D represents a directed acyclic graph where leaves correspond to triangles in T.
   - As triangles are split or merged due to point insertions, changes are reflected in D without altering the triangulation's correctness.

6. **Correctness and Analysis**: The algorithm is proven correct by showing no illegal edges remain after processing calls to LEGALIZEEDGE. Its expected running time complexity is O(n log n) when using a suitable point location data structure like a trapezoidal decomposition or a quadtree, which allows for efficient triangle search and edge flips in the triangulation.

This algorithm offers an effective method for computing Delaunay triangulations, crucial in various applications such as terrain modeling, computer graphics, and mesh generation. By employing a randomized incremental approach combined with legalization techniques, it efficiently maintains a valid Delaunay configuration while inserting points one by one.


This text discusses interval trees, a data structure used for efficiently handling windowing queries on axis-parallel line segments. Here's a detailed summary and explanation:

1. **Problem Context**: Windowing queries involve determining the set of objects (in this case, line segments) that lie within a specified rectangular region or "window" in a 2D space. This problem is relevant for various applications like vehicle navigation systems, flight simulation, printed circuit board design, and more.

2. **Data Structure - Interval Trees**: To address the windowing query problem efficiently, we use an interval tree data structure tailored to axis-parallel line segments.

3. **Segment Intersections with Window**: A segment can intersect a window in four ways:
   - Completely inside the window
   - Touching one boundary once
   - Crossing two boundaries (touching both horizontal and vertical sides)
   - Overlapping one or both boundaries

4. **Range Query on Endpoints**: Most segments will have at least one endpoint within the query window. To identify these intersecting segments, we can perform a range query on the 2n endpoints of the n line segments using a range tree (a 2D generalization of binary search trees). Range trees allow us to find all endpoints falling within the window in O(log2 n + k) time, where k is the number of reported points.

5. **Segment Selection**: After identifying endpoint pairs corresponding to intersecting segments using a range tree, we extract and return those line segments. This approach ensures that we efficiently locate all relevant segments for a given query window.

6. **Storage Efficiency**: Interval trees store n axis-parallel line segments using O(n log n) storage. While the data structure itself is not explicitly defined in this text snippet, it is implied to be an adaptation of range trees optimized for handling axis-parallel segments and windowing queries efficiently.

In essence, interval trees for windowing queries on axis-parallel line segments combine the principles of range trees with segment-specific optimizations to achieve efficient querying within specified rectangular windows. This data structure plays a crucial role in applications requiring fast access to objects confined within particular regions of 2D space.


The provided text discusses several data structures used for geometric queries, specifically focusing on finding segments intersecting a query window or point. Here's a summary of the main points:

1. **Interval Trees**: An interval tree is a binary search tree that stores intervals (or in this case, line segment endpoints) and allows efficient range reporting queries. It uses O(n) storage and can perform queries in O(log n + k) time, where k is the number of reported intervals.

2. **Priority Search Trees**: A priority search tree is a binary search tree that stores points based on their x-coordinates while maintaining a partitioning by y-coordinate. This structure allows for efficient range reporting queries, particularly when the range is unbounded in one direction. It uses O(n) storage and can perform queries in O(log n + k) time.

3. **Segment Trees**: A segment tree is an extension of interval trees, designed to handle segments with arbitrary orientations (not just horizontal). It stores intervals at internal nodes based on their "span" over elementary intervals defined by the endpoints of input segments. Segment trees use O(n log n) storage and can perform queries in O(log n + k) time.

4. **Application to Windowing Problem**: These structures can be applied to solve the windowing problem for axis-parallel line segments, as well as for arbitrary-oriented line segments by representing them with their bounding boxes. The solution involves combining a range tree (for endpoints within the window) with an intersection query on boundary edges and using segment trees or interval trees with priority search trees for segments crossing the window's edge.

5. **Dynamic Data Structures**: There has been significant research on making these static structures dynamic, allowing insertion and deletion of elements while maintaining efficiency. Decomposable searching problems provide a framework for transforming static data structures into dynamic ones.

6. **Higher Dimensions**: These concepts can be extended to higher dimensions, with multi-level segment trees used for stabbing queries in d-dimensional space, achieving storage complexity O(n log^(d-1) n) and query time O(log^d n).

7. **R-trees**: In geographic information systems, R-trees are widely used for storing spatial objects (like points, line segments, polygons) and answering intersection queries. They are designed for disk storage and, despite worst-case linear query time, perform well in practice. Variations like the PR-tree can achieve near-optimal query performance when dealing with axis-parallel hyperrectangles as both data and query objects.

The exercises at the end of the section explore alternative implementations and efficiency analyses for these data structures in various scenarios.


The Painter's Algorithm is a technique used in computer graphics for rendering 3D scenes, particularly in solving the hidden surface removal problem. The goal of this algorithm is to determine which parts of objects are visible at each pixel on the screen, effectively managing occlusions caused by overlapping or intersecting objects.

Here's a detailed explanation of how the Painter's Algorithm works:

1. **Initialization**: Begin with an empty frame buffer (the final image that will be displayed) and a sorted list of polygons (3D shapes like triangles). This sorting can be done based on the polygon's depth (distance from the viewer), typically using a front-to-back or back-to-front method.

2. **Painter's Loop**: Iterate over each polygon in the sorted list:

   - For each pixel covered by the current polygon, check if it's already been painted with a different color. If not, proceed to step 3; otherwise, skip this polygon as it is occluded by another closer polygon.
   
   - Paint the pixel with the color of the current polygon. This involves calculating which pixels fall within the projected silhouette (2D representation) of the 3D polygon on the screen.

3. **Update Frame Buffer**: After painting all visible pixels for the current polygon, update the frame buffer to reflect these changes. This might involve blending the new colors with existing ones using techniques like alpha blending or depth buffering to manage transparency and occlusions correctly.

4. **Repeat until Done**: Continue this process for each polygon in the sorted list. The final image stored in the frame buffer will be the rendered scene, where visible parts of objects replace the hidden ones.

The Painter's Algorithm is effective because it only considers polygons that are closer to the viewer (i.e., have smaller depth values) when rendering. This ensures that closer objects block the view of those further away, simulating real-world occlusions. 

However, this algorithm can be inefficient for complex scenes with many overlapping polygons since it may need to process each polygon multiple times if they are not correctly sorted or if there are transparency effects involved. To address these limitations, more sophisticated techniques like depth buffering, z-buffering, or scanline algorithms are often employed. These methods maintain a depth map in memory and use it to quickly determine which pixels need updating during the rendering process.


The text discusses Binary Space Partition (BSP) trees, a data structure used for efficient hidden surface removal in computer graphics. BSP trees are binary trees that partition space using hyperplanes, dividing it into regions, with each node representing a hyperplane and its children representing the two half-spaces created by the hyperplane. The leaves of the tree store object fragments or entire objects within their corresponding regions.

Two algorithms for constructing BSP trees are presented: 

1. **2DBSP**: This algorithm constructs a BSP tree for a set of line segments in the plane using randomized splitting lines. It generates a binary space partition by recursively dividing the plane with hyperplanes, always choosing the next line segment's containing line as the splitter. Free splits are made when possible (i.e., when a line can divide multiple segments without further fragmentation). The algorithm's expected number of fragments is O(n log n), and it has an expected construction time of O(n^2 log n).

2. **3DBSP**: This algorithm extends the 2D version to triangles in 3D space, also employing randomized splitting planes (planes containing a triangle) while making free splits where possible. The analysis shows that the expected number of object fragments generated is O(n^2), and there exist configurations for which any BSP tree must have size Ω(n^2).

The text also introduces the concept of density to classify scenes based on how close objects are to each other relative to their sizes, affecting the complexity of constructing a BSP. It then presents an algorithm called `LOWDENSITYBSP2D` tailored for low-density scenes (where objects are relatively well-separated), which leverages bounding boxes and guards (vertices of these boxes) to create an efficient BSP tree with size O(n log λ), where λ is the density of the input set.

The `LOWDENSITYBSP2D` algorithm works as follows:

1. Compute the multiset G(S) of 4n bounding-box vertices for a given set S of n objects in the plane.
2. Initialize k = 1, a flag 'done' to false, and an encompassing square U for S.
3. In a loop:
   - Double k, run `PHASE1` with U, G(S), and the new value of k. If all leaf regions intersect at most 5k objects (checked by computing the number of object fragments in each leaf region), set 'done' to true; otherwise, continue the loop.
4. For each leaf µ of the resulting BSP tree T:
   - Compute the set S(µ) of object fragments within the region corresponding to µ.
   - If |S(µ)| > 5k, reset 'done' to false and continue the loop.
5. Replace each leaf µ in T by a BSP tree Tµ computed using `2DRANDOMBSP` on S(µ).
6. Return T.

This algorithm ensures that, for sets of disjoint line segments in the plane, the BSP's size is O(n log λ), where λ is the density of the input set. This approach improves upon the worst-case quadratic bound for constructing BSP trees when dealing with low-density scenes, providing a more efficient solution tailored to such cases.


The text discusses the application of Minkowski sums in robot motion planning, specifically for a translating planar robot moving among non-intersecting polygonal obstacles. The main results are:

1. Minkowski Sums Property: The C-obstacle (configuration space obstacle) of an obstacle P and a robot R is the Minkowski sum of P and -R(0,0), where -R(0,0) denotes the reflection of R about its reference point. This is proven using the definition of Minkowski sums.

2. Complexity of Minkowski Sums: The complexity of a Minkowski sum between two convex polygons P and R with n and m vertices respectively is O(n+m). For non-convex polygons, if one polygon is convex and the other is not, the complexity is O(nm), while for both being non-convex, it's O(n^2m^2). These bounds are tight in the worst case.

3. Free Configuration Space Complexity: For a convex robot R translating among a set of n polygonal obstacles with total edges n, the complexity of the free configuration space Cfree(R,S) is O(n). This is proven by triangulating each obstacle and showing that the C-obstacles form a set of pseudodiscs due to their convexity, which allows the use of Theorem 13.9 to conclude linear complexity for their union (forbidden space).

4. Algorithm to Compute Forbidden Space: An algorithm FORBIDDENSPACE is provided to compute the forbidden space Cforb(R,S) using a divide-and-conquer approach on Minkowski sums of triangulated obstacles. The time complexity of this algorithm is O(nlog2 n), derived from the triangulation step (O(mlogm) for each obstacle with m vertices) and the union computation (using an overlay algorithm, which can be done in logarithmic time given doubly-connected edge lists).

The overall strategy involves reducing the motion planning problem to computing Minkowski sums of obstacles and the robot, leveraging properties of these sums to ensure the resulting configuration spaces are manageable. The triangulation step and recursive union computation allow for efficient calculation of the forbidden space (and consequently, the free space), with a time complexity that scales well with the number of obstacle edges.


The text describes a method for generating non-uniform triangular meshes for a 2D square domain with disjoint polygonal components, using quadtrees as a key component. Here's a detailed summary and explanation of the approach:

1. **Quadtree Construction**: The algorithm begins by constructing a quadtree subdivision within the given square, considering all input components. It splits squares until they reach unit size or cease to intersect any component edge. This ensures that the mesh will be non-uniform, with smaller triangles near component edges and larger ones further away.

2. **Balancing Quadtree**: To ensure well-shaped triangles (specifically 45°-90°-45°) and conforming mesh properties, the quadtree subdivision is made balanced using Algorithm BALANCEQUADTREE. This algorithm ensures that any two neighboring squares differ at most by a factor of 2 in size, which helps avoid issues like many small triangles surrounded by large ones or irregularly shaped triangles.

3. **Mesh Generation**: The subdivision is then triangulated to produce the final mesh:

   - Squares with no internal vertices (and not already intersected by a component edge) are divided by adding diagonals, resulting in well-shaped 45°-90°-45° triangles.
   - For squares with internal vertices on their sides, Steiner points are added at the center, connected to all boundary vertices, producing 45°-90°-45° triangles as well.

4. **Properties of the Mesh**: The resulting mesh has several desirable properties:

   - **Non-uniformity**: Smaller triangles near component edges and larger ones further away, following the non-uniform requirement.
   - **Conforming**: Triangles respect input components' boundaries, ensuring no internal vertices from one triangle to another on shared edges.
   - **Well-shaped**: All triangles are 45°-90°-45°, meeting the angle criteria.

5. **Complexity and Time**: The number of resulting mesh triangles is O(p(S)logU), where p(S) is the sum of perimeters of components in S and U is a scaling factor (2^j). Preprocessing time is O(p(S)log²U).

The approach combines quadtrees' hierarchical subdivision with balancing techniques to efficiently generate well-shaped, non-uniform triangular meshes that respect input components. This method adapts and builds upon earlier work on structured mesh generation, optimizing for both shape and size requirements in the presence of disjoint polygonal obstacles within a square domain.


The text discusses the concept of partition trees for solving 2-dimensional range queries, specifically half-plane range counting problems, which involves determining the number of points in a set lying within a specified half-plane.

1. **Partition Trees**: A partition tree is a hierarchical data structure used to store and efficiently query information about a set of points in the plane. The tree consists of nodes, where each node corresponds to a triangle (or segment) that encloses a subset of the point set. 

2. **Simplicial Partition**: This is a collection of triangles (and possibly segments), where each triangle contains a subset of the points and their union covers all points in the set. The crossing number, which measures how many times a line intersects these triangles, is crucial for the efficiency of the partition tree.

3. **Query Algorithm**: To answer a half-plane range query using a partition tree, we start at the root and traverse down the tree recursively. We only visit children whose enclosing triangles intersect the query half-plane. The points in the queried region are represented as the disjoint union of the canonical subsets (the subset of points within each triangle) of these selected nodes.

4. **Storage Analysis**: The partition tree uses O(n) storage, which is optimal since we need to store information about each point. Each node stores a triangle and possibly some metadata about its enclosed subset of points. 

5. **Query Time Analysis**: The query time depends on the crossing number of the simplicial partition used in the tree. By choosing a suitable simplicial partition (one with low crossing number), we can achieve a query time of O(n^(1/2 + ε)), where ε is a small positive constant. This is suboptimal compared to logarithmic or polynomial-time solutions for simpler 1D problems, but it's the best possible for the more complex 2D half-plane range counting problem using a partition tree structure.

6. **Comparison with Other Structures**: The approach taken here (partition trees) can be compared with other structures like range trees and segment trees. These structures aim to precompute information about all possible subsets that could appear in queries, leading to fast query times. However, the number of such subsets often makes this approach impractical due to storage limitations. Instead, partition trees focus on storing information for canonical subsets (a carefully chosen subset of potential query results) and expressing each query as a union of these precomputed subsets. This trade-off between query time and storage is a common theme in geometric data structures.

In summary, the text introduces partition trees as an effective method to handle 2D range queries, particularly half-plane range counting problems. These trees leverage simplicial partitions to efficiently organize points, enabling fast querying at the cost of suboptimal query times but optimal storage usage. The approach highlights the trade-offs inherent in designing geometric data structures, where faster queries typically require more storage or other compromises.


The text discusses data structures for range searching problems in computational geometry, specifically focusing on triangular (simplex) range searching in the plane. Two primary approaches are presented: partition trees and cutting trees.

1. Partition Trees:
   - A partition tree divides the space into a collection of simpler regions called canonical subsets.
   - For half-plane range searching, the number of canonical subsets is O(n), requiring linear storage. However, to achieve logarithmic query time, one needs approximately quadratic (Ω(n^2)) canonical subsets.
   - The partition tree stores these canonical subsets and uses them for efficient querying. The query time depends on the number of triangles in the partition crossed by the boundary of the query region. For triangular queries, this crossing number is at most 3c√r, leading to a query time of O(n^(1/2 + ε)).
   - Construction time for a partition tree is O(n^(1+ε)), and it can report points in O(k) additional time, where k is the number of reported points.

2. Cutting Trees:
   - A cutting tree partitions the space using disjoint triangles (called a (1/r)-cutting), unlike partition trees that allow overlapping regions.
   - For any set L of n lines in the plane and parameter r, a (1/r)-cutting with O(r^2) triangles can be constructed in O(nr) time using Theorem 16.7.
   - A cutting tree stores the lower and upper canonical subsets for each triangle in the partitioning. These subsets are used to efficiently count or report lines intersected by a query point.
   - Using a two-level cutting tree, lines below a pair of query points can be selected in O(log^2 n) time with O(n^(2+ε)) storage using Lemma 16.9.
   - By combining partition trees and cutting trees, data structures can be designed that use less storage than cutting trees while achieving better query times than partition trees.

The text also mentions higher-dimensional simplex range searching with similar results:
- Simplicial partitions exist for R^d such that the crossing number is O(r^(1 - 1/d)).
- Using these simplicial partitions, a partition tree can be constructed for simplex range searching in R^d with linear storage and O(n^(1-1/d + ε)) query time. The query time can be improved to O(n^(1-1/d)(log n)^O(1)).

Finally, the text highlights that while exact range searching has lower bounds for query times in terms of storage usage, approximate range searching allows for logarithmic query times and linear storage with additional factors.


Title: "Convex Hull Computations"

The convex hull of a set of points in the plane or higher dimensions is the smallest convex polygon (in 2D) or polytope (in higher dimensions) that contains all the points. Computing the convex hull has numerous applications, including computer graphics, computational geometry, and robotics. This article provides an overview of convex hull computations, focusing on algorithms in two and three dimensions.

1. Graham's Scan Algorithm:
Graham's scan is a popular algorithm for computing the convex hull of a set of points in 2D. It operates by sorting the points based on their polar angles with respect to a reference point, typically the leftmost or lowest point (the "extreme" point). The sorted list is then traversed, and any point that forms a left turn with the current convex hull edge is added to the hull. This process continues until all points are processed.

Pseudocode:
1. Find an extreme point p.
2. Sort the remaining points q based on their polar angle with respect to p (counterclockwise order).
3. Initialize the hull H with p and the next sorted point.
4. For each subsequent q in the sorted list, determine if it forms a left turn with the last two edges of H. If so, add q to H.
5. Return H as the convex hull.

Time complexity: O(n log n) due to sorting and O(n) for processing points.

2. Jarvis's March Algorithm (Gift Wrapping):
Jarvis's march is another algorithm for computing the convex hull of a set of points in 2D. It works by selecting the leftmost point, p0, and iteratively finding the next point, pi, that forms a counterclockwise turn with the current edge connecting the last two points (pi-1 and pi-2) on the hull. This process continues until all points are processed or a duplicate is found.

Pseudocode:
1. Find an extreme point p0.
2. Initialize the hull H with p0.
3. While there are unprocessed points, do the following:
   a. Select pi as the next point forming a counterclockwise turn with H's last two edges (pi-1 and pi-2).
   b. Add pi to H.
4. Return H as the convex hull.

Time complexity: O(nh), where n is the number of points, and h is the number of vertices on the hull. In practice, h is often proportional to n, making it O(n^2) in worst-case scenarios but O(n log n) on average.

3. QuickHull Algorithm:
QuickHull is a divide-and-conquer algorithm for computing the convex hull of a set of points in 2D or higher dimensions. It works by selecting an initial extreme point and recursively partitioning the set into two subsets based on their relationship to a hyperplane passing through the current extreme points. The process continues until all points are processed, resulting in a nested sequence of convex hulls that eventually converges to the final convex hull.

Pseudocode:
1. Find an extreme point p0.
2. Initialize the hull H with p0 and recursively partition the set into two subsets using hyperplanes passing through current extreme points.
3. For each subset, repeat steps 2-3 until all points are processed or a duplicate is found.
4. Return H as the convex hull.

Time complexity: O(n log n) in practice, although worst-case time complexity can be as high as O(n^2).

4. Gift Wrapping for 3D Convex Hulls:
The gift wrapping algorithm extends to three dimensions by selecting an extreme point p0 and iteratively finding the next extreme point pi that forms a counterclockwise turn with the current convex hull edge connecting the last two points (pi-1 and pi-2). This process continues until all points are processed or a duplicate is found.

Pseudocode:
1. Find an extreme point p0.
2. Initialize the hull H with p0.
3. While there are unprocessed points, do the following:
   a. Select pi as the next point forming a counterclockwise turn with H's last two edges (pi-1 and pi-2).
   b. Add pi to H.
4. Return H as the convex hull.

Time complexity: O(n^2) in worst


The provided text appears to be an extensive index of terms and concepts related to computational geometry, computer graphics, and algorithm analysis. Here's a detailed summary and explanation of some key topics:

1. **Convex Hull**: The set of all points in a space that form the smallest convex polygon containing a given set of points. It's fundamental in computational geometry with applications in various fields like data compression, pattern recognition, and more.

   - Convex Hull Algorithm (e.g., Graham's Scan): Efficient algorithms to compute the convex hull for a set of points in 2D space.
   - Gift Wrapping Algorithm: Another method for computing the convex hull, particularly useful when dealing with 3D points or other objects.

2. **Triangulation**: Dividing a polygon into triangles without any intersecting edges. This concept is crucial in various fields like computer graphics and computational geometry.

   - Ear Clipping: A popular method for triangulating simple polygons by identifying 'ears' (triangles formed by three consecutive vertices of the polygon).
   - Delaunay Triangulation: Special triangulations where no point is inside the circumcircle of any triangle, useful in mesh generation and data analysis.

3. **Voronoi Diagram**: A partitioning of a space into regions based on distance to points in a specific subset of the space. Each region contains all points closer to its generating point than to any other.

   - Voronoi Cells: The regions defined by Voronoi diagrams, each associated with a unique generating point.
   - Delaunay Triangulation and Voronoi Diagram Duality: The dual relationship between Delaunay triangulations and Voronoi diagrams; they provide alternative ways to represent the same geometric information.

4. **Line Segment Intersection**: Algorithms for determining whether two line segments intersect, with applications in computer graphics (e.g., collision detection) and computational geometry (e.g., range searching).

   - Bentley-Ottmann Algorithm: An efficient method for solving the line segment intersection problem, used to build a planar map from a set of line segments.

5. **Range Searching**: Finding all points in a dataset that lie within a given query region (e.g., point inside a rectangle). Efficient data structures like range trees and k-d trees are employed for this purpose.

   - Range Trees: Balanced binary search trees designed to answer range queries efficiently.
   - k-d Trees: Multidimensional binary search trees optimized for nearest neighbor searches and range queries.

6. **Sweep Algorithms**: A technique for solving geometric problems by processing points or lines in a particular order, often along a sweep line moving through the space.

   - Plane Sweep Algorithm: Applied to solve problems involving line segments (e.g., finding intersections) or polygons (e.g., convex hull computation).

7. **Robotics and Motion Planning**: The study of designing, controlling, and optimizing robotic systems' movements in their environment.

   - Configuration Space: A mathematical representation of the set of all possible positions a robot can assume during its motion.
   - Path Planning: Techniques to find collision-free trajectories for robots moving in complex environments (e.g., using roadmaps or potential fields).

8. **Graph Theory and Algorithms**: Fundamental concepts and methods from graph theory, including trees, shortest paths, and minimum spanning trees, with applications across computer science.

   - Dijkstra's Algorithm: Efficient method for finding the shortest path between nodes in a graph, assuming non-negative edge weights.
   - Prim's/Kruskal's Algorithm: Algorithms for computing the Minimum Spanning Tree (MST) of a connected, undirected graph with weighted edges.

9. **Dynamic Data Structures**: Data structures that efficiently support insertion and deletion operations while maintaining certain properties or providing query capabilities.

   - Dynamic Convex Hull: Maintaining the convex hull of a set of points as they are inserted/deleted dynamically.
   - Link/Cut Tree: A data structure for efficiently updating and querying tree structures under edge insertions/deletions.

These topics form the core of computational geometry, providing the theoretical foundation and algorithms essential for solving various geometric problems in computer science, robotics, and other fields.


### Book-online-Aug0619

Chapter 15, titled "Communication Complexity: Modeling Information Bottlenecks," delves into an information-theoretic model for two-party communication known as communication complexity. Despite its simplicity, this model unveils profound depth and breadth, with basic results having significant implications for understanding a diverse range of computational models.

The chapter begins by introducing the concept of communication complexity and its focus on modeling information bottlenecks in communication scenarios between two parties. This is achieved through the study of how much information must be exchanged between these parties to solve a given problem, with the goal of minimizing this exchange while still achieving the desired result.

The chapter proceeds by presenting several key results and concepts within communication complexity:

1. **Deterministic Communication Complexity**: This measures the minimum number of bits that two parties need to exchange deterministically to compute a function on their inputs. Key problems, such as the equality function and inner product, are examined in this context.
2. **Randomized Communication Complexity**: Building upon deterministic communication complexity, this variant allows for randomness in the communication process. The chapter explores how randomness can help reduce the number of bits needed for communication, sometimes dramatically so.
3. **Information Theory Connections**: The discussion highlights the connections between communication complexity and information theory, such as the use of entropy to quantify the amount of uncertainty or surprise in a message. These connections lead to an understanding of fundamental limits on communication, like the channel capacity theorem.
4. **Applications and Extensions**: The chapter reviews various applications of communication complexity in different areas, including VLSI design (time-area tradeoffs), formula lower bounds, proof complexity, extension complexity, and pseudo-randomness. It also discusses how this model suggests extensions to classical problems in information theory and coding theory.
5. **Interactive Information Theory and Coding Theory**: This section delves into the relationship between communication complexity and interactive information theory, which studies problems where multiple rounds of interaction are allowed. The chapter explores protocol compression (information complexity) and its connection to direct sum theorems. Error correction in interactive communication is also examined.

Throughout this chapter, the author emphasizes the surprising depth and breadth that arise from studying such a simple model of two-party communication. The results and concepts discussed not only reveal fundamental limits on information exchange but also provide insights into various computational models and have practical implications in fields like VLSI design and coding theory.


The P vs. NP question is a central open problem in computer science, which asks whether every problem whose solution can be efficiently verified (i.e., problems in the class NP) can also be solved efficiently (i.e., are in the class P). This question has significant implications for mathematics and various fields, as it addresses the possibility of solving all "interesting" problems that we can recognize and verify solutions for.

The class P consists of problems that can be solved efficiently by a deterministic algorithm, while NP includes problems where given a potential solution (a certificate or witness), one can verify its correctness quickly. The P vs. NP question asks if these two classes are equal (P = NP) or not (P ≠ NP).

The importance of the P vs. NP question stems from its philosophical and practical implications:

1. Philosophical significance: If P = NP, it would mean that all problems we can recognize and verify solutions for are also solvable efficiently. This would have profound consequences for our understanding of computation, problem-solving, and the limits of automation.
2. Practical implications: Resolving P vs. NP in either direction (P ≠ NP or P = NP) would significantly impact various fields, such as cryptography, optimization, artificial intelligence, and more. For instance, if P = NP, many hard problems currently used for secure communication and data protection would lose their security guarantees, while new efficient algorithms could revolutionize numerous industries and scientific disciplines.

The P vs. NP question is unique in its broad impact on mathematics and other fields, as it touches upon the possibility of automating and efficiently solving a wide range of problems that humans currently tackle using creativity, intuition, and heuristics. The question's resolution would provide insights into the nature of computation, problem-solving, and the limits of automation, potentially transforming our understanding of these concepts.

The P vs. NP question is also closely related to other areas in computer science, such as proof complexity, approximation algorithms, and the study of hardness and randomness. Despite decades of research, the question remains unresolved, and its resolution continues to be an active area of investigation in theoretical computer science.


Boolean circuits are a fundamental model of computation that can be seen as the hardware counterpart to software algorithms. They compute functions by applying a sequence of Boolean operations, or gates, to input bits, producing output bits. The universal set of gates typically used is {∧ (AND), ∨ (OR), ¬ (NOT)}.

A Boolean circuit consists of input wires, gate wires, and an output wire. Each gate wire can be connected to the output of one or more previous gate wires or input wires. The function computed by a circuit is determined by the specific connections between gates and the order in which they are applied.

Boolean circuits can be represented graphically, with gates depicted as nodes and wires connecting them. The fan-in of a gate refers to the number of input wires it has, while the fan-out is the number of output wires connected to its result. The depth of a circuit is the length of the longest path from an input to the output, and its size (or complexity) is the total number of gates.

Boolean circuits have been extensively studied in the context of computational complexity theory due to their connection with Turing machines and their amenability to combinatorial analysis. Research on lower bounds for Boolean circuits aims to demonstrate that certain functions require exponentially many gates or have a high depth, implying that they cannot be computed efficiently by any circuit.

Lower bound techniques for Boolean circuits include:

1. **Time-space tradeoffs**: These methods relate the time complexity of a function with its space (memory) requirements. By showing that a function requires exponential time to compute even when given auxiliary space, one can establish a lower bound on the circuit size.
2. **Gate elimination and separation**: Techniques that remove or separate gates in a circuit without changing its computed function can reveal structural properties that lead to lower bounds. For instance, if a gate's removal significantly alters the function, it suggests that the gate is essential for computation, implying a lower bound on the circuit size.
3. **Circuit lower bounds via communication complexity**: This approach exploits the connection between circuit complexity and communication complexity by demonstrating that certain functions require exponential communication between parties to compute, implying a high circuit depth or size.
4. **Algebraic methods**: These techniques use properties of Boolean functions, such as their degree or symmetry, to establish lower bounds on circuit complexity. For example, the famous De Morgan's laws and other identities can be used to relate the complexity of different circuits computing the same function, leading to lower bound results.
5. **Natural proofs**: Introduced by Razborov and Rudich, natural proofs are a framework for constructing lower bounds based on properties that are "natural" or easy to express algebraically. This approach aims to avoid explicit circuit constructions while still demonstrating the existence of hard functions. However, it has limitations, as shown by the Natural Proofs Barrier, which states that certain types of natural properties cannot lead to strong circuit lower bounds unless P ≠ NP is false.
6. **Circuit lower bounds via interactive proofs**: This method uses interactive proof systems to establish circuit lower bounds. By demonstrating that a function requires exponential communication or computational power in an interactive setting, one can infer a high circuit depth or size.
7. **Lower bounds from hardness amplification**: Techniques like the PCP theorem and other hardness amplification methods can be used to construct functions that are hard to compute by circuits of a certain size, even when given additional resources such as randomness or auxiliary inputs.

Despite significant progress in Boolean circuit lower bounds, proving strong results separating P from NP remains elusive. Many of the mentioned techniques have established lower bounds for specific function families or under restricted assumptions but have not yet led to a general separation result between P and NP. The challenge lies in finding a unifying approach that can capture the full power of non-uniform algorithms, including Boolean circuits, while avoiding the limitations imposed by relativization and algebrization barriers.


Proof complexity is a subfield of mathematical logic that studies the length and structure of proofs for propositional tautologies. The main goal is to classify these tautologies based on the difficulty of their proofs, similar to how circuit complexity classifies functions according to computational complexity. Proof systems are polynomial-time algorithms that decide whether a given statement T has a proof π such that M(π, T) = 1.

Key features of a good proof system include completeness (every true statement has a proof), soundness (no false statement has a proof), and verification efficiency (easily checking the validity of proofs in polynomial time). An example of a simple truth-table proof system, MTT, is provided, which accepts a formula T as a theorem if evaluating it on all possible inputs results in true. However, MTT's proofs are exponential in length regarding the number of variables, leading to the concept of proof length and polynomially bounded proof systems.

Theorem 6.5 (Cook & Reckhow, [CR79]) states that a polynomially bounded proof system exists if and only if NP = coNP. This theorem highlights the connection between proof complexity and computational complexity.

The chapter then explores three concrete proof systems: algebraic (Nullstellensatz and Polynomial Calculus), geometric (Cutting Planes and Sum-of-Squares), and logical (Frege and Resolution).

1. Algebraic Proof Systems:
	* Nullstellensatz (NS) proof system: Based on Hilbert's Nullstellensatz, which states that if the polynomials f₁, f₂, ..., fₘ have no common root, then the constant function 1 is in the ideal generated by these polynomials. A natural measure of proof length is the description length of the polynomials as lists of all their coefficients (dense representation).
	* Polynomial Calculus (PC) proof system: Introduced by Clegg, Edmonds, and Impagliazzo, this system's lines are polynomials with two deduction rules capturing the definition of an ideal: addition of two ideal elements and multiplication of an ideal element by any polynomial. Proof length is determined by the minimal degree d of any proof, practically determining the proof length in this representation.
2. Geometric Proof Systems:
	* Cutting Planes (CP) proofs: A refutation derives a basic contradiction from axioms represented as linear inequalities with integer coefficients. The pigeonhole principle PHPm n has polynomial-size CP proofs, but exponential lower bounds exist for other tautologies, such as CLIQUE k n, which encodes many instances of the pigeonhole principle.
	* Sum-of-Squares (SOS) proofs: A stronger geometric proof system for polynomials over the reals, introduced for optimization, machine learning, and complexity. It utilizes the Positivstellensatz theorem to prove that a set of real polynomials has no common root by exhibiting other polynomials satisfying specific conditions. SOS is more powerful than CP and PC, as it can be exponentially stronger for certain tautologies while remaining automatizable (polynomial-time proof finding).
3. Logical Proof Systems:
	* Frege proof system: Allows manipulating formulas without restrictions. The cut rule, Modus Ponens, is the nontrivial derivation rule. Frege systems can polynomially simulate Polynomial Calculus and Cutting Planes systems.
	* Resolution proof system: A subsystem of Frege with interesting structural limits on formulas. It has a single nontrivial derivation rule (Modus Ponens) and is the most widely studied system due to its use in automated theorem provers. The major open problem in proof complexity is finding any tautology that does not have polynomial-size proofs in the Frege system (Open Problem 6.10).


The text discusses abstract pseudo-randomness, a concept that extends computational pseudo-randomness to arbitrary families of observers. It highlights three examples of pseudo-random properties: Ramsey graphs, weak tournaments, and good codes.

1. Ramsey graphs: A graph is (r log n)-Ramsey if it contains no clique or independent set of size r. Erdős proved that almost every graph on n vertices is (3 log n)-Ramsey. The challenge is to find explicit pseudo-random objects, like a Ramsey graph, with better parameters.

2. Weak tournaments: A tournament is w-weak if it contains no dominating set of size w. Erdős proved that almost every tournament is (1/3 log n)-weak. The Paley tournament, suggested by Graham and Spencer, is an explicit pseudo-random object in this setting, constructed using the quadratic character function χ over finite fields.

3. Good codes: A subspace V of dimension n/10 over F_n^2 is a distance-d linear code if every two vectors in V differ in at least d coordinates. Varshamov proved that almost every such subspace is a distance-n/10 linear code. The challenge is to find explicit, efficient codes with these properties.

The text also introduces the concept of pseudo-randomness in general, where a property S ⊂ U is ϵ-pseudo-random if |S| ≥ (1 - ϵ)|U|. This notion is used to study various problems in mathematics and computer science, often referred to as "finding hay in haystacks." The text discusses how this framework can be applied to the Riemann Hypothesis and the P vs. NP question.

For the Riemann Hypothesis, the drunkard's walk analogy is used to formulate a pseudo-random property: a sequence z ∈ U_n (the set of all n-walks) is d-homebound if it ends up within d of the pub (i.e., |∑z_i| ≤ d). The Riemann Hypothesis is equivalent to the statement that the Möbius sequence µ_n is n^(1/2 + δ)-homebound for every δ > 0.

The P vs. NP question can be framed in terms of pseudo-randomness by asking whether SAT (the problem of determining whether a boolean formula is satisfiable) is hard to compute. Almost all functions are hard to compute, and the challenge is to determine if SAT falls into this category.


Interactive proof systems (IP) are a type of probabilistic proof system where both the prover and verifier engage in a series of interactions to establish the validity of a statement. In IP, the verifier is allowed to be probabilistic, meaning it can toss coins and make random decisions during the interaction with the prover. The prover's goal is to convince the verifier that a given input belongs to a target set S, while the verifier aims to ensure correctness with high probability.

IP consists of two main components: completeness and soundness. Completeness requires that the prover always convinces the verifier when the input is indeed in S. Soundness demands that no cheating prover can convince the verifier that an input not in S belongs to S with a probability significantly greater than 1/2, regardless of the prover's computational power.

The class IP contains all sets S for which there exists a probabilistic polynomial-time verifier that satisfies these conditions. Initially, it was believed that IP could only prove statements in NP, as deterministic verifiers cannot gain additional power from interaction. However, the introduction of multiple provers in the MIP (multiple-prover interactive proof) model by Ben-Or et al. [BOGKW89] revealed its potential to encompass a broader range of problems.

One significant milestone was achieved by Lund, Fortnow, Karloff, and Nisan [LFKN90], who demonstrated that IP proofs can be given for every set in coNP. This result implies that tautologies, which are not expected to have short NP-proofs (as it would lead to NP = coNP), do have short interactive proofs within the IP class.

Furthermore, Shamir [Sha92] provided a complete characterization of IP by showing its equivalence to PSPACE, the class of functions computable with polynomial memory and potentially exponential time. This equivalence revealed that IP can capture problems considered much harder than NP and coNP, such as finding optimal strategies for games.

In summary, interactive proof systems (IP) are a powerful tool in theoretical computer science that allows for the efficient verification of statements using probabilistic interactions between a prover and verifier. The class IP encompasses problems in coNP, demonstrating its ability to prove statements beyond NP's scope. Additionally, Shamir's characterization of IP as equivalent to PSPACE showcases its capacity to handle complex problems previously thought to be intractable.


The text discusses several topics related to quantum mechanics, quantum computing, and their interaction with classical complexity theory. Here's a detailed summary:

1. **Quantum Algorithms**: Quantum algorithms leverage quantum phenomena like superposition and entanglement to potentially solve certain problems exponentially faster than classical algorithms. Examples include Shor's algorithm for factoring large numbers and Grover's search algorithm. However, the power of quantum algorithms is not fully understood, with most experts believing they are stronger than probabilistic algorithms (BPP) but not capable of solving NP-complete problems.

2. **Quantum Computing Challenges**: Building a practical quantum computer faces several challenges:
   - **Entanglement Maintenance**: Quantum states must be maintained in a complex entangled form, which is difficult due to the fragility of these states.
   - **Decoherence Noise**: The state of a quantum computer is affected by its environment, leading to errors and loss of coherence. Error-correcting codes are used to combat this, but their practical implementation is still uncertain.
   - **Theoretical Limitations**: Some researchers propose that quantum mechanics may need revision for very large systems, which could explain the slow progress in building a large-scale universal quantum computer.

3. **Quantum Proofs and Hamiltonian Complexity**: The concept of proof is extended to the quantum setting with QMA (Quantum Merlin-Arthur), where a BQP machine verifies a short quantum state witness. Kitaev discovered a QMA-complete problem, the ground state energy of local Hamiltonians, which has led to a better understanding of the complexity of finding ground states in various quantum systems.

4. **Ground States and Entanglement**: The study of ground states and their entanglement structure has revealed deep connections between quantum mechanics and computational complexity. The area law conjecture suggests that for gapped systems, entanglement is proportional to the cut size of the interaction graph, with implications for understanding the computational complexity of finding these states.

5. **Quantum Interactive Proofs**: These are proof systems where a BQP prover interacts with a classical BPP verifier. This concept is motivated by testing quantum mechanical predictions and could have broader applications in certifying randomness from quantum devices under certain assumptions.

6. **Quantum Randomness Certification**: No-signaling, a weak physical assumption that allows boxes to produce correlated outputs without communication, has been used to demonstrate the power of quantum strategies over classical ones in games like CHSH. This gap between classical and quantum strategies can be exploited to certify randomness from no-signaling devices using techniques inspired by randomness extractors.

In summary, while significant progress has been made in understanding quantum mechanics' computational implications, many open questions remain. The development of practical quantum computers continues to face substantial technical hurdles, and the interplay between quantum theory and classical complexity theory offers rich areas for further exploration.


The text discusses several interactions between computational complexity theory and various mathematical fields. Here are the main points:

1. Number Theory: Gauss' challenge of efficiently testing primality and factoring integers has led to significant developments in algorithmic number theory. The AKS04 deterministic algorithm for primality testing, which uses polynomials, was a breakthrough that de-randomized a previous probabilistic algorithm by Agrawal, Kayal, and Saxena (AB03). Factoring integers remains an open problem with implications in cryptography.

2. Combinatorial Geometry: The Kakeya needle problem asks for the smallest area of a planar region containing a unit-length segment in every direction. Besicavitch's solution shows that this area can be arbitrarily close to zero, while Dvir's work using the polynomial method proved the finite field analog conjecture, establishing lower bounds on the size of Kakeya sets in higher dimensions.

3. Operator Theory: The Kadison-Singer problem, originally posed by Dirac and Kadison-Singer in 1959, concerns the extension of pure states from diagonal operators to the full algebra of continuous linear operators on a Hilbert space. Marcus, Spielman, and Srivastava (MSS13b) recently resolved this problem using techniques from operator theory, discrepancy theory, Banach space theory, signal processing, and probability.

4. Metric Geometry: The study of distortion in metric spaces has connections to computational complexity. Linial, London, and Rabinovich (LLR95) applied geometric ideas to algorithmic problems like sparsest cut, while Khot and Vishnoi (KV05) used computational assumptions to prove lower bounds on the distortion of embedding L2^2 into L1.

5. Group Theory: Computational complexity theory has influenced group theory by introducing concepts like generation and random generation problems. These problems involve determining whether an element belongs to a subgroup generated by a set of elements, or generating a random element in the subgroup, without explicitly listing all subgroup elements. The discrete logarithm problem and integer factoring problem are examples related to these issues in specific groups.

These examples demonstrate how computational complexity theory has enriched various mathematical fields by providing new perspectives, techniques, and problems. Conversely, insights from these mathematical areas have also contributed to the development of computational complexity theory.


14.3 Finite Automata and Counting:

This section explores the power of finite automata with limited memory, specifically focusing on counting problems. It demonstrates that even with a fixed amount of memory, these automata can solve tasks previously thought impossible due to lower bounds on space complexity.

14.3.1 Deterministic Finite Automata (DFAs):
- DFAs are Turing machines with constant space, allowing 2-way input access but no writing capability.
- They compute regular languages, which are sets of sequences with strong periodicity structure.
- Majority function, which determines if a binary sequence has more 1's than 0's, cannot be computed by DFAs.

14.3.2 Nondeterministic Finite Automata (NFAs) and Alternating Automata:
- Rabin and Scott [RS59] proved that deterministic 2-way finite automata (2DFAs) are equal in power to nondeterministic ones (2NFAs).
- They also showed that alternating 2-way finite automata (2AFA) compute no more sets than 2DFAs, i.e., only regular languages.

14.3.3 Probabilistic Finite Automata (PFAs):
- Adding randomness to DFAs creates PFAs, which can only compute regular languages in the 1-way model (1PFA).
- Surprisingly, 2-way probabilistic finite automata (2PFAs) can count arbitrarily high.

Theorem 14.10 [Fre81]: There is a 10-state 2PFA that computes Majority with probability ≥2/3 on every input, and for every ϵ > 0, there is an integer c = c(ϵ) and a c-state 2PFA that computes Majority with probability ≥1 −ϵ for every input.

14.3.4 Nonuniform Finite Automata:
- Allowing advice (input-independent information) to nonuniform finite automata, Barrington [Bar86] proved that Majority can be computed with only 3 bits of memory using a 5-state nonuniform 2DFA.
- This result holds for every function computed by a polynomial-size Boolean formula.

The proof uses a nonsolvable group and reversibility in the automaton's construction, demonstrating that short advice can be powerful even with limited memory.

14.3.5 Implications:
- These results show that some lower bounds on space complexity may be false due to the surprising power of finite automata with limited resources.
- They also highlight the importance of understanding the relative power of different computational models and their capabilities.


On-line algorithms are designed to make periodic decisions without knowing future events. They are used in various real-life scenarios such as investment, gym memberships, dating, memory management, and taxi dispatching. The main challenge is to determine the best course of action for each incoming signal when future signals are unknown.

Competitive analysis, proposed by Sleator and Tarjan [ST85], is a bold approach to modeling the quality of on-line algorithms. This method disregards any knowledge about potential future event distributions and instead compares the performance of the on-line algorithm against the best possible algorithm with hindsight, known as an "oﬀ-line" or "clairvoyant" algorithm.

In competitive analysis, the goal is to minimize the competitive ratio, which is the worst-case ratio of the cost incurred by the on-line algorithm to that of the optimal off-line algorithm for any input sequence. A lower competitive ratio indicates better performance of the on-line algorithm compared to the ideal offline solution.

Some examples of on-line algorithms include:

1. Ski rental problem: Renting skis for a weekend trip, where the demand (need for skis) is unknown until the weekend arrives. The goal is to minimize the total cost over multiple trips by deciding whether to rent or buy skis each time.
2. Cache-oblivious algorithms: Managing cache memory in computer systems without knowing the access pattern of future data requests. The aim is to optimize cache usage and minimize cache misses.
3. Auctions and combinatorial optimization: Making bids in auctions or solving combinatorial optimization problems, where the optimal solution depends on incomplete information about future events or other bidders' actions.
4. Machine learning and online prediction: Updating models based on streaming data without knowing future instances. The objective is to maintain model accuracy and adapt to changing patterns in the data.

On-line algorithms have applications in various fields, including game theory, convex optimization, learning theory, and inductive inference. They are essential for making decisions under uncertainty and can significantly impact efficiency and performance in real-life scenarios.


The two main approaches to understanding computational learning are the linguistic/recursion-theoretic approach (Section 17.3) and the statistical, PAC (Probably Approximately Correct) learning approach (Section 17.4).

1. Linguistic/Recursion-Theoretic Approach:
   - Assumes that data arrives adversarially with a finite "teaching" period after which the learner must make no mistakes.
   - Identification in the limit is a key concept, where an algorithm makes only a finite number of mistakes and then converges to the correct answer.
   - Gold's enumeration technique (identiﬁcation through enumeration) is a powerful method for identifying complex function families, provided they are enumerable and each function can be eﬃciently tested for consistency with data.
   - Examples of identifiable classes in the limit include Boolean functions computable by polynomial-time algorithms (P), rational polynomials over Q, and hyperplanes with margin µ.

2. Probably Approximately Correct (PAC) Learning:
   - Assumes that data is generated randomly, with a focus on quantitative bounds on sample size, algorithmic eﬃciency, and tolerating prediction errors with low probability.
   - A concept class F = {f : X →{0, 1}} is PAC-learnable if there exists an efficient (polynomial time) algorithm that can learn f ∗∈F within error ϵ and confidence δ using T samples from an arbitrary distribution D on X.
   - The VC dimension (Vapnik-Chervonenkis dimension) of a concept class F is the key combinatorial parameter determining PAC learnability:
     - If VC dim(F) is finite, then F is PAC-learnable with sample complexity T ≈ O(1/ϵ (d log 1/ϵ + log 1/δ)), where d = VC dim(F).
     - Proper PAC learning, where hypotheses are restricted to the target class F, has similar learnability conditions.
   - Eﬃciency and optimization: While VC dimension determines learnability in principle, eﬃcient algorithms are crucial for practical implementation. The Vapnik-Chervonenkis theorem provides optimal learning rates but does not directly offer eﬃcient algorithms.
   - Agnostic PAC learning relaxes the assumption of knowing the target class F by focusing on the hypothesis class H and aiming to approximate the best hypothesis h∗∈H with respect to the data. The same VC dimension-based learnability conditions apply, showing that the VC dimension of H determines agnostic learnability as well.
   - Compression and Occam's razor: PAC learning provides a framework for proving Occam's Razor (prefer simpler explanations) through compression arguments. A simpler consistent explanation is more likely to be flagged as incorrect if it is indeed incorrect, leading to learnability. Conversely, learnability implies compression in the sense that large labeled samples from F can be compressed to shorter consistent hypotheses with high probability.

These two approaches offer diﬀerent perspectives on learning, with the statistical PAC framework dominating practical applications due to its focus on eﬃciency and robustness in real-world scenarios.


The text discusses recent advances in cryptography, focusing on three main topics: homomorphic encryption, delegation of computation, and program obfuscation.

1. Homomorphic Encryption (HE): This is a public-key encryption scheme that enables computations on encrypted data without decrypting it first. Initially, partial HE schemes were developed for addition or multiplication operations over specific rings. However, the challenge was to create a fully homomorphic encryption (FHE) system that supports both operations simultaneously. After 40 years of research, Gentry's PhD thesis presented an FHE scheme in 2009, based on lattice problems. The construction was complex and costly, but it sparked further work to simplify and improve its efficiency. Notable advancements include [BV14], which simplifies Gentry's initial construction and relies on the learning with errors assumption.

2. Delegation of Computation: This application addresses scenarios where a weak computational device (Alice) wants to delegate a computationally heavy task to a stronger machine (Bob). Alice may not trust Bob due to potential laziness, faults, or malice. The goal is for Bob to convince Alice that the answer is correct efficiently. Goldwasser, Kalai, and Rothblum [GKR08] formulated doubly efficient interactive proof requirements: polynomial-time provers and nearly linear-time verifiers. The problem was solved by Kalai, Raz, and Rothblum [KRR14], who demonstrated that assuming FHE, there exists a 1-round doubly efficient interactive argument for any computation.

3. Program Obfuscation: This is a powerful cryptographic primitive that transforms Boolean circuits into obfuscated versions while preserving functionality but hiding all other aspects about them. An ideal obfuscator would map input C to output O(C) such that O(C) computes the same function as C and reveals no more information than black-box access. Software companies would benefit from such an algorithm, which remains an active area of research due to its numerous potential applications and challenges.

In summary, these recent advances in cryptography demonstrate progress in creating sophisticated encryption schemes (homomorphic and program obfuscation) and efficient delegation methods for computationally intensive tasks while preserving privacy and security guarantees.


The Theory of Computation (ToC) is a fundamental and revolutionary field that has significantly impacted various disciplines beyond computer science and engineering. This chapter explores ToC's interactions with different fields, including its "parents" (computer science and engineering, mathematics), neighboring disciplines (optimization, coding and information theory, statistical physics), and more remote sciences (biology, economics).

1. Close collaborations and interactions:
   - Computer Science and Engineering (CS&E): ToC has contributed to the development of foundational theories underlying computational systems. These include algorithms, data structures, computational complexity, databases, system verification, programming languages, and more. Theoretical ideas often preceded practical applications and remained useful even after technologies became obsolete.
   - Mathematics: Early on, ToC used mathematical techniques from combinatorics. As the field expanded, it required tools from diverse mathematical areas like topology, geometry, algebra, analysis, number theory, and algebraic geometry. Collaborations led to new purely mathematical results in these fields and rethought existing structures.

2. Optimization: ToC's connections with optimization are natural since both study efficient algorithms. Breakthroughs like the PCP theorem, which developed from computational complexity considerations, have enriched the understanding of algorithmic power and limits within optimization. LP and SDP hierarchies, powerful algorithmic paradigms in optimization, have become clearer due to works connecting these areas with ToC.

3. Coding and Information Theory: ToC's relationship with coding and information theory involves studying efficient encoding and decoding methods for data transmission. Error-correcting codes, rate-distortion theory, and source coding are examples of this interaction.

4. Statistical Physics: ToC has found applications in statistical physics through the study of computational complexity in physical systems. This includes understanding phase transitions, computing properties of spin systems, and developing algorithms for simulating quantum systems.

5. Biology and Economics: ToC's impact on biology involves modeling biological processes using computational methods, such as cellular automata, evolutionary algorithms, and networks. In economics, ToC has contributed to algorithmic game theory, mechanism design, and auction theory.

6. Philosophy and Technology: ToC's theoretical foundations have influenced philosophical discussions on the nature of computation, intelligence, and consciousness. It also plays a crucial role in technological advancements, shaping our understanding of information processing and communication systems.

The study of ToC transcends human-made artifacts and explores natural and artificial processes of all types. Its expanding connections and interactions with various sciences integrate computational modeling, algorithms, and complexity into theories of nature and society, driving a new scientific revolution.


The text discusses various contributions and interactions between computational theory (CT) and other disciplines, highlighting the impact of CT on understanding fundamental concepts and creating novel definitions. Here's a detailed summary and explanation:

1. **Computational Complexity as a Tool for Understanding Fundamental Concepts**:
   - Computational theory has been used to provide fresh perspectives on long-standing philosophical, scientific, and societal concepts. For instance, Alan Turing's "Computing Machinery and Intelligence" (1950) offers a new approach to defining intelligence.
   - Other examples include:
     - **Intelligence**: Turing proposed the "Imitation Game" or "Turing Test" to define artificial intelligence, focusing on a machine's ability to mimic human conversation.
     - **Game**: CT has contributed to game theory by introducing complexity-theoretic concepts like PPAD (Polynomial-time Approximation of Discrete Problems) class, which helps understand the computational difficulty of finding equilibria in games.

2. **Novel Definitions and Redefinitions**:
   - Computational theory has sometimes redefined or provided alternative definitions for various notions:
     - **Collusion, Coordination, Conflict**: CT has helped formalize these concepts by considering the computational resources required to achieve them, leading to new perspectives on their nature and significance.
     - **Equilibrium**: Complexity theory has contributed to game theory by introducing computational aspects into equilibrium concepts, such as proving that computing Nash equilibria is hard for classes like PPAD.
     - **Evolution**: Les Valiant's work on evolvability views evolution as a restricted form of his PAC learning methodology, offering a computational perspective on biological processes.

3. **Interactions with Other Disciplines**:
   - Computational theory has interacted with and influenced various fields, including:
     - **Physics**: In quantum gravity research, CT concepts like quantum circuit complexity are being used to analyze black hole radiation and wormhole length problems, offering new ways to understand fundamental physical phenomena.
     - **Economics**: Complexity theory has enriched game theory by introducing computational aspects into equilibrium concepts (e.g., PPAD class for computing equilibria) and by providing hardness results limiting the power of mechanisms. It has also contributed to understanding information asymmetry in financial markets, as seen in [ABBG11].
     - **Social Science**: The advent of the Internet has led to collaborations between computer and social scientists, enriching social science by making it more quantitative. Examples include studying network growth, power structures, and dynamic processes on networks using CT tools.

4. **Philosophical Implications**:
   - The text emphasizes that computational theory's impact extends beyond practical applications to philosophical considerations:
     - It has shown that computation can provide fresh insights into fundamental concepts like intelligence, coordination, and evolution.
     - By introducing complexity-theoretic aspects into these notions, CT has highlighted their inherent computational difficulty or resource requirements, offering new perspectives on their nature and significance.

In summary, computational theory's impact goes beyond solving practical problems; it has contributed to redefining and understanding fundamental concepts across various disciplines by providing a computational perspective. This interdisciplinary influence highlights the intellectual depth of computational theory and its potential to illuminate long-standing philosophical, scientific, and societal questions.


Summary:

Title: "The Theory of Computation (ToC): Past, Present, and Future"

This chapter provides an overview of the Theory of Computation (ToC), its history, contributions, and potential future developments. The author argues that ToC is a distinct and essential scientific discipline, with profound implications for mathematics, science, technology, and society.

1. Historical Background:
   - The formalism for computation was established by Alan Turing's 1936 paper introducing the Turing machine model, leading to the creation of a computational theory.
   - ToC's focus on understanding computational limits, efficiencies, and possibilities has uncovered significant insights, making it one of the greatest scientific achievements in history.

2. Contributions to Mathematics:
   - ToC has developed fundamental principles and tools that enable progress in various mathematical areas, such as number theory, geometry, and graph theory.
   - It has contributed to understanding computational hardness, randomness, and pseudorandomness, with implications for cryptography, coding theory, and machine learning.

3. Impact on Science and Technology:
   - ToC's concepts and methods are crucial for modeling scientific phenomena, simulating complex systems, and developing algorithms in fields like physics, biology, economics, and artificial intelligence.
   - Its study of computational complexity has provided insights into the limits of efficient computation, informing the design of hardware, software, and computational models.

4. Social and Philosophical Implications:
   - ToC explores profound intellectual questions about computation, such as the nature of intelligence, the possibility of artificial general intelligence, and the philosophical implications of algorithmic decision-making.
   - It contributes to understanding the foundations of mathematics and logic, with relevance to Gödel's incompleteness theorems and Hilbert's program.

5. Challenges and Future Directions:
   - As ToC expands and diversifies, it faces challenges related to maintaining a central core of fundamental ideas while accommodating new research areas and applications.
   - The author suggests that preserving cohesion within the field is crucial for facilitating the exchange of fundamental insights across disciplines.
   - They propose administrative changes in undergraduate education, such as creating dedicated ToC majors, certificates, and programs to support this goal.

6. Conclusion:
   - The author emphasizes that understanding ToC's past achievements and potential is vital for its continued development and integration with other scientific disciplines.
   - They advocate for recognizing ToC as an independent academic discipline, fostering interdisciplinary collaborations, and promoting its core principles in undergraduate education.

In essence, this chapter highlights the significance of ToC as a distinct scientific discipline with profound implications across various domains. It underscores the need for maintaining a central core while embracing diversity and interdisciplinary collaborations to drive future advancements in computation theory, mathematics, science, and technology.


Title: "Graph Theory and Its Applications"

This paper presents an overview of graph theory, focusing on connections and applications within the field. Graph theory is a mathematical discipline that studies graphs, which are structures used to model pairwise relations between objects. The study of graphs has wide-ranging applications in various domains such as computer science, physics, biology, economics, and social networks.

1. Basics of Graph Theory:
   A graph G consists of vertices (or nodes) and edges connecting pairs of these vertices. It can be represented by a pair (V(G), E(G)), where V(G) is the set of vertices and E(G) is the set of edges, each being an unordered pair of distinct vertices.

2. Graph Types:
   Different types of graphs include undirected, directed, weighted, unweighted, cyclic, acyclic (trees), connected, disconnected, simple (no loops or multiple edges between the same nodes), and multigraphs (allows loops and multiple edges).

3. Graph Properties:
   Key properties include degree (number of edges incident to a vertex), connectivity (minimum number of elements removed that disconnects the graph), planarity (can be drawn in a plane without edge crossings), chromatic number (smallest number of colors needed for proper coloring), and clique and independent sets.

4. Graph Algorithms:
   Various algorithms are used to solve problems related to graphs, such as depth-first search (DFS) and breadth-first search (BFS) for traversal, Dijkstra's algorithm for shortest paths in weighted graphs, Prim's and Kruskal's algorithms for minimum spanning trees, and network flow algorithms like the Ford-Fulkerson method.

5. Applications of Graph Theory:
   a) Computer Science: Network routing, data structures (adjacency lists/matrices), and combinatorial optimization problems (traveling salesman problem, graph coloring).
   
   b) Physics: Molecular structure analysis, electrical circuits modeling, and quantum computing.
   
   c) Biology: Protein-protein interaction networks, gene regulatory networks, and social network analysis in epidemiology.
   
   d) Economics: Game theory, market equilibrium models, and transportation networks.
   
   e) Social Networks: Analyzing relationships between individuals or organizations, identifying communities and influencers, and modeling information spread.

In conclusion, graph theory is a fundamental area of mathematics with diverse applications across multiple disciplines. Understanding its concepts and algorithms enables better modeling, analysis, and solution of complex problems in various fields.


The provided text is a bibliography of references cited in a research paper titled "Mathematics and Computation" by Avi Wigderson. The paper itself is not included, but the bibliography provides a list of sources that were referenced or used as background for the content within the paper.

Here's a summary of some key topics, authors, and papers mentioned in the bibliography:

1. **Cryptography**: Many references deal with cryptography-related topics such as public-key cryptosystems (e.g., El Gamal and Orlitsky [EGO84]), zero-knowledge proofs (e.g., Feynman [Fey86]), and the role of relativization in complexity theory (Formanek [For94]).

2. **Complexity Theory**: There are numerous references to fundamental concepts in complexity theory, including P vs NP (Fortnow [For13], Garey and Johnson [GJ79]), hardness results for various problems (e.g., Impagliazzo et al. [IKW02], Khot [Kho02]), and average-case complexity (Impagliazzo [Imp95b]).

3. **Quantum Computing**: Several references discuss quantum computing, such as Aaronson and Arkhipov's paper on the power of quantum supremacy (Aaronson & Arkhipov [AA13]), and various papers about quantum algorithms for specific problems (e.g., Aharonov et al. [AHK+19], Kitaev [Kit03]).

4. **Interactive Data Compression** (El Gamal & Orlitsky, 1997): This work introduces an interactive data compression protocol and analyzes its communication complexity.

5. **Expander Graphs**: There are multiple references to expander graphs (e.g., Hoory, Linial, Wigderson [HLW06]), which play a significant role in the theory of computation due to their applications in derandomization and complexity lower bounds.

6. **Learning Theory and Statistical Learning**: Papers like Jerrum et al. (1989) on approximating the permanent and Jerrum, Sinclair, & Vigoda (2004) on polynomial-time approximation algorithms for permanents showcase learning theory's connection to complexity.

7. **Computational Geometry**: References like Hass & Pippenger (1999) discuss computational geometry problems and their complexity in terms of graph bisection, as well as the Markov Chain Monte Carlo method by Jerrum (1996).

8. **Communication Complexity**: The bibliography includes papers on communication complexity theory, such as Kleinberg & Papadimitriou (2004) and references to earlier works like Karp & Lipton [KL82] and Krajíček (1994).

9. **Proof Complexity**: Works by Jukna (2012), Krajíček's "Bounded Arithmetic, Propositional Logic, and Complexity Theory" (1995), and his later book on proof complexity [Kra19] are significant in this area.

10. **Approximation Algorithms**: Papers on approximation algorithms for problems like MAX CUT by Khot et al. (2007) demonstrate the ongoing interest in finding efficient solutions to NP-hard problems.

The bibliography reflects a broad range of topics within theoretical computer science, demonstrating the interconnectedness of various subfields and their common interest in understanding computational complexity and developing algorithms for solving challenging problems efficiently.


The text provided is a bibliography listing various research papers and books related to the field of computer science, mathematics, and theoretical computer science. Here's a summary of some key topics and notable works mentioned:

1. Learning Theory:
   - Kearns & Valiant (1994a): "Cryptographic limitations on learning Boolean formulae and finite automata" discusses the inherent limitations of learning algorithms when dealing with cryptographic primitives.
   - Kearns & Valiant (1994b): "An introduction to computational learning theory" provides an overview of the field, covering topics like PAC learning, VC dimension, and Occam's Razor.

2. Complexity Theory:
   - Kalai & Vempala (2003): "Efficient algorithms for universal portfolios" deals with the design of efficient trading strategies using computational methods.
   - Khot, Vishnoi (2005): "The unique games conjecture, integrality gap for cut problems and embeddability of negative-type metrics into ℓ1" proposes a conjecture related to the hardness of approximating certain optimization problems.

3. Graph Theory & Combinatorics:
   - Karchmer & Wigderson (1990): "Monotone circuits for connectivity require super-logarithmic depth" establishes lower bounds on the depth required by monotone circuits to compute connectivity functions in graphs.
   - Lackenby (2015, 2016): "A polynomial upper bound on Reidemeister moves" and "The efficient certification of knottedness and Thurston norm" focus on the computational complexity of problems related to low-dimensional topology.

4. Algorithmic Game Theory & Mechanism Design:
   - Nisan, Ron (1996): "Algorithmic mechanism design" explores how algorithms can be designed to implement desired outcomes in economic situations.

5. Cryptography:
   - Lasserre (2001, 2009): "Global optimization with polynomials and the problem of moments," and "Moments, positive polynomials and their applications" introduce techniques for solving optimization problems using semidefinite programming, which have applications in cryptography.

6. Quantum Computing:
   - Shor (1994a, 1997): "Algorithms for quantum computation: discrete logarithms and factoring," and "Scheme for reducing decoherence in quantum computer memory" present fundamental algorithms and techniques in quantum computing.
   - Simons (2010): "Selected applications of LLL in number theory" discusses the application of the Lenstra–Lenstra–Lovász (LLL) lattice reduction algorithm to computational number theory and cryptography.

7. Machine Learning:
   - Minkowski (1910): "Geometrie der Zahlen" provides a foundational work on geometric algebra, which underlies many machine learning algorithms.
   - Landsberg (2012): "Tensors: geometry and applications" explores the role of tensors in various mathematical and computational contexts, including machine learning.

8. Distributed Computing & Algorithms:
   - Linial (1992): "Locality in distributed graph algorithms" investigates how local information can be used to develop efficient distributed graph algorithms.
   - Lovász (2012): "Large networks and graph limits, volume 60" explores the asymptotic behavior of large graphs using techniques from measure theory.

This bibliography represents a small sample of the rich literature in theoretical computer science, mathematics, and related fields. It showcases various research areas, methodologies, and applications within these domains.


Title: "Arithmetic Circuits: A Survey of Recent Results and Open Questions" by Shpilka and Yehudayoff (2010)

In this survey paper, Aleksandr Shpilka and Alexander Yehudayoﬀ present a comprehensive overview of recent advancements in the study of arithmetic circuits, along with highlighting some open problems in the field. Arithmetic circuits are mathematical models used to represent polynomial functions, which play a significant role in computational complexity theory.

1. **Arithmetic Circuit Model**: An arithmetic circuit is defined as a directed acyclic graph where each gate computes an arithmetic operation (+, ×) and the input variables correspond to leaves of the circuit. The depth of such a circuit refers to the longest path from an input variable to the output gate.

2. **Complexity Measures**: The authors discuss several complexity measures for arithmetic circuits, such as size (number of gates), depth, width (maximum number of gates on any level), and rank (dimension of the subspace spanned by the circuit).

3. **Lower Bounds**: A primary focus of the paper is lower bounds on the complexity of arithmetic circuits. The authors detail several notable results, including:
   - Valiant's formula size lower bound for computing the determinant [Val79a].
   - Strassen's matrix multiplication algorithm and its implications on depth-3 circuit size [Str73a].
   - Szegedy's result showing a superpolynomial separation between depth-3 and depth-4 circuits for computing the permanent polynomial [Sze12].

4. **Upper Bounds**: The authors also discuss techniques for constructing small arithmetic circuits, such as:
   - Fast parallel computation of polynomials using few processors by Valiant et al. [VSBR83].
   - Depth-3 arithmetic circuits over fields of characteristic zero by Shpilka and Wigderson [SW01].

5. **Open Problems**: The paper concludes with a list of open problems in the field, which include:
   - Determining tight lower bounds for depth-4 circuits computing permanent.
   - Closing the gap between lower and upper bounds on small-depth circuits computing various polynomials (e.g., determinant, permanent).

6. **Relation to Other Areas**: The authors highlight connections between arithmetic circuits and other areas of computational complexity theory, such as communication complexity, circuit complexity, and proof complexity.

In summary, this paper by Shpilka and Yehudayoﬀ provides a detailed overview of the arithmetic circuit model, presenting recent advances in lower bounds, upper bounds, and open problems. It establishes the importance of studying arithmetic circuits as a tool for understanding fundamental questions in computational complexity theory.


### TheoryOfComputation

The text introduces an undergraduate course on the Theory of Computation, focusing on three main areas: Complexity Theory, Computability Theory, and Automata Theory. The purpose of this course is to understand the mathematical properties of computer hardware and software, define computations and algorithms rigorously, and explore the limitations of computers.

1. **Complexity Theory** deals with classifying problems based on their difficulty or computational hardness. It aims to provide a rigorous proof that problems perceived as difficult are indeed hard to solve.

2. **Computability Theory**, arising from the works of Gödel, Turing, and Church in the 1930s, investigates which mathematical problems can be solved by computers and provides formal definitions for computer, algorithm, and computation. The development of these theoretical models eventually led to real-world computers.

3. **Automata Theory** focuses on defining and analyzing different computational models such as finite automata, context-free grammars, and Turing machines. It aims to determine whether these models have equal power or if one model can solve more problems than another.

The course covers Automata Theory first, followed by Computability Theory. Complexity Theory is covered in a separate course (COMP 3804). This theory is essential for computer science as it deals with the fundamental capabilities and limitations of computers, influencing various fields like programming languages, compilers, artificial intelligence, and computer security.

The mathematical preliminaries required for this course include understanding sets, integers, rational numbers, real numbers, subsets, unions, intersections, Cartesian products, complements, binary relations, functions, equivalence relations, graphs, alphabets, strings, languages, Boolean operations, and proof techniques.

Proof techniques discussed in the text are:
- Direct proofs: These involve approaching the theorem directly to prove its truth.
- Constructive proofs: They not only show existence but also give a method of creating the object in question.
- Nonconstructive proofs: These demonstrate the existence of an object without actually constructing it.
- Proofs by contradiction: By assuming the negation of the statement and deriving a logical contradiction, we can prove that the original statement must be true.
- Pigeonhole principle: If n + 1 or more objects are placed into n boxes, at least one box will contain two or more objects. This principle has surprising consequences, such as proving the existence of subsequences in a sequence of numbers.
- Proofs by induction: A method to prove statements for all positive integers by first establishing their truth for the base case (usually n = 1) and then showing that if they hold for an arbitrary integer k ≥ 1, they must also be true for k + 1.


This chapter discusses finite automata and regular languages, focusing on deterministic (DFA) and nondeterministic finite automata (NFA).

1. Deterministic Finite Automata (DFA):
   - A 5-tuple M = (Q, Σ, δ, q0, F), where Q is a finite set of states, Σ is an alphabet, δ: Q × Σ → Q is the transition function, q0 is the start state, and F ⊆ Q is the set of accept states.
   - DFA processes input strings one symbol at a time, moving from state to state based on the transition function.
   - Example given: A toll gate controller that accepts sequences of 5, 10, and 25 cent coins until reaching 25 cents or more.

2. Nondeterministic Finite Automata (NFA):
   - Similar structure to DFA but allows multiple next states for a given state-symbol pair (including zero or more states).
   - Transition function δ: Q × Σϵ → P(Q), where Σϵ = Σ ∪ {ϵ}.
   - NFA accepts a string if there exists at least one path that satisfies specific conditions, without hanging before reading the entire input.

3. Regular Operations:
   - Union (A ∪ B): Set of all strings in A or B.
   - Concatenation (AB): Set of strings formed by concatenating w ∈ A and w' ∈ B.
   - Star (A∗): Set of all finite concatenations of strings from A, including the empty string ϵ.

4. Closure Properties:
   - The set of regular languages is closed under union. Proven using DFAs to construct a new DFA that recognizes the union language.
   - Regular languages are also closed under concatenation and star operations, which will be proven later in the text by introducing more general finite automata, such as NFAs.

5. Equivalence of DFA and NFA:
   - Theorem 2.5.1 demonstrates that every NFA can be converted into an equivalent DFA (L(M) = L(N)). This implies DFAs and NFAs have the same computational power.


The text discusses the equivalence between Deterministic Finite Automata (DFAs) and Nondeterministic Finite Automata (NFAs), as well as the closure properties of regular languages under various operations.

### Equivalence of DFAs and NFAs

1. **Construction from NFA to DFA:**
   - For each state `r` in the NFA, the ϵ-closure `C_ϵ(r)` is defined as the set of all states reachable by zero or more ε-transitions.
   - The start state `q'` of the DFA is defined as the ϵ-closure of the initial state `{q}` of the NFA: `q' = C_ϵ({q})`.
   - For any set of NFA states `R` in the DFA, the transition function `δ'(R, a)` is defined as the union of ϵ-closures of transitions from each state `r ∈ R`: `δ'(R, a) = [r∈R C_ϵ(δ(r, a))]`.
   - The set of accepting states in the DFA, `F'`, consists of all sets `R` that intersect with the original NFA's accepting states `F`: `F' = {R ∈ Q' : R ∩ F ≠ ∅}`.

2. **Theorem 2.5.2:**
   - This theorem states that a language is regular if and only if there exists an NFA that accepts it, formalizing the construction process from NFAs to DFAs.

### Closure Under Regular Operations

1. **Union (Theorem 2.6.1):**
   - If `A1` and `A2` are regular languages over the same alphabet Σ, then their union `A1 ∪ A2` is also a regular language. This is proven by constructing an NFA that simulates computations of both NFAs.

2. **Concatenation (Theorem 2.6.2):**
   - If `A1` and `A2` are regular languages over the same alphabet Σ, then their concatenation `A1 * A2` is also a regular language. This is shown by constructing an NFA that simulates reading a string from `A1`, followed by a string from `A2`.

3. **Star (Theorem 2.6.3):**
   - If `A` is a regular language, then its Kleene star `A*` is also regular. This is demonstrated through an NFA that can simulate zero or more repetitions of strings in the original language.

### Regular Expressions and Languages

1. **Definition of Regular Expressions (Deﬁnition 2.7.1):**
   - The rules for constructing regular expressions include: ϵ, ∅, each symbol `a` from the alphabet Σ, union (`∪`), concatenation (`*`), and Kleene star (`∗`).

2. **Language Description by Regular Expressions (Deﬁnition 2.7.2):**
   - The language described by a regular expression is defined recursively based on the structure of the expression.

3. **Algebraic Identities for Regular Expressions (Theorem 2.7.4):**
   - These identities allow manipulation of regular expressions, aiding in proving equalities between languages.

4. **Equivalence Theorem (Theorem 2.8.1):**
   - This theorem states that a language is regular if and only if it can be described by a regular expression. Proven by showing every regular expression describes a regular language and vice versa through converting DFAs to expressions.

5. **Pumping Lemma (Section 2.9):**
   - A tool for proving languages are not regular by demonstrating the lack of certain structural properties inherent to all regular languages, such as memory constraints and repetitive structure in long strings.

The detailed explanations above provide a comprehensive understanding of how DFAs and NFAs relate, how regular languages behave under various operations, and how they can be represented using regular expressions, culminating in the Pumping Lemma for identifying non-regular languages.


The provided text discusses Context-Free Grammars (CFGs) and their role in defining context-free languages, which are a class of formal languages that have recursive structures. Here's a detailed summary and explanation:

1. **Context-Free Grammars (CFGs):**
   - A CFG is a 4-tuple G = (V, Σ, R, S), where:
     - V is a finite set of variables or non-terminal symbols.
     - Σ is a finite set of terminal symbols.
     - V ∩Σ = ∅ (no overlap between variables and terminals).
     - S is the start variable from V.
     - R is a finite set of rules, each having the form A → w, where A ∈V and w ∈(V ∪Σ)*.

2. **Derivation in CFGs:**
   - A string uwv can be derived from uAv by applying rule A →w to the string uAv.
   - A string v can be derived from u if one of the following holds:
     1. u = v, or
     2. there exist an integer k ≥2 and strings u₁, ..., uₖ such that u = u₁ ⇒u₂ ⇒...⇒uₖ = v.

3. **Language of a CFG:**
   - The language L(G) of a CFG G is the set of all strings in Σ∗that can be derived from the start variable S (S*⇒w).
   - A language L is context-free if there exists a CFG G such that L(G) = L.

4. **Examples:**

   a. **Properly Nested Parentheses:**
      - V = {S}, Σ = {a, b}, R = {S →ϵ, S →aSb, S →SS}
      - Derives strings like ((())) and derives the language of properly nested parentheses.

   b. **Nonregular Language 0^n1^n:**
      - G₁ = (V₁, Σ, R₁, S₁), where V₁ = {S₁}, Σ = {0, 1}, R₁ = {S₁ →ϵ, S₁ →0S₁1}
      - Derives strings like 0^n1^n for n ≥0.

   c. **Complement of Nonregular Language:**
      - L = {0^n1^n : n ≥0}, L' = L complement.
      - G = (V, Σ, R, S), where V = {S, S₁, S₂}, Σ = {0, 1}, and R consists of rules:
        S → S₁ | S₂
         S₁ → ϵ | 0S₁1
         S₂ → 0 | 0S₂ | 0S₂1
      - Derives L' by handling strings of type 1., 2., and those containing '10' as substrings.

These examples demonstrate that context-free grammars can generate a wide range of languages, including some nonregular ones like 0^n1^n and its complement. Understanding CFGs is crucial for defining programming language syntax and compiling them into executable code.


The text discusses Context-Free Languages (CFL) and Pushdown Automata (PDA), focusing on their equivalence. Here's a detailed summary:

1. **Context-Free Grammars (CFG)**: A CFG is a set of production rules that generate all possible strings in a given language. The grammar G = (V, Σ, R, S) consists of a set of variables V, an alphabet Σ, a set of rules R, and a start variable S.

2. **Chomsky Normal Form (CNF)**: A CFG is said to be in CNF if all its production rules have one of three forms:
   - A → BC (where A, B, C are variables; B ≠ $, C ≠ $)
   - A → a (where A is a variable and a is a terminal)
   - $ → ϵ

3. **Pushdown Automata (PDA)**: PDA is a computational model that accepts languages described by CFGs. It consists of:
   - Tape: A sequence of cells, each containing a symbol from an alphabet Σ or a blank symbol 2.
   - Stack: A last-in, first-out structure with symbols from an alphabet Γ, including a special symbol $.
   - State Control: A finite set of states Q, one of which is the start state q.

4. **Deterministic PDA (DPDA)**: In DPDA, for each configuration (state, tape symbol, stack top), there's exactly one computation step possible. The transition function δ : Q × (Σ ∪ {2}) × Γ → Q × {N, R} × Γ* determines the next state, direction of tape head movement, and stack action (pop, push).

5. **Nondeterministic PDA (NPDA)**: NPDA allows for multiple possible computation steps from a given configuration. The transition function δ : Q × (Σ ∪ {2}) × Γ → Pf(Q × {N, R} × Γ*) returns a set of possible next configurations.

6. **Equivalence of CFG and NPDA**: The main theorem states that for any language A over an alphabet Σ, A is context-free (i.e., generated by some CFG) if and only if there exists an NPDA that accepts A. This equivalence demonstrates that CFGs and NPDA are equally powerful in describing formal languages.

7. **Construction of NPDA from CFG**: To prove the theorem, one direction involves constructing an NPDA from a given CFG (in CNF). The construction parallels the CFG's derivation process:
   - Each variable in the CFG corresponds to a state in the PDA.
   - The start variable $ corresponds to the start state of the PDA.
   - For each rule A → BC, add transitions based on reading B and pushing C onto the stack (and moving the tape head as necessary).
   - For rules A → a, add transitions that read a terminal symbol and push the corresponding variable onto the stack.
   - For the rule $ → ϵ, ensure the PDA accepts when the stack is empty and the tape head is at the correct position.

This construction shows how CFGs and NPDA can describe the same set of languages, highlighting their equivalence in computational power.


Title: Summary of Chapter 3 - Context-Free Languages and the Pumping Lemma

1. **Context-Free Grammars (CFG):** A CFG is a set of production rules that generate context-free languages. It consists of a vocabulary V, an alphabet Σ, a set of production rules R, and a start symbol S. The power of a CFG is determined by its ability to handle nested structures, thanks to the Chomsky Normal Form (CNF), which restricts rules to be either A → BC or A → a.

2. **Pushdown Automata (PDA):** PDAs are automata that use an auxiliary storage stack, allowing them to recognize context-free languages. They have states Q, input alphabet Σ, stack symbols Γ, transition function δ, start state q0, and accepting states F. The instructions for constructing a PDA from a CFG are provided in the chapter.

3. **Equivalence of PDAs and CFGs:** It's proven that every context-free grammar can be converted into an equivalent nondeterministic pushdown automaton (NPDA), and vice versa, although only the conversion from CFG to NPDA is given. This establishes the equivalence between these two models.

4. **Pumping Lemma for Context-Free Languages:** The pumping lemma is a crucial tool for proving that certain languages are not context-free. It states that any sufficiently long string in a context-free language can be "pumped" (repeated) according to specific rules, yielding another valid string in the language.

   - **Theorem 3.7.2:** This claim explains how to transform a CFG-generated string into an NPDA configuration and vice versa, establishing the connection between them.
   
   - **Proof of Equivalence:** Using Claim 3.7.2, it's shown that a string belongs to L(G) (the language generated by G) if and only if the NPDA accepts this string, proving that CFGs and NPDAs recognize the same class of languages.

5. **Applications of the Pumping Lemma:** The pumping lemma is used to prove that certain languages are not context-free by demonstrating contradictions when attempting to pump strings according to its rules. Three examples are provided:

   - Language A = {anbncn : n ≥0} (not context-free)
   - Language B = {wwR : w ∈{a, b}∗} (context-free)
   - Language C = {ambncmn : m ≥0, n ≥0} (not context-free)

6. **Exercises:** The chapter concludes with several exercises that ask students to apply the concepts learned in constructing CFGs for various languages, proving properties of languages using automata and lemmas, and designing PDAs for given languages. These exercises help reinforce understanding of context-free languages and their recognition mechanisms.


Title: Summary and Explanation of Chapter 5: Decidable and Undecidable Languages

This chapter explores the limitations of Turing machines, specifically focusing on decidable and undecidable languages. Here's a detailed summary and explanation:

1. **Decidability**: A language A is decidable if there exists a Turing machine M that correctly determines whether any input string w belongs to A or not. This means that for every string w, the computation of M on w either terminates in an accept state (if w ∈ A) or in a reject state (if w ∉ A).

   - **ADFA**: The language consisting of pairs ⟨M, w⟩ where M is a deterministic finite automaton accepting w. This language is decidable because we can construct an algorithm that simulates the computation of M on w and checks if it accepts w.
   
   - **ANFA**: Similar to ADFA but for non-deterministic finite automata. It's also decidable, achieved by converting the non-deterministic automaton into a deterministic one using an algorithmic construction.
   
   - **ACFG**: The language consisting of pairs ⟨G, w⟩ where G is a context-free grammar and w ∈ L(G). This language is decidable as we can convert the CFG to Chomsky normal form and then check if w can be derived from the start symbol using 2n - 1 steps.

2. **Undecidability**: A language that is not decidable is called undecidable. An example of an undecidable language is ATM, which consists of pairs ⟨M, w⟩ where M is a Turing machine accepting w. The proof of its undecidability uses contradiction and involves constructing another Turing machine D that behaves oppositely to the assumed decider for ATM.

3. **Countable Sets**: The concept of countability is introduced, which states that a set A is countable if it's finite or there exists a bijection between A and N (the natural numbers). The set R of real numbers is shown not to be countable using Cantor's diagonal argument. 

4. **The Halting Problem**: The halting problem (Halt) is proven undecidable by showing that assuming it were decidable leads to a contradiction, similar to the proof for the uncountability of real numbers. This is done by constructing a Java program D that performs the opposite action of an assumed decider H for the halting problem.

5. **Rice's Theorem**: This theorem states that any non-trivial semantic property (a property depending on the language accepted) of Turing machines forms an undecidable language. In other words, if P is a subset of all Turing machines that satisfies certain conditions, then P is undecidable. Examples include properties like whether a machine accepts the empty string or has a finite language.

6. **Enumerability**: A language A is enumerable if there exists a Turing machine M such that, for every string w, if w ∈ A, then M accepts w in a finite number of steps, and if w ∉ A, then M does not accept w (either rejects or doesn't terminate). Every decidable language is also enumerable. The Hilbert's problem, which asks whether a polynomial with integer coefficients has an integral root, is shown to be enumerable but undecidable.

This chapter concludes that while Turing machines can simulate any algorithm we intuitively consider computable (Church-Turing thesis), there are fundamental limitations to what can be computed—most problems are undecidable or merely enumerable rather than decidable.


The Complexity Class NP (Nondeterministic Polynomial Time) is a set of decision problems for which a 'yes' answer can be verified quickly, even if finding the answer may take longer. This class is defined using nondeterministic Turing machines, which are theoretical machines that can explore multiple computational paths simultaneously.

1. **Definition**: A language L is in NP if there exists a polynomial-time verifier V and a polynomial p(n) such that for every string x:

   - If x ∈ L (i.e., x is a 'yes' instance), then there exists a certificate c (a witness) of length at most p(|x|) such that V(x,c) = 1.
   - If x ∉ L (i.e., x is a 'no' instance), then for every certificate c of length at most p(|x|), V(x,c) = 0.

In simpler terms, an NP problem has the property that if we're given a proposed solution (certificate), we can quickly check whether it's correct or not. However, finding such a solution in the first place might be difficult and could potentially require exponential time.

2. **Examples**:

   - **3-Coloring Problem**: Given a graph G, is there a valid 3-coloring of its vertices? Here, the certificate c is a color assignment for each vertex. The verifier V can quickly check if two adjacent vertices have different colors.
   
   - **Subset Sum Problem**: Given a set S of integers and a target sum B, does S contain a subset whose elements sum up to B? The certificate c is a subset of S. The verifier V checks the sum of the elements in c against B.

3. **Relationship with P**: It's conjectured that NP ≠ P, meaning there are problems in NP that cannot be solved in polynomial time by a deterministic Turing machine (i.e., they're not in P). However, proving this separation is one of the biggest open problems in computer science—the famous P vs. NP problem.

4. **NP-Completeness**: A problem is NP-complete if it's in NP and every problem in NP can be reduced to it in polynomial time. In other words, an NP-complete problem is at least as hard as the hardest problems in NP. Examples of NP-complete problems include the Traveling Salesman Problem, Vertex Cover, and 3SAT (3-Satisfiability).

5. **Practical Implications**: Many real-world optimization and search problems are NP-hard or NP-complete, meaning they're difficult to solve exactly but can be approximated or solved in specific cases using heuristics or specialized algorithms. Understanding the complexity class NP helps guide our expectations for what's computationally feasible and informs the development of efficient algorithms.


The text discusses the concept of NP-completeness, which is a class of problems in computational complexity theory that are considered the hardest in NP (Non-deterministic Polynomial time). The key idea is that if a problem in NP can be reduced to an NP-complete problem using polynomial-time reductions, then solving the NP-complete problem would imply a solution for all problems in NP.

Here's a detailed explanation:

1. **NP (Non-deterministic Polynomial time)**: This is a class of decision problems where, given a "certificate" or "witness", one can verify that the certificate proves the answer to be 'yes' in polynomial time. In other words, there exists a polynomial-time algorithm for checking the correctness of solutions (certificates), but finding such a solution might not be efficient.

2. **NP-completeness**: A problem is NP-complete if it belongs to NP and is at least as hard as every other problem in NP. This means that any problem in NP can be reduced to an NP-complete problem using polynomial-time reductions. If there exists a polynomial-time algorithm for solving an NP-complete problem, then P = NP (Polynomial time equals Non-deterministic Polynomial time), which is one of the most significant open questions in computer science.

3. **Reductions**: A reduction from problem A to problem B is a way to transform instances of problem A into instances of problem B such that the solutions to the transformed problem (B) can be used to solve the original problem (A). If there exists a polynomial-time reduction from A to B, and A ∈ NP, then B is also in NP.

4. **Cook-Levin Theorem**: This theorem states that the Boolean Satisfiability Problem (SAT), a well-known NP-complete problem, can be reduced in polynomial time to any other problem in NP. In other words, if there exists an efficient algorithm for solving SAT, then all problems in NP can be solved efficiently.

5. **Example of an NP-complete problem - Domino Tiling**: The text provides an example of an NP-complete problem called "Domino Tiling." Given a frame with certain constraints and tile types, the problem asks whether it's possible to completely fill the frame using the given tiles without violating the constraints. This problem is proven to be NP-complete by reducing another known NP-complete problem (3SAT) to Domino Tiling in polynomial time.

6. **Implications**: If an efficient algorithm for solving any NP-complete problem were found, it would imply P = NP, as we could solve all problems in NP using that algorithm. This would have profound implications for computer science and cryptography, as many security protocols rely on the assumption that certain problems are hard to solve efficiently (i.e., P ≠ NP).

In summary, understanding NP-completeness is crucial because it helps classify problems according to their inherent difficulty and provides a framework for comparing the relative complexity of various computational tasks. The existence of NP-complete problems also highlights the gap between what we can verify efficiently (NP) and what we can find efficiently (P), which remains an open question in computer science.


The text provided discusses several concepts related to computational complexity theory, specifically focusing on NP-complete problems. Here's a detailed explanation:

1. **Turing Machines and Domino Puzzles**: The text first explains how Turing machines can be used to model domino puzzles. Each configuration of the puzzle corresponds to a state of the Turing machine, and the movement of the tape head mirrors the placement of dominoes. This establishes an equivalence between solutions to domino puzzles and computations of certain Turing machines.

2. **NP-completeness**: NP-complete problems are a class of computational problems that are both in NP (can be solved in non-deterministic polynomial time) and can reduce any other NP problem to them, meaning they are at least as hard as the hardest problems in NP.

3. **SAT Problem**: The Satisfiability (SAT) problem is about determining if a Boolean formula can be satisfied by some assignment of truth values to its variables. The text shows that SAT is NP-complete by reducing the Domino problem to SAT. It does this by encoding each domino configuration as a Boolean formula, where variables represent tile types at different positions, and clauses enforce the rules of the puzzle (e.g., no two tiles of the same type can occupy the same position).

4. **3-SAT Problem**: 3-SAT is a special case of SAT where each clause has exactly three literals. The text proves that 3-SAT is NP-complete by reducing SAT to 3-SAT: if a given Boolean formula in any form (not necessarily 3-CNF) can be satisfied, it can also be converted into an equivalent 3-CNF formula that can also be satisfied.

5. **Clique Problem**: The Clique problem involves finding the largest complete subgraph (a subset of vertices where every two are connected by edges) in a given graph. The text proves that Clique is NP-complete using reductions from 3-SAT, demonstrating how any instance of 3-SAT can be translated into an equivalent Clique problem and vice versa.

6. **Other NP-Complete Problems**: The text lists several other problems known to be NP-complete, such as the Traveling Salesman Problem, Bin Packing, Time Tables, Motion Planning, Map Labeling, etc. These problems vary widely in their nature (graph theory, optimization, scheduling), but all share the property that a solution can be verified quickly (in polynomial time) but finding such a solution might require exploring an exponentially large search space.

The exercises at the end of the section ask for proofs related to complexity classes (like P and NP), reductions between problems, and properties of specific languages (like SAT, 3-SAT, Clique, etc.). They aim to deepen understanding of these concepts through practice.


The given text provides a summary of several key concepts in computational theory, specifically focusing on different classes of formal languages and their properties. Let's break down the details:

1. **Regular Languages**: These are the simplest class of formal languages. A language is regular if there exists a deterministic finite automaton (DFA), nondeterministic finite automaton (NFA), or regular expression that describes it. Regular languages are closed under union, concatenation, Kleene star, and subset operations. The Pumping Lemma for Regular Languages can be used to prove that certain languages aren't regular.

2. **Context-Free Languages**: These are more complex than regular languages. A language is context-free if it can be generated by a context-free grammar or accepted by a nondeterministic pushdown automaton (NPDA). Every regular language is context-free, but not every context-free language is regular (e.g., {anbn : n ≥ 0}). Context-free languages are closed under union and concatenation, but not necessarily for intersection and complement operations. The Pumping Lemma for Context-Free Languages can be used to prove that certain languages aren't context-free.

3. **Decidable and Enumerable Languages**: These classes of languages are defined based on "reasonable" computational devices like Turing machines or Java programs. Every context-free language is decidable, but not all decidable languages are context-free (e.g., {anbncn : n ≥ 0}). All enumerable languages are also decidable, but not vice versa. The Halting Problem is an example of a language that's enumerable but not decidable. Moreover, there exist languages that are decidable but not enumerable.

4. **Complexity Classes**: These classes categorize problems based on computational complexity. 

   - P: This class includes all decision problems that can be solved by a deterministic Turing machine in polynomial time.
   
   - NP: This class includes all decision problems for which a solution can be verified by a deterministic Turing machine in polynomial time, given a certificate (or "proof") that is polynomial-length. The key point here is that while we don't know if there are problems in NP that aren't in P, it's widely believed that such problems exist.
   
   - NP-Complete: These are the hardest problems in NP. A problem is NP-complete if it's in NP and every problem in NP can be reduced to it in polynomial time. In other words, an NP-complete problem is at least as hard as any other problem in NP.

5. **Church-Turing Thesis**: This is a hypothesis that states any function computable by an algorithm can also be computed by a Turing machine. Essentially, all reasonable models of computation are equivalent to Turing machines.

The summary concludes with a diagram illustrating the relationships among these language classes and complexity classes. Regular languages are a subset of context-free languages, which are a subset of decidable languages, which in turn are a subset of enumerable languages. All these subsets are contained within the class of all languages. NP is a subset of P, but it's unknown if they're equal (i.e., whether P = NP). Furthermore, NP-complete problems sit atop the NP class in terms of complexity.


### book

The chapter "Congruences" introduces the concept of modular arithmetic, often referred to as 'clock arithmetic'. It lays the foundation for understanding groups by exploring integers modulo n. If n is a prime number, these integers form a field. This chapter covers the following topics:

1. Basic Properties (Section 1.1):
   - Definition of congruence modulo n: a ≡ b (mod n) if a − b is divisible by n.
   - Proof that congruence is an equivalence relation, meaning it's reflexive, symmetric, and transitive.

2. Divisibility Tests (Section 1.2):
   - Explanation of simple tests for divisibility by small numbers using congruences:
     - Test for divisibility by 2 or 5: a is even/divisible by 5 if its last digit a₀ is even/0 or 5, respectively.
     - Test for divisibility by 3 or 9: sum of digits of the number is divisible by 3 or 9.
     - Test for divisibility by 7: b − 2a₀ is divisible by 7, where a = 10b + a₀.

3. Common Divisors (Section 1.3):
   - Explanation of greatest common divisor (gcd) and least common multiple (lcm).
   - The Euclidean algorithm to compute gcd(a, b), showing it involves successive division with remainder until reaching zero, and expressing the gcd as a linear combination of a and b.

4. Solving Congruences (Section 1.4):
   - Theorem stating that if (a, n) = 1, then the congruence ax ≡ b (mod n) has a unique solution modulo n.
   - Example demonstrating how to solve a specific linear congruence using the Euclidean algorithm.

5. The Integers Modulo n (Section 1.5):
   - Discussion on the integers modulo n and their properties, setting the stage for understanding groups.

6. Introduction to Software (Section 1.6):
   - Brief mention of computational tools that can be used in group theory, preparing students for integrating technology into mathematical problem-solving.


This text discusses the concept of permutations and permutation groups, focusing on their properties, notation, and generation. Here's a detailed summary:

1. **Permutations**: A permutation is a bijective (one-to-one and onto) mapping of a set to itself. The number of permutations in S_n (the set of all permutations of n elements) is n! (n factorial). Permutations can be represented using mapping notation or cycle notation, which lists the images of each element under the permutation in a cyclic order.

2. **Cycles**: An r-cycle is a special type of permutation that maps the first number to the second, the second to the third, and so on, until the r-th number maps back to the first, while leaving all other elements fixed. Any permutation can be written as a product (composition) of disjoint cycles, and any cycle can be expressed as a product of transpositions (2-cycles).

3. **Sign of a Permutation**: The sign of a permutation is defined based on its determinant when represented as a matrix operation. A permutation is even if it's the product of an even number of transpositions and odd otherwise. Even permutations commute with each other, as do odd ones.

4. **Permutation Groups**: A permutation group G is a non-empty subset of Sn (the set of all permutations of n elements) that's closed under multiplication (composition) and inverses. The symmetric group Sn itself is a permutation group, known as the full or symmetric group. Other examples include alternating groups An (even permutations), cyclic groups generated by a single permutation, and groups generated by a set of permutations.

5. **Cyclic Groups**: Cyclic groups are special types of permutation groups generated by a single permutation α. The order of α (the smallest positive integer r such that α^r = identity) equals the number of elements in the group ⟨α⟩ generated by α.

6. **Generators**: A subset g of permutations generates a permutation group G if every element in G can be written as a product of elements from g. The smallest permutation group containing g is called the group generated by g. This group consists of all products of finitely many elements from g, not just finite products.

7. **Software and Calculations**: The 'Groups.m' package provides tools for calculations in permutation groups using Mathematica. Permutations are represented as lists of their images, with a specific header (M) for the package to interpret them correctly. Various operations and properties can be computed using this software.


The provided text discusses linear groups, which are sets of invertible matrices with specific algebraic properties. These groups are defined over a field F, and they must satisfy two conditions: (i) if α, β ∈ G, then αβ ∈ G; (ii) if α ∈ G, then α^-1 ∈ G. The general linear group GL(n, F) is an example of a linear group, where F can be any field and n represents the size of the matrices.

The special linear group SL(n, F) is another important linear group, consisting of all n × n matrices with determinant 1, also defined over field F. The order of a linear group G (denoted as |G|) is the number of elements in G if it's finite; otherwise, it's denoted as ∞.

A key aspect of linear groups is the concept of generators, which are similar to permutations. For an element α in GL(n, F), its order |α| is defined as the smallest integer n such that α^n = I (the identity matrix). If no such n exists, we say that α has infinite order.

A cyclic linear group generated by α is {α^b | b ∈ Z}, which means it consists of all powers of α, including negative ones if its order is infinite. A linear group G is said to be generated by a subset g ⊂ G if every element in G can be expressed as a product of elements from g and their inverses.

The text provides several examples of linear groups over different fields:
1. T = {(1 b 0 1) | b ∈ F}, where there's a one-to-one correspondence between F and T, under matrix multiplication.
2. N(p) = {(a b 0 d) | a, b, d ∈ F_p, ad ≠ 0}, upper triangular matrices in GL(2, F_p).
3. G(p) = {(a b br a) | a, b ∈ F_p, a^2 - b^2r ≠ 0}, where r is not a square in F_p.
4. GL(2, F_p), the general linear group over finite field F_p, with order (p-1)^2*p*(p+1).

The package 'Groups.m' allows for calculations involving linear groups over a finite field F_p. It uses a chosen prime p, and matrices are represented differently to account for modulo arithmetic. Functions similar to those used in permutation groups (like Inverse, .) apply here as well. The package provides a mechanism for reducing calculations modulo the chosen prime at each step.

The text concludes by introducing an algorithm for expressing a matrix in GL(2, F) using a set of generators. It uses row and column operations to transform a given matrix into a product of these generator matrices. This process is applicable over any field F, but the specific number of required generators might vary depending on whether F is finite or infinite.


The text discusses several concepts related to group theory, a fundamental area of abstract algebra. Here's a detailed summary and explanation of the key points:

1. **Group**: A set G with a binary operation (multiplication) that is associative, has an identity element, and every element has an inverse. Examples include permutation groups, linear groups, integers under addition, non-zero elements in a field under multiplication, and more.

2. **Homomorphism**: A mapping between two groups preserving the group operation. For example, the mapping from permutations to their corresponding permutation matrices is a homomorphism, as is the determinant map from GL(n, F) to F^x (the multiplicative group of non-zero elements in field F).

3. **Isomorphism**: A bijective homomorphism; it's like an equality sign for groups. If there exists an isomorphism between two groups G and H, we say they are isomorphic, written G ≅ H. For instance, the exponential mapping exp: R → S (where S is the positive real numbers under multiplication) is an isomorphism.

4. **Direct Product**: Given two groups G and H, their direct product G × H is a group with operation defined component-wise. The projections onto each factor are homomorphisms. An example is R^2 with vector addition.

5. **Subgroups**: Non-empty subsets of a group that themselves form a group under the same operation. Conditions include closure under the operation and inverses being within the subset. Cyclic subgroups are generated by a single element, and any subgroup can be generated by a set of elements (called generators).

6. **Orthogonal Groups**: Subgroups of linear groups consisting of orthogonal matrices – those preserving the Euclidean distance. Examples include SO(n), the special orthogonal group (matrices with determinant 1), and O(n), the orthogonal group (including both oriented and unoriented rotations).

7. **Cyclic Subgroups and Generators**: If G is a group and α ∈G, then ⟨α⟩ = {α^n | n ∈Z} is the cyclic subgroup generated by α. A group is cyclic if it equals such a subgroup. Infinite cyclic groups are isomorphic to Z (integers), while finite ones of order n are isomorphic to Z/nZ.

8. **Subgroups Generated by Sets**: Given a subset g ⊂ G, the subgroup ⟨g⟩ consists of all elements expressible as products of elements from g and their inverses, following specific rules. For GL(n, Z), SL(n, Z) is a significant example: matrices with determinant 1.

The text also includes several exercises to deepen understanding of these concepts, such as proving properties of cyclic groups, demonstrating that certain mappings are homomorphisms, and exploring the structure of specific groups like braid groups and orthogonal groups.


The text discusses several topics related to group theory, symmetry groups, and group actions. Here's a detailed summary of each section:

1. **Symmetries of Regular Polygons**:
   - The symmetry group Sym(X) of an object X in Euclidean n-space is the set of all isometries that map X to itself.
   - For regular polygons, symmetries include rotations and reflections.
   - For P3 (equilateral triangle), there are 6 symmetries: 3 rotations (2π/3, 4π/3, 2π) and 3 reflections. This group is isomorphic to S3.
   - For P4 (square), there are 8 rotations (π/2, π, 3π/2, 2π) and 4 reflections, totaling 12 symmetries. The group D4, isomorphic to Sym(P4).

2. **Symmetries of Platonic Solids**:
   - Proper symmetries (isometries in SO(3)) are considered for Platonic solids.
   - For the tetrahedron, there are 12 proper symmetries, forming the alternating group A4.
   - The cube and its dual, the octahedron, have 24 proper symmetries, forming the group O (or SO(3) × {±1}).
   - The dodecahedron and its dual, the icosahedron, have 60 proper symmetries, forming the group A5.

3. **Improper Symmetries**:
   - Improper symmetries are isometries with det = -1 (e.g., reflections).
   - For the cube, there are 9 reflections and 15 rotatory reflections, totaling 24 improper symmetries.

4. **Symmetries of Equations**:
   - The symmetry group of a polynomial equation is its Galois group, reflecting symmetries among roots.
   - Examples are given for specific equations (x^4 + x^3 + x^2 + x + 1 and x^4 - 2x^2 - 2 = 0) to illustrate how the symmetry group is determined by preserving algebraic relations among roots.

5. **Group Actions**:
   - Definition: A group G acts on a set X if there's a mapping G × X → X such that (αβ)·x = α·(β·x) and 1·x = x for all α, β ∈G and x ∈X.
   - Examples: G acting on itself by multiplication or conjugation are provided.

6. **Orbits and Stabilizers**:
   - The orbit of a point x ∈ X is the set {α·x | α ∈ G}.
   - The stabilizer of x is the subgroup {α ∈G | α·x = x}.
   - Example: For G = V' (a subgroup of S4) acting on X = {1, 2, 3, 4}, orbits and stabilizers are computed.

7. **Conjugacy Classes**:
   - In a group action by conjugation, the orbit of an element ξ ∈ G is called its conjugacy class (Cξ).
   - The stabilizer of ξ, also known as the centralizer Zξ(G), consists of elements commuting with ξ.
   - Conjugacy classes in Sn are defined by cycle types; for example, possible cycle types in S5 are {1}, {2}, ..., {5}.

8. **Fractional Linear Transformations**:
   - GL(2, F) (2x2 invertible matrices over a field F) acts on the projective line P^1(F).
   - This action is defined by [a b; c d] · [x : y] = [ax + by : cx + dy].
   - This action will be further explored in chapter 10 and will play a significant role in chapter 12.

The text provides a foundation for understanding group actions, symmetry groups, and their applications to various mathematical objects like polygons, Platonic solids, and polynomial equations. It also introduces the concept of conjugacy classes and sets the stage for further exploration of fractional linear transformations in subsequent chapters.


Summary of Key Points and Explanation from Section 9.4 on Finite Subgroups of SO(3):

This section explores the classification of finite subgroups within the group of rotations in three dimensions, denoted as SO(3). The goal is to determine which types of groups can be finite subgroups of SO(3).

1. **Action on S2**: Any group G acting on S2 (the unit sphere) induces a set X of fixed points, where each point x ∈ X represents a pair of antipodal points that are invariant under the action of some non-identity element α ∈ G. 

2. **Orbit Decomposition**: The orbits O1, ..., Os of G's action on X divide S2 into regions with equivalent symmetry properties. Each orbit Oj consists of nj pairs of antipodal points, where |G|/nj = |Stab(x)| for any x ∈ Oj (a property from equation 9.1).

3. **Burnside's Formula Application**: Using Burnside's counting lemma, we establish the relationship between the number of orbits s and the sizes nj of these orbits:

   Sum_j=1^s (1/nj) = s - 2 + 2|G|

4. **Inequality Analysis**: By analyzing this equation, we deduce that s (the number of orbits) must be less than or equal to 3 due to the inequality derived from the terms' non-negativity and upper limit of 1/2:

   2 > s / 2 ⇒ s < 4

5. **Case Analysis**: Given s ≤ 3, we consider three cases:

   - (i) If s = 1, then there is only one orbit O1 containing all fixed points. This implies that G must act transitively and regularly on S2, meaning it must be either cyclic (G ≅ Cn), dihedral (Dn), the full group of rotations (T), or the identity group (O).

   - (ii) If s = 2, then there are two distinct orbits O1 and O2. The inequality suggests that n1 and n2 must both be equal to 2. In this scenario, G's structure is more complex, involving subgroups of SO(3) that contain rotations through different axes (like axis-reflection pairs).

   - (iii) If s = 3, it implies three orbits O1, O2, and O3 with nj ≥ 2. The specific group structures in this case are yet to be fully determined but may involve more complicated subgroup configurations within SO(3).

In conclusion, Section 9.4 demonstrates that finite subgroups of SO(3) can be classified into cyclic groups (Cn), dihedral groups (Dn), the full group of rotations (T), or the identity group (O). The method involves studying the fixed points under these group actions and applying Burnside's counting lemma to derive essential constraints on possible orbit structures. Further exploration is needed for a complete classification in the case where s = 3.


The Sylow Theorems are fundamental results in group theory that provide crucial information about the structure of finite groups, especially those with a large order. These theorems were developed by Ludwig Sylow in 1872 and consist of three main statements regarding Sylow p-subgroups (p being a prime divisor of the order of the group).

1. **Existence Theorem**: For every finite group G, for each prime p dividing |G|, there exists at least one subgroup H ⊂ G whose order is the highest power of p that divides |G|. Such a subgroup H is called a Sylow p-subgroup.

   Proof: Let n = |G| and write n = apr where (a, p) = 1. The number of subsets T with pr elements in G is given by (n choose pr), i.e., |X| = (n choose pr). G acts on X by (α, T) → αT. This action decomposes X into disjoint orbits. By Lemma 11.2, p does not divide |X|, so one of these orbits has order a, meaning it consists of right cosets of Sylow p-subgroups.

2. **Congruence Theorem**: If np denotes the number of Sylow p-subgroups in G, then np ≡ 1 (mod p).

   Proof: As mentioned earlier, each orbit of length a contains elements from right cosets of different Sylow p-subgroups. Since p does not divide |G/H| where H is any Sylow p-subgroup, there must be an orbit whose order isn't divisible by p. By applying the formula (9.2), we deduce that np ≡ 1 (mod p).

3. **Conjugacy Theorem**: All Sylow p-subgroups of G are conjugate to each other.

   Proof: Let H be a Sylow p-subgroup, and consider the set of left cosets G/H. The group K acts on this set by left multiplication, resulting in disjoint orbits. Since p does not divide |G/H|, there exists an orbit whose order is not divisible by p. For any α ∈ G, if αH belongs to such an orbit, then for every κ ∈ K, α−1κα ∈ H, implying that α−1Kα ⊂ H and ultimately α−1Kα = H. Hence, K is conjugate to H via the element α.

These Sylow Theorems have significant implications for understanding finite groups:

- They help in identifying important subgroups of a given group, which can simplify the task of determining its structure.
- In combination with other results (e.g., Lagrange's Theorem), they enable us to determine the order and structure of various subgroups within a larger group.
- Sylow p-subgroups play an essential role in classifying finite simple groups, which are fundamental building blocks in group theory.

The normalizer NG(H) (the largest subgroup of G containing H such that every element of NG(H) commutes with H) is also crucial in Sylow theory: The number of conjugates of a Sylow p-subgroup H is given by |G|/|NG(H)|, and this quantity divides the index a = |G|/|H|.


The text discusses several key concepts related to Abelian groups, specifically focusing on free abelian groups and their structure. Here's a summary and explanation of the main points:

1. Free Abelian Groups: A group G is finitely generated if there exists a finite subset g ⊂G such that G = ⟨g⟩. In an abelian group, a basis of G is a set of generators {α₁, ..., αₙ} with no non-trivial relations among them (i.e., a₁α₁ + ... + aᵢαᵢ = 0 implies a₁ = ... = aᵢ = 0). If such a basis exists, the group G is called free abelian, and its rank is the number of elements in the basis.

2. Torsion Subgroup: For any abelian group G, define Gₖ as {α ∈G | nα = 0 for some n ∈Z}. This subgroup Gₖ is called the torsion subgroup of G. Finite Abelian groups are not free because they have non-trivial elements that satisfy nα = 0 for a fixed n > 1.

3. Rank and Classification: Every finitely generated abelian group G has a decomposition into its free part (Gₖ) and torsion part (Gₖ). The rank of G is the number of elements in a basis of the free part Gₖ. This rank determines essential properties of the group, such as isomorphism classes.

4. Kernel and Homomorphisms: For an abelian group G with generators {α₁, ..., αₙ} and a homomorphism f : Zⁿ →G given by (a₁, ..., aₙ) ↦ aᵢαᵢ, the kernel of f is a subgroup of Zⁿ. By analyzing this kernel using row and column reduction of integer matrices, one can obtain a basis for the kernel in terms of the original generators {α₁, ..., αₙ}.

5. Row and Column Reduction: Given an n × m integer matrix A = (aij), row and column operations are applied to diagonalize A using only integral operations. This process involves multiplying rows/columns by -1, swapping rows/columns, and adding a multiple of one row/column to another. The resulting diagonalized matrix B reveals the rank and structure of the original subgroup H in G.

In essence, this text lays the foundation for understanding the structure of finite Abelian groups by introducing free Abelian groups, their bases, torsion subgroups, and the classification theorem that connects these concepts. Row and column reduction of integer matrices is presented as a tool to analyze subgroups within free Abelian groups, providing insight into their structure and rank.


This text discusses polynomial rings, focusing on properties of polynomials with coefficients in an arbitrary field F. Key points include:

1. Polynomial definition: A polynomial f(x) in F[x] is expressed as amxm + ... + a1x + a0, where an ∈F and m ≥ 0. The degree of the polynomial (deg f) is determined by the highest monomial's coefficient with a non-zero value.

2. Polynomial operations: Polynomials can be added and multiplied similarly to integers. For two polynomials f and g, their degrees satisfy deg(fg) = deg f + deg g.

3. Long division for polynomials (Theorem 14.1): Given non-zero polynomials f and g in F[x], there exist unique q and r in F[x] such that g = qf + r, where deg r < deg f. This theorem is proved using mathematical induction on n - m, with n and m being the degrees of g and f, respectively.

4. Common divisors: A polynomial f divides another polynomial g (denoted by f | g) if g = qf for some q ∈F[x]. The leading coefficient determines whether a non-zero scalar divides any given polynomial in F[x].

5. Greatest common divisor (GCD): For two polynomials f and g, their greatest common divisor is defined as the monic (leading coefficient of 1) common divisor with the highest degree. It can be found using the Euclidean algorithm similar to integers, which involves successive division steps reducing the remainder's degree until reaching zero.

These properties allow for a rich algebraic structure in F[x], resembling that of the integers Z, enabling further study of polynomial equations and their symmetries.


The provided text describes the Berlekamp algorithm for factoring polynomials over a finite field F_p. Here's a detailed explanation:

1. **Solving the congruence:** The first step is to solve the congruence g^p - g ≡ 0 (mod f), where f is the given polynomial in F_p[x]. This congruence has solutions in the quotient ring F_p[x]/(f).

2. **Finding a basis of solutions:** Let's denote the solutions as {g_1, g_2, ..., g_r}. These form a basis for the solution space W of the congruence. The dimension r of this space is crucial because it tells us how many relatively prime factors the original polynomial f has.

3. **Determining coefficients (a_i):** For each solution g_k, find all a in F_p such that (g_k - a, f) ≠ 1. This means that f and g_k - a share a common factor other than 1. The set of all such a for each k corresponds to the factors of f.

4. **Identifying irreducible factors:** If there are r distinct a values for some k, then we have found r relatively prime factors of f. If not, repeat the process with the next polynomial g obtained by solving the congruence (g - a, f) ≠ 0 (mod f).

5. **Deriving irreducible polynomials:** Once you've identified the factors q_i = p^m_i i, where i is irreducible, determine i as follows:
   - If the derivative q'_i ≠ 0, then i = q_i / (q_i, q'_i).
   - If q'_i = 0, then q_i(x) = ˜q_i(x^p), where ˜q_i is irreducible in F_p[x^p]. In this case, find its derivative and repeat the process.

The algorithm continues until all factors are found, and it terminates because each step reduces the degree of the polynomial being factored.

This algorithm is efficient for factoring polynomials over finite fields, especially when compared to brute-force methods that check every possible factor. It leverages properties of finite fields and polynomial congruences to systematically identify irreducible factors.


The text discusses the concept of extension fields in algebra, focusing on simple extensions. Here's a detailed summary and explanation:

1. **Extension Fields**: Let E and F be fields with E containing F (i.e., E ⊃ F). In this context, E is called an extension field of F, and F is considered a subfield of E. The notation E/F refers to the extension of F by E. Examples include Q(√2) and Q(ω), which are extensions of the rational numbers Q and are subfields of the complex numbers C.

2. **Simple Extensions**: A special type of extension is a simple extension, where there exists an element ζ ∈ E such that E = F(ζ). This means E is generated by adjoining ζ to F. The set F(ζ) consists of all rational expressions in ζ with coefficients from F.

3. **Theorem on Simple Extensions (16.3)**: For a simple extension E = F(ζ), the following two cases are possible:

   - **Case 1**: There exists an irreducible polynomial f ∈ F[x] such that F(ζ) is isomorphic to F[x]/(f). This means that adjoining ζ to F effectively "splits" the polynomial f into linear factors in the field extension. In other words, the minimal polynomial of ζ over F is irreducible and generates the field extension.

   - **Case 2**: F(ζ) is isomorphic to the field of rational functions F(x). This occurs when ζ is transcendental over F, meaning it is not a root of any non-zero polynomial with coefficients in F. In this case, adjoining ζ to F results in a larger field that contains all possible rational expressions in ζ.

4. **Proof of Theorem 16.3**:

   - The theorem states that for E = F(ζ), either E is isomorphic to F[x]/(f) for some irreducible polynomial f ∈ F[x], or E is isomorphic to F(x).
   
   - To prove this, consider the evaluation map φ: F[x] → E defined by φ(g) = g(ζ). This map sends a polynomial g to its value at ζ. The kernel of this map consists of all polynomials in F[x] that have ζ as a root, i.e., the ideal (f) generated by the minimal polynomial f of ζ over F.
   
   - By the First Isomorphism Theorem for rings, F[x]/(f) is isomorphic to the image of φ, which is F(ζ). Thus, F(ζ) ≅ F[x]/(f), where f is irreducible because we chose it to be minimal.
   
   - If no such irreducible polynomial f exists (i.e., ζ is transcendental over F), then the image of φ is all of E, and F(ζ) ≅ F(x).

This theorem provides a clear characterization of simple extensions, making them easier to understand and work with in algebraic structures. It shows that every simple extension either "splits" by adjoining a root of an irreducible polynomial or remains as a larger field containing rational expressions in the adjoined element.


The provided text discusses several concepts in abstract algebra, specifically focusing on field extensions and cyclotomic polynomials. Here's a detailed summary of the key points:

1. **Field Extensions**: A field extension E/F is a pair of fields where F is a subfield of E. The notation F(ζ) means the smallest subfield of E containing both F and ζ. If f(x) ∈ F[x] is a polynomial, the field F(ζ) for some root ζ of f can be described as a simple extension:
   - If f(ζ) = 0, then ¯ϵ_ζ : F[x]/(f) → F(ζ) is an isomorphism.
   - If f(ζ) ≠ 0 for all ζ and f ∈ F[x], the map ϵ_ζ : F[x] → F(ζ), defined by ϵ_ζ(g/h) = g(ζ)/h(ζ), is a homomorphism, and its image is an extension of F containing ζ.

2. **Algebraic and Transcendental Elements**: An element ζ ∈ E is algebraic over F if there exists a non-zero polynomial f(x) ∈ F[x] such that f(ζ) = 0. If no such f exists, ζ is transcendental over F. Algebraic numbers are those that are algebraic over Q (the rational numbers), and transcendental numbers are not algebraic over Q.

3. **Splitting Fields**: Given a polynomial f(x) ∈ F[x], the splitting field of f is the smallest extension E/F such that f splits into linear factors in E[x]. The degree [E : F] of an extension is defined as the dimension of E as an F-vector space.

4. **Cyclotomic Polynomials**: For a positive integer n, the nth cyclotomic polynomial Φ_n(x) is the monic polynomial whose roots are the primitive nth roots of unity. It's defined recursively using the formula x^n - 1 = ∏_d|n Φ_d(x). Cyclotomic polynomials have integer coefficients and are irreducible over Q.

5. **Finite Fields**: If F is a field of characteristic p, then for any positive integer r, there exists a unique finite field F_pr (called the Galois field of order pr) with pr elements. The number of monic, irreducible polynomials in F_p[x] of degree r is denoted by N(p, r), and they satisfy the relation ps = ∑_{r|s} N(p, r)r.

The text also includes examples and plots to illustrate these concepts:
- The behavior of cubic polynomials based on their discriminant.
- A demonstration showing how the discriminant controls the number of real roots for a family of real cubics.
- Plots of semi-cubical parabolas, illustrating regions where the discriminant is positive or negative, corresponding to different root behaviors.


The Galois Correspondence is a fundamental theorem in Galois Theory that establishes a one-to-one correspondence between subgroups of the Galois group (Gal(E/F)) and intermediate fields (subfields K such that F ⊆ K ⊆ E). This relationship provides crucial insights into understanding field extensions.

1. **Subgroup H < Gal(E/F) implies Intermediate Field Fix(H):** Given a subgroup H of the Galois group, Fix(H) = {a ∈ E | α(a) = a for all α ∈ H} is an intermediate field containing F. This means that Fix(H) is a subset of E that forms a field and contains the base field F.

2. **Intermediate Field K implies Subgroup Gal(E/K):** For any intermediate field K (F ⊆ K ⊆ E), the set Gal(E/K) = {α ∈ Gal(E/F) | α(a) = a for all a ∈ K} is a subgroup of Gal(E/F). In other words, it's the collection of automorphisms in the Galois group that preserve elements in K.

The key to understanding these relationships lies in Theorem 17.11:

**Theorem 17.11:** Let E be a field and G a finite group of automorphisms of E. Set F = Fix(G). Then [E : F] ≤ |G|.

This theorem essentially says that the degree of the extension (the dimension of E as a vector space over F) is less than or equal to the order of the group of automorphisms G. The proof involves showing that any n elements in E are linearly dependent over F when n exceeds the order of the group |G|.

The proof proceeds by considering an arbitrary set of n elements (ζ1, ..., ζn) ∈ E and constructing a system of equations based on these elements under the action of each automorphism in G. By leveraging the properties of groups and linear algebra, it is demonstrated that this system of equations must have a non-trivial solution lying in F if n > |G|, implying the linear dependence over F.

This theorem, along with its corollaries (17.6 and 17.7), forms the foundation of the Galois Correspondence. It asserts that for each subgroup H of Gal(E/F), there exists an intermediate field Fix(H) = {a ∈ E | α(a) = a for all α ∈ H}, and conversely, for every intermediate field K (with F ⊆ K ⊆ E), there is a corresponding subgroup Gal(E/K). This one-to-one correspondence uncovers deep connections between the algebraic structure of fields and their symmetry groups.


The text discusses the Galois theory of polynomials, focusing on quartics (degree 4 polynomials). Here's a summary and explanation of key concepts and results:

1. **Galois Groups of Quartics**: The text describes how to determine the Galois group of an irreducible quartic polynomial by examining its discriminant δ and the cubic resolvent r(x). Based on whether δ is a square in F (the base field) or not, the Galois group can be identified as:

   - A4 (Alternating group): If δ ∈ F.
   - S4 (Symmetric group): If δ ̸∈ F and r(x) is irreducible.
   - V4 (Klein four-group): If δ ̸∈ F and r(x) is reducible, splitting into a linear factor and an irreducible quadratic.
   - D4 or C4: If δ ̸∈ F and r(x) is reducible, splitting into two irreducible quadratics.

2. **Cubic Resolvent**: The cubic resolvent of a quartic f(x) = x^4 + b_2 x^2 + b_1 x + b_0 is defined as:

   r(x) = x^3 - 2b_2 x^2 + (b_2^2 - 4b_0)x + b_2^1

   Its discriminant, δr, is equal to the discriminant of f.

3. **Geometry of the Cubic Resolvent**: The cubic resolvent can be interpreted geometrically using conics in C^2 (complex 2-dimensional space). By substituting x^2 = y and considering the intersection of two conics, one can determine the Galois group based on the degeneracy of these conics.

4. **Software**: The text mentions a Mathematica package called 'Quartics' that computes the cubic resolvent and determines the Galois group according to the criteria outlined in the text.

5. **General Equation of the nth Degree (Theorem 19.1)**: This section introduces a theorem stating that, for an irreducible polynomial f(x) ∈ Q[x] of degree p (where p is prime), if f has exactly two non-real roots, then its Galois group Gal(f) is isomorphic to the symmetric group S_p. The proof uses properties of Sylow subgroups and transitivity of the Galois group.

6. **Examples**: Several examples of irreducible polynomials with specific Galois groups are given, including:

   - A cubic with negative discriminant (∆ < 0) has Galois group S_3.
   - f(x) = x^5 − 6x + 2 ∈ Q[x] has Galois group S_5.

7. **Symmetric Functions**: The text briefly mentions how symmetric functions can sometimes help determine the Galois group of a polynomial over Q, particularly when reductions modulo primes reveal cycle types of elements in the Galois group.

In summary, this text presents methods for determining Galois groups of quartics using discriminants and cubic resolvents, as well as a general result (Theorem 19.1) about the Galois group of irreducible polynomials with specific root properties. It also touches on geometric interpretations and computational tools for analyzing these groups.


The provided text discusses the concept of solving polynomial equations by radicals, a method that involves expressing the roots of an equation using arithmetic operations (addition, subtraction, multiplication, division) and nth roots. The main focus is on cubic equations, but the topic also extends to higher-degree polynomials.

1. **Formulas for a Cubic**: Cardano's formulas provide a way to find the roots of a cubic equation in the form f(x) = x^3 + a_1x + a_0, assuming a_1 ≠ 0. These formulas involve Lagrange resolvents ξ2 and ξ3, which are expressed in terms of the coefficients a_0 and a_1, and cube roots of expressions involving these coefficients. The roots themselves can then be recovered using linear combinations of ξ2 and ξ3.

2. **Cyclic Extensions**: A cyclic extension is an algebraic field extension E/F where Gal(E/F), the Galois group, is a cyclic group. In characteristic 0, if F contains all nth roots of unity, then the splitting field of x^n - a over F is a cyclic extension, and can be written as F(n√a) for some a ∈ F. The proof involves constructing a Lagrange resolvent ξ such that ξ^n ∈ F and E = F(ξ).

3. **Solution by Radicals in Higher Degrees**: The question of whether formulas similar to Cardano's can be derived for higher-degree polynomials is explored. It turns out that while this is possible for quartics, it cannot be done for equations of degree 5 or higher due to the nature of their Galois groups. Specifically, if the Galois group is Sn (the symmetric group on n elements), then the equation is not solvable by radicals when n ≥ 5.

4. **Galois Theory and Radical Solvability**: The text also touches upon the connection between a polynomial's Galois group and its solvability by radicals. It's shown that if the Galois group is Sn for n ≥ 5, then the polynomial cannot be solved by taking radicals. This is established using the simplicity of An (the alternating group on n elements) when n > 4, and properties of normal subgroups in simple groups.

5. **Mathematica Calculations**: The text includes examples using Mathematica's Solve function to find roots of cubic and quartic equations, demonstrating a more reliable alternative to Cardano's formulas that can handle complex numbers and other edge cases.

In summary, the text explores the methodology for solving cubic polynomials via radicals, discusses cyclic extensions in the context of field theory, and presents results from Galois theory showing why higher-degree equations generally cannot be solved by radicals. It also provides a computational aspect through Mathematica examples, illustrating the practical side of these theoretical concepts.


The document provides an overview of ruler-and-compass constructions and their relationship to algebraic problems involving polynomial equations. Here's a detailed summary:

1. **Algebraic Interpretation**: The text introduces the idea that geometric constructions can be translated into algebraic problems, particularly in solving polynomial equations. For instance, trisecting an angle τ is equivalent to finding a solution for the cubic equation 4x³ - 3x - cos(τ) = 0.

2. **Field of Constructible Lengths (L)**: The set L consists of all lengths that can be constructed using ruler and compass, given some initial points and lengths. It's shown to be a field containing Q(a₁, ..., ar), where a₁, ..., ar are the given real numbers. Moreover, if b ∈ L, then √b ∈ L.

3. **Constructibility**: A real number ζ is constructible using ruler and compass if and only if it lies in an extension E/Q(a₁, ..., ar) that can be built up as a sequence of quadratic extensions. This means [Q(a₁, ..., ar, ζ) : Q(a₁, ..., ar)] is a power of 2.

4. **Regular Polygons**: The constructibility of regular polygons is discussed. If Pn is a regular n-gon that can be constructed, then φ(n), the Euler's totient function, must be a power of 2. Conversely, if φ(n) is a power of 2, then Pn is constructible.

5. **Periods**: The concept of periods is introduced for primitive pth roots of unity (ω), where p is prime. A period ωH is defined as the sum of all elements in a subgroup H of Gal(Φp) applied to ω. These periods play a crucial role in constructing cyclotomic fields and regular polygons.

6. **Galois Theory**: The text briefly mentions Galois theory, which is used to study the solvability of polynomial equations by radicals. It's stated that if a polynomial is solvable by radicals, its Galois group must be a solvable group.

The exercises at the end of the document delve into specific problems related to these concepts, such as proving irreducibility of certain polynomials, constructing specific lengths using ruler and compass, and finding minimal polynomials of trigonometric functions over Q.


The provided list consists of mathematical terms related to various branches of algebra, number theory, and group theory. Here's a detailed explanation of some key concepts:

1. **Group**: A group is a set equipped with an operation that combines any two of its elements to form a third element in such a way that four conditions called group axioms are satisfied. These include closure, associativity, identity and invertibility. Examples include the integers under addition or multiplication, where the operations are associative, commutative (except for non-abelian groups), have an identity element, and every element has an inverse.

2. **Orthogonal Group**: Specifically, the Orthogonal Group O(n) is the group of n×n orthogonal matrices, which are square matrices whose columns and rows are orthonormal vectors (meaning they are unit vectors and orthogonal to each other). The determinant of such a matrix can be ±1. 

3. **Orthogonal Matrix**: A square matrix Q with real entries is orthogonal if its transpose is also its inverse, i.e., Q^T = Q^(-1). This implies that the columns (and rows) form an orthonormal set of vectors.

4. **Polynomial**: A polynomial is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. For example, 3x^2 + 2x - 5 is a polynomial in x. The degree of the polynomial (highest power of the variable) plays an important role in understanding its properties.

5. **Prime Field**: In field theory, a prime field is either the finite field with p elements, denoted as F_p, or the field of rational numbers Q, depending on whether the characteristic is a prime number p or 0, respectively.

6. **Projective Line**: The projective line over a field K (denoted as P^1(K)) is a geometric structure that generalizes the concept of a line to include points at infinity. It can be thought of as the set of all lines through the origin in the affine line over K, along with a point at infinity for each direction.

7. **Ring**: A ring is an algebraic structure consisting of a set equipped with two binary operations usually called addition and multiplication, where the set is an abelian group under addition and associative under multiplication. Multiplication need not be commutative. Examples include the integers (Z) and polynomials (F[x]).

8. **Field**: A field is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on rational and real numbers do. Every field contains at least two numbers, 0 and 1, and every non-zero element has a multiplicative inverse. Examples include the rational numbers (Q), real numbers (R), and complex numbers (C).

9. **Symmetric Group**: The symmetric group S_n on n letters is the group whose elements are all the permutations of n items and whose group operation is the composition of permutations. It's a fundamental example of a finite group.

10. **Polynomial Ring**: In abstract algebra, a polynomial ring in one variable over a commutative ring R is denoted by R[x]. It consists of all polynomials in x with coefficients from R. 

11. **Separable Polynomial**: A separable polynomial is a nonconstant polynomial whose roots are distinct in its splitting field. In characteristic 0 (like the rational numbers Q), every irreducible polynomial is separable, but this isn't true for fields of positive characteristic.

12. **Simple Extension**: If E is an extension field of F and α is algebraic over F, then the set {β ∈ E | β is algebraic over F(α)} forms a subfield of E called the simple transcendental extension of F(α) over F, denoted by F(α).

13. **Sylow p-subgroup**: In group theory, Sylow p-subgroups are certain p-subgroups of a given finite group G; they are named after Ludwig Sylow. The number of Sylow p-subgroups is congruent to 1 modulo p and divides the order of G.

14. **Splitting Field**: Given a field extension L/K, if there exists a larger field E such that L is a subfield of E and the minimal polynomial of every element of L splits into linear factors over E, then E is called a splitting field of L/K.

These terms are fundamental in understanding various aspects of algebraic structures and their properties. They underpin many advanced concepts in abstract algebra, number theory, and geometry.


### cmbook

Continued fractions are a fascinating representation of numbers, expressing them as an "infinite" fraction where each term is itself a fraction. The general form of a continued fraction is given by:

    a_0 + 1/(a_1 + 1/(a_2 + 1/(...)))

Here's a detailed explanation and example to illustrate their concept:

**Explanation:**

1. **Basic Concept**: A continued fraction represents a number as the sum of an integer part followed by the reciprocal of another fraction, which itself is composed similarly (i.e., having another integer plus a reciprocal). This nesting can continue indefinitely.

2. **Notation**: The general form of a continued fraction is written as:

    \[a_0 + \frac{1}{a_1 + \frac{1}{a_2 + \frac{1}{\ddots}}}\]

   where \(a_0, a_1, a_2, \dots\) are integers.

3. **Representation**: Each \(a_n\) represents the numerator (or sometimes denominator) of the fraction in the nth position. The overall value of the continued fraction is approached as the number of terms increases.

4. **Convergence**: For most continued fractions, the sequence of partial sums converges to a specific real number or complex number depending on whether \(a_n\) are real or complex.

**Example: The Golden Ratio (φ)**:

The golden ratio φ is famously represented by the continued fraction:

    \[\phi = 1 + \frac{1}{1 + \frac{1}{1 + \frac{1}{1 + \dots}}}\]

   Let's compute this for a few terms to see how it converges:

- \( \phi_0 = 1 \)
- \( \phi_1 = 1 + \frac{1}{1} = 2 \)
- \( \phi_2 = 1 + \frac{1}{2} = \frac{3}{2} = 1.5 \)
- \( \phi_3 = 1 + \frac{1}{1.5} = \frac{4}{3} \approx 1.3333 \)
- \( \phi_4 = 1 + \frac{1}{1.3333} \approx 1.6667 \)
- \( \phi_5 = 1 + \frac{1}{1.6667} \approx 1.6000 \)
- \( \phi_6 = 1 + \frac{1}{1.6000} \approx 1.6154 \)
- \( \phi_7 = 1 + \frac{1}{1.6154} \approx 1.6190 \)
- \( \phi_8 = 1 + \frac{1}{1.6190} \approx 1.6176 \)

As we can see, the sequence of fractions \(\phi_n\) approaches a limit around 1.61803398875..., which is the golden ratio φ ≈ 1.61803398875 (rounded to 12 decimal places).

**Properties and Uses**:

- **Irrational Numbers**: Many irrational numbers, like √2, e, and π, can be expressed as continued fractions, often providing a way to approximate these constants.
  
- **Algebraic Numbers**: Even algebraic numbers (roots of polynomials with integer coefficients) have continued fraction representations.

- **Approximation and Computation**: Continued fractions are useful for numerical approximations, allowing one to find rational numbers that closely match irrational values, which can be essential in computational mathematics and computer science applications.

Continued fractions bridge the gap between discrete sequences (fractions) and continuous mathematical concepts (irrational numbers), offering both theoretical insights and practical computational tools.


1. Value of π Approximation using Recurrence Relation:

The provided script implements a recurrence relation to approximate the value of π based on the infinite series:

π ≈ 2 + ∑(1/(n-j)) for j = 0, ..., n-1

Here's a MATLAB implementation for this series approximation:

```MATLAB
function pi_approx = pi_recurrence(n)
    s = 1; % Initializing the sum (s0)

    for j = 0 : n - 1
        term = 1 / (j + 1); % Calculating the next term of the series
        s = s + term; % Updating the sum (sj)
    end

    pi_approx = 2 + 2/s; % Final approximation of π
end
```

You can call this function with a desired value for `n` to approximate π. For example, `pi_recurrence(100)` will compute an approximation using the first 100 terms of the series.

2. Finite and Infinite Series:

A finite series is a sum of a finite sequence of numbers, while an infinite series is the sum of an infinite sequence. The provided examples show how to approximate π using both finite (truncated) and infinite series in MATLAB.

3. Principles of Programming: Arrays & Functions:

- Arrays are used for storing sequences or multiple variables with integer length and indexed elements. In MATLAB, arrays can be created and manipulated easily, allowing for efficient storage and manipulation of data.
- Functions in programming languages (including MATLAB) are 'black boxes' that take inputs and produce outputs based on a set of instructions. They allow modularizing code and improving reusability and readability.

4. Conditional Statements:

Conditional statements, such as `if`, `elseif`, and `else`, enable decision-making within MATLAB programs by executing different actions depending on whether certain conditions are met. Relational operators (`<`, `<=`, `==`, `>=`, `>`, `~=`) compare values to determine the truth or falsity of a condition, which is stored as 1 (true) or 0 (false).

5. Root Finding: Bisection Method & Newton's Method:

- The bisection method is an iterative algorithm for finding roots of a continuous function within a given interval `[a, b]` where the function changes sign (`f(a) * f(b) < 0`). It works by repeatedly dividing the interval in half and selecting a subinterval that contains the root.
- Newton's method is another iterative approach for finding roots of a differentiable function `f(x)` given an initial guess `x0`. The method updates the guess using the formula: `xn+1 = xn - f(xn)/f'(xn)`. Its convergence can be faster than bisection but may also diverge under certain conditions.


The probability of ending up at location j on a Galton board with n rows of pegs is given by the formula:

    P(j) = (n!)/(2^n * j!(n - j)!) for j = 0, 1, ..., n

Here's a detailed explanation of this formula:

1. **Factorials**: The factorial function, denoted by "!", is used to represent the product of all positive integers up to that number. For example, 5! = 5 × 4 × 3 × 2 × 1 = 120.

2. **Combination**: The term (n choose j), or C(n,j), represents the number of ways to choose j items from a set of n distinct items without regard to the order of selection. It is given by the formula:

    C(n, j) = n! / (j!(n - j)!)

3. **Probability calculation**: On the Galton board, at each peg, the ball has an equal probability (1/2) of going left or right. The number of paths to location j is given by the combination C(n, j), since you have n choices for pegs where you go left and then j of those must be chosen to end up at location j.

4. **Final probability**: Since each path has a probability of (1/2)^n (because there are n decisions and each decision has a 1/2 chance of being correct), the overall probability of ending up at location j is:

    P(j) = C(n, j) * (1/2)^n

5. **Simplification**: The provided formula simplifies this expression by rewriting it as:

    P(j) = n! / (2^n * j!(n - j)!)

This formula calculates the probability of the ball ending up at location j after dropping through n rows of pegs on a Galton board. It takes into account all possible paths and their probabilities, giving us a comprehensive statistical model for predicting outcomes in such experiments.


(a) The given statement can be proved using combinatorial principles. We start by considering a binary tree representing n coin tosses, where each internal node has two children (representing heads or tails), and the leaves represent the possible outcomes. 

For a single coin toss, there are 2 possible outcomes: Heads (H) or Tails (T). So, for n tosses, we have 2^n possible outcomes.

Now, let's analyze the number of ways to get exactly j heads in n tosses:

1. For each sequence with j heads, there is a corresponding sequence with (n-j) tails.
2. The position of these j heads can be chosen from n+1 possible positions (including the beginning and the end of the sequence).
3. Thus, we select j positions for heads out of n+1 total positions, which is given by the binomial coefficient C(n+1, j) = (n+1)! / [j! * (n-j)!].
4. For each selection of j positions, there are 2^(n-j) ways to fill the remaining (n-j) tails positions.
5. Combining these, we get the total number of sequences with exactly j heads as C(n+1, j) * 2^(n-j).

Summing this for all possible values of j from 0 to n gives us the total number of outcomes:

∑_{j=0}^{n} C(n+1, j) * 2^(n-j) = ∑_{j=0}^{n} (n+1)! / [j! * (n-j)!] * 2^(n-j).

To simplify this expression, we can use the binomial theorem. The binomial theorem states that for any real numbers a and b and non-negative integer n:

(a + b)^n = ∑_{j=0}^{n} C(n, j) * a^(n-j) * b^j.

Setting a = 1 and b = 2, we get:

2^(n+1) = (1 + 2)^(n+1) = ∑_{j=0}^{n+1} C(n+1, j) * 2^j.

This can be rewritten as:

2^(n+1) = C(n+1, 0) * 2^0 + ∑_{j=1}^{n} C(n+1, j) * 2^j + C(n+1, n+1) * 2^(n+1).

Simplifying, we find:

2^(n+1) = 1 + ∑_{j=1}^{n} C(n+1, j) * 2^j + 2^(n+1),

which implies that:

∑_{j=1}^{n} C(n+1, j) * 2^j = 2^(n+1) - 1.

Recall our expression for the total number of outcomes: ∑_{j=0}^{n} (n+1)! / [j! * (n-j)!] * 2^(n-j). We can rewrite this using the binomial coefficient property C(n, j) = C(n, n-j):

∑_{j=0}^{n} (n+1)! / [j! * (n-j)!] * 2^(n-j) = ∑_{j=0}^{n} (n+1)! / [(n-j)! * j!] * 2^(n-j).

This is equivalent to:

(n+1) * [∑_{j=1}^{n} C(n+1, j) * 2^j],

which we've shown equals (n+1) * [2^(n+1) - 1].

Thus, the total number of outcomes is:

n ∑_{j=1}^{n} C(n, j) = n * 2^n.

(b) To modify the theory where the probability of a ball being deflected left at each peg is p ≠ 1/2, we can adjust the initial conditions and update rules accordingly:

1. Initial Conditions: If initially, the ball has a probability q of being deflected left (q ≠ 0.5), then at time step k = 1, u(1, j) = q for the central cell and u(1, j) = 0 for all other cells j ≠ n/2.

2. Update Rules: For interior cells (1 < j < n-1), the update rule becomes:

   u(k+1,j) = f(u(k,j−1), u(k,j), u(k,j+1)), where f is now a function that depends on p. Specifically, if we consider the case where the deflection probability at each peg is p (with 0 < p < 1), then:

   - If all three neighboring cells are right-deflected (RRR): u(k+1,j) = 0 with probability p.
   - Otherwise, u(k+1,j) = 1 with probability (1-p).

For edge cells (j = 1 or j = n), the update rules are:

   - u(k+1,1) = 0 and u(k+1,n) = 0 since there's no left-deflecting neighbor.
   - For all other edges (j = 2 to n-1), if both neighboring cells are right-deflected: u(k+1,j) = 0 with probability p; otherwise, u(k+1,j) = 1 with probability (1-p).

These adjustments allow for a more general model where the deflection probabilities at each peg can be tuned using parameter p. The code adaptation would involve modifying the initialization and update logic to accommodate these changes.


The text provided discusses dynamical systems, which describe the evolution of a system of variables through either differential equations (continuous dynamical systems) or difference equations (discrete dynamical systems). These systems are often derived from modeling physical phenomena and can exhibit fascinating behavior.

11.1 Introduction to Dynamical Systems:

- Dynamical systems are a significant part of applied mathematics, describing the evolution of variables through time, either continuously or discretely. They are usually derived from modeling physical phenomena and can display intriguing behaviors.

11.1.1 Example of a Continuous Dynamical System:
- A simple example is a population model where the growth rate of a species' population p(t) is proportional to its current size, modeled by the first-order ordinary differential equation (ODE):

  dp/dt = αp, for t > t0, with initial condition p(t0) = p0.

- The solution to this ODE can be found analytically as p(t) = p0 * exp(αt), representing exponential growth if there are abundant resources and no threats.

11.1.2 Example of a Discrete Dynamical System:
- A discrete version of the above model is given by the recurrence relation:

  pn = αpn−1, for n > 0 with initial condition p0.

- The solution to this difference equation is easy to derive analytically as pn = α^n * p0.

11.1.3 Matlab's ODE Solver (ode45):
- To solve a first-order ODE like the one in (11.1), Matlab provides the built-in solver ode45. The script demonstrates using ode45 to solve the population model:

  ```matlab
  global alpha; % shared value of alpha elsewhere in the code
  alpha = 1; % set value of alpha
  [t, y] = ode45(@popfun, [0, 1], 1); % solve ODE system over range 0 < t < 1 with initial condition y(1) = 1
  plot(t, y); % plot solution against time
  ```

- This script calls a separate user-defined function popfun.m that computes the derivative (dy/dt).

- Key points:
  - The 'global' keyword allows variables set in one part of the code to be shared with other parts.
  - ode45 takes three arguments: function defining the derivative, time range, and initial condition. It outputs arrays of discrete time steps (t) and corresponding solution values (y).
  - A user-defined function is required for computing the derivative, which ode45 calls during its execution.

11.2 Prey vs Predator Model:
- The text then introduces a more complex model involving two species—rabbits (r(t)) and foxes (f(t)). Their population dynamics are described by coupled non-linear ODEs:

  dr/dt = αr - βrf,
  d f/dt = δrf - γf,

for t > 0 with initial conditions r(0) = r0 and f(0) = f0. Here, α, β, γ, and δ are assumed to be positive constants.

- The model takes into account interactions between the two species:
  - Rabbit population grows exponentially (αr) but is reduced by predation from foxes (−βrf).
  - Fox population grows due to consuming rabbits (δrf) but declines naturally (−γf).

- The constants α, β, γ, and δ represent various factors such as birth rates, death rates, predation efficiencies, etc. The model's behavior depends on these parameters' values.

The text concludes by hinting at the potential for studying fascinating behaviors exhibited by dynamical systems, including the prey-predator model mentioned above.


Summary:

1. **Predator-Prey Model (Lokta-Volterra Equations):** This is a system of two coupled, non-linear ordinary differential equations that model the dynamics between a predator (foxes) and its prey (rabbits). The model describes how populations grow or decline based on their interactions.

   Equations:
   - dr/dt = αr - βrf (growth of rabbits, reduced by predation)
   - df/dt = δrf - γf (decline of foxes due to lack of food, exponential decay when no rabbits present)

   Here, α is the rabbit growth rate, β is the predation rate, γ is the fox death rate, and δ is the conversion efficiency.

2. **Equilibrium Solutions:** The equilibrium solutions are points where both populations remain constant over time (dr/dt = 0 and df/dt = 0). They are found by setting the right-hand side of each equation to zero:
   - f* = α/β, r* = γ/δ or f* = 0, r* = 0

   These equilibrium points represent scenarios where either predator or prey populations dominate, or both are absent.

3. **Numerical Solution:** Due to the non-linear nature of these equations and the coupled ODE system (two equations for two unknowns), analytical solutions aren't possible in closed form. Instead, numerical methods like ode45 in MATLAB can be used to solve them. The provided code (lv.m) demonstrates how to do this, producing time series plots of rabbit and fox populations over time as well as a phase portrait illustrating their relationship.

4. **Logistic Map:** A discrete-time model describing the evolution of a population, here represented by p(n). It's given by:
   - pn = αpn−1 (1 −pn−1), for n ≥ 1

   Here, α is a parameter controlling the growth rate. The equilibrium solutions are found by setting pn = x* and solving x* = αx*(1 −x*), giving x* = 1 −1/α or x* = 0.

   MATLAB code (lmap.m) numerically explores the long-term behavior of this system for different values of α, revealing various dynamic regimes including convergence to a constant value, periodicity, and chaos.

5. **Lorenz Attractor:** A set of chaotic solutions arising from a simplified model of atmospheric convection. The system is described by three non-linear ODEs:
   - dx/dt = σ(y −x)
   - dy/dt = x(ρ −z) −y
   - dz/dt = xy −βz

   For specific parameter values (σ, β, ρ), this system exhibits chaotic behavior where trajectories are sensitive to initial conditions. MATLAB code (lorenz.m) generates plots illustrating the complex, aperiodic dynamics of this system in phase space.

6. **Netflix Prize Problem:** An optimization challenge posed by Netflix in 2006 to improve its movie recommendation algorithm. The goal was to develop a method that could predict user preferences for movies with at least a 10% improvement over the existing system ("Cinematch"). This involved predicting missing ratings in a sparse matrix representing users' evaluations of films on a 1-5 scale, using techniques like matrix factorization.

7. **Matrix Factorization Approach:** A key method employed in solving the Netflix Prize problem. The central idea is to approximate the original rating matrix R as the product of two lower-dimensional matrices P and Q, capturing user preferences for categories (P) and movie characteristics corresponding to these categories (Q). Minimizing a cost function that quantifies the prediction error guides the iterative refinement of P and Q towards optimal values.

8. **MATLAB Code:** Provides an implementation of the matrix factorization approach using gradient descent optimization to solve the Netflix Prize problem. The code iteratively updates matrices P and Q, aiming to minimize the difference between predicted ratings (P*Q^T) and actual ratings in R. It also includes regularization to prevent overfitting and bias correction to isolate user and movie preferences from overall averages.

9. **Improvements:** Discusses enhancements to the basic matrix factorization approach, including:
   - Regularization: Adding a penalty term to discourage large values in P and Q, helping to prevent overfitting.
   - Isolating Bias: Subtracting user and movie averages from the rating matrix before applying matrix factorization, then re-incorporating these averages after obtaining predictions. These modifications lead to more robust and less sensitive solutions across different simulations.


The provided text contains solutions to various problems related to numerical methods, geometrical arguments, and programming using MATLAB. Here's a detailed summary of each section:

1. Geometric Argument and Recurrence Relation:
   The geometric argument involves the relationship between two terms in a sequence (dn and dn+1), which is rearranged to express dn+1 in terms of dn. This recurrence relation is then used to approximate a specific mathematical constant.

   The rearrangement goes as follows:
   ((dn/2)^2 - (dn+1/2)^2) = (dn+1/2)^2 + ((s*(dn/2)^2 + 1) - 1)^2
   This can be simplified to:
   dn+1/2 = sqrt((dn/2)^2 + (s*(dn/2)^2 + 1 - 1)^2)

2. MATLAB Scripts for Sequences and Series:
   a. A script to approximate π using a specific sequence is provided, with varying convergence depending on the number of terms (n).

   b. Another script calculates the sum of cubes of integers up to n (denoted as Σj^3), comparing it to the given formula Σj=1..n j^3 = (n*(n+1)/2)^2.

   c. A third script explores the Euler-Mascheroni constant, γ, which is the limit of the difference between the harmonic series and the natural logarithm. The script visualizes this relationship using a plot of sn - ln(n) against n.

3. MATLAB Scripts for Various Problems:
   Several scripts are given to solve different problems using MATLAB, including calculating factorials, Stirling's approximation for n!, Pascal's triangle, sinc function, square wave function, Newton's method, bisection method, secant method, bubble sort algorithm, and series convergence tests.

4. Explanations of Convergence and Error:
   The text explains the concept of convergence in numerical methods and error analysis for different integration techniques (rectangular midpoint rule, trapezium rule, and Simpson's Rule). It demonstrates how errors decrease as the number of subintervals (n) increases, with specific rates depending on the method used.

   For example:
   - The rectangular midpoint rule has an error approximately proportional to 1/n when f'(a) = f'(b), meaning doubling n reduces the error by a factor of four.
   - Simpson's Rule has errors that decrease even faster, roughly proportional to 1/n^4, implying quartic improvement in accuracy for each doubling of n.

5. Additional Problem Solutions:
   The text includes solutions for various mathematical problems, such as computing Catalan numbers, evaluating spherical Bessel functions (sph(x,n)), and simulating random walks in one and two dimensions. These solutions are presented in MATLAB code format, often accompanied by explanations of the underlying mathematics or computational methods.


The provided text appears to be a collection of problem solutions and code snippets related to numerical methods, differential equations, and MATLAB programming. Here's a detailed summary and explanation of each section:

### 1. Error Ratio and Integration Techniques
- **Error Ratio:** The ratio of consecutive errors is approximately 16, indicating that the error decreases by a factor of 1/16 as n (presumably step size or iteration count) doubles.
- **Improper Integral:** A technique for solving an improper integral using substitution: t = u/(1 - u), leading to a transformed integral and subsequent evaluation with MATLAB.

### 2. Numerical Methods
- **Euler's Method vs Midpoint Method:** Comparison of Euler's method and the Midpoint method, showing that the Midpoint method (with a modified line in the code) produces similar results but may be more accurate.
- **Challenges with ln(sin x) Integration:** Discussion on the difficulty of numerically integrating `ln(sin(x))` due to its behavior near x = 0 and suggestions for alternative methods, such as using Maclaurin series approximations.

### 3. Vector Calculations
- **Cross Product and Dot Product Properties:** Demonstrates the use of cross product (cross) and dot product (dot) operations on vectors to verify algebraic identities.

### 4. Linear Algebra Problems
- **Matrix Inversion and Eigenvalue Computation:** Various exercises involving matrix inversion, determinant calculation, and eigenvalue computation using MATLAB's built-in functions.

### 5. Numerical Methods for ODEs
- **Power Method for Eigenvalues:** Implementation of the power method to approximate the dominant eigenvalue and eigenvector of a given matrix.

### 6. PageRank Algorithm
- **Page Ranking Problem:** Scripts implementing different versions of the PageRank algorithm, involving matrix operations to calculate the rankings of web pages based on link structures.

### 7. Dynamical Systems
- **Population Model and Chaos Theory:** Code for solving ordinary differential equations (ODEs) that model population dynamics, showcasing chaotic behavior in different parameter regimes using Lorenz attractor as an example.

### 8. Additional Problems and Solutions
- **Various Numerical Methods and Dynamical Systems:** Includes additional problems and solutions covering topics like numerical integration techniques, solving systems of ODEs, and analyzing dynamical systems' stability and behavior.

Each section provides detailed explanations and code snippets that demonstrate how to implement and analyze various mathematical concepts using MATLAB, focusing on numerical methods, differential equations, linear algebra, and computational dynamics. The solutions also highlight the importance of understanding algorithmic steps and parameter sensitivity in numerical simulations.


### comsoc

The Handbook of Computational Social Choice is an authoritative overview of the field that combines computer science and economics to study collective decision-making processes. The book is divided into four main parts, each focusing on a different aspect of computational social choice:

1. Part I: Voting - This section covers various aspects of voting rules and their analysis. It includes an introduction to the theory of voting (Chapter 2), which discusses classical themes such as Condorcet extensions, scoring rules, and run-offs. Fishburn's classification of voting rules is also presented, grouping them into C1, C2, and C3 classes based on their properties.

   - Chapter 3: Tournament Solutions focuses on C1 functions (voting rules that depend only on pairwise majority comparisons) represented by directed graphs called tournaments. The chapter covers topics like McGarvey's Theorem, strategyproofness, implementation via binary agendas, and extensions to weak tournaments.

   - Chapter 4: Weighted Tournament Solutions discusses C2 functions (voting rules that depend on weighted pairwise majority comparisons) represented by weighted directed graphs. It focuses on prominent voting rules such as Kemeny's rule, the maximin rule, and Schulze's method, analyzing their computation, approximation, and fixed-parameter tractability, with a particular emphasis on Kemeny's rule.

   - Chapter 5: Dodgson's Rule and Young's Rule examines C3 voting rules (requiring strictly more information for winner determination) such as Dodgson's and Young's rules. The complexity of their winner determination problem is analyzed, along with methods for bypassing the intractability through approximation algorithms, fixed-parameter tractable algorithms, and heuristic algorithms.

   - The remaining chapters in Part I address specific methodologies for analyzing voting rules, including topics like manipulation barriers (Chapter 6) and further exploration of axiomatic, strategic, and computational aspects of voting.

2. Part II: Fair Allocation - This section focuses on the problem of allocating indivisible goods to individuals with heterogeneous preferences in a fair manner. It distinguishes between divisible (Chapter 12) and indivisible (Chapter 13) goods, with an emphasis on cake-cutting algorithms for dividing resources fairly among multiple parties.

   - Chapter 12: Fair Allocation of Indivisible Goods introduces preference structures and the fairness vs. efficiency trade-off. It covers methods for computing fair allocations and protocols designed to achieve them.

   - Chapter 13: Cake Cutting Algorithms presents classic cake-cutting algorithms, analyzing their complexity and discussing optimal methods for fairly dividing a resource among several parties with different preferences.

3. Part III: Coalition Formation - This section addresses questions arising when agents can form coalitions and each have preferences over these coalitions. It includes two-sided matching problems (Chapter 14), hedonic games (Chapter 15), and weighted voting games (Chapter 16).

   - Chapter 14: Matching under Preferences focuses on two-sided preferences in matching markets, discussing various solution concepts like stable marriage and roommates.

   - Chapter 15: Hedonic Games examines the situation where agents' preferences depend purely on the members of the coalition they are part of, studying solution concepts and computational complexity.

   - Chapter 16: Weighted Voting Games explores scenarios where coalitions emerge to achieve specific goals (e.g., passing a bill in parliament), discussing voter weight versus voter power and simple games.

4. Part IV: Additional Topics - This section covers topics that do not fit neatly into the first three thematic parts, including judgment aggregation (Chapter 17), applications of the axiomatic method to reputation systems on the Internet (Chapter 18), and knockout tournaments (Chapter 19).

In summary, this handbook provides a comprehensive introduction to computational social choice by exploring various aspects such as voting rules, fair allocation, coalition formation, and additional topics. It aims to be accessible for students, scholars from computer science and economics, and other researchers interested in mathematical and social sciences. The book highlights the interdisciplinary nature of computational social choice by incorporating theoretical computer science concepts like computational complexity theory and approximation algorithms into traditional social choice problems.


Chapter 2 of the Handbook of Computational Social Choice provides an introduction to the theory of voting. It focuses on multicandidate voting with ranked ballots, where each voter submits a linear ordering of alternatives, specifying their preferences from most favored to least favored.

The chapter discusses three fundamental results in voting theory:

1. Majority cycles (Condorcet's principle): A situation where collective preference violates individual rationality, with a majority preferring alternative A over B, another majority preferring B over C, and yet another majority preferring C over A.
2. Arrow's Impossibility Theorem: Every voting rule for three or more alternatives either violates Independence of Irrelevant Alternatives (IIA) or is a dictatorship, where the election outcome depends solely on one designated voter's ballot.
3. Gibbard-Satterthwaite Theorem (GST): Every strategyproof social choice function (SCF), other than a dictatorship, fails to be immune to manipulation by individual voters.

The chapter introduces several voting rules within the SCF context and discusses their properties or axioms. It covers:

- Plurality voting: Selects the alternative with the most votes (greatest number). Criticized for potentially electing an unpopular candidate due to a mere plurality, not majority, of support.
- Copeland: Assigns points based on pairwise majority victories and defeats, disregarding margins. The Copeland score is the difference between the number of alternatives preferred by strict majority and those that prefer it. The alternative with the highest score wins.
- Borda (both symmetric and asymmetric): Awards points to alternatives according to their rankings on individual ballots, with higher ranks receiving more points. The symmetric version gives equal weights to all positions, while the asymmetric version assigns different weights (e.g., 2 for first place, 1 for second place, and 0 for last).

The chapter also discusses three axioms that distinguish voting rules: anonymity, neutrality, and Pareto property:

- Anonymity: Each pair of voters plays interchangeable roles; swapping the ballots of two voters does not affect the outcome.
- Neutrality: Interchangeable alternatives (equivalent to swapping their positions in every ballot) result in an equivalent outcome.
- Pareto property (Pareto optimality): The winning set never contains a dominated alternative; that is, no alternative exists that another alternative beats in pairwise comparisons according to all voters' preferences.

These axioms are often uncontroversial and help identify interesting SCFs while minimizing the risk of unintended consequences. The chapter also mentions other groups of axioms (milder, stronger) and strategyproofness, with impossibility results like Arrow's Theorem and IIA controversies.

In summary, this chapter offers an overview of voting theory, focusing on SCFs using ranked ballots. It introduces fundamental results such as majority cycles and the GST and discusses key voting rules (plurality, Copeland, Borda) and their axiomatic properties, including anonymity, neutrality, and Pareto property. The chapter lays the groundwork for understanding various aspects of strategic manipulation in voting systems.


The Gibbard-Satterthwaite Theorem states that any resolute (non-imposed), nondictatorial, and strategyproof Social Choice Function (SCF) for three or more alternatives must be a dictatorship. This means that some voter's preferences always determine the outcome of the election.

The proof involves several lemmas:
1. Push-Down Lemma: If an SCF is resolute and down monotonic, then there exists a profi le where voters can manipulate their rankings to achieve their desired outcome.
2. Adjustment Lemma: A Pareto, resolute, and down monotonic SCF must be imposed (non-manipulable).
3. Splitting Lemma: If a dictating set is split into disjoint subsets Y and Z, then either Y or Z must also be a dictating set.
4. The proof of the Adjustment Lemma uses the Push-Down Lemma to show that if an SCF is resolute, Pareto, and down monotonic, it cannot have any manipulable voters.

Limitations of the Gibbard-Satterthwaite Theorem include:
1. It applies only to resolute SCFs, while many real-world voting rules are irresolute (allowing ties).
2. Real-life conditions often prevent a single voter from knowing other voters' ballots and predicting the outcome of manipulations.
3. The theorem does not cover Social Decision Schemes that use different types of inputs or outputs for elections.

A generalization, the Duggan-Schwartz Theorem, suggests that even allowing a large number of ties (ties as numerous as the Omninominator rule) may not be enough to achieve strategyproofness in irresolute SCFs.

Black's Theorem and Sen's Possibility Theorem demonstrate that certain domain restrictions can guarantee transitivity and strategyproofness for specific types of profi les (single-peaked and value-restricted, respectively).


This chapter discusses Tournament Solutions in the context of aggregating binary preferences from individual agents to a group, focusing on Social Choice Functions (SCFs) based solely on the dominance relation, known as C1 functions. The majority rule is characterized by May's Theorem for two alternatives, and most common voting rules satisfy its axioms in this case.

The chapter begins by introducing key concepts: a set of voters N, a set of alternatives A, and a preference profile R = (≿1, ..., ≿n), where ≿i is the preference relation of voter i. The majority relation ≿ for R is defined such that a ≿ b if and only if |{i ∈ N : a ≿i b}| ≥ |{i ∈ N : b ≿i a}|.

McGarvey's Theorem states that for any complete relation over alternatives, there exists a preference profile with at most n ≤ m(m-1) voters that induces this relation. This theorem implies that every majority relation can be obtained from some preference profile, and it is used to demonstrate that, in odd-numbered voter cases, the dominance relation (the asymmetric part of the majority relation) is antisymmetric, connex, and irreflexive—thus, forming a tournament.

A tournament solution is a function S mapping each tournament T = (A, ≻) to a nonempty subset S(T) of its alternatives A, known as the choice set. Tournament solutions are required not to distinguish between isomorphic tournaments and should satisfy certain properties: monotonicity, stability, and composition-consistency.

Monotonicity means that an alternative remains in the choice set when its dominion is expanded without altering anything else. Stability requires that a chosen subset of alternatives is selected from the union of two other subsets if and only if it is selected from each subset individually. Composition-consistency implies that the solution chooses "best" alternatives from "best" components, as defined by decompositions of tournaments.

The chapter then introduces several common tournament solutions:

1. Trivial (TRIV): Always selects all alternatives, satisfying monotonicity, stability, and composition-consistency but is not discriminatory.
2. Condorcet Non-losers (CNL): Selects all alternatives except for Condorcet losers, violating stability and composition-consistency while satisfying monotonicity.
3. Copeland Set (CO): Chooses alternatives with maximal dominion size. It satisfies monotonicity but not stability or composition-consistency.
4. Slater Set (SL): Selects alternatives that are maximal elements in strict linear orders derived from the tournament by inverting minimal edges. SL violates stability and composition-consistency, and membership is NP-hard to decide.
5. Markov Set (MA): Determines alternatives based on their maximum probability in a stationary distribution of a Markov chain defined by the tournament's adjacency matrix. MA satisfies monotonicity but not stability or weak composition-consistency, with polynomial-time computation.
6. Bipartisan Set (BP): Based on maximal lotteries over alternatives, satisfying monotonicity, stability, and composition-consistency. Computation is in polynomial time using a linear feasibility problem.
7. Uncovered Set (UC): Selects maximal elements according to the covering relation—a transitive subrelation of the dominance relation. UC satisfies monotonicity and composition-consistency but not stability; it can be computed in polynomial time via matrix multiplication.
8. Banks Set (BA): Chooses maximal alternatives from all inclusion-maximal transitive subtournaments, violating stability and composition-consistency. Deciding membership is NP-complete, and there's no known polynomial-time algorithm for its computation.

The chapter concludes by discussing methods to refine tournament solutions based on stability criteria, leading to new solutions like the top cycle, minimal covering set, and minimal extending set. These solutions aim to maintain or improve properties such as monotonicity, stability, and composition-consistency while providing alternative ways to aggregate preferences in diverse contexts.


Kemeny's Rule is a social choice function that aggregates individual preferences into a collective ranking by maximizing agreement with the input profiles. It was first introduced by Kemeny (1959) as a method to find the linear order that agrees with the maximum number of pairwise comparisons from given preference profiles.

The rule is defined based on the concept of majority margins, which are differences in the number of voters preferring one alternative over another. A weighted tournament, represented by an antisymmetric matrix M where entries (M)xx = 0 and (M)xy = mR(x, y) for x ≠ y, can be derived from a preference profile R.

Kemeny's Rule selects the linear order with the minimum score, which is calculated as the sum of distances to each individual preference order in terms of inversions (Kendall's tau distance). This rule is a C2 function since it depends only on pairwise majority margins and not on their absolute values.

The computational complexity of Kemeny's Rule has been studied extensively:

1. Kemeny Score: Given a preference profile R and an integer k, the problem asks whether there exists a linear order with score at most k. This decision problem is NP-complete for even n ≥ 4 and for odd n when n is unbounded (Bartholdi et al., 1989a; Hudry, 1989). The case of odd n ≥ 3 remains open.

2. Kemeny Winner: This decision problem asks whether there exists a linear order that ranks a given alternative x as the first and has minimum score with respect to R. Its complexity is also NP-complete for even n ≥ 4 and for odd n when n is unbounded (Bartholdi et al., 1989b; Hudry, 1989).

3. Kemeny Ranking: This problem asks whether there exists a linear order that ranks one alternative x above another y with minimum score. Its complexity is even harder and belongs to the class 𝜆P2 of problems solvable via parallel access to NP (Hemaspaandra et al., 2005).

4. Kemeny Rank Aggregation: The optimization problem asks for finding a linear order that has minimum score with respect to R. Its computational complexity is also believed to be hard, as efficient algorithms are unlikely to exist due to its membership in the class 𝜆P2.

Due to these NP-completeness and 𝜆P2-completeness results, exact polynomial-time algorithms for Kemeny's Rule are generally not feasible. Researchers have explored alternative approaches:

1. Exponential-Time Parameterized Algorithms: These methods aim to solve the problem more efficiently by targeting specific parameters within the input instance. However, their effectiveness is limited in practice due to large parameter values.

2. Polynomial-Time Approximation Algorithms: These algorithms trade solution quality for polynomial running time. A 5-approximation algorithm for Kemeny Rank Aggregation was proposed by Coppersmith et al. (2010), ordering alternatives based on increasing Borda scores.

3. Practical Heuristics and Exact Methods: Various heuristic and exact methods have been developed to address real-world applications of Kemeny's Rule, even though they may not guarantee optimality or polynomial runtime in all cases. Examples include the minimum feedback arc set approach, local search algorithms, and integer programming formulations.

In summary, while Kemeny's Rule offers an attractive way to aggregate individual preferences into a collective ranking based on agreement, its computational complexity poses significant challenges for finding exact solutions efficiently. Approximation algorithms and heuristic methods provide practical alternatives for tackling these problems in various applications.


The Dodgson winner problem, which determines the winner of an election under Dodgson's voting system, has been proven to be 𝑶p_2-complete by Hemaspaandra et al. (1997a). This result implies that unless NP = coNP, the problem is not NP-complete and is considered highly intractable.

The complexity of the Dodgson winner problem being 𝑶p_2-complete was established through a complex proof structure. The core idea involves proving several seemingly easy properties about Dodgson elections to ultimately demonstrate hardness. These properties include:

1. Trapping potential scores within two adjacent values, using an NP-hardness reduction (L1).
2. Creating a "double exposure" merging key information from two elections in polynomial time (L2).
3. Summing the Dodgson scores of candidates across multiple elections to equal the score of a designated candidate in a single election (dodgsonsum, Lemma 5.2).

The proof exploits the structure of these properties and uses them in conjunction with Wagner's technical lemma (Lemma 5.3) to establish 𝑶p_2-hardness for the Dodgson winner problem. This technique involves demonstrating that, given k inputs satisfying χA(x1) ≥ · · · ≥ χA(x2k), where A is an NP-complete set, the function f (x1, ..., x2k) belongs to B if and only if ∥{i | xi ∈A}∥≡1 (mod 2).

In essence, proving easy properties about Dodgson elections allows for a more organized structure that can be harnessed by polynomial-time many-one reductions to establish the problem's high level of computational difficulty under the 𝑶p_2 complexity class. This approach contrasts with NP-hardness proofs, which typically focus on demonstrating hardness without explicitly exploiting organized structures within the problem.


The Gibbard-Satterthwaite Theorem is a fundamental result in voting theory, which establishes impossibility results for voting rules with unrestricted preference domains (m ≥ 3 alternatives). The theorem states that any such voting rule must exhibit at least one of three undesirable properties:

1. Dictatorial: There exists a single fixed voter whose preferred alternative is always chosen, regardless of the other voters' preferences. This means that the decision-making power lies solely with this individual, disregarding the collective will of the group.

2. Imposing: At least one alternative does not win under any possible preference profile (combination of individual rankings). In other words, there is an alternative that can never be selected as the winner, no matter how voters rank their preferences. This property renders the voting rule useless since it excludes certain alternatives from ever being elected.

3. Manipulable (not strategyproof): There exist preference profiles in which at least one voter has an incentive to misreport her true preferences, aiming for a better outcome. In other words, voters can benefit by strategically casting their votes differently than their genuine preferences, thus engaging in manipulation or strategic voting.

This theorem implies that, without restrictions on voter preferences, it is impossible to design a voting rule that satisfies all desirable properties such as non-dictatorship, non-imposition, and strategyproofness simultaneously. As a result, various approaches have been taken to address this challenge, including imposing restrictions on the preference domain or employing computational complexity as a barrier to manipulation, which is the main focus of the chapter.

The undesirability of strategic voting arises from the following reasons:
- It violates the principle of one person, one vote, as manipulating voters effectively alter their weight in the election.
- Manipulation can lead to outcomes that do not reflect the true preferences of the electorate, undermining democratic principles and legitimacy.
- Strategic voting introduces uncertainty and complexity into the voting process, potentially causing confusion among voters and increasing the likelihood of errors or mistakes.

In summary, the Gibbard-Satterthwaite Theorem highlights the inherent difficulties in designing a perfect, manipulation-free voting rule with unrestricted preferences. This motivates the exploration of alternative approaches, such as computational complexity barriers to manipulation, which this chapter delves into.


Table 7.1 presents three types of preference profiles required by different voting rules, specifically focusing on the Borda election system. The table outlines how voters rank candidates (A through F) based on their preferences, assigning points to each ranking position as follows:
- Position 1: 5 points
- Position 2: 4 points
- Position 3: 3 points
- Position 4: 2 points
- Position 5: 1 point
- Position 6: 0 points (last place)

For each voter profile, the table shows the specific ranking of candidates.

1. Voter 1's preference profile:
   - First choice: a
   - Second choice: c
   - Third choice: b
   - Fourth choice: f
   - Fifth choice: e
   - Sixth (last) choice: d

   In this case, voter 1 assigns the highest preference to candidate 'a,' followed by 'c,' then 'b,' and so on. Candidate 'd' is ranked last with 0 points.

2. Voter 2's preference profile:
   - First choice: b
   - Second choice: a
   - Third choice: f
   - Fourth choice: c
   - Fifth choice: e

   Here, voter 2 ranks candidate 'b' first, followed by 'a,' then 'f,' and so on. Candidate 'c' is ranked fourth with 3 points (4 - 1 = 3), 'e' gets 2 points (5 - 3 = 2), and candidate 'a' receives 4 points (5 - 1 = 4).

This table illustrates how each voter's preference profile affects their ranking of candidates in a Borda election system, ultimately influencing the overall score for each candidate. The scores are calculated by summing up the points assigned to each rank across all voters' profiles.


The text discusses various types of control in the context of voting systems, focusing on the manipulation of elections by a chair or authority responsible for organizing the election. The four primary control types are:

1. Constructive Control by Adding/Deleting Candidates/Voters: This involves changing the set of candidates or voters to influence the outcome of the election in favor of a designated candidate (p).
   - **Adding Spoilers (CCAUC)**: The chair can add spoiler candidates from a given set B, hoping that these candidates will weaken p's competitors. A variant with a bound k on the number of added spoilers (CCAC) is also considered.
   - **Deleting Candidates/Voters (CCDC/CCDV)**: The chair can delete up to k candidates or votes to eliminate p's worst rivals, making p more likely to win.

2. Constructive Control by Partitioning Candidates/Voters: This involves dividing the set of candidates or voters into subsets and organizing elections in stages.
   - **Runoff Partition (CCRPC-TE/TP)**: The chair partitions the candidates into two groups, and each group holds a pre-election using the given voting rule. Depending on the tie-handling rule (TE: Ties Eliminate; TP: Ties Promote), either unique or multiple winners proceed to the final stage.
   - **Partition of Voters (CCPV-TE/TP)**: The chair partitions the set of voters into two groups, with each group voting separately in a pre-election. Similar to CCRPC, the tie-handling rule determines which winners proceed to the final stage.

3. Destructive Control: These variants aim to prevent a designated candidate (p) from becoming the unique winner of the election resulting from the chair's control action. They are denoted by replacing the initial "C" with a "D," e.g., DCDC for "destructive control by deleting candidates."

The text also mentions various voting rules and their susceptibility or resistance to these control types, as well as some immunity results. Immunity means that it is impossible for the chair to manipulate the election outcome through a given control type. Resistance indicates that the voting rule remains robust against manipulation attempts, while vulnerability implies that manipulation can be successful under certain conditions.

The table (Table 7.3) in the text summarizes the complexity of control problems for several prominent voting rules, indicating whether each rule is immune, susceptible, vulnerable, or resistant to different control types. Some rules, like Copelandα with 0 < α < 1, SP-AV, fallback, Bucklin, and NRV, are resistant to all constructive control types considered in the study. However, these rules may have vulnerabilities when it comes to destructive control variants.

The text concludes by mentioning that while some natural voting rules with P-time winner determination exist (resistant to most control types), their practicality might be limited due to complexities in understanding and implementing the rules. An alternative is to combine well-known rules artificially to create resistant systems, although these may not be appealing in practice. The study of control in voting systems helps understand potential vulnerabilities and develop strategies to ensure fair elections.


The Consensus-Based Framework for Rationalizing Voting Rules

The consensus-based approach to rationalizing voting rules is rooted in the idea of reaching agreement among voters while minimizing changes to their preferences. This framework involves two main components: a notion of consensus (agreement) and a distance measure between preference profiles. Here's a detailed summary and explanation of the key aspects:

1. Consensus Classes:
   A consensus class is a pair (X, w), where X is a non-empty set of preference profiles over a candidate set A, and w is a mapping that assigns a unique candidate to each profile in X. The assigned candidate is called the consensus choice or winner. The consensus class must be anonymous (identical under voter permutations) and neutral (invariant under candidate renaming).

   Examples of consensus classes include:
   - Strong Unanimity (S): All voters report the same preference order; the consensus choice is the top-ranked candidate.
   - Unanimity (U): Some candidate ranks first for all voters; the consensus choice is that candidate.
   - Majority (M): More than half of the voters rank a common candidate first; the consensus choice is that candidate.
   - Condorcet (C): A proﬁle with a Condorcet winner (a candidate who beats all others in pairwise elections); the consensus choice is the Condorcet winner.
   - Transitivity (T): Majority preferences form a transitive relation; the consensus choice is the Condorcet winner.

2. Distances:
   A distance on preference profiles measures the magnitude of changes between two proﬁles. It satisfies non-negativity, identity of indiscernibles, symmetry, and triangle inequality. For a distance over individual votes (d), a profile distance (d^) can be defined as d^((u1, ..., un), (v1, ..., vn)) = Σ(d(ui, vi)).

   Examples of distances include:
   - Discrete Distance: Measures the number of voters with different top choices.
   - Swap Distance (Kendall Tau, Kemeny): Counts swaps of adjacent candidates required to transform one profile into another.
   - Footrule Distance (Spearman): Sums absolute differences in candidate positions across profiles.
   - Weighted Footrule Distance: Modifies the footrule distance by assigning weights to positions.
   - ℓ∞-Sertel Distance: Measures the maximum position difference between corresponding candidates in two profiles.
   - Edge Reversal (Pseudo)Distance: Counts edge reversals needed in pairwise majority graphs to transform one proﬁle into another.

3. Rationalization of Voting Rules:
   A voting rule can be rationalized by identifying a consensus class and distance that explain its behavior. For instance, Plurality can be explained using Unanimity and Swap Distance, while Borda can be rationalized with Unanimity and Footrule Distance. The Consensus-Based Framework is versatile, allowing for the derivation of properties (e.g., monotonicity) from its components and providing a systematic approach to constructing new voting rules by combining known distances and consensus classes.


The text discusses two main approaches for rationalizing voting rules: consensus-based and probabilistic (Maximum Likelihood Estimator or MLE) methods.

1. Consensus-Based Approach:
This approach, introduced by Elkind et al. (2010a), focuses on the concept of distance rationalizability. It involves defining a pseudo-distance (dins, dswap, ddiscr, dfr, dsert) between preference profiles and using it to determine winners based on consensus classes (C, S, U, M). The Kemeny rule, Dodgson rule, Plurality, Borda rule, Copeland rule, and Maximin are rationalized within this framework.

The main advantage of the consensus-based approach is that it provides a unified way to analyze various voting rules by defining a pseudo-distance on preference profiles. However, as shown by Lerer and Nitzan (1985), any voting rule can be distance rationalizable if we don't impose restrictions on the distance used, which makes this framework too permissive for gaining insights into the properties of specific rules.

2. Probabilistic Approach (Maximum Likelihood Estimator or MLE):
This approach, inspired by Condorcet's probabilistic model and the maximum likelihood estimation principle, represents voting rules as maximum likelihood estimators (MLE) under specific noise models for voters' preferences. The MLE framework distinguishes between two main types of MLE rules:

   a. MLERIV Rules: These rules aim to estimate the most likely ranking given a preference profile and then select winners based on this estimated ranking. They are constructed by associating each voter's preference with a probability distribution and maximizing the likelihood of the observed preference profile under these distributions. Examples include the Kemeny rule, scoring rules, and refinements like MLE∞intr and MLE1intr (Young's interpretations of Condorcet's proposal).

   b. MLEWIV Rules: These rules estimate winners directly by maximizing the likelihood of a candidate being the correct winner given their position in each vote. Neutral MLEWIV rules are simply scoring rules, as shown by Proposition 8.16 (Elkind et al., 2010b).

The probabilistic approach provides insights into the behavior of voting rules under various noise models and allows for a more nuanced understanding of how different factors (e.g., voter accuracy, independence) affect rule outcomes. However, it also faces challenges in characterizing all MLE rules axiomatically due to the complexity introduced by noise models.

In summary, both consensus-based and probabilistic approaches offer valuable perspectives on voting rule rationalization. The consensus-based framework provides a unified way to analyze various rules through distance rationalizability, while the probabilistic MLE approach offers insights into how different noise models affect rule outcomes and winner selection. Understanding these frameworks is essential for studying the properties and behavior of election methods in theoretical and applied contexts.


Sequential Voting in Combinatorial Domains:

Sequential voting is a method for preference aggregation in combinatorial domains, where each variable (or issue) is addressed one at a time. The process involves the following components:

1. **Order of Issues**: An order O over the set of variables X = {X1, ..., Xp} determines the sequence in which voters express their preferences for each variable. Without loss of generality, we can assume O = X1 ▷ X2 ▷ ... ▷ Xp.

2. **Local Voting Rules**: For each issue i (i ≤ p), there is a local voting rule ri that determines the outcome based on voters' preferences for that specific variable Di. These rules can be resolute, where a clear winner is chosen, or irresolute, allowing for ties or multiple outcomes.

3. **Sequential Voting Protocol (SeqO(r1, ..., rp))**: This protocol outlines the steps of sequential voting:
   - For each issue i in order O, ask voters to report their preferences ≻i_t over Di given the current values d1, ..., dt-1 for previously decided variables.
   - Collect all reported preferences and form a profile Pt = (<≻1_t, ..., ≻n_t>).
   - Apply the local voting rule ri to Pt, obtaining decision dt = ri(Pt).
   - Communicate dt back to voters.

4. **Voter's Behavior**: In each step t, voters provide their preferences for issue Xt based on the current state of the other variables (d1, ..., dt-1). This introduces complexity because a preference for one issue may depend on the results of previous issues. The challenge lies in defining marginal or local preferences unambiguously when they depend on undecided variables.

5. **O-Legality**: To ensure voters can report their preferences without ambiguity, the concept of O-legality is introduced. A preference relation ≻ over A is O-legal if, given the order O = X1 ▷ X2 ▷ ... ▷ Xp, voters' preferences for each variable Xt only depend on the current state (d1, ..., dt-1) and not on future variables (Xt+1, ..., Xp).

By following this sequential voting protocol, the method aims to balance expressivity and cost in preference aggregation over combinatorial domains. It allows voters to provide their preferences issue by issue while maintaining a low communication burden. However, designing appropriate local voting rules that handle dependency between issues is crucial for its successful implementation.


This chapter, authored by Craig Boutilier and Jeffrey S. Rosenschein, discusses incomplete information and communication requirements in voting, focusing on methods for determining winners or making decisions with partial preference knowledge. The key theme is the use of partial preferences to reduce communication and informational burdens without compromising decision quality.

10.2 Models of Partial Preferences:
- **Basic Notation**: Alternatives A = {a1, ..., am} and voters N = {1, ..., n}. Each voter i has a preference order ≻i over A.
- **Partial Votes/Profiles**: Partial information πi (partial ordering) about voter i's preferences, with completions C(πi) being all complete votes extending πi.
- **Probabilistic Preference Models**: Distributions over voter preferences, such as Impartial Culture (IC), Impartial Anonymous Culture (IAC), Mallows φ-model, rifle independence model, etc., used for probabilistic analysis of voting outcomes with incomplete information.

10.3 Solution Concepts with Partial Preferences:
- **Possible and Necessary Winners**:
  - A is a possible winner under  if there's an R ∈C() such that f(R) = a.
  - A is a necessary winner under  if f(R) = a for all R ∈C().
  - Computation complexity varies with voting rule and number of alternatives; generally NP-complete or coNP-complete for various rules like STV, Condorcet, Borda, Copeland, maximin, Bucklin, etc.

10.3.1 Possible Winners:
- Sufficient partial information to determine the winner (or rule out alternatives).
- Related to coalitional manipulation problem; if a is not a possible winner under , it cannot be manipulated by a constructive coalition.

10.3.2 Minimax Regret:
- Measure the difference between the score of alternative a and the optimal score in the worst case (given any completion).
- PMR(a, a', ) = maxR∈C() [s(a', R) - s(a, R)]; MMR() = mina∈A MR(a, ), where MR(a, ) is the maximized regret under partial profile .
- Minimax optimal alternative may not be a possible winner but provides a general method for selecting winners in incomplete information scenarios.

10.3.3 Probabilistic Solution Concepts:
- Utilize probabilistic preference models to assess voting outcomes' likelihood, considering phenomena like Condorcet cycles, manipulation opportunities, and expected loss under social welfare measures.

10.4 Communication and Query Complexity:
- Formal models quantifying the information needed in worst cases to determine winners using specific voting rules.
- Two varieties of models:
  1. Communication complexity (Yao, 1979; Kushilevitz and Nisan, 1996): Measures number of bits communicated between voters and mechanism.
     * Upper bounds provided by deterministic protocols (e.g., O(nm log m) for rank-based rules), lower bounds using fooling sets technique.
  2. Query complexity: Measures the number of queries voters need to answer, sensitive to query form due to varying information carried by different queries.

This chapter provides an overview of techniques and models for dealing with incomplete preference information in voting scenarios, enabling more efficient group decision-making while maintaining adequate decision quality.


11.2 What Is a Resource Allocation Problem?

An allocation problem in the context of fair resource distribution involves several key components that together define the economic environment within which the allocation takes place. Here's a detailed explanation of these components:

1. **Set of Agents**: This refers to the individuals or entities participating in the allocation process. These can be individual people, government agencies, firms, or other artificial agents representing real-world entities. The number and characteristics of these agents play a significant role in shaping the allocation problem's complexity and constraints.

2. **Resource Data**: This component pertains to unproduced endowments of goods that are available for distribution. These goods can be either consumed directly or, when production opportunities are defined, used as inputs in the production process. The nature, quantity, and characteristics of these resources significantly influence the allocation problem's structure and potential solutions.

3. **Production Opportunities**: In some cases, these unproduced endowments can be utilized not only for direct consumption but also as inputs in the creation of new goods or services through production processes. The availability and nature of these production opportunities further expand the problem's complexity by introducing interdependencies among resources and potential outputs.

4. **Preferences**: Each agent within the economy has a set of preferences over the possible bundles of resources they might receive as an allocation. These preferences can be complete, transitive, and reflexive, or they may exhibit more complex structures like satiation or diminishing marginal rates of substitution. Understanding these preferences is crucial for assessing the fairness and efficiency of potential allocations.

5. **Feasibility Constraints**: These are the rules governing which resource bundles can actually be produced or allocated within the economy. They may include physical constraints (e.g., limited availability of certain resources), technological constraints (e.g., production processes requiring specific inputs in particular ratios), or social norms and regulations (e.g., prohibitions on allocating certain resources to specific agents).

6. **Fairness Criteria**: These are the principles that guide the allocation process, ensuring that it aligns with notions of fairness. Examples include Pareto efficiency (no agent can be made better off without making someone else worse off), envy-freeness (no agent prefers another's bundle more than their own), proportionality (agents receive a share proportional to some measure of their entitlement or need), and various solidarity requirements (e.g., equal opportunities, egalitarian-equivalence).

By understanding these components—agents, resources, production opportunities, preferences, feasibility constraints, and fairness criteria—one can specify a resource allocation problem concretely within an economic context. This specification allows for the application of axiomatic approaches to fair allocation, as well as computational methods for addressing various types of allocation problems.


This chapter of "The Theory of Fair Allocation" focuses on the fair division of indivisible goods among agents with different preferences. In this context, a resource is a set O = {o1, ..., op} of objects (also referred to as goods or items) that must be allocated whole and cannot be divided or broken down into smaller parts. This assumption applies to various real-world scenarios such as dividing physical objects like houses or cars in divorce settlements, allocating courses or Earth observation images among students, etc.

The chapter discusses several fair division problems under this context:

1. Classical fair division problems: A social endowment of ℓ indivisible goods needs to be distributed among a group N of agents, each equipped with a preference relation ≿i over the commodity space R^ℓ. Preferences satisfy classical assumptions like continuity, monotonicity, and convexity.

2. Fair division problems with single-peaked preferences (Sprumont, 1991): A social endowment of a single commodity must be fully distributed among a group N of agents with single-peaked preferences. These preferences have a peak amount, p(≿i), up to which increasing consumption increases welfare, and beyond that level, further increase decreases welfare.

3. Claims problems (O'Neill, 1982): A social endowment of a single good must be distributed among a group N of agents with incompatible claims ci on it, where the total claim exceeds the endowment. Agents have monotonic preferences. Typical applications include bankruptcy and taxation problems.

4. Partitioning nonhomogeneous continua: A social endowment consisting of an indivisible and nonhomogeneous continuum must be partitioned among a group N of agents, with each agent i having preferences ≿i over its measurable subsets. The base model assumes monotonicity of preferences with respect to set inclusion.

5. Object allocation problems: A social endowment O of indivisible goods has to be assigned to a group N of agents, where each agent i ∈N can consume only one object and has preferences ≿i defined over O. This is the base case for object allocation problems, which may involve assigning offices, tasks, or other resources among individuals.

6. Objects-and-money allocation problems: An enriched version of object allocation problems where each agent i ∈N can consume some amount of money and one object. Consumptions of money may be unrestricted in sign, or restricted to a lower bound (e.g., zero). Each agent has preferences defined over R × O, perhaps [a, ∞[×O for some a ∈R.

7. Priority-augmented object allocation problems (Balinski and S¨onmez, 1999; Abdulkadiro˘glu and S¨onmez, 2003): An extension of the previous model where each object a ∈O is equipped with a priority order πa over its possible recipients. Applications include school choice problems, where objects are seats in schools, and priorities depend on factors like sibling attendance, walk-zone proximity, waiting list length, and academic records.

8. Matching agents to each other: A partition of the agent set into two sets is made, with preferences over the agents in the component of the partition to which they do not belong. The objective is to form pairs containing one agent from each set. This can involve various models like strict or indifferent preferences, one-to-one or several-to-one matching, and even distribution of an infinitely divisible good (e.g., money) among paired agents.

Throughout the chapter, the authors emphasize the importance of understanding fair allocation practices in real-world scenarios and examining their desirable features as well as potential shortcomings. They also introduce various axiomatic principles and concepts to define fair solutions for these problems, such as no-envy, egalitarian-equivalence, resource monotonicity, population monotonicity, welfare dominance under preference replacement, consistency, and model-specific requirements like duality in claims problems.

In the following chapters, this theoretical framework is applied to solve specific fair division problems of indivisible goods using algorithmic methods and axiomatic approaches, providing a comprehensive treatment of the field.


The Adjusted Winner Procedure is an algorithm designed for fair division problems involving two agents with additive utility functions. It operates in two phases: the "winning phase" and the "adjusting phase."

1. Winning Phase:
   - The items are allocated to each agent based on their individual valuations, i.e., an item is given to the agent who values it the most.
   - This results in a provisional allocation where potentially one agent may have higher utility than the other.

2. Adjusting Phase:
   - If there's a disparity in utilities after the winning phase, the "adjusting" process begins. In this phase, items are transferred from the richer agent (richest) to the poorer agent (poorest). The transfer happens according to an increasing order of the ratio of their valuations for each item.
   - The algorithm continues transferring items until either both agents have equal utility or the richest agent becomes the poorest after a transfer.

3. Equitable Allocation:
   - Once the disparity cannot be reduced further, the last transferred item 'g' needs to be split between the two agents. The fraction of this item allocated to the poorer agent is calculated as follows:

   \[
   \text{Fraction for Poor Agent} = \frac{\text{Poor's Valuation of g} + \text{Poor's Valuation of Remaining Items}}{\text{Rich's Valuation of g} + \text{Poor's Valuation of g}}
   \]

   - The rich agent receives the rest, ensuring that both agents end up with equal utility.

Properties of the Adjusted Winner Procedure:
- Equitable (Both agents have the same utility at the end).
- Envy-free (No agent envies another's allocation).
- Pareto-optimal (There is no other allocation where one agent could gain without causing harm to another).

This procedure provides a simple and efficient way for two agents to divide items while ensuring fairness, even when the agents have different valuations for the objects. It demonstrates that fair division can be achieved through iterative processes involving direct comparisons of individual utilities and adjustments based on those comparisons.


The text discusses various aspects of cake cutting algorithms, which are used to divide a heterogeneous divisible resource among multiple agents with different preferences. Here's a summary and explanation of the key points:

1. **Model**: The problem involves n agents and a cake represented by the interval [0, 1]. Each agent has a valuation function Vi that assigns a value to any subinterval I ⊆[0, 1]. These functions satisfy normalization (Vi(0, 1) = 1), divisibility, nonnegativity, and additivity.

2. **Fairness properties**: The fairness properties considered are proportionality (each agent values their piece at least 1/n), envy-freeness (each agent prefers their own piece to any other piece), and equitability (every two agents value their pieces equally).

3. **Classic cake cutting algorithms**:

   - **Proportionality for n = 2: Cut and Choose** - Agent 1 cuts the cake into two equal-value pieces, and Agent 2 chooses their preferred piece. This results in a proportional allocation (and also envy-free).
   
   - **Proportionality for any n: Dubins-Spanier and Even-Paz** - These algorithms guarantee proportionality for any number of agents. The Dubins-Spanier algorithm uses a continuously moving knife, while the Even-Paz algorithm is more computationally efficient, requiring O(n log n) queries in the Robertson-Webb model.
   
   - **Envy-Freeness for n = 3: Selfridge-Conway** - This algorithm provides an envy-free allocation for three agents using a series of cuts and selections by each agent.

4. **Complexity of Cake Cutting**: The computational complexity of cake cutting is analyzed in terms of the number of queries required to find a fair allocation. The Robertson-Webb model supports two types of queries: evali(x, y) (asking agent i to evaluate interval [x, y]) and cuti(x, α) (asking agent i to cut a piece of cake worth α starting at x).

   - **Lower Bound for Proportional Cake Cutting**: Any proportional cake-cutting algorithm requires Ω(n log n) queries in the Robertson-Webb model (Edmonds and Pruhs, 2006b).
   
   - **The Complexity of Envy-Free Cake Cutting**: While there are finite envy-free algorithms for three agents (Selfridge-Conway), extending these to any number of agents is challenging. The Brams-Taylor algorithm has unbounded running time, and current lower bounds show that envy-free cake cutting requires Ω(n^2) queries in the Robertson-Webb model (Procaccia, 2009).

5. **Optimal Cake Cutting**: The text also covers optimal fair allocations, focusing on piecewise constant valuation functions with known parameters. Algorithms can compute envy-free and equitable allocations efficiently by partitioning intervals between marks reported by agents. However, when contiguous allocations are required, the problem becomes NP-hard to approximate within a factor of Ω(√n) (Bei et al., 2012).

In summary, cake cutting algorithms deal with dividing a heterogeneous resource among multiple agents with varying preferences. Various fairness properties and complexity analyses are discussed, along with specific algorithms for different numbers of agents. The challenge lies in balancing fairness and computational efficiency, especially when contiguous allocations are required.


The text provided discusses two types of matching problems under preferences: bipartite with two-sided preferences (Section 14.2) and one-sided preferences (Section 14.3).

**14.2 Two-Sided Preferences:**

This section focuses on the Hospitals/Residents problem (hr), which is a bipartite matching problem where both hospitals and residents have preferences over each other. The problem involves assigning residents to hospitals while respecting capacity constraints of the hospitals.

1. **Introduction and Preliminary Deﬁnitions:**
   - An instance I of hr consists of a set R of residents, a set H of hospitals, acceptable pairs E ⊆ R × H, and capacities cj for each hospital hj ∈ H.
   - Each resident ri ∈ R has a preference list ranking the acceptable hospitals A(ri), and each hospital hj ∈ H has a preference list ranking the acceptable residents A(hj).

2. **Stable Matchings:**
   - A matching M is stable if there's no blocking pair (ri, hj) that would improve both resident ri and hospital hj by forming a partnership.
   - Stable matchings ensure that no resident-hospital pair can mutually benefit from leaving their current assignment to form a new partnership.

3. **Gale-Shapley Algorithms:**
   - The Resident-Oriented Gale-Shapley (RGS) algorithm involves residents applying to hospitals in order of preference, with hospitals accepting the best available candidates up to their capacity.
   - The Hospital-Oriented Gale-Shapley (HGS) algorithm has hospitals offering posts to residents based on their preferences and capacities.

4. **Classical Results:**
   - Every hr instance admits at least one stable matching, as proven by Gale and Shapley (1962).
   - The RGS and HGS algorithms construct the unique resident-optimal and hospital-optimal stable matchings, respectively, in O(m) time.

5. **Strategic Results:**
   - The Impossibility Theorem states that no mechanism can be both stable and strategyproof (i.e., truthful reporting is a weakly dominant strategy for all agents).
   - The RGS mechanism is strategyproof for residents, meaning that truthfully reporting preferences is a weakly dominant strategy for residents, while hospitals have no strategyproof mechanism.

**14.3 One-Sided Preferences:**

This section discusses the House Allocation problem (ha) and its extension to Housing Markets (hm), where only applicants have preferences over houses/items.

1. **Introduction and Preliminary Deﬁnitions:**
   - An instance I of ha consists of a set A of applicants, a set H of houses, and acceptable pairs E ⊆ A × H.
   - Each applicant ai ∈ A has a preference list ranking the acceptable houses A(ai).

2. **Pareto Optimality:**
   - A matching M is Pareto optimal if no applicant can improve their assignment without making another applicant worse off.
   - The Serial Dictatorship (SD) algorithm finds all Pareto optimal matchings, assigning each applicant to their most-preferred available house in a predetermined order.

3. **Classical Results:**
   - All Pareto optimal matchings can be constructed using the SD algorithm, which runs in O(m) time.
   - The Random Serial Dictatorship (RSD) mechanism is a random version of SD that produces a probability distribution over matchings, and determining positive probabilities is #P-complete.

4. **Housing Markets:**
   - An instance I of hm includes an initial endowment M0, where each applicant has one house initially.
   - The Top Trading Cycles (TTC) algorithm constructs a weak core matching that can be implemented as the Core Mechanism for assigning houses.

5. **Strategic Results:**
   - No mechanism can be both stable and strategyproof for housing markets with strict preferences, as shown by Roth's Impossibility Theorem.


The Top Covering Algorithm (TCA) is designed to compute a core stable partition for hedonic games that satisfy top responsiveness, as discussed in Sections 15.3.4 and 15.4.3 of the text. Here's a detailed explanation of the algorithm:

1. **Initialization**: The TCA begins with an initial partition π that is empty (π = ∅), and a set R1 containing all agents N.

2. **Iterative process**: For each round k from 1 to |N| (the total number of agents):

   a. Select agent i from the current set Rk such that the size of the connected component CC(i, Rk) is less than or equal to the size of the connected component CC(j, Rk) for every other agent j in Rk. This selection ensures that the chosen coalition will be smaller or equal in size to at least one other possible coalition.

   b. Create a new coalition Sk by taking the connected component CC(i, Rk). Add this coalition to the current partition π: π ←π ∪{Sk}.

   c. Update the set of remaining agents Rk+1 by removing the newly formed coalition Sk from Rk: Rk+1 ←Rk \ Sk.

3. **Termination**: If there are no remaining agents (i.e., Rk+1 = ∅), then the algorithm has found a core stable partition and returns π. Otherwise, continue to the next round.

4. **Final output**: Once all agents have been assigned to coalitions, the TCA returns the partition π as the core stable solution.

The key idea behind the Top Covering Algorithm is to iteratively form coalitions based on connected components in a graph where vertices represent agents and edges indicate that two agents are neighbors (i.e., they prefer each other's company). By always selecting an agent with the smallest connected component, TCA ensures that the resulting partition will be core stable for games satisfying top responsiveness.

The algorithm's correctness and efficiency are supported by the following facts:

- For top responsive preferences, TCA is strategyproof, meaning no coalition of agents can benefit from misrepresenting their preferences (Aziz and Brandl, 2012).
- In addition to core stability, if the preference profile satisfies certain natural constraints in addition to top responsiveness, TCA returns a partition that satisfies stronger notions of stability (Aziz and Brandl, 2012).

The Top Covering Algorithm provides an efficient method for computing core stable partitions in games with top responsive preferences. Its performance is closely tied to the structure of connected components in the underlying preference graph, making it well-suited for analyzing hedonic games with specific preference structures like B-hedonic games.


Weighted Voting Games are a class of cooperative games used to model decision-making situations where a binary yes/no decision is made by a set of voters, each with an assigned weight. The game's value function determines if a coalition (subset) of voters wins based on whether the sum of their weights meets or exceeds a given quota. These games have applications in various real-world scenarios, such as legislative bodies and resource allocation problems.

The core, Shapley value, and Banzhaf index are key solution concepts for cooperative games:

1. The Core: A set of imputations (payoff distributions) where no coalition can improve its payoff by deviating from the imputation.
2. Shapley Value: A method to distribute the total value among players based on their marginal contributions, measured as the average over all possible orderings of players.
3. Banzhaf Index: A simpler measure of voting power that calculates the probability a player can turn a losing coalition into a winning one by joining it.

The computational properties of weighted voting games reveal some interesting results:

1. Veto Player problem: Identifying whether a given player is a veto player (whose presence is necessary for any winning coalition) can be solved in polynomial time.
2. Dummy Player problem: Determining if a player is a dummy (never contributes to a winning coalition) is coNP-complete, meaning it's computationally hard unless P= NP. However, pseudopolynomial algorithms exist for small weights.
3. Shapley Value and Banzhaf Index: Computing these indices in weighted voting games is #P-complete and also pseudopolynomial when weights are large or small, respectively.

An important observation is that the weight assigned to a voter doesn't necessarily reflect their actual power (voting influence) within the game. There can be instances where players with equal power have significantly different weights and vice versa. This phenomenon, known as "weighted voting doesn't work," has been observed in various decision-making bodies like legislative systems.

Additionally, changing the quota or adding/dividing a player's weight can affect voting power unexpectedly:

1. Paradox of New Members: Adding players to a game may increase existing players' power instead of reducing it.
2. Paradox of Size: Splitting an existing player into multiple identities with equal weights doesn't necessarily preserve their original power distribution. Instead, their combined power might increase or decrease significantly, depending on the structure of the game.

These counterintuitive behaviors highlight the complex relationship between voter weight and voting power in weighted voting games.


Judgment Aggregation (JA) is a formal framework used to model collective decision-making processes where individual judgments about propositional formulas are aggregated into a single, coherent set of judgments. This concept was introduced by Christian List and Philip Pettit in 2002 as a means to understand the challenges and potential solutions for aggregating consistent judgments from multiple individuals, particularly when dealing with interdependent propositions.

The basic setup of JA involves:

1. An agenda () - a finite, nonempty subset of propositional formulas that is closed under complementation and does not contain any doubly negated formulas. The agenda represents the set of statements to be judged by the group of individuals or "judges."
2. A judgment set (J) - a subset of the agenda consisting of formulas accepted by a judge, along with their complements if they are rejected. Judgment sets are complete and consistent, meaning that for every formula in the agenda, either it or its negation is included.
3. A profile - an n-tuple (n > 1) of judgment sets, one for each judge. The profile represents the individual assessments of the judges regarding the formulas in the agenda.
4. A judgment aggregation rule (aggregator) - a function that maps every profile into a single collective judgment set. This rule aggregates the individual judgments to produce a coherent, possibly complete and consistent, set of judgments for the group.

Some crucial properties or axioms of an aggregator include:

- Unanimity: If all judges accept a formula ϕ, then the collective decision also accepts it (ϕ ∈ f(J) if ϕ ∈ Ji for all i).
- Anonymity: The collective decision is indifferent to permutations of individual judges. In other words, the outcome depends only on the pattern of acceptances and not on which judge made a particular assessment (f(J) = f(Jπ), where π is any permutation of N).
- Neutrality/Independence: The collective decision treats all formulas symmetrically; if two formulas ϕ and ψ are accepted by the same set of judges, then their collective acceptance depends only on that pattern (ϕ ∈ f(J) ⇔ ψ ∈ f(J) if N J
ϕ = N J
ψ).
- Monotonicity: If a formula is accepted by an additional judge, it should still be accepted in the collective decision.

The List and Pettit impossibility theorem (2002) demonstrates that no aggregator satisfying Anonymity, Neutrality/Independence, and Independence can guarantee both Completeness and Consistency simultaneously for any nontrivial agenda containing p, q, and ¬(p ∧q). This highlights the fundamental challenge in JA – finding a reliable aggregation method that ensures consistent collective decisions while preserving essential properties like Anonymity, Neutrality/Independence, and Monotonicity.

To address this impossibility, researchers have explored various strategies:

1. Weakening or relaxing the axioms to find aggregators that provide more lenient guarantees on consistency, completeness, or other desirable properties.
2. Restricting the class of agendas for which consistent aggregation is possible by imposing additional conditions or assumptions.
3. Introducing value restrictions on individual judgments, such as requiring that no minimally inconsistent set contains two formulas accepted by the same judge.
4. Employing alternative distance-based aggregation methods like the Kemeny rule or Slater rule, which achieve consistency at the cost of increased computational complexity.
5. Utilizing premise-based rules that partition the agenda into premises and conclusions and ensure completeness and consistency under specific conditions.

These strategies help researchers better understand the possibilities and limitations of judgment aggregation in various contexts, from legal decision-making to artificial intelligence systems employing logical reasoning for autonomous agents.


The chapter discusses the application of the axiomatic approach to Internet-based multiagent systems, focusing on three illustrative studies: graph ranking systems (PageRank algorithm), trust-based recommendation systems, and multilevel marketing (affiliate marketing).

1. **Graph Ranking Systems (PageRank Algorithm):** The classical theory of social choice deals with aggregating individual rankings into a global or social ranking. In the Internet setting, agents and alternatives coincide, leading to new axioms for graph ranking systems. PageRank is a well-known algorithm used by search engines like Google to rank web pages based on their importance, determined by the structure of the link graph.

   The authors present an axiomatic characterization of PageRank, which involves defining a set of axioms that capture desired properties of a ranking system for graphs. These axioms include:

   - **Isomorphism:** The ranking procedure should be independent of vertex naming.
   - **Self Edge:** If a vertex ranks another vertex higher than itself without self-looping, adding a self-loop shouldn't decrease its rank relative to other vertices.
   - **Vote by Committee:** Replacing direct links with indirect ones through committees should maintain the relative ranking.
   - **Collapsing:** Collapsing identical vertices that have disjoint predecessors and successors should preserve the relative ranking of other vertices.
   - **Proxy:** Distributing importance from multiple sources through a proxy shouldn't change the relative ranking of other vertices.

   Proposition 18.8 states that PageRank satisfies all these axioms, ensuring their soundness.

2. **Trust-Based Recommendation Systems:** These systems provide personalized rankings or recommendations to each agent based on trust relationships and local rankings, instead of aggregating preferences into a global ranking. Challenges include determining how to aggregate both measures of trust and preference optimally. The axiomatic approach can help characterize the desired properties of such systems for design purposes.

3. **Multilevel Marketing (Affiliate Marketing):** In this form of marketing, products are sold through referrals generated by previous customers, with credit offered in exchange for successful referrals. The axiomatic approach can be applied to study the fair distribution of credit among participants, addressing issues like how to balance the contributions of direct and indirect referrers.

The chapter emphasizes that while the "big data" approach is popular in designing Internet systems due to available user-generated information, it doesn't provide a comprehensive tool for exploring the vast design space. The axiomatic approach, combining formal speciﬁcation of clear system requirements with implementation, offers an alternative method for multiagent systems design on the Internet, augmented with conceptual depth and evidence of its effectiveness in various illustrative studies.


The text discusses knockout tournaments, a type of competition where each match results in a single winner, and the loser is eliminated. These tournaments are represented by binary trees, with players as leaves and winners at internal nodes. The winner is determined recursively based on matches between child nodes' winners.

Properties of knockout tournaments include:
1. They are Condorcet-consistent, meaning if a Condorcet winner (a candidate preferred by a majority over all others) exists, they will win.
2. Knockout tournaments are Smith-consistent; if candidates can be split into two sets where each set's members beat the other set's members by majority, then a candidate from the first set will always win regardless of seeding.
3. They do not satisfy the reinforcement or participation criteria (reinforcement ensures that if two groups elect the same winner using subsets of voters, the entire group should also elect that winner; participation states no voter has an incentive to abstain).
4. Knockout tournaments are not neutral, as renaming candidates can change the winner due to seeding dependence on candidate names.
5. They do not guarantee Pareto-optimality when there are five or more candidates, meaning there exists a voter profile where an alternative candidate is unanimously preferred over the elected one.

Despite these drawbacks, knockout tournaments remain popular due to their simplicity and efficiency (requiring no more matches than players). They also discourage strategic losses since a single loss eliminates a player.

The text also mentions an O(m^2) time algorithm for calculating the probability of winning a knockout tournament given a probability matrix P, where P[i, j] is the probability that i beats j (0 ≤ P[i, j] = 1 - P[j, i] ≤ 1). This algorithm uses dynamic programming to compute q(j, v), the probability that player j reaches tree node v of T. The base case is q(j, S(j)) = 1, and for internal nodes with children r and l, q(j, v) = q(j, r) * ∏ (1 - P[l, i]) if j is seeded at a descendant of r.


The provided text discusses various aspects of agenda control in knockout tournaments, focusing on different types of manipulation and their complexity. Here's a detailed summary:

1. **General Agenda Control Problem**: The problem involves determining whether there exists a knockout tournament (T, S) such that a given candidate i wins with probability at least p. This is divided into four variants based on the control over tree T and seeding S:
   - Full Agenda Control: Chairman can modify both T and S.
   - P-Agenda Control: Chairman can modify only S, with probabilistic input (a probability matrix P).
   - Deterministic Agenda Control: Similar to P-Agenda Control but with deterministic input (tournament graph G).

2. **Full Agenda Control**: In a deterministic setting, there's a simple algorithm running in O(m) time that checks if the input candidate i can reach all other nodes in the tournament graph G. This is based on two claims:
   - Claim 19.1: If i wins, then every node j is reachable from i in G.
   - Claim 19.2: If all nodes are reachable from i, then there exists a tree T and seeding S such that i wins.

3. **P-Agenda Control**: For the probabilistic case, Vu et al. (2009a) showed that unbalanced tournaments with i only playing in the last round are optimal, but no polynomial-time algorithm is known to find this solution.

4. **Extensions and Further Reading**:
   - Real-world experiments by Russell (2010) show that agenda control for generated tournaments can be easily solved under various restrictions.
   - Vu et al. (2009b) analyze the performance of heuristics in solving P-Agenda Control for both balanced and caterpillar voting trees using real data from tennis and basketball.
   - Horen and Riezman (1985) study optimal seeding to maximize the probability that the best player wins or improve the expected quality of the winner in balanced knockout tournaments for m = 4 and m = 8.
   - Kr¨akel (2014) investigates game-theoretic scenarios where players exert effort, aiming to maximize total expected effort or probability that a strong player wins in four-player knockout tournaments.

5. **Further Topics**: Other extensions and further readings include:
   - Complexity of multiple elimination tournaments by Stanton and Vassilevska Williams (2013).
   - Analysis of complex competition formats like the PGA TOUR's FedExCup by Connolly and Rendleman (2011) and the seeding and selection efficiency in soccer world cup and Olympics by Pauly (2014).

The text concludes with acknowledgments to reviewers, editors, and contributors who helped shape the content.


The provided text is a list of references related to various topics in the field of social choice theory, preference aggregation, voting systems, and coalition formation games. Here's a detailed summary and explanation of some key concepts and papers mentioned:

1. **Preference Aggregation and Voting Systems**: Many papers focus on aggregating individual preferences into a collective decision, often through voting systems. Some notable works include:
   - *Dagan (1996)*: A note on Thomson's characterizations of the uniform rule.
   - *Cohen (1951)*: Introducing the concept of a social welfare function.
   - *Coppersmith, Fleischer, and Rudra (2006, 2010)*: Algorithms for ordering weighted tournaments based on the number of wins.
   - *Conitzer, Sandholm, and Tengin (2007)*: Complexity of elections with few candidates hard to manipulate.

2. **Condorcet Method**: This is a voting system where each voter ranks candidates, and the Condorcet winner is the candidate who would beat every other candidate in pairwise comparisons. Some papers discuss its properties and variants:
   - *Debreu (1954)*: Representation of a preference ordering by a numerical function.
   - *Duggan and Le Breton (2001)*: Mixed re finements of Shapley's saddles and weak tournaments.

3. **Coalition Formation Games**: These games model situations where agents form coalitions to achieve their goals. Papers in this area include:
   - *Drissi-Bakhkhat and Truchon (2004)*: Maximum likelihood approach to vote aggregation with variable probabilities.
   - *Dubins and Freedman (1981)*: Machiavelli and the Gale-Shapley algorithm.

4. **Manipulation and Strategyproofness**: Many papers investigate how agents can manipulate voting systems to achieve their preferred outcomes, or the conditions under which a system is resistant to manipulation:
   - *Dubey and Shapley (1979)*: Mathematical properties of the Banzhaf power index.
   - *Duggan and Le Breton (2001)*: Strategic candidacy and voting procedures.
   - *Conitzer, Sandholm, and Tengin (2007)*: Complexity of elections with few candidates hard to manipulate.

5. **Fair Division**: This area focuses on dividing resources among agents while ensuring fairness. Papers include:
   - *Dubins and Spanier (1961)*: How to cut a cake fairly.
   - *Deng, Papadimitriou, and Safra (2003)*: On the complexity of equilibria.

6. **Multi-issue Domains**: Some papers explore voting systems in multi-issue domains, where each agent has preferences over multiple issues:
   - *Deegan, Katsirelos, Narodytska, and Walsh (2014)*: Complexity of and algorithms for the manipulation of Borda, Nanson's, and Baldwin's voting rules.

7. **Judgment Aggregation**: This is a field studying how to combine individual judgments into collective judgments while satisfying certain properties like consistency or independence:
   - *Dietrich (2007)*: A generalised model of judgment aggregation.
   - *Dietrich and List (2004, 2007a, 2007b, 2010)*: Various works on models of judgment aggregation, consistency, and stability.

8. **Parameterized Complexity**: Some papers apply parameterized complexity theory to voting-related problems:
   - *Downey and Fellows (1999, 2013)*: Parameterized complexity.

These references provide a comprehensive overview of the state-of-the-art in social choice theory, preference aggregation, voting systems, coalition formation games, fair division, multi-issue domains, judgment aggregation, and related topics.


The provided text is a list of references related to the field of social choice theory, voting systems, and fair division of resources. Here's a summary of some key topics, authors, and papers mentioned:

1. **Voting Systems**: The text covers various voting rules such as Dodgson's rule, Copeland's rule, Borda count, Bucklin, Approval voting, Schulze method, Range voting, Normalized Range Voting (NRV), and Majority Judgment.

2. **Manipulation and Control**: Many references focus on the complexity of manipulation or control in different voting systems. This includes works on bribery, strategic behavior, and convergence to equilibria in plurality voting.

3. **Approximability and Inapproximability**: Some papers discuss the approximability and inapproximability results for social welfare optimization in multiagent resource allocation, such as stable matching problems.

4. **Fair Division**: The text also covers fair division problems, including the cake-cutting protocol, probabilistic marriage problems, and probabilistic fair division with ordinal preferences.

5. **Judgment Aggregation**: Several references deal with judgment aggregation, a method of combining individual judgments or opinions into collective ones. Topics include distance-based models, classics of social choice, and axioms for cooperative decision making.

6. **Game Theory**: The text includes papers on game theory applications to voting systems, such as Nash equilibrium, strategic behavior, and mechanism design.

7. **Complexity Theory**: Many references touch upon complexity theory, including fixed-parameter algorithms, approximability, and inapproximability results for social welfare optimization problems.

8. **Specific Authors and Papers**: Some notable authors mentioned include Arrow, Gibbard, Satterthwaite, Nanson, Moulin, Myerson, Merrill, McKelvey & Niemi, Meir et al., Mossel et al., Moulin & Thomson, Munera et al., and others. Specific papers include:

   - Arrow (1950): "A Modification of the Condorcet Criterion"
   - Gibbard (1973): "Manipulating Votes"
   - Satterthwaite (1975): "Strategy-proofness and the Independence of Irrelevant Alternatives Axiom"
   - Moulin (1988a): "Axioms of Cooperative Decision Making"
   - Moulin (2004): "Fair Division and Collective Welfare"
   - Meir et al. (2013): "Algorithms for Strategyproof Classiﬁcation"
   - Narodytska & Walsh (2013): "Manipulating Two Stage Voting Rules"
   - Obraztsova et al. (2015a): "Analysis of Equilibria in Iterative Voting Schemes"

This list provides a comprehensive overview of the literature on voting systems, fair division, and related topics in social choice theory and game theory.


The text provided is a list of terms, concepts, and theorems related to social choice theory, cooperative game theory, and fair division. Here's a detailed summary and explanation of some key topics:

1. **Voting Rules**:
   - **Approval Voting Rule (AV)**: Each voter approves or disapproves each candidate. The candidate with the most approvals wins.
   - **Borda Count**: Each voter ranks candidates, and points are assigned based on rankings (e.g., 1st place gets n-1 points, where n is the number of candidates). The candidate with the highest total score wins.
   - **Condorcet Voting Rule**: A candidate is a Condorcet winner if they beat every other candidate in pairwise comparisons. If no Condorcet winner exists, various methods (e.g., Copeland, Ranked Pairs) are used to determine the winner.
   - **Copeland Voting Rule**: A candidate beats another if more voters rank them higher than the other candidate. The candidate with the most "beats" wins.

2. **Manipulation**:
   - **Bribery**: Influencing voters or candidates to change their votes or preferences, respectively. This can occur in various forms, such as priced bribery (voters pay money) or swap-bribery (voter A swaps votes with voter B).
   - **Coalitional Manipulation**: A group of voters collectively manipulates the election outcome by coordinating their votes.

3. **Aggregators and Judgment Aggregation**:
   - An aggregator takes individual judgments (e.g., preferences, binary relations) as inputs and produces a single output (e.g., a social welfare ordering).
   - Judgment aggregation is concerned with combining individuals' judgments into collective judgments that satisfy certain axioms (e.g., unanimity, consistency).

4. **Fair Division**:
   - The cake division problem involves dividing a heterogeneous resource among selfish agents with additive or single-peaked preferences.
   - Fairness criteria include proportionality, envy-freeness, and equitability. Algorithms for fair division often involve procedures like the Adjusted Winner, Last Diminisher, and the Envy-Free Cake Cutting algorithm.

5. **Game Theory**:
   - **Cooperative Game**: A game where players form coalitions to maximize their collective payoff. Examples include the core, Shapley value, and cooperative game with non-transferable utility.
   - **Noncooperative Game**: A game where players act independently, aiming to maximize their individual payoffs. Examples include strategic-form games and extensive-form games.

6. **Axioms and Properties**:
   - **Anonymity**: The outcome of an election depends only on voters' preferences, not their identities.
   - **Neutrality/Independence of Irrelevant Alternatives (IIA)**: The ranking of candidates does not depend on the presence or absence of other candidates.
   - **Monotonicity**: If a candidate becomes more preferred by some voters, their rank cannot decrease.
   - **Strategyproofness/Manipulability**: Voters have no incentive to misrepresent their preferences, as doing so would not change the outcome (impossible) or might even harm their preferred candidate (manipulable).

7. **Complexity Theory**:
   - The computational complexity of determining whether a given instance of a problem (e.g., manipulation, control) has a solution is studied using techniques from complexity theory.

8. **Additional Concepts and Results**:
   - **Agenda Control Problem**: Determining the minimum number of candidates or voters that need to be added, deleted, or partitioned to ensure a desired outcome (e.g., a specific candidate wins).
   - **Bribery Problem**: The computational complexity of finding the minimum amount of money needed to bribe voters to achieve a desired outcome.
   - **Dichotomy Result**: A result that distinguishes between easy and hard cases for a given problem (e.g., control, manipulation).
   - **Distance-based Aggregation**: Combining judgments based on their distance or similarity in some metric space.


The document provided is an index of terms related to social choice theory, mechanism design, and game theory. Here's a detailed summary and explanation of some key concepts:

1. **Voting Rules**: These are methods for collective decision-making based on individual preferences. Examples include Plurality Voting (choosing the candidate with the most votes), Instant Runoff Voting (eliminating the least preferred candidate in each round until a majority winner is found), and Approval Voting (voters can approve multiple candidates, with the one approved by the most voters winning).

2. **Tournament Solutions**: These are methods for ranking competitors based on pairwise comparisons. Examples include Schulze Method and Kemeny's Rule. They aim to find a ranking that is transitive (if A beats B and B beats C, then A should beat C) and Condorcet-winning if possible (a candidate who would win against every other in pairwise comparisons).

3. **Stable Matching Problems**: These are problems where stable pairs (or groups) must be formed based on preferences of multiple entities. The most famous example is the Stable Marriage Problem, which finds a stable matching between two equally sized sets of items (like students and colleges), each having their own preference list.

4. **Manipulation**: This refers to strategies used by voters or candidates to influence the outcome of an election in their favor by strategically misrepresenting their preferences. 

5. **Judgment Aggregation**: This involves combining individual judgments (like "guilty" or "not guilty") into a collective judgment while satisfying certain axioms, such as non-dictatorship and independence of irrelevant alternatives. 

6. **Mechanism Design**: This is the study of designing rules for strategic agents to produce desirable outcomes. It often involves creating incentives for truthful revelation of private information.

7. **Core and Welfare**: The core of a cooperative game is a set of allocations where no coalition can improve upon their share by deviating unilaterally or collectively. Welfare refers to the total satisfaction or benefit achieved by all participants. 

8. **Paradoxes**: These are situations where different outcomes seem equally reasonable based on individual preferences, yet a societal choice rule leads to a contradictory or counterintuitive result (e.g., Condorcet's Paradox).

9. **Axioms**: These are rules that a desirable voting rule or mechanism should satisfy. Examples include Individual Rationality (no participant is worse off under the outcome than they would be under any other feasible outcome), Non-Dictatorship (no single individual can always determine the outcome), and Independence of Irrelevant Alternatives (the ranking between two candidates shouldn't change if we add or remove irrelevant alternatives).

10. **Computation Complexity**: This field studies the resources (like time, space) needed to solve computational problems. The document mentions concepts like inapproximability (impossibility results for approximation algorithms), parameterized complexity (studying the complexity with respect to a specific parameter of the input), and PPAD (a complexity class related to finding fixed points).

This index is a rich source of information for researchers, students, or anyone interested in understanding the theoretical foundations and nuances of social choice theory, mechanism design, and game theory.


### fxtbook

This chapter, "Bit Wizardry," focuses on low-level algorithms related to binary words (i.e., sequences of bits). It introduces various functions for manipulating these binary representations, which are essential in many computational tasks. Here's a detailed explanation of the key concepts discussed in this chapter:

1.1 Trivia

   1.1.1 Little Endian vs Big Endian
   The order in which bytes within an integer are stored in memory can differ between machine architectures. In little-endian systems, the least significant byte comes first, whereas big-endian systems store the most significant byte first. For example, the hexadecimal value 0x0D0C0B0A would be stored as follows:

   - Big Endian: mem: 0D 0C 0B 0A
   - Little Endian: mem: 0A 0B 0C 0D

   This distinction is crucial when working with memory addresses that increase from left to right. When casting pointers to char types, the results will vary depending on the machine's endianness. On a little-endian machine, *(char *)(&V) would yield the value modulo 256 (0x0A), while on a big-endian machine, it yields the value divided by 2^24 (0x0D).

   For portable code, separate implementations for big-endian and little-endian architectures are often required when dealing with serialization or transferring data between systems. The C union type may also necessitate distinct treatments depending on endianness.

   1.1.2 Size of pointer is not size of int
   In programming for 32-bit architectures, casting pointers to integers (and vice versa) often works correctly because the sizes of int and long coincide. However, this approach will fail on 64-bit machines where the size of a pointer differs from that of an integer type. To avoid issues with different machine architectures, it is advisable to cast pointers to larger types or refrain from casting pointers to integers altogether for more portable code.

   1.1.3 Shifts and division
   With two's complement arithmetic, performing multiplication by a power of 2 corresponds to a left shift, while division by a power of 2 is equivalent to a right shift for unsigned types. For signed types, division by a power of 2 still results in a right shift; however, the higher bits are filled with ones or zeros depending on whether the original value was positive or negative. This behavior corresponds to rounding towards negative infinity.

   The compiler automatically optimizes these operations where possible. Additionally, computing remainders modulo a power of 2 for unsigned types can be achieved using bitwise AND operations (b % 32 == b & (32-1)).

   These low-level optimizations are critical in various computational tasks and should be well understood when working with binary representations. The chapter also provides examples of assembly code for x86 and AMD64 architectures, which can help readers understand how these operations are implemented at the machine level.


The provided text discusses various bit manipulation techniques and optimizations, primarily for use in low-level programming, such as C or assembly language. Here's a detailed summary and explanation of the key points:

1. **Division by Constants**: The text explains that division by compile-time constants can be replaced with multiplication and shifts during compilation to improve performance. For example, `a / 10` is compiled into a sequence of instructions that effectively multiply by a precomputed value (0xcccccccd) and then perform a right shift by three bits. This optimization applies similarly to the modulo operation when the modulus is a compile-time constant.

2. **Pitfalls**: The text highlights two pitfalls related to bit manipulation in C:

   - **Two's Complement Pitfall**: In two's complement representation, the most negative value (all bits set except the sign bit) is equal to its negative. This can lead to unexpected behavior if not accounted for, such as `if(x < 0)`, where x could be this special value.

   - **Shift Pitfall**: The C standard does not define the behavior of a right shift on signed integers when the shift count exceeds the number of bits in the integer type (BITS_PER_LONG). Some compilers may produce incorrect results if they implement a right shift as a logical shift, which could lead to undefined behavior.

3. **Bit Manipulation Functions**: The text provides several functions for bit manipulation:

   - **Testing, Setting, and Deleting Bits**: Functions like `test_bit`, `set_bit`, `clear_bit`, and `change_bit` allow for checking, setting, or clearing individual bits within a word (long integer). These functions are straightforward and assume that the indices used do not exceed BITS_PER_LONG.

   - **Copying a Bit**: The `copy_bit` function copies a bit from one position to another in a word using XOR operations. A more complex version, `mask_copy_bit`, allows copying bits based on mask patterns.

   - **Swapping Two Bits**: Functions like `bit_swap` and `bit_swap_01` swap the values of two specific bits within a word.

4. **Operations on Low Bits or Blocks of a Word**: These functions focus on manipulating the lowest set bit (lowest one) or blocks of ones/zeros at the low end of a word:

   - **Isolating, Setting, and Deleting the Lowest One**: Functions like `lowest_one`, `clear_lowest_one`, and `set_lowest_zero` isolate, clear, or set the lowest set bit in a word.

   - **Computing the Index of the Lowest One**: The `lowest_one_idx` function calculates the index (position) of the lowest set bit using an algorithm involving multiple AND operations with shifted versions of the input word. An alternative, assembler-based implementation (`asm_bsf`) is also provided for platforms that support it.

   - **Isolating Blocks of Zeros or Ones at the Low End**: Functions like `low_ones`, `low_zeros`, and `lowest_block` isolate blocks of zeros or ones near the low end of a word.

5. **Creating Transitions at the Lowest One**: The `lowest_one_10edge` and `lowest_one_01edge` functions create rising or falling edges, respectively, at the position of the lowest set bit in a word.

6. **Extraction of Ones, Zeros, or Blocks Near Transitions**: These functions, like `low_match`, isolate values near transitions (places where adjacent bits have different values) within two input words.

7. **Creating Blocks of Ones**: The `bit_block` and `cyclic_bit_block` functions generate bit blocks of a specified length starting at a given position within a word, with the latter allowing for wrapping around the word boundary if necessary.

8. **Optimization Considerations**: The text emphasizes the importance of proper documentation, benchmarking, and understanding platform-specific optimizations when working with low-level code. It also warns against deleting unoptimized versions of critical functions without keeping them as a fallback in case of issues during porting or under specific conditions (e.g., extreme register pressure).

These techniques and functions are useful for optimizing performance, manipulating bit-level data, and understanding low-level programming concepts. They can be applied in various domains, such as embedded systems, cryptography, and computer graphics, where efficient manipulation of individual bits or groups of bits is essential.


The provided text discusses various bit manipulation techniques and functions, primarily focusing on C/C++ implementations. Here's a detailed summary and explanation of the key concepts:

1. **Bit Manipulation Functions:**

   - **`single_ones(ulong x)`**: Returns a word with only the isolated ones (set bits) of `x` set. This is achieved by bitwise ANDing `x` with its right-shifted version and then with its left-shifted version, followed by an OR operation.
   - **`single_zeros_xi(ulong x)`**: Returns a word with only the isolated zeros (unset bits) of `x` set. It uses the `single_ones()` function by passing the bitwise NOT of `x`.
   - **`single_zeros(ulong x)`**: Returns a word with only the isolated zeros of `x` set. It's similar to `single_zeros_xi()`, but it assumes that bits outside the word are zero, so it directly uses bitwise operations without considering them.
   - **`single_values(ulong x)`**: Returns a word where only the isolated ones and zeros of `x` are set. This is done by XORing `x` with its left-shifted version and then ANDing the result with the XOR of `x` and its right-shifted version.
   - **`single_values_xi(ulong x)`**: Similar to `single_values()`, but it ignores outside values, meaning it assumes all bits outside the word are zero.
   - **`border_ones(ulong x)`**: Returns a word where only those ones of `x` are set that lie next to a zero. This is achieved by bitwise ANDing `x` with the bitwise OR of its left-shifted and right-shifted versions, followed by a bitwise NOT operation.
   - **`border_values(ulong x)`**: Returns a word where those bits of `x` are set that lie on a transition (change from 0 to 1 or vice versa). This is done by XORing `x` with its left-shifted version and then ORing the result with the XOR of `x` and its right-shifted version.
   - **`high_border_ones(ulong x)`**: Returns a word where only those ones of `x` are set that lie right to (next lower bin) a zero. This is achieved by bitwise ANDing `x` with the result of XORing `x` with its right-shifted version.
   - **`low_border_ones(ulong x)`**: Returns a word where only those ones of `x` are set that lie left to (next higher bin) a zero. This is done by bitwise ANDing `x` with the result of XORing `x` with its left-shifted version.
   - **`block_border_ones(ulong x)`**: Returns a word where only those ones of `x` are set that are at the border of a block of at least 2 bits. This is achieved by bitwise ANDing `x` with the result of XORing its left-shifted and right-shifted versions.
   - **`low_block_border_ones(ulong x)`**: Returns a word where only those bits of `x` are set that are at the left of a border of a block of at least 2 bits. This is done by first applying `block_border_ones()` and then ANDing the result with `x` right-shifted by one.
   - **`high_block_border_ones(ulong x)`**: Returns a word where only those bits of `x` are set that are at the right of a border of a block of at least 2 bits. This is done by first applying `block_border_ones()` and then ANDing the result with `x` left-shifted by one.
   - **`block_ones(ulong x)`**: Returns a word where only those bits of `x` are set that are part of a block of at least 2 bits. This is achieved by bitwise ANDing `x` with the result of ORing its left-shifted and right-shifted versions.

2. **Computing the Index of a Single Set Bit:**

   - **Cohen's Trick**: Uses a modulus `m` such that all powers of 2 are different modulo `m`. A table `mt[]` is created with size `m`, where `mt[(2^j) mod m] = j` for `j > 0`. The index of the lowest set bit in `x` can be found by reducing `x` modulo `m` and then looking up the result in `mt[].`
   - **De Bruijn Sequence**: Uses a binary De Bruijn sequence of size `N`. A table `dbt[]` is created such that the entry with index `wi` points to `i`. The index of the lowest set bit in `x` can be found by multiplying `x` with the De Bruijn sequence, right-shifting the result, and then looking up the result in `dbt[].`
   - **Floating-Point Numbers**: This method converts an integer into a floating-point number, reads off the position of the highest set bit from the exponent, and isolates the lowest bit before conversion. However, this technique is slow and machine-dependent.

3. **Operations on High Bits or Blocks of a Word:**

   - **Isolating the Highest One and Finding Its Index**: The highest set bit in a word can be isolated using various methods, including a bit-scan instruction if available or auxiliary functions like `highest_one_01edge()` and `highest_one_idx()`.
   - **Isolating the Highest Block of Ones or Zeros**: Functions like `high_zeros(ulong x)` and `high_ones(ulong x)` isolate the highest block of zeros or ones, respectively. These functions use bitwise OR operations combined with right shifts to set the corresponding bits in the result.

4. **Functions Related to the Base-2 Logarithm:**

   - **`ld(ulong x)`**: Returns the floor value of the base-2 logarithm of `x`, i.e., the largest integer `k` such that `2^k <= x < 2^(k+1)`.
   - **`one_bit_q(ulong x)` and `is_pow_of_2(ulong x)`**: Determine whether `x` is a power of two (including zero).
   - **`next_pow_of_2(ulong x)` and `next_exp_of_2(ulong x)`**: Return the next power of two or exponent, respectively, greater than or equal to `x`. These functions are useful in algorithms involving powers of two, such as Fast Fourier Transform (FFT) with lengths restricted to powers of two.

5. **Counting the Bits and Blocks of a Word:**

   - **`bit_count(ulong x)`**: Counts the number of set bits (ones) in `x` using a bitwise search via masks, requiring O(log2(BITS PER LONG)) operations. The function uses multiple AND and OR operations with specific masks to count bits in groups of 2, 4, 8, 16, 32, and 64 bits, respectively.

These functions and techniques provide a comprehensive set of tools for bit manipulation and analysis, which can be useful in various applications, such as image processing, data compression, and algorithms involving binary representations.


The provided text discusses various methods for counting bits in a binary word, optimizing these methods for specific scenarios, and using bit manipulation techniques. Here's a detailed summary and explanation of the key concepts:

1. **Bit Counting Methods**: The text presents several algorithms for counting the number of set (i.e., 1) bits in a binary word. These methods vary in their efficiency depending on the average number of set bits in the input data.

   - **Sparse Counting** (`bit_count_sparse`): This method is efficient when the input word has only a few set bits. It iteratively clears the least significant set bit and increments a counter until no more set bits are left.

     ```cpp
     static inline ulong bit_count_sparse(ulong x) {
         ulong n = 0;
         while (x) { ++n; x &= (x-1); }
         return n;
     }
     ```

   - **Dense Counting** (`bit_count_dense`): This method is used when the input word has many set bits. It first applies sparse counting to the complement of the input word and subtracts the result from the total number of bits (i.e., `BITS_PER_LONG`).

     ```cpp
     static inline ulong bit_count_dense(ulong x) {
         return BITS_PER_LONG - bit_count_sparse(~x);
     }
     ```

   - **Specialized Counting** (`bit_count_3`, `bit_count_15`): These methods are optimized for words with a limited number of set bits. For example, `bit_count_3` is designed for words with up to 3 set bits and uses a combination of bitwise operations and multiplication by a magic constant.

2. **Bit Block Counting**: The text introduces functions for counting the number of bit blocks (contiguous sequences of set bits) in a binary word. For instance, `bit_block_count` returns the total number of bit blocks, while `bit_block_ge2_count` counts only those with two or more set bits.

   ```cpp
   static inline ulong bit_block_count(ulong x) {
       return (x & 1) + bit_count((x^(x>>1)) / 2);
   }

   static inline ulong bit_block_ge2_count(ulong x) {
       return bit_block_count(x & ((x<<1)&(x>>1)));
   }
   ```

3. **Vertical Addition**: This technique optimizes bit counting by processing multiple words simultaneously using a "vertical" addition approach, which reduces the number of required bitwise operations compared to a straightforward loop. The provided example demonstrates this method for counting bits in an array of binary words.

   ```cpp
   static inline ulong bit_count_v(const ulong *x, ulong n) {
       ulong b = 0;
       const ulong *xe = x + n + 1;
       while (x+15 < xe) {
           b += bit_count_v15(x);
           x += 15;
       }
       // Process remaining elements...
   }
   ```

4. **Bitwise Operations for Set Testing**: The text discusses using bitwise operations to test whether a given word `u` is a subset of another word `e` (i.e., whether all set bits in `u` are also set in `e`). This can be done efficiently using AND, OR, and NOT operations.

   ```cpp
   static inline bool is_subset(ulong u, ulong e) {
       return (u & e) == u;
   }
   ```

5. **Bit Indexing**: The text presents a method for finding the index of the i-th set bit in a binary word `x`. This is achieved by iteratively narrowing down the search range using bitwise operations and multiplication by magic constants.

   ```cpp
   static inline ulong ith_one_idx(ulong x, ulong i) {
       // ... (implementation details omitted for brevity)
   }
   ```

In summary, the text covers various bit manipulation techniques tailored to different scenarios, optimizing performance by leveraging properties of binary representations and employing efficient algorithms. These methods find applications in areas such as data compression, cryptography, and parallel processing.


This text discusses several topics related to bit manipulation, focusing on branchless algorithms and binary necklaces. Let's break it down into sections for clarity:

### Avoiding Branches
The text begins by explaining how branches can be expensive operations, particularly on CPUs with long pipelines. To mitigate this, the author suggests replacing certain branch conditions with branchless alternatives. For example, instead of:
```c
if ( (x<0) || (x>m) ) { ... }
```
You could use:
```c
if ( (unsigned)x > m ) { ... }
```
If `m` is a power of 2, an even more efficient approach is to use:
```c
if ( ( (ulong)x | (ulong)y ) > (unsigned)m ) { ... }
```
The text also provides examples of branchless functions for computing maximum and minimum values. For instance, the `max0` function returns `x` if it's non-negative or zero otherwise:
```c
static inline long max0(long x)
{
    return x & ~(x >> (BITS_PER_LONG - 1));
}
```
This works by leveraging arithmetic right shift and bitwise AND operations to clear all bits when `x` is negative. The complementary function, `min0`, returns zero for positive inputs:
```c
static inline long min0(long x)
{
    return x & (x >> (BITS_PER_LONG - 1));
}
```

### Bit-wise Rotation of a Word
The text discusses bit-wise rotation functions, which are not natively supported in C or C++. These operations can be emulated using bitwise operators. For example, left rotation by `r` bits can be achieved with:
```c
static inline ulong bit_rotate_left(ulong x, ulong r)
{
    return (x << r) | (x >> (BITS_PER_LONG - r));
}
```
Right rotation is similar but with the arguments swapped. If your CPU supports it, you can use an assembler instruction for better performance:
```c
static inline ulong bit_rotate_right(ulong x, ulong r)
{
    #if defined BITS_USE_ASM
        return asm_ror(x, r);
    #else
        return (x >> r) | (x << (BITS_PER_LONG - r));
    #endif
}
```
Here's an example of rotation using only a part of the word length:
```c
static inline ulong bit_rotate_left(ulong x, ulong r, ulong ldn)
{
    ulong m = ~0UL >> ( BITS_PER_LONG - ldn );
    x &= m;
    x = (x << r) | (x >> (ldn - r));
    x &= m;
    return x;
}
```
Finally, the text provides functions for signed rotation: `bit_rotate_sgn`.

### Binary Necklaces
This section discusses binary necklaces, which are cyclic permutations of bit words. The author presents several related functions and a class to generate these necklaces. Here's an overview:

#### Cyclic Matching, Minimum, and Maximum
The `bit_cyclic_match` function checks how often the second argument needs to be rotated right to match the first. It returns the rotation count or ~0UL if no match is found within the word length. The function can also be adapted to work with a specified number of leading bits (`ldn`).

The `bit_cyclic_min` function computes the cyclic minimum (i.e., the lexicographically smallest cyclic permutation) of a given bit word.

#### Cyclic Period and Binary Necklaces
The `bit_cyclic_period` function calculates the minimal positive rotation that transforms a word into itself, which corresponds to the period of the binary necklace. The implementation uses efficient methods to avoid unnecessary rotations and checks only for periods that are divisors of the word length.

#### Generating All Binary Necklaces
The author presents a class, `bit_necklace`, for generating all binary necklaces using the FKM algorithm. This class has properties like the necklace itself (`a_`), its period (`j_`), and the number of bits in the words (`n_`). The `next()` method generates successive necklaces, while `is_lyndon_word()` checks if a necklace is a Lyndon word.

### Computing Cyclic Distance
The `bit_cyclic_dist` function computes the minimal bit count of `(t ^ b)` where `t` ranges over all cyclic rotations of `a`. If the arguments are cyclic shifts of each other, it returns zero. A version for partial words is also provided.

In summary, this text delves into various techniques for efficient bit manipulation, focusing on branchless algorithms and binary necklace generation. These methods can be valuable in performance-critical applications where minimizing branch mispredictions and optimizing bitwise operations are essential.


The text discusses various operations related to binary words (i.e., integers represented in binary format) in the context of bit wizardry, a field focused on efficient manipulation of bits within these words. Here's a detailed summary and explanation of the key concepts and functions presented:

1. Cyclic XOR and Inverse (Section 1.13.5):
   - `bit_cyclic_rxor(ulong x)`: This function performs a cyclic XOR operation on an input word `x`. The result is obtained by XORing the original word with its right-rotated version by one bit. The number of set bits (ones) in the resulting word will be even, and it will be the same as the complement of the original word.
   - `bit_cyclic_inv_rxor(ulong x)`: This is the inverse function of `bit_cyclic_rxor()`. It takes an input word `x` with an even number of set bits, then returns a word `v` such that applying `bit_cyclic_rxor()` to `v` will yield the original `x`.

2. Reversing Bits (Section 1.14):
   - Bit reversal is the process of swapping the positions of the bits in a binary word, resulting in a new order where the least significant bit becomes the most significant, and vice versa.
   - Various auxiliary functions are provided to help reverse adjacent blocks or groups of bits within a word efficiently:
     1. `bit_swap_1(ulong x)`: Swaps pairs of adjacent bits.
     2. `bit_swap_2(ulong x)`, `bit_swap_4(ulong x)`, and `bit_swap_8(ulong x)`: Swap groups of 2, 4, or 8 bits, respectively.
     3. `bit_swap_16(ulong x)`: Swaps groups of 16 bits (for 32-bit architectures).
   - The primary function for reversing the bit order in a word is `revbin(ulong x)`, which applies the auxiliary functions successively to achieve full bit reversal. On 64-bit architectures, it may utilize an inline assembler (`asm_bswap`) for more efficient byte swapping.

3. Generating Bit-Reversed Words (Section 1.14):
   - The text presents several methods for generating bit-reversed words in a specific order:
     1. `revbin(ulong x)`: Reverses the entire bit order of an input word `x`.
     2. `revbin_t(ulong x, ulong ldn)`: Reverses only the least significant `ldn` bits of the input word `x`, padding the remaining bits with zeros. This function uses a table-lookup approach for efficiency when `ldn` is not too small.
     3. `revbin_upd(ulong r, ulong h)`: Updates the bit-reversed word `r` by one position while maintaining the counting order, using a lookup table (`utab`) and bit manipulations to minimize branching.

4. Bit-wise Zip (Section 1.15):
   - The `bit_zip(ulong x)` function rearranges the bits in a binary word such that even-indexed bits from the lower half become odd-indexed, while odd-indexed bits from the upper half become even-indexed. This operation is also called bit zip or bit unzip (inverse of bit zip).
   - The text presents multiple implementations of `bit_zip(ulong x)` and its inverse (`bit_unzip(ulong x)`) to demonstrate different optimization techniques and trade-offs between readability, performance, and code size.

5. Alternative Techniques for In-Order Generation (Section 1.14.4):
   - The text briefly mentions alternative approaches to generating bit-reversed words in a specific order:
     1. A branchless loop from Brent Lehmann, involving divisions that can be expensive on some architectures.
     2. A recursive algorithm for generating all N-bit bit-reversed words in order.
     3. Generating revbin pairs in a pseudo-random order for certain applications.

Throughout the text, various optimizations and trade-offs are explored to achieve efficient bit manipulations in C/C++ code. These techniques can be useful when working with binary representations of integers in performance-critical scenarios such as digital signal processing, cryptography, or computer graphics.


The text discusses several concepts related to bit manipulation and number theory, specifically focusing on Gray code, parity, reversed Gray code, and sequency of binary words. Here's a detailed summary and explanation of each concept:

1. **Gray Code**: The Gray code is a binary numeral system where two successive values differ in only one bit. It is useful for applications requiring minimal changes between adjacent values. The given C++ function `gray_code(ulong x)` computes the Gray code of a 64-bit unsigned integer `x`. The inverse Gray code can be computed using three methods:

   - Version 1: Integration modulo 2, which uses a loop to calculate the inverse by XORing with shifted versions of itself.
   - Version 2: Applying the Gray code computation BITS_PER_LONG-1 times, which is more efficient and faster than Version 1.
   - Version 3: Using the property that gray ** (2^n) = id for n-bit words, this version applies the Gray code operation multiple times to achieve the inverse.

2. **Parity**: The parity of a binary word is its bit-count modulo 2, indicating whether the number of set bits is even or odd. Functions `parity(ulong x)` and `asm_parity(ulong x)` compute the parity of a 64-bit unsigned integer `x`. The latter uses assembly code for better performance on specific CPU architectures (x86 and AMD64).

3. **Byte-wise Gray Code and Parity**: These functions (`byte_gray_code(ulong x)`, `byte_inverse_gray_code(ulong x)`, and `byte_parity(ulong x)`) compute the Gray code, inverse Gray code, and parities of 8-bit segments within a larger binary word.

4. **Incrementing (counting) in Gray Code**: This section introduces methods for efficiently incrementing Gray codes while preserving their properties. The `next_gray2(ulong x)` function generates the next even-numbered Gray code value, starting from an even input. To increment odd-numbered values, a modified approach is provided.

5. **Thue-Morse Sequence**: This sequence is generated by counting the bit-pairs modulo 2 for binary words. The given C++ class `thue_morse` computes this sequence using efficient methods based on bitwise operations and parity checks.

6. **Golay-Rudin-Shapiro (GRS) Sequence**: This sequence is defined by counting the bit-pairs modulo 2 for specific binary words. The provided C++ function `grs_negative_q(ulong x)` returns +1 for indices where the GRS sequence has the value −1, based on a method that counts overlapping bit-pairs and applies parity checks.

7. **Reversed Gray Code**: This concept involves computing the Gray code of the bit-reversed word and then reversing the result again. The given C++ functions `rev_gray_code(ulong x)` and `inverse_rev_gray_code(ulong x)` efficiently compute the reversed Gray code and its inverse, respectively.

8. **Bit Sequency**: The sequency of a binary word is the number of zero-one transitions in the word. The provided C++ function `bit_sequency(ulong x)` calculates this value by counting the transitions in the Gray code of the input word. To account for the lowest bit, add 1 to the result.

These concepts and functions are valuable tools in various applications involving bit manipulation, error-correcting codes, and combinatorial problems, among others.


The text describes several invertible binary word transforms, named 'blue', 'yellow', 'red', and 'green'. These transforms are self-inverse or third roots of identity, meaning they can be applied twice to return the original value (involutions) or three times to do so. The transforms scramble binary words while preserving certain properties like bit count and parity.

1. Blue Code: This code is an involution, denoted as B. It uses a specific mask 'm' that shifts right over iterations until the mask reaches zero. The blue code maps any range [0...2^k - 1] onto itself. It can be used for fast composition of binary polynomials with x + 1 and has potential applications in randomization of binary words.

2. Yellow Code: Also an involution (denoted as Y), the yellow code uses similar masks to blue but shifts left instead of right. Its scrambling might be beneficial for randomizing binary words, though it's not explicitly stated.

3. Red Code: Denoted as R, this transform is a third root of identity, meaning applying it three times results in the original word (R³ = id). It uses masks that shift left or right based on iteration, and it can be computed using bit-reversal of blue code (rB).

4. Green Code: Denoted as E, this is another third root of identity like red, with the property that E² = R and E³ = id. It's computed by applying bit-reversal to yellow code (rY).

The relations between these transforms are detailed in equations (1.19-1a) to (1.19-6), showing how they can be composed or reversed. For instance, Blue and Yellow are related as B = YRY and Y = BRB, while Red and Green are interconnected via R = ERE and E = RRR. Bit-reversal (r) plays a significant role in defining these relationships.

The transforms' effects on the Gray code (g) are described by equations (1.19-9a) to (1.19-9d), showing how applying Gray code followed or preceded by another transform (B, Y, R, E) can lead back to the original word or result in a specific transformation.

These transforms have potential applications in various computational tasks involving binary words, such as randomization, polynomial composition, and bit manipulation. However, the text does not explicitly detail these use cases; it primarily focuses on their definitions, properties, and relationships.


The Radix-2 (Minus Two) representation is a unique way to represent non-negative integers using powers of -2 instead of 2. The representation takes the form of an infinite series, where each term is either 0 or -2 raised to some power k. For integers, this series terminates after a finite number of terms, with the highest nonzero tk being at most two positions beyond the highest bit in the binary representation of the absolute value of the integer (using two's complement).

Key points and functions related to Radix-2 representation are:

1. Conversion from Binary to Radix-2:
The function `bin2neg(x)` converts a binary number x into its Radix-2 representation. This algorithm involves adding and XORing the input with a specific constant (0xaaaaaaaaUL for 32 bits) to produce the Radix-2 representation. The inverse of this conversion, i.e., converting back from Radix-2 to binary, is achieved through `neg2bin(x)`, which reverses these steps by subtracting and XORing with the same constant.

2. Fixed Points:
The sequence of fixed points for the Radix-2 conversion starts as 0, 1, 4, 5, 16, 17, 20, 21, 64, 65, 68, 69, 80, 81, 84, 85, ..., where each fixed point has ones only at even positions in its binary representation. This sequence is known as the Moser-De Bruijn sequence (A000695).

3. Gray Code of Radix-2:
The Gray code for the numbers in the range 0 to k (where k = (4n - 1)/3, n ∈ ℕ) can be generated using the Radix-2 representation. The sequence of these Gray codes forms a Gray code for the specified set of integers.

4. Generating Radix-2 words in order:
To generate Radix-2 words in order, one can start with the largest 111...111 (with n ones) and systematically move bits to even positions by summing them up. This process will produce all Radix-2 words in ascending order.

The Radix-2 representation provides an alternative way of representing non-negative integers, offering unique properties such as fixed points and connections to Gray codes. It is particularly interesting for its relationship with the Moser-De Bruijn sequence and the possibility to generate Radix-2 words systematically using bit manipulation techniques.


The provided text discusses three different orders for generating bit combinations (binary words with a specific number of set bits): Co-lexicographic (colex) order, Lexicographic (lex) order, and Shifts-order.

1. **Co-lexicographic (colex) order**: In this order, the reversed sets are sorted. The method to find the next combination involves determining the lowest block of ones and moving its highest bit one position up, then shifting the rest of the block to the low end of the word. The program [FXT: bits/bitcombcolex.h] provides a routine (next_colex_comb) to compute the next combination in co-lexicographic order. The predecessor can be found using the inverse method, which involves finding the successor in co-lexicographic order and then taking its bitwise complement if it's not zero. The first and last combinations can be computed using [FXT: bits/bitcombcolex-demo.cc].

2. **Lexicographic (lex) order**: In this order, the sets are sorted. The binary words corresponding to combinations in lex order are the bit-reversed complements of the words for combinations in co-lexicographic order. The program [FXT: bits/bitcomblex-demo.cc] demonstrates how to compute the subset-lex sequence efficiently by iterating through combinations in co-lexicographic order, reversing them using the revbin() function (section 1.14), and applying an auxiliary mask (m) and last combination (l).

3. **Shifts-order**: This order is obtained from the shifts-order for subsets by discarding all subsets whose number of elements are not equal to k and reversing the list order. The first combination is [1k0n−k], and the successor is computed by shifting the bits in a specific way (as shown in figure 1.24-D). Figure 1.24-C illustrates combinations in shifts-order for k = 1, 2, 3, 4.

The text also mentions nonadjacent form (NAF) representations of signed binary numbers and provides routines to convert between binary and NAF representations. The Fibbinary numbers are defined as the sequence of values whose negative part in the NAF representation is zero, which corresponds to the sequence [0, 1, 2, 4, 5, 8, 9, 10, 16, ...]. These numbers are also known as the Fibonacci binary numbers.


The provided text discusses various methods for generating bit combinations, minimal-change orders, and subsets of a given binary word. Here's a detailed summary and explanation of each section:

1. **Generating Bit Combinations**

   - **Simple Split (S)**: If the rightmost one is not in position zero (least significant bit), shift the word to the right and return the combination.
   - **Finished?**: If the combination is the last one ([0n], [0n−11], [10n−k1k−1]), then return zero.
   - **Shift Back**: Shift the word to the left such that the leftmost one is in the leftmost position (this can be a no-op).
   - **Simple Split (S-2)**: If the rightmost one is not the least significant bit, move it one position to the right and return the combination.
   - **Split Second Block**: Move the rightmost bit of the second block (from the right) of ones one position to the right and attach the lowest block of ones, then return the combination.

   An implementation for these methods is provided in [FXT: bits/bitcombshifts.h]. The generated combinations' speed varies depending on the parameters `n` and `k`.

2. **Minimal-Change Order**

   - **igc_next_minchange_comb(ulong x)**: A routine to return the inverse Gray code of the next combination in minimal-change order. The input must be the inverse Gray code of the current combination.
   - **next_minchange_comb(ulong x, ulong last)**: An alternative method that uses `igc_next_minchange_comb()` and returns the next combination in minimal-change order.

   Successive combinations differ in exactly two positions, as shown in Figure 1.24-E. The algorithm is efficient, generating combinations at a high rate (e.g., about 96 million per second for `32
12
`).

3. **Generating Bit Subsets of a Given Word**

   - **Counting Order**: To generate all subsets of the set of ones of a binary word, use sparse counting ideas from section 1.8.1. The implementation is provided in [FXT: class bit_subset in bits/bitsubset.h]. With a word like [...11.1.], subsequent `next()` and `prev()` calls produce subsets in counting order. About 1.1 billion subsets per second are generated with both methods.
   - **Minimal-Change Order**: A method to isolate the changing bit from counting order that does not depend on shifting is introduced. The implementation is provided in [FXT: class bit_subset_gray in bits/bitsubset-gray.h]. With a word like [...11.1.], subsequent `next()` and `prev()` calls produce subsets in minimal-change order, generating about 365 million subsets per second with both methods.

4. **Binary Words in Lexicographic Order for Subsets**

   - The text provides an example of binary words corresponding to nonempty subsets of the 4-element set in lexicographic order with respect to subsets. It also shows a table of binary words and their corresponding subsets.
   - The `next()` and `prev()` methods are discussed for generating the next and previous word in lexicographic order, respectively.

In summary, the text covers various techniques for generating bit combinations, minimal-change orders, and subsets of given binary words. It presents implementations and discusses their efficiency based on the provided examples and figures.


The given text discusses several topics related to binary words, their representation, and algorithms for generating them. Here's a detailed summary and explanation of each section:

1. **Binary Words in Lexicographic Order (Section 1.26):**

   - The text introduces two functions (`next_lexrev()` and `prev_lexrev()`) that generate binary words in lexicographic order, starting from the one-bit word at position n−1.
   - These functions isolate the lowest bit for manipulation to create the next subset, making the process efficient.
   - The function `negidx2lexrev(k)` converts a rank (index) to its corresponding binary word in lexicographic order, and `lexrev2negidx(x)` does the reverse conversion.
   - The number of bits required to represent a lex-word equals the sequence's entry A108918 in [312].

2. **Minimal Decompositions into Terms 2^k −1 (Section 1.26.3):**

   - This section describes how binary words can be represented as sums of distinct powers of 2 minus one (`2^k −1`).
   - It provides examples and a formula to compute the minimal number of terms required for such decompositions.

3. **Fixed Points (Section 1.26.4):**

   - The text discusses fixed points in the conversion between binary words and their lexicographic order indices.
   - A sequence of fixed points is presented, along with a generating function (equation 1.26-2).
   - It provides an alternative method to compute the number of terms needed for a decomposition using the `is_lexrev_fixed_point(x)` function.

4. **Recursive Generation and Relation to Power Series (Section 1.26.5):**

   - The section introduces two functions (`C()` and `A()`) that generate bit-reversed binary words in reversed lexicographic order and a power series, respectively.
   - It explains the relationship between these generated sequences and power series, showing how the lowest bits of the k-th word correspond to coefficients in the power series.

5. **Fibonacci Words (Section 1.27):**

   - Fibonacci words are binary words that do not contain consecutive ones. The text provides functions (`is_fibrep()`, `bin2fibrep()`, and `fibrep2bin()`) for testing, converting to, and from Fibonacci representations.
   - Two functions, `next_fibrep()` and `prev_fibrep()`, are introduced to generate all n-bit Fibonacci words in lexicographic order and its reverse.

6. **Gray Code Order (Section 1.27.2):**

   - The text discusses generating a Gray code for binary Fibonacci words, based on the radix −2 representations' minimal-change combination.
   - It provides an implementation of the `bit_fibgray` class to generate and manipulate Fibonacci Gray codes.

In summary, this text presents various algorithms and methods related to binary word representation, lexicographic order generation, Fibonacci words, and their conversions. These techniques are useful in different areas, including combinatorics, information theory, and fast Walsh transforms.


The Hilbert curve is a continuous fractal space-filling curve introduced by David Hilbert. It is named after him due to his work on this mathematical concept. The Hilbert curve is significant because it can fill any two-dimensional space without overlapping, making it useful in various fields such as computer graphics, data analysis, and image processing.

The curve's path is determined by a sequence of moves (dx, dy) where dx and dy are integer values that can be either -1, 0, or +1. At each step n of the Hilbert curve, exactly one of dx and dy is zero. This means that at each step, the curve either moves horizontally (+1 or -1 for dx) or vertically (+1 or -1 for dy), but not diagonally.

The function `hilbert_p(t)` computes the direction of the n-th move (dx + dy) of the Hilbert curve using the parity of the number of threes in the radix-4 representation of t. The radix-4 representation is a way to express numbers using base 4 digits, where each digit can be one of the values {0, 1, 2, 3}.

Here's how `hilbert_p(t)` works:

1. It first computes the value 'd' by performing bitwise AND operations on 't' with the hexadecimal masks `0x5555555555555555UL` and `(t & 0xaaaaaaaaaaaaaaaaUL) >> 1`. This operation effectively isolates the bits corresponding to the threes in the radix-4 representation of 't'.
2. The function then returns the parity (odd or even) of 'd', which indicates whether dx + dy equals -1 (return 0) or +1 (return 1).

The optimized version of `hilbert_p(t)` uses a series of bitwise XOR operations to isolate and compute the parity more efficiently:

1. It performs a bitwise AND operation with the mask `0xaaaaaaaaaaaaaaaaUL` and right-shifts 't' by one position, effectively isolating the bits corresponding to the threes in the radix-4 representation.
2. The function then applies a series of bitwise XOR operations with 't' shifted by powers of 2 (2, 4, 8, 16, and 32) to further isolate and compute the parity more efficiently.
3. Finally, it returns the least significant bit of the result, which indicates whether dx + dy equals -1 (return 0) or +1 (return 1).

Once you have computed p using `hilbert_p(t)`, you can determine m (dx - dy) by using the relationship between p and m. Since exactly one of dx and dy is zero, m will be either -p or p, depending on whether dx is positive or negative (similar for dy).

The Hilbert curve's moves and turns are shown in Figure 1.31-B, where 'dir' indicates the direction (>^<^^>v>^>vv<v>>^>v>>^<^>^<<v<^^^>v>>^<^>^<<v<^<<v>vv<^<v<^^>^<) and 'turn' represents the change in direction (0--+0++--++0+--0-++-0--++--0-++00++-0--++--0-++-0--+0++--++0+--). These patterns help visualize how the Hilbert curve navigates through two-dimensional space.


The provided text discusses several space-filling curves, including the Hilbert curve, Z-order curve, dragon curve, alternate paper-folding sequence, terdragon, and hexdragon. Here's a detailed summary of each:

1. **Hilbert Curve**: This is a continuous fractal curve that passes through every point in a square grid exactly once. It has four possible moves (directions): right (+x), down (-y), up (+y), and left (-x). The direction and turn between steps are determined by the functions `hilbert_dir()` and `hilbert_turn()`, respectively.

   - `hilbert_dir(t)`: Returns a 2-bit value (`d`) encoding the direction of the move with step `t`. The values are:
     ```
     d : direction
     0 : right (+x: dx=+1, dy= 0)
     1 : down (-y: dx= 0, dy=-1)
     2 : up (+y: dx= 0, dy=+1)
     3 : left (-x: dx=-1, dy= 0)
     ```

   - `hilbert_turn(t)`: Returns the turn (left or right) between steps `t` and `t-1`. The returned value is 0 for no turn, +1 for a right turn, and -1 for a left turn.

2. **Z-order Curve**: This curve visits all points in each quadrant before moving to the next one. It's generated by separating bits at even and odd indices of a linear parameter `t`. The conversion between the linear coordinate and pair of coordinates (x, y) is done using bit manipulation functions `lin2zorder()` and `zorder2lin()`.

   - `lin2zorder(t, x, y)`: Transforms the linear coordinate `t` to Z-order coordinates `x` and `y`.
   - `zorder2lin(x, y)`: Transforms Z-order coordinates `x` and `y` back to a linear parameter `t`.

3. **Dragon Curve**: This curve is generated by interpreting ones as 'turn left' and zeros as 'turn right' in the paper-folding sequence. The curve has two types of turns: left (`^`) and right (`v`). The net rotation after `k` steps, modulo 4 (to ignore multiples of 360 degrees), is computed by counting ones in the Gray code of `k`.

4. **Alternate Paper-Folding Sequence**: This sequence is generated by folding a strip of paper alternately from the left and right. The resulting curve has turns interpreted as 'left' (L) and 'right' (R). The net rotation after `k` steps, modulo 4, is computed similarly to the dragon curve.

5. **Terdragon Curve**: This curve turns to the left or right by 120 degrees depending on the sequence 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1.

6. **Hexdragon Curve**: This curve is similar to the terdragon but with a more complex turning sequence (not provided in the text). The first 729 segments of this curve are shown in the text.

Each of these curves has its unique generation process and rules, leading to different patterns and properties. They are used in various fields, including computer graphics, data structures, and fractal geometry.


The provided text discusses various aspects of permutations, a concept from combinatorial mathematics. Here's a detailed summary and explanation:

1. **Permutations**: A permutation of n elements is an arrangement of these elements into a specific order. It can be represented as an array, where the element at position k is moved to position xk when the permutation X is applied to F = [f0, f1, ..., fn-1]. The identical permutation leaves all elements in their positions.

2. **Basic Operations**:
   - **Application of Permutation (apply_permutation)**: This function applies a given permutation to an array.
   - **Identity Check (is_identity)**: Determines if a permutation is the identity permutation, i.e., whether f[k]==k for all k=0...n-1.
   - **Fixed Points Count (count_fixed_points)**: Returns the number of fixed points in a permutation, where a fixed point is an index where the element is not moved.
   - **Derangement Check (is_derangement)**: Determines if a permutation is a derangement, i.e., whether f[k]!=k for all k. Two arrays are mutual derangements if fk ̸= gk for all k.
   - **Connected Permutation Check (is_connected)**: A connected permutation contains no proper preﬁx mapped to itself. This function checks if max(f0, f1, ..., fk) > k for all k < n −1.
   - **Valid Permutation Check (is_valid_permutation)**: Verifies that each index in the valid range appears exactly once.

3. **Representation as Disjoint Cycles**: Every permutation consists entirely of disjoint cycles. A cycle is a subset of indices rotated by one position by the permutation. The inverse of a permutation can be found by reversing every arrow in each cycle.

4. **Cyclic Permutations**: A permutation consisting of exactly one cycle is called cyclic. There are (n −1)! cyclic permutations of n elements.

5. **Sign and Parity of a Permutation**: The sign of a permutation is +1 if the number of transpositions (cycles of length 2) is even, and -1 if it's odd. This parity is unique modulo 2.

6. **Compositions of Permutations**: Applying several permutations to an array, one by one, results in a composition of permutations. The operation of composition is not commutative.

7. **Inverse of a Permutation**: A permutation f is the inverse of g if f · g = id (identity permutation). The routine make_inverse computes the inverse of a given permutation. For in-place computation, each cycle is inverted.

8. **Involutions**: A permutation which is its own inverse is called an involution. This can be checked by verifying if max cycle length is <= 2.

The text also mentions routines and functions for generating random permutations, cyclic permutations, involutions, and derangements, as well as methods for applying special permutations like the revbin permutation, the Gray permutation, and matrix transposition. Additionally, it discusses dragon curves generated by string substitution based on radix-R counting, where R is a base (5, 7, 9, 13).


The given text discusses various methods for generating random permutations and applying them to arrays of data. Here's a detailed summary:

1. **Random Permutation**: The random_permute function (line 3) randomly permutes an array f of n elements using the Fisher-Yates shuffle algorithm, also known as Knuth shuﬄe. It iteratively selects a random index i between 0 and k (inclusive), then swaps the element at position k with the element at position i. This process continues until all elements have been visited.

2. **Random Cyclic Permutation**: The random_permute_cyclic function generates a random cyclic permutation of an array f. It works similarly to the previous function but only permutes the last k-1 elements, leaving the 0th element unchanged. This is known as Sattolo's algorithm.

3. **Random Prefix Permutation**: The random_permute_pref function generates a prefix of a random permutation. It fills the first m elements of an array f with a random permutation of the first m+1, m+2, ..., n elements. This is achieved by iterating over the desired positions k and selecting a random index i from the remaining n-k elements.

4. **Random Permutation with Prescribed Parity**: The random_permute_parity function generates a random permutation with a specified parity (even or odd number of transpositions). It does this by initially generating a random permutation and then adjusting it if necessary to achieve the desired parity.

5. **Random Permutation with Preserved Order of m Smallest Elements**: The random_ordm_permutation function generates a random permutation where the first m smallest elements are in ascending order. This is done by first generating a random permutation and then rearranging these m elements accordingly.

6. **Random Permutation with Specified Cycle Type**: The random_permute_cycle_type function generates a random permutation with a specified cycle type (i.e., the number of cycles of each length). This is achieved by iteratively creating cycles of the desired lengths using the random_cycle helper function.

These functions provide various ways to generate and apply random permutations, which are useful in many applications such as simulations, cryptography, and combinatorial algorithms. The efficiency and effectiveness of these methods depend on the specific requirements of the task at hand.


The text discusses several methods for generating random permutations, focusing on specific types such as self-inverse (involution) permutations, derangements (permutations without fixed points), connected permutations, and the revbin permutation. Here's a detailed summary of each method:

1. **Random Self-Inverse Permutation:**
   - A self-inverse permutation, or involution, is a permutation that is its own inverse.
   - The probability of generating a 2-cycle (a pair of elements swapped) at each step is R(n) = I(n−1)/I(n), where I(n) is the number of involutions of n elements.
   - To avoid overflow issues with large n, R(n) is updated using a numerically stable recurrence relation: R(n + 1) = 1 / (1 + n * R(n)).
   - The routine `random_permute_self_inverse` generates such permutations by randomly deciding whether to create a 2-cycle or a fixed point based on the probability R(n).

2. **Random Derangement:**
   - A derangement is a permutation without any fixed points (elements that map to themselves).
   - The probability of generating a cycle closure at each step is B(n) = (n−1) D(n−1)/D(n), where D(n) is the number of derangements of n elements.
   - For large n, B(n) is close to 1/n, and a table of precomputed values can be used for efficiency.
   - The routine `random_derange` generates such permutations by randomly deciding whether to join two cycles or create a fixed point based on the probability B(n).

3. **Random Connected Permutation:**
   - A connected permutation is an indecomposable permutation, meaning it cannot be split into smaller non-trivial permutations.
   - The routine `random_connected_permutation` generates such permutations using the rejection method: it repeatedly generates random permutations until a connected one is obtained. This method is efficient because the probability of generating a disconnected permutation decreases with n.

4. **Revbin Permutation:**
   - The revbin permutation swaps elements whose binary indices are mutual reversals (bit-reversal or bitrev permutation). For example, for length n = 256, element x = 4310 = 001010112 is swapped with the element whose index is ˜x = 110101002 = 21210.
   - The key optimization for generating this permutation is updating only the bit-reversed values, using the revbin_upd function to compute ]x + 1 efficiently.
   - Further optimizations can be made by exploiting symmetries in the permutation, such as recognizing that swaps for certain pairs are independent and can be processed simultaneously.

Each of these methods has its applications in various fields, including numerical analysis, cryptography, and random number generation. The revbin permutation, in particular, is useful in fast Fourier transform (FFT) algorithms due to its ability to rearrange data efficiently.


The text discusses several array permutations, which are operations that rearrange elements within an array according to specific rules. Here's a detailed explanation of each permutation:

1. **Radix Permutation**: This is a generalization of the revbin permutation (discussed later) to arbitrary radices. In radix r, pairs of indices with reversed values are swapped. For example, in radix 10 and n = 1000, elements with indices 123 and 321 would be swapped. The radix permutation is self-inverse; meaning applying it twice will return the original array. The code for this permutation is provided in [FXT: perm/radixpermute.h], but it must be called with n as a perfect power of r.

2. **In-place Matrix Transposition**: This involves swapping elements in an array to transpose a matrix without using extra memory. For non-square matrices, this gets complex due to the need to identify cycles of the underlying permutation. When n (the total number of elements) is a power of 2, multiplications modulo n - 1 are cyclic shifts, making computations simpler and avoiding overflows.

3. **Rotation by Triple Reversal**: This technique rotates an array by s positions using three reversals without temporary memory. For left rotation (move elements towards the start), it reverses the first s elements, then reverses the remaining n-s elements, and finally reverses the whole array. Right rotation is achieved similarly but with reversed steps. This trick can also be used to swap two blocks in an array by reversing four ranges (first block, range between blocks, last block, and the entire array).

4. **Zip Permutation**: This permutation moves elements from the lower half of the array to even indices and those from the upper half to odd indices. It requires an even-sized array. The routine is straightforward, with one loop filling even indices and another for odd ones. The inverse, unzip, performs the reverse operation. If the array size n is a power of 2, zip can be computed as a transposition of a 2 × n/2 matrix using revbin permutations.

5. **XOR Permutation**: This permutation swaps an element at index k with another at index x XOR k. It's self-inverse and requires that the array length n is divisible by the smallest power of 2 greater than x for correct operation (e.g., if x = 1, n must be even; if x = 2 or 3, n must be divisible by 4). 

Each of these permutations has its use cases depending on the specific requirements of the task at hand, such as memory constraints, computational efficiency, and the desired end result of rearranging array elements.


Selection sort is a simple sorting algorithm that has a time complexity of O(n^2), making it less efficient for large datasets compared to other algorithms like quicksort or mergesort. However, its simplicity makes it suitable for small arrays or educational purposes.

Here's how selection sort works:

1. The array is divided into two parts: the sorted section at the left end and the unsorted section at the right end. Initially, the sorted section contains only the first element of the array, while the rest forms the unsorted section.

2. In each iteration (from i = 0 to n-1), the algorithm searches for the minimum value in the unsorted part of the array (i.e., from index i to n-1).

3. Once found, this minimum element is swapped with the first element of the unsorted section (index i). This effectively extends the sorted section by one element.

4. The process repeats for the remaining elements in the unsorted section until the entire array is sorted.

Here's a step-by-step breakdown using the example string 'nowsortme':

1. Start with the first character, 'n'. This becomes the minimum so far, and it's placed at position 0 (the sorted section).

2. Move to the next unsorted element ('o'). Since 'o' is less than 'n', we update our minimum and its position to ('o', 1).

3. Continue this process for all elements:

   - For 'w' at index 2, it's greater than the current minimum 'o', so no change.
   - 's' at index 3 is less than 'o', update minimum to 's' and position to 2.
   - 'r' at index 4 is greater than 's', no change.
   - 't' at index 5 is less than 's', update minimum to 't' and position to 4.
   - 'm' at index 6 is less than 't', update minimum to 'm' and position to 5.
   - 'e' at index 7 is greater than 'm', no change.
   - 'o' at index 8 is equal to the current minimum, so no change (duplicate elements can be handled differently based on requirements).

4. After iterating through all indices, the sorted array is ['e', 'm', 'n', 'o', 'o', 'r', 's', 't', 'w'].

The FXT library implementation for selection sort in C++ is as follows:

```cpp
template <typename Type>
void selection_sort(Type *f, ulong n) {
    // Sort f[] (ascending order).
    // Algorithm is O(n*n), use for short arrays only.
    for (ulong i = 0; i < n; ++i) {
        Type v = f[i]; // Store the current minimum value
        ulong m = i;   // Position of the minimum
        ulong j = n;    // Search from the end of the unsorted section

        while (--j > i) {
            if (f[j] < v) {
                m = j;  // Update position of minimum
                v = f[m]; // Update current minimum value
            }
        }

        // Swap minimum with first element of unsorted section
        if (m != i) {
            Type temp = f[i];
            f[i] = v;
            v = temp;
            m = i;
        }
    }
}
```

Key points to remember:
- Selection sort has a time complexity of O(n^2), making it inefficient for large datasets.
- It's simple and easy to understand, with minimal memory overhead (O(1) additional space).
- The algorithm is stable, meaning that equal elements maintain their relative order.
- Use selection sort when dealing with small arrays or educational purposes due to its simplicity.


The text discusses several sorting algorithms, their complexities, and variations. Here's a detailed summary:

1. **Verification Routines**: Two functions are provided for checking the order of sequences:
   - `is_sorted<Type>(const Type *f, ulong n)`: Checks if a sequence is in ascending order.
   - `is_falling<Type>(const Type *f, ulong n)`: Checks if a sequence is in descending order.

2. **Quicksort**: This algorithm has an average-case complexity of O(n log(n)). It works by partitioning the array and recursively sorting the sub-arrays on either side of the partition index. The partition function rearranges the array such that all elements to the left of the pivot are less than or equal to it, and all elements to the right are greater than or equal to it. To avoid worst-case scenarios with already sorted input, a median-of-three method is used to select the pivot.

3. **Counting Sort**: This algorithm sorts an n-element array of unsigned 8-bit values in two passes through the data. It allocates an array C and counts occurrences of each value in the input array F. Then, it reorders F based on these counts, preserving the relative order of equal elements (a stable sort). The method can be extended to sort larger integer variables by using a bit mask and a mapping function M.

4. **Radix Sort**: This algorithm sorts an n-element array of unsigned integers by processing each bit position in turn. It uses counting, cumulating, and copying stages to sort the data based on specific bit ranges. The routine `radix_sort` implements this method for 8-bit integers, while `counting_sort_core` handles the core counting sort logic for arbitrary bit ranges.

5. **Merge Sort**: Merge sort is a divide-and-conquer algorithm with a complexity of O(n log(n)). It sorts an array by recursively dividing it into smaller sub-arrays, sorting those sub-arrays, and merging them back together. The routine `merge_sort` performs this operation, while `merge_sort_rec4` implements a 4-way merge sort for improved locality of reference.

6. **Heapsort**: Heapsort is another O(n log(n)) algorithm that uses a heap data structure to sort an array. It builds a heap from the input array, repeatedly extracts the maximum element (or minimum for descending order), and restores the heap property. The routine `heap_sort` implements this method.

7. **Binary Search**: Binary search is an efficient O(log(n)) algorithm for searching in sorted arrays. It repeatedly divides the search interval in half, comparing the target value with the middle element to determine which half to continue searching in. The function `bsearch` performs this operation, while `bsearch_geq` and `bsearch_approx` offer variations that search for elements greater than or equal to a given value and approximate matches within a certain tolerance, respectively.

8. **Variants of Sorting Methods**:
   - **Index Sorting**: This involves sorting an array of indices x such that f[x[k]] is in ascending order. The routine `idx_selection_sort` demonstrates this using the selection sort algorithm for short arrays.

These algorithms and routines provide various options for sorting and searching data, each with its strengths and trade-offs depending on the specific use case and data characteristics.


Title: Summary and Explanation of Equivalence Classes Algorithm and Examples

1. **Algorithm for Decomposition into Equivalence Classes**

   The provided algorithm aims to determine the equivalence classes of a set S under a given equivalence relation. Here's a detailed breakdown:

   - **Initialization**: Each element in S is initially considered its own equivalence class (Qk = k for all 0 ≤k < n).
   - **Search for Equivalent Element**: The algorithm searches for an equivalent element to the current one by comparing it with previously processed elements (j) and setting their equivalence class index (Qk) to that of the equivalent element (Qj).
   - **Iterate**: The process continues until all elements have been checked, at which point each element is assigned to its respective equivalence class.

   The algorithm's time complexity depends on the number of equivalence tests required: n-1 tests for when all elements are in the same class and n(n-1)/2 tests for when each element is alone in its own class.

2. **Equivalence Relation as a Function**

   In this context, an equivalence relation (equiv_q) is treated as a function that returns true if two input arguments are equivalent according to the given relation. This function is supplied by the user and must conform to the properties of reflexivity, symmetry, and transitivity for it to qualify as an equivalence relation.

3. **Examples of Equivalence Classes**

   The provided examples illustrate various applications of the equivalence classes concept:

   - **Integers Modulo m**: Two integers a and b are equivalent if (a-b) is a multiple of some fixed integer m > 0. The set Z of natural numbers partitions into m subsets, with x ≡ y if and only if x ≡ y (mod m).
   - **Binary Necklaces**: In this case, two n-bit binary words are equivalent if there exists a cyclic shift by k positions (0 ≤k < n) such that the shifted word equals the other. The equivalence relation is captured by the function `n_equiv_q()`. For example, with n = 4, we obtain six distinct classes.
   - **Unlabeled Binary Necklaces**: Similar to binary necklaces but also considers complements of words. Two words are equivalent if one can be obtained from the other through a cyclic shift or bitwise complementation. This is represented by `nu_equiv_q()`. For n = 4, there are four such classes.
   - **Binary Bracelets**: Two bracelets (n-bit binary words) are equivalent if they match after rotation and possible reversal. The equivalence relation is captured by the function `b_equiv_q()`, yielding six distinct classes for n = 4.
   - **Unlabeled Binary Bracelets**: Similar to binary bracelets but allows bitwise complementation in addition to rotation and reversal. This results in four equivalence classes for n = 4, as represented by the function `bu_equiv_q()`.

Each example demonstrates how the concept of equivalence classes can be applied to different types of data, providing a way to group similar elements based on a user-defined relation while preserving essential properties.


A queue (FIFO - First In, First Out) is a linear data structure that follows the principle of "first come, first served." It has two primary operations:

1. **Push()**: This operation adds an element to the rear (back) of the queue. If the queue is not full, the new element is inserted, and the position counter (often referred to as 'wpos' for write position) is incremented.

2. **Pop()**: This operation removes the front (first) element from the queue. It retrieves this element and updates the position counter (often referred to as 'rpos' for read position). If the queue becomes empty after removal, 'rpos' is reset to zero or an appropriate value indicating an empty state.

3. **Peek()**: This operation allows you to examine the front element without removing it. Like pop(), it retrieves this element and updates the 'rpos', but does not alter the queue's size.

Here is a simple implementation of a Queue using C++:

```cpp
#include <vector>
#include <stdexcept> // for std::out_of_range

template<typename T>
class Queue {
    private:
        std::vector<T> data; // The queue's storage
    public:
        void push(const T& value) {
            data.push_back(value);
        }

        T pop() {
            if (data.empty()) {
                throw std::out_of_range("Queue is empty");
            }
            T front = data.front();
            data.erase(data.begin()); // Remove the first element
            return front;
        }

        const T& peek() const {
            if (data.empty()) {
                throw std::out_of_range("Queue is empty");
            }
            return data.front();
        }
};
```

In this implementation:

- `push(const T& value)` adds an element to the end of the vector, which acts as our queue.
- `pop()` removes and returns the first element in the vector. If the queue is empty, it throws an exception (`std::out_of_range`).
- `peek() const` returns a constant reference to the front element without removing it. Again, if the queue is empty, it throws an exception.

This simple Queue uses a dynamic array (vector) to store elements, allowing for efficient insertion and removal of elements from either end. The 'front' of the queue is always the first element in this vector, and the 'rear' (or 'back') is the last position where an element was inserted.


The provided text discusses two data structures: Queue and Deque, along with a brief overview of Heap and Priority Queue. I will summarize and explain each section in detail.

1. Queue:
A queue is a First-In-First-Out (FIFO) data structure where elements are added at the rear (push operation) and removed from the front (pop operation). The given code implements a dynamic queue that grows when necessary, with an optional growth feature controlled by `gq_` variable.

   - Class definition:
     ```cpp
     template <typename Type>
     class queue {
     public:
       Type *x_; // pointer to data
       ulong s_;  // allocated size (# of elements)
       ulong n_;  // current number of entries in buffer
       ulong wpos_;// next position to write in buffer
       ulong rpos_;// next position to read in buffer
       ulong gq_;  // grow gq elements if necessary, 0 for "never grow"
     };
     ```
   - Constructor initializes the queue with a given size and growth parameter:
     ```cpp
     explicit queue(ulong n, ulong growq=0) {
       s_ = n;
       x_ = new Type[s_];
       n_ = 0;
       wpos_ = 0;
       rpos_ = 0;
       gq_ = growq;
     }
     ```
   - push operation inserts an element at the rear and increments the write position (`wpos_`):
     ```cpp
     ulong push(const Type &z) {
       if (n_ >= s_) {
         if (0 == gq_) return 0; // growing disabled
         grow();
       }
       x_[wpos_] = z;
       ++wpos_;
       if (wpos_ >= s_) wpos_ = 0;
       ++n_;
       return n_;
     }
     ```
   - peek operation reads from the front without removing it:
     ```cpp
     ulong peek(Type &z) {
       z = x_[rpos_];
       return n_;
     }
     ```
   - pop operation removes and returns the element at the front:
     ```cpp
     ulong pop(Type &z) {
       ulong ret = n_;
       if (0 != n_) {
         z = x_[rpos_];
         ++rpos_;
         if (rpos_ >= s_) rpos_ = 0;
         --n_;
       }
       return ret;
     }
     ```
   - grow method resizes the internal buffer by doubling its size (`gq_`):
     ```cpp
     private:
       void grow() {
         ulong ns = s_ + gq_; // new size
         rotate_left(x_, s_, rpos_);
         x_ = ReAlloc<Type>(x_, ns, s_);
         wpos_ = s_;
         rpos_ = 0;
         s_ = ns;
       }
     ```

2. Deque (Double-Ended Queue):
A deque allows insertion and deletion at both ends efficiently (O(1) time complexity). The provided code includes a dynamic deque that grows when necessary.

   - Class definition:
     ```cpp
     template <typename Type>
     class deque {
     public:
       Type *x_; // data (ring buffer)
       ulong s_;  // allocated size (# of elements)
       ulong n_;  // current number of entries in buffer
       ulong fpos_;// position of first element in buffer
       ulong lpos_;// position of last element in buffer plus one
       ulong gq_;  // grow gq elements if necessary, 0 for "never grow"
     };
     ```
   - Constructors and destructors are similar to the queue implementation.
   - insert_first operation adds an element at the front:
     ```cpp
     ulong insert_first(const Type &z) {
       if (n_ >= s_) {
         if (0 == gq_) return 0; // growing disabled
         grow();
       }
       --fpos_;
       if (fpos_ == -1UL) fpos_ = s_ - 1;
       x_[fpos_] = z;
       ++n_;
       return n_;
     }
     ```
   - insert_last operation adds an element at the rear:
     ```cpp
     ulong insert_last(const Type &z) {
       if (n_ >= s_) {
         if (0 == gq_) return 0; // growing disabled
         grow();
       }
       x_[lpos_] = z;
       ++lpos_;
       if (lpos_ >= s_) lpos_ = 0;
       ++n_;
       return n_;
     }
     ```
   - extract_first and extract_last operations remove and return elements from the front and rear, respectively:
     ```cpp
     ulong extract_first(Type & z) {
       if (0 == n_) return 0;
       z = x_[fpos_];
       ++fpos_;
       if (fpos_ >= s_) fpos_ = 0;
       --n_;
       return n_ + 1;
     }

     ulong extract_last(Type & z) {
       if (0 == n_) return 0;
       --lpos_;
       if (lpos_ == -1UL) lpos_ = s_ - 1;
       z = x_[lpos_];
       --n_;
       return n_ + 1;
     }
     ```
   - read operations allow reading elements without removing them:
     ```cpp
     ulong read_first(Type & z) const {
       if (0 == n_) return 0;
       z = x_[fpos_];
       return n_;
     }

     ulong read_last(Type & z) const {
       return read(n_-1, z); // ok for n_==0
     }

     ulong read(ulong k, Type & z) const {
       if (k >= n_) return 0;
       ulong j = fpos_ + k;
       if (j >= s_) j -= s_;
       z = x_[j];
       return k + 1;
     }
     ```
   - grow method resizes the internal buffer by doubling its size (`gq_`):
     ```cpp
     private:
       void grow() {
         ulong ns = s_ + gq_; // new size
         rotate_left(x_, s_, fpos_);
         x_ = ReAlloc<Type>(x_, ns, s_);
         fpos_ = 0;
         lpos_ = n_;
         s_ = ns;
       }
     ```

3. Heap and Priority Queue:
Heaps are binary trees where the parent node is less than or equal to its children (max-heap). The provided code includes a function to test if an array represents a heap, as well as functions for heapify and build_heap operations.

   - Test heap function checks whether an array has heap property:
     ```cpp
     template <typename Type>
     ulong test_heap(const Type *x, ulong n) {
       const Type *p = x - 1; // make one-based
       for (ulong k = n; k > 1; --k) {
         ulong t = (k >> 1); // parent(k)
         if (p[t] < p[k]) return k - 1; // in {1, 2, ..., n}
       }
       return 0; // has heap property
     }
     ```
   - Heapify function ensures the heap property between a node and its children:
     ```cpp
     template <typename Type>
     void heapify(Type *z, ulong n, ulong k) {
       ulong m = k; // index of max of k, left(k), and right(k)
       const ulong l = (k << 1); // left(k);
       if ((l <= n) && (z[l] > z[k])) m = l; // left child (exists and) greater than k
       const ulong r = (k << 1) + 1; // right(k);
       if ((r <= n) && (z[r] > z[m])) m = r; // right child (ex. and) greater than max(k,l)
       if (m != k) {
         swap2(z[k], z[m]);
         heapify(z, n, m);
       }
     }
     ```
   - Build heap function reorders an array into a heap from the bottom up:
     ```cpp
     template <typename Type>
     void build_heap(Type *x, ulong n) {
       Type *z = x - 1; // make one-based
       ulong j = (n >> 1); // max index such that node has at least one child
       while (j > 0) {
         j = (j == 1) ? 0 : ((j - 1) / 2);
         heapify(z, n, j);
       }
     }
     ```


The provided text discusses several data structures and their implementations: Heap, Priority Queue, Bit-array, and Left-right array.

1. **Heap**: A heap is a specialized tree-based data structure that satisfies the heap property. The root of the tree is the largest (max-heap) or smallest (min-heap) element. The given text describes a max-heap implementation in C++. The `heapify` function ensures the heap property by comparing and swapping elements, with a time complexity of O(log n). Insertion into a heap takes O(log n) time, as does extraction of the maximum (or minimum) element.

2. **Priority Queue**: A priority queue is an abstract data type that supports insertion and removal of elements according to their priority. It can be implemented using a binary heap. The given text describes a generic priority queue class in C++ that can extract either the maximum or minimum element, depending on compile-time options. This priority queue maintains two arrays: one for storing times (t1_) and another for storing associated events (e1_).

3. **Bit-array**: A bit-array is an array of Boolean values used to save memory when dealing with small data types. The provided text describes a C++ class `bitarray` that offers methods for testing, setting, and clearing bits. It also includes optimized versions using CPU instructions (for AMD64 architecture) or macros for other architectures.

4. **Left-right array**: A left-right array (LR-array) is a data structure used to keep track of free and set indices in an array. It supports marking the k-th free index as set, marking the k-th set index as free, and finding how many indices of the same type are left or right to any given index. The given text presents a C++ class `left_right_array` that uses binary search to implement these operations in O(log n) time.

The LR-array's key feature is its ability to efficiently mark free and set indices using a recursive initialization method (`init_rec`) and two main methods:

   - `get_free_idx(ulong k)` returns the k-th free index, with a time complexity of O(log n).
   - `get_set_idx_chg(ulong k)` returns the k-th set index while changing it to a free index, also with a time complexity of O(log n).

These operations are essential for managing dynamic arrays or other data structures that require efficient manipulation of free and set indices.


The text discusses the generation of combinations, which are subsets of a set containing a specific number of elements. Combinations are also known as "k-subsets" or "k-combinations" of an n-set (a set with n elements). The number of ways to choose k elements from a set of n elements is given by the binomial coefficient, denoted as "n choose k" or "k out of n":

(n choose k) = n! / (k!(n - k)!)

or equivalently, using falling factorial notation:

(n choose k) = n^(k) / k!

This coefficient is also known as the binomial coefficient and can be calculated using the formula:

(n choose k) = [(n - k + 1) * (n - k + 2) * ... * n] / k!

Two common orders for generating combinations are lexicographic (lex) and co-lexicographic (colex). 

Lexicographic order generates the combinations in increasing dictionary order, where each combination is smaller than the next one. For example, when generating 3-combinations from {0,1,2}, the sequence would be: {{0,1,2}}, {{0,1,2},{0,2,1},{1,0,2},{1,2,0},{2,0,1},{2,1,0}}.

Co-lexicographic order generates combinations in decreasing dictionary order when the elements are reversed. For example, in co-lexicographic order, 3-combinations from {0,1,2} would be: {{0,1,2}},{{0,1,2},{0,2,1},{1,2,0},{2,1,0},{2,0,1},{1,0,2}}.

The text provides C++ class implementations for generating combinations in both lexicographic and co-lexicographic orders. These classes store the combination as an array of integers, where each integer is less than or equal to k (the number of elements in the combination) and greater than or equal to 0. The classes also include methods for initializing a new combination, computing successors and predecessors, and accessing the current combination.

The lexicographic order class (`combination_lex`) generates combinations by incrementing the smallest possible element that can be increased without violating the order constraints. If no such element exists (i.e., the current combination is the last one), it resets to the first combination.

The co-lexicographic order class (`combination_colex`) works similarly but increments elements from right to left, reversing the order before applying the increment and then reverting it back afterward. This process ensures that combinations are generated in decreasing lexicographic order when viewed as arrays of integers.

These implementations provide efficient ways to generate all possible combinations of a given size from an n-element set, which can be useful for various combinatorial problems and applications.


The Eades-McKay strong minimal-change order (SMMC) is an improvement over the standard Gray code for generating combinations. In a standard Gray code, only one element changes position between two successive combinations, but when an element moves across another, multiple elements in the set representation change. The SMMC aims to minimize these additional changes.

In the SMMC order, each combination is represented as a "delta-set," which is a binary vector where 1s denote elements included and 0s denote excluded elements. Two consecutive combinations differ by exactly one bit flip (from 0 to 1 or from 1 to 0). This property ensures that only two elements change in the set representation for any pair of adjacent combinations, hence the "strong minimal-change" name.

To illustrate this order, let's consider an example with n = 6 and k = 3:

| Delta-set | Set Representation          | Elements Changed         |
|------------|-----------------------------|--------------------------|
| {0,1,2}    | {0,1,2}                     | {3,4,5}                  |
| {0,1,3}    | {0,1,3}                     | {2,4,5}                  |
| {0,2,3}    | {0,2,3}                     | {1,4,5}                  |
| {1,2,3}    | {1,2,3}                     | {0,4,5}                  |
| {0,1,4}    | {0,1,4}                     | {2,3,5}                  |
| {0,2,4}    | {0,2,4}                     | {1,3,5}                  |
| {1,2,4}    | {1,2,4}                     | {0,3,5}                  |
| {0,3,4}    | {0,3,4}                     | {1,2,5}                  |
| {1,3,4}    | {1,3,4}                     | {0,2,5}                  |
| {2,3,4}    | {2,3,4}                     | {0,1,5}                  |

As shown in the table, each pair of consecutive combinations differs by exactly one bit flip. This is why SMMC is called "strong minimal-change," as it ensures that only two elements change for any adjacent combination pairs. 

The Eades-McKay algorithm generates these delta-sets systematically to ensure this property holds. The generation process involves a careful manipulation of the set sizes and element placements, ensuring that each flip results in the minimal possible changes across adjacent combinations. This order is valuable for applications where minimizing changes between successive elements is crucial, such as in optimization algorithms or when simulating dynamic systems.


The text discusses two specific ordering methods for combinations of numbers, known as the Eades-McKay (EMK) order and two-close orderings via endo/enup moves. 

1. **Eades-McKay (EMK) Order**: This is a strong minimal-change order where only one entry in the set representation changes per step, making it a Gray code with homogeneous moves. The EMK sequence can be generated recursively or iteratively. 

   - **Recursive Generation**: The recursive algorithm for generating combinations in EMK order is provided. It involves three cases: increasing the last element, decreasing the second-to-last while keeping the last fixed, and keeping the second-to-last fixed while increasing the last. 

   - **Iterative Generation via Modulo Moves**: An iterative algorithm for generating combinations in EMK order is also presented. This method uses modulo steps to compute successors. The rate of generation varies depending on the number of elements (n) and combination size (k). 

2. **Two-close Orderings via Endo/Enup Moves**: These are two different ways to generate sequences with specific properties related to even and odd numbers. 

   - **Endo Sequence**: This order starts with all odd numbers from 1 up to m in increasing order, followed by all even numbers from m down to 0 in decreasing order. A routine for generating the successor in endo order is provided. 

   - **Enup Sequence (Even Numbers Up, Odd Numbers Down)**: This sequence starts with all even numbers from 0 up to m in increasing order, followed by all odd numbers from m down to 1 in decreasing order. The routine for generating the successor in enup order is also provided. These two sequences are reversals of each other.

The text concludes by defining functions to compute the x-th number in enup order with a maximal digit (m) and the previous number in both endo and enup orders for a given number (x).


The provided text describes four different orderings for generating combinations, which are visualized in Figure 6.7-A. Here's a detailed explanation of each ordering:

1. **Lexicographic Order**: This is the natural, increasing order where elements are arranged from smallest to largest. In other words, it follows the dictionary order. For example, for the set {0, 1, 2}, the lexicographic combinations are (0, 1, 2), (0, 2, 1), (1, 0, 2), etc.

2. **Gray Code**: Gray code is a binary numeral system where two successive values differ in only one bit. This property makes it useful for applications requiring minimal change between adjacent values, like in error correction and digital communications. In the context of combinations, the Gray code ordering ensures that each combination differs from its neighbors by just one element.

3. **Complemented Enup Order**: Enup stands for "even-numbered up." This order is generated using an algorithm called `endo_num`. It maps numbers to combinations in a way that moves right on even positions. The term "complemented" means that the sequence considers complements of sets (i.e., all possible subsets except the set itself) when generating combinations.

4. **Complemented Eades-McKay Sequence**: This is another ordering for generating combinations, similar to the Complemented Enup Order but following a different algorithm. The Eades-McKay sequence was developed by David Eades and John McKay in their 1992 paper "A new Gray code for combinations." Like the Complemented Enup Order, it considers complements of sets when generating combinations.

The text also presents a recursive algorithm to generate these orders using a class named `comb_rec`. This algorithm accepts a visitor function that gets called with each generated combination and parameters determining which order (`rq_`) to use (lexicographic, Gray code, complemented enup, or complemented Eades-McKay) and whether to reverse the order (`nq_`).

The recursion is implemented in the `next_rec()` function. It uses an array `rv_` to store the current combination and updates it based on the chosen order. The conditions for updating the combination are controlled by the `rq_` parameter, which can take values 0 (lexicographic), 1 (Gray code), 2 (complemented enup), or 3 (complemented Eades-McKay).

In summary, this section of the text discusses four different ways to order combinations and provides a flexible recursive algorithm for generating these orders using customizable parameters.


The provided text discusses two topics related to combinatorics, specifically compositions and combinations, along with their respective generation methods and transformations between them.

**Compositions:**

A composition of a non-negative integer `n` into at most `k` parts is an ordered sequence of non-negative integers `(x0, x1, ..., xk-1)` such that `x0 + x1 + ... + xk-1 = n`, and each part `xi` is less than or equal to `n`. Order matters in compositions; different sequences are considered distinct even if they sum up to the same value. For instance, `(0, 1, 5, 1)` and `(5, 0, 1, 1)` are separate 4-compositions of 7.

Two algorithms for generating compositions in co-lexicographic order (colex) were presented:

1. `composition_colex`: This algorithm uses an array to store the composition's parts and has methods to set the first (`first()`) and last (`last()`) composition, as well as to compute successor (`next()`) and predecessor (`prev()`) compositions. The complexity of this implementation is efficient for dense cases where `k` (number of parts) is much greater than `n` (sum).

2. `composition_ex_colex`: This optimized algorithm handles sparse cases efficiently by introducing an additional variable (`p0_`) to track the position of the first non-zero entry. The methods for computing successor and predecessor are adaptations from the `composition_colex` class, making it equally fast for dense or sparse compositions.

**Combinations:**

A combination is a selection of items without regard to their order. In this context, the text refers to combinations in terms of choosing `K` elements out of `N`, denoted as `B(N, K)`. The relationship between compositions and combinations is established through delta sets:

- A run of `r` consecutive ones in a binary representation corresponds to an entry `r` in a composition at the corresponding position.

The text mentions that while the sequence of combinations can be represented as a Gray code (a binary reflected Gray code), the sequence of compositions is not a Gray code.

**Conversion between Compositions and Combinations:**

The provided text includes a conversion routine `comp2comb` to transform a given composition into its corresponding combination:

1. Input:
   - `p[]`: Array representing the composition.
   - `k`: Number of parts in the composition.
   - `b[]`: Output array for storing the combination (initialized with enough space).

2. Process:
   - Iterate through each part of the composition (`j` from 0 to `k-1`).
   - For each part (`pj`), add that number of consecutive values starting from `z` to the output combination array `b[]`.
   - Increment `z` to point to the next available value.

This routine transforms a composition into its equivalent combination by assigning values in the output array based on the length and position of each part within the input composition.


The text discusses two methods for generating all subsets of a set with n elements, namely lexicographic order with sets and delta sets. 

1. Lexicographic Order with Sets:
   - This method represents each subset as a binary array where 1s denote the presence of an element in the subset and 0s denote its absence. 
   - The class `subset_lex` in the FXT library implements this approach. It initializes an array `x_` of size n, with the first call to `first()` setting all elements to zero (representing the empty set).
   - The `next()` function increments the last non-zero element in the binary representation and returns its index plus one, which corresponds to the number of elements in the new subset.
   - The generation speed is around 176 million subsets per second using standard arrays or approximately 192 M/s with optimized array usage.

2. Lexicographic Order with Delta Sets:
   - In this method, each subset is represented as a delta set, where the i-th element indicates whether element i is included in the subset (non-zero) or not (zero). 
   - The class `subset_deltalex` in the FXT library implements this approach. It initializes an array `d_` of size n+1 with sentinel value 0 at the end.
   - The `first()` function sets all elements to zero, representing the empty set. 
   - The `next()` function uses binary counting to find the next non-zero element, increments it by one, and returns its index plus one (representing the number of elements in the new subset).
   - This method generates around 176 million subsets per second using standard arrays or approximately 192 M/s with optimized array usage.

Both methods generate all nonempty subsets of a set with n elements, totaling 2^n - 1 subsets. The main difference lies in the representation: sets use binary arrays (0s and 1s), while delta sets represent each subset as an array where non-zero values indicate included elements.

The lexicographic order ensures that subsets are generated in a systematic, ascending manner. By changing the direction of increments at odd or even positions in the recursion formula for generating combinations (as shown in Figure 7.4-A and B), different minimal-change orders (Gray codes) can be obtained to generate compositions and combinations with specific properties.


The provided text discusses different methods for generating subsets of a given set in an order that minimizes changes between consecutive subsets. This is known as the "minimal-change order". The following are the three main approaches outlined:

1. **Gray Code with Delta Sets:**
   - This method represents each subset as a delta-set, where only the difference from the previous subset is stored. 
   - The class `subset_gray_delta` implements this approach. It uses the Gray code of binary words to determine the order in which subsets are generated.
   - When generating the next subset, it finds the position where the Gray code changes and toggles that bit. The position of change (`j_`) is then returned. 
   - The `prev()` function works similarly but in reverse, decrementing the counter until it reaches zero.
   - This method can generate about 180 million subsets per second.

2. **Gray Code with Set Representation:**
   - In this approach, each subset is represented as a set of numbers from 1 to n. The class `subset_gray` implements this.
   - It uses the Gray code for minimal-change order but represents subsets differently: `x_[k_] = n_` means that element `n_` is included in position `k_`. 
   - The `next()` and `prev()` methods adjust the subset by either removing or adding elements based on specific conditions.
   - This method can generate about 241 million subsets per second with `next()`, and around 167 M/s with `prev()`.

3. **Computing Positions of Change (Ruler Function):**
   - Unlike the previous methods that generate full subsets, this class `ruler_func` only computes the positions where changes occur in generating subsets in Gray code order. 
   - It uses a technique called the "Ruler function sequence", which produces a sequence of numbers where each number indicates the position of change when generating a Gray code.
   - The `next()` method returns these positions of change, allowing users to generate subsets or perform other operations based on these positions.
   - This method can generate about 244 M/s with pointers and 293 M/s with arrays.

Each method has its own trade-offs in terms of memory usage, computational complexity, and speed. The choice between them would depend on the specific requirements and constraints of the application at hand.


This text discusses various methods for ordering subsets of a given set using different sequences or algorithms, with a focus on De Bruijn sequences and shift-based orders. Here's a detailed summary and explanation:

1. **De Bruijn Sequence Ordering**: This method generates an ordering for all subsets of a given set based on a cyclic binary sequence (De Bruijn sequence). Each subset is the result of shifting its predecessor to the right and adding the next element from the De Bruijn sequence. The example provided is for a 5-element set, with the De Bruijn sequence being:

   `10001100101011101000`

   This ordering has the property that each column in the delta set representation (a visualization of subsets) is a circular shift of the first column.

2. **Sequency-Complemented Subsets**: By complementing elements at even indices, an alternative ordering can be obtained. This approach generates more frequent transitions between subsets with smaller sequencies (a measure of the distance between binary sequences). The resulting order corresponds to a complement-shift sequence.

3. **Shifts-Order for Subsets**: Figure 8.4-A illustrates an ordering where all linear shifts of a word appear in succession. This order is generated using a simple recursive algorithm (line 4-15). The transitions not involving shifts change just one bit, resulting in a minimal-change shifts-order as shown in Figure 8.4-B.

4. **Minimal-Change Shifts-Order**: This order ensures that transitions not involving shifts change only one bit (Figure 8.4-B). The recursion functions `F` and `G` are used to generate this ordering, with `F(2*x)` for non-shift transitions and `G(2*x+1)` for shift transitions.

5. **Fibonacci Words in Shifts-Order**: A simple variation of the previous algorithm can be employed to generate Fibonacci words (infinite sequences where each finite prefix appears as a substring) in an order where all shifts appear in succession (Figure 8.4-C). The recursion function `B(ulong x)` is used for this purpose, with transitions that generally change more than one bit.

6. **k-subsets within a Range**: The text also covers algorithms to generate k-subsets of an n-set where the value of k lies in a given range (kmin ≤ k ≤ kmax). It introduces a recursive algorithm (`class ksubset_rec`) that generates subsets in one of sixteen possible orders, determined by the variable `rq`. The order can be lexicographic or Gray codes, and it accepts a user-defined function to process each subset.

In summary, this text explores different ways to order subsets of a set using sequences (De Bruijn) and recursive algorithms that generate patterns like linear shifts or minimal bit changes between consecutive subsets. These methods find applications in combinatorics, coding theory, and other areas requiring systematic exploration of subset combinations.


The provided text describes two algorithms for generating k-subsets of a given set, where the size of the subsets (k) lies within a specified range. Both algorithms aim to produce these subsets in an order that minimizes changes between consecutive subsets, known as "minimal-change" or "Gray code" order.

1. Recursive Algorithm:

The recursive algorithm is defined by the `ksubset_rec` class and its methods. Here's a detailed explanation of the key components:

   - **Initialization (lines 2-18)**: The constructor initializes the parameters for subset generation, including the total number of elements (`n_`), the minimum (`kmin_`) and maximum (`kmax_`) allowed subset sizes, and some auxiliary variables. It ensures `kmin` is less than or equal to `kmax`, and if not, it swaps their values. If `kmin` is greater than `n`, it's set to `n`. Similarly, if `kmax` exceeds `n`, it's clipped at `n`.

   - **Next Subset Generation (lines 19-39)**: The `next_rec` method generates the next k-subset in the minimal-change order. It uses a helper function `next_rec(long d)` that navigates through the subsets level by level, incrementing or decrementing elements based on certain conditions determined by `rq_` and `pq_` variables. The changes depend on whether the current position is even or odd, and if a nonzero `nq` is provided, it reverses the order.

2. Iterative Algorithm:

The iterative algorithm generates k-subsets in minimal-change order using the `ksubset_gray` class. Here's an explanation of its key components:

   - **Initialization (lines 11-39)**: The constructor initializes parameters similarly to the recursive algorithm, with additional variables for tracking the current subset (`S_`) and position within it (`j_`). It sets up the initial state, ensuring `kmin` is at least 1.

   - **First Subset (lines 22-48)**: The `first()` method initializes the first k-subset in minimal-change order by populating the `S_` array with values from `n_ - kmin + i`, where `i` ranges from 1 to `kmin`.

   - **Last Subset (lines 30-39)**: The `last()` method generates the last k-subset in minimal-change order by setting `S_[1]` to 1 and adjusting subsequent values accordingly. It also updates the position counter `j_` based on the current value of `kmin`.

   - **Prev_Even/Odd Methods (lines 49-67)**: These private helper methods manage moving backward through the k-subsets, updating the `S_` array and adjusting the position counter `j_` as necessary. They handle cases where elements can touch the sentinel (`S[0]`) or not, and whether the current position is even or odd.

Both algorithms aim to generate k-subsets in a minimal-change order, which is useful for various applications such as combinatorial testing, data encoding, and more. The provided code snippets show how these algorithms can be implemented in C++.


The provided text describes several concepts related to number theory, specifically focusing on mixed radix numbers and their orders. Here's a detailed explanation:

1. **Mixed Radix Numbers**: A mixed radix representation of a number x is given by equation (9.0-1):

   x = ∑(k=0 to n-1) ak * ∏(j=0 to k-1) mj

   where 0 ≤ ak < mj, and the total sum should not exceed Qn⁻¹ j=0 mj, ensuring that n digits are sufficient to represent x. When all radii (mj) are equal (M = [r, r, ..., r]), this simplifies to the standard radix-r representation (9.0-2).

2. **Two-Close Order**: This is a specific ordering of k-subsets where one element is inserted or removed, or moves by at most two positions. The changes are homogenous, meaning they only cross a zero and all changes are either insertions or deletions, not a mix of both.

3. **Mixed Radix Lexicographic Order (mixedradix_lex)**: This class generates mixed radix numbers in lexicographic order. It uses an array 'a_' for digits and another array 'm1_' for the radices minus one. The constructor initializes these arrays, and a recursive function `next_rec()` is used to generate the next number in the sequence.

4. **Initialization (mixedradix_init)**: This is an auxiliary function that initializes the vector of nines (digits equal to the respective radix minus one) for mixed radix classes. If all radices are given, it directly assigns each radix-1 value; otherwise, if no radices are provided, it assumes a uniform radix r = m0 + 1.

5. **Various Orders**: The text mentions different visual orders for mixed radix numbers: counting (lexicographic), Gray code, modular Gray code, gslex, endo, and endo Gray. These orders represent the same set of numbers but in different sequences, which can be useful for various applications like error detection and correction in data transmission.

6. **Performance**: The text also provides performance metrics for generating subsets using these algorithms: about 150 million subsets per second with 'next()', 130 million with 'prev()', and 75 million with the two-close order algorithm. 

In summary, this text discusses mixed radix numbers, their representations, and different ordering methods. It also introduces a C++ class `mixedradix_lex` to generate mixed radix numbers in lexicographic order and provides context about related orders and performance metrics.


The text discusses various algorithms for generating mixed radix numbers, which are numbers represented in a non-standard positional numeral system. Mixed radix systems use different bases (radices) for each digit position rather than the uniform base we're used to in decimal (base 10).

**9.1: Lexicographic Order Generation**

The lexicographic order (dictionary order, alphabetical order) is a natural way to arrange mixed radix numbers. The given C++ code snippet provides two functions, `next()` and `prev()`, for generating the next and previous mixed radix numbers in lexicographic order, respectively. 

1. **Initialization**: The number of digits `n_` is initialized, and arrays `a_`, `m1_`, and `j_` are created to store the current number, its radices (minus one), and the position of the last change, respectively.

2. **`next()` function**: This increments the mixed radix number by setting all digits equal to their respective base minus one (`m1_[k] - 1`) at the lower end to zero and incrementing the next digit. It does this until it finds a digit that is less than its base, then increments it. 

3. **`prev()` function**: This decrements the mixed radix number by setting all zero digits at the lower end to their respective base minus one and decrementing the next digit. 

The time complexity for generating numbers in lexicographic order varies with the radix vector `M`. For a uniform radix (all elements of `M` are equal), the number of carries when incrementing is maximal, leading to slower generation rates.

**9.2: Gray Code Order Generation**

Gray code, also known as reflected binary code, is a binary numeral system where two successive values differ in only one digit. The text describes two algorithms for generating mixed radix numbers in Gray code order, which minimizes the number of changes between consecutive numbers. 

1. **Constant Amortized Time (CAT) Algorithm**: This algorithm uses an array `i_` to store the 'directions' (+1 or -1) for each digit. The direction is flipped when incrementing a digit would cause it to exceed its base, and the digit is updated accordingly. 

2. **Loopless Algorithm**: This is an optimized version of the Gray code generation algorithm. It uses arrays `f_` (focus pointer) and `d_` (direction). When incrementing, if the new value for a digit would exceed its base, it changes direction by looking up the next position in `f_`.

**9.3: gslex Order Generation**

gslex order is another way to generate mixed radix numbers. The provided code snippet describes an algorithm for this order but doesn't go into detail on how it works. 

The key points are:
- It uses radices [2, 3, 4] and [4, 3, 2] as examples.
- Numbers are represented as arrays where each element corresponds to a digit in the mixed radix system.
- The positions of changes between consecutive numbers follow a specific pattern, shown in Figures 9.2-A and 9.2-B for Gray code order, and presumably different for gslex order.

The performance metrics (generation rates in M/s) are provided for each algorithm under various radix conditions (radix 2, 4, and 8), highlighting that the speed can vary significantly depending on the radix vector used. 

In summary, these algorithms provide methods for generating mixed radix numbers in different orders, each with its own advantages in terms of the pattern of changes between consecutive numbers, which can be crucial for various applications like combinatorial search or simulation.


The text describes four distinct methods for ordering mixed radix numbers, each with its own algorithm and implementation. 

1. **Generalized Subset Lexicographic (gslex) Order:**

   This order is a generalization of the subset lexicographic order to mixed radix numbers. It's generated by incrementing digits starting from zero until a digit different from one is encountered, then increment that digit. If all subsequent digits are also one, decrement them before moving to the next higher digit. The generator works in constant amortized time and generates approximately 123 million objects per second for radix 2, increasing to around 273 M/s for radix 16.

   Implementation details include an array `a_` for storing digits, another array `m1_` that holds the radix minus one at each position, and a sentinel at `a_[n_]`. The method `next()` computes successors using this algorithm.

2. **Alternative gslex Order:**

   This variant of the gslex order is achieved by reversing the list of numbers, reversing the words within each number, and replacing non-zero digits with their corresponding radii minus the digit value. The generation rate remains similar to the regular gslex order.

3. **Endo Order:**

   In endo order, successors are generated by incrementing elements in a way that reflects the nature of the mixed radix system. This method uses an additional array `le_` that stores the last nonzero element for each position in endo order (2 if the radix is greater than 1, else 1). The generation rate is between approximately 115 million and 180 million numbers per second, depending on the radix.

   Implementation involves an extra array `le_` to store these values, with methods `first()` and `last()` setting up initial and final states respectively. The method `next_endo()` handles the increment operation for endo order.

4. **Endo Gray Code:**

   This is a modification of the CAT algorithm for Gray code applied to mixed radix numbers in endo order. It introduces an array `i_` that tracks the direction of movement (+1 for forward, -1 for backward). 

   In the computation of the successor, the algorithm checks if it needs to increment or decrement based on the current digit and its corresponding radix. If there's an overflow (when incrementing), it changes direction. The method `next()` uses this strategy. 

The 'Gray code' version ensures that only one digit (bit) flips between successive numbers, preserving a property of standard Gray codes in binary systems for mixed-radix settings.


The provided text discusses methods for representing permutations using factorial number systems, specifically falling factorial base and rising factorial base. These representations are known as Lehmer codes (or inversion tables) and their inverses.

1. **Lehmer Code/Inversion Table**: Each permutation of n elements can be represented uniquely by an (n-1)-digit number in the falling factorial base. This representation, called the Lehmer code or inversion table, is computed by counting the number of elements to the right of each index k that are less than the element at that index. For example, for permutation P = [3, 0, 1, 4, 2], the inversion table I = [3, 0, 0, 1] is obtained by counting how many elements with larger indices are smaller than each position's element.

2. **Conversion Routines**:

   - `perm2ffact`: Converts a permutation to its Lehmer code (falling factorial base).
   - `ffact2perm`: Converts a Lehmer code back to the original permutation.
   - `perm2rfact`: Converts a permutation to its rising factorial representation.
   - `rfact2perm`: Converts a rising factorial representation back to the original permutation.

3. **Inverse Permutations**: Routines for computing inverse permutations from Lehmer codes (`ffact2invperm`) and rising factorial representations (`rfact2invperm`).

4. **Large Arrays**: For handling large arrays (e.g., millions of entries), left-right array data structures are used to optimize the conversion process, reducing computational complexity to O(n log n).

5. **Falling vs Rising Factorial Bases**: Falling factorial base uses radices in descending order (2, 3, 4, ...), while rising factorial base uses ascending radices (2, 3, 4, ...). Permutations corresponding to Lehmer codes and their reversed/complemented versions are shown for both bases.

6. **Applications**: These factorial representations of permutations have applications in various fields, including combinatorics, computer science, and cryptography.


The text discusses various methods to represent permutations using factorial numbers, focusing on the conversion of routines that compute permutations from factorial numbers into those that compute inverse permutations. The changes required for this conversion are straightforward: swapping 'x[a] = b' with 'x[b] = a'.

1. **Falling Factorial Base (ffact)**: This method uses rotations in the computation of a permutation from its Lehmer code. The routine `perm2ffact_rev(const ulong *x, ulong n, ulong *fc)` computes the factorial representation of a given permutation using this method. Its inverse is `ffact2perm_rev(const ulong *fc, ulong n, ulong *x)`.

2. **Rising Factorial Base (rfact)**: This method also uses rotations but in a different manner than the falling factorial base. The routine `perm2rfact_rev(const ulong *x, ulong n, ulong *fc)` computes the rising factorial representation of a given permutation, and its inverse is `rfact2perm_rev(const ulong *fc, ulong n, ulong *x)`.

3. **Representation via Swaps**: This method represents permutations using swaps. The complexity of this direct implementation is O(n). The routines `perm2ffact_swp(const ulong *x, ulong n, ulong *fc)` and `perm2rfact_swp(const ulong *x, ulong n, ulong *fc)` compute the factorial representations using swaps for falling and rising factorial bases, respectively. The inverse routines are not explicitly mentioned in this text but can be inferred as 'ffact2perm_swp' and 'rfact2perm_swp'.

4. **Representation via Rotations**: There are two types of rotations used here: fixed length with variable rotation amount (ffact2perm_rot, rfact2perm_rot) and fixed rotation amount with variable length (perm2ffact_rot, perm2rfact_rot). These methods provide alternative ways to compute permutations from factorial numbers.

The text also includes algorithms for calculating the number of inversions in a permutation using both O(n^2) and O(n log n) methods. It concludes with visual representations (Figure 10.1-C and Figure 10.1-D) that depict falling and rising factorial numbers alongside their corresponding permutations, illustrating how changes in the code lead to different permutation representations.


The provided text describes four different methods for generating permutations of a set of n elements, each following a specific order. Here's a detailed summary and explanation of these methods:

1. **Falling Factorial (ffact) Order**:
   - The falling factorial representation encodes a permutation as an array where the element at position k is the difference between the original value at that position and k.
   - The corresponding inverse permutation has the rising factorial representation, which is digit-reversed.
   - The routines `perm2ffact_swp` (for falling base) and `ffact2perm_swp` generate permutations in this order, while `ffact2invperm_swp` computes the inverse permutation.

2. **Rising Factorial (rfact) Order**:
   - The rising factorial representation encodes a permutation as an array where the element at position k is the difference between the original value at that position and n-1-k.
   - The corresponding inverse permutation has the falling factorial representation, which is digit-reversed.
   - The routines `perm2rfact_swp` (for rising base) and `rfact2perm_swp` generate permutations in this order, while `rfact2invperm_swp` computes the inverse permutation.

3. **Lexicographic Order**:
   - In lexicographic order, permutations are sorted as if they were numbers written in ascending order.
   - The class `perm_lex` implements an iterative algorithm for generating permutations in this order, with the method `next()` computing the next permutation with each call.

4. **Co-Lexicographic Order**:
   - In co-lexicographic order (colex), permutations are sorted such that, at any position, the value decreases as we move rightward across the permutation.
   - The class `perm_colex` implements an algorithm for generating permutations in this order, using rising factorial numbers to update the permutation when necessary.

5. **Reversing Prefixes Order**:
   - In this order, whenever the first j digits of a mixed radix number (with radii [2, 3, 4, ...]) change with an increment, the permutation is updated by reversing the first j+1 elements.
   - The class `perm_rev` generates permutations in this order using the method described above.

Each method has its own advantages and applications, such as finding interesting orders for permutations or serving as a basis for generating other permutation orders. These routines can be used to efficiently generate permutations for various combinatorial problems and algorithms.


The provided text discusses several methods for generating permutations in a minimal-change order, which is a strategy that minimizes the number of elements swapped during each update to generate the next permutation. This approach is useful in various applications such as combinatorial algorithms and statistical simulations where the order of generation can significantly impact performance.

1. **Heap's Algorithm**:
   Heap's algorithm generates permutations in a minimal-change order using mixed radix representation with rising factorial base. The idea is to increment the mixed radix number while keeping it in ascending order, swapping elements only when necessary to maintain this property. This method ensures that each update changes at most one digit in the mixed radix representation, resulting in a Gray code for permutations up to four elements.

   - **Class Implementation**: `perm_heap`
   - **Next Permutation Generation**: The algorithm increments the mixed radix number by finding the rightmost non-increasing element and swapping it with the subsequent element if needed. It uses an auxiliary counter to keep track of the swaps and reset it when certain conditions are met, optimizing for common cases.
   - **Performance**: Generates about 133 million permutations per second on average.

2. **Optimized Heap's Algorithm (Heap's Algorithm with Counter)**:
   This is a further optimization of Heap's algorithm that recognizes and handles five specific swap patterns (0,1), (0,2), (0,1), (0,2), and (0,1) separately. By doing so, it reduces the number of comparisons needed, leading to faster permutation generation.

   - **Class Implementation**: `perm_rev2`
   - **Next Permutation Generation**: The counter is used to quickly handle common swap patterns, significantly speeding up the algorithm. It generates around 275 million permutations per second on average.

3. **Lipski's Minimal-Change Orders**:
   Lipski introduced several variants of Heap's method, each with a unique way of incrementing mixed radix numbers to generate minimal-change orders. These variations aim to improve the generation rate and adaptability for various permutation sizes.

   - **Class Implementation**: `perm_gray_lipski`
   - **Next Permutation Generation**: The algorithm increments the mixed radix number based on different strategies controlled by a parameter `r`. Each value of `r` corresponds to a specific minimal-change order, offering flexibility in choosing the most efficient variant for given use cases. The generation rate varies depending on the chosen order but typically ranges from 100 million to 280 million permutations per second.

4. **Wells' Algorithm**:
   Wells' algorithm is another method for generating permutations, which differs from Heap's by using a different strategy for incrementing mixed radix numbers and handling swaps. It offers various variants controlled by a parameter `d`.

   - **Next Permutation Generation**: The algorithm increments the mixed radix number according to specific rules based on the parameter `d`, resulting in different minimal-change orders. The provided text does not detail the full implementation or performance metrics of Wells' algorithm, but it's mentioned that optimizations similar to those for Heap's method should be apparent.

In summary, these algorithms focus on generating permutations with minimal changes (swaps) between consecutive permutations by utilizing mixed radix representations and various strategies for incrementing the representation. The choice of algorithm depends on factors such as the desired permutation size, performance requirements, and flexibility in choosing different minimal-change orders. Optimizations like counters and handling specific swap patterns can significantly improve generation rates across these methods.


The text discusses two permutation orders, Trotter's strong minimal-change order and Star-transposition order, along with their implementations.

1. **Trotter's Strong Minimal-Change Order (Section 10.7):**

   This ordering ensures that in each step of generating the next permutation, only adjacent elements are swapped, making it a "minimal change" order. The algorithm is based on H. F. Trotter's construction from 1962 and has been optimized for efficiency.

   **Key Points:**
   - The algorithm generates permutations of n elements.
   - It uses sentinel elements (0 and n) at the lower and upper ends of the array to simplify boundary conditions.
   - An auxiliary array `d_` stores direction flags (+1 or -1), initially set to +1, indicating whether to move an element up (-1) or down (+1).
   - The `next()` method finds the smallest element whose neighbor is greater (in the case of moving up) and swaps them, changing the direction of all elements that couldn't be moved.
   - The `prev()` method is almost identical but negates the direction to find the previous permutation.

   **Optimizations:**
   - A special case is handled for the element zero, which moves most often. This optimization speeds up computations significantly.

2. **Star-transposition Order (Section 10.8):**

   In this ordering, successive permutations differ by a swap of the first element with another element (star transposition). The list of inverse permutations always includes the movement of zero.

   **Key Points:**
   - Each permutation differs from the previous one by swapping the first element with some other element.
   - The inverse permutations' sequence of positions swapped with the first position is entry A123400 in [312].
   - The sequence of positions of the element zero is entry A159880, which can be constructed by considering permutations described in section 10.4 on page 245 and computing inverse permutations as shown in figure 10.8-C.

   **Implementation:**
   - The class `perm_star` generates the star-transposition order and is implemented in [FXT: comb/perm-star.h].
   - The demonstration program to generate listings of star-transposition permutations is [FXT: comb/perm-star-demo.cc].

Both algorithms generate a significant number of permutations per second, with Trotter's algorithm generating around 145 million and Star-transposition order generating about 190 million permutations per second. The Star-transposition order is particularly interesting for studying the movement patterns of elements within permutations.


This text discusses three different minimal-change orders for generating permutations, each using a unique base for mixed radix numbers. These methods are based on falling factorial numbers, rising factorial numbers, and permuted factorial numbers, respectively.

1. **Permutations with Falling Factorial Numbers (Section 10.9.1)**:

   The Gray code for mixed radix numbers with a falling factorial base is used to generate permutations in Trotter's minimal-change order. This method involves swapping elements based on changes in the mixed radix sequence, where a digit 'p' (position) changes by 'd' (= ±1) in the sequence, implying that element 'p' of the permutation is swapped with its right neighbor for d = +1 or left neighbor for d = -1.

   The class `perm_gray_ffact2` implements this algorithm, using a loopless mixed-radix Gray code routine and storing current permutations in arrays `x_` and their inverse permutations in `ix_`. The most recently swapped elements are tracked by `sw1_` and `sw2_`.

2. **Permutations with Rising Factorial Numbers (Section 10.9.2)**:

   This method uses a Gray code for numbers in rising factorial base to generate permutations. A recursive construction for this order is shown in Figure 10.9-C, and the corresponding Gray code is displayed in Figure 10.9-B. The class `perm_gray_rfact` implements a constant amortized time (CAT) algorithm for generating these permutations using mixed radix Gray codes with rising factorial base.

   To compute the next permutation, elements are swapped based on the position 'j' of the digit changed and its direction 'd' (= ±1). The element to be swapped is determined by searching for the greatest smaller (for d > 0) or smallest greater (for d < 0) element relative to the changing element.

3. **Permutations with Permuted Factorial Numbers (Section 10.9.3)**:

   This method generates permutations using a base of permuted factorial numbers. The swaps are determined by positions and directions similar to the previous methods, but here, elements are indexed using factorial numbers that have been permuted in some way. The class responsible for this algorithm is not explicitly mentioned, but it likely follows a similar structure as `perm_gray_ffact2` or `perm_gray_rfact`.

Each of these methods generates permutations with minimal changes between consecutive permutations, making them efficient for generating sequences of permutations in various contexts. The speeds at which they generate permutations are provided: around 155 million per second for a simple implementation, 80-90 million per second for the loopless versions based on falling and rising factorial bases. These algorithms offer alternatives to standard permutation generation methods and can be tailored depending on specific use cases or requirements.


The text discusses several permutation orders, each with its unique characteristics. Here's a detailed explanation of the four orders mentioned:

1. **Mixed Radix Numbers (Gray Code):**
   - This order is derived from Gray codes for factorial numbers.
   - The radix vector used here is [2, 3, 5, 4].
   - For even n, the last permutation in this order is a cyclic shift of one position from the first.
   - An implementation of this order is provided in FXT (Fast eXact Algorithms and Tools) library as `perm_gray_rot1`.

2. **Derangement Order:**
   - In this order, no two successive permutations share any element at the same position.
   - There's no derangement order for n = 3.
   - An implementation of the underlying algorithm is provided in FXT library as `perm_derange`.
   - This order ensures that each permutation differs from its predecessor by at least one element, maximizing transitions between permutations.

3. **Cyclic Shift Order (Falling Factorial Base):**
   - This order generates permutations via cyclic shifts.
   - It creates a derangement order if n is even but not for odd n.
   - The algorithm for generating all permutations with k transitions (where 2 ≤k ≤n and k ̸= 3) is given in [297].
   - An implementation of this order is provided in FXT library as `perm_rot`.

4. **Orders where the Smallest Element Always Moves Right:**
   - The text discusses a variant of Trotter's construction, specifically an ordering where the first element always moves right.
   - This order can be generated using an interleaving process (shown in Figure 10.11-A).
   - The second half of permutations is the reversed list of the reversed permutations in the first half.
   - An implementation of this order is provided in FXT library as `perm_mv0`.

The derangement order ensures that each permutation differs from its predecessor by at least one element, making it an order with maximum transitions between successive permutations. The mixed radix numbers (Gray Code) ensure minimal changes between successive permutations. Cyclic shift orders generate permutations via cyclic shifts and can create derangement orders for even n but not for odd n. Lastly, the orders where the smallest element always moves right are generated through an interleaving process, ensuring that the first element moves to the right in each step.


The text discusses two permutation generation algorithms, Ives' algorithm and single track orders, along with a specific type of Gray code for permutations known as single track Gray codes.

**Ives' Algorithm:**

- This algorithm generates permutations in an order where most updates involve moving the smallest element to the right by one position.
- The rate of generation is approximately 180 million permutations per second when using pointers and around 190 M/s with arrays.
- Optimizations include a special case for when only the first element needs to be moved, which can be handled more efficiently, and modifying certain conditions in the loop to improve performance.
- The algorithm is implemented in the `perm_ives` class, with additional optimizations like counters (`ctm_` and `ctm0_`) to handle the easy case (moving only the first element) and bitwise operations for checking equality.

**Single Track Orders:**

- This method generates permutations in a single track order, where each column is a cyclic shift of the previous one.
- The order is constructed recursively using mixed radix counting with rising factorial base.
- The algorithm is implemented in the `perm_st` class, generating around 123 million permutations per second.
- A single track Gray code can be derived from a Gray code for n-1 elements by ensuring that the first and last permutation are cyclic shifts of each other (applicable only to even lengths). This results in a single track order with just n-1 extra transpositions for all permutations of n elements.
- The number of distinct single track orders is ((n - 1)!)!, and there are ((n - 1)! - 1)! single track orders starting with the identity permutation, where each run of (n - 1)! elements begins at position k.

**Gray Code for Permutations:**

- A Gray code is a binary code in which adjacent codewords differ by only one bit. In the context of permutations, this means that consecutive permutations differ by a single transposition (swap of two elements).
- Single track Gray codes have the additional property where each column (or row) is a cyclic shift of the previous one. These codes are constructed using a Gray code for n - 1 elements and ensuring that the first and last permutation are cyclic shifts of each other.

These algorithms and methods provide efficient ways to generate permutations in specific orders, with varying levels of optimization and unique properties suitable for different applications.


The text discusses various types of permutations, their properties, and generating functions associated with them. Here's a summary:

1. **Stirling Cycle Numbers (s(n, m))**: These numbers represent the number of ways to partition n elements into m cycles. They satisfy the recurrence relation s(n, m) = s(n - 1, m - 1) + (n - 1) * s(n - 1, m). The exponential generating function for these numbers is exp(L(z)), where L(z) is defined as a summation of terms with base (k-1)! * t_k * z^k/k.

2. **Permutations with Prescribed Cycle Type**: If a permutation has c_i cycles of length i, the number of such permutations is given by Z_n,C = n! / (c_1! c_2! ... c_n! 1^(c_1) 2^(c_2) ... n^(c_n)).

3. **Prefix Conditions**: Certain types of permutations can be generated efficiently using routines that produce lexicographically ordered lists subject to conditions for all prefixes. The condition must be supplied as a function pointer during the creation of a class instance. Examples include involutions, up-down permutations, connected permutations, and derangements.

   - **Involutions**: These are self-inverse permutations (a permutation that is its own inverse). The sequence I(n) starts as 1, 2, 4, 10, ... and satisfies the recurrence relation I(n) = I(n - 1) + (n - 1) * I(n - 2).

   - **Derangements**: These are permutations where no element appears in its original position. The sequence D(n) starts as 0, 1, 2, 9, ... and satisfies the recurrence relation D(n) = (n - 1) * [D(n - 1) + D(n - 2)].

   - **Connected Permutations**: These permutations cannot be split into two non-empty sub-permutations that are themselves permutations. The sequence C(n) starts as 1, 1, 3, 13, ... and satisfies the recurrence relation C(n) = n! - Σ (k=1 to n-1) k! * C(n - k).

   - **Alternating Permutations**: These permutations alternate between increasing and decreasing subsequences. The sequence A(n) starts as 1, 1, 2, 5, ... and satisfies the recurrence relation A(n) = 1/2 * Σ (k=0 to n-1) binomial(n - 1, k) * A(k) * A(n - 1 - k).

The text also mentions generating functions for these sequences, which are useful in combinatorics and number theory. These functions allow us to encode information about sequences in a compact way and provide tools for manipulating and analyzing the sequences.


The text discusses different types of permutations with specific restrictions on the movement of elements. Here's a detailed summary:

1. **Permutations where no element can move more than one place to the right (M(n))**:
   - M(n) = 2^n - 1, as listed in A000079 in [312].
   - These permutations are also known as self-inverse permutations or involutions.
   - Figure 11.2-A shows a Gray code for these permutations when n=5, where no element lies more than one place to the right of its position in the identical permutation.

2. **Permutations where no element can move more than one place to the left (F(n))**:
   - F(n) is the (n + 1)-st Fibonacci number.
   - These permutations are self-inverse and also exhibit the property that no element moves more than one place to the right.
   - Figure 11.2-B displays a Gray code for these permutations when n=7.

3. **Permutations where an element can move k − 1 ≤ p(k) ≤ k + d places (both left and right)**:
   - This is a generalization of the previous two cases.
   - A Gray code for such permutations can be generated using binary words with at most d consecutive ones, as demonstrated in Figure 11.2-C for n=6 and d=2.

The algorithms for generating these permutations involve recursive functions:

**Recursive Algorithm for Cyclic Permutations**:
- This algorithm generates all permutations of n elements by placing each element in the first position and recursively generating permutations of the remaining elements.
- The order of generation corresponds to the alternative factorial representation with a falling base, as shown in Figure 11.4-A.

**Minimal-Change Order for Cyclic Permutations**:
- All cyclic permutations can be generated using a mixed radix Gray code with a falling factorial base (as depicted in Figure 11.4-C).
- Two successive permutations differ at three positions, indicating a constant amortized time (CAT) implementation for generating these permutations.

The provided C++ classes and functions implement these algorithms:

- `perm_rec` class generates all permutations of n elements using the recursive algorithm described above.
- `perm_involution` class specializes in generating self-inverse permutations or involutions.
- `cyclic_perm` class generates cyclic permutations, with an option to generate only these specific permutations by modifying the recursive function's loop condition.


The text discusses two methods for generating k-permutations of n elements, specifically focusing on the lexicographic order and minimal-change (Gray code) order.

**Lexicographic Order:**

1. The class `kperm_lex` generates k-permutations in lexicographic order using mixed radix numbers, where changes are restricted to the first k elements.
2. The constructor initializes the permutation array (`p_`), its inverse (`ip_`), and a falling factorial number array (`d_`).
3. The `first()` method sets up the initial state by populating `p_` with numbers from 0 to n-1, `ip_` with the same values, and `d_` with zeros.
4. In the `next()` method:
   - It identifies the leftmost changed position (`i`) within the first k elements.
   - It increments the mixed radix number at position `i` until a change is found.
   - It swaps the element at position `i` with the smallest element to its right that is greater than `p[i]`.
   - It then updates the positions of the swapped elements in both `p_` and `ip_`.
5. This method generates k-permutations efficiently for small k, with a rate of around 80 million permutations per second for k=4 and n=100 (best case), and about 30 million permutations per second for k=n=12 (worst case).

**Minimal-Change Order (Gray Code):**

1. The class `kperm_gray` generates k-permutations in minimal-change order, specifically using the first inverse permutations in Trotter's order.
2. The update routine differs from the lexicographic order generator by checking whether the left element of the swapped pair lies within the k-prefix.
3. This method ensures that each new permutation differs from the previous one by only one element swap, creating a Gray code for k-permutations.
4. The generation rate for this method is not explicitly provided but is expected to be comparable to the lexicographic order generator, depending on the values of k and n.

Both methods allow for the efficient generation of k-permutations, with the choice between them depending on whether lexicographic order or minimal-change order (Gray code) is preferred for a given application.


The text discusses the permutations of multisets, which are collections of elements where repetition is allowed but order does not matter. A multiset can be represented as (r0, r1, ..., rk-1), indicating that there are r0 elements of type 0, r1 elements of type 1, and so on, up to rk-1 elements of type k-1. The total number of elements in the multiset is n = Σ(rj from j=0 to k-1).

The text covers three methods for generating permutations of a multiset: recursive generation, iterative generation using lexicographic order, and iterative generation using an ordering based on prefix shifts (also known as cool-lex order).

1. Recursive Generation:
   - The given routine generates all permutations in lexicographic order when called with argument zero.
   - It uses a recursive approach to generate each permutation by selecting elements from different "buckets" or types, represented by the rj values.
   - The efficiency of this method can be improved by maintaining a list of pointers to the next nonzero bucket (nk[]), reducing work for regular permutations to less than e (Euler's number) times the number of generated permutations.

2. Iterative Generation (Lexicographic Order):
   - This method generates the next permutation in lexicographic order given an initial state.
   - The algorithm involves finding the rightmost pair with a smaller element before a larger one, swapping elements to maintain a falling sequence to the right of the swap position, and reversing the order of elements to the right.

3. Iterative Generation (Prefix Shifts/Cool-Lex Order):
   - This ordering is described in [360], and it involves a cyclic shift of a prefix for each transition.
   - Each permutation is related to the previous one by shifting a contiguous subsequence of elements to the right or left, maintaining the relative order within the shifted subsequence.

The text also provides examples and code snippets for implementing these methods in C++, including classes like `mset_perm_lex_rec`, `mset_perm_lex`, and `mset_perm_pref`. The performance of these implementations is also discussed, with generation rates ranging from tens to hundreds of millions of permutations per second, depending on the size and composition of the multiset.


The text discusses two methods for generating Gray codes, which are binary sequences that change by only one bit at a time, for permutations of multisets and strings with certain restrictions.

1. **Permutation of Multisets (Section 13.2):**

   The paper presents an algorithm for generating permutations of multisets in "cool-lex" order. A multiset is a collection that allows multiple instances of the same element. Here's a summary:

   - The algorithm uses an array to represent the multiset and determines the length of the longest non-increasing prefix in a simple manner.
   - It initializes with a 'first()' function, which assigns indices based on the frequency of elements in the multiset.
   - The 'next()' function scans for the longest non-increasing prefix and rotates it to generate the next permutation.
   - If the number of elements in the prefix equals the total minus one (indicating the last permutation), it rotates the entire array and checks if this is indeed the last permutation.
   - Otherwise, it compares the last element of the prefix with the element two positions rightward, rotates accordingly, and updates the length of the next longest non-increasing prefix.

   The performance of the algorithm varies depending on the multiset: 68 M/s for permutations of 12 elements, 46 M/s for combinations (6 choose 2), and 62 M/s for permutations of (2, 2, 2, 3, 3, 3).

2. **Gray Code for Multiset Permutations (Section 13.2.4):**

   This section introduces an alternative method using a linked list for multiset permutations, as suggested by the original paper. The key points are:

   - The algorithm calculates the length of the next longest non-increasing prefix with just one comparison.
   - It stores this length in a variable 'ln_' and uses fast update when enabled via a preprocessor directive (#define MSET_PERM_PREF_LEN).
   - The initialization is modified to account for the new computation method.

   The performance improves slightly: 71 M/s for permutations of 12 elements, 62 M/s for combinations (30 choose 15), and 69 M/s for permutations of (2, 2, 2, 3, 3, 3).

3. **Minimal-Change Order (Section 13.2.4):**

   This section presents a Gray code algorithm for multiset permutations proposed by Fred Lunnon. It's a generalization of Trotter's order for permutations. The key aspects are:

   - The algorithm generates a sequence where each successive permutation differs from the previous one by swapping two adjacent elements only if necessary.
   - The implementation uses classes 'mset_perm_gray' in the file 'comb/mset-perm-gray.h'.
   - It has functions for initialization, data retrieval, and swap position extraction.

   The performance is about 40 M/s when generating the Gray code for multiset permutations (FXT: comb/mset-perm-gray-demo.cc).

4. **List Recursions (Section 14.1):**

   This section discusses a method for generating Gray codes using list recursions, where a relation like W(n) = [0 0 . W(n −2)] + [1 0 . W R(n −2)] + [1 2 0 . W(n −3)] implies another version obtained by reversing the order of sublists and additionally reversing each sublist.

   - The recursion relation leads to a linear recurrence for the number of strings (w(n)) with certain properties, which in this case are Fibonacci numbers.
   - An implementation of such an algorithm is provided in 'comb/fib-alt-gray-demo.cc'.

5. **Fibonacci Words (Section 14.2):**

   This section introduces Fibonacci words, binary sequences where each word is either "0", "1", or "01". The key points are:

   - Fibonacci words follow a specific pattern based on the Fibonacci sequence, where w(n) represents the number of n-digit Fibonacci words.
   - The recursion relation for w(n) is w(n) = 2w(n −2) + w(n −3), starting with w(0) = 1 (an empty string).
   - This results in the Fibonacci sequence for w(n): 1, 1, 2, 3, 5, 8, 13, 21, ...

   The text also provides a recursive function 'X_rec' to generate Fibonacci words based on these rules.


This text discusses several topics related to string generation, specifically focusing on Gray codes for strings with certain restrictions. 

1. **Fibonacci Words**: These are binary words (strings of 0s and 1s) that do not contain two consecutive ones. A recursive routine is provided to generate these words, with a modification to create a Gray code through the Fibonacci words. The algorithm operates in constant amortized time and can generate about 70 million objects per second.

2. **Generalized Fibonacci Words**: This generalization allows for a fixed maximum number (r) of consecutive ones in binary words. The list recursion for generating these words is provided, along with examples for r=1 through r=5. 

3. **Gray Codes for Generalized Fibonacci Words**: A Gray code is a sequence of integers where each term differs from the previous one by only one bit (in binary representation). For generalized Fibonacci words (with up to r consecutive ones), a list recursion is given to generate such a Gray code. The text also mentions an alternative Gray code for words without substrings 111 (r=2) and another for words without substrings 1111 (r=3).

4. **Run-Length Limited (RLL) Words**: These are binary strings where the number of consecutive zeros or ones is at most r, with r ≥2. The RLL(2) words in lexicographic order correspond to Fibonacci words in minimal change order. A recursive routine is provided for generating these words.

5. **Digit x followed by at least x zeros**: This section presents a table showing the lexicographic order of RLL(2) words and their corresponding changes in Fibonacci words. The changes indicate whether a bit stays the same (no change), or if it flips (indicated by 1).

In summary, this text explores various methods for generating specific types of binary strings, including Fibonacci words, generalized Fibonacci words, and RLL words. It also details techniques for creating Gray codes for these sequences, providing recursive algorithms and visual examples to illustrate the concepts.


The text discusses several topics related to Gray codes for specific types of binary strings, focusing on their definitions, properties, and algorithms for generating these codes. Here's a detailed summary and explanation of each section:

14.6 Generalized Pell words

14.6.1 Gray code for Pell words
- Definition: Pell words are ternary (base 3) strings without the substrings "21" or "22." A Gray code is a minimal-change order of such strings.
- Figure 14.6-A shows the start and end of the lists in counting order and Gray code order for 5-digit Pell words.
- The recursive algorithm for generating this Gray code is provided, along with an implementation in C++.
- The computation of a power series related to the Pell Gray code is mentioned.

14.6.2 Gray code for generalized Pell words
- Definition: Generalized Pell words are radix-(r+1) strings where the substring rx (with x ≠ 0) is forbidden.
- The recursion formula for generating a Gray code of such words is given, depending on whether r is even or odd.
- Figure 14.6-B displays a Gray code for 4-digit radix-3 strings with no substring 3x (where x ≠ 0).
- An implementation of the algorithm is provided, along with generating functions for the number of words in these sequences.

14.7 Sparse signed binary words

14.7 Sparse signed binary words
- Definition: These are binary strings where zeros can be replaced by either +1 or -1, but there cannot be two consecutive nonzero digits.
- Figure 14.7-A shows a Gray code for 6-digit sparse signed binary words, with 'P' and 'M' denoting +1 and -1, respectively.
- The recursive algorithm for generating this Gray code is provided, along with an implementation in C++.
- The number of n-digit sparse signed binary numbers (S(n)) and positive n-digit sparse signed binary numbers (P(n)) are given, along with recurrence relations and initial conditions.

14.8 Strings with no two consecutive nonzero digits

14.8 Strings with no two consecutive nonzero digits
- Definition: These strings consist of zeros and ones, where no two consecutive digits are nonzero (i.e., there cannot be two consecutive 1s).
- Figures 14.8 provide examples of such strings for different lengths (n = 1 to n = 30).
- Two recursive algorithms for generating orders close to Gray codes are presented:
  a) pos_rec() generates an almost Gray code, with N non-Gray transitions and X excess digit changes from a true Gray code.
  b) pos_AAA() and pos_BBB() generate a more refined ordering with approximately n/2 non-Gray transitions for larger n.

In summary, these sections discuss various types of binary string Gray codes and near-Gray codes, focusing on Pell words, generalized Pell words, sparse signed binary words, and strings with no two consecutive nonzero digits. They provide definitions, examples, recursive algorithms, and implementations in C++, along with generating functions for the number of words in these sequences. The text also explores properties like minimal-change orders and the relationship between different types of binary string orders.


The text describes various types of well-formed parentheses strings, also known as paren strings, which are lists of n pairs of parentheses that adhere to specific rules. These rules ensure the strings are valid and non-nested. Here's a detailed explanation of each type discussed:

1. Co-lexicographic order (Figure 15.1):
   - This is the standard lexicographic (dictionary) order for parentheses strings, where the comparison starts from the leftmost parenthesis.
   - Strings are listed in ascending order based on their co-lexicographic value.
   - For example, the first few lines of Figure 15.1 show the following strings:
     1: (((((()))))
     22: ()()(()())

2. Gray codes for specific restrictions (Figures 14.8-A, 14.9-A, 14.10-A):
   - These figures illustrate Gray codes for parentheses strings with certain restrictions on the digits or substrings.
     - Figure 14.8-A: Length-4 radix-4 strings with no two consecutive nonzero digits.
     - Figure 14.9-A: Binary strings with no two consecutive zeros (with r = 1, 2, 3).
     - Figure 14.10-A: Binary strings without substring 1x1 (where x is either 0 or 1).
   - Gray codes are a sequence of codes in which two successive code words differ by only one bit (or digit).

3. Recursion for generating lists (Equations 14.8-1, 14.9-1, 14.10-1):
   - These equations define recursive structures for generating the respective lists of parentheses strings based on specific rules:
     - Equation 14.8-1: For length-n strings with radix (r+1) and no two consecutive nonzero digits.
     - Equation 14.9-1: For length-n strings with radix (r+1) and no two consecutive zeros, with different versions for even and odd r.
     - Equation 14.10-1: For length-n binary strings without substring 1x1.

4. Recurrence relations for counting elements (Equations 14.8-2, 14.9-2, 14.10-2):
   - These equations provide recurrence relations to calculate the number of valid parentheses strings based on specific restrictions:
     - Equation 14.8-2: For length-n strings with no two consecutive nonzero digits (denoted as dr(n)).
     - Equation 14.9-2: For length-n binary strings without two consecutive zeros (denoted as zr(n)).
     - Equation 14.10-2: For length-n binary strings without substring 1x1 (denoted as v(n)).

5. Generating functions for counting elements (Equations 14.8-3, 14.9-3, 14.10-3):
   - These equations provide generating functions that can be used to calculate the number of valid parentheses strings based on specific restrictions:
     - Equation 14.8-3: For length-n strings with no two consecutive nonzero digits (denoted as dr(n)).
     - Equation 14.9-3: For length-n binary strings without two consecutive zeros (denoted as zr(n)).
     - Equation 14.10-3: For length-n binary strings without substring 1x1 (denoted as v(n)).

6. Generalization to k-ary Dyck words (at the end of Section 15):
   - The text briefly mentions a generalization of parentheses strings called k-ary Dyck words, where each parenthesis can be replaced by any of k symbols, resulting in a broader family of combinatorial objects.

In summary, this text describes various well-formed parentheses string lists (co-lexicographic order and Gray codes) and their respective recurrence relations and generating functions based on specific restrictions such as no two consecutive nonzero digits, no two consecutive zeros, or absence of certain substrings like 1x1 or 1xy1. The text also mentions a generalization to k-ary Dyck words for broader combinatorial applications.


The provided text describes a method for generating valid strings of parentheses, which are also known as Dyck words. These strings follow certain rules: they start and end with the same type of parenthesis, every opening parenthesis "(" must be followed by a closing parenthesis ")", and at no point can the number of closing parentheses exceed the number of opening ones.

The method uses a restricted growth string (RGS) representation for these valid strings. A RGS is a sequence where each term ak satisfies 0 ≤ ak ≤ ak-1 + 1, meaning that each term increases by at most one from the previous term. 

For generating the next valid parentheses string, two operations are performed:

1. **Increment Operation**: The highest digit (rightmost in the sequence) where this digit is less than or equal to its predecessor (ak ≤ ak-1) is incremented. All subsequent digits (ai for i > k) are then reset to 0. This operation ensures that only one value changes in each step, maintaining a Gray code property which means that consecutive strings differ by a single parenthesis swap.

2. **Decrement Operation**: The highest non-zero digit aj is decremented, and all subsequent digits ai for i > j are set to ai-1 + 1. This operation moves the "highest" increase point leftwards, again ensuring only one change per step.

The class `catalan` in the provided C++ code implements this method. It takes as input the number of pairs (n), and by default generates RGSs in near-perfect minimal-change order, where exactly two symbols change with each step. 

The class has three main components: 
- `as_`: an array storing the digits of the RGS, ensuring that each element is less than or equal to its predecessor plus one.
- `d_`: an array indicating the direction (+1 or -1) for the next recursive step.
- `n_` : The number of digits (paren pairs).

The method also includes a string representation of the generated parentheses, which is constructed on demand using a char array (`str_`). 

Finally, Figure 15.2-A to C illustrate this concept with examples for n=4 and n=5. These figures show the RGSs, their corresponding parenthesis strings, delta sets (difference between consecutive strings), and difference strings in lexicographic order. The minimal-change order ensures that the distance between consecutive strings is minimized, making the sequence more uniform and predictable.


The text discusses various algorithms for generating minimal-change orders of parenthesis strings, also known as Dyck words. Here's a detailed summary and explanation:

1. Recursion (next_rec):
   - The function `next_rec(k-1)` generates the next Catalan number string by recursion. It uses a helper variable `d` to store the current digit (0 or 1) and `as[k]` to keep track of consecutive zeros.
   - If `ns1` (next significant bit) is zero, it means no valid continuation exists, so the function returns false. Otherwise, it updates `d_[k]` with the new digit and increments `as[k]`.
   - The base case for this recursion isn't shown in the provided code snippet but would be where `k` reaches a predefined limit or condition.

2. Gray Code Order:
   - This order generates parenthesis strings while ensuring that each string differs from its successor by only one bit change (ignoring the first bit, always 1).
   - The algorithm uses recursion with two cases: forward (`0==z`) and backward (`!z`). For forward, it iterates through possible values for `rv[d]`, while for backward, it iterates in reverse.

3. Prefix Shift Order (Cool-Lex):
   - This order generates parenthesis strings where each string differs from its successor by a cyclic shift of a prefix, ignoring the first bit which is always 1.
   - The algorithm generates binary words corresponding to parenthesis strings and then maps them back to the parenthesis representation.

4. Catalan Numbers:
   - The number of valid combinations of n parentheses pairs (Dyck words) is given by the nth Catalan number, Cn = (2n choose n)/(n+1).
   - These numbers are also represented as sequence A000108 in the Online Encyclopedia of Integer Sequences (OEIS).
   - The generating function for Catalan numbers is C(x) = 1 + x + 2x^2 + 5x^3 + ...
   - The Catalan numbers satisfy a convolution property: Cn = Σ[k=0 to n-1] C_k * C_(n-1-k).

5. Restricted Growth Strings (RGS):
   - RGS is another order for generating parenthesis strings where each string differs from its predecessor by changing exactly two positions and maintaining the order of non-changing positions.
   - The table provided shows examples of RGS for n = 1 to 5, listing the binary representation of Dyck words along with their positions in the RGS sequence.

The different orders (recursion, Gray code, prefix shift, and RGS) serve various purposes such as efficient string generation, minimizing changes between adjacent strings, or generating strings with specific properties like strong minimal-change order where changes occur only in adjacent positions for even values of n.


The text discusses three related topics: Increment-i Restricted Growth Strings (RGS), k-ary Dyck words, and k-ary trees. 

1. **Increment-i RGS**: These are sequences where the first element is 0, and each subsequent element does not exceed the previous one by more than i. For example, a 4-length Increment-2 RGS could be [0, 2, 3, 6]. The case where i=1 corresponds to regular Restricted Growth Strings (RGS).

2. **k-ary Dyck Words**: These are binary words where any prefix contains at least k-1 ones for every zero. A correspondence exists between Increment-i RGS and k-ary Dyck words, where k equals i+1. 

3. **k-ary Trees**: The length-n increment-i RGS also describe k-ary trees with n internal nodes. Starting from the root, moving outwards by i positions for each '1' (increment) and then following back by one position for each '0', constructs these trees.

**Generation in Lexicographic Order**: The text provides an algorithm to generate increment-i RGS in lexicographic order. This is achieved through a class named `dyck_rgs` with methods like `next()` that returns the index of the first changed element in the restricted growth string (RGS). 

**Gray Codes with Homogeneous Moves**: The text mentions a loopless algorithm for generating a Gray code where all moves are homogeneous. This is visualized in Figure 15.5-B, showing the positions of ones in delta sets for 3-ary Dyck words. The corresponding implementation is given in the `dyck_gray` class.

**Gray Codes with Homogeneous and Two-Close Moves**: A more specific Gray code is presented where all transitions are both homogeneous (all '1's or '0's change by the same amount) and two-close (the difference between the old and new values is either +2 or -2). This is shown in Figure 15.5-C, with its implementation provided in the `dyck_gray2` class. 

In summary, this text presents methods for generating specific types of strings (Increment-i RGS) that have applications in constructing Dyck words and k-ary trees. It also details algorithms to generate Gray codes—sequences used to enumerate combinatorial objects without repetition while maintaining a close relationship between successive elements—for these structures with constraints on the type of moves allowed.


The text discusses several aspects related to integer partitions, which are the different ways a number can be expressed as a sum of smaller positive integers. Here's a detailed summary:

1. **General Integer Partitions**: The problem of finding all integer partitions of a given number x (Fig 16.0-A) is solved by an iterative algorithm presented in [FXT: class partition in comb/partition.h]. This algorithm generates the partitions in lexicographic order.

2. **Partitions into m parts**: A different algorithm is provided for generating all integer partitions of n into exactly m parts (Fig 16.3-A). This method, described in [FXT: class mpartition in comb/mpartition.h], starts with an initial partition containing m−1 units and the element n−m+1, then modifies this partition by increasing elements to satisfy the condition that no part is less than the final part by at least two units.

3. **Number of Integer Partitions**: The text presents generating functions for various types of partitions:

   - Unrestricted partitions: This is given by η(x), where η(x) = ∏(1-x^n) for all n > 0 (Eq 16.4-6a).
   
   - Partitions into an even or odd number of parts: These are derived using Jacobi's identity (Eq 16.4-4), which relates two products involving q-series. The special cases a = -1, b = 0 and a = 0, b = 1 correspond to partitions into even/odd numbers of distinct parts (Eqs 16.4-2a and 16.4-2b).
   
   - Partitions into exactly m parts: This is derived from Jacobi's identity with a = 0 and b = 1, leading to the generating function Q_m(x) = ∏((1 - x^k)/(1 - x^(k+1))) for k from 1 to m-1 (Eq 16.4-3b).
   
   - Partitions into distinct parts: This is derived using Cauchy's identity (Eq 16.4-5) with a = -1 and b = 0, resulting in the generating function η(x)/η+(x), where η+(x) = ∏(1/(1 - x^n)) for all n > 0 (Eq 16.4-6b).
   
   - Partitions into square-free parts: This is a more complex topic, not directly addressed in the provided text.

These generating functions allow one to study properties of integer partitions using techniques from combinatorics and analysis. They can be used, for example, to derive closed forms or asymptotic estimates for the number of partitions of a given size.


The text discusses the topic of integer partitions, focusing on two types: unrestricted partitions and partitions into distinct parts.

**Unrestricted Partitions (Section 16.4.1):**

- The number of integer partitions of n is denoted by Pn, which forms sequence A000041 in [312]. Figure 16.4-A provides values for Pn when n ≤50.
- P(n, m) represents the number of partitions of n into exactly m parts. It satisfies the recurrence relation:

  P(n, m) = P(n - 1, m - 1) + P(n - m, m), for n > 0 and m > 0.

   With the initial condition P(0, 0) = 1.
- The generating function for partitions into exactly m parts is:

  ∞∑_{n=1} P(n,m) x^n = (x^m Q_{k=1}^{∞} (1 - x^k))^{-1}

   The rows in Figure 16.4-B correspond to fixed powers of x.
- The generating function for the number Pn of integer partitions of n is found by setting u=1 in:

  ∞∑_{n=0} P_n x^n = (1 - x)^(-1) = η(x), where η(x) is known as Euler's function.
- The generating function for partitions into parts r + j (for j = 0, 1, ...) with maximal part r can be expressed using the Rogers-Ramanujan identities.

**Partitions into Distinct Parts (Section 16.4.2):**

- The generating function for Dn, the number of partitions of n into distinct parts, is:

  ∞∑_{n=0} D_n x^n = ∏_{n=1}^∞ (1 + x^n) = η+(x), where η+(x) is Euler's function for distinct parts.
- The number of partitions into distinct parts equals the number of partitions into odd parts:

  η+(x) = η(x^2)/(η(x)) = ∏_{k=1}^∞ (1 - x^{2k-1})^{-1}.
- D(n, m), the number of partitions of n into exactly m distinct parts, has a generating function:

  ∞∑_{n=0} D(n,m) x^n = x^(m(m+1)/2) ∏_{k=1}^m (1 - x^k).
- The connection between relations 16.4-24 and 16.4-13 can be seen by decomposing the Ferrers diagram of a partition into m distinct parts into a triangle of size m(m+1)/2 and a partition into at most m elements.

In summary, integer partitions are ways to express an integer as a sum of positive integers, possibly with restrictions on the parts (e.g., distinct or bounded). The text explores generating functions for these partitions, recurrence relations, and various identities connecting different types of partitions.


The text discusses the concept of set partitions, focusing on two methods for generating all possible set partitions of a given set size (n). 

1. Recursive Generation:
   - The list Z_n contains all set partitions of an n-element set S_n = {1,2,...,n}. 
   - To generate Z_n, we use a complete list of partitions Z_{n-1} for the (n-1)-element set. For each partition P in Z_{n-1}, new partitions are created by appending n to the first, second, ..., last subset, or as a single-element set {n}. This process is repeated n-1 times, starting with the only partition {{1}} of the 1-element set. 
   - Figure 17.1-A illustrates this recursive construction for n = 4, while the right column shows all set partitions of 4 elements.

2. Minimal-Change Order:
   - A modified version of the recursive construction generates set partitions in a minimal-change order. This can be achieved by incrementing (adding an element) from left to right or right to left. 
   - The interleaving process is depicted in Figure 17.1-B, which demonstrates how this method works similarly to Trotter's construction for permutations. By changing the direction of incrementation in each recursive step, we obtain a minimal-change order (Figure 17.1-C). 
   - The C++ class `setpart` stores set partitions in an array of signed characters, where negated values indicate the last element in a subset. The generation work is proportional to P_n^k * B_k, where B_k represents the k-th Bell number, and P_n^k denotes the sum of the first n natural numbers raised to the power k.

This class `setpart` includes parameters like `xdr`, which determines the order in which partitions are created (minimal-change by default), and `dr0`, which sets the initial direction for each recursive step—either starting with a single partition containing all elements or beginning with individual singleton sets for each element.


The provided text discusses various aspects of set partitions, including their representation as restricted growth strings (RGS), generating functions for the number of set partitions (Stirling numbers and Bell numbers), and algorithms for generating RGSs in different orders. Here's a detailed summary:

1. **Set Partitions and Restricted Growth Strings (RGS):**
   - A set partition is a division of a finite set into non-empty subsets, called blocks or parts. The number of ways to partition an n-set into k subsets is given by the Stirling numbers of the second kind (S(n, k)).
   - RGSs are a compact way of representing set partitions. In an RGS s = [s0, s1, ..., sn−1], each si represents the size of the i-th block plus one. The condition aj ≤ 1 + maxi<j(ai) ensures that the RGS corresponds to a valid set partition.

2. **Generating Functions:**
   - The ordinary generating function (OGF) for Bell numbers (the sum of Stirling numbers in each row) is given by:
     ```
     ∞
     X
     n=0
     Bn xn = ∞
     X
     k=0
     xk Qk j=1 (1 −j x)
     ```
   - The exponential generating function (EGF) for Bell numbers is exp[exp(x) - 1].

3. **Stirling Numbers and Bell Numbers:**
   - Stirling numbers of the second kind, S(n, k), can be computed using the recurrence relation:
     ```
     S(n, k) = k * S(n −1, k) + S(n −1, k −1)
     ```
   - Bell numbers (sum of Stirling numbers in each row) can be computed using the recurrence relation:
     ```
     Bn+1 = Σ(k=0 to n) binomial(n-2, k-1) * Bk
     ```

4. **RGS Generation Algorithms:**
   - The text describes three classes for generating RGSs in different orders:

     a. **Lexicographic Order (setpart_rgs_lex):**
        - Generates RGSs for set partitions in lexicographic order.
        - Uses an array m[] to keep track of the maximum value allowed for each position in the RGS.
        - The successor method finds the first digit that can be incremented, increments it, and adjusts the maxima accordingly.

     b. **Set Partitions into p Parts (setpart_p_rgs_lex):**
        - Generates RGSs for set partitions of an n-set into exactly p parts.
        - Initializes the RGS with a specific pattern that ensures p subsets.
        - The successor method checks if the digit is less than p and repairs the rightmost digits when needed.

     c. **Minimal-Change Order (setpart_rgs_gray):**
        - Generates RGSs for set partitions in minimal-change order, which corresponds to a Gray code for set partitions.
        - Uses an additional array of directions to track changes during recursion.
        - The successor method adjusts the RGS based on the direction array and repairs the tail when necessary.

     d. **Max-Increment RGS (rgs_maxincr):**
        - A generalization of RGS for set partitions where sk ≤ maxj<k(sj) + i, with i being a parameter.
        - The provided class generates these RGSs in lexicographic order.

These algorithms allow for efficient generation and manipulation of restricted growth strings representing set partitions, which is crucial for various combinatorial problems and applications.


This text discusses a specific type of restricted growth strings (RGS), which are sequences used to represent set partitions, along with their properties and generating functions. Here's a detailed summary:

1. **Restricted Growth Strings (RGS)**: RGSs are sequences where each element is non-decreasing and the difference between consecutive elements is bounded by a fixed integer 'i'. The text focuses on two types of RGSs: max-increment RGSs and F-increment RGSs.

   - **Max-Increment RGSs (sk ≤ i + maxj<k(sj))**: These are the standard RGSs, where each element sk is less than or equal to 'i' plus the maximum of all previous elements (sj for j < k). The text provides sequences for various values of 'i' (1, 2, 3, and 4) and mentions that these sequences correspond to specific entries in the Online Encyclopedia of Integer Sequences (OEIS).

   - **F-Increment RGSs**: These are a generalization of max-increment RGSs. In an F-increment RGS (sk ≤ F(k) + i), the 'maximum' function F(k) increases only when the last increment (sk - sk-1) is equal to 'i'. The text also provides sequences for various values of 'i' and mentions their OEIS entries.

2. **Generating Functions**:

   - **Exponential Generating Function (EGF)**: The EGF for RGSs with a given increment 'i' is provided in the text as exp[x + Σ(exp(j*x)/j! from j=1 to i) - 1]. This formula generates sequences that correspond to the number of RGSs of a given length.

   - **Ordinary Generating Function (OGF)**: The OGF for F-increment RGSs is stated as exp[Σ(exp(j*x)/j! from j=1 to i) - 1]. This function generates sequences that correspond to the total number of RGSs up to a given length.

3. **Sequences and their properties**: The text mentions several integer sequences related to RGSs, such as Bell numbers (A000110 in OEIS), and provides their EGFs and OGFs. It also states that the number of F-increment RGSs of length 'n' with increment 'i', denoted Fn,i, can be calculated using Stirling numbers of the second kind S(n, k) via the formula Fn,i = Σ(in-k * S(n, k)) from k=0 to n.

4. **Visual Representations**: The text includes figures that visually represent RGSs for different lengths and increments, along with their corresponding arrays of 'max' values (for max-increment RGSs) or 'F' values (for F-increment RGSs). These figures help in understanding the structure of these sequences.

In summary, this text discusses two types of generalized restricted growth strings and provides their generating functions, related integer sequences, and visual representations to understand their properties better.


The FKM (Fredericksen, Kessler, Maiorana) algorithm is an efficient method for generating all length-n k-ary pre-necklaces, which are a subset of the necklaces. Pre-necklaces are strings that serve as prefixes to some necklace. Not every pre-necklace is a necklace itself; however, they provide an effective way to generate all possible necklaces.

The algorithm works by iteratively modifying a zero-initialized string of length 'n' with values from 0 to k-1 (where k is the number of possible values for each element in the string). Here's a detailed explanation of its steps:

1. **Initialization**: Start with an all-zero string F = [f₁, f₂, ..., fn] and set j = 1.

2. **Visit pre-necklace F**: Check whether the current string F is a necklace or a Lyndon word based on specific conditions:
   - If 'j' divides 'n', then F is a necklace.
   - If 'j' equals 'n', then F is a Lyndon word.

3. **Find largest index for increment**: Locate the largest index 'j' where f_j < k-1. 
   - If no such index exists (meaning all elements in F are k-1), terminate the algorithm as we've reached the last pre-necklace, which is [k-1, k-1, ..., k-1].

4. **Increment and copy periodically**:
   - Increment f_j. 
   - Fill the suffix starting at position f_j+1 with copies of [f₁, ..., f_j], effectively extending the current pre-necklace. 

5. **Repeat**: Go back to step 2 with the updated string F.

Key points in the implementation are:
- The use of a delta sequence (dv_) to quickly determine whether the current pre-necklace is also a necklace or Lyndon word by checking divisors of 'n'.
- Optimizations such as skipping unnecessary increment checks when j equals n.

The provided C++ class `necklace` in FXT implements this algorithm, with additional methods for generating necklaces and Lyndon words directly without producing pre-necklaces first. The rate of generation for binary necklaces using this implementation is approximately 128 M/s.

The FKM algorithm is highly efficient for generating k-ary strings, making it suitable for applications involving necklace enumeration in combinatorics and other fields where the concept of necklaces plays a crucial role.


Title: Generating Binary Lyndon Words with Mersenne Exponent Length

This text discusses various algorithms to generate binary Lyndon words (BLWs) of lengths that are exponents of Mersenne primes, denoted as Mn = 2^n - 1. 

1. **Binary Lyndon Words Generation via Primitive Roots**: The first method described generates BLWs by using the binary expansions of powers of a primitive root r of Mn. A primitive root is an integer that generates all non-zero residues modulo Mn. For example, with n = 7 and M7 = 127, the primitive root r = 3 generates a sequence of BLWs as shown in Figure 18.1-B.

2. **Constant Amortized Time (CAT) Algorithm**: This algorithm is for generating k-ary pre-necklaces of length N. The core function, `crsms_gen`, uses recursion to fill an array `f` with values and visit the pre-necklaces when the length exceeds N. This method can generate binary, ternary, or 5-ary pre-necklaces efficiently.

3. **Order with Fewer Transitions**: Another approach generates binary BLWs in an order that minimizes transitions between successive words by selecting valid words from a modified binary Gray code. The routine `xgen` performs this generation, and Figure 18.1-C illustrates the output for 8-bit Lyndon words.

4. **Order with At Most Three Changes Per Transition**: This algorithm generates necklaces in an order where at most three elements change with each update. The provided recursion (`gen3`) produces binary length-8 necklaces, as shown in Figure 18.1-E. Selecting the necklaces from complemented Gray codes of n-bit binary words yields the same list.

5. **Binary Necklaces via Gray-Cycle Leaders**: This method generates BLWs by using cycle leaders of the Gray permutation, which can be obtained through Reed-Muller transforms of these leaders. Figure 18.1-G displays the correspondence between cycles and cyclic shifts for length-8 binary necklaces.

6. **Binary Necklaces via Cyclic Shifts and Complements**: This algorithm generates all nonzero binary necklaces using cyclic shifts and complements of the lowest bit. The provided code snippet uses functions `sigma` (cyclic shift) and `tau` (complement) to recursively generate BLWs in lexicographic order, as shown in Figure 18.1-H for lengths n = 3 to 8.

7. **Lex-min De Bruijn Sequence from Necklaces**: The text also describes obtaining the lexicographically minimal De Bruijn sequence by concatenating primitive parts of necklaces in lex order. An implementation class `debruijn` is provided for this purpose.

In summary, this text presents multiple algorithms to generate binary Lyndon words with lengths being Mersenne exponents and discuss various ways to order these BLWs to minimize transitions between successive words. It also explains how to derive the lex-min De Bruijn sequence from necklaces and showcases different methods for generating necklaces themselves using cyclic shifts, complements, and Gray permutation cycle leaders.


The provided text discusses two main topics related to necklaces, specifically binary necklaces, and Lyndon words (aperiodic necklaces), focusing on their count and properties. Here's a detailed summary and explanation:

1. **Binary Necklaces:**

   - A binary necklace is a circular sequence of 0s and 1s. For example, "0011" and "1100" are binary necklaces of length 4.
   - The total number of binary necklaces of length `n` (denoted as `Nn`) can be calculated using the formula:

     Nn = (1/n) Σ φ(d) * 2^(n/d), where d divides n, and φ is Euler's totient function.

   - This sequence (A000031) lists the number of binary necklaces for various lengths (`n`). The formula takes into account that each divisor `d` of `n` contributes φ(d) * 2^(n/d) unique necklaces, normalized by the total number of possible sequences (which is 2^n).

2. **Binary Lyndon Words:**

   - A binary Lyndon word is an aperiodic (non-repeating) circular sequence of 0s and 1s.
   - The count of binary Lyndon words of length `n` (denoted as `Ln`) is given by:

     Ln = (1/n) Σ µ(d) * 2^(n/d), where d divides n, and µ is the Möbius function.

   - This sequence (A001037) lists the number of binary Lyndon words for various lengths (`n`). Similar to necklaces, each divisor `d` of `n` contributes µ(d) * 2^(n/d) unique Lyndon words.

3. **Special Cases:**

   - For prime length `p`, the number of binary Lyndon words (and irreducible polynomials of degree `p`) equals L_p = N_p - 2. This is because there are exactly `(p-1)/p * p` Lyndon words with one '1', and so on, up to '(p-1)/p * p' Lyndon words with all '1s'.
   - The difference of 2 accounts for the necklaces consisting entirely of 0s or 1s.

4. **Relations between Necklace Counts:**

   - There's a relationship between the total count (`Nn`), necklaces with fixed zeros (`N(n, n0)`), and Lyndon words with fixed zeros (`L(n, n0)`) given by:

     Nn = Σ N(n, d), where the sum is over all divisors `d` of `n`.

   - This can be derived using Möbius inversion.
   - Similarly, Lyndon words satisfy Ln = Σ L(n, d).

5. **Binary Necklaces with Fixed Density:**

   - N(n, n0) is the count of binary necklaces of length `n` with exactly `n0` zeros (and `n-n0` ones).
   - The formula for N(n, n0) is:

     N(n, n0) = (1/n) Σ φ(j) * (n/j choose n0/j), where j divides both n and n0.

6. **Binary Lyndon Words with Fixed Density:**

   - L(n, n0) is the count of binary Lyndon words of length `n` with exactly `n0` zeros.
   - The formula for L(n, n0) is:

     L(n, n0) = (1/n) Σ µ(j) * (n/j choose n0/j), where j divides both n and n0.

The text also provides tables of these counts for small values of `n` and various fixed densities or zeros. These formulas and relationships provide ways to calculate the number of necklaces and Lyndon words with specific properties, which can be useful in combinatorics, coding theory, and other fields involving circular sequences.


The text discusses Hadamard matrices and conference matrices, which are special types of matrices with significant applications in coding theory, combinatorics, and signal processing. Here's a detailed summary and explanation:

1. **Hadamard Matrices**:
   - A Hadamard matrix is an N × N orthogonal matrix with entries ±1, where N is typically a power of 2. The property that makes it Hadamard is that the dot product of any two distinct rows (or columns) is zero and the dot product of any row (column) with itself is N/2.
   - For even N = 2n, an explicit construction using the Kronecker product of smaller Hadamard matrices exists: H_N = [H_(N/2) | H_(N/2); H_(N/2) | -H_(N/2)]. This construction doubles both the size and the number of rows/columns.
   - The determinant of an N × N Hadamard matrix is ±N^(N/2).

2. **Conference Matrices**:
   - A conference matrix C_Q (where Q = q + 1 for odd prime q) is a Q × Q matrix with zero diagonal and entries ±1, satisfying C_Q * C_Q^T = (Q-1) * I, where I is the identity matrix. If Q ≡ 1 mod 4, then C_Q is symmetric; if Q ≡ 3 mod 4, it's antisymmetric.
   - A Hadamard matrix can be obtained from a conference matrix: for symmetric matrices, simply add 1 to the diagonal. For antisymmetric matrices, construct a 2Q × 2Q matrix using [C_Q | I; -I | C_Q].

3. **Construction via Finite Fields**:
   - Conference and Hadamard matrices can also be constructed using finite fields GF(q^n), where q is an odd prime and n is a positive integer. The elements z0, ..., z_(q^n-1) are ordered, and the quadratic character of their differences determines the entries of C_Q.
   - To compute these characters efficiently, precompute a table using polynomial operations in GF(q).

4. **Hadamard Matrices via LFSR (Linear Feedback Shift Register)**:
   - This method constructs Hadamard matrices using maximum length binary shift register sequences (SRS), signed appropriately (-1 for 1s and +1 for 0s). The N × N matrix H is then filled with cyclic shifts of this sequence, with the first row/column set to all ones.

5. **Applications**:
   - Hadamard matrices are used in various areas such as signal processing (e.g., Walsh-Hadamard transforms), error-correcting codes, and combinatorial designs. Conference matrices are related to Hadamard matrices and have applications in coding theory and combinatorics.

In essence, the text discusses different methods for constructing special matrices with specific properties (±1 entries and orthogonality) that find applications in various fields of mathematics and engineering.


The text discusses the representation of combinatorial structures as paths or cycles in directed graphs. It uses the example of Gray codes of n-bit binary words, which are sequences where only one bit changes between consecutive words. A convenient way to visualize this search space is through a directed graph, with nodes representing the binary words and edges connecting nodes if their values differ by exactly one bit.

The text introduces terminology for working with graphs: "node" instead of "vertex," "edge" (sometimes called arc), and terms like simple path, full path, cycle (or circuit), Hamiltonian cycle, Hamiltonian graph, loops (edges starting and ending at the same node), pseudo-graphs (graphs that may contain loops), and multigraphs (graphs with multiple edges between nodes).

The text also describes a simple representation of directed graphs using arrays for storing nodes and outgoing edges. An example is given for initializing the complete graph, where every node has an edge connecting it to all other nodes.

To search full paths starting from some position p0, additional data structures are required: arrays to record visited nodes (rv_[]) and tags indicating whether a node has been visited (qq_[]). A recursive algorithm is employed for the path search, which can find at least one object, generate all objects, or show that no such object exists. The method used is called backtracking.

The text provides examples of using this graph representation to search for permutations in a complete graph and De Bruijn sequences in a De Bruijn graph. It also mentions a modified De Bruijn graph for generating complement-shift sequences.

In summary, the text outlines how directed graphs can be used as a tool for searching combinatorial structures, with examples involving permutations and sequences like Gray codes and De Bruijn sequences. The algorithms presented are based on backtracking and can be optimized to handle larger graph sizes efficiently.


The provided text discusses two types of Gray codes, Modular Adjacent Changes (MAC) and Adjacent Changes (AC), and their search using a conditional graph traversal algorithm. 

1. **Modular Adjacent Changes (MAC) Gray Codes**: These are Gray codes where the difference between successive elements in the delta sequence can only change by ±1 modulo n. The text presents an algorithm to find such paths, with the condition that canonical paths start as 0-->1-->3. The search is performed using `all_cond_paths()` function, which takes a condition-imposing function (`cfunc_mac` in this case) and other parameters.

   The condition function (`cfunc_mac`) checks if the difference between successive delta values (modulo n) equals ±1. It does this by comparing the current value (`p`), previous value (`p1`), and the value before that (`p2`). If the bitwise AND of `c` (the difference between `p` and `p1`) and `c1` (the difference between `p1` and `p2`, rotated left by one) is true, or vice versa, the condition is met.

2. **Adjacent Changes (AC) Gray Codes**: These are Gray codes where the difference between successive elements in the delta sequence equals ±1. The search algorithm for AC paths discards track-reflected solutions and considers canonical paths to start with a value less than or equal to ⌈n/2⌉. 

   The condition function (`cfunc_ac`) checks if the difference between successive delta values equals ±1, but avoids track-reflected solutions by ensuring the first value is less than `cf_mt` (mid track). It performs similar bitwise operations as `cfunc_mac`, comparing the current value (`p`), previous value (`p1`), and the value before that (`p2`).

The text also mentions that MAC Gray codes exist for n ≤ 7, while AC Gray codes exist for n ≤ 6. No MAC or AC Gray codes are known to exist for n ≥ 8 and n = 7 respectively. The search times indicate the computational complexity of these problems increases significantly with n.

The text concludes by mentioning an ad-hoc algorithm to compute delta sequences for AC Gray codes for n ≤ 6, though no specific details about this algorithm are provided in the given excerpt.


The text discusses the application of Gray codes to Lyndon words, which are non-repeating binary strings that are lexicographically smaller than all their rotations (cyclic permutations). 

In the context of graph theory, a directed graph is constructed where nodes represent n-bit Lyndon words. Edges connect nodes if one word can be transformed into another by flipping exactly one bit (a single transition). The goal is to find paths through this graph that traverse all nodes exactly once and return to the starting node, known as Hamiltonian cycles in directed graphs or Eulerian circuits when each node has equal in-degree and out-degree.

1. **Gray Codes for Lyndon Words**: The text starts by discussing Gray codes for n-bit binary Lyndon words where n is a prime number. A Gray code is a sequence of n-bit binary numbers such that each pair of consecutive numbers differs in only one bit. For Lyndon words, this means that the sequence visits all non-repeating binary strings exactly once while changing only one bit at a time. The text mentions a 5-bit example and shows how to extend this concept to larger prime numbers like 7 and 11 using graph search methods.

2. **Graph Search with Edge Sorting**: To find such paths, an algorithm is employed that traverses the graph nodes in an arbitrary order but sorts the outgoing edges of each node according to a specific comparison function (lexicographic order in this case). This sorting can significantly impact the search efficiency, as demonstrated by comparing the time it takes to find the first path for both sorted and unsorted graphs.

3. **Lucky Paths**: The concept of "lucky paths" is introduced. A lucky path is one where no U-turns (backtracking) occur during traversal—meaning that once a node is visited, its neighbors are always traversed in the correct order without needing to revisit any previously explored paths. If such a lucky path exists for a given graph and starting point, it is typically found very quickly, as the number of operations scales linearly with the number of edges.

4. **Edge Sorting Impact**: The example provided shows that sorting the edges can drastically reduce the time needed to find the first path in the graph. For an 8-bit problem, the unsorted graph takes 1.14 seconds, whereas the sorted one requires only 0.03 seconds.

5. **Lyndon Words Graph Construction**: Nodes in this graph correspond to n-bit Lyndon words, and edges exist between two nodes if their corresponding Lyndon words differ by a single bit flip (one transition). The path through the graph represents a Gray code for the Lyndon words.

6. **Finding Gray Codes via Graph Search**: The algorithm searches for paths that visit each node exactly once (Hamiltonian cycle) or, in some cases, allows for arbitrary rotations of the Lyndon words (Eulerian circuit with degree equality). The search is performed using a recursive function `next_path()`, which explores possible transitions from the current node.

7. **Challenges with Larger Primes**: The text mentions that while finding Gray codes for smaller primes like 7 is feasible, larger ones (like 11 and 13) pose significant computational challenges due to the exponential growth in the number of nodes and edges as n increases. Even after sorting the edges, the search times become prohibitive for very large graphs.

In summary, this text explores the application of Gray codes to Lyndon words represented as directed graphs. It highlights how edge sorting can significantly impact search efficiency and introduces the concept of "lucky paths"—traversals without backtracking that can be found rapidly if they exist. The challenges in applying these methods to larger graphs due to exponential growth in complexity are also discussed.


The Decimation in Time (DIT) FFT algorithm, also known as the Cooley-Tukey FFT algorithm, is a method for efficiently computing the Discrete Fourier Transform (DFT) of a sequence. This algorithm leverages the property that the k-th element of the DFT can be expressed as a combination of the DFTs of the even and odd indexed subsequences of the original sequence.

Here's a detailed explanation:

1. **Notation**:
   - Let `a` be a length-n sequence, where n is a power of 2.
   - `a(even)` and `a(odd)` denote the length-n/2 subsequences of even and odd indexed elements, respectively.
   - `a(left)` and `a(right)` denote the left and right subsequences, respectively.

2. **Key Identity**:
   The identity that forms the basis of the DIT FFT is:

   F[a]k = Σ x=0^n-1 ax * zx k = Σ x=0^n/2-1 a2x * z2x k + Σ x=0^n/2-1 a2x+1 * z(2x+1) k

   Here, `z` is an n-th primitive root of unity (zn = 1 and zj ≠ 1 for 0 < j < n), and `σ` is the sign of the transform.

3. **Rewriting the DFT**:
   The above identity can be rewritten in terms of j and δ, where k ∈{0, 1, ..., n −1} = j + δ * n/2:

   F[a]j+δ*n/2 = Σ x=0^n/2-1 a(even)x * z2x (j+δ*n/2) + zj*n/2 * Σ x=0^n/2-1 a(odd)x * z2x (j+δ*n/2)

   This shows how to compute the j+δ*n/2-th element of the DFT from the DFTs of the even and odd indexed subsequences.

4. **Algorithm**:
   The DIT FFT algorithm proceeds recursively by dividing the sequence into even and odd indexed subsequences, computing their DFTs, and then combining these results to obtain the DFT of the original sequence. This process is known as decimation in time because it involves reducing the sample rate (decimating) by a factor of 2 at each step.

5. **Time Complexity**:
   The DIT FFT algorithm has a time complexity of O(n log(n)), which is a significant improvement over the naive O(n^2) algorithm for computing the DFT. This makes it a crucial tool in many areas of signal processing, image processing, and other fields where Fourier transforms are used.

6. **Sign of Transform**:
   The operator `S` in the algorithm depends on the sign of the transform (σ = ±1). It represents a shift operation that multiplies each element by z^σ*2π*i*x/n, where x is the index and n is the length of the sequence.


The provided text describes various aspects of Fast Fourier Transform (FFT) algorithms, focusing on higher radix methods to reduce trigonometric computations. Here's a summary and explanation of the key points:

1. **Radix-2 Decimation in Time (DIT) FFT**: This is the basic form of FFT, which recursively divides the input sequence into even and odd indexed elements, applies a twiddle factor (S0/2 or S1/2), and combines the results. The complexity is O(n log2 n).

   - **Recursive implementation**: Uses multiple function calls to compute the transform, with each step having O(n) work.
   - **Iterative implementation**: Avoids function call overhead by using a single loop, but still requires workspace. It can be made in-place and non-recursive.

2. **Radix-4 DIT FFT**: This extends the radix-2 method to use four subsequences (0%4, 1%4, 2%4, 3%4) instead of two. It reduces trigonometric computations by simplifying multiplications with complex factors (0, ±i) into simpler operations.

   - **General radix-r DIT FFT**: The formula for the general case is provided, allowing for any radix r that divides n.

3. **Radix-2 Decimation in Frequency (DIF) FFT**: This method splits the Fourier sum into left and right halves, recursively applying the transform to these halves with twiddle factors.

   - **Recursive implementation**: Similar to DIT, but with a different memory access pattern that can be less efficient for large n.
   - **Iterative implementation**: Non-recursive version that works in-place and saves trigonometric computations by swapping inner loops.

4. **Saving Trigonometric Computations**: The text discusses two methods to reduce the computational cost of sine and cosine calculations:

   - **Lookup Tables**: Precompute and store necessary values, reducing the need for real-time calculations. This is effective for FFTs of small lengths but can lead to cache problems for large sequences.
   - **Recursive Generation**: Use trigonometric recursion to compute values, with a stable version provided that loses less than 3 bits of precision even for very long FFTs.

5. **Higher Radix FFT Algorithms (Radix-4 and General r)**: These methods further reduce trigonometric computations by using more subsequences (r instead of 2). They also simplify special cases where sines and cosines equal ±√1/2. The general radix-r DIT and DIF FFT steps are provided.

6. **Implementation of Radix-r FFTs**: For higher radices, the revbin_permute routine is replaced by a radix_permute function that swaps elements based on reversing their radix-r expansion. The pseudocode for a radix r = px DIT FFT is given.

In summary, the text presents various FFT algorithms and methods to optimize their performance, focusing on higher radix techniques to reduce trigonometric computations and improve efficiency.


The Split-Radix Algorithm is a method used in Fast Fourier Transform (FFT) computation that combines both radix-2 and radix-4 decompositions to achieve a lower operation count compared to the radix-4 FFT. This algorithm splits the length-N=2^n FFT into one length-N/2 and two length-N/4 FFTs, using relations from both radix-2 (Decimation in Frequency, DIF) and radix-4 decompositions.

The key idea behind the Split-Radix FFT is to use a radix-2 relation for even indices and a radix-4 splitting for odd indices, albeit in a slightly reordered form. 

For the Decimation In Time (DIT) version of the split-radix FFT:

1. The length-N/2 FFT is calculated using a radix-2 decomposition:
   F[a](0/2)^n = F[a(0%2)]^n + S1/2 * F[a(1%2)]^n, where S1/2 is the sign factor (±1 or ±i) depending on the FFT sign (is).
   
2. The length-N/4 FFTs are calculated using a radix-4 decomposition:
   F[a](1/4)^n = F[a(0%4)]^n - S2/4 * F[a(2%4)]^n + i*σ * S1/4 * F[a(1%4)]^n - S2/4 * F[a(3%4)]^n,
   F[a](3/4)^n = F[a(0%4)]^n - S2/4 * F[a(2%4)]^n - i*σ * S1/4 * F[a(1%4)]^n - S2/4 * F[a(3%4)]^n,
   where σ is the sign of the transform (+1 or -1), and S2/4 and S1/4 are sign factors (±1 or ±i).

The operation count for Split-Radix FFT is lower than that of radix-4 FFT due to this efficient use of both radix-2 and radix-4 decompositions. It's particularly advantageous when N is large, as it reduces the number of complex multiplications compared to a pure radix-4 approach. 

In practical implementations, the split-radix algorithm would involve procedures for calculating each FFT step (length-N/2 and length-N/4), with these procedures being further optimized for specific cases (e.g., hard-coded for σ=±1 or using complex numbers). The actual splitting of work into radix-2 and radix-4 steps, as well as the handling of sign factors, would be implemented within these procedures. 

The Split-Radix DIF algorithm can be seen as an extension of this concept, where both even and odd indices are handled with a combination of radix-2 and radix-4 relations, ultimately achieving a similar reduction in operation count while offering flexibility in handling different FFT signs.


The provided code snippets appear to be a C++ implementation of the Split-Radix algorithm for computing the Discrete Fourier Transform (DFT), specifically designed for real-valued inputs. This algorithm is efficient and takes advantage of the symmetries of the DFT for real inputs to reduce computational complexity.

Here's a summary and explanation of the key sections:

1. **Complex Multiplication and Splitting:**

   - Lines 43-52: These lines define complex multiplication for two pairs of complex numbers (r1, s1) and (ss1, cc1), and (r2, s2) and (cc3, ss3). The results are split into real and imaginary parts. This is used in the subsequent butterfly operations of the FFT algorithm.

2. **Butterfly Operations:**

   - Lines 54-60: These lines perform complex multiplications followed by summation/subtraction (represented by `sumdiff3`) for processing pairs of data points. This is a key step in the FFT algorithm, known as the "butterfly operation."

3. **Decrement and Loop Control:**

   - Lines 61-82: These lines handle loop control and decrementing the index 'id' to traverse through all necessary data points in the FFT computation. The variable 'i0' is used to keep track of the current data point being processed.

4. **Permute and Swap Operations:**

   - Lines 83-92: These lines perform permutations and swaps on the data arrays, which are essential for arranging the output in the correct order (revbin_permuted). This step is necessary because the FFT algorithm naturally produces results in a different order than the standard Fourier Transform.

5. **Additional Utility Functions:**

   - The code uses several utility functions (`revbin_permute`, `swap`, etc.) which are not detailed in these snippets but are likely defined elsewhere in the source file or library, handling tasks like array permutation and swapping elements within arrays.

6. **DIF Core Implementation:**

   - The DIF (Decimation In Frequency) core is implemented starting from line 21 in the provided code. This part of the FFT algorithm processes the data in frequency space by dividing it into even and odd indexed components, then recursively applying the butterfly operations.

The Split-Radix FFT algorithm described here is a more complex version of the Cooley-Tukey radix-2 FFT, designed to reduce computational complexity for certain types of inputs (like real-valued sequences) by exploiting their symmetries. The provided pseudocode and C++ implementations serve as a guideline for understanding and implementing this algorithm in various programming languages.


The text provided discusses various aspects of convolution, correlation, and related algorithms using the Fast Fourier Transform (FFT). Here's a detailed summary and explanation:

1. **Cyclic Convolution**: This is the type of convolution considered in the text. For two sequences `a` and `b` of length `n`, the cyclic convolution `h = a ⊛ b` is defined as `h_τ = Σ_{x+y ≡ τ (mod n)} a_x * b_(τ-x)`. This means that indices wrap around, creating a circular or periodic convolution.

2. **Direct Computation**: The text provides a C++ implementation of the direct computation of cyclic convolution using nested loops. This method, while straightforward, has a time complexity of O(n^2), making it inefficient for large sequences.

   ```cpp
   template <typename Type>
   void slow_convolution(const Type *f, const Type *g, Type *h, ulong n) {
       for (ulong tau=0; tau<n; ++tau) {
           Type s = 0.0;
           for (ulong k=0; k<n; ++k)
               s += f[k] * g[(tau-k+n)%n];
           h[tau] = s;
       }
   }
   ```

3. **Fast Fourier Transform (FFT) Based Convolution**: The text focuses on algorithms that leverage the FFT to compute convolution more efficiently. These methods have a time complexity of O(n log n), making them suitable for large sequences.

   - **One-dimensional Convolution via FFT**: This method involves transforming both sequences using FFT, element-wise multiplication in the frequency domain, and inverse FFT to obtain the convolution result. The complex conjugate symmetry property is used to reduce computational cost by only computing half of the frequency domain.

   - **Multi-dimensional Convolution via FFT**: For multi-dimensional arrays (e.g., matrices), the text discusses applying 1D FFTs first along rows, then columns (row-column algorithm) or vice versa. Transposing the array before the column pass can improve performance due to better memory access patterns.

   - **Matrix Fourier Algorithm (MFA)**: This is a variant of the row-column method for 1D FFTs, optimized for data lengths `n = R * C`. It involves applying FFTs on columns and rows, followed by element-wise multiplication with trigonometric factors and a final matrix transposition.

   - **Transposed Matrix Fourier Algorithm (TMFA)**: This is a variation of the MFA that accesses memory in consecutive address ranges, avoiding the need for a final transpose if an inverse transform follows immediately.

4. **Weighted Convolution**: The text introduces weighted convolution, where each element of one sequence is multiplied by a corresponding weight before convolution. This can be computed efficiently using FFT-based methods by incorporating the weights into the frequency domain multiplication step.

5. **Convolution for Z-Transform Computation**: The text explains how fast convolution algorithms can be used to compute the Z-transform of sequences of arbitrary length, which is a crucial operation in signal processing and control theory.

6. **Rader's Algorithm**: For arrays of prime length, Rader's algorithm provides an efficient method for computing the discrete Fourier transform (DFT) using convolution. This algorithm leverages the properties of cyclotomic polynomials to reduce the computational complexity compared to direct DFT computation.

In summary, the text presents various FFT-based algorithms for efficient convolution and related operations, emphasizing their time complexity advantages over direct computation methods. It covers one-dimensional and multi-dimensional cases, weighted convolutions, Z-transform computation, and specialized algorithms like Rader's algorithm for prime lengths.


The provided text discusses various aspects of convolution and correlation, including both direct computation methods and Fast Fourier Transform (FFT)-based approaches. Here's a detailed summary:

1. **Cyclic Convolution**: This is the operation of two sequences where the output is wrapped around, meaning that elements from the end of one sequence can pair with elements from the beginning of the other. The complexity for direct computation is O(n^2), making it slow for large n but suitable for small lengths.

2. **FFT-based Cyclic Convolution**: Using FFT allows convolution to be computed more efficiently, requiring only O(n log(n)) operations. This method exploits the convolution property of the Fourier transform (F[a ⊛b] = F[a] · F[b]). The algorithm involves three main steps:
   - Transform both input sequences using a Forward FFT.
   - Perform element-wise multiplication in the transformed domain.
   - Apply an Inverse FFT to get the result.

3. **Linear (Acyclic) Convolution**: This is similar to cyclic convolution but without wrapping around indices, resulting in a sequence of twice the length. It can be computed using zero-padded cyclic convolution. The semi-symbolic table for linear correlation differs from that of cyclic correlation, with elements arranged in a full matrix rather than a triangular one.

4. **Cyclic Correlation**: This measures the similarity between two sequences as a function of the displacement of one relative to the other. It's also known as circular correlation. The semi-symbolic table for this operation is provided, showing non-wrapping index relationships.

5. **Direct Computation Methods**: For both cyclic and linear correlations, direct computation methods are given in C++ code. These involve nested loops that compute sums of products between elements from different shifts of the sequences. To avoid index overflow/underflow checks, an alternative version is provided that uses modulo arithmetic within the loops.

6. **Zero-padded Data for Linear Correlation**: When computing linear correlation (which requires zero-padding), specific elements (f[k], g[k] for k=n/2...n-1) are assumed to be zero, optimizing the computation by avoiding unnecessary multiplications with zeros.

7. **Auto-correlation and Cross-correlation**: Auto-correlation refers to correlating a sequence with itself, while cross-correlation involves distinct sequences. The term "auto-correlation function" (ACF) is often used for auto-correlation sequences.

In summary, the text covers both theoretical aspects (definitions, semi-symbolic tables) and practical computational methods for cyclic and linear convolutions/correlations, with a focus on optimization techniques like zero-padding and FFT-based approaches to improve efficiency.


The provided text discusses various aspects of correlation, convolution, and related algorithms. Here's a detailed summary and explanation:

1. **Correlation Algorithms**: Two correlation algorithms are presented. The first algorithm (lines 12-14) computes the cyclic correlation between two sequences `f[]` and `g[]`, storing the result in an array `h[]`. It iterates over the sequences, multiplying corresponding elements and summing the results. This algorithm has a time complexity of O(n^2).

   The second algorithm (lines 17-22) is similar but designed for self-correlation (when one sequence is the same as the other), which can be more efficiently computed using Fast Fourier Transform (FFT) techniques.

2. **Fast Correlation via FFT**: This method leverages the Fast Fourier Transform to calculate correlation more efficiently. The algorithm (lines 7-21) first transforms both input sequences using an FHT (Fast Hartley Transform), then multiplies each element by its complex conjugate (or real conjugate for real sequences), and finally transforms back using another FHT. This method significantly reduces computational complexity compared to the O(n^2) algorithm.

3. **Correlation, Convolution, and Circulant Matrices**: The text explains how cyclic correlation and convolution correspond to multiplication with circulant matrices. For a given sequence `a[]`, its cyclic correlation with itself (autocorrelation) can be represented as a matrix-vector product with a specific circulant matrix `Ra`. Similarly, the cyclic convolution is represented by another circulant matrix `Ca`. The Fourier transform diagonalizes these circulant matrices, simplifying their analysis and computation.

4. **Weighted Fourier Transforms and Convolutions**: These are variations of standard Fourier transforms and convolutions that involve a weight sequence `v[]`. The weighted Fourier transform (lines 1-8) scales each input element by its corresponding weight before applying the FFT. Its inverse operation (lines 9-16) reverses this scaling after the inverse FFT. Weighted convolution (lines 22.4.2) is a modified form of convolution where each summand in the standard convolution definition is multiplied by a weight. The weighted cyclic convolution can be computed efficiently using the FFT, as shown in the provided code snippets.

5. **Negacyclic Convolution**: This is a specific type of weighted cyclic convolution where weights are chosen such that `V_n = -1`. It results in a negacyclic (or skew circular) convolution, which separates the cyclic and wrapped parts of the standard convolution more distinctly for real sequences. The negacyclic convolution can be computed efficiently using FFT-based methods, as demonstrated by the provided code snippets.

In summary, the text covers various algorithms for computing correlation and convolution, including naive O(n^2) methods, FFT-based fast correlation, and weighted/negacyclic convolutions. These techniques offer trade-offs between simplicity and computational efficiency, with FFT-based methods generally providing significant speedups for large sequences.


The provided text discusses the Walsh Transform, a discrete transform similar to the Fourier Transform but without involving multiplications. It's also known as the Walsh-Hadamard Transform or simply Hadamard Transform. The Walsh Transform uses a Walsh-Kronecker basis, which consists of functions that take on values +1 and -1.

The text presents a 2D visualization of these basis functions in Figure 23.1-A, where asterisks denote the value +1 and blank entries denote -1. The Walsh Transform can be derived from a Fast Fourier Transform (FFT) by removing multiplications with sines and cosines.

The text also mentions a specific implementation of the Walsh Transform using a radix-2 Decimation in Time (DIT) FFT, which is presented as a C++ function `slow_walsh_wak_dit2`. This function takes a double pointer `f` to the input data and its length `ldn`, and it seems to perform the Walsh Transform using a nested loop structure.

The outer loop iterates over different bit lengths (`ldm`) from 1 to `ldn`, while the inner loops handle the actual transformation process. The variable `m` represents the current bit length, and `mh` is half of that. The variable `r` iterates over the input data in steps of `m`.

However, the text notes a problem with this implementation without providing further details. Despite this issue, understanding the structure of this code can give insights into how to implement a Walsh Transform using an FFT-like approach.

The Walsh Transform has applications in various fields, including signal processing and error-correcting codes, due to its efficiency and lack of multiplication operations. It's also used in the computation of XOR (dyadic) convolutions, which can be done efficiently by the Walsh Transform. Other related transforms mentioned in the text include the Slant Transform, Reed-Muller Transform, and Arithmetic Transform.


The text discusses various aspects of the Walsh transform, a type of discrete Fourier-like transform used for signal processing. Here's a detailed summary and explanation:

1. **Walsh Transform Basics**: The Walsh transform is a real-valued, orthogonal transform that operates on sequences of integers. It has two main properties: it's self-inverse (its eigenvalues are ±1), and it doesn't involve multiplication operations, making it computationally efficient.

2. **Radix-2 Decimation in Time (DIT) Algorithm**: The given code snippet implements a radix-2 DIT algorithm for the Walsh transform. This algorithm performs n log2(n) additions (and subtractions), making it faster than the naive approach but still suffers from non-local memory access patterns, which can degrade performance on modern processors due to cache inefficiency.

3. **Improved Algorithm**: A more efficient version of the Walsh transform is presented, which achieves a speedup of about 8x for n = 2^21 (with double precision) compared to the original algorithm. This improvement is achieved by reordering the memory access pattern, making it more cache-friendly.

4. **Eigenvectors of the Walsh Transform**: The text explains how to compute eigenvectors of the Walsh transform. Given a sequence 'a', two eigenvectors can be computed: u+ = W(a) + a (with eigenvalue +1) and u- = W(a) - a (with eigenvalue -1). The routine `walsh_wak_eigen` generates an eigenvector by adding a scaled delta peak to the corresponding basis function.

5. **Kronecker Product**: This mathematical concept is introduced as a powerful tool for dealing with orthogonal transforms and their fast algorithms. It allows expressing complex matrix operations (like the Walsh transform) in terms of simpler ones, facilitating algorithm development and analysis.

6. **Higher Radix Walsh Transforms**: The text discusses methods for generating short-length Walsh transforms (DIF and DIT variants) using a generator approach. This allows creating custom transform routines for various lengths, including higher radix (e.g., radix-4, radix-8) algorithms.

7. **Performance Comparison**: The text presents performance comparisons of different Walsh transform implementations, including a matrix variant. It highlights that the choice of algorithm depends on factors like cache size and transform length, with the localized Walsh transform (avoiding expensive transposition operations) generally outperforming the matrix version for large datasets.

In summary, the text covers the theoretical aspects and practical implementations of the Walsh transform, including improvements in memory access patterns, eigenvector computation, and higher-radix algorithms. It also introduces the Kronecker product as a useful mathematical tool for analyzing and developing fast transforms.


The text discusses the Walsh-Hadamard Transform (WHT) and its variants, focusing on localized versions that provide improved performance for large data sets by optimizing memory access patterns. Here's a detailed summary and explanation of the key points:

1. **Localized Walsh Transforms**: These are optimized implementations of the Walsh-Hadamard Transform (WHT) designed to minimize cache misses and improve performance on large data sets. They are based on a decimation-in-frequency (DIF) or decimation-in-time (DIT) approach, depending on whether the transform is executed in a forward (DIF) or reverse (DIT) order.

2. **Memory Access Pattern**: The localized algorithms use a specific memory access pattern that results in excellent performance for large transforms. This pattern involves processing smaller chunks of data at a time, keeping more data in the cache when larger sub-arrays are accessed.

3. **Comparison with Matrix Algorithm**: Figure 23.5-A compares the speed of localized Walsh transforms (both DIF and DIT) with the matrix algorithm. For small sizes, the localized algorithms have similar speed to the radix-4 version they fall back to. However, for larger sizes, the localized algorithms outperform the matrix algorithm, even when considering only one transposition. The DIF version is slightly faster for very large transforms due to its starting with smaller chunks of data.

4. **Recursive Implementation**: Both DIF and DIT versions are implemented recursively, with base cases handling small transform sizes directly (without falling back to full Walsh transforms). This recursive structure allows for efficient execution of larger transforms by breaking them down into smaller sub-problems.

5. **Iterative Versions**: Iterative versions of the algorithms are also discussed, with the DIF algorithm executing Haar transforms at positions f + 2, f + 4, f + 6, ..., and the length of the transform at position f + s determined by the lowest set bit in s. The DIT algorithm follows a similar pattern but with reversed binary words in reversed lexicographic order.

6. **Walsh-Paley Transform**: A different ordering of the Walsh basis, known as the Walsh-Paley basis (figure 23.6-A), is also mentioned. This transform can be computed using the `walsh_pal` function provided in [FXT: walsh/walshpal.h].

In summary, the text presents optimized localized Walsh transform algorithms that leverage efficient memory access patterns to achieve better performance for large data sets. It compares these localized transforms with a matrix algorithm and discusses iterative versions of both decimation-in-frequency (DIF) and decimation-in-time (DIT) algorithms. Additionally, it briefly mentions the Walsh-Paley transform as an alternative basis ordering for the Walsh Transform.


The text discusses the Sequency-ordered Walsh transforms, focusing on two functions to compute the k-th basis function of the transform: walsh_wal_basis() and walsh_wak(). 

1. **Walsh-Paley Basis (walsh_pal_basis())**: This function generates a Walsh-Paley basis, which is ordered by frequency. The sequence starts with the first element having a sequency of 0, and each subsequent line represents an increment in sequency value. Asterisks denote a value of +1, while blank entries represent -1.

2. **Walsh-Kacmarz Basis (walsh_wal_basis())**: This function generates a Walsh-Kacmarz basis, which is sequency-ordered. The sequency here refers to the number of sign changes in each Walsh function. Unlike the Walsh-Paley basis, this one doesn't increase linearly with sequence numbers; instead, it jumps between values. Asterisks denote +1, and blanks represent -1.

The sequency ordering is important because in many applications, particularly signal processing, it's beneficial to order the basis functions by their 'speed' or frequency content of the signal they represent.

To achieve this ordering, a three-step process is described:

- `n = (1UL<<ldn);` sets `n` to 2 raised to the power of `ldn`. This computes the total number of basis functions for the given depth `ldn`.
- `walsh_wak(f, ldn);` applies a Walsh-Kacmarz transform, ordering the basis functions according to their binary representation.
- `revbin_permute(f, n);` and `inverse_gray_permute(f, n);` apply permutations to rearrange the order of the transformed data into sequency-order. The first permutation (`revbin_permute`) is a reverse bit-reversal (or bit-reverse) permutation, which is commonly used for fast Fourier transforms. The second permutation (`inverse_gray_code`), inverts the Gray code ordering applied by `walsh_wak`.

The sequency-ordered transform can be computed more efficiently using the radix-2 decimation in frequency (DIF) algorithm, detailed in `walsh_wal_dif2_core()`. This core routine is then wrapped in a function (`walsh_wal`) that applies the necessary permutations before and after the DIF computation.

An alternative ordering of sequencies, first even ascending then odd descending, is also presented in `walsh_wal_rev()`, using a decimation-in-time (DIT) approach instead of DIF. This alternative order can be useful for certain applications, such as wavelet transforms.

In summary, the text discusses different methods to order Walsh basis functions based on their sequency (frequency content), providing source code and visual representations of these ordered bases. It also explains the procedures used to achieve this ordering in the context of computational efficiency for transforms like the Fast Walsh-Hadamard Transform.


The provided text describes various forms of the Walsh Transform, a discrete transform named after the Polish mathematician Joseph Walsh. The Walsh Transform is used in signal processing and related fields for analyzing functions or sequences into their frequency components. Here's a detailed summary and explanation:

1. **Reversed Sequency-Ordered Walsh Transform (Figure 23.7-B):**
   - This transform uses basis functions with specific sequencies (patterns of '1's) as shown in the figure. The sequencies range from n/2 down to 1, where 'n' is the length of the sequence.
   - Basis functions are represented by asterisks (*) for +1 and blank spaces for -1.
   - The transform can be computed using three methods: a combination of reverse binary permutation (revbin_permute), Gray code permutation (gray_permute), and Walsh-Wak (walsh_wak).

2. **Self-Inverse Walsh Transform with Sequencies n/2 or n/2 - 1 (Figure 23.7-C):**
   - This transform's basis functions have sequencies of either n/2 or n/2 - 1 at even and odd indices, respectively. 
   - It is self-inverse, meaning applying the transform twice to a function yields the original function.
   - The transform can be computed using two methods: first by applying Gray code permutation (grs_negate), reverse binary permutation (revbin_permute), and Walsh-Gray (walsh_gray), then reversing these operations; or vice versa.

3. **Walsh Transform with Sequencies n/2 for Even Indices, n/2 - 1 for Odd Indices (Figure 23.7-E):**
   - Similar to the previous variant but swaps even and odd indices' sequency types.
   - This transform can also be computed using two methods: first applying reverse binary permutation, negating the sequence, Walsh-Gray, then reversing these operations; or vice versa.

The provided C++ code snippets (using templates for type generality) define functions to compute these variants of the Walsh Transform. The basis function computations use bitwise operations and logical functions like 'parity' and 'revbin', with the gray_code function generating Gray codes, which are used in permutations.

These transforms have applications in areas like signal processing, error-correcting codes, and data compression due to their fast computation (O(n log n) using a radix-2 decimation-in-time Fast Walsh Transform algorithm) and ability to represent signals in frequency space efficiently.


The provided code snippets are related to various transform algorithms used in digital signal processing, specifically focusing on the Walsh Transform and its relatives. Here's a detailed explanation of each section:

1. **Walsh-Hadamard Transform (WHT)**: The first part discusses two variations of the Walsh-Hadamard Transform (WHT), which is a type of orthogonal transform named after Joseph L. Walsh and Marshall H. Stone (Hadamard).

   - **walsh_q2_basis()**: This function computes the Walsh-Q transform, also known as the dyadic convolution. It takes an array `f`, its length `n`, and an index `k` as input. The function uses a semi-symbolic scheme (Figure 23.8-A) to calculate the result. The key steps include calculating `qk` using `grs_negative_q(k)`, reversing and gray coding the index `k`, and then iterating over `i` from 0 to `n-1`. For each `i`, it calculates `x` as the XOR of `i` and `k`, applies parity function, calculates `qi`, performs an XOR operation with `qk` and `qi`, and finally assigns `-1` or `+1` to `f[i]` based on whether `x` is zero.

   - **dyadic_convolution()**: This function computes the dyadic convolution using a fast algorithm involving Walsh Transform. It takes two arrays `f` and `g`, along with their length `ldn`. The function performs Walsh Transforms on both input arrays, multiplies corresponding elements, and then applies an inverse Walsh Transform to obtain the result in array `g`.

2. **Slant Transform**: This is a variant of the discrete Fourier transform (DFT) or the fast Fourier transform (FFT). It involves pre- and post-processing steps in addition to a Walsh Transform.

   - **slant()**: The slant function performs the slant transform on an input array `f` with length `ldn`. After applying a Walsh Transform, it applies a series of operations (inverse gray permutation, unzip_rev(), and revbin_permute()) to rearrange the elements.

   - **inverse_slant()**: This function computes the inverse slant transform. It involves post-processing steps in reverse order: `revbin_permute()`, `zip_rev()`, and `gray_permute()`, followed by a Walsh Transform.

3. **Arithmetic Transform (Y+ and Y-)**: These are two forms of an arithmetic transform, which are mutually inverse. Their basis functions are shown in Figure 23.10-A.

   - **arith_transform_plus()** and **arith_transform_minus()**: These functions compute the positive and negative signs of the arithmetic transform, respectively. They use a radix-2 decimation-in-frequency (DIF) algorithm similar to the WHT but with different operations in the inner loop. The length-2 transforms are also provided for both Y+ and Y-.

In summary, these code snippets demonstrate various transform algorithms used for different purposes, such as convolution, slanting data, and arithmetic transformations. They showcase how these transforms can be implemented efficiently using techniques like fast algorithms (Walsh Transform) and permutation operations. Each transform has its specific use case depending on the application's requirements.


The text discusses several discrete transforms, including the Walsh-Hadamard (W), arithmetic (Y), Reed-Muller (R), and reversed Reed-Muller (E) transforms. These transforms are self-inverse and have basis functions that can be visualized as matrices or bit arrays.

1. **Walsh-Hadamard Transform (W):** This transform is defined using addition operations (+). The basis functions for Wn are obtained by the Kronecker product of smaller W2 matrices. The inverse Wn is also the transpose of Wn.

2. **Arithmetic Transform (Y):** Similar to the Walsh-Hadamard transform, but with subtraction (-) instead of addition. The basis functions of Yn are identical to those of Wn. The self-inverse property of Yn can be verified by checking that Yn * Yn = I (identity matrix).

3. **Reed-Muller Transform (R):** Derived from the arithmetic transform by replacing addition and subtraction with XOR operations. The Reed-Muller transform is also self-inverse, and its basis functions are identical to those of Y+.

4. **Reverse Reed-Muller Transform (E):** Another name for the Reed-Muller transform when it's viewed as a self-inverse operation. Its basis functions are the same as R.

The text also introduces two convolutions: OR-convolution and AND-convolution, which operate on binary sequences using bitwise operations. The OR-convolution hτ of sequences a and b is defined as hτ = ∑(i∨j=τ ai * bj), where ∨ denotes bitwise OR. The AND-convolution follows a similar definition but uses bitwise AND instead.

Fast algorithms for computing these convolutions are provided, along with explanations of their implementations in code snippets. These transforms and convolutions find applications in various areas of computer science and engineering, including signal processing, coding theory, and cryptography.


The text discusses several types of convolutions related to sequences, focusing on the Fast Walsh-Hadamon Transform (FWHT), which is an efficient method for computing these convolutions. Here's a detailed summary:

1. **OR-convolution**: This operation involves element-wise bitwise OR between two sequences and then applying a Fast Walsh-Hadamard Transform (FWHT). The key relation used in its computation is h = Y - * (Y +[a] * Y +[b]), where Y + and Y - denote the arithmetic transforms. The FWHT implementation for OR-convolution is provided, with an overall time complexity of O(n log n).

2. **AND-convolution**: This operation involves element-wise bitwise AND between two sequences, followed by applying a reversed arithmetic transform. The key relation used in its computation is h = B - * (B +[a] * B +[b]), where B + and B - denote the reversed arithmetic transforms. Similar to OR-convolution, the FWHT implementation for AND-convolution is given, with an overall time complexity of O(n log n).

3. **MAX-convolution**: This operation involves finding the maximum value between pairs of indices in two sequences and then performing an element-wise multiplication before summing up these products. The naive implementation has a quadratic time complexity (O(n^2)). However, it's shown that this convolution can be computed linearly (O(n)) using cumulative sums: h[k] = f[k]*g[k] + sf*g[k] + sg*f[k], where sf and sg are cumulative sums of f and g respectively.

4. **Weighted Arithmetic Transform**: This is a generalization of the Walsh-Hadamard transform, where each element in the transformed sequence is a weighted sum of the original elements based on bitwise relationships. The basis functions for this transform are shown in Figure 23.14-A, with weights depending on the number of ones (binary expansion) in the indices.

5. **Subset Convolution**: This operation involves finding pairs of indices where the binary OR equals a given index and the binary AND equals zero. It's computationally intensive due to many missing products compared to other convolutions. A method with time complexity O(n (log n)^2) is presented, which computes a weighted version of this convolution first, then extracts the desired result via a series of operations.

In summary, the Fast Walsh-Hadamard Transform (FWHT) is a powerful tool for efficiently computing various types of sequence convolutions by exploiting properties of binary representations and symmetries in the operations involved. The methods presented offer improvements over naive implementations, reducing time complexities from quadratic to linear or logarithmic, depending on the specific operation.


The Haar Transform is a type of discrete wavelet transform that does not involve trigonometric functions. It's an invertible transformation used to decompose signals or data into different frequency components. The standard Haar Transform works on sequences (or arrays) whose length is a power of 2, decomposing the signal into approximation and detail coefficients at each level of the decomposition.

1. **Standard Haar Transform**:
   - The transform consists of log2(n) steps where n is the length of the sequence.
   - In each step, adjacent pairs of elements in the sequence are summed (for approximations) or differenced (for detail coefficients).
   - Sums go to the lower half of the array, and differences go to the upper half.
   - The process repeats for successively halved segments of the array until individual elements remain.

   Mathematically, if `f` is the input sequence:
   
   ```
   For i = 0 to log2(n)-1
     For j = 0 to n/2^i - 1
        A[j] = Sum from k=0 to 1 of f[2j+k] * W[i, k]
     End for
     For j = n/2^i to n-1
        D[j] = Diff from k=0 to 1 of f[2j+k] * W[i, k]
     End for
   Next i
   ```

   Here, `A` represents the approximation coefficients and `D` represents the detail coefficients. `W[i,k]` are the transformation matrices (for standard Haar, they're simple ±1).

2. **Orthogonality**:
   - The standard Haar transform can be made orthogonal by scaling the transformation matrices appropriately (usually by √2). This means that the inverse transform can be directly obtained by multiplying with the transpose of the forward transform matrix.

3. **In-place Variants**:
   - Standard Haar Transform requires additional memory for temporary storage of intermediate results. In-place variants aim to perform the transform without this extra space, typically at the cost of increased computational complexity.

4. **Applications**:
   - The Haar Transform is fundamental in wavelet theory and has applications in signal processing, image compression (like JPEG 2000), data analysis, and more due to its ability to capture both frequency and spatial information effectively. 

The provided code snippets implement the standard Haar transform in C++. The first version allocates extra memory for intermediate results, while the second version uses an optional workspace parameter supplied by the caller to reduce memory usage, making it an in-place variant. The inverse transform is similarly implemented but in reverse order.


The provided text discusses various versions of the Haar Transform, a mathematical operation used in signal processing and data compression. The Haar Transform decomposes a function or signal into a sum of a series of orthonormal basis functions, which are step-like functions in the case of the Haar Transform.

1. **Standard (Non-Inplace) Haar Transform**: This is the basic version where a temporary storage array (`g`) is used to store intermediate results during the transformation process. It's defined by the following operations:

   - `x = f[k]` (assigning the value at index k in array `f` to variable x)
   - `y = f[mh+k] * v` (assigning the product of the value at index mh+k in array `f` and scalar v to y)
   - `g[j] = x + y` (storing the sum of x and y into g at index j)
   - `g[j+1] = x - y` (storing the difference of x and y into g at index j+1)

2. **In-place Haar Transform**: This version performs the transformation without requiring additional storage, thus saving memory. It achieves this by cleverly rearranging (or permuting) the data in place before and after each level of the transform. The permutation is handled by two routines: `haar_permute()` and its inverse `inverse_haar_permute()`. The actual transform is performed by `haar_inplace()`, which is equivalent to the sequence `haar_inplace(); haar_permute();`.

3. **Non-normalized Haar Transform**: This version does not normalize the basis functions, meaning the nonzero entries have different absolute values compared to the normalized version. The forward transform (`haar_nn()`) and its inverse (`inverse_haar_nn()`) are straightforward implementations without normalization factors. An in-place version of this transform is also provided (`haar_inplace_nn()`), along with its inverse (`inverse_haar_inplace_nn()`).

4. **Transposed Haar Transform**: This version involves a different set of basis functions (shown in Figure 24.4-A). Despite the change in basis functions, an unnormalized transposed Haar transform can be performed using routines `transposed_haar_nn()` and its inverse `inverse_transposed_haar_nn()`.

Each of these versions has its use cases depending on the specific requirements of a task, such as memory constraints or computational efficiency. The text also mentions relationships between different transformations via permutations (`PH`), which can be programmed using routines like `haar_permute()` and `inverse_haar_permute()`.


The provided text discusses various aspects of the Haar Transform, focusing on its reversed and transposed versions, as well as their relations with Walsh Transforms. Here's a detailed summary:

1. **Reversed Haar Transform (H_rev)**:
   - Basis functions are depicted in Figure 24.5-A. They resemble those of the original Haar Transform but are reversed.
   - The routine `haar_rev_nn` computes this transform in-place without using a temporary array.
   - It's based on a radix-2 DIF (Difference-in-Few) implementation similar to Walsh transforms, with modifications to handle the reverse operation.

2. **Transposed Reversed Haar Transform (Ht_rev)**:
   - This is essentially the inverse of the reversed Haar Transform (`haar_rev_nn`). It's computed via `transposed_haar_rev_nn`.
   - The routine follows a similar radix-2 DIT (Difference-in-Twos) structure as Walsh transforms but with the necessary modifications for inversion.

3. **Relating Haar and Walsh Transforms**:
   - **Algorithm WH1 and WH1T**: These show how to compute a Walsh transform using one length-n Haar Transform (reversed), followed by multiple smaller Haar Transforms (reversed). The difference lies in whether these smaller transforms are applied before or after the larger ones.
   - **Algorithm WH2 and WH2T**: Similar, but here the Walsh Transform is computed via the inverse of the reversed Haar Transform.

4. **Relating Walsh and Haar Transforms (via algorithms HW1, HW1I, HW2, and HW2I)**:
   - These algorithms illustrate how to compute Haar Transforms using Walsh Transforms. They demonstrate that certain Walsh Transforms can be used as building blocks for Haar Transforms, although this method is not efficient for computing the Haar Transform directly due to its O(n log n) complexity compared to the O(n) complexity of direct Haar Transform computation.

5. **Prefix Transform and Prefix Convolution**:
   - The text concludes by introducing the concept of the prefix transform and prefix convolution, which are likely related to discrete signal processing operations (like convolution). However, specific details about these concepts aren't provided in the given excerpt.

The Haar and Walsh transforms are integral parts of signal processing, data compression, and image analysis, with their basis functions playing a crucial role in understanding and manipulating signals at different scales or frequency bands.


The text discusses various aspects of the Hartley transform, a trigonometric transform that maps real data to real data. Here's a detailed summary and explanation:

1. **Definition and Symmetries**: The discrete Hartley Transform (H[a]) of a length-n sequence 'a' is defined as ck = (1/√n) * Σ(ax * [cos(2πkx/n) + sin(2πkx/n)]), where k ranges from 0 to n-1. It's similar to the Discrete Fourier Transform but uses cosine and sine instead of cosine and imaginary sine. The Hartley transform of a real sequence remains real, and it is its own inverse (H[H[a]] = a). It also preserves symmetry properties: symmetric sequences yield symmetric transforms, and anti-symmetric sequences yield anti-symmetric transforms.

2. **Fast Hartley Transform (FHT)**: An efficient algorithm for computing the Hartley transform in O(n log n) time is called the Fast Hartley Transform (FHT).

3. **Radix-2 FHT Algorithms - Decimation in Time (DIT)**: The radix-2 DIT FHT involves recursively splitting the input sequence into even and odd indexed elements, applying the transform to each half, and then combining the results using a specific operator X 1/2. This operator is equivalent to S1/2 in Fourier Transform algorithms.

   - **Pseudocode for Recursive Radix-2 DIT FHT**: The provided pseudocode outlines the steps for computing the FHT recursively using DIT. It involves creating temporary arrays for even and odd indexed elements, applying the transform to each half, and then combining the results using the hartley_shift function.

   - **hartley_shift Function**: This function implements the X 1/2 operator, which transforms each element ck of the input sequence c by ck cos(πk/n) + cn-k sin(πk/n).

4. **Non-recursive Radix-2 DIT FHT**: An alternative approach to compute the FHT non-recursively is also outlined in the text, although specific pseudocode isn't provided. This method would involve a depth-first traversal of the binary tree representation of the sequence length, applying the transform at each node, and combining results similarly to the recursive case.

5. **Optimization of hartley_shift**: An optimized version of the hartley_shift function is presented, which leverages symmetry properties of trigonometric functions for improved efficiency. This version uses precomputed constants and parallel assignments to reduce computational overhead.

In summary, this text introduces the Hartley transform, its properties, and efficient algorithms (FHT) for its computation using radix-2 decimation in time (DIT). It also provides pseudocode for both recursive and non-recursive implementations of the DIT FHT and discusses an optimized version of a crucial function (hartley_shift) involved in these algorithms.


The provided text discusses several algorithms related to the Hartley Transform (HT), which is a discrete Fourier-like transform, but uses real coefficients. The text covers both the Decimation in Time (DIT) and Decimation in Frequency (DIF) methods for computing the FHT (Fast Hartley Transform).

1. **Radix-2 DIT FHT Algorithm**: This algorithm begins by reordering (permuting) the input data, then recursively divides the transform size into halves until a base case is reached (a size of 1), and finally combines the results using a sum/diff operation. The base case computes the Hartley transform for a single element directly.

2. **Radix-2 DIF FHT Algorithm**: This algorithm also starts with data reordering but uses a different approach to compute the transform. It performs a series of sum and difference operations followed by a Hartley shift, which is a rotation in the frequency domain. The process is recursive, reducing the size of the problem at each step until it reaches the base case.

3. **Complex FFT by FHT**: The relationship between Complex Fourier Transform (CFT) and Hartley Transform (HT) is established. It's shown that a complex CFT can be computed using two separate real HTs, or vice versa. This conversion involves rearranging and scaling the input data according to specific rules.

4. **Real FFT by FHT**: A method for computing Real Fast Fourier Transform (FFT) via Hartley Transform is detailed. The process involves transforming the real sequence into its Hartley representation, then applying a post-processing step to obtain the desired real FFT result. The inverse operation, transforming an FFT back to a real sequence using FHT, is also described.

5. **Higher Radix FHT Algorithms**: The text mentions that higher radix versions of FHT can be derived from existing FFT algorithms by wrapping the FFT steps with conversion operators (T). This method inherently takes advantage of trigonometric factor symmetry. However, detailed splitting methods for specific radices like 4 or split-radix are referenced to other works.

6. **Convolution via FHT**: The convolution property of Hartley Transform is used to create an algorithm for cyclic convolution of two real sequences. This method leverages the efficiency of FHT by performing multiplication and addition in a transformed domain, reducing computational complexity compared to direct time-domain convolution.

In summary, these algorithms provide efficient methods for computing various types of discrete Fourier transforms (DFT, FFT, CFT) using the Hartley Transform. They achieve this by leveraging symmetry properties and recurrence relations inherent to these transforms, leading to significant computational savings compared to direct implementations.


The given text discusses various aspects of the Fast Hartley Transform (FHT), a computational method related to the Discrete Fourier Transform (DFT). Here's a detailed summary and explanation of the key points:

1. **Cyclic Convolution using FHT**: The cyclic convolution of two real sequences can be computed using FHT, as shown in the pseudocode provided. This involves transforming both input sequences, performing element-wise multiplication in the transformed domain (convolution), and then transforming back to the original domain. Normalization is applied at the end.

2. **C++ Implementations**: The text presents C++ implementations for cyclic convolution (`fht_convolution`) and self-convolution (`fht_auto_convolution`). These implementations use auxiliary functions like `fht_mul` and `fht_sqr` to handle the element-wise multiplication in the transformed domain.

3. **Avoiding Revbin Permutations**: The text mentions that, similar to FFT-based convolutions, revbin permutations can be omitted with FHT-based convolutions for improved performance. This is achieved by accessing data in reverse binary order during computation and post-processing.

4. **Negacyclic Convolution via FHT**: A pseudocode is provided for the negacyclic auto-convolution using FHT. This involves preprocessing (Hartley shift), transforming data, performing convolution in the transformed domain, transforming back, and postprocessing (another Hartley shift). C++ implementations for this are given as well.

5. **Localized FHT Algorithms**: The text introduces localized versions of the FHT, which aim to improve cache performance by processing smaller chunks of data at a time. Two versions are presented: Decimation in Time (DIT) and Decimation in Frequency (DIF). These algorithms use recursion to handle larger lengths by breaking them down into smaller, manageable subproblems.

6. **2-dimensional FHTs**: The text explains how to compute 2-dimensional FHTs using a row-column algorithm similar to the 2D FFT. Post-processing is required to transform the row-column FHT result into a true 2D FHT.

7. **Automatic Generation of Transform Code**: The text discusses the concept of generating FFT and FHT code automatically. This involves creating a program that reads existing FFT/FHT code as input and generates optimized, localized versions tailored for specific hardware characteristics (like cache size). The process includes partial evaluation, where current loop variable values are printed as comments to help identify corresponding parts in the generated code.

8. **Eigenvectors of Fourier and Hartley Transforms**: The text briefly mentions eigenvectors of both Fourier and Hartley transforms. It provides a relationship between a sequence's symmetric part (aS) and its Fourier transform, which could be useful in understanding the properties of these transforms.

In summary, the text covers various aspects of the Fast Hartley Transform, including efficient convolution algorithms, C++ implementations, optimization techniques like avoiding revbin permutations, 2D FHT computation, and automated code generation for improved performance tailored to specific hardware characteristics. It also briefly touches on eigenvectors of Fourier and Hartley transforms.


The text discusses Number Theoretic Transforms (NTTs), a variant of the Fast Fourier Transform (FFT) that operates over finite fields, specifically modular arithmetic. NTTs are used for exact computations without rounding errors, making them suitable for high-precision applications like multiplication algorithms.

**Key Points:**

1. **Prime Moduli**: The choice of a prime modulus m is crucial for implementing NTTs. A primitive n-th root of unity (r) exists in Z/mZ if and only if n divides the maximal order R = p - 1, where p is the prime. This means that suitable primes are of the form p = v*n + 1.

2. **Conditions for NTTs**: Two conditions must be met:
   - The transform length n must divide the maximal order R (condition 26.1-1).
   - The modulus m must be coprime to the transform length n (condition 26.1-2). This is automatically satisfied if m is prime, as all primes are coprime to any integer less than themselves.

3. **Implementation**: NTTs can be implemented using similar algorithms as FFTs but with modular arithmetic replacing complex arithmetic. The text provides pseudocode and C++ implementations for Radix-2 Decimation in Time (DIT) and Decimation in Frequency (DIF) NTTs, as well as Radix-4 versions of these transforms.

4. **Convolution with NTTs**: One significant application of NTTs is the computation of exact integer convolutions, which are essential in high-precision multiplication algorithms. When dealing with large numbers that don't fit into a single machine word, it's recommended to use multiple smaller primes and apply the Chinese Remainder Theorem to combine results.

**Pseudocode Examples**:

- Radix-2 DIT NTT:
  - Pseudocode involves finding a primitive n-th root of unity (rn), performing a bit reversal permutation, then iterating over decreasing powers of 2 (ldm) to perform butterfly operations.

- Radix-2 DIF NTT:
  - Similar to DIT but in reverse order, starting from the highest power of 2 and moving downwards. The final step involves a bit reversal permutation.

- Radix-4 NTTs:
  - These are extensions of the Radix-2 transforms, involving additional steps for handling four elements at a time. Both DIT and DIF versions are provided.

The text also includes examples of suitable prime numbers for various transform lengths, generated using a program called `FXT: mod/fftprimes-demo.cc`. These primes are selected to allow transforms of lengths that divide specific numbers (e.g., 2^44, 2^40, etc.), ensuring the existence of primitive roots required for NTTs.


The Karatsuba algorithm is an efficient method for multiplying large numbers, reducing the number of operations from O(N^2) (as in traditional multiplication) to O(N^log_2(3)) ≈ O(N^1.585). This speedup is achieved by splitting each input number into two parts and recursively applying the Karatsuba formula.

Here's a step-by-step explanation of the 2-way splitting (Karatsuba) algorithm:

1. Divide both input numbers A and B into two parts, such that their lengths are approximately equal:
   - A = x * a_high + a_low
   - B = x * b_high + b_low

   Here, 'x' is chosen to be the square root of the base (e.g., 10 for decimal numbers) rounded to the nearest integer.

2. Compute three intermediate products:
   - P1 = a_low * b_low
   - P2 = x * (a_low * b_high + a_high * b_low)
   - P3 = x^2 * a_high * b_high

3. Combine the intermediate results to obtain the final product using the following formula:
   A * B = P1 + x * P2 + P3

This 2-way splitting scheme reduces the number of multiplications required from four (in traditional multiplication) to three, thus improving efficiency for large numbers. The algorithm can be further optimized by choosing an optimal value for 'x' that minimizes the number of operations.

The Karatsuba algorithm has a time complexity of O(N^log_2(3)) ≈ O(N^1.585), making it more efficient than traditional multiplication for sufficiently large input sizes. It forms the basis for more advanced multiplication algorithms like Toom-Cook and FFT-based methods.


The text discusses fast multiplication algorithms, focusing on splitting schemes and Fast Fourier Transform (FFT) based methods.

**Splitting Schemes for Multiplication:**

1. **Two-way Splitting (Karatsuba Algorithm):** This method reduces the number of multiplications needed to compute the product of two numbers by recursively splitting them into half-precision parts. The Karatsuba algorithm requires three multiplications of half precision for one full precision multiplication, yielding a time complexity of O(N^1.585).

   Formula: A * B = (1 + x) * a0 * b0 + x * (a1 - a0) * (b0 - b1) + (x + x^2) * a1 * b1
   Here, 'x' is a power of the radix.

2. **Three-way Splitting:** This scheme splits each number into three parts rather than two, further reducing the number of multiplications needed. Two notable three-way splitting methods are:

   - **Zimmermann's 3-way multiplication:** This scheme requires five multiplications of length N/3 and has a time complexity of approximately O(N^1.465).
   - **Bodrato and Zanoni's 3-way multiplication:** This method also needs five multiplications of length N/3, but it only involves one division by 2 instead of two divisions by 3 found in Zimmermann’s scheme. Its time complexity is similar to Zimmermann's, approximately O(N^1.465).

3. **Four-way and Five-way Splitting:** These methods extend the splitting concept to four or five parts, respectively, providing even more efficient multiplication algorithms with higher complexity (O(N log_4(7) ≈ N^1.403) for 4-way and O(N log_5(7) ≈ N^1.3928) for 5-way).

**Fast Multiplication via FFT:**

The Fast Fourier Transform (FFT)-based algorithm is another efficient method to multiply two numbers represented in a given radix 'R'. This approach treats the multiplication of integers as a polynomial multiplication, followed by linear convolution and carry operations.

1. **Numbers as Almost Polynomials:** An N-digit integer A can be viewed as a polynomial of degree (N - 1) with coefficients representing its digits in radix R. The product of two such numbers is nearly their polynomial product.

2. **Polynomial Multiplication as Linear Convolution:** The coefficients ck of the resulting polynomial can be computed via linear convolution, which involves multiplying sequences A and B at various points (roots of unity). This process transforms both sequences using FFTs, multiplies them element-wise, and then inverse transforms to obtain the polynomial C.

3. **Complexity and Radix/Precision Considerations:** The FFT-based algorithm has a time complexity of O(N log N) due to the two FFT operations (for forward and inverse transformations). However, it requires at least 2N - 2 points for evaluation (zero-padding), which limits the radix R based on the desired precision. For large precisions, the cumulative sums (ck) must be representable as integer numbers with the data type used for the FFTs, imposing restrictions on the maximum value L that can appear in the product.

In summary, both splitting schemes and FFT-based multiplication methods significantly reduce the computational complexity of multiplying two large integers by dividing them into smaller parts or treating them as polynomials. These techniques are essential in developing fast algorithms for high-precision arithmetic, cryptography, and numerical computations involving large numbers.


Title: Division, Square Root, and Cube Root - Root Extraction Methods

1. **Inverse and Division**: The standard division operation is computationally expensive for high-precision numbers. Instead, the inverse of a divisor (d) is computed using Newton's method, also known as the Newton-Raphson iteration:

   x_(k+1) = x_k + x_k * (1 - d * x_k)

   This iterative process starts with an initial approximation (x0 ≈ 1/d), and each subsequent step doubles the number of accurate digits. The multiplication operation in this method requires only half the precision of the current value, making it efficient.

2. **Convergence and Precision**: The convergence rate of this method is quadratic, meaning that with each iteration, the number of correct digits approximately doubles. This results in a total computational cost less than three full-precision multiplications, including the final multiplication to obtain the quotient (a/d).

3. **Third-Order Correction**: For higher precision requirements, a third-order correction can be applied:

   x_(k+1) = x_k + x_k * (1 - d * x_k) + x_k * (1 - d * x_k)^2

   This ensures maximum precision in the final result.

4. **Long and Short Divisions**: If both operands have equal precision, it is called a long division; if one operand fits within a machine word, it's known as short division. Similarly, multiplication with full-precision numbers is referred to as long multiplication, while short multiplication occurs when one operand fits into a machine word.

5. **Example**: The given figure (29.1-A) illustrates the first few iterations of computing the inverse of 3.1415926 starting from an initial two-digit approximation (0.31). Each step improves the precision until the desired level is achieved.


Title: Initial Approximations for Iterations in High-Precision Arithmetic

In high-precision arithmetic, direct conversion to machine floating-point numbers and subsequent use of the Floating-Point Unit (FPU) may not always be feasible due to limitations in representable values. This section discusses methods for generating initial approximations suitable for iterative computations without relying on FPU operations.

1. **Taylor Series Expansion:**

   The first method involves expanding the target function around a nearby point where a high-precision approximation is known or can be computed easily. For instance, consider computing √d with d being a large number:

   - Step 1: Obtain an initial guess x0 for √d by using simpler approximations like Newton's method or other suitable techniques.
   - Step 2: Use the Taylor series expansion of f(x) = (√x - x0)^2 around x0:
     $$f(x) \approx \frac{1}{4x_0} (x - x_0)^2$$
   - Step 3: Solve for √d:
     $$\sqrt{d} \approx x_0 + \frac{(d - x_0^2)}{2x_0}$$

   This method is simple and can be extended to other functions by adjusting the Taylor series expansion accordingly.

2. **Pade Approximants:**

   Padé approximants are rational functions that provide increasingly accurate approximations of a given function around a specific point. They have the advantage of being more efficient than traditional Taylor series expansions for certain functions, especially when dealing with large arguments or high-precision arithmetic.

   - Step 1: Identify the Padé approximant [m,n] for f(x) around x0, where m and n are integers specifying the orders of the numerator and denominator polynomials, respectively.
   - Step 2: Evaluate the approximant at d to obtain an initial approximation for √d:
     $$\sqrt{d} \approx \frac{\text{Numerator}[m,n](d)}{\text{Denominator}[m,n](d)}$$

   The choice of [m, n] depends on the desired balance between accuracy and computational complexity. Higher-order Padé approximants generally yield better precision but at the cost of increased complexity.

3. **Continued Fractions:**

   Continued fractions offer another approach for generating initial approximations in high-precision arithmetic. They provide increasingly accurate rational approximations to irrational numbers, making them suitable for computing square roots and other functions.

   - Step 1: Generate the continued fraction representation of √d using well-known algorithms like the Gauss-Legendre algorithm or the modified Lagrange inversion formula.
   - Step 2: Truncate the continued fraction at a suitable depth to obtain an initial approximation for √d as a rational number.

   This method is particularly useful when dealing with transcendental functions and irrational numbers, providing rapid convergence even for large arguments.

In summary, these methods for generating initial approximations in high-precision arithmetic aim to bypass limitations associated with machine floating-point representations. By leveraging Taylor series expansions, Padé approximants, or continued fractions, one can efficiently obtain accurate starting values for iterative computations without relying on FPU operations. The choice of method depends on the target function, desired accuracy, and available computational resources.


The text discusses various mathematical techniques for root extraction, focusing on inverse roots, the exponential function, and applications of matrix square roots. Here's a detailed summary and explanation of each section:

1. Inverse Roots:
   The technique presented involves expressing a number d in the form d = M · RX, where M is the mantissa (0 ≤ M < 1), R is the radix (base), and X is the exponent (X ∈ Z). The inverse root d^(-1/a) can then be computed as M^(1/a) * R^(Y/a) * R^(Z), where Y = X % a, Z = ⌊X/a⌋, and X = a*Z + Y.

   The provided C++ code (approx_invpow) implements this technique:
   - Line 4-5: Convert the hfloat number d to double dd for easier computation.
   - Line 6: Compute M^(1/a) using pow(dd, 1.0/(double)a).
   - Lines 7-9: Calculate Z and Y based on the exponent of d.
   - Line 10: Compute R^(Y/a) using pow((double)d.radix(), (double)Y/a).
   - Line 11: Multiply M^(1/a) * R^(Y/a) to get dd.
   - Lines 12-13: Convert the result back to hfloat c and adjust its exponent.

2. Exponential Function:
   For the exponential function f(d) = exp(d), the input d is expressed as M · RX, where X = ⌊d/log(R)⌋ and M = exp(d - X * log(R)). The initial approximation for exp(d) is computed directly using these values.

   The provided C++ code (approx_exp) implements this technique:
   - Line 4-5: Convert the hfloat number d to double dd for easier computation.
   - Lines 6-8: Calculate X and M based on the input d and the radix R.
   - Line 9: Assign M as the initial approximation of exp(d).
   - Line 10: Adjust the exponent of c to match the computed value of exp(d).

3. Applications of Matrix Square Root:

   a. Re-orthogonalization:
      This method is used to transform a rotation matrix A, which has deviated from being orthogonal (due to cumulative errors), to the closest orthogonal matrix E. The transformation uses the inverse square root iteration with d = AT * A and x = 1/(2*(1 + AT * A)).

   b. Polar Decomposition:
      The polar decomposition of a matrix A is represented as A = ER, where E is an orthogonal matrix and R is a positive semi-definite matrix. This decomposition can be computed using the inverse square root iteration for both E and R matrices.

   c. Sign Decomposition:
      The sign decomposition represents a matrix A as S * N, where S is its own inverse (eigenvalues are ±1) and N has positive eigenvalues. This decomposition can be obtained through an iterative process similar to the polar decomposition.

   d. Pseudo-Inverse:
      The pseudo-inverse of a matrix A, denoted by A+, is defined as (A^T * A)^(-1) * A^T. It exists even when A^(-1) does not and provides the best possible solution (in a least-squares sense) for Ax = b. The pseudo-inverse can be computed using an iteration similar to that of matrix inversion.

These mathematical techniques are valuable in various fields, such as numerical analysis, linear algebra, and graphics applications, for tasks like orthogonalization, decomposition, and solving systems of linear equations.


Schröder's formula is an nth-order iteration for a simple root r of a function f(x), given by the expression:

Sn(x) = x - ∑[n-1]_k=1 (−1)^k * f(x)^k / k! * (1/f'(x))^(k-1)

This formula provides an efficient way to compute higher-order iterations for finding the roots of a function. It was introduced by Ernst Schröder in 1870 and is derived using power series reversion techniques or systematically through a recursive relation.

Schröder's formula can be used to construct various higher-order iterations, such as Newton's method (n=2) and Halley's method (n=3). The recursion relation for the coefficients Un in Schröder's formula is:

Un = (2n - 3) * f'(x) * U_(n-1) - f(x) * U'_n-1,

where U_0 = 1 and U'_n denotes the derivative of U_n with respect to x.

The formula also has applications in other areas, such as finding divisionless iterations for polynomial roots (29.7.3). Schröder's iteration can be seen as a generalization of Newton's method by including higher-order derivatives of f(x) and its inverse.

In summary, Schröder's formula provides an elegant way to create higher-order iterations for finding the roots of a function. It is derived from power series reversion techniques and offers a flexible framework for generating various high-order methods like Newton's method, Halley's method, and others. This formula is essential in numerical analysis and can be applied in various areas, including polynomial root finding and other optimization problems.


The Complete Elliptic Integral of the First Kind, denoted as K(k), is a fundamental function in mathematics, particularly in the study of elliptic curves and special functions. It's defined for the parameter k (0 < k ≤ 1) as follows:

K(k) = ∫_0^π/2 [1 / √(1 - k²sin²θ)] dθ

This integral represents the arc length of a portion of an ellipse. 

The AGM-based algorithm for computing K(k) is as follows:

1. Initialize two variables, `a0 = 1` and `b0 = sqrt((1 - k) / (1 + k))`.
2. Perform the AGM iteration until convergence:

   For n from 0 to a pre-determined number of iterations:
      - Compute `an+1 = (an + bn) / 2`
      - Compute `bn+1 = sqrt(an * bn)`
3. Once the iteration converges, compute `K(k)` as:

   K(k) = π / (2 AGM(1, √((1 - k) / (1 + k))))

Where `AGM` refers to the Arithmetic-Geometric Mean function discussed in Section 31.1. This method has super-linear convergence and is highly efficient for calculating K(k) with high precision. 

4. The value of K(k) can be extended to k > 1 using the symmetry relation:

   K(1/k) = K(k)

5. The derivative of K(k) with respect to k can be expressed in terms of complete elliptic integrals of the second and third kind, denoted as E(k) and Π(n; k), respectively:

   dK(k)/dk = (E(k) - K(k)) / (2k)

6. At k = 1, K(1) is a logarithmic singularity, meaning it approaches infinity as k approaches 1 from below:

   lim_(k→1^-) K(k) = ∞ 

7. The complementary form of the complete elliptic integral of the first kind is given by:

   K'(k) = (π / 2) * Σ[(−1)^n * (2n − 1)!] / [n! * (n + 1)! * (1 - k^(2n))]

This series converges for all k in the interval (0, 1]. 

The complete elliptic integral of the first kind, K(k), appears in many areas of mathematics and physics, including number theory, differential equations, and quantum mechanics. Its efficient computation is crucial for various high-precision numerical applications.


The given text discusses various mathematical concepts related to elliptic integrals, the Arithmetic-Geometric Mean (AGM), theta functions, eta functions, and singular values. Here's a detailed summary:

1. **Elliptic Integral K(k)**:
   - Definition: The complete elliptic integral of the first kind, denoted by K(k), is defined as `K(k) = ∫_0^(π/2) dϑ / sqrt(1-k^2 sin^2 ϑ)`.
   - Special Values: `K(0) = π/2` and `lim_{k→1-} K(k) = +∞`.
   - Formula: It can be expressed as `K(k) = (π/2) F((1/2), (1/2); 1; k^2)`, where F is the Gaussian hypergeometric function. Alternative series expansions are also provided.

2. **AGM-based Computation**:
   - The connection to AGM: `K(k) = (π/2) * AGM(1, √(1-k^2))`, where AGM is the Arithmetic-Geometric Mean function.
   - Approximation for k close to 1: `K(k) ≈ log(4 / √(1-k^2))`.

3. **Product Forms**: Two product forms are provided for efficient computation of K'(k) and E'(k), which are related to K(k) and E(k) respectively. These product forms involve iterative calculations with square roots and inverse square roots.

4. **Higher-order Products**: The text also discusses higher-order products for K and E, which can be used for more accurate computations but require more complex iterations.

5. **Elliptic Integral of the Second Kind (E(k))**:
   - Definition: `E(k) = ∫_0^(π/2) sqrt(1-k^2 sin^2 ϑ) dϑ`.
   - Special Values: `E(0) = π/2` and `E(1) = 1`.
   - Relation to K(k): `E(k) = (π/2) * F(-1/2, 1/2; 1; k^2)`, where F is the Gaussian hypergeometric function.

6. **Theta Functions**: Theta functions Θ2, Θ3, and Θ4 are defined as infinite series involving powers of a variable 'q'. They satisfy certain identities and relations that allow them to be connected to elliptic integrals via the AGM.

7. **Eta Function**: Although not explicitly defined in the given text, eta functions (denoted by η(z)) are related to theta functions and play a significant role in number theory, particularly in the study of modular forms.

8. **Singular Values**: These are not explicitly discussed in the provided text but are likely related to singular values in linear algebra or singular value decomposition (SVD), which can be connected to elliptic integrals through certain transformations.

These concepts are crucial in various areas of mathematics, including number theory, complex analysis, and computational methods for approximating mathematical constants like π. The AGM-based computations provide efficient ways to calculate these elliptic integrals, while the product forms and higher-order products offer alternative methods with varying levels of complexity and accuracy.


The text presents algorithms for calculating specific hypergeometric functions using the Arithmetic-Geometric Mean (AGM) iteration method. Here's a detailed explanation of the given algorithms:

1. **Hypergeometric Function F(1/2, 1/2; z)**

This algorithm calculates the hypergeometric function F(1/2, 1/2; z), where |z| < 1/2. The AGM iteration is used to compute this function efficiently with quadratic convergence:

    a. Initialize `a_0 = √z` and `b_0 = 1 - z`.
    
    b. Perform the following iterations until convergence (i.e., until `a_n - b_n` is sufficiently small):
        
        i. Compute the arithmetic mean `c_n = (a_n + b_n) / 2`.
        ii. Compute the geometric mean `d_n = √(a_n * b_n)`.
        iii. Update `a_{n+1} = c_n - d_n` and `b_{n+1} = c_n + d_n`.

    c. The desired hypergeometric function F(1/2, 1/2; z) is then given by:
    
        F(1/2, 1/2; z) = (2 / π) * arctan(√z) - ∑_{k=0}^{N-1} (1 / (2k + 1)) * ((a_N - b_N)^(2k+1))

2. **Transformations for Hypergeometric Functions**

The text also provides two transformations that can be applied to the hypergeometric functions F(1/2 ± s, 1/2 ∓ s; z) and F(1/4 ± t, 1/4 ∓ t; z):

    a. For F(1/2 ± s, 1/2 ∓ s; z), where |z| < 1/2:
    
        F(1/2 + s, 1/2 - s; z) = (4 / π) * arctan(√(1 - z) / √z) * F(1/4 + s/2, 1/4 - s/2; 4z(1 - z))
    
    b. For F(1/4 ± t, 1/4 ∓ t; z), where |z| < 1:
    
        F(1/4 + t, 1/4 - t; z) = (1 / π) * arctan(√(1 - z^2)) * F(1/2 + 2t, 1/2 - 2t; (1 - √(1 - z))/2)

These transformations allow the calculation of different hypergeometric functions by relating them to previously computed values.


The text provides several algorithms for computing the value of specific hypergeometric functions, which are closely related to the computation of the mathematical constant π. These algorithms use a method known as Arithmetic-Geometric Mean (AGM), a numerical method that converges superlinearly (faster than linear convergence). 

1. **Basic AGM Algorithm for Computing π**: The first algorithm is based on Gauss's original formulation of the AGM, and it involves initializing two variables `a0` and `b0`, then iteratively updating them according to the rules:

    - `ak+1 = (ak + bk) / 2`
    - `bk+1 = sqrt(ak * bk)`

    The computation of π is derived from these iterations by using a formula involving the arithmetic-geometric mean and an error term. This algorithm has second-order convergence (#FPM=98.4 for computing π to 4 million digits).

2. **Schönhage's Variant**: Another AGM variant, due to Schönhage, is also presented. It uses a different initial condition (`a0 = 1`, `b0 = sqrt(6 + sqrt(2)) / 4`) and has fewer full precision multiplications (#FPM=78.424 for the same π computation).

3. **AGM Variants by Borwein**: Two additional AGM-based iterations are introduced, both with fourth-order convergence (faster than the second-order methods above). They use different initial conditions and transformations:

    - Variant 1 uses `a0 = 1`, `b0 = sqrt(6 + sqrt(2)) / 4` (#FPM=99.5 for the bilinear variant, #FPM=155.3 for the quartic).
    - Variant 2 uses `a0 = 1`, `b0 = sqrt(6 - sqrt(2)) / 4` (#FPM=108.2 for the bilinear variant, #FPM=169.5 for the quartic).

4. **Second-Order Iteration**: A simpler iteration with quadratic convergence is presented:

    - `y0 = sqrt(2) - 1`,
    - `a0 = 1 / 2`,
    - `yk+1 = (1 - yk^2)^(-1/2) - 1`,
    - `ak+1 = ak * (1 + yk+1)^2 - 2^(k+1) * yk+1`.

    This iteration (#FPM=255.7 for the same π computation) demonstrates a technique to save operations by reusing intermediate results.

5. **Borwein's Quartic Iterations**: Two quartic (fourth-order convergence) iterations are described:

    - Variant r = 4 uses `y0 = sqrt(2) - 1`, `a0 = 6 - 4*sqrt(2)`,
    - Variant r = 16 uses a specific initial condition and provides approximately double the precision for each step.

These algorithms serve as efficient ways to compute π, with varying numbers of full precision multiplications (#FPM), depending on their order of convergence and the specific implementation chosen. They are all based on the AGM method, which has been known since Gauss's time and has been rediscovered and refined by various mathematicians over the years.


The text discusses methods for computing the natural logarithm using the Arithmetic-Geometric Mean (AGM) method, as well as through inverting the exponential function. 

1. **AGM-based computation**: The AGM method is used to compute the logarithm by relating it to the AGM iteration. The formula given is:

   |log(d) - R'(10^-n) + R'(10^-n * d)| <= n / 10^(2*(n-1)) (32.1-1a)
   log(d) ≈ R'(10^-n) - R'(10^-n * d) (32.1-1b)

   Here, 'R' likely refers to some function derived from the AGM iteration, and n is a parameter determining the precision. The term R'(10^-n) can be computed once and reused for subsequent logarithm calculations. 

   The argument of the logarithm (d) is first normalized so that it falls within the interval [1/2, 1]. If it's outside this range, an argument reduction is performed using log(M sf - f * log(s)), where M is the mantissa, s = sqrt(2), and f is an integer chosen such that M*s^f is in the desired interval. 

   The logarithm of the radix (r) can be computed using Θ3(q) and Θ2(q), which are functions related to the AGM iteration. Once π and log(r) are known, log(d) can be computed using the above relations.

2. **Computation by inverting the exponential function**: This method involves using the inverse relationship between the logarithm and the exponential function. Given an efficient algorithm for the exponential function (exp), we can compute the logarithm as follows:

   y := 1 - d * exp(-x)
   log(d) = x + log(1 - y)

   The last term, log(1 - y), is expanded as a power series and truncated after the n-th power of y to yield an iteration of order n:

   x_(k+1) = Φ_n(x_k) := x_k - (y + y^2/2 + y^3/3 + ... + y^(n-1)/n-1)

   Alternatively, Padé approximants can be used to construct higher order iterations. These are rational function approximations of log(1 - z), which can save computational effort compared to the power series method by requiring only one division instead of multiple exponentiations for high-order terms. The Padé approximants P[i,j](z) produce iterations of order i + j + 1.

   In summary, both methods presented leverage properties of logarithmic and exponential functions to compute the natural logarithm. The AGM-based method uses relationships derived from the AGM iteration, while the inversion method relies on the inverse relationship between logarithms and exponents, with options for either power series or Padé approximants to enhance precision.


The text discusses several methods for computing special functions, specifically focusing on logarithms, exponentials, and their related series. Here's a summary of the key points:

1. **Padé Approximants**: These are rational function approximations to analytic functions, often used when dealing with functions that are difficult to compute directly. For example, Padé approximants for log(1+z) and arctan(z) are provided, along with methods for their computation using recurrence relations.

2. **Argument Reduction**: This technique is employed to improve the efficiency of computing special functions when the argument (input value) is large. For logarithms, this involves using the functional equation log(za) = a*log(z) and reducing z to a smaller value close to 1 by setting a = 1/N, where N is a large integer. Similar techniques are applied for arctan.

3. **Power Series**: The exponential function, inverse trigonometric functions (arcsin and arccos), and logarithms can be computed using power series. These methods involve iterating over the series terms until a desired precision is reached. High-order iterations are preferred due to the expense of computing logarithms.

4. **AGM-based Computation**: The Arithmetic-Geometric Mean (AGM) method can be used for high-precision computation of the exponential function. This involves solving for k and k' such that x = π K'/K, then using a relation involving 2n-th roots to compute exp(-x).

5. **Curious Series**: The text also presents some intriguing series representations for logarithms, including relations involving Fibonacci numbers (F_k) and Pell numbers (P_k).

6. **Simultaneous Computation of Logarithms**: A method is described to compute the logarithms of a given set of small primes simultaneously using a function L(z) = 2*arccoth(z). This allows for faster computation by expressing each prime's logarithm as a linear combination of terms L(Xi), where Xi are large integers, and the series converges quickly.

In summary, this text provides various methods to compute special functions (logarithms, exponentials, arctan) efficiently using techniques like Padé approximants, argument reduction, power series, AGM-based computation, and intriguing series representations. These methods cater to different scenarios based on the required precision and computational resources available.


The text describes a shift-and-add algorithm for computing the base-b logarithm (log_b(x)) with limited resources, which only requires shifts, additions, comparisons, and table lookups. Here's a detailed explanation of the algorithm:

1. **Table Initialization**: First, create a lookup table (`shiftadd_ltab`) containing values `Ak = log_b(1 + 1/2^k)`. This table is generated by the function `make_shiftadd_ltab(double b)`, which calculates these values and stores them for future use.

2. **Algorithm Input**: The algorithm takes two inputs:
   - Argument `x` (≥ 1): For which we want to compute log_b(x).
   - Number of iterations `n`: Determines the precision of the result.

3. **Initialization**: Set initial values for variables:
   - `t0 = 0`: Accumulator for the final result.
   - `e0 = 1`: A value used in the computation, initially set to 1.
   - `k = 1`: Iteration counter, starting from 1.

4. **Main Loop**: The algorithm then enters a loop that iterates `n` times (or until the desired precision is reached).

   In each iteration (`k`):
   - Compute `uk = ek * (1 + 2^(-k))`. This calculates an intermediate value using the current exponent `ek` and the iteration index `k`.
   - Compare `uk` with `x`:
     - If `uk ≤ x`, set `dk = 1`; otherwise, set `dk = 0`. This determines whether to add the corresponding table value.
   - If `dk ≠ 0`, update:
     - `tk+1 = tk + Ak`: Add the lookup table value to the accumulator.
     - `ek+1 = uk`: Update the exponent for the next iteration.
   - If `dk = 0`, keep the current values: `tk+1 = tk` and `ek+1 = ek`.
   - Increment `k`.

5. **Termination**: The loop exits when `k = n` (or the desired precision is reached), and the final result is stored in `tk`.

This shift-and-add algorithm for logarithm computation offers several advantages, such as:
- It only requires basic arithmetic operations (shifts, additions, comparisons) and table lookups.
- It can be adapted to use integer arithmetic by scaling the values appropriately.
- The algorithm is particularly useful in hardware implementations with limited resources or when precise logarithms are needed for specific applications like binary search algorithms.


The provided text discusses two types of algorithms for computing mathematical functions, specifically logarithms (log_b(x)) and exponentials (b^x), using limited computational resources. The methods are shift-and-add and CORDIC (Coordinate Rotation Digital Computer).

1. Shift-and-Add Algorithms:

   a. Logarithm Computation (log_b(x)):
   - This algorithm computes the logarithm of x to base b, where b > 0 and b ≠ 1. It uses a precomputed lookup table, shiftadd_ltab[], containing values Ak = log_b((2^k)/(2^k-1)).
   - The main loop iterates k times (n <= ltab_n), updating variables t (accumulated result), e (multiplier), and v (power of 1/2). It checks if the current estimate u is less than or equal to x, and if so, adds the corresponding table value Ak to the accumulator.
   - The process continues until the desired precision is reached or a maximum number of iterations is exceeded.

   b. Exponential Computation (b^x):
   - This algorithm computes b^x for b > 1 and x ∈ R. It also uses the same lookup table, shiftadd_ltab[], but with values Ak = log_b((2^k)/(2^(k-1))).
   - The main loop iterates k times (n <= ltab_n), updating variables t (accumulated result), e (multiplier), and v (power of 1/2). It checks if the current estimate u is less than or equal to x, and if so, adds the corresponding table value Ak to the accumulator.
   - The process continues until the desired precision is reached or a maximum number of iterations is exceeded.

2. CORDIC Algorithms:

   CORDIC algorithms are used for computing functions like sine, cosine, exp, and log using only multiplication by powers of 2 (shifts), additions, subtractions, and comparisons. They require a precomputed lookup table with as many entries as the desired accuracy in bits.

   The circular case (sine and cosine computation) is discussed:
   - Initialize x0 = K, y0 = 0, z0 = θ, where K is a scaling constant.
   - Iterate using the formulas for xk+1, yk+1, and zk+1, which involve the precomputed arctan values from cordic_ctab[].
   - The process converges if -r ≤ z0 ≤ r, where r is the sum of all arctan(2^(-k)).

   Both shift-and-add and CORDIC algorithms are designed to minimize the use of multiplications, relying instead on shifts and additions, making them suitable for environments with limited computational resources.


The text discusses the Binary Splitting (binsplit) algorithm, a method for efficiently computing power series, particularly useful when the coefficients are rational numbers. Here's a detailed explanation of the key points:

1. **Binary Splitting Algorithm**: This is an efficient way to compute sums and products by recursively splitting them into smaller parts. It's based on the idea of dividing the sum or product into two parts, evaluating each part recursively, and then combining the results.

2. **Factorial Computation**: The algorithm is first illustrated using factorials (n!). For example, to compute 8!, it breaks down as follows: F(1,8) -> F(1,4) -> F(1,2) -> F(1,1), then combining the results. This depth-first approach localizes memory access and avoids cache problems associated with pairwise processing.

3. **Polynomial Computation**: The same principle can be applied to compute polynomials from their roots. Each term of the polynomial is a product of factors (x - root_i), and the algorithm recursively computes these products.

4. **Summation Scheme for Power Series**: For power series PN-1 k=0 ak, the algorithm uses ratios Rk = ak/ak-1 to recursively compute the sum. It defines Rm,n as the sum from m to n, and expresses it in terms of smaller sub-sums.

5. **Rational Implementation**: For rational coefficients, the algorithm can be implemented using rationals directly. The function R(m, n) computes the sum from m to n recursively, using the relation Rm,n = Rm + Rm · Rm+1 + ... + Rm · ... · Rx + Rm · ... · Rx · [Rx+1 + ... + Rx+1 · ... · Rn].

6. **Integer Implementation**: In languages without rational support, the algorithm can be adapted to use integers. It involves separate computations for numerators and denominators, with a reduction step to simplify fractions if desired.

7. **Performance**: The binary splitting algorithm significantly outperforms naive methods for power series, especially those with good convergence properties (like arctan(1/10)). Its complexity is O(log N · M(N)), where M(N) is the complexity of a multiplication of two N-bit numbers.

8. **Extending Prior Computations**: The algorithm can be extended to compute sums to higher precision by reusing previously computed ratios, making it efficient for progressive refinement.

9. **Radix Conversion**: A variation of the binary splitting algorithm is used for fast radix conversion (changing between number systems). This involves recursively applying a relation that splits a sum into two parts based on powers of 2.

10. **AGM vs Binary Splitting for π Computation**: While both methods can compute π, binary splitting generally outperforms AGM-based iterations due to better memory access patterns. However, it may require more memory if the coefficients grow rapidly, which can be mitigated by converting to floating-point numbers at appropriate stages.


Recurrences are mathematical relationships that define a sequence based on its previous terms. Specifically, a k-th order recurrence relation is given by:

an = Σ(j=1 to k) mj * an-j

where 'mj' are constants, and 'n' represents the term number in the sequence. The sequence is defined not just by this recurrence relation but also by its initial k terms (a0, a1, ..., ak-1). 

The Fibonacci numbers and Lucas numbers are examples of sequences defined by linear, homogeneous recurrences with constant coefficients. 

For instance, the Fibonacci sequence (Fn) is defined by:

Fn = Fn-1 + Fn-2, for n > 1, with initial conditions F0 = 0 and F1 = 1.

Similarly, the Lucas numbers (Ln) are defined as:

Ln = Ln-1 + Ln-2, for n > 1, but with different initial conditions L0 = 2 and L1 = 1.

The term 'linear' in this context means that each term an is a linear combination of the previous k terms. The term 'homogeneous' signifies that there's no term independent of the previous ones (i.e., no constant term). Lastly, 'with constant coefficients' implies that the multipliers mj are not functions of n but constants.

Solving recurrence relations can be complex, especially for higher-order recurrences or non-constant coefficient cases. However, they're fundamental in many areas of mathematics and computer science, including algorithm analysis, dynamic programming, numerical methods, and more. 

In the context of the provided code snippet (Ri(m, n, i=0)), it appears to be a recursive function implementing a variant of the recurrence relation for generating numbers based on their radix representation. The function takes three arguments: m (the starting number), n (the end number), and i (an indentation level used for printing). It calculates and returns the sum of terms in a series related to the radix conversion process, which might be part of an algorithm for fast multiplication or similar numerical tasks. 

It's important to note that understanding this specific code snippet requires additional context, as it seems to be a piece of a larger program or library. Without knowing the exact purpose and surrounding code, a comprehensive explanation isn't possible.


The text discusses several aspects of recurrence relations, including their computation using matrix powers and polynomial arithmetic, as well as methods for handling inhomogeneous recurrences and generating functions.

1. **Fast Computation Using Matrix Powers**: This method uses the characteristic polynomial p(x) to create a companion matrix M. The k-th term of a recurrence can be computed by calculating the leftmost column of M^k (mod p(x)) and multiplying it with the initial values vector v. This approach is efficient when powering algorithms are used, especially for high-order recurrences.

2. **Faster Computation Using Polynomial Arithmetic**: This method further optimizes the matrix power approach by exploiting the structure of polynomial multiplication. It uses modular polynomial multiplications, reducing the complexity from O(log k * n^3) to O(log k * n^2) or even O(log k * n * log n), depending on the multiplication algorithm used.

3. **Inhomogeneous Recurrences**: The text explains how to transform inhomogeneous recurrence relations (those with an additional polynomial term P(n)) into homogeneous ones of higher order. This is done by repeatedly subtracting shifted versions of the original relation until the constant or polynomial term vanishes.

4. **Recurrence Relations for Subsequences**: The text describes methods for finding recurrences for subsequences of a given sequence. For two-term recurrences, it provides closed forms for the coefficients Ak using Chebyshev polynomials. For general order n recurrences, it suggests using the companion matrix and its powers to find the characteristic polynomial of the stride-s subsequence recurrence.

5. **Generating Functions for Recurrences**: A generating function is a power series where the k-th coefficient equals the k-th term of the recurrence. The text explains how to construct such functions using the characteristic polynomial and initial terms of the sequence.

6. **Binet Forms for Recurrences**: These are closed-form expressions for terms in a linear recurrence relation. For a two-term recurrence, it provides a formula involving the roots of the characteristic polynomial. The text also mentions Binet forms for n-term recurrences and conditions under which they hold.

The main advantage of these methods is their efficiency, especially for high-order recurrences and when modular arithmetic or polynomial multiplication algorithms optimized by Fast Fourier Transform (FFT) are used. These techniques find extensive applications in number theory, combinatorics, and other areas of mathematics and computer science.


The text discusses Chebyshev polynomials of the first and second kinds, denoted as Tn(x) and Un(x), respectively. These are defined using trigonometric functions or through explicit formulas involving binomial coefficients. 

1. **Definitions**: 
   - Tn(x) = cos[n arccos(x)]
   - Un(x) = sin[(n+1)arccos(x)] / sqrt[1-x^2]

   For integer n, both are polynomials. The first few are provided in figures 35.2-A and 35.2-B.

2. **Explicit Formulas**:
   - Tn(x) = Σ [(-1)^k * (n-k-1)! / (k! * (n-2k)! * (2x)^(n-2k))] for k=0 to ⌊n/2⌋
   - Un(x) = Σ [(-1)^k * (n-k)! / (k! * (n-2k)! * (2x)^(n-2k))] for k=0 to ⌊n/2⌋

3. **Properties**: 
   - The n+1 extrema of Tn(x) are at x_k = cos(kπ/n), where −1 ≤ x_k ≤ +1, with values ±1.
   - The n zeros of Tn(x) are at x_k = cos((k-1/2)π/n), for k=1 to n.

4. **Expansion**: 
   - For even n: x^n = 1/(2n) * (n choose n/2) + Σ [(n choose 2k-n) * T_(n-2k)(x)] for k=0 to ⌊n/2⌋ - 1
   - For odd n: x^n = Σ [(n choose (2k-n)) * T_(n-2k)(x)] for k=0 to (n-1)/2

5. **Recurrence Relations**: 
   - Nn = 2xNn−1 −Nn−2, where N can be either T or U
   - Subsequence recurrences: Nn+1 = [2x] * Nn - Nn−1, Nn+2 = [2(2x^2 -1)] * Nn - Nn−2, etc.

6. **Generating Functions**: 
   - 1 −xt / (1 −2xt + t^2) = Σ tn Tn(x), and similarly for Un(x).

7. **Binet Formulas**:
   - Tn(x) = (x + sqrt[x^2 -1])^n + (x - sqrt[x^2 -1])^n / 2, and Un(x) = sqrt[(x^2 -1)/(4x)] * [(x + sqrt[x^2 -1])^n - (x - sqrt[x^2 -1])^n].

8. **Composition Law**: 
   - T_m(T_n(x)) = T_(mn)(x).

9. **Fast Computation Algorithms**:
   - The text presents efficient algorithms for computing these polynomials, particularly for large n. These methods exploit the recurrence relations and avoid expensive operations like multiplication by x or division.

   For Chebyshev polynomials of the first kind (Tn), the algorithm recursively computes pairs [Tn-1, Tn] using a binary splitting method. Similarly, an efficient method for Un is described using the relation Un = (Tn - xTn+1) / (1 - x^2). 

These Chebyshev polynomials have numerous applications in numerical analysis and approximation theory, particularly for approximating functions or solving differential equations over intervals. They are named after Pafnuty Chebyshev, a 19th-century Russian mathematician who introduced them.


This section discusses hypergeometric series and functions, which are a set of special functions that include many useful mathematical functions like logarithms and sines as special cases.

**Deﬁnition and Basic Operations (36.1)**

The hypergeometric series F(a, b; c | z) is defined as:

F(a, b; c | z) = ∑_{k=0}^∞ [(a)__k (b)__k] / [(c)__k k!] * z^k

where (x)__n denotes the Pochhammer symbol or rising factorial power: x(x+1)(x+2)...(x+n-1), and (x)__0 = 1. The variable z is called the argument, while a, b, and c are parameters. 'a' and 'b' are upper parameters, and 'c' is the lower parameter.

Hypergeometric series can have any number of parameters, and they correspond to hypergeometric functions when they converge. Hypergeometric functions with rational arguments can be computed using binary splitting methods (34.1.2).

**Derivative and Differential Equation (36.1)**

The n-th derivative of a hypergeometric function f(z) = F(a, b; c | z) is:

d^n/dz^n [F(a, b; c | z)] = [(a)__(n+1) (b)__(n+1)] / [(c)__(n+1) n!] * F(a+n, b+n; c+n | z)

The function f(z) = F(a, b; c | z) is a solution of the differential equation:

z (1 - z) d^2 f/dz^2 + [c - (1 + a + b) z] df/dz - a b f = 0

**Evaluations for Fixed Argument (36.1)**

Closed-form evaluations of hypergeometric functions at specific points are given for certain conditions. For example, the evaluation at z=1 for F(a, b; c | z) is:

F(a, b; c | 1) = Γ(c) * Γ(c - a - b) / [Γ(c - a) * Γ(c - b)] if Re(c - a - b) > 0 or b ∈ N and b < 0

**Extraction of Even and Odd Part (36.1)**

The even part E[f(z)] and odd part O[f(z)] of a hypergeometric series are given by:

E[F(a, b; c | z)] = F(a/2, (a+1)/2, b/2, (b+1)/2; c/2, (c+1)/2, 1/2 | z^2)
O[F(a, b; c | z)] = a * b / c * z * F((a+1)/2, (a+2)/2, (b+1)/2, (b+2)/2; (c+1)/2, (c+2)/2, 3/2 | z^2)

**Multisection by Selecting Terms with Exponents s mod M (36.1)**

For a power series H(z), the multisection operation extracts terms whose exponent of z is congruent to s modulo M:

H[s,M](z) = 1/M ∑_{k=0}^{M-1} ω^(-s k) * H(ω^k z)

where ω = exp(2πi/M). For hypergeometric functions, this operation involves replacing each upper and lower parameter A with (A+s)/M, ..., (A+s+M-1)/M, the argument z with X*z^M, and multiplying by z^s.

**Transformations of Hypergeometric Series (36.2)**

Hypergeometric series parameters in the upper or lower row can be swapped: F(a, b, c | z) = F(b, a, c | z). Identical elements in the lower and upper row can be canceled: F(a, b, C | z) = F(a, b | z) if C appears in both rows.

**Elementary and Contiguous Relations (36.2)**

Identities of contiguous relations are given by:

(a - b) * F(a, b; c | z) = a * F(a+1, b; c | z) - b * F(a, b+1; c | z)
(a - c) * F(a, b; c+1 | z) = a * F(a+1, b; c+1 | z) - c * F(a, b; c | z)

**Pfaﬀ's Reﬂection Law and Euler's Identity (36.2)**

Pfaﬀ's reﬂection law relates the value of a hypergeometric function at z to its value at -z:

1/(1-z)^a * F(a, b; c | -z / (1-z)) = F(a, c-b; c | z)


The provided text discusses various transformations of hypergeometric series, which are special functions that appear frequently in mathematical physics, combinatorics, and other areas. These transformations allow for the manipulation and simplification of expressions involving these functions. Here's a detailed summary and explanation of some key points:

1. **Euler's Transformation**: Euler's transformation is given by equation (36.2-9), which expresses the ratio of two hypergeometric functions as a Padé approximant for the r-th root, provided both series terminate. This means that under certain conditions, the ratio simplifies to a rational function approximation.

2. **Generalizations of Euler's Transformation**: The text presents two generalized forms of Euler's transformation (36.2-10 and 36.2-11) for hypergeometric functions of type 3F2, where one upper parameter exceeds a lower parameter by 1. These transformations involve specific constants (f and g) derived from the parameters a, b, and c.

3. **Gauss' Transformations**: Gauss provided two quadratic transformations for hypergeometric series, as seen in equations (36.2-12a) and (36.2-12b). These transformations relate 2F1 series with different arguments through the variable z. A rewritten form of these transformations is also given (36.2-13), which connects 2F1 series for the argument 1−z/2.

4. **Whipple's Identity**: Whipple's identity (36.2-14) establishes a connection between two specific hypergeometric functions of type 3F2. This identity is useful in simplifying or relating different expressions involving these functions, particularly when certain parameters are equal or related.

5. **Special Cases and Further Transformations**: The text presents several special cases and additional transformations derived from Gauss' and Whipple's identities, such as (36.2-15), (36.2-16), (36.2-18a/b), (36.2-19a/b), and others. These relations provide further simplification or manipulation possibilities for hypergeometric series expressions.

6. **Clausen's Product Formula**: This formula (36.2-24) connects two different hypergeometric functions of type 2F1 and 3F2, offering a way to express one in terms of the other under specific conditions. Goursat's relation (36.2-25) is another similar product-type identity for hypergeometric series.

7. **Kummer Transformation**: The Kummer transformation (36.2-33) relates two 1F1 (confluent hypergeometric functions) by expressing one in terms of the other, multiplied by an exponential function. This transformation is particularly useful for manipulating expressions involving confluent hypergeometric series.

8. **Additional Transformations**: Various other transformations are presented, connecting hypergeometric functions of different types (e.g., 1F1 to 2F3) and involving relationships between parameters (e.g., (36.2-34), (36.2-37), and (36.2-40)). These transformations offer further flexibility in working with hypergeometric series expressions.

These transformations, identities, and relationships are essential tools for manipulating, simplifying, or evaluating complex expressions involving hypergeometric functions. They find applications in diverse fields such as physics, engineering, and mathematics, allowing researchers to tackle problems that would otherwise be challenging or impossible to solve directly.


The provided text is a section from a mathematical reference on Hypergeometric series, specifically focusing on elementary functions expressed as hypergeometric functions. Here's a detailed summary and explanation of the content:

1. **Power, Root, and Binomial Series (36.3.1):**
   - The reciprocal of (1-z)^a can be represented as a Hypergeometric function F(a; ; z), and similarly for (1+z)^a. These are given by relations 36.3-1a and 36.3-1b, respectively.
   - An important special case is 1/(1-z) = F(1; ; z).

2. **Additional Identities:**
   - The text provides several identities involving Hypergeometric functions with specific parameters. For example, F((-n); n+1; n; -z) = (1-2z)(1-z)^(n-1), showing a relationship between these functions and polynomial expressions.

3. **Chebyshev Polynomials:**
   - The Chebyshev polynomials T_n(x) can be expressed using Hypergeometric functions, as shown in 36.3-7a, 36.3-7b, and 36.3-7c. 

4. **Hermite Polynomials (36.3.3):**
   - The Hermite polynomials H_n(z) are defined by a recurrence relation involving the previous two terms. The Hypergeometric representation of H_n(z) for non-negative integer n is given in 36.3-10.

5. **Exponential and Logarithm (36.3.4):**
   - The exponential function exp(z) equals F(; ; z), and the natural logarithm log(1+z) can be expressed as a Hypergeometric series in 36.3-12a.

6. **Bessel Functions and Error Function (36.3.5):**
   - The Bessel functions of the first kind J_n(z) and modified Bessel functions of the second kind I_n(z) are expressed using Hypergeometric functions in 36.3-14a and 36.3-14b, respectively.
   - The Error function erf(z), defined as √π/2 * z * F(1/2; 3/2; -z^2), is given in 36.3-15a.

7. **Trigonometric and Hyperbolic Functions (36.3.6):**
   - Series expansions for sine, hyperbolic sine, cosine, and hyperbolic cosine are provided as Hypergeometric functions. For example, sin(z) = z * F(3/2; ; -z^2/4).

8. **Inverse Trigonometric and Hyperbolic Functions (36.3.7):**
   - Series expansions for arctan(z), arctanh(z), arccoth(z), and the inverse of logarithm are given using Hypergeometric functions. For example, arctan(z) = z * F(1/2; 1/3; -z^2).

These relations provide powerful tools for expressing elementary functions as Hypergeometric series, enabling a wide range of mathematical manipulations and approximations.


The provided text discusses several mathematical concepts, primarily focusing on Cyclotomic Polynomials, Möbius Inversion Principle, Lambert Series, and continued fractions.

1. **Cyclotomic Polynomials**: These are polynomials with integer coefficients whose roots are the primitive nth roots of unity (complex numbers that satisfy x^n = 1). The degree of these polynomials equals Euler's totient function φ(n), which counts the positive integers up to n that are coprime to n. For example, the 63rd cyclotomic polynomial is Y63(x) = x^36 - x^33 + x^27 - x^24 + x^18 - x^12 + x^9 - x^3 + 1.

   The algorithm to compute these polynomials involves prime factorization of n, then recursively dividing the variable by each prime factor and finally adjusting the resultant polynomial with respect to the highest power of primes in the factorization.

2. **Möbius Inversion Principle**: This principle relates two arithmetic functions through a summation process. The Möbius function µ(n) is central to this principle. It's defined such that it equals 0 for numbers with square factors, +1 for 1, and (-1)^k for n being the product of k distinct primes.

   The key property of the Möbius function is that the sum over all divisors d of a number n equals 1 if n = 1, and 0 otherwise: ∑_{d|n} µ(d) = {1 if n=1; 0 otherwise}.

   Interestingly, this function can also be expressed as a sum of the primitive nth roots of unity.

3. **Lambert Series**: The text mentions Lambert series but does not provide explicit definitions or formulas. Generally, a Lambert series is a power series whose terms are the values of an arithmetic function at integer arguments, weighted by the reciprocals of consecutive powers of some base. They're often used in number theory for summing up divisors.

4. **Continued Fractions**: The text briefly mentions continued fractions but does not provide details about their computation algorithms. A continued fraction is an expression obtained through a recursive process of representing a number as the sum of its integer part and the reciprocal of another number, then repeating this process with the new number. They're useful in various areas of mathematics including number theory and approximation theory.

In summary, the text provides insights into cyclotomic polynomials, an essential concept in algebraic number theory, and introduces related mathematical principles like Möbius Inversion and Lambert series. It also hints at continued fractions without delving into their specific computation methods.


The text discusses various mathematical concepts related to power series, Lambert series, and infinite products. Here's a detailed summary and explanation of the key points:

1. **Multiplicative Functions and Möbius Inversion**: The Möbius function (μ(n)) is multiplicative, meaning it satisfies certain conditions for prime factorization. A multiplicative function has values that can be computed using prime factors of n. The Möbius inversion principle provides a way to switch between two functions, f(n) and g(n), such that f(n) = Σ_{d|n} g(d)μ(n/d).

2. **Cyclotomic Polynomials**: These are polynomials with integer coefficients whose roots are primitive nth roots of unity. They play a significant role in number theory and have a product form involving the Möbius function. The cyclotomic polynomial Φ_n(x) can be expressed as Σ_{d|n} μ(d) x^{n/d}.

3. **Lambert Series**: These are series of the form L(x) = Σ_{k>0} a_k x^k / (1 - x^k). They have conversions to power series and can be used to represent certain mathematical objects, like the number of divisors function d(n).

4. **Conversion from Power Series to Infinite Products**: The text describes algorithms to convert power series into infinite products of specific forms:

   - **Q_k>0 (1 - x^k)^b_k**: This involves finding coefficients b_k such that the power series' derivative divided by the series itself equals a Lambert series with coefficients -k * b_k. The product form is then obtained by applying the inverse of this transformation to the Lambert series.

   - **Q_k>0 (1 + x^k)^c_k**: For these, a greedy algorithm is used to subtract and add terms from the power series to obtain a Lambert series-like form, which can then be converted into a product.

   - **Eta Products**: The eta function η(x) = ∏_{j>0} (1 - x^j) can be used to represent certain power series as infinite products. Algorithms are provided to convert between power series and eta/eta+ products.

These conversions are useful in various areas of mathematics, including number theory, combinatorics, and the study of integer partitions. They allow researchers to switch between different representations of mathematical objects, potentially simplifying computations or revealing underlying structures.


The text discusses continued fractions, their properties, and methods for computing them. Here's a detailed summary:

**Continued Fractions:**

A continued fraction is an expression of the form K(a, b) = a0 + b1/(a1 + b2/(a2 + b3/(...))). The bk are called partial numerators, and ak are partial denominators. If bk+1 is set to zero, the value obtained is called the k-th convergent of the continued fraction: Pk/Qk = a0 + b1/(a1 + b2/(a2 + ... + bk-1/(ak-1 + bk/ak))).

**Simple Continued Fractions:**

A simple continued fraction has all bk equal to 1. Rational numbers have terminating simple continued fractions. Solutions of quadratic equations with non-rational roots have eventually periodic simple continued fractions.

**Convergents and Approximation:**

The k-th convergent (in lowest terms) is the best rational approximation to a real number x, where the inequality |x - Pk/Qk| < 1/(Q_k * Q_(k-1)) holds. The equality can only occur for terminating continued fractions.

**Computing Simple Continued Fractions:**

The simple continued fraction of a real number x can be computed using an iterative process that repeatedly applies the floor function to x and stores the integer part as the next term in the continued fraction.

**Continued Fractions of Polynomial Roots:**

For algebraic numbers (roots of polynomial equations with integer coefficients), simple continued fractions can be computed by finding successive real roots of shifted and negated reciprocal versions of the original polynomial.

**Computation of Convergents:**

The sequences of partial numerators Pk and denominators Qk for a given continued fraction can be computed using recurrence relations: Pk = ak * P_(k-1) + bk * P_(k-2), Qk = ak * Q_(k-1) + bk * Q_(k-2). These sequences give the convergents of the continued fraction.

**Evaluation:**

A function to compute the numerical value x from a simple continued fraction uses an iterative process that starts with the last term and moves backward through the sequence, adding 1 divided by each term. For general (non-simple) continued fractions, this process needs to be adapted to account for the partial numerators bk as well.

The text also mentions methods for computing continued fractions for algebraic numbers and references resources for further study.


The text discusses a variation of the iteration for the inverse function, focusing on two specific functions I(y) and J(y). 

1. **Function I(y):** This is defined as the infinite series 1/(1 - y), which can also be expressed as (1 + y)(1 + y^2)(1 + y^4)...(1 + y^k).... The sign of each term in this product alternates, starting with a positive one. 

2. **Function J(y):** This is defined similarly to I(y), but the signs are reversed: (1 - y)(1 - y^2)(1 - y^4)...(1 - y^k).... The sequence of zeros and ones in the binary expansion of this function is known as the Thue-Morse sequence (A106400). 

The parity number, P, is a constant derived from J(y) when y = 1/2. It can be computed using the iteration for K(y), which is defined as half the square of the difference between I(y) and J(y). 

Several relations are provided that connect these functions: 

- I(y) * I(-y) = I(y^2) / 2 
- I(y) = J(y^2) / J(y) 
- I(-y) = (1 - y) / (1 + y) * I(y) 
- J(-y) = (1 + y) / (1 - y) * J(y) 

These relations show how the functions I and J are related to each other and to their inverses. 

The text also provides algorithms for computing I(y) and J(y), as well as for the parity number P. These algorithms involve iterative processes that manipulate the variable y according to specific rules. 

Finally, an inverse function for J(y), denoted as binpart(y), is introduced. This function can be computed without division, making it useful in contexts where division operations are costly or problematic.


The provided text discusses several iterations related to various mathematical sequences, each with its unique properties and functional equations. Here's a detailed explanation of each:

1. **Period-Doubling Sequence (Section 38.5):**
   - Definition: T(y) is defined as the sum of y^(2n)/(1+(-1)^n), for all non-negative integers n. This results in an infinite series: T(y) = Σ (−1)^n * y^(2n) / (1 - y^(2n)).
   - Representation: The sequence starts with 0, and each subsequent term is generated by replacing every 0 with 11 and every 1 with 10. This can be visualized through a string substitution process (Figure 38.5-A).
   - Generating Function: T(y) = y + y^3 + y^4 + y^5 + y^7 + y^9 + y^11 + ..., which represents the power series of the period-doubling sequence.
   - Functional Equation: The key equation governing T(y) is T(y) + T(y^2) = y / (1 - y), connecting the function at two different powers of y.
   - Constant: The constant T := T(1/2) is a transcendental number with various base representations and continued fraction expansion.

2. **Golay-Rudin-Shapiro Sequence (Section 38.3):**
   - Definition: Q(y) is generated by an iterative process that starts with y and involves three operations in each step: updating the sequence (Ln), calculating a new term (Rn = Ln + Rn^2 * (Ln - Rn)), and squaring the current term (Yn+1 = Y_n^2).
   - Function: Q(y) is defined as an infinite series: 1 + y + y^2 - y^3 + y^4 + y^5 - y^6 + ..., representing a specific pattern of positive and negative y-powers.
   - Constant: The Golay-Rudin-Shapiro constant Q, calculated as 1/2 * Q(1/2), is also transcendental with various base representations (hexadecimal in this context).
   - Functional Equations: Several functional equations govern the behavior of Q(y), such as Q(y^2) = Q(y) + Q(-y)/2, and more complex relations involving powers of y.

3. **Thue Constant (Section 38.2):**
   - Definition: The Thue constant T is constructed via a substitution process where every '0' becomes '111', and every '1' becomes '110'. This results in an infinite string, from which the power series T(y) = Σ y^(3n), for n = 0, 1, ..., can be derived.
   - Functional Equation: The Thue constant obeys a specific equation involving y, y^2, and y^3 terms, linking its value at different powers of y.

4. **Ruler Function (Section 38.4):**
   - Definition: The ruler function R(y) is defined through an iterative process starting with y and using a series of updates involving squaring the current term (Yn+1 = Y_n^2), updating sequence values based on these squared terms, and adding specific contributions based on n.
   - Constant: The constant R := R(1/2)/2 is transcendental, with its binary representation detailing a pattern of ones and zeros.
   - Functional Equation: A key equation governing R(y) involves y, y^2, and y^4 terms.

Each of these iterations and constants has unique properties and plays significant roles in various mathematical contexts, such as number theory, dynamical systems, and symbolic dynamics. They often exhibit intriguing patterns and connections that are still objects of ongoing study and exploration.


The text discusses several iterations related to various mathematical sequences, specifically focusing on the period-doubling sequence, generalizations of this sequence, and iterations connected with binary digit summations and Gray codes. 

1. Period-Doubling Sequence: The period-doubling sequence is a series of 0s and 1s where each term (starting from the second) is formed by doubling the position of the last non-zero digit in the previous term, then replacing all zeros in between with ones. This results in a sequence that has a pattern of repeating at intervals of 2^n for n ≥ 0. 

2. Generalizations: 
    - The functional equation F(y) + F(y3) = y/(1-y) leads to a new function F(y), which is a Lambert series with coefficients given by R(k) depending on the divisibility of k by 2 and 3.
    - Another generalization involves the functional equation F(y) + F(y^2) + F(y^3) = y/(1-y). This can be solved recursively to produce a function F(y), which is also shown to satisfy this equation through power series analysis.

3. Iterations related to Binary Digit Sum: 
    - A string substitution rule generates the "1's-counting sequence" (A000120 in OEIS), where each term represents the sum of binary digits of natural numbers up to that point. The corresponding function S(y) is defined by recursive rules involving powers and sums of previous terms, leading to a power series with coefficients given by the sum-of-digits sequence.
    - A similar approach generates a weighted sum of binary digits (revbin constant), denoted W(y). The resulting function W(y) satisfies a functional equation involving y and its powers.

4. Iterations related to Binary Gray Code: 
    - The Gray code is an ordering of binary numbers such that each successive value differs by only one bit. An iteration is defined where the power series coefficients are the binary Gray code of the exponent of y, leading to a function G(y). This function has associated functional equations and its constant value at y=1/2 (Gray code constant) is also discussed.

In all these iterations, the focus is on defining sequences or functions recursively based on string substitution rules or arithmetic operations, then exploring their properties through power series analysis and identifying underlying patterns and functional relationships. These mathematical constructions provide insights into number theory, combinatorics, and sequence analysis.


The text discusses several iterations related to binary Gray code, focusing on functions G(y), F(y), R(y), E(y), H(y) that generate sequences with specific properties. Here's a detailed summary of each:

1. **G(y) - Gray Code Series**: This is defined by the infinite series (38.8-5). It can be computed everywhere except on the unit circle, and its coefficients are powers of 2 in magnitude. The function G(y) relates to the Gray code sequence, a binary numeral system where two successive values differ by only one bit.

2. **F(y) - Differences of the Gray Code**: F(y) = (1 - y)G(y), which gives the power series whose coefficients are the successive differences of the Gray code. Its sequence corresponds to the ruler function, which outputs the highest power of 2 that divides a given number.

3. **R(y) - Sum of Gray Code Digits**: This is defined by the infinite series (38.8-12). It represents the sum of binary digits in the Gray code sequence for k ≥0. The constant R := R(1/2)/2 has a specific value (38.8-10) and a binary expansion that corresponds to the paper-folding sequence.

4. **P - Paper-Folding Constant**: Defined as P = (R + 1)/2, this constant has various representations in different bases. It is related to the paper-folding process used to create origami figures.

5. **E(y) - Differences of Sum of Gray Code Digits**: This function is derived from F(y) by (38.8-17), with all power series coefficients except for the constant term being ±1. It's defined and computable everywhere except on the unit circle, with a specific iteration given in (38.8-19).

6. **H(y) - Hilbert Curve Encoding Function**: This function is defined through an iterative process (38.9-1), generating a sequence of complex numbers that, when interpreted as movement instructions, trace out the Hilbert curve in the complex plane. The real and imaginary parts alternate, representing right/left turns and up/down movements.

The text also introduces a simplified algorithm for computing H(y) (38.9-5), using two auxiliary functions P(y) and M(y). These are computed via their respective iterations (38.9-3 and 38.9-4), making the overall computation more efficient.

In addition, the text discusses a sparse power series iteration (38.10) for computing the reciprocal of (1 - y), with applications in number theory and continued fractions. The function F(y) generated by this iteration has interesting properties concerning its binary and decimal expansions.


The text discusses two iterations related to mathematical sequences, specifically the Fibonacci numbers and Pell numbers. 

**Fibonacci Iteration (Section 38.11):**

1. **Function Definition**: The function A(y) is defined through an iterative process that involves Fibonacci numbers. It's initialized with L0=0, R0=1, l0=1, and r0=y. 

2. **Iteration Rules**: 
   - ln+1 = rn (where rn = yFn+1)
   - rn+1 = rn ln (where ln = yFn+2)
   - Ln+1 = Rn
   - Rn+1 = Rn + rn+1 Ln + rn+1 ln

3. **Power Series**: After the n-th step, the series in y is correct up to order Fn+2 −1, with a convergence rate of √(5+1)/2 ≈ 1.6180. The power series representation of A(y) is given by:

   A(y) = 1 + y^2 + y^3 + y^5 + y^7 + y^8 + y^10 + y^11 + y^13 + y^15 + y^16 + y^18 + y^20 + ...

4. **Relation to Fibonacci Numbers**: The coefficients of the power series correspond to the Fibonacci numbers where the index is even (A022342). 

5. **Continued Fraction Representation**: There's a continued fraction representation for (1 − 1/q) A(1/q), which grows doubly exponentially with large q values.

**Pell Iteration (Section 38.12):**

1. **Function Definition**: The function B(y) is defined using an iterative process involving Pell numbers. It starts with L0=1, R0=1+y, l0=y, and r0=y. 

2. **Iteration Rules**: 
   - ln+1 = rn (where rn = yFn+1)
   - rn+1 = r^2_n ln (where ln = yFn+2)
   - Ln+1 = Rn
   - Rn+1 = Rn + rn+1 Rn + r^2_n+1 Ln

3. **Power Series**: After the n-th step, the series in y is correct up to order pn (where pn are Pell numbers), with a convergence rate of √2 + 1 ≈ 2.4142. The power series representation of B(y) is:

   B(y) = 1 + y + y^3 + y^4 + y^6 + y^7 + y^8 + y^10 + y^11 + y^13 + y^14 + y^15 + y^17 + y^18 + ...

4. **Pell Palindromic Constant**: The function P(y) is defined similarly but results in a palindromic sequence, specifically when R0=1+y+y^2, it computes 1-y. 

5. **Continued Fraction Representation**: Like the Fibonacci case, there's a continued fraction representation for (1 - 1/q) B(1/q), which also grows doubly exponentially with large q values.

Both iterations have applications in various areas of mathematics and computer science, particularly in sequence generation and digital signal processing due to their recursive nature and palindromic properties.


The text discusses the implementation of arithmetic operations modulo m, which are essential in various fields such as cryptography, error correcting codes, and digital signal processing. The modular arithmetic functions include addition, subtraction, multiplication, power, inversion, and division.

1. **Addition and Subtraction**: These are straightforward to implement using the modulus operation. If `a >= b`, then `sub_mod(a, b, m)` returns `a - b`. Otherwise, it returns `m - b + a`. The addition table for moduli 13 and 9 is provided in Figure 39.1-A.

2. **Multiplication**: Multiplication modulo m is more complex. A naive approach would limit the modulus to half of the word size. However, a more efficient method involves using the formula: `a · b = (⌊a·b/m⌋ * m + ⟨a·b⟩m) mod z`, where `z` is the word size. This technique allows for larger moduli by using floating-point arithmetic to compute the most significant bits and integer arithmetic for the least significant bits.

   The implementation uses a 64-bit unsigned integer type (`uint64`) for both operands and result, and a 64-bit floating-point type (`float64`, typically `long double`) for intermediate calculations. It computes `y = ⌊a·b/m⌋ * m` using floating-point arithmetic, then calculates the residue `r = a·b - y`. If `r` is negative, it adds `m` to get the positive residue.

   The normalization step (line 10) is optional and can be omitted if the modulus is less than `2^62`. For fixed moduli, the division by `m` can be replaced with multiplication by the modular inverse of `m`, which only needs to be computed once.

3. **Power**: The power operation modulo m can be implemented using exponentiation by squaring, a method that reduces the number of multiplications needed. This algorithm works by repeatedly squaring the base and multiplying into the result when the current bit of the exponent is 1.

4. **Inversion and Division**: Modular inversion (finding the modular multiplicative inverse) can be computed using the extended Euclidean algorithm. Division modulo m can then be performed by multiplying the dividend by the modular inverse of the divisor.

These operations are crucial in various applications, including primality tests and cryptographic algorithms that rely on modular arithmetic. The choice of data types (64-bit integers and 64-bit floating-point numbers) ensures efficient computation while handling large moduli.


The order of an element a modulo m, denoted as ord_m(a), is the smallest positive integer k such that a^k ≡ 1 (mod m). If no such k exists, we say that the order is infinite. The order of any finite group element divides the order of the group, which in modular arithmetic is Euler's totient function φ(m) for coprime a and m.

For prime moduli p, the possible orders are 1, 2, ..., φ(p). For composite moduli, the situation is more complex. The order of an element modulo a composite number n can be found by factoring n into its prime factors and applying the Chinese Remainder Theorem (CRT) to compute the order modulo each prime power factor.

The order of an element modulo m can be computed using the following algorithm:

1. Factorize m into its prime powers, i.e., m = p1^e1 * p2^e2 * ... * pk^ek where pi are distinct primes and ei are positive integers.
2. For each prime power pj^ej, compute ord_pj^ej(a) using the following steps:
   a. If pj is odd, set k = 1 while (a^k) mod pj^ej ≠ 1; then ord_pj^ej(a) = k.
   b. If pj = 2 and ej > 1, set k = 1 while (a^k) mod 2^(ej+1) ≠ 1; then ord_pj^ej(a) = k * 2^(ej-1).
3. Using the CRT, compute the order of a modulo m as:
   ord_m(a) = lcm(ord_p1^e1(a), ..., ord_pk^ek(a))

This algorithm relies on the fact that the order of an element modulo a prime power pj^ej is related to its order modulo pj. For odd primes, the order modulo pj^ej is simply the smallest k such that a^k ≡ 1 (mod pj^ej). For even primes, particularly 2, the situation is more complicated due to the presence of small subgroups. In this case, we look at the order modulo a larger power of 2 and then adjust accordingly.

The orders of elements play a crucial role in number theory and cryptography. They are used, for instance, in the Diffie-Hellman key exchange and the ElGamal encryption system. Moreover, the order of an element modulo m determines the size of the subgroup generated by that element, which is essential in understanding the structure of multiplicative groups modulo m.


The given text discusses the concept of order in modular arithmetic, particularly within the context of groups. Here's a detailed summary and explanation:

1. **Order (r)**: For an element 'a' modulo 'm', the order r = ord(a) is defined as the smallest positive integer such that ar ≡ 1 (mod m). If no such integer exists, the order is undefined. The order essentially tells us how many times we need to multiply 'a' by itself to get back to 1, modulo 'm'.

2. **Roots of Unity**: An element 'a' whose r-th power equals 1 (i.e., ar ≡ 1 (mod m)) is called an r-th root of unity. If ax ≢ 1 for all x < r but ar ≡ 1, then 'a' is a primitive r-th root of unity. A primitive r-th root of unity generates the cyclic group of order r under multiplication modulo m.

3. **Maximal Order (R(m))**: The maximal order R(m) for an element modulo m is the highest order among all elements. For prime modulus p, R(p) = p-1. Elements of this maximal order are known as primitive roots or generators of the multiplicative group.

4. **Index (i)**: The factor by which the order of an element falls short of the maximal order is called its index. If 'r' is the order and 'i' is the index, then i * r = R(m).

5. **Group Theory Connection**: These concepts come from group theory. In the multiplicative group modulo m (consisting of invertible elements under multiplication), the order refers to how often we need to multiply an element to reach the identity (1) in this group. The additive group (all elements under addition) has simpler orders, being the smallest integer 'm' such that ma ≡ 0 (mod m).

6. **Prime Modulus**: If the modulus m is prime (p), then Z/pZ forms a finite field Fp = GF(p), where all non-zero elements have inverses and division is possible. The maximal order R(p) = p - 1, and elements of this order are called primitive roots modulo p or generators modulo p. To test if an element 'g' is a primitive root, we check whether g^((p-1)/q) ≠ 1 (mod p) for all prime divisors q of p - 1.

7. **Composite Modulus**: For composite modulus m, the concept becomes more complex due to the presence of non-invertible elements. The totient function ϕ(m) counts numbers less than 'm' and coprime to 'm'. This function is multiplicative: ϕ(x1 * x2) = ϕ(x1) * ϕ(x2) for coprime x1, x2. If m has prime factorization m = p₁^e₁ * ... * pₖ^eₖ, then |(Z/mZ)*| = ϕ(m). The multiplicative group (Z/mZ)* is isomorphic to the direct product of groups modulo prime powers.

8. **Cyclic vs Non-cyclic Groups**: Not all multiplicative groups modulo m are cyclic (i.e., generated by a single element). A group is cyclic if and only if ϕ(m) is even, thanks to a theorem by Gauss. For example, (Z/8Z)* is not cyclic because ϕ(8) = 4, which is even, but (Z/10Z)* is non-cyclic despite ϕ(10) = 4, due to the presence of multiple generators of order 2.

In essence, this text provides a comprehensive overview of order in modular arithmetic, its relation to group theory, and how these concepts apply differently for prime versus composite moduli. It also introduces key functions like the totient function ϕ(m) and discusses properties of cyclic groups within this context.


The text discusses various aspects of modular arithmetic, focusing on cyclic groups, noncyclic groups, generators, and quadratic residues. Here's a summary of the main points:

1. **Cyclic Groups**: A group (Z/mZ)* is called cyclic if it contains an element whose powers generate all invertible elements in the group. This maximal order is denoted as R(m). If R(m) equals the number of invertible elements, |(Z/mZ)*| = ϕ(m), then the group is cyclic; otherwise, it's noncyclic.

2. **Non-Cyclic Groups**: In noncyclic groups, no single element generates all invertible elements. The given example (Z/15Z)* shows such a case where no element of maximal order can generate all units.

3. **Maximal Order Calculation**: Algorithms are provided for computing the maximal order R(m) and finding an element of maximal order in a group (Z/mZ)*:
   - `maxorder(m)` computes R(m).
   - `order(x, m)` calculates the order of a given element x in (Z/mZ)*.
   - `maxorder_element(m)` finds an element in (Z/mZ)* with maximal order.

4. **Generators**: In cyclic groups, generators are elements whose powers cycle through all group elements. The number of generators is ϕ(ϕ(n)). For prime moduli p, any generator modulo p is also a generator modulo 2^k * p for k ≥ 1 if it's odd; otherwise, g + pk is a generator.

5. **Quadratic Residues**: These are values 'a' that satisfy the equation x² ≡ a (mod p), where p is prime. The quadratic residues can be determined using the Legendre symbol, which has properties like the law of quadratic reciprocity and relations for specific arguments.

6. **Kronecker Symbol**: A generalization of the Legendre symbol for composite moduli, allowing us to determine whether a number is a quadratic residue modulo a composite modulus.

7. **Square Roots Modulo m**: Algorithms are presented for computing square roots modulo primes (p = 4k + 3), prime powers (pe), and composites:
   - For primes p = 4k + 3, the square root is given by ±a((p+1)/4) mod p.
   - For prime powers pe with e ≥ 2, Newton's iteration can be used. The case p = 2 requires a separate treatment.

These concepts and algorithms are crucial in various areas of number theory and cryptography.


The text discusses the Rabin-Miller test, a probabilistic method used to prove the compositeness of an integer. The test is based on strong pseudoprimes (SPPs), composite numbers for which certain properties hold regarding modular exponentiation with respect to a base 'a'.

1. **Pseudoprimes and Strong Pseudoprimes:** A number n is called a pseudoprime to base a if `an-1 ≠ 1 (mod n)`. If `an-1 = 1 (mod n)`, then n is said to be a strong pseudoprime to base a. Carmichael numbers are composite numbers that are pseudoprimes to all bases relatively prime to them.

2. **Rabin-Miller Test for Compositeness:** This test uses the concept of strong pseudoprimes to prove compositeness with high probability. It involves examining the sequence `b, b^2, ..., b^(2t)`, where `n-1 = q*2^t` and `q` is odd. If neither of the conditions (1) `b ≡ 1 (mod n)` nor (2) `b^e ≡ -1 (mod n)` for some `0 <= e < t` hold, then n is composite.

3. **Algorithm Steps:**

   a. Compute `n-1 = q * 2^t`.
   b. Initialize `b = a^q (mod n)`.
   c. If `b == 1`, return true (n is probably prime).
   d. For `e` from 0 to `t-1`:
      - Update `b = b^2 (mod n)`.
      - If `b == n-1`, break the loop.
   e. If `b != n-1`, return false (n is composite).

4. **Probability of Error:** For a composite number, the probability that it passes the Rabin-Miller test for a single base is at most 1/4. Therefore, by testing multiple bases, we can significantly increase our confidence in the compositeness of a number.

5. **Efficiency and Restrictions:** While the test does not guarantee primality, it effectively rules out compositeness with high probability. To speed up the test for small numbers (e.g., n < 2^32), one can restrict base testing to common bases like 2, 3, and 5, and use lookup tables for known composite SPPs.

6. **Extensions:** The concept of strong pseudoprimes extends beyond two bases. Numbers that are SPP to several chosen bases or even all prime bases up to a certain limit can be constructed. However, a number being an SPP to bases `a1` and `a2` does not necessarily imply it's an SPP for the base `a1*a2`.

In summary, the Rabin-Miller test is a powerful tool in computational number theory for distinguishing prime from composite numbers probabilistically. Its effectiveness lies in its ability to rule out compositeness with high confidence using modular exponentiation and carefully chosen base sequences.


The text describes several methods for proving primality, which are primarily used to demonstrate the primality of numbers that satisfy specific conditions. Here's a detailed explanation of each method:

1. **Pratt's Certiﬁcate of Primality**: This method requires the factorization of n-1 and the determination of a primitive root modulo p (where p is prime). The certiﬁcate is represented as a tree, where each level corresponds to a prime factor of n-1. Each leaf node provides primality proof for its parent, with increasing levels representing successive factorizations. This method is only practical for numbers of special forms due to the complexity of computing large factorizations.

2. **Pocklington-Lehmer Test**: This test requires knowledge of a partial factorization of n-1. It checks whether, for each prime factor q of F (where F = n - 1), there exists an integer a such that a^((n-1)/q) ≡ 1 (mod n) and gcd(a^((p-1)/q) - 1, p) = 1. If this condition holds for all prime factors, then n is prime. This test can be applied to numbers of the form p = F * U + 1, where F's factorization is known, and U < F.

3. **Tests for n = k 2^t + 1 (Proth's Theorem)**: If there exists an integer a such that a^((n-1)/2) ≡ -1 (mod n), then n must be prime (Proth's theorem). This test is particularly useful for "Fermat numbers" of the form Fn = 2^(2^n) + 1, where it suffices to check if 3^x ≡ -1 (mod Fn) with x = 2^(t-1).

4. **Tests for n = k 2^t −1 (Lucas-Lehmer Test)**: The Lucas-Lehmer test is used to prove primality of Mersenne numbers, i.e., numbers of the form n = 2^p - 1 where p is prime. It states that if H_(2^p - 2) ≡ 0 (mod n), then n is prime, where H is a sequence defined recursively as H_0 = 1, H_1 = 2 and H_k = 4H_(k-1) - H_(k-2). The test can be efficiently computed using the index doubling formula.

5. **Lucas Test**: This generalization of the Lucas-Lehmer test applies to numbers n = k*2^t - 1 where k is odd, 2^t > k, and k ≠ 0 (mod 3), ensuring that n ≡ 1 (mod 3). If H_(n+1)/4 ≡ 0 (mod n), then n is prime. The sequence H can be computed using the index doubling formula, similar to the Lucas-Lehmer test.

Each of these methods has its own strengths and limitations, and their applicability depends on the specific form of the number being tested for primality. These tests build upon the properties of modular arithmetic and number theory, particularly concerning orders of elements in modular rings and cyclotomic polynomials.


Title: Complex Modulus: The Field GF(p^2)

This section discusses the concept of complex modulus in the context of finite fields, specifically GF(p^2), where p is a prime number. It explores the construction and properties of this field, focusing on efficient reduction methods for certain quadratic polynomials.

1. **Construction of Complex Numbers:** The standard way to construct complex numbers involves taking pairs of real numbers (a, b) = a + bi, with component-wise addition and multiplication defined by:
   - Addition: (a, b) + (c, d) = (a + c, b + d)
   - Multiplication: (a, b)(c, d) = (ac - bd, ad + bc)

   This construction forms a field.

2. **Complex Finite Fields GF(p^2):** When the ground field is the integers modulo a prime p (GF(p)), and an irreducible polynomial c(x) of degree n with coefficients in GF(p), we obtain an extension field GF(p^n). For binary fields, GF(2^n) is treated in Chapter 42.

3. **Irreducible Polynomials:** The construction depends on the choice of irreducible polynomial c(x). In particular, for primes p = 4k + 3 (where -1 is a quadratic non-residue), x^2 + 1 is irreducible, leading to GF(p^2) being denoted by GF((4k+3)^2). For p = 4k + 1, the polynomial x^2 + x + 1 can be used instead.

4. **Multiplication Efficiency:** Certain quadratic polynomials allow for efficient multiplication methods that require fewer scalar multiplications and additions than standard multiplication algorithms. These include:
   - C = x^2 + 1 (reducible modulo p = 4k + 3): Multiplication costs three real multiplications and five real additions.
   - C = x^2 + d (irreducible if −d is not a square): Multiplication also costs three scalar multiplications when multiplication by d is cheap.

5. **Primitive Roots:** For primes p with the lowest k bits set, the maximal order in GF(p^2) equals N = 2^(k+1). An algorithm exists for constructing primitive 2^j-th roots in GF(p^2) for j = 2, 3, ..., a (where 2a is the largest power of 2 dividing p^2 - 1), outlined in [149].

6. **Mersenne Primes:** For Mersenne primes p = 2^e - 1, an element of order 2^(e+1) (in GF(p^2) with field polynomial x^2 + 1) can be constructed more directly by computing √-3 and 1/√2 without modular reduction.

In summary, this section introduces the concept of complex modulus in finite fields, specifically GF(p^2), and discusses efficient multiplication methods for certain quadratic polynomials. It also provides algorithms for constructing primitive roots within these fields, with special consideration given to Mersenne primes.


The text discusses methods to solve the Pell equation, which is a Diophantine equation of the form x^2 - D*y^2 = ±1, where D is a positive nonsquare integer. The solutions are sought for integers x and y.

**39.13.1 Solution via Continued Fractions:**

This method uses simple continued fractions to find the solutions of the Pell equation (x^2 - D*y^2 = +1) and similar equations with negative signs. The key idea is that the convergents of the continued fraction of √D are close approximations to √D, and their squares (P_k^2 - D*Q_k^2) give the values e_k.

For a given D, the continued fraction of √D can be calculated using an efficient algorithm for square roots (mentioned but not detailed in the text). Once we have the convergents P_k/Q_k and their corresponding e_k = P_k^2 - D*Q_k^2, we look for the smallest non-trivial k such that e_k equals +1 or -1.

For example, if D = 53:
- CF(√53) = [7, 3, 1, 1, 3, 14, ...] (with period length 6),
- The smallest non-trivial solution for x^2 - 53*y^2 = +1 is at k=9: P_8^2 - 53*Q_8^2 = +1.

This method works for all nonsquare positive integers D, and it's particularly efficient when D is a prime of the form 4k+1 or has no factors of the form 4k+3.

**39.13.2 Multiplying and Powering Solutions:**

Given two solutions (x, y) and (r, s) to Pell equations with signs e and f:
- x^2 - D*y^2 = e,
- r^2 - D*s^2 = f,

we can derive new solutions by multiplying these pairs:
- (U, V) := (x*r + D*y*s, x*s + y*r), which is also a solution with sign ef.

We can also raise solutions to any power using matrix exponentiation. Define the matrix M = [x, D*y; y, x] and its k-th power M^k. Then the k-th power of (x, y) is given by (X_k, Y_k) = M^k * [1; 0].

For example, if r^2 - D*s^2 = +1 (the smallest nontrivial solution), then x_k^2 - D*y_k^2 = e for all odd k. Moreover, if r^2 - D*s^2 = -1, then x_k^2 - D*y_k^2 = -1 for all odd k.

This method allows us to generate an infinite sequence of solutions from a single pair by multiplication and matrix exponentiation, providing a systematic way to find higher-index solutions to the Pell equation.


The text discusses various aspects related to hypercomplex numbers, specifically focusing on algebras, multiplication tables, and the Cayley-Dickson construction. Here's a detailed summary and explanation:

1. **Algebra**: An algebra is an n-dimensional vector space (over a field) equipped with component-wise addition and a multiplication table that defines the product of any two components. The product of elements x = ∑αk ek and y = ∑βj ej is given by x · y = ∑k,j [(αk · βj) mk,j], where mk,j = ek ej are arbitrary elements from the algebra (potentially linear combinations of components ei).

2. **Multiplication Table**: A multiplication table for an algebra can be arbitrary, but in this context, we're interested in algebras over the real numbers where the product of two components equals ±1 times another component. For example, complex numbers are a 2-dimensional algebra with the familiar multiplication table:

   0   1
   0: +0  +1
   1: +1 -0

3. **Cayley-Dickson Construction**: This construction recursively defines multiplication tables for certain algebras where the dimension is a power of 2. Given elements a, A, b, and B from a (2n−1)-dimensional algebra U, define the multiplication rule for an algebra V (of dimension 2n) as:

   (a, b) · (A, B) := (a · A - B · b*, a* · B + A · b), where b* is the conjugate of b.

   The construction ensures that ek· ej = ±ek for any two nonzero components k and j (with k ≠ j).

4. **Signs in Multiplication Table**: Figures 39.14-A and B illustrate multiplication tables and sign patterns for sedenions, an example of a 16-dimensional algebra obtained via the Cayley-Dickson construction. The multiplication table exhibits partial antisymmetry, meaning ek· ej = -ej· ek whenever both k and j are nonzero (and k ≠ j).

5. **Fast Quaternion Multiplication**: The text mentions a method for multiplying quaternions using the dyadic convolution scheme with eight real multiplications. This involves initializing four temporary results (c0, c1, c2, c3), performing a length-4 dyadic convolution, and then correcting the results through normalization and additional multiplications.

6. **Eight-Square Identity**: This identity relates to matrices C, A, and B as shown in Figure 39.14-F. The sum of products of squared components from vectors A and B equals the sum of squared components of vector P (obtained by matrix multiplication CA).

   Σk=0^(n-1) Ak^2 · Σk=0^(n-1) Bk^2 = Σk=0^(n-1) Pk^2

This identity is useful in simplifying certain mathematical expressions and algorithms involving squared vectors or matrices.


The text discusses various arithmetic operations with Binary Polynomials, which are polynomials with coefficients in the finite field GF(2) (also known as Z/2Z). Here's a detailed summary and explanation of the key points:

1. **Basic Arithmetic Operations:**

   - Addition is performed using the XOR operation, identical to regular polynomial addition but with coefficients reduced modulo 2.
   - Subtraction is also equivalent to addition in this context.
   - Multiplication by x (the independent variable) is a left shift operation, similar to integer multiplication but without carry.

2. **Multiplication and Squaring:**

   - Multiplication of two binary polynomials A and B is identical to usual polynomial multiplication except that no carry occurs. The provided code snippet performs this operation using bitwise operations.
   - Squaring a binary polynomial involves shifting the bits from position k to position 2k, effectively doubling each exponent.

3. **Optimization:**

   - The routines for squaring and multiplication can be optimized by partially unrolling loops to avoid branches, which speeds up execution.
   - A bit-zip function (bit_zip0) can also be used for squaring, provided that the upper half of the bits of the argument are zero.

4. **Exponentiation:**

   - Binary exponentiation is implemented using a loop that repeatedly squares the base and multiplies when necessary. Overflow may occur for large exponents.

5. **Quotient and Remainder:**

   - The remainder (R) of A modulo B can be computed by initializing A = a, then subtracting B multiplied by the highest power of x less than or equal to deg(A). This process continues until deg(B) > deg(A).
   - The quotient (Q) is computed similarly but without discarding the remainder.

6. **Greatest Common Divisor (GCD):**

   - The binary GCD algorithm uses a loop to repeatedly reduce both numbers by their highest power of 2 until they become odd, then applies a series of shifts and subtractions to find the GCD.

7. **Exact Division:**

   - This method leverages the relation C = (1 + Y)(1 + Y^2)...(1 + Y^(2n)) mod x^(2n+1) for binary polynomials C with constant term 1. It uses shifts and subtractions to compute R = A/C when A is an exact multiple of C, making it efficient when the number of nonzero coefficients in C-1 (k) is small.

The primary application of these operations is in the study and implementation of Linear Feedback Shift Registers (LFSRs), which will be covered in Chapter 41. The binary polynomial arithmetic forms the foundation for computations within binary finite fields, as discussed in Chapter 42.


The text discusses various algorithms for performing arithmetic operations with binary polynomials over the field GF(2), where arithmetic is performed using XOR (exclusive or) for addition and subtraction, and a bitwise reduction for multiplication. Here's a detailed summary and explanation of the key points:

1. **Binary Polynomial Representation**: Binary polynomials are represented as a sum of powers of x, where each coefficient is either 0 or 1. For example, A = a3*x^3 + a2*x^2 + a1*x + a0.

2. **Bitwise Operations for Arithmetic**:
   - Addition and subtraction are performed using XOR (⊕): A ⊕ B = C (i.e., a ⊕ b = c).
   - Multiplication is performed by shifting left and subtracting the modulus if the shifted-out bit is 1, utilizing an auxiliary variable 'h' representing the highest set bit of the modulus.

3. **Multiplication Routine**: The multiplication routine `bitpolmod_mult(a, b, c, h)` computes (A * B) mod C by repeatedly performing XOR and shifting operations based on the bits of 'b'.

4. **Squaring Optimization**: Squaring can be optimized using a precomputed table of residues x^2k mod C for various k, reducing the number of multiplications required.

5. **Exponentiation Algorithms**:
   - Right-to-left powering algorithm (`bitpolmod_power(a, e, c, h)`) uses repeated squaring and multiplication to compute (A ^ e) mod C.
   - Left-to-right powering algorithm provides an alternative approach using a different loop structure.

6. **Division by x**: Division by x is possible if the modulus has a nonzero constant term, allowing for efficient computation of (A / x) mod C using bitwise operations. The inverse of x can also be computed efficiently under this condition.

7. **Extended GCD (EGCD) Method**: The method to compute EGCD (bitpol_egcd(u, v, iu, iv)) remains the same as in section 39.1.4 on page 767, using a standard Euclidean algorithm with bitwise operations for polynomial division and coefficient updates.

8. **High-degree Polynomials**: For very high-degree binary polynomials, FFT-based methods can be used, but simpler splitting schemes (Karatsuba, Toom-Cook) are generally more efficient unless the cost of multiplication is significantly higher than addition.

9. **Toom-Cook Algorithms for Binary Polynomials**: These algorithms use only constants 0 and 1 in their splitting schemes, making them suitable for binary polynomial arithmetic over GF(2). The 3-way and 4-way splitting methods are provided for multiplication of binary polynomials with degrees 3N and 4N, respectively.

These techniques enable efficient computation with binary polynomials, particularly when working with high-degree polynomials where the naive O(N^2) approach becomes impractical due to its quadratic time complexity.


The text discusses various aspects related to binary polynomials, focusing on irreducible and primitive polynomials, their properties, and methods for testing their irreducibility and primitivity.

1. **Binary Polynomials**: These are polynomials with coefficients in the binary field GF(2), where addition is equivalent to XOR, and multiplication is equivalent to AND or bitwise shift operations.

2. **Irreducible Polynomials**: A polynomial is irreducible if it has no nontrivial factors (factors other than 1 and itself). For binary polynomials, having a nonzero constant term makes the polynomial reducible because x is always a factor. Similarly, if the number of nonzero coefficients (of odd degree) is even for binary polynomials, they are reducible due to the presence of the factor x + 1.

3. **Testing for Irreducibility**:
   - **Ben-Or Test**: This test checks if gcd(x^(2^k)-x mod C, C) ≠ 1 for any k < d, where d is the degree of the polynomial. It suffices to check only the first ⌊d/2⌋ tests because a factor of degree f implies another one of degree d-f.
   - **Rabin's Test**: This test checks two conditions: x^(2^d) ≡ x (mod C) and, for all prime divisors pi of d, gcd(x^(2^d/pi)-x mod C, C) = 1. The test involves GCD computations and squarings to update the power of x.
   - **Strong Pseudo-Irreducible (SPI) Test**: This test checks if a polynomial has no linear factors, x^(2^k) ≠ x for all k < d, and x^(2^d) = x without needing GCD computations.

4. **Primitive Polynomials**: A polynomial is primitive if its order (period) is maximal, which means the powers of x generate all nonzero binary polynomials of degree less than the polynomial's degree. Primitivity implies irreducibility but not vice versa.

5. **Testing for Primitivity**: The text mentions an algorithm to determine whether a given irreducible binary polynomial is primitive by computing the order of its root using modular exponentiation. The provided GP language code and C++ implementations demonstrate this method.

6. **Möbius Function (µ)**: This function appears in the formula for calculating the number of irreducible polynomials of degree n, which involves summing over all divisors d of n weighted by µ(n/d). The Möbius function is defined via a specific recurrence relation.

The text also includes tables and figures displaying the number of irreducible and primitive binary polynomials up to degrees 40. These counts are essential in various applications, such as coding theory and finite field arithmetic.


This text discusses various aspects of binary polynomials, focusing on irreducible, primitive, self-reciprocal, and trinomial polynomials. Here's a detailed summary:

1. **Lyndon Words and Irreducible Polynomials:**
   - Lyndon words are non-increasing sequences of symbols that are strictly smaller than any of their proper prefixes. The formula for the number of Lyndon words (In) is given as In = 2^(n-2)/n when n is prime. This relationship is based on relation 18.3-2 from page 380, and the sequence is entry A001037 in [312].
   - For large degrees n, the probability that a randomly chosen polynomial is irreducible is about 1/n. However, for polynomials in two or more variables, this probability tends to 1 as n increases.

2. **Primitive Binary Polynomials:**
   - The number of primitive binary polynomials (Pn) of degree n is given by Pn = ϕ(2^n - 1)/n, where ϕ denotes Euler's totient function. If n is the exponent of a Mersenne prime, then Pn equals In (the number of irreducible polynomials).
   - Values for Pn up to degree 40 are provided in Figure 40.6-B and are entry A011260 in [312].

3. **Irreducible Non-Primitive Polynomials:**
   - The difference Dn = In - Pn represents the number of irreducible non-primitive polynomials. If n is a Mersenne prime exponent, then Dn equals 0. A list of these polynomials up to degree 12 is provided in [FXT: data/all-nonprim-irredpoly.txt].

4. **Transformations Preserving Irreducibility:**
   - Reciprocal Polynomial: The reciprocal of an irreducible polynomial remains irreducible, with the order preserved. The reciprocal can be calculated using bit manipulation operations.
   - Composition with x + 1: If a polynomial p(x) is irreducible, then p(x + 1) (the composition with x + 1) is also irreducible, although this transformation does not necessarily preserve the order of the polynomial.

5. **Self-Reciprocal Polynomials:**
   - Self-reciprocal polynomials are those that are equal to their own reciprocals. These can be generated from irreducible polynomials with nonzero linear coefficients using specific transformations. The self-reciprocal polynomials of degree 2n are factors of x^(2^n) + 1, and the order of a self-reciprocal polynomial of degree 2n is a divisor of 2^n + 1.

6. **Irreducible and Primitive Polynomials of Special Forms:**
   - The text provides lists of irreducible and primitive polynomials for low degrees (up to 11) and trinomials (polynomials with exactly three non-zero coefficients) for even lower degrees (up to 49). These lists are available in the associated data files mentioned.

7. **Notation:**
   - PP: Primitive Polynomial
   - SRP: Self-Reciprocal Polynomial
   - µ(d): Möbius function, a multiplicative function in number theory used to count the square-free integers up to a given integer.


The text discusses various types of irreducible polynomials over the Galois Field GF(2), which consists of two elements, 0 and 1. These polynomials have applications in error-correcting codes, cryptography, and digital signal processing. Here's a detailed explanation of the topics discussed:

1. **Irreducible Trinomials**: A trinomial is an expression with three terms. In this context, we're looking at polynomials of the form `x^n + x^k + 1`, where `n` and `k` are integers. The text provides a list of irreducible trinomials for degrees up to 49. It also mentions Swan's theorem regarding conditions under which such trinomials are reducible.

2. **Primitive Trinomials**: A primitive polynomial is an irreducible polynomial whose roots generate the multiplicative group of the finite field it defines. In other words, all non-zero elements in the field can be expressed as a power of a root of the polynomial. The text provides lists of primitive trinomials for different degrees and mentions that no irreducible trinomial exists for primes `n ≡ 13 (mod 24)` or `n ≡ 19 (mod 24)`.

3. **Irreducible Pentanomials**: A pentanomial is a polynomial with exactly five non-zero coefficients. The text mentions that irreducible primitive pentanomials exist for all degrees `n >= 5`, but this has not been proven yet. It provides examples of such polynomials for small degrees.

4. **Primitive Minimum-Weight and Low-Bit Polynomials**: These are special types of primitive polynomials with certain properties related to their weights (sum of absolute values of coefficients) and the positions of non-zero coefficients. The text provides lists and descriptions of such polynomials for different conditions.

5. **All Primitive Low-Bit Polynomials for Certain Degrees**: These are primitive polynomials where only a few bits (coefficients) are set to 1, and these bits are as low as possible in the representation of the degree. The text provides lists for specific degrees, such as 256.

6. **Primitive Low-Block Polynomials**: A low-block polynomial has a special form where most coefficients are zero except for a block of consecutive non-zero coefficients at the end. These polynomials exist for many degrees and are easy to store in an array due to their structure.

7. **Irreducible All-Ones Polynomials**: An all-ones polynomial is one where every coefficient (except possibly the leading one) is 1. These polynomials are irreducible when the degree plus one is a prime number for which 2 is a primitive root. The text provides a list of such primes and degrees up to 400 where these polynomials are irreducible.

8. **Irreducible Alternating Polynomials**: These are polynomials of the form `1 + x + x^3 + ... + x^(2d+1)`, which can be irreducible only when `d` is odd. The text provides a list up to degree 1000.

9. **Primitive Polynomials with Uniformly Distributed Coefficients**: These are primitive polynomials where the coefficients are (roughly) equally spaced. Lists for degrees from 9 to 660 are provided in [278].

10. **Irreducible Self-Reciprocal Polynomials**: A self-reciprocal polynomial is one that remains unchanged when its coefficients are reversed and `n` (the degree) is replaced by `(n+1)/2`. The text provides a list of all irreducible self-reciprocal polynomials up to degree 22.

11. **Generating Irreducible Polynomials from Lyndon Words**: This section discusses an algorithm that generates irreducible polynomials from Lyndon words, which are special types of strings with certain properties. The algorithm uses a primitive polynomial and a generator to construct a new polynomial based on powers of the generator modulo the given polynomial.

The text also mentions various files containing lists of these polynomials for different conditions, such as [FXT: data/all-trinomial-primpoly.txt], [FXT: data/pentanomial-primpoly.txt], etc., which can be found in the referenced sources.


Linear Feedback Shift Registers (LFSR) are a type of shift register used for generating pseudorandom sequences, particularly in digital systems. They operate based on the principles of modular arithmetic with binary polynomials, as discussed in section 40.3. 

The fundamental structure of an LFSR consists of a shift register with feedback connections. The register contains bits (usually denoted as 'w') that shift leftward with each clock pulse. The rightmost bit is typically set to zero during initialization. 

In an LFSR, the feedback connection involves combining (or "tapping") certain bits from the shift register and feeding them back into the input. This combination is performed using exclusive-OR (XOR) operations, symbolized by the plus sign (+). The tap positions and the coefficients used in this process define the specific type of LFSR. 

The crucial component of an LFSR is its characteristic polynomial, represented as c = 1 + aT + a^2 T^2 + ... + a^k T^k, where 'a' represents the taps (feedback positions), and 'T' signifies the left shift operator. For instance, in Fig. 41.1, the degree of the polynomial is 4, with coefficients c = {1, 0, 1, 1}, corresponding to tap positions at bits 1, 2, and 4 from the right. 

The sequence generated by an LFSR repeats after a certain number of steps, known as its period. The maximum possible length for an LFSR of degree 'n' is 2^n - 1. Sequences with this maximal length are called m-sequences and exhibit optimal statistical properties. 

LFSRs are versatile in their applications, including random number generation, error detection (through Cyclic Redundancy Check, CRC), spread spectrum communication protocols, and hardware testing. Their simplicity allows for efficient hardware implementation, making them popular choices in digital systems where pseudorandom sequences are required.


This text discusses Linear Feedback Shift Registers (LFSRs), a mechanism used to generate shift register sequences (SRS). LFSRs are based on the principle of shifting bits and conditionally feeding back certain bits, determined by a primitive polynomial C. This polynomial is also known as the connection polynomial.

1. **Linear Feedback Shift Register (LFSR) Basics**:
   - An SRS is generated by computing Ak = x^k mod C for k = 0, 1, ..., 2n-1 and setting bit k of the SRS to the least significant bit of Ak. 
   - If C is a primitive polynomial of degree n, the SRS will contain all nonzero words of length n and will also cycle through all nonzero words when a word W is updated by left shifting and adding the bit of the SRS.

2. **Efficient Generation of SRS**:
   - A simple way to generate an SRS is by repeatedly dividing (XOR'ing) by x, shifting left, and conditionally XOR-ing with C.

3. **LFSR Implementation in C++**:
   - The provided code shows a class `lfsr` that implements the LFSR mechanism, generating a shift register sequence based on a primitive polynomial of degree n. The period of the SRS is 2^n - 1.

4. **Galois and Fibonacci Setup**:
   - Galois setup involves shifting left and conditionally XOR-ing with C if the shifted out bit is 1.
   - Fibonacci setup involves shifting right and conditionally XOR-ing with a shifted version of C (C >> 1) if the rightmost bit was originally 1.

5. **Cyclic Redundancy Check (CRC)**:
   - CRCs are hash functions that generate binary words of fixed length as hash values. They are used for error detection in data transmission and storage.
   - The CRC function computes h = s mod c, where s is the binary polynomial corresponding to the input sequence and c is a binary primitive polynomial.
   - A C++ implementation of 64-bit CRC (class `crc64`) is provided, with methods for initializing, resetting, setting internal state, feeding bits or bytes into the CRC, and computing the CRC value.

6. **Optimization via Lookup Tables**:
   - To speed up CRC computation, especially for larger word sizes, lookup tables can be used. These tables store precomputed values of specific shifts and XOR operations with C. The size of the table is 2^n (for n bits), and its entries are computed during initialization based on the polynomial C.

In summary, this text explains the Linear Feedback Shift Register mechanism, its implementation in C++, and the use of CRCs for error detection. It also discusses how lookup tables can optimize CRC computation by precomputing shift and XOR operations with the primitive polynomial.


The text discusses various aspects of shift register sequences, focusing on Linear Feedback Shift Registers (LFSRs), Feedback Carry Shift Registers (FCSRs), and their applications in generating cyclic redundancy checks (CRCs) and De Bruijn Sequences (DBS).

1. **Linear Feedback Shift Registers (LFSRs):**
   - LFSRs are used for generating pseudo-random sequences based on a linear recurrence relation. The generation is done using a primitive polynomial of degree n, which ensures the maximal length sequence.
   - The primitive polynomial's coefficients determine the feedback connections in the register. For example, with a 4-bit register and primitive polynomial x^4 + x + 1 (or 1001), bits are fed into the leftmost position using XOR operations based on the polynomial's coefficients.
   - The text presents an optimized routine for CRC computation using lookup tables in LFSRs, significantly improving speed when table sizes of 16 or 256 words are used.

2. **Parallel CRCs:**
   - A more efficient method for checksumming is computing CRCs for each bit of the input word in parallel. The 'pcrc64' class example demonstrates this approach using an array of 64 words, with a Fibonacci setup and the pentanomial x^64 + x^4 + x^3 + x^2 + 1 as the primitive polynomial.

3. **Generating all revbin pairs:**
   - The text explains how to generate all nonzero revbin (reflected binary) pairs using an LFSR with a primitive polynomial c and its reciprocal cr. This is achieved by computing the next pair using a function 'revbin_next'.

4. **The number of m-sequences and De Bruijn sequences:**
   - An m-sequence (maximal length sequence) is generated when the LFSR's polynomial is primitive. The total number of such sequences equals Pn = ϕ(2^n − 1)/n, where ϕ denotes Euler's totient function.
   - De Bruijn Sequences (DBS) are obtained by inserting a single zero in the longest run of zeros from an m-sequence. The number of DBSs for given n is Sn = 2^(2n−1 − n).

5. **Auto-correlation of m-sequences:**
   - An auto-correlation function (ACF) is defined for a sequence M, where Sk = +1 if Mk = 1 and -1 otherwise. For an m-sequence, this ACF has Cτ = L (length of the sequence) when τ = 0 and -1 for other values of τ. Most truncated De Bruijn sequences do not have this property.
   - Specific sequences constructed with primes q can have an auto-correlation satisfying Cτ = L−1 if τ = 0 and −1 otherwise. These sequences can be used in Hadamard matrix construction (Chapter 19).

6. **Feedback Carry Shift Registers (FCSRs):**
   - FCSRs are the modulo counterpart of LFSRs, using a prime c with primitive root 2. The powers of 2 modulo c run through all nonzero values less than c.
   - An FCSR's internal state (a) and output word (w) evolve according to the provided code snippet, with w not necessarily covering all possible values < c but rather a subset of c−1 distinct values < 2^(word_size).

The text provides insights into various applications of shift register sequences in error-correcting codes, pseudorandom number generation, and combinatorial sequence construction. It also offers code snippets for implementing LFSRs and FCSRs in C++.


Linear Hybrid Cellular Automata (LHCA) are a type of one-dimensional cellular automaton where two different rules are applied based on the position of each cell. The next state of an LHCA is determined by Rule 150 for cells where a certain rule condition (r) is met, and Rule 90 otherwise. This is implemented in a branch-free manner, making hardware implementation straightforward.

The rules are defined by examining the eight possible states of a cell with its neighbors and interpreting the resulting pattern as a binary number. For example, Rule 90 corresponds to the binary number 01011010 (90 in decimal), while Rule 150 corresponds to 10010110 (150 in decimal).

The computation of the next state with an LHCA can be summarized as follows:

1. Compute `r & x` (bitwise AND between rule and current cell state).
2. Compute `t = (x >> 1) ^ (x << 1)` (bit-shift operations on current cell state).
3. Apply the XOR operation with the result of step 1: `t ^= r`.
4. Perform bitwise AND with the mask `m`: `t &= m`.
5. Return the resulting value as the next state.

The length of the automaton is defined by `m`, which must be a burst of the n lowest bits (n being the length of the automaton).

Certain rule vectors `r` can lead to maximal periods in LHCA, where all nonzero values occur for m = 2^n - 1. Minimum-weight rules that result in maximal period are listed and can be found in [FXT: bpol/lhcarule-minweight.cc].

LHCA rules can be converted to binary polynomials using a recursive formula, where `pk` is calculated based on previous values of `pk−1` and `pk−2`, with the rule bits (`ri`) taken into account. This process generates a polynomial of degree `n`.

Additive Linear Hybrid Cellular Automata (ALHCA) are a broader category that includes LHCA, where the next state of any two words `a` and `b` satisfies `N(a) + N(b) = N(a + b)`. The action of such an automaton can be described by a matrix over GF(2), with the binary polynomial corresponding to the automaton being the characteristic polynomial of that matrix.

The conversion from ALHCA rules to binary polynomials involves determining the matrix whose k-th row corresponds to `N(ek)`, where `ek` is a word with only bit `k` set. The resulting polynomial can be computed using the characteristic polynomial algorithm for matrices over GF(2). This method generalizes the LHCA conversion process described earlier, allowing for the analysis of various additive cellular automata beyond just LHCAs.


This text discusses the binary finite fields GF(2^n), focusing on their arithmetic properties, particularly for n not greater than BITS_PER_LONG (a constant defining the maximum number of bits that can be represented by a long integer). The following points summarize the key details and explanations:

1. **Field Representation**: In binary finite fields GF(2^n), elements are polynomials modulo an irreducible polynomial C with coefficients in the ground field GF(2). The arithmetic is polynomial arithmetic modulo C. Different irreducible polynomials of degree n represent the same 42.1: Arithmetic and basic properties 893 field, making them isomorphic to each other.

2. **Characteristic**: The characteristic of GF(2^n) is 2. Two elements are identified if their difference is a multiple of 2 (equivalently, adding any element p times results in zero).

3. **Linear Functions**: Linear functions f(x) in GF(2^n) have the form ∑(0 ≤ k < n) uk * x^(2k), where u_k are elements from the ground field GF(2). These linear functions can be computed using lookup tables, known as square-and-multiply algorithms.

4. **Squaring**: Squaring (and raising to any power 2^k) is a linear operation in GF(2^n), meaning that the square of an element can be calculated efficiently by precomputing and utilizing the powers of x modulo C.

5. **Trace Function**: The trace Tr(a) of an element a in GF(2^n) is defined as the sum of its powers: Tr(a) = ∑(0 ≤ j < n) a^(2j). The trace function is linear and can be computed using lookup tables based on a precomputed trace vector tv.

6. **Inverse and Square Root**:
   - The inverse of a nonzero element a in GF(2^n) is given by a^(Q-2), where Q = 2^n.
   - Every element has a unique square root s, which can be computed using an efficient lookup table method with precomputed √x values.

7. **Order and Primitive Roots**: The order of an element in GF(2^n) is the smallest positive exponent r such that a^r = 1. An element of maximal order (2^n - 1) is called a generator or primitive root, as its powers generate all nonzero elements in the field.

8. **Implementation**: A C++ class GF2n provides an implementation for computations in GF(2^n), including constructors to create objects from integers and other types, initialization methods, and arithmetic operations like addition, multiplication, squaring, trace calculation, and finding the inverse or square root of elements. The irreducible polynomial is chosen by default, but can be specified if needed.

In summary, this text presents the basics of working with binary finite fields GF(2^n), including their representation as polynomials modulo an irreducible polynomial, arithmetic operations, and efficient methods for tasks like squaring, computing traces, inverses, and square roots using lookup tables. The provided C++ class GF2n serves as a practical tool to perform computations within these fields, taking advantage of precomputed data structures (such as trace vectors) to optimize performance.


The text discusses the topic of binary finite fields, specifically GF(2^n), and provides information about various aspects such as generators, minimal polynomials, trace vectors, and solving quadratic equations within this field. Here's a detailed summary and explanation:

1. **Generators**: The document introduces two examples of generating elements (g) in GF(2^4). These are visualized using tables showing the powers of g under both primitive and non-primitive polynomial moduli. The class GF2n provides operations like addition, subtraction, multiplication, division, inverse calculation, powering, order determination, and trace computation for these elements.

2. **Minimal Polynomials**: A minimal polynomial of an element in GF(2^n) is defined as the lowest-degree polynomial with that element as a root. The algorithm to compute this polynomial involves finding the smallest positive integer r such that a^(2^r) = a, then constructing a product of binomial factors (x - a^(2^k)) for k from 0 to r-1. This result is guaranteed to have coefficients in GF(2^n), but they will be binary (either 0 or 1).

3. **Fast Computation of Trace Vector**: The trace vector, which records the trace (sum of elements in GF(2^n)) for each power of a generator element, can be computed using two methods:

   - **Newton's Formula**: This method uses recursion to compute trace values based on previously calculated ones and coefficients of the irreducible polynomial. It does not involve any polynomial modular reductions, making it efficient even for calculating just one trace.
   
   - **Division of Power Series**: This variant of the algorithm treats trace vector computation as a division operation between two power series. While conceptually elegant, its practical efficiency depends on the method used for high-order multiplication or division of power series, which should ideally be FFT-based methods for large n.

4. **Properties of Trace Vector**: Certain properties of the trace vector are discussed:
   - For binary polynomials of odd degree with nonzero coefficients only at odd indices, t0 = 1 and all other ti are zero unless i equals a multiple of n.
   - For even-degree binary polynomials with nonzero odd coefficients only at positions less than halfway through the degree, only tn−i components are non-zero where i < n/2.

5. **Solving Quadratic Equations**: The document points out that while extracting a square root in GF(2^n) is straightforward, solving quadratic equations (ax^2 + bx + c = 0) is not as simple due to the lack of a standard division operation by 2. Therefore, standard quadratic solution formulas from real or complex number algebra do not apply directly in this context.

Throughout these discussions, the focus is on binary finite fields (GF(2^n)), with special attention given to the structure and properties that arise due to their binary nature. This includes unique characteristics of generators, minimal polynomials, trace vectors, and the challenges presented when attempting to apply familiar algebraic operations or formulas from other number systems within this binary field context.


This text discusses the representation of elements in a binary finite field GF(2^n) using normal bases. Normal bases offer an alternative way to represent elements, which can simplify certain arithmetic operations. Here's a detailed summary and explanation:

1. **Normal Basis Representation**: An element `a` in GF(2^n) can be represented as a sum of powers of a normal basis, i.e., `a = ∑_{k=0}^{n-1} ak x^(2k)`. This is different from the standard representation where elements are represented as sums of powers of the generator 'x'.

2. **Multiplication Algorithm**: Multiplication in normal basis can be done using a multiplication matrix M. Given two elements `a` and `b`, their product `p = a · b` can be computed by performing dot products between cyclically shifted versions of `a` and `b` with the matrix M.

3. **Multiplication Matrix**: The multiplication matrix M is derived from the companion matrix C of the field polynomial (which is irreducible). To find M, we compute matrices A and D, where:
   - A has rows equal to x^(2k) mod C,
   - D = A * CT * A^(-1),
   - M's entries Mi,j are given by D_j',i' where i' = -i mod n and j' = j - i mod n.

4. **Normal Polynomial Test**: A polynomial c is normal if the nullspace of the matrix whose k-th row equals x^(2k) mod c is empty. This means that the roots of c are linearly independent.

5. **Solving Reduced Quadratic Equation**: The reduced quadratic equation x^2 + x = f has two solutions if Tr(f) = 0 (i.e., the number of ones in the normal representation of f is even). One solution can be computed using the inverse of the reversed Gray code of f.

6. **Number of Binary Normal Bases**: The text also provides a table showing the number of binary normal bases for different values of n, which grows rapidly with n.

The use of normal bases simplifies multiplication and trace computation but may complicate other operations like addition and exponentiation. They are particularly beneficial in hardware implementations due to their regular structure, making them suitable for pipelined arithmetic circuits.


Title: Summary of Normal Bases in GF(2^n)

Normal bases are a special type of basis for the finite field GF(2^n), which play a significant role in various areas of mathematics, including coding theory and cryptography. This summary focuses on key aspects related to normal bases in binary finite fields (GF(2^n)), their properties, computation methods, and related concepts like dual bases and self-dual bases.

1. **Normal Bases**: A normal basis for GF(2^n) is a basis where the minimal polynomial of each element is a normal polynomial—an irreducible polynomial that divides x^(2^n) - 1 but not its proper factors. Normal bases have several desirable properties, such as fast multiplication algorithms and simple field arithmetic.

2. **Number of Normal Bases (An)**: The number of degree-n binary normal polynomials is denoted by An. This sequence is A027362 in the Online Encyclopedia of Integer Sequences ([312]). No explicit formula for An exists, but it can be computed using factorizations or efficient algorithms.

3. **Computation Methods**:
   - Exhaustive search: For small degrees (n < 25), normal polynomials can be generated by selecting irreducible polynomials that are also normal. Using the Lyndon word-based mechanism for generating all irreducible polynomials, the computation is relatively fast.
   - Invertible circulant matrices: The number of binary normal bases equals the number of invertible circulant n×n matrices over GF(2). Circulant matrices can be constructed from Lyndon words with odd weight, and their invertibility is determined by testing coprimality with x^n - 1.

4. **De Bruijn Graph Connection**: Interestingly, the number of degree-n binary normal polynomials (An) equals the number of cycles in the De Bruijn graph B(2, n), which are binary sequences of length 2^n without repeated subsequences of length less than n.

5. **Dual and Self-dual Bases**: For a basis A = {a0, a1, ..., an−1}, a dual (or complementary) basis B = {b0, b1, ..., bn−1} satisfies Tr(ak bj) = δk,j for 0 ≤ k, j < n. Self-dual bases are normal bases that are also their own duals.

   - To check if the roots of an irreducible polynomial C form a self-dual basis: compute its trace polynomial T (using gf2n_xx2k_trace), and test if T = 1 mod x^n - 1 using bitpol_normal2_q.
   - Computing dual bases involves finding the minimal polynomial CS of the dual (normal) basis β, which can be done with gf2n_dual_normal routine.

6. **Self-dual Normal Bases**: A self-dual normal basis is a normal basis that's also its own dual. The number of such bases for GF(2^n), denoted Sn (and Zn if the field polynomial is primitive), forms sequences A135488 and A135498 in [312]. No formula exists for these values, but they can be computed using specialized algorithms like the one provided by Max Alekseyev.

In conclusion, normal bases are a vital concept in GF(2^n) with deep connections to combinatorics (De Bruijn graphs), linear algebra (circulant matrices), and finite field theory (dual and self-dual bases). Although no explicit formula exists for the number of normal or self-dual normal bases, efficient algorithms allow their computation up to relatively large degrees. Understanding these concepts is crucial for applications in various areas of mathematics and computer science, including coding theory, cryptography, and algorithmic algebra.


The text discusses various types of normal bases for binary finite fields GF(2^n), which are crucial for efficient computations within these fields. Here's a detailed summary:

1. **Self-Dual Normal Basis (SDNB)**:
   - No self-dual normal basis exists when n is a multiple of 4.
   - The function `sdn(m,p)` calculates the number of distinct self-dual normal bases for GF(p^m) over GF(p), where p is prime.
   - If m is divisible by p, it uses a different formula; otherwise, it factors (x^m - 1)/(x - 1) mod p and iterates through the factors to find self-dual pairs.

2. **Conversion Between Normal and Polynomial Representation**:
   - For a normal field polynomial C, conversion between representations is possible using matrices Z and Z^(-1).
   - Matrix Z's k-th column contains x^(2k) mod C, and the normal representation b can be obtained as b = Z^(-1) * a (where a is the polynomial representation).

3. **Optimal Normal Bases (ONB)**:
   - Type-1 ONBs exist when p := n + 1 is prime, and 2 is a primitive root modulo p (also for n = 0 and n = 1). The sequence of such n is A071642.
   - Type-2 ONBs exist when p := 2n + 1 is prime, satisfying certain conditions related to the order of 2 modulo p. These n values are listed in A054639.
   - For both types, the field polynomial is the all-ones polynomial (1 + x + ... + x^n). Multiplication matrices for ONBs have sparse structures with at most two nonzero entries per row/column.

4. **Gaussian Normal Bases (GNB)**:
   - GNB generalize ONBs and have multiplication matrices with more nonzero entries as t (the type parameter) increases.
   - A type-t GNB exists for n if p := t n + 1 is prime, and gcd(n, t n/r2) = 1 where r2 is the order of 2 modulo p. No GNB exist when n is divisible by 8.

5. **Computation of Multiplication Matrix**:
   - For type-t GNBs (t > 2), an algorithm to compute the multiplication matrix involves setting up a vector F, iterating through it with powers of r (an element of order t modulo p), and populating the multiplication matrix M based on F values.

The text also mentions algorithms for constructing self-dual normal bases ([229]), conversion functions between representations in C++ class GF2n, and the use of Pascal's triangle to compute certain polynomials. It concludes by mentioning the Lucas Correspondence Theorem for computing binomial coefficients modulo a prime q.


The provided text is an excerpt from the appendix of a book about finite fields, specifically discussing Gaussian normal bases (GNB) over GF(2^n). Here's a detailed summary and explanation of the content:

**Gaussian Normal Bases (GNB):** A GNB is a special basis for the finite field GF(2^n), which allows for efficient arithmetic operations. The text focuses on two aspects related to GNBs: determining whether they exist for given n and t, and computing the corresponding field polynomial when they do.

1. **Existence of Gaussian Normal Bases:**

   - The file `[FXT: data/gauss-normal-types.txt]` lists, for each 2 ≤ n ≤ 1032, the smallest ten values of t such that a type-t GNB exists over GF(2^n).
   - Different values of t may not necessarily lead to different multiplication matrices, especially for small values of n.

2. **Computing the Field Polynomial:**

   The field polynomial (also called normal polynomial) corresponding to a given pair (n, t) can be computed using two algorithms: one that works with complex numbers and another that operates within GF(2).

   **Algorithm using Complex Numbers:**

     1. Set p = tn + 1 and determine r such that the order of r modulo p equals t.
     2. For 1 ≤ k ≤ n, compute w_k = ∑_{j=0}^{t-1} exp(a_k * 2πi/p), where a_k = 2^k * r^j mod p.
     3. Let z(x) = Π_{k=1}^n (x - w_k), which is a polynomial with real integer coefficients.
     4. Return the polynomial with coeﬃcients reduced modulo 2.

   **Algorithm working in GF(2):**

     1. Set p = tn + 1 and determine r such that the order of r modulo p equals t.
     2. Initialize M = ∑_{k=0}^{p-1} x^k (all computations are done modulo M).
     3. If t equals 1, return M.
     4. Set F_0 = 1 (modulo M).
     5. For 1 ≤ k ≤ n:
        - Compute Z_k = ∑_{j=0}^{t-1} x^(2^k * r^j) mod M.
        - Update F_k = (x + Z_k) * F_(k-1) mod M.
     6. Return F_n.

   The second algorithm avoids inexact arithmetic but has a larger polynomial modulus of degree p − 1 = n t, making it slower for large t compared to the complex number method.

3. **Optimization and Symmetry Exploitation:**

   - The computation can be optimized using trigonometric recursion (mentioned but not detailed in the provided text).
   - For even types t, real values can be used instead of complex numbers, reducing computational cost without loss of accuracy.

4. **Verification:**

   Results for type-1 bases can be verified using specific relations, while results with type-2 bases can be checked using different relations (42.8-2a and 42.8-2b).

5. **GP Implementation:**

   The text provides GP routines for computing the field polynomials using both algorithms mentioned above (`gauss_zpoly` and `gauss_poly`, `gauss_poly2`). These functions use complex arithmetic or work entirely within GF(2) to compute the normal polynomial of a type-t Gaussian normal basis.

In summary, this section delves into the existence and computation of Gaussian normal bases over GF(2^n), presenting two algorithms for determining whether such bases exist and computing their corresponding field polynomials when they do. The text also discusses optimizations and verification methods along with GP implementations of these computations.


The paper "Time- and Space-Efficient Evaluation of Some Hypergeometric Constants" by Howard Cheng, Guillaume Hanrot, Emmanuel Thomé, Eugene Zima, and Paul Zimmermann presents an algorithm for evaluating hypergeometric constants with high precision. The authors focus on the calculation of values like π, log(2), and Catalan's constant (G) using a combination of symbolic manipulation and numerical methods.

The algorithm employs a technique called "series acceleration" to speed up the convergence of series representations for these constants. This method leverages techniques such as Euler-Maclaurin summation, Pade approximation, and Richardson extrapolation to achieve high accuracy with fewer terms in the series.

The paper also discusses an implementation of this algorithm using the GNU Multiple Precision Arithmetic Library (GMP) and MPFR libraries for arbitrary precision arithmetic. The authors demonstrate that their method outperforms previous techniques in both time and space complexity, making it a practical choice for computing hypergeometric constants with high precision.

In summary, "Time- and Space-Efficient Evaluation of Some Hypergeometric Constants" by Cheng et al. presents an efficient algorithm for calculating hypergeometric constants such as π, log(2), and Catalan's constant (G) using series acceleration techniques. The authors provide both theoretical analysis and practical implementation details, showcasing the advantages of their approach in terms of speed and memory usage.


Title: Fast Fourier Transform (FFT) Algorithms

The Fast Fourier Transform (FFT) is a computationally efficient algorithm for computing the Discrete Fourier Transform (DFT), which is widely used in signal processing, digital image processing, and many other areas of mathematics and engineering. The main idea behind FFT is to reduce the number of complex multiplications required by exploiting the symmetries and periodicities of the DFT.

1. Background:
   - Discrete Fourier Transform (DFT): Given a sequence of N complex numbers x[0], x[1], ..., x[N-1], the DFT is defined as X[k] = ∑\_{n=0}^{N-1} x[n] * exp(-j2πkn/N), where j is the imaginary unit, and k ranges from 0 to N-1.
   - Complex Multiplication: A complex multiplication involves two real multiplications and one real addition (or subtraction).

2. Cooley-Tukey FFT Algorithm:
   - The Cooley-Tukey algorithm, introduced in 1965, is the most widely used FFT algorithm. It recursively divides the DFT of size N into smaller DFTs of size N/2, reducing the number of complex multiplications from O(N^2) to O(N log N).
   - The algorithm relies on the fact that any even-length DFT can be computed by combining two half-length DFTs using twiddle factors (complex exponentials).

3. Radix-2 Decimation in Time (DIT) and Decimation in Frequency (DIF):
   - The radix-2 Cooley-Tukey FFT algorithm uses either decimation in time (DIT) or decimation in frequency (DIF) to divide the DFT into smaller subproblems.
   - In DIT, the input sequence is divided into two halves, and each half is transformed separately. Then, the even-indexed coefficients are combined using twiddle factors, followed by the odd-indexed coefficients.
   - In DIF, the input sequence is divided into two sets of coefficients with even and odd indices. Each set is then transformed separately, and the results are combined using twiddle factors.

4. Split-Radix FFT:
   - The split-radix algorithm extends the radix-2 Cooley-Tukey FFT to handle prime or composite sizes other than powers of 2. It combines radix-2 and radix-3 (or higher) decompositions, reducing the number of complex multiplications further.

5. FFT Applications:
   - Signal Processing: FFT is used for analyzing and manipulating signals in various domains, such as audio processing, image compression, and communication systems.
   - Digital Image Processing: In image processing, FFT is applied to perform tasks like filtering, edge detection, and pattern recognition.
   - Solving Linear Systems: The Cooley-Tukey algorithm can be used to solve linear systems by computing the eigenvalues of a matrix through the DFT of its coefficients.

6. Challenges and Optimizations:
   - Memory Access Patterns: FFT algorithms can have irregular memory access patterns, leading to cache misses and performance degradation. Techniques like bit-reversal permutations and data reordering aim to mitigate this issue.
   - Parallelism: Modern hardware architectures support parallel processing, allowing for efficient implementations of FFT on multi-core processors, GPUs, and other specialized platforms (e.g., FPGAs).

In summary, the Fast Fourier Transform (FFT) is a crucial algorithm in signal processing, digital image processing, and many other fields due to its ability to efficiently compute Discrete Fourier Transforms with significantly reduced computational complexity compared to direct evaluation. The Cooley-Tukey algorithm, along with various extensions like split-radix FFT, forms the basis for most modern FFT implementations.


The provided text appears to be an index of terms related to various mathematical, computational, and algorithmic concepts. Here's a detailed summary and explanation of some key topics:

1. **Fourier Transform and Related Concepts**:
   - **Fourier Transform (FT)**: A mathematical technique used for transforming a time signal or function into the frequency domain. It is widely applied in signal processing, image analysis, and solving partial differential equations. The discrete version of FT, known as Discrete Fourier Transform (DFT), is a fundamental operation in digital signal processing.
   - **Fast Fourier Transform (FFT)**: An efficient algorithm to compute the DFT and its inverse. FFT algorithms reduce the computational complexity from O(n^2) for naive methods to O(n log n). Different variants of FFT, such as Cooley-Tukey and Prime Factor Algorithm (PFA), are used based on the problem's specifics.
   - **Convolution Property**: Describes how convolution in the time domain corresponds to multiplication in the frequency domain, which is crucial for various applications like filtering and image processing.
   - **Fractional Fourier Transform (FrFT)**: An extension of the standard Fourier transform that allows for a continuous rotation angle, providing more flexibility compared to traditional FFT.

2. **Graph Theory**:
   - **Path in Graph**: A sequence of adjacent nodes connecting two specific nodes within a graph. Finding paths is essential in many applications, such as network routing and finding shortest connections between points.
   - **Loopless Algorithm**: An approach that avoids cycles or loops during the execution process to prevent redundant computations or infinite recursions.

3. **Number Theory and Algebraic Structures**:
   - **Galois Fields (GF)**: Finite fields introduced by Évariste Galois, where the number of elements is a power of a prime, and operations are performed modulo an irreducible polynomial of degree n over GF(p). They play a crucial role in coding theory and cryptography.
   - **Binary Finite Fields (GF(2^n))**: Special cases of Galois fields where the characteristic is 2, with elements represented as binary words. These are fundamental in error-correcting codes and cryptography.

4. **Combinatorics and Permutations**:
   - **Gray Codes**: A sequence of integers where each number differs from its neighbors by one in terms of binary representation. They minimize the number of bit changes, useful for applications like error correction and data compression. Different variants exist, such as reflected Gray code and recursive Gray codes.
   - **Permutations and Combinations**: Arrangements or selections of elements from a set, respectively. Various representations (e.g., lexicographic order, factorial base) and algorithms (e.g., Heap's algorithm for permutations) are discussed in the index.

5. **Algorithmic Techniques and Optimizations**:
   - **In-place Routine**: Algorithms that perform computations without requiring additional memory, optimizing space complexity.
   - **Interpolation Binary Search and Linear Interpolation**: Efficient search algorithms combining binary search with interpolation to achieve faster searches in sorted arrays.
   - **Parallel Assignment (with Pseudocode)**: A programming technique where multiple variables are updated simultaneously using a single statement, improving efficiency on parallel architectures.

6. **Number Sequences and Series**:
   - The index lists numerous integer sequences, including well-known ones like Fibonacci numbers, Catalan numbers, and Bell numbers. These sequences often have connections to combinatorics, number theory, and various mathematical constants.

The provided index serves as a comprehensive reference for diverse mathematical and computational concepts, their applications, and related algorithms, which can be valuable resources for researchers, engineers, and students in fields like computer science, mathematics, and engineering.


The text provided is an index from a technical or mathematical context, listing various terms related to algorithms, number theory, combinatorics, and other areas of mathematics. Here's a detailed explanation of some key topics:

1. **Run-Length Limited (RLL) words**: RLL codes are a type of data compression technique used in digital systems. In these codes, contiguous sequences of data values are replaced by pairs consisting of the count (run length) and value. The number 310 refers to the specific constraint or code rate used in this context.

2. **Rogers-Ramanujan identities**: These are two beautiful infinite series identities discovered independently by Leonard James Rogers and Srinivasa Ramanujan, both formulated in the early 20th century. They describe relationships between certain partitions of integers. The number 347 likely refers to a specific reference or page number where these identities are discussed.

3. **Roots**:
   - **Extraction**: Finding roots (or zeros) of polynomials or functions.
   - **Inverse**: The multiplicative inverse in a given ring (e.g., modulo arithmetic).
   - **Modulo pn (p-adic)**: Computing roots in p-adic number fields.
   - **Of a Polynomial, Divisionless Iterations**: Methods for finding roots without using polynomial division.
   - **Primitive**: A root that generates all other roots when raised to appropriate powers. The term is used in different contexts like GF(2^n) (Galois Field) and modulo m (modular arithmetic).
   - **Of Mersenne Primes**: Roots of unity related to Mersenne primes, which are prime numbers that can be written in the form 2^p − 1 for some prime p.

4. **Roots of Unity**: Complex numbers whose nth power is 1 (where n is a positive integer). When they sum to zero, it's related to specific properties of these numbers and their cyclotomic polynomials.

5. **Rotation**:
   - **Bit-wise**: A shift operation where bits are moved circularly within a fixed-size container.
   - **By Triple Reversal**: A technique in combinatorics involving permutations, which can be visualized as a rotation through three axes.

6. **Row-Column Algorithm**: A method used for matrix multiplication or other operations, where elements are processed row by row and column by column in a systematic manner.

7. **Ruler Constant & Ruler Function**: The ruler constant is the smallest positive real number such that there exists a ruler of that length with no two marks at a distance less than the constant from each other. The ruler function generates the sequence of lengths of all possible rulers for a given integer.

8. **Sedenions**: An 16-dimensional algebraic system, extending the concept of complex numbers and quaternions. They have applications in areas like theoretical physics and geometry.

9. **Selection Sort**: A simple sorting algorithm that repeatedly selects the smallest (or largest) element from the unsorted part of the array and moves it to its correct position in the sorted part.

10. **Sentinel Element**: A special value used to simplify array or list manipulation algorithms, such as searching or inserting elements. It acts as a marker for the end of valid data.

11. **Self-Correlation**: A measure of similarity between a signal and a delayed copy of itself regarding amplitude at different points in time.

12. **Self-Dual (Basis over GF(2^n))**: In Galois Field theory, self-dual bases are special types of basis elements for GF(2^n) that have specific symmetry properties.

13. **Self-Inverse Permutation, Random**: A permutation that is its own inverse, chosen randomly from the set of all possible permutations of a given size.

14. **Self-Reciprocal Polynomial**: A polynomial that remains unchanged under the substitution n ↦ 1/n (except for constant terms).

15. **Set Partition & Subset**: Concepts in combinatorics where elements are divided into non-empty subsets such that each element appears in exactly one subset, and all possible subsets of a given set, respectively.

16. **Shift Operator**: Used in various transforms (e.g., Fourier or Hartley) to represent the shift operation. It's often denoted by 'e' raised to the power of 'i2πft', where f is the frequency and t is time.

17. **Shift Register Sequence (SRS)**: A type of pseudorandom number generator using a shift register, cycling through a sequence determined by its initial state and feedback polynomial.

18. **Short Division & Short Multiplication**: Efficient algorithms for division and multiplication in specific numeric systems or with particular constraints on the operands.

19. **Sieve of Eratosthenes**: An ancient algorithm used to find all prime numbers up to any given limit by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting from 2.

20. **Sign Decomposition of a Matrix**: Breaking down a matrix into components related to its positive and negative eigenvalues or singular values.

21. **Sign of a Permutation/Fourier Transform**: The sign (or parity) of a permutation or the phase factor in the Fourier transform, indicating whether the transformation preserves or reverses orientation.

22. **Signed Binary Representation**: A way to represent signed integers using binary digits, often including a separate bit for the sign and adjusting magnitude accordingly.

23. **Simple Continued Fraction**: A representation of real numbers as the limit of a sequence of simpler fractions, where each fraction's numerator is one more than its denominator, and so on.

24. **Single Track**: Refers to different encoding schemes for combinatorial objects (like permutations or subsets) that can be represented as sequences of '1's and '0's along a single line.

25. **Singular Value Decomposition (SVD)**: A factorization of a real or complex matrix, expressing it as the product of three matrices: an orthogonal/unitary matrix, a diagonal matrix, and another orthogonal/unitary matrix. It has applications in data compression, noise reduction, and solving linear systems.

26. **Skew Circular Convolution**: A generalization of circular convolution where the roles of input sequences are swapped but with a phase shift between them.

27. **Slant Transform & Sequency-Ordered Slant Transform**: Methods for analyzing or transforming signals/data, particularly in the context of wavelets and multiresolution analysis, where 'sequency' refers to the order of nonzero coefficients in a binary sequence representation.

28. **Smart Compiler**: A compiler that employs advanced techniques (like optimization, parallelization, etc.) to generate more efficient machine code from high-level source code.

29. **Stable Sort**: A sorting algorithm that maintains the relative order of records with equal keys (i.e., it's stable under equality).

30. **Stack (LIFO)**: A Last-In-First-Out data structure where elements are added and removed from one end, often used for function calls in programming languages or undo operations.

These topics represent a mix of fundamental concepts and specific algorithms/techniques within mathematics and computer science. They span areas like algebra, number theory, combinatorics, and algorithm design, with applications ranging from data compression to signal processing and cryptography.


