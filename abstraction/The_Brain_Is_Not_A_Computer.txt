85 billion. It's a number that doesn't really compute in your head, does it? It's too big.
It's completely abstract. But let's try for a second to ground it. We're talking 85 billion
neurons. That's the best guess we have right now for what's inside your skull. And that's not even
the whole story. Not even close. You have to add another 85 billion glial cells, just sort of,
you know, keeping the lights on. And just to make the problem even bigger, that's only what's inside
the cranium. We're not even touching the spinal cord or the enteric nervous system, you know,
the second brain around your intestines. Which has more neurons than a cat's entire brain,
by the way. Right. It's just the scale is staggering. So you have this biological system of such
overwhelming, almost ludicrous complexity that the numbers just start meaning anything. It is,
and I don't think this is hyperbole, the most complex object in the known universe. It's the
ultimate hardware. But here's the question that really gets me. And it's kind of the whole mission
for our deep dive today. How on earth do we, with our very, very limited minds, even start to wrap
our heads around something that complex? It feels like trying to, I don't know, download the internet
onto an old floppy disk. It's the ultimate paradox, isn't it? The brain trying to understand the brain.
It's this perfect recursive loop that neuroscience has been wrestling with since, well, forever.
Right. And usually when you read a pop neuroscience book or listen to a show,
you get these really neat, tidy little stories. This part of the brain is for fear. This chemical
makes you happy. The brain is a computer. Exactly. But today, today we're going to kind
of ruin all that for you. In a good way, hopefully. Yeah. We're going to peel back the layers and look
at the messy, complicated structure underneath. We're taking a deep dive into a really fascinating,
and I have to say pretty dense work by M. Chiramuta. It's called The Brain Abstracted,
Simplification in the History and Philosophy of Neuroscience.
And the whole point today is to explore this, this fundamental tension at the heart of science
itself. Which is the tension between the chaotic, hyper-complex reality of biology and our very human,
very deep-seated need for simple, elegant rules.
And Chiramuta's core argument here is really going to challenge how you think about science.
We have this idea of science as a mirror, right? You just hold it up to nature and it reflects the
truth. Sure. That's the classic view. Scientific realism, the God's eye view, as they call it. You
look, you see the world as it is. But this book says science isn't a mirror at all. It's a filter,
or maybe even a better word is a tool. The big aha moment here, and if you only take one thing away
from this dive, this should be it, is that science doesn't discover simplicity in the brain.
It creates it. It constructs it. Okay. We need to unpack that right now because that sounds,
that sounds pretty radical. You're saying the simplicity we see in our textbooks, the neat
diagrams that it isn't actually in the brain? Not in the way we think it is. The brain is what
the author calls the most complicated thing. It's essentially infinite in its detail. If we tried
to describe it exactly as it is, every single molecule, every interaction would just drown.
It would be pure noise, data without meaning. Exactly. So to get any understanding at all,
we are forced to ignore huge chunks of reality. We have to. Well, we have to abstract.
So every scientific model we have is a simplified map, not the actual territory. Precisely. And today,
we're going to look at how that map gets drawn. We're going to talk about something called the
bandpass filter of human thought. Then we'll get into the three main strategies scientists use to
simplify the brain. Math, reduction, and analogy. And after that, we're going to explore a really
new philosophy called haptic realism. Which, believe it or not, involves tapping on melons.
I am genuinely looking forward to the melon part. It'll make sense. I promise. And then we'll wrap
up with a big cautionary tale from history, the rise and fall of reflex theory, to see what happens when
we fall in love with our simplified maps and forget about the real world. A journey from the
petri dish to the philosophy of AI. Okay. Let's start. Section one, the complexity trap.
Let's begin with that textbook neuron you mentioned. We all have that picture in our
heads from high school biology class. I can see it right now. It looks like a little tree, right?
You have the roots, the dendrites, where the input comes in. Then you have the cell body that does the
processing. And then the long trunk, the axon, sends the output signal to the next neuron.
Input, process, output. A simple biological switch. Yeah. Transistor. Easy. But the source
material just completely demolishes that tidy little image. It argues that real biology is,
it's messy, it's wet, and it absolutely refuses to conform to our simple models. Let's take those
dendrites, the inputs. The roots of the tree. Okay. In that simple model, they're just passive wires,
right? They just carry electricity to the center. But what we're finding out now is that dendrites can
act like tiny computers all by themselves. Wait, the wire is a computer? Essentially, yeah. Yeah.
They can perform their own complex calculations. They can filter signals, amplify them, even perform
logic functions before the main signal even gets to the cell body. So you have computation happening
before the main computation. That's already breaking the model. And it gets worse. Look at the synapse,
the gap between two neurons. We think of it as a simple switch. It's on or it's off. It fires or it
doesn't. Binary code. Zeros and ones. It's the whole foundation of the computer metaphor for the
brain. We wish. The source points out that a single synapse, one tiny connection,
contains between 2,000 and 3,000 different types of proteins. Hold on. You said 2,000 types of
proteins, not just 2,000 protein molecules. 2,000 distinct kinds of proteins. That's its proteome.
And each one is doing something different, changing the sensitivity, adjusting the timing,
anchoring receptors. It's a city in there, not a switch. Wow. Okay. And then you have the glial
cells. You know, the cells we used to call brain glue because we thought they just held the important
neurons in place. That's where the name glia comes from, right? Greek for glue. Exactly. We ignored
them because they don't fire electrically in the same way. But it turns out they're doing all this
crucial work. They're regulating blood flow, cleaning up waste, modulating signals between neurons.
There are 16 different kinds of astrocytes alone. Protoplasmic, fibrous, radial metal. It sounds like
a list of alien species from a sci-fi novel. It does. But the point is, to make things manageable,
we usually just lump them all together. We say glia or support cells. We have to delete the difference
between protoplasmic and fibrous just to be able to talk about them at all. Because if you didn't,
you would just be overwhelmed. You'd be stuck. And this brings us to a really cool concept from the book,
which is the Heraclitian brain. Heraclitus. He's the Greek philosopher who said you can't step in
the same river twice. Exactly. Because it's not the same river and you're not the same person.
The brain is like that river. It is physically changing every single microsecond. We talk about
brain plasticity, but we usually mean it on the scale of like learning a new language over a few years.
But the source argues it's constant. It's happening right now. You have metabolic flux,
chemical gradients are shifting, proteins are folding and unfolding. One neuron is connected
to 10,000 others. So the level of interactivity is just. It's off the charts.
So the brain is never the same object from one moment to the next.
Never. If you could take a snapshot of your brain now and another one a millisecond later,
you are technically looking at two different physical objects.
That sounds like an absolute nightmare for a scientist. I mean, how do you establish a fact
about something that changes before you can even measure it?
And that is the absolute core of the problem. This is where the idea of the bandpass filter
becomes so important. I love the analogy the book uses for this, the lawn.
Oh, this was great. It really made the idea of abstraction click for me. So, okay, walk me
through it. You're standing on your porch. You're looking out at your lawn. What do you see?
I see grass. It's green. Maybe there are some flowers. Daisies.
Right. You say daisies. You see all the little white and yellow flowers and you group them into
one category. Look at the daisies. But what happens if you get down on your hands and knees
with a magnifying glass and you look at just two of those flowers?
They're completely different. I mean, no two are identical. Every petal has a slightly different
curve. One is a little bigger. One has a tiny bug on it. One is tilted a different way.
Exactly. And if you had to describe every single one of those unique details,
every petal curve, every bug, you could never just say, look at the pretty lawn. You'd be stuck
describing data for the rest of your life. You couldn't communicate the general idea.
Right. So to create the concept daisy, I have to actively ignore the differences. I have to blur
the details. That's the filter. In audio, a bandpass filter lets a certain range of frequencies through
and blocks everything else. Our thought process is doing the same thing with reality. You let the general
daisiness pass through, but you filter out the specific unique details of this flower right here.
So thinking itself is an act of subtraction. We're deleting parts of reality to create meaning.
That is literally the definition of abstraction. The word means to draw away. And to do neuroscience,
scientists have to do the same thing. They have to ignore that neuron A is slightly different from neuron B
in order to create a neuron type. If they didn't, they just have billions of unique objects and no
general laws. We do this everywhere, not just in science. The book mentions a census. To count
people, you have to turn them into a unit. You strip away their entire life, their hopes, their fears,
what they eat for breakfast, just to make the data manageable. It's a necessary violence, as you put
it earlier. We do it to reality to make it understandable. We filter the world. We only let
the information that matters to us right now come through. But it means we're always losing
something. We're losing the particular, the unique. If you don't filter, you have chaos. If you do
filter, you have a simplified model that isn't, you know, technically true. And that's the trap.
Okay. So if the brain is this infinitely complex thing and we're forced to filter it,
how would you scientists actually do it? What are the tools for this simplification?
The book lays out three big strategies. And once you see them, you start to see them everywhere.
They're really the pillars of modern science. The first strategy is
mathematization. Turning the world into numbers?
Yep. Think about counting sheep. It's the classic example. To count a flock of sheep,
what do you have to do in your head? I just, I look at them and I go, one, two, three.
But to do that, you have to pretend they're all identical units. You have to ignore that one
sheep is scruffy and one is fat and one is a lamb and one is old. To the number one,
they're all exactly the same. Right. The philosopher Henri Bergson made this great point.
Math assumes homogeneity or seamness, but nature is all about heterogeneity or difference.
Exactly. There are no numbers floating around in nature. They're only unique things. We're the
ones who project the numbers onto them. When we model the brain with math, we're swapping out the
real, lumpy, unique neuron for a perfectly smooth mathematical average.
It's like using autotune on a singer's voice. You make the pitch perfect, you smooth it out,
but you lose some of that raw human character. The texture is gone.
That is a perfect analogy. Math is the autotune of reality. It lets us solve the equations,
but it also creates a distance between us and the physical thing. Once a neuron becomes just a
variable in an equation, you forget that it's a living cell that needs energy and gets tired.
Okay. So strategy one is math. Make it abstract and numerical. What's the second strategy?
Strategy number two is reduction, or as the book calls it, creating a microcosm.
This is the Petri dish.
This is the Petri dish.
Right.
The real world is just too loud, too uncontrolled. There are too many variables.
So what do scientists do? They take a thin slice of brain tissue and they put it in a dish.
Or they put a rat in a completely empty box. They reduce the complexity of the environment
to study one small part in isolation.
Which makes sense. I mean, if you want to understand how a car engine works,
you're probably going to take it out of the car. You don't need the whole car driving down the highway
to understand how a piston fires.
But there's a huge risk involved in that. And the source calls this the problem of context independence.
Okay. That's a bit of jargon. Let's break that down.
It's the assumption that the piece of brain in the dish behaves in the exact same way
as that piece of brain inside a living, breathing, running animal.
You're assuming that the context, the body, the hormones, the environment,
doesn't change how the part functions.
And in biology, that's almost never a safe assumption.
Almost never. A neuron behaves completely differently depending on the chemical soup
it's swimming in at any given moment. It depends on whether the animal is hungry
or scared or asleep. If you only study a neuron in a quiet state in a dish,
you might totally miss how it acts when the animal is, say, hunting for food.
It's like studying a tiger that was born and raised in a zoo cage.
You might learn a lot about its anatomy, its muscles, but you would learn nothing
about what it means to be a tiger in the jungle. You miss the hunting behavior
because you've removed the jungle.
You're studying an artifact of the lab, not the real thing.
Okay. So we have math and reduction. And the third strategy.
This one is my favorite. Analogy.
It's so powerful and so pervasive. We use things that we made to explain the thing that we are.
We use artifacts to explain organisms.
The book talks about how this changes with technology. Back in the 17th century,
the most complex, amazing machine anyone had ever seen was the clock.
Gears, springs, hidden mechanisms. It was magic.
So naturally, people started to see the whole universe as a giant clockwork machine.
The heart is a pump. The limbs are levers and pulleys. The brain is a complex set of gears.
And today, the most complex thing we make is the computer.
So the brain is a computer.
Right. It processes information. It has memory storage. It has circuits.
We talk about neural wiring and mental bandwidth and things being hard-coded.
But the source makes the great point that this is a simplifying lens.
It lets us see certain things, like how signals are transmitted,
but it also hides other, maybe more important things.
Well, think about the most basic difference. A computer is modular. You can take out the RAMSBIC and plug in a new one.
The processor is totally separate from the hard drive. It was designed by an engineer with a blueprint.
And the brain.
The brain grew itself. It's a self-organizing, self-repairing fluid system.
In the brain, memory and processing happen in the very same places. There's no separation between hardware and software.
That's a huge point. In a computer, the software is abstract. I can run the same program on a thousand different machines.
But in the brain, the software, your thoughts, your memories, is the physical structure of the hardware.
They're inseparable. And it's so interesting how the analogy we choose shapes the very questions we ask.
If you think the brain is a computer, you go looking for the algorithm. You search for the code.
And if you think it's a clock, you go looking for the gears.
But what if the brain is neither of those things?
Then you might be spending your whole career looking for something that just isn't there.
You're hunting for a ghost.
Okay, so this brings us to what I think is the most challenging, but also the most interesting philosophical turn in the book.
If all our models are simplified, if they're mathematically idealized, reduced down to a patriotish, and based on these flawed analogies,
then in what sense are they even true?
Are they true at all?
I mean, if the map leaves out most of the territory, is it basically a lie?
Or at least a useful fiction. Are we just making up stories that help us feel like we're in control?
This is where Achiromita introduces a new philosophy of science called haptic realism.
Haptic. Like the haptic feedback on my phone. The little buzz I feel when I type.
Exactly. It comes from the sense of touch.
Yeah.
For centuries, the philosophy of science has been obsessed with the visual metaphor.
And we talked about it earlier, the mirror, the God's eye view. The idea that the scientist is a detached observer just watching nature from a distance.
Like a camera taking a picture.
But the book argues we need to shift from a visual metaphor to a tactile one. A touch metaphor.
So instead of seeing, we're handling...
Think about how you use your hands. You don't just look at an object. You interact with it.
The example in the book is tapping a melon at the grocery store.
Which I do all the time, even though I have no idea what I'm listening for. But yeah, I tap it.
Why do you tap it?
To see if it's ripe, I guess. To hear if it sounds hollow?
Right. You're poking it. You are actively provoking the melon to get a response.
You're not just looking at its green skin.
Haptic realism suggests that this is what science is.
We can never know the brain in itself. The pure, unadulterated reality.
We can only know the brain as it reacts to our experiments, to our pokes and prods with electrodes.
So the knowledge isn't in a perfect picture of the thing. The knowledge is in the interaction.
It's the feeling of how the thing pushes back when you poke it.
Precisely. We handle the brain with our tools.
If I stimulate this nerve, the leg kicks.
That is a haptic truth. It tells me how the system responds to my intervention.
It doesn't tell me everything about that nerve, but it tells me if I push here, it moves there.
But doesn't that make truth relative?
If I poke it one way, I get one result.
If someone else pokes it another way, they get a different result.
It leads to a philosophy called perspectivism.
The idea is, because the brain is so hyper-complex, there's no single true map that can capture everything.
There are multiple valid maps.
Multiple valid maps, depending on what you want to do.
A neurosurgeon needs a map that shows her the physical geography of the brain.
We're not to cut so the patient can still talk.
That's a structural map.
Okay.
But an AI researcher might need a map of how neurons compute probabilities.
That's a purely mathematical map.
It's just a set of equations.
And those two maps would look completely different.
They might be unrecognizable to each other, but, and this is the key, they can both be true.
True in what sense then?
True in the sense that they are useful for a successful interaction.
Did the surgery work?
Was the patient okay?
Then the surgeon's map was true for that purpose.
Did the AI model make accurate predictions?
Then the mathematical map was true for that purpose.
Truth isn't about a perfect picture.
It's about a successful intervention.
This feels like such a huge shift.
It's more humble.
We're not discovering the timeless laws of nature carved on stone tablets.
We're more like mechanics, building tools to handle a very complicated engine.
We are engaged with the world, not standing apart from it.
And speaking of humility, or maybe the lack of it, the book gives us this incredible cautionary tale from history, a time when scientists fell so in love with their simple map that they completely missed the territory.
The reflex theory.
This takes us to section four.
Okay, so the scene.
What time period are we talking about here?
This is the early 20th century, and this was the dominant idea.
You have huge names.
Yvonne Pavlov, Jacques Loeb, Charles Sherrington.
Pavlov and his dogs, right?
The bell rings, the dog salivates.
That's the most famous example.
But it was so much bigger than that.
It was an entire worldview.
The central idea was that they had found the atom of all behavior.
They believed that any complex action, playing the piano, writing a book, fighting a war, was just a long chain of simple automatic reflexes all strung together.
That's the ultimate reductionist dream, isn't it?
Break everything down to its simplest possible unit, the reflex arc.
Stimulus and nerve fires, muscle contracts.
Done.
And they were very explicit about this.
Jacques Loeb, one of the main proponents, said he wanted to find the rationalistic elements of life.
Just like chemistry has a periodic table of elements, he wanted a periodic table of behaviors.
Yeah.
If he could just map every single reflex, he believed he could predict everything an animal or a human would ever do.
It sounds so elegant, so clean.
So what went wrong?
Why don't we see ourselves as just bundles of reflexes anymore?
Because the messy reality of biology started to push back.
It turned out that the simple reflex wasn't so simple.
It wasn't stable.
The book talks about the scratch reflex in dogs, as an example.
Right.
If you tickle a certain spot on a dog's back, its leg starts scratching.
Simple reflex.
But Sherrington, who studied this obsessively, found that the reflex was incredibly variable.
It changed depending on the dog's posture.
If the dog was standing up to walk, the scratch reflex was suppressed.
If the dog was tired, the reflex was weak.
So it wasn't a stable, predictable atom of behavior.
It was fluid.
It depended on what the rest of the animal was doing.
It depended on the whole organism.
And then you get the really weird story about Pavlov dogs going into a hypnotic stupor.
That does not sound good.
It's fascinating.
Pavlov was obsessed with isolating these reflexes perfectly.
He built something called the Tower of Silence, a laboratory that was soundproof, vibration-proof.
He even tried to control for the smell of the researcher.
He wanted to study the biological machine in a complete vacuum.
To a total reduction of the environment.
No outside context at all.
None.
But when the dogs were put through these incredibly rigid, repetitive conditioning experiments in this silent box, day after day, they didn't just behave like machines.
They started to shut down.
They would fall into these strange, trance-like stupors.
Some became weirdly aggressive.
They broke.
The system broke.
It did.
And a neurologist named Kurt Goldstein, who was a huge critic of this whole theory, argued that these so-called simple reflexes were actually artifacts.
They were things you only see in the lab.
They only appear when you isolate a nerve or damage the animal's brain or lock it in a silent box for weeks.
So you never see a pure reflex in a healthy animal lighting its life in the real world.
Never.
Goldstein said the simplicity was manufactured.
It was a pathological state, not a natural one.
It was like we created a robot dog in the lab and then declared that all dogs are robots.
But, and this is the really wild part, people stuck with this theory for a long, long time.
I mean, the behaviorists, people like B.F. Skinner, were still pushing this well into the 1950s.
If the biology was showing it was flawed, why did they hang on so tight?
It comes back to that crucial shift we talked about, the difference between understanding and control.
The behaviorists, especially John B. Watson and Skinner, basically stopped caring if a reflex was a real thing inside the animal's head.
They became what are called instrumentalists.
Instrumentalists, as in does the instrument work?
Is it useful?
Exactly.
Watson famously wrote that the goal of psychology was to get life phenomena under our control.
It wasn't biology anymore.
It was engineering.
If pretending an animal is just a bundle of reflexes allows me to train a pigeon to guide a missile, which, by the way, Skinner actually did.
Wait, what? Skinner trained pigeons to guide missiles?
It was called Project Pigeon.
During World War II, he trained pigeons to peck at a target on a small screen, and their pecks would steer the fins of a gliding bomb.
It actually worked, too, in simulations.
The military never used it, but the point is, for Skinner, if the model gave him control, it was a good model.
That's amazing.
But even that engineering approach eventually hit a wall, right?
The book tells the story of the Brellins.
Yes, Marion and Keller Brellant.
They were students of Skinner, total true believers.
They left the university to start a company that used these strict conditioning principles to train animals for commercials and county fairs.
Like getting a chicken to play tic-tac-toe.
That kind of thing.
And at first, it worked really well.
But then, things started to go wrong.
They wrote this famous paper called The Misbehavior of Organisms.
Which is a great title.
A direct shot at Skinner's big book, The Behavior of Organisms.
Well, it was.
They described how they tried to train a raccoon to pick up a coin and put it in a piggy bank.
The rule is simple.
Perform the action, get a food reward.
But after a while, the raccoon just wouldn't let go of the coin.
It started rubbing the coins together, dipping them in and out of the bank.
It was washing them like raccoons instinctively do with their food in the wild.
Exactly.
Its deep-seated evolutionary instincts, its biological wetware, completely overrode the simple conditioning.
The complexity of the real animal broke through the simplified model.
It wasn't a blank slate you could program.
It was a raccoon.
So the lesson is, you can simplify for the sake of control, but if you simplify too much, the messy reality will eventually bite you back.
You lose your ability to predict what happens in the real world because you've ignored the organism's own internal complexity.
The map was just too simple.
The territory revolted.
This feels so incredibly relevant to everything happening today with artificial intelligence.
We're constantly told that AI is like the brain, or it's based on the brain.
It is the exact same dynamic playing out again.
We're just back to the computer analogy.
We're projecting our newest, most complicated machine onto ourselves.
Right.
We treat the brain as an information processor, and we assume that if we can just copy the computation, the math will end up with real intelligence.
But the source issues a strong warning here about something called biological naturalism.
This is the argument that intelligence might not just be software that you can run on any old hardware.
It might actually depend on all those messy, specific biological details we talked about.
The specific proteins, the 16 types of astrocytes, the feeling of a body getting hungry or tired.
All of it.
The idea is if we abstract all of that away and just say intelligence is just math, we might be making the exact same mistake the reflexologist made.
We think we've found the atoms of thought computation when really we've just created another convenient but ultimately incomplete model.
And you can see this tension today with large language models.
They are amazing at prediction.
They can predict the next word in a sentence with incredible accuracy.
But do they actually understand what they're saying?
And this connects perfectly to the tradeoff that Shirimuta highlights.
We now have these AI systems like deep learning that are so complex that we, the creators, don't really understand how they work inside.
They're black boxes.
We have traded understanding for prediction.
We can control the output, but we've lost our grasp of the mechanism.
It's the inverse of the old science.
We used to simplify things to understand them.
Now we build massively complex things to control them.
In both cases, there's this gap between our map and the territory.
We're either holding a map that's too simple to be true, like the reflex, or a map that's too complex for us to even read, like a modern AI.
So after all of this, what's the big takeaway for us, for you listening?
We've gone through filters and petri dishes and melon tapping and rebellious raccoons.
It feels like we've dismantled a lot of the certainty we thought we had.
I think the takeaway is a call for a kind of intellectual courage.
It's so tempting to grab onto the simple story.
The brain is just a computer.
Behavior is just reflexes.
Depression is just a chemical imbalance.
It makes us feel safe.
It makes us feel like we've solved the puzzle.
But the reality is what the philosopher Leibniz called the divine machine.
Exactly.
He called organisms divine machines because they're machines made of other machines, which are made of other machines, all the way down.
There's an infinite depth.
Every time you zoom in on a cell, you don't find simplicity.
You find even more complexity.
A computer chip, if you zoom in far enough, just becomes plain silicon.
The cell becomes organelles, then proteins, then atoms.
And the organization goes all the way down.
And the book ends on this really beautiful philosophical note, Dr. Ignorantia.
Knowing ignorance, or learned ignorance, it doesn't mean giving up.
It's not about despair.
It's the wisdom of knowing that our models are finite cups, and we're trying to use them to hold an infinite ocean.
We can capture a little bit of the water, and that's incredibly useful.
But we should never mistake our cup for the entire sea.
That's it, exactly.
We should absolutely use the models.
Use the math.
Use the analogies.
They are how we build medicines and create technologies.
But we must never forget that they are tools that we build.
They are our filters.
They are not the world itself.
There's a quote from Gutha that the author reflects on near the end.
In the human mind, just as in the universe, there is no top or bottom.
It gives you vertigo just thinking about it, and it leaves you with a final provocative thought.
If science is a form of touch, a way of handling the world, what does it say about us that we seem so obsessed with turning this living, breathing, mysterious brain into a machine that we can take apart and control?
Maybe the biggest mystery isn't just in the neurons themselves.
Maybe it's also in the observer, in us, trying so hard to pin them down.
I think that's a perfect place to leave our dive for today.
This has been, well, my brain feels a little bit abstracted, but in a very good way.
I feel like I've zoomed out.
It's been a real pleasure handling these complex ideas with you.
Thanks for listening.
Keep asking the hard questions.
Keep tapping the melons.
And always remember, the map is not the territory.
We'll see you next time on the Deep Dive.
